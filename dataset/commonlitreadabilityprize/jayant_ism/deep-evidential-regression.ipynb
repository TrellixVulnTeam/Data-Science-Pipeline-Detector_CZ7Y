{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit Readability\nCurrently, most educational texts are matched to readers using traditional readability methods or commercially available formulas. However, each has its issues. Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence). As a result, they lack construct and theoretical validity. At the same time, commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available.\n\nCommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods.\n\nIn this competition, youâ€™ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport spacy as sp\nimport tensorflow as tf\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-08T12:32:04.549702Z","iopub.execute_input":"2021-07-08T12:32:04.550125Z","iopub.status.idle":"2021-07-08T12:32:11.000902Z","shell.execute_reply.started":"2021-07-08T12:32:04.55004Z","shell.execute_reply":"2021-07-08T12:32:10.99997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Deep Evidential Regression](https://www.mit.edu/~amini/pubs/pdf/deep-evidential-regression.pdf)\nDeterministic neural networks (NNs) are increasingly being deployed in safety critical domains, where calibrated, robust, and efficient measures of uncertainty are crucial. In this paper, we use a method for training non-Bayesian NNs to estimate a continuous target as well as its associated evidence in order to learn both aleatoric and epistemic uncertainty. We accomplish this by placing evidential priors over the original Gaussian likelihood function and training the NN to infer the hyperparameters of the evidential distribution.","metadata":{}},{"cell_type":"code","source":"class DenseNormalGamma(tf.keras.layers.Layer):\n    \"\"\"Implements dense layer for Deep Evidential Regression\n    \n    Reference: https://www.mit.edu/~amini/pubs/pdf/deep-evidential-regression.pdf\n    Source: https://github.com/aamini/evidential-deep-learning\n    \"\"\"\n    \n    def __init__(self, units):\n        super(DenseNormalGamma, self).__init__()\n        self.units = int(units)\n        self.dense = tf.keras.layers.Dense(4 * self.units, activation=None)\n\n    def evidence(self, x):\n        return tf.nn.softplus(x)\n\n    def call(self, x):\n        output = self.dense(x)\n        mu, logv, logalpha, logbeta = tf.split(output, 4, axis=-1)\n        v = self.evidence(logv)\n        alpha = self.evidence(logalpha) + 1\n        beta = self.evidence(logbeta)\n        return tf.concat([mu, v, alpha, beta], axis=-1)\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], 4 * self.units)\n\n    def get_config(self):\n        base_config = super(DenseNormalGamma, self).get_config()\n        base_config['units'] = self.units\n        return base_config","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:36:19.008388Z","iopub.execute_input":"2021-07-08T12:36:19.008908Z","iopub.status.idle":"2021-07-08T12:36:19.01887Z","shell.execute_reply.started":"2021-07-08T12:36:19.008865Z","shell.execute_reply":"2021-07-08T12:36:19.018069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def NIG_NLL(y, gamma, v, alpha, beta, reduce=True):\n    twoBlambda = 2*beta*(1+v)\n\n    nll = 0.5*tf.math.log(np.pi/v)  \\\n        - alpha*tf.math.log(twoBlambda)  \\\n        + (alpha+0.5) * tf.math.log(v*(y-gamma)**2 + twoBlambda)  \\\n        + tf.math.lgamma(alpha)  \\\n        - tf.math.lgamma(alpha+0.5)\n\n    return tf.reduce_mean(nll) if reduce else nll\n\ndef NIG_Reg(y, gamma, v, alpha, beta, reduce=True):\n    error = tf.abs(y-gamma)\n\n    evi = 2*v+(alpha)\n    reg = error*evi\n\n    return tf.reduce_mean(reg) if reduce else reg\n\ndef EvidentialRegression(y_true, evidential_output, coeff=1.0):\n    \"\"\"Implements loss for Deep Evidential Regression\n    \n    Reference: https://www.mit.edu/~amini/pubs/pdf/deep-evidential-regression.pdf\n    Source: https://github.com/aamini/evidential-deep-learning\n    \"\"\"\n    \n    gamma, v, alpha, beta = tf.split(evidential_output, 4, axis=-1)\n    loss_nll = NIG_NLL(y_true, gamma, v, alpha, beta)\n    loss_reg = NIG_Reg(y_true, gamma, v, alpha, beta)\n    return loss_nll + coeff * loss_reg","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:36:19.523786Z","iopub.execute_input":"2021-07-08T12:36:19.524312Z","iopub.status.idle":"2021-07-08T12:36:19.533935Z","shell.execute_reply.started":"2021-07-08T12:36:19.524239Z","shell.execute_reply":"2021-07-08T12:36:19.533124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation\nFirst we start by loading in the training and test data and view the first 5 samples.","metadata":{}},{"cell_type":"code","source":"# load the training and test data\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv').drop(columns = ['url_legal', 'license'])\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv').drop(columns = ['url_legal', 'license'])\n\n# display a sample of the training data\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:36:20.550117Z","iopub.execute_input":"2021-07-08T12:36:20.550729Z","iopub.status.idle":"2021-07-08T12:36:20.609131Z","shell.execute_reply.started":"2021-07-08T12:36:20.550692Z","shell.execute_reply":"2021-07-08T12:36:20.60828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we convert the text snippets to feature vectors using the pre-trained spaCy model. Any other embedding model or feature engineering techniques can be used here as long as they are properly preprocessed and compatible with TensorFlow.","metadata":{}},{"cell_type":"code","source":"# load large English spacy model\nnlp = sp.load('en_core_web_lg')\n\n# get spacy embeddings for training data\nwith nlp.disable_pipes():\n    train_vectors = pd.DataFrame(\n        np.array([nlp(text).vector for text in train['excerpt']])\n    )\n    \n# get spacy embeddings for test data\nwith nlp.disable_pipes():\n    test_vectors = pd.DataFrame(\n        np.array([nlp(text).vector for text in test['excerpt']])\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:36:22.672819Z","iopub.execute_input":"2021-07-08T12:36:22.673163Z","iopub.status.idle":"2021-07-08T12:38:09.466849Z","shell.execute_reply.started":"2021-07-08T12:36:22.673133Z","shell.execute_reply":"2021-07-08T12:38:09.46586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we train a simple dense neural network with the deep evidential loss described earlier.","metadata":{}},{"cell_type":"code","source":"# build model\nmodel = tf.keras.Sequential(\n    [\n        tf.keras.layers.Dense(units =20, activation = 'relu'),\n        tf.keras.layers.Dense(units = 10, activation = 'relu'),\n        DenseNormalGamma(1)\n        \n    ]\n)\n\n# compile model\nmodel.compile(\n    optimizer = 'adam', \n    loss = EvidentialRegression,\n    metrics = ['mse']\n)\n\n# create early stopping callback\ncallback1 = tf.keras.callbacks.EarlyStopping(\n    monitor = 'val_loss', \n    mode = 'min',\n    patience = 25,\n    restore_best_weights = True\n)\n\n# create reduce LR callback\ncallback2 = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.25, \n    patience = 5, \n    verbose = 0,\n    mode = 'min'\n)\n\n# fit model to training data\nhistory = model.fit(\n    x = train_vectors, \n    y = train['target'], \n    validation_split = 0.2, \n    batch_size = 4,\n    epochs = 500,\n    callbacks = [callback1, callback2],\n    verbose = 1\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:09.468215Z","iopub.execute_input":"2021-07-08T12:38:09.46851Z","iopub.status.idle":"2021-07-08T12:38:41.740349Z","shell.execute_reply.started":"2021-07-08T12:38:09.468476Z","shell.execute_reply":"2021-07-08T12:38:41.739519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the trained model, we can compute the predicted target as well as measures of aleatoric and epistemic uncertainty.\n\n> [**Aleatoric uncertainty** is also known as statistical uncertainty, and is representative of unknowns that differ each time we run the same experiment. ... **Epistemic uncertainty** is also known as systematic uncertainty, and is due to things one could in principle know but do not in practice.](https://en.wikipedia.org/wiki/Uncertainty_quantification)","metadata":{}},{"cell_type":"code","source":"# compute predictions on training data\ny_pred = model.predict(train_vectors)\n\n# compute variance and std from learned parameters\nmu, v, alpha, beta = (y_pred[:, i] for i in range(y_pred.shape[1]))\n\nvar_a = beta / (alpha - 1)\nvar_e = beta / (v * (alpha - 1))","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:41.741957Z","iopub.execute_input":"2021-07-08T12:38:41.742236Z","iopub.status.idle":"2021-07-08T12:38:41.914933Z","shell.execute_reply.started":"2021-07-08T12:38:41.742207Z","shell.execute_reply":"2021-07-08T12:38:41.913959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model performs decently well in predicting the expected target variable. However, there is definitely room for improvement as the predicted target distribution is a truncated Gaussian.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x = train['target'], y = mu, kind = 'hex')\nplt.xlabel('Target')\nplt.ylabel('Predicted Target')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:41.916741Z","iopub.execute_input":"2021-07-08T12:38:41.917114Z","iopub.status.idle":"2021-07-08T12:38:42.485122Z","shell.execute_reply.started":"2021-07-08T12:38:41.917057Z","shell.execute_reply":"2021-07-08T12:38:42.484227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scale of aleatoric uncertainty is ~10 times smaller than the epistemic uncertainty, meaning that significant improvements can likely be made to the model without overfitting to the specific data. However, the scale of aleatoric uncertainty is still much larger than the scale of the target variable which is very worrying. This could indicate data quality issues of which there have been several examples posted.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x = np.sqrt(var_e), y = np.sqrt(var_a), kind = 'hex')\nplt.xlabel('Epistemic Uncertainty')\nplt.ylabel('Aleatoric Uncertainty')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:42.48669Z","iopub.execute_input":"2021-07-08T12:38:42.487001Z","iopub.status.idle":"2021-07-08T12:38:43.13666Z","shell.execute_reply.started":"2021-07-08T12:38:42.486972Z","shell.execute_reply":"2021-07-08T12:38:43.135633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aleatoric uncertainty seems to increase with the target variable (i.e. texts labeled as 'easier to read' themselves are labeled more noisily).","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x = train['target'], y = np.sqrt(var_a), kind = 'hex')\nplt.xlabel('Target')\nplt.ylabel('Aleatoric Uncertainty')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:43.137857Z","iopub.execute_input":"2021-07-08T12:38:43.138138Z","iopub.status.idle":"2021-07-08T12:38:43.651208Z","shell.execute_reply.started":"2021-07-08T12:38:43.138104Z","shell.execute_reply":"2021-07-08T12:38:43.650008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This trend is more pronounced when looking at the predicted target variable.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x = mu, y = np.sqrt(var_a), kind = 'hex')\nplt.xlabel('Predicted Target')\nplt.ylabel('Aleatoric Uncertainty')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:43.652841Z","iopub.execute_input":"2021-07-08T12:38:43.653215Z","iopub.status.idle":"2021-07-08T12:38:44.218782Z","shell.execute_reply.started":"2021-07-08T12:38:43.653178Z","shell.execute_reply":"2021-07-08T12:38:44.217992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no clear relationship between the target variable and epistemic uncertainty. Again, this is good news for indicating room to improve the model. The bad news is that the uncertainty is extremely high.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x = train['target'], y = np.sqrt(var_e), kind = 'hex')\nplt.xlabel('Target')\nplt.ylabel('Epistemic Uncertainty')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:44.221473Z","iopub.execute_input":"2021-07-08T12:38:44.22213Z","iopub.status.idle":"2021-07-08T12:38:44.829069Z","shell.execute_reply.started":"2021-07-08T12:38:44.222086Z","shell.execute_reply":"2021-07-08T12:38:44.828118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nothing particularly noteworthy changes for the predicted target here.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x = mu, y = np.sqrt(var_e), kind = 'hex')\nplt.xlabel('Predicted Target')\nplt.ylabel('Epistemic Uncertainty')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:44.830723Z","iopub.execute_input":"2021-07-08T12:38:44.831373Z","iopub.status.idle":"2021-07-08T12:38:45.455959Z","shell.execute_reply.started":"2021-07-08T12:38:44.831291Z","shell.execute_reply":"2021-07-08T12:38:45.454975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There does not seem to be any dependence between RMSE and either type of uncertainty.","metadata":{}},{"cell_type":"code","source":"sns.lmplot(\n    data = pd.DataFrame({\n        'RMSE' : np.sqrt((train['target'] - mu)**2),\n        'STD_A' : np.sqrt(var_a)\n    }),\n    x = 'RMSE',\n    y = 'STD_A'\n)\nplt.xlabel('RMSE')\nplt.ylabel('Aleatoric Uncertainty')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:45.457403Z","iopub.execute_input":"2021-07-08T12:38:45.458029Z","iopub.status.idle":"2021-07-08T12:38:46.208196Z","shell.execute_reply.started":"2021-07-08T12:38:45.457989Z","shell.execute_reply":"2021-07-08T12:38:46.207287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(\n    data = pd.DataFrame({\n        'RMSE' : np.sqrt((train['target'] - mu)**2),\n        'STD_E' : np.sqrt(var_e)\n    }),\n    x = 'RMSE',\n    y = 'STD_E'\n)\nplt.xlabel('RMSE')\nplt.ylabel('Epistemic Uncertainty')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:46.209506Z","iopub.execute_input":"2021-07-08T12:38:46.209813Z","iopub.status.idle":"2021-07-08T12:38:46.722379Z","shell.execute_reply.started":"2021-07-08T12:38:46.209766Z","shell.execute_reply":"2021-07-08T12:38:46.721482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute predictions\ny_pred = model.predict(test_vectors)\nmu, v, alpha, beta = (y_pred[:, i] for i in range(y_pred.shape[1]))\ntest['prediction'] = mu","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:46.72352Z","iopub.execute_input":"2021-07-08T12:38:46.723811Z","iopub.status.idle":"2021-07-08T12:38:46.776555Z","shell.execute_reply.started":"2021-07-08T12:38:46.723782Z","shell.execute_reply":"2021-07-08T12:38:46.775535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize dataframe to hold predictions\npredictions = pd.DataFrame()\n\n# add ID and final predicted target column\npredictions['id'] = test['id']\npredictions['target'] = test['prediction']\n\n# save predictions to CSV for submission\npredictions.to_csv('/kaggle/working/submission.csv', index = False)\n\n# display first five predictions\npredictions.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:38:46.777675Z","iopub.execute_input":"2021-07-08T12:38:46.777949Z","iopub.status.idle":"2021-07-08T12:38:46.794801Z","shell.execute_reply.started":"2021-07-08T12:38:46.777922Z","shell.execute_reply":"2021-07-08T12:38:46.793943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}