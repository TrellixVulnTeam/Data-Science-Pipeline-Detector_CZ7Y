{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport tensorflow_addons as tfa\nfrom transformers import BertConfig, BertModel, BertTokenizer\nimport transformers\nimport tokenizers\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deconstruct(dtf, type=\"train\"):\n    dtf[\"excerpt\"] = dtf[\"excerpt\"].apply(lambda x: x.replace(\"\\n\", \" \"))\n    dtf[\"excerpt\"] = dtf[\"excerpt\"].apply(lambda x: x.replace(\"'s\", \"\"))\n    dtf[\"excerpt\"] = dtf[\"excerpt\"].apply(lambda x: x.replace(\"'\", \" \"))\n    dtf[\"excerpt\"] = dtf[\"excerpt\"].apply(lambda x: x.replace(\"-\", \" \"))\n    dtf[\"excerpt\"] = dtf[\"excerpt\"].apply(lambda x: x.replace(\"‘\", \" \"))\n\n    dtf[\"excerpt\"] = dtf[\"excerpt\"].apply(lambda x: x.replace(\"”\", \"\"))\n    dtf[\"excerpt\"] = dtf[\"excerpt\"].apply(lambda x: re.sub(r\"[0-9]+\\–[0-9]+\", \"VREMYAPROMdsfsf1111\",x))\n    dtf[\"excerpt\"] = dtf[\"excerpt\"].apply(lambda x: x.replace(\"°\", \" temperatura \"))\n#     TRAIN_TABLE[\"excerpt\"] = TRAIN_TABLE[\"excerpt\"].apply(lambda x: x.replace(\"—\", \" \"))\n    dtf[\"count_w\"] = dtf[\"excerpt\"].apply(lambda x: len(x.split(\" \")))\n    if type==\"train\":\n        dtf[\"split\"] = dtf[\"target\"].apply(lambda x: math.trunc(x))\n    return dtf, dtf[\"count_w\"].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_TABLE = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nTEST_TABLE, _ = deconstruct(TEST_TABLE,\"test\")\nlist_test = TEST_TABLE[\"excerpt\"].to_list()\nmax_lens = 256\nmax_word = 30000\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"../input/tfbert-large-uncased/vocab.txt\", do_lower_case=True)\ndef ret_batch(list_data):\n    encoded =  tokenizer.batch_encode_plus(\n            list_data,\n            add_special_tokens=True,\n            max_length=max_lens,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n    return encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_test = ret_batch(list_test)\ndatas_test = [ np.array(encoded_test[\"input_ids\"], dtype=\"int32\"),\n               np.array(encoded_test[\"attention_mask\"], dtype=\"int32\"),\n               np.array(encoded_test[\"token_type_ids\"], dtype=\"int32\")\n             ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"configuration = BertConfig.from_pretrained(f'/kaggle/input/bert-tensorflow/bert-large-uncased-whole-word-masking-finetuned-squad-config.json')\nbase_path = '/kaggle/input/bert-tensorflow/bert-large-uncased-whole-word-masking-tf_model.h5'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    input_ids = tf.keras.layers.Input(\n        shape=(max_lens,), dtype=tf.int32, name=\"input_ids\"\n    )\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_lens,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_lens,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n    bert_model = transformers.TFBertModel.from_pretrained(base_path, config=configuration)\n    bert_model.trainable = False\n    outputs = bert_model(\n        input_ids, attention_masks, token_type_ids\n    )\n    bi_lstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(256, return_sequences=True, )\n    )(outputs[0])\n    bi_lstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(128, return_sequences=True)\n    )(bi_lstm)\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.1)(concat)\n    output = tf.keras.layers.Dense(1)(dropout)\n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n    )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss = tf.keras.losses.MeanAbsoluteError(),\n        metrics=tf.keras.metrics.RootMeanSquaredError(),\n    )\n    return model\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.load_weights(\"../input/zefir-book-v1/w2.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(datas_test)\nTEST_TABLE[\"target\"] = pred\nname = \"submission.csv\"\nsub = TEST_TABLE[[\"id\",\"target\"]]\nsub.to_csv(name, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}