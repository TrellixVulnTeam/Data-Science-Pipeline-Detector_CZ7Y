{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pytorch BERT + Stratified K Fold [train]\n## Introduction\n\nThis notebook is the BERT baseline (using HuggingFace) with stratified K fold.  \nI stratify the dataset on the target distribution.\n\nInference part is here:  \nhttps://www.kaggle.com/atsushiiwasaki/commonlit-bert-stratified-k-fold-baseline-infer\n\nYou can choose BERT variants from\n* BERT\n* DISTILBERT\n* ROBERTA\n\nHyper-parameters / Optimizer / Scheduler or any settings for them are not optimized.  \nPlease try some experiments. Thanks.\n\n## Contents\n1. Libraries\n1. Configuration\n1. Data (Dataset, DataLoader)\n1. Criterion\n1. Model\n1. Optimizer\n1. Training/Inference\n1. Run\n1. Calculate CV Score\n1. OOF prediction vs target\n\n## Update\n* v9  : fully connected layer -> kaiming_normal\n* v10 : back to Distilbert (LB: 0.497, CV: 0.533)\n* v11 : change random seed\n* v12 : change random seed\n* v14 : chage max_len in tokenizer 210 -> 250, change random seed 777 -> 28\n* v15 : add CV score calculation section\n* v16 : n_bins=10 -> 20 when stratify\n* v17 : add layer_norm / batch size 32 -> 16\n* v18 : Distilbert -> Roberta\n* v19 : adjust Scheduler\n* v22 : add a figure (oof prediction vs target)\n* v23 : remove \"break\" in the training loop","metadata":{"execution":{"iopub.status.busy":"2021-05-25T12:33:58.375746Z","iopub.execute_input":"2021-05-25T12:33:58.376152Z","iopub.status.idle":"2021-05-25T12:33:58.380414Z","shell.execute_reply.started":"2021-05-25T12:33:58.376117Z","shell.execute_reply":"2021-05-25T12:33:58.37947Z"}}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport gc\nfrom pprint import pprint\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set(style='darkgrid')\n\nfrom sklearn.model_selection import StratifiedKFold\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:17.847777Z","iopub.execute_input":"2021-06-06T23:24:17.84814Z","iopub.status.idle":"2021-06-06T23:24:17.871613Z","shell.execute_reply.started":"2021-06-06T23:24:17.848094Z","shell.execute_reply":"2021-06-06T23:24:17.870543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoModel\nfrom transformers import AutoTokenizer\nfrom transformers import AutoConfig","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:17.873201Z","iopub.execute_input":"2021-06-06T23:24:17.873558Z","iopub.status.idle":"2021-06-06T23:24:17.886365Z","shell.execute_reply.started":"2021-06-06T23:24:17.873522Z","shell.execute_reply":"2021-06-06T23:24:17.88549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"DEBUG = False\n\nTRAIN = '../input/commonlitreadabilityprize/train.csv'\nTEST = '../input/commonlitreadabilityprize/test.csv'\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device: ', device.type)\n\nSEED = 28\nseed_everything(SEED)\n\ncfg ={}","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:17.890277Z","iopub.execute_input":"2021-06-06T23:24:17.890546Z","iopub.status.idle":"2021-06-06T23:24:17.901326Z","shell.execute_reply.started":"2021-06-06T23:24:17.89052Z","shell.execute_reply":"2021-06-06T23:24:17.900488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BERT\nBERT = 'bert-base-uncased'\n\n# Distilbert\nDISTILBERT = 'distilbert-base-uncased'\n\n# Roberta\nROBERTA = 'roberta-base'\n\n\n\ncfg ={}\n\nARCH_PATH = ROBERTA\n\ncfg['train'] = {'n_folds': 5}","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:17.904755Z","iopub.execute_input":"2021-06-06T23:24:17.905066Z","iopub.status.idle":"2021-06-06T23:24:17.912559Z","shell.execute_reply.started":"2021-06-06T23:24:17.905028Z","shell.execute_reply":"2021-06-06T23:24:17.911369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"## Stratify on target distribution","metadata":{}},{"cell_type":"code","source":"def get_bin_stratified(df, n_bins=20, n_splits=5):\n    df['bin'] = pd.cut(df.target, n_bins, labels=[i for i in range(n_bins)])\n    \n    df['fold'] = np.nan\n\n    skf = StratifiedKFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n    gen_skf = skf.split(df.id, y=df.bin)\n\n    for fold, (idx_train, idx_val) in enumerate(gen_skf):\n        df.loc[idx_val, 'fold'] = fold\n\n    df['fold'] = df['fold'].astype('int8')","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:17.914002Z","iopub.execute_input":"2021-06-06T23:24:17.914594Z","iopub.status.idle":"2021-06-06T23:24:17.92204Z","shell.execute_reply.started":"2021-06-06T23:24:17.914555Z","shell.execute_reply":"2021-06-06T23:24:17.920999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(TRAIN)\nget_bin_stratified(df)\n\nplt.figure(figsize=(12, 6))\nfor fold in range(cfg['train']['n_folds']):\n    sns.histplot(data=df.loc[df.fold==fold], x='target', bins=20, hue='fold', label=f'fold{fold}')\n    \nplt.title('Target Distribution for Each Fold')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:17.92334Z","iopub.execute_input":"2021-06-06T23:24:17.923895Z","iopub.status.idle":"2021-06-06T23:24:18.466385Z","shell.execute_reply.started":"2021-06-06T23:24:17.923854Z","shell.execute_reply":"2021-06-06T23:24:18.465435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"cfg['tokenizer'] ={'name': ARCH_PATH, \n                   'max_length': 210}\n\ntokenizer = AutoTokenizer.from_pretrained(cfg['tokenizer']['name'])","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:18.467862Z","iopub.execute_input":"2021-06-06T23:24:18.468232Z","iopub.status.idle":"2021-06-06T23:24:21.452273Z","shell.execute_reply.started":"2021-06-06T23:24:18.468193Z","shell.execute_reply":"2021-06-06T23:24:21.45119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    text = df.loc[SEED, 'excerpt']\n    print('Text Length ', len(text.split(' ')))\n    print()\n    \n    text_tokenized = tokenizer.encode_plus(\n                        text,\n                        add_special_tokens=True,\n                        padding='max_length',\n                        max_length=cfg['tokenizer']['max_length'], \n                        truncation=True\n                        )\n    \n    for key, value in text_tokenized.items():\n        print(key, type(value))\n        print(value)\n        print()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-06T23:24:21.456753Z","iopub.execute_input":"2021-06-06T23:24:21.457001Z","iopub.status.idle":"2021-06-06T23:24:21.464399Z","shell.execute_reply.started":"2021-06-06T23:24:21.456975Z","shell.execute_reply":"2021-06-06T23:24:21.463575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class CommonLitDataset(Dataset):\n    \n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.df.loc[index, 'excerpt']\n        inputs = self.tokenizer.encode_plus(\n            text,                                 \n            add_special_tokens=True,\n            padding='max_length',\n            max_length=self.max_len,\n            truncation=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        \n        if cfg['tokenizer']['name']=='bert-base-uncased':\n            token_type_ids = inputs['token_type_ids'] \n        else:\n            token_type_ids = 1.\n        \n        target = self.df.loc[index, ['target']]\n        \n        return {\n            'ids': torch.LongTensor(ids),\n            'mask': torch.LongTensor(mask),\n             'token_type_ids': torch.tensor(token_type_ids)\n            },{\n            'target': torch.Tensor(target)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.466823Z","iopub.execute_input":"2021-06-06T23:24:21.467107Z","iopub.status.idle":"2021-06-06T23:24:21.479433Z","shell.execute_reply.started":"2021-06-06T23:24:21.467058Z","shell.execute_reply":"2021-06-06T23:24:21.478512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    ds = CommonLitDataset(df=df, \n                          tokenizer=tokenizer, \n                          max_len=cfg['tokenizer']['max_length'])\n    assert len(df) == len(ds)\n    \n    ds = iter(ds)\n    inputs, targets = next(ds)\n    \n    for k, v in inputs.items():\n        print(k, v.dtype)\n        print(v)\n        print()\n        \n    for k, v in targets.items():\n        print(k, v.dtype)\n        print(v)\n        print()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-06T23:24:21.482632Z","iopub.execute_input":"2021-06-06T23:24:21.482953Z","iopub.status.idle":"2021-06-06T23:24:21.491649Z","shell.execute_reply.started":"2021-06-06T23:24:21.482926Z","shell.execute_reply":"2021-06-06T23:24:21.490724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"cfg['dl_train'] = {\n    'batch_size': 8 if device.type=='cpu' else 32, \n    'shuffle': True, \n    'num_workers': os.cpu_count(), \n    'pin_memory': True\n}\n\ncfg['dl_val'] = {\n    'batch_size': 8 if device.type=='cpu' else 64, \n    'shuffle': False, \n    'num_workers': os.cpu_count(), \n    'pin_memory': True\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.492989Z","iopub.execute_input":"2021-06-06T23:24:21.493355Z","iopub.status.idle":"2021-06-06T23:24:21.500985Z","shell.execute_reply.started":"2021-06-06T23:24:21.493319Z","shell.execute_reply":"2021-06-06T23:24:21.50021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    ds = CommonLitDataset(df=df, \n                          tokenizer=tokenizer, \n                          max_len=cfg['tokenizer']['max_length'])\n    \n    dl = DataLoader(ds, **cfg['dl_train'])\n    \n    for data in dl:\n        print(data[0]['ids'].detach().cpu().size())\n        break","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-06T23:24:21.503784Z","iopub.execute_input":"2021-06-06T23:24:21.504094Z","iopub.status.idle":"2021-06-06T23:24:21.510387Z","shell.execute_reply.started":"2021-06-06T23:24:21.504068Z","shell.execute_reply":"2021-06-06T23:24:21.509451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"cfg['model'] = {'name': ARCH_PATH}","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.512853Z","iopub.execute_input":"2021-06-06T23:24:21.51316Z","iopub.status.idle":"2021-06-06T23:24:21.518734Z","shell.execute_reply.started":"2021-06-06T23:24:21.513123Z","shell.execute_reply":"2021-06-06T23:24:21.517634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitBERT(nn.Module):\n    \n    def __init__(self, name, dropout=True):\n        super(CommonLitBERT, self).__init__()\n        self.bert = AutoModel.from_pretrained(name)\n        self.name = name\n        \n        if name == BERT:\n            self.in_features = self.bert.pooler.dense.out_features\n        elif name == DISTILBERT:\n            self.in_features = self.bert.transformer.layer[5].output_layer_norm.normalized_shape[0]\n        elif name == ROBERTA:\n            self.in_features = self.bert.pooler.dense.out_features\n        else:\n            self.in_features = 768\n        \n        self.fc = nn.Linear(self.in_features, 1)\n        self.dense = nn.Linear(self.in_features, self.in_features)\n        self.activation = nn.Tanh()\n        self.dropout = nn.Dropout(p=0.2)\n        self.layer_norm = nn.LayerNorm(self.in_features)\n        \n        torch.nn.init.kaiming_normal_(self.dense.weight)\n        torch.nn.init.kaiming_normal_(self.fc.weight)\n        \n    def forward(self, ids, mask, token_type_ids):\n        if self.name == BERT:\n            last_hidden_state, output = self.bert(ids,\n                                                  attention_mask=mask,\n                                                  token_type_ids=token_type_ids,\n                                                  return_dict=False)\n        elif self.name == DISTILBERT:\n            last_hidden_state = self.bert(ids, \n                                           attention_mask=mask, \n                                           return_dict=False)\n            first_token_tensor = last_hidden_state[0][:, 0]\n            output = self.dense(first_token_tensor)\n            output = self.activation(output)\n            \n        elif self.name == ROBERTA:\n            last_hidden_state, output = self.bert(ids,\n                                                  attention_mask=mask,\n#                                                   token_type_ids=token_type_ids,\n                                                  return_dict=False)\n        \n        output = self.layer_norm(output)\n        output = self.dropout(output)\n        output = self.fc(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.520148Z","iopub.execute_input":"2021-06-06T23:24:21.520603Z","iopub.status.idle":"2021-06-06T23:24:21.533941Z","shell.execute_reply.started":"2021-06-06T23:24:21.520563Z","shell.execute_reply":"2021-06-06T23:24:21.533143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    model = CommonLitBERT(name=cfg['model']['name'])\n    data = next(iter(dl))\n    inputs = data[0]\n    outputs = model(**inputs)\n    print(outputs)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-06T23:24:21.535258Z","iopub.execute_input":"2021-06-06T23:24:21.53581Z","iopub.status.idle":"2021-06-06T23:24:21.545577Z","shell.execute_reply.started":"2021-06-06T23:24:21.535774Z","shell.execute_reply":"2021-06-06T23:24:21.544705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Criterion","metadata":{}},{"cell_type":"code","source":"def CommonLitMetric(y_pred, y_gt):\n    assert y_pred.size() == y_gt.size()\n    \n    metric = nn.MSELoss()\n    metric = torch.sqrt(metric(y_pred, y_gt))\n    return metric","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.546908Z","iopub.execute_input":"2021-06-06T23:24:21.547341Z","iopub.status.idle":"2021-06-06T23:24:21.554259Z","shell.execute_reply.started":"2021-06-06T23:24:21.547303Z","shell.execute_reply":"2021-06-06T23:24:21.553452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    def RMSE_grad(y_pred, y_gt):\n        # y_pred differential\n        delta = y_pred - y_gt\n        N = len(delta)\n\n        dL = delta / N\n        dy = torch.sqrt((delta**2).sum() / N)\n\n        return dL/dy\n    \n    y_pred = torch.tensor([[6], [5]], dtype=torch.float32, requires_grad=True)\n    y_gt = torch.tensor([[2], [4]], dtype=torch.float32, requires_grad=True)\n    \n    metric = CommonLitMetric(y_pred, y_gt)\n    metric.backward()\n    \n    for i in range(len(y_pred)):\n        assert y_pred.grad[i] == RMSE_grad(y_pred, y_gt).data[i], f'{i}th element is not consistent.'","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-06T23:24:21.555679Z","iopub.execute_input":"2021-06-06T23:24:21.556139Z","iopub.status.idle":"2021-06-06T23:24:21.563747Z","shell.execute_reply.started":"2021-06-06T23:24:21.556103Z","shell.execute_reply":"2021-06-06T23:24:21.562754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer","metadata":{}},{"cell_type":"code","source":"from torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\n\nfrom transformers import get_cosine_schedule_with_warmup\nfrom transformers import AdamW\n\ncfg['optim'] = {'lr': 8e-6, \n#                 'weight_decay': 0.01\n               }\ncfg['scheduler'] = {'num_warmup_steps': 3, \n                    'num_training_steps': 7, \n#                     'num_cycles': 1,\n                   }","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.565354Z","iopub.execute_input":"2021-06-06T23:24:21.565963Z","iopub.status.idle":"2021-06-06T23:24:21.575502Z","shell.execute_reply.started":"2021-06-06T23:24:21.565924Z","shell.execute_reply":"2021-06-06T23:24:21.574713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    model = CommonLitBERT(name=cfg['model']['name'])\n    optim = AdamW(model.parameters(), **cfg['optim'])\n    scheduler = get_cosine_schedule_with_warmup(optim, **cfg['scheduler'])\n    \n    lrs = []\n\n    for epoch in range(50):\n        lr = optim.param_groups[0]['lr']\n        lrs.append(lr)\n\n        optim.step()\n        scheduler.step()\n        \n    plt.plot(lrs, marker='o')\n    plt.xlabel('Steps')\n    plt.ylabel('Learning Rate')\n    plt.title('LR Scheduler Plot')\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-06T23:24:21.576848Z","iopub.execute_input":"2021-06-06T23:24:21.577249Z","iopub.status.idle":"2021-06-06T23:24:21.584469Z","shell.execute_reply.started":"2021-06-06T23:24:21.577213Z","shell.execute_reply":"2021-06-06T23:24:21.583412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training / Inference","metadata":{}},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler\nfrom torch.cuda.amp import autocast","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.585972Z","iopub.execute_input":"2021-06-06T23:24:21.586675Z","iopub.status.idle":"2021-06-06T23:24:21.592604Z","shell.execute_reply.started":"2021-06-06T23:24:21.586636Z","shell.execute_reply":"2021-06-06T23:24:21.591715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg['train'] ={\n    'n_folds': 5,\n    'n_epochs': 100\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.593575Z","iopub.execute_input":"2021-06-06T23:24:21.596225Z","iopub.status.idle":"2021-06-06T23:24:21.600461Z","shell.execute_reply.started":"2021-06-06T23:24:21.596185Z","shell.execute_reply":"2021-06-06T23:24:21.59933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StoreLoss:\n    \n    def __init__(self, fold):\n        self.loss_train_mean = []\n        self.loss_train_std = []\n        self.loss_val_mean = []\n        self.loss_val_std = []\n        \n        self.fold = fold\n        \n    def get_loss(self, loss_train, loss_val):\n        self.loss_train_mean.append(loss_train[0])\n        self.loss_train_std.append(loss_train[1])\n        self.loss_val_mean.append(loss_val[0])\n        self.loss_val_std.append(loss_val[1])\n        \n    def plot_loss(self):\n        \n        def get_ax(ax, loss_train, loss_val, title='mean'):\n            ax.plot(loss_train, marker='o', label='train')\n            ax.plot(loss_val, marker='x', label='val')\n            ax.set_xlabel('Epoch')\n            ax.set_ylabel(f'RMSE ({title})')\n            ax.set_title(f'RMSE({title}) vs Epoch at fold {self.fold}')\n            ax.legend()\n            return ax\n        \n        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n        \n        ax[0] = get_ax(ax[0], self.loss_train_mean, self.loss_val_mean, title='mean')\n        ax[1] = get_ax(ax[1], self.loss_train_std, self.loss_val_std, title='std')\n        \n        \n        fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.601953Z","iopub.execute_input":"2021-06-06T23:24:21.602399Z","iopub.status.idle":"2021-06-06T23:24:21.6138Z","shell.execute_reply.started":"2021-06-06T23:24:21.602362Z","shell.execute_reply":"2021-06-06T23:24:21.612986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    store = StoreLoss(fold=0)\n    \n    for epoch in range(10):\n        loss_train = np.random.rand(2)\n        loss_val = np.random.rand(2)\n        \n        store.get_loss(loss_train, loss_val)\n    \n    store.plot_loss()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-06T23:24:21.618043Z","iopub.execute_input":"2021-06-06T23:24:21.618321Z","iopub.status.idle":"2021-06-06T23:24:21.623874Z","shell.execute_reply.started":"2021-06-06T23:24:21.618291Z","shell.execute_reply":"2021-06-06T23:24:21.622943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(model, dl, criterion, optim, scheduler):\n    scaler = GradScaler()\n    \n    loss_train = []\n    loss_total = 0\n    \n    model.train()\n    model.to(device)\n    \n    progress_bar = tqdm(dl, desc='train')\n    \n    for i, data in enumerate(progress_bar):\n        optim.zero_grad()\n        \n        inputs = {key: value.to(device) for key, value in data[0].items()}\n        targets = data[1]['target'].to(device)\n        \n        with autocast():\n            outputs = model(**inputs)\n            loss = criterion(outputs, targets)\n#         loss.backward()\n        \n        scaler.scale(loss).backward()\n        \n        loss_train.append(loss.item())\n        loss_total += loss.item()\n        \n        progress_bar.set_postfix({'RMSE(batch)': loss.item(), \n                                  'RMSE(ave)': loss_total / (i+1), \n                                  'lr': optim.param_groups[0]['lr']})\n        \n        scaler.step(optim)\n        scaler.update()\n#         optim.step()\n    \n    return np.mean(loss_train), np.std(loss_train)\n\ndef val_fn(model, dl):\n    scaler = GradScaler()\n    \n    loss_val = []\n    loss_total = 0\n    \n    model.eval()\n    model.to(device)\n    \n    progress_bar = tqdm(dl, desc='val')\n    \n    with torch.no_grad():\n        for i, data in enumerate(progress_bar):\n            inputs = {key: value.to(device) for key, value in data[0].items()}\n            targets = data[1]['target'].to(device)\n            \n            with autocast():\n                outputs = model(**inputs)\n                loss = CommonLitMetric(outputs, targets)\n            \n            loss_val.append(loss.item())\n            loss_total += loss.item()\n            \n            progress_bar.set_postfix({'RMSE(batch)': loss.item(), 'RMSE(ave)': loss_total / (i+1)})\n    \n    loss_val_2 = np.array(loss_val)**2 * cfg['dl_val']['batch_size'] / len(dl.dataset)\n    print('RMSE for validation set overall: ', np.sqrt(loss_val_2.sum()))\n    \n    return np.sqrt(loss_val_2.sum()), np.std(loss_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.625688Z","iopub.execute_input":"2021-06-06T23:24:21.626154Z","iopub.status.idle":"2021-06-06T23:24:21.639584Z","shell.execute_reply.started":"2021-06-06T23:24:21.625996Z","shell.execute_reply":"2021-06-06T23:24:21.638435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_one_epoch(model, train_dl, val_dl, criterion, optim, scheduler):\n    inputs_train = {\n        'model': model, \n        'dl': train_dl, \n        'criterion': criterion, \n        'optim': optim, \n        'scheduler': scheduler\n    }\n\n    inputs_val = {'model': model, \n                  'dl': val_dl}\n\n    loss_train = train_fn(**inputs_train)\n    loss_val = val_fn(**inputs_val)\n    \n    return loss_train, loss_val","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.641086Z","iopub.execute_input":"2021-06-06T23:24:21.641449Z","iopub.status.idle":"2021-06-06T23:24:21.650002Z","shell.execute_reply.started":"2021-06-06T23:24:21.641414Z","shell.execute_reply":"2021-06-06T23:24:21.649072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dls_for_n_fold(df, fold, tokenizer):\n    train_df = df.loc[df.fold!=fold].reset_index(drop=True)\n    val_df = df.loc[df.fold==fold].reset_index(drop=True)\n    \n    train_ds = CommonLitDataset(\n        train_df, \n        tokenizer=tokenizer, \n        max_len=cfg['tokenizer']['max_length']\n    )\n    \n    val_ds = CommonLitDataset(\n        val_df, \n        tokenizer=tokenizer, \n        max_len=cfg['tokenizer']['max_length']\n    )\n    \n    train_dl = DataLoader(train_ds, **cfg['dl_train'])\n    val_dl = DataLoader(val_ds, **cfg['dl_val'])\n    \n    return train_dl, val_dl","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.651347Z","iopub.execute_input":"2021-06-06T23:24:21.651855Z","iopub.status.idle":"2021-06-06T23:24:21.660753Z","shell.execute_reply.started":"2021-06-06T23:24:21.651817Z","shell.execute_reply":"2021-06-06T23:24:21.659982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://qiita.com/ku_a_i/items/ba33c9ce3449da23b503\n\nclass EarlyStopping:\n    \n    def __init__(self, patience=2, seq=False):\n        self.patience = patience\n        self.counter = 0\n        self.best_score = None\n        self.stop = False\n        \n    def __call__(self, loss, model, optim, cfg, path):\n        if self.best_score is None:\n            self.best_score = loss\n            self.save_checkpoint(model, optim, cfg, path)\n        elif loss < self.best_score:\n            print(f'Loss decreased {self.best_score} -> {loss}.')\n            self.best_score = loss\n            self.counter = 0\n            self.save_checkpoint(model, optim, cfg, path)\n        else:\n            self.counter += 1\n            if self.counter > self.patience: self.stop = True\n                \n    def save_checkpoint(self, model, optim, cfg, path):\n        save_list = {'model': model.state_dict(), \n#                      'optim': optim.state_dict(), \n                     'cfg': cfg}\n        SAVE_PATH = path\n        torch.save(save_list, SAVE_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.66223Z","iopub.execute_input":"2021-06-06T23:24:21.662797Z","iopub.status.idle":"2021-06-06T23:24:21.671939Z","shell.execute_reply.started":"2021-06-06T23:24:21.66273Z","shell.execute_reply":"2021-06-06T23:24:21.671212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run","metadata":{}},{"cell_type":"code","source":"pprint(cfg)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.673234Z","iopub.execute_input":"2021-06-06T23:24:21.673588Z","iopub.status.idle":"2021-06-06T23:24:21.687305Z","shell.execute_reply.started":"2021-06-06T23:24:21.673549Z","shell.execute_reply":"2021-06-06T23:24:21.686345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    seed_everything(SEED)\n        \n    df = pd.read_csv(TRAIN)\n    get_bin_stratified(df, n_splits=cfg['train']['n_folds'])\n\n    tokenizer = AutoTokenizer.from_pretrained(cfg['tokenizer']['name'])\n    \n    for fold in range(cfg['train']['n_folds']):\n        print('Fold:', fold)\n#         if fold in [0, 1, 2]: continue\n        store = StoreLoss(fold=fold)\n        es = EarlyStopping()\n\n        train_dl, val_dl = get_dls_for_n_fold(df, fold, tokenizer)\n\n        model = CommonLitBERT(name=cfg['model']['name'])\n        criterion = CommonLitMetric\n        optim = AdamW(model.parameters(), **cfg['optim'])\n        scheduler = get_cosine_schedule_with_warmup(optim, **cfg['scheduler'])\n        if optim.param_groups[0]['lr']==0:\n            optim.step()\n            scheduler.step()\n\n        inputs = {'model': model,\n                  'train_dl': train_dl,\n                  'val_dl': val_dl,\n                  'criterion': criterion,\n                  'optim': optim,\n                  'scheduler': scheduler}\n\n        for epoch in range(cfg['train']['n_epochs']):\n            loss_train, loss_val = run_one_epoch(**inputs)\n            \n            store.get_loss(loss_train, loss_val)\n            \n            es(loss_val[0], model, optim, cfg, path=f'CommonLitBERT_fold{fold}.tar')\n            if es.stop:\n                print('Early Stop !')\n                print()\n                break\n\n            scheduler.step()\n            \n        store.plot_loss()\n        \n        del model, optim\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.688482Z","iopub.execute_input":"2021-06-06T23:24:21.688808Z","iopub.status.idle":"2021-06-06T23:24:21.698719Z","shell.execute_reply.started":"2021-06-06T23:24:21.688783Z","shell.execute_reply":"2021-06-06T23:24:21.697719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmain()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:24:21.700237Z","iopub.execute_input":"2021-06-06T23:24:21.70089Z","iopub.status.idle":"2021-06-06T23:29:20.660565Z","shell.execute_reply.started":"2021-06-06T23:24:21.700849Z","shell.execute_reply":"2021-06-06T23:29:20.659576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate CV Score","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = 'CommonLitBERT'\n\ndef val_fn_cv(model, dl):\n    scaler = GradScaler()\n    preds = []\n    \n    model.eval()\n    model.to(device)\n    \n    progress_bar = tqdm(dl, desc='cv')\n    \n    with torch.no_grad():\n        for i, data in enumerate(progress_bar):\n            inputs = {key: value.to(device) for key, value in data[0].items()}\n            targets = data[1]['target'].to(device)\n            \n            with autocast():\n                outputs = model(**inputs)\n            \n            preds.append(outputs.detach().cpu().numpy())\n    \n    preds = np.concatenate(preds)\n    \n    return preds\n\ndef main_cv():\n    seed_everything(SEED)\n    \n    df = pd.read_csv(TRAIN)\n    get_bin_stratified(df, n_splits=cfg['train']['n_folds'])\n    df['oof'] = np.nan\n\n    tokenizer = AutoTokenizer.from_pretrained(cfg['tokenizer']['name'])\n    \n    for fold in range(cfg['train']['n_folds']):\n        train_dl, val_dl = get_dls_for_n_fold(df, fold, tokenizer)\n\n        model = CommonLitBERT(name=cfg['model']['name'])\n        PATH = os.path.join(MODEL_NAME + f'_fold{fold}.tar')\n        saved_contents = torch.load(PATH, map_location=device)\n        \n        model.load_state_dict(saved_contents['model'])\n        if fold==0:\n            cfg_for_train = saved_contents['cfg']\n            print('Configuration for training:')\n            print()\n            pprint(cfg_for_train)\n            print()\n        \n        print('Fold:', fold)\n        \n        inputs = {'model': model,\n                  'dl': val_dl}\n        \n        preds = val_fn_cv(**inputs)\n        df.loc[df.fold==fold, 'oof'] = preds\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:29:20.662124Z","iopub.execute_input":"2021-06-06T23:29:20.662478Z","iopub.status.idle":"2021-06-06T23:29:20.674001Z","shell.execute_reply.started":"2021-06-06T23:29:20.66244Z","shell.execute_reply":"2021-06-06T23:29:20.673095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndf = main_cv()\ndf.to_csv('oof_df.csv', index=False)\n\nmse = mean_squared_error(df['target'], df['oof'])\nrmse = np.sqrt(mse)\nprint('CV score: ', rmse)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:29:20.675517Z","iopub.execute_input":"2021-06-06T23:29:20.675901Z","iopub.status.idle":"2021-06-06T23:30:06.724129Z","shell.execute_reply.started":"2021-06-06T23:29:20.675861Z","shell.execute_reply":"2021-06-06T23:30:06.723281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OOF Prediction vs Target","metadata":{}},{"cell_type":"code","source":"temp_df = pd.DataFrame()\ntemp_df['x'] = np.linspace(-3.5, 1.5, 10)\ntemp_df['y'] = temp_df['x']\n\nplt.figure(figsize=(8, 8))\nsns.scatterplot(data=df, x='target', y='oof', label='oof vs target')\nsns.lineplot(data=temp_df, x='x', y='y', color='orange')\nplt.title('OOF Prediction vs Target')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T23:31:43.628755Z","iopub.execute_input":"2021-06-06T23:31:43.629099Z","iopub.status.idle":"2021-06-06T23:31:43.884484Z","shell.execute_reply.started":"2021-06-06T23:31:43.629065Z","shell.execute_reply":"2021-06-06T23:31:43.883526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference Part is here !\nhttps://www.kaggle.com/atsushiiwasaki/commonlit-bert-stratified-k-fold-baseline-infer","metadata":{}}]}