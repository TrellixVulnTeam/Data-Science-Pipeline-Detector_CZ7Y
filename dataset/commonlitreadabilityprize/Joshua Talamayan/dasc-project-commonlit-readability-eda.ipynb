{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Libraries:** \n\n**Exploratory Data Analysis** \\\nI used the following libraries for exploratory data analysis: \\\nmatplotlib \\\nseaborn \\\ncollections (Counter) \\\nplotly.express \\\nscipy.stats \n\n**Models Used** \\\nI decided to run 7 different regression models: \\\nRandom Forest Regression \\\nGradient Boosting Regression \\\nSupport Vector Regression \\\nAdaBoost Regression \\\nXGBoost Regression \\\nRidge Regression \\\nLinear Regression \n","metadata":{}},{"cell_type":"code","source":"#Obtain Libraries\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\nimport plotly.express as px\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\n\nimport xgboost as xgb\n\nimport scipy.stats as st\nfrom statistics import mean\n\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\n\nimport re\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-29T17:06:41.931205Z","iopub.execute_input":"2022-03-29T17:06:41.93193Z","iopub.status.idle":"2022-03-29T17:06:45.24711Z","shell.execute_reply.started":"2022-03-29T17:06:41.931824Z","shell.execute_reply":"2022-03-29T17:06:45.246286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n\n# Remove incomplete entries\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:06:45.249096Z","iopub.execute_input":"2022-03-29T17:06:45.249508Z","iopub.status.idle":"2022-03-29T17:06:45.393137Z","shell.execute_reply.started":"2022-03-29T17:06:45.249464Z","shell.execute_reply":"2022-03-29T17:06:45.392212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:06:45.396367Z","iopub.execute_input":"2022-03-29T17:06:45.396705Z","iopub.status.idle":"2022-03-29T17:06:45.420011Z","shell.execute_reply.started":"2022-03-29T17:06:45.396674Z","shell.execute_reply":"2022-03-29T17:06:45.419288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.loc[train_df['id'] == 'c12129c31']","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:06:45.421357Z","iopub.execute_input":"2022-03-29T17:06:45.421823Z","iopub.status.idle":"2022-03-29T17:06:45.434737Z","shell.execute_reply.started":"2022-03-29T17:06:45.421792Z","shell.execute_reply":"2022-03-29T17:06:45.433916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading in the Data** \\\nThe training and test data sets are read in. \\\nThe submission file for reference is also read in.\n\n**Training Dataset** \\\nFor the training dataset, I removed any entries that contained a target and standard error value of 0. \\\nAfter the entries are removed, I take a look at the train data.\n\n","metadata":{}},{"cell_type":"code","source":"#====== Preprocessing function ======\ndef preprocess(data):\n    excerpt_processed=[]\n    for e in data['excerpt']:\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed ","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:06:45.436051Z","iopub.execute_input":"2022-03-29T17:06:45.436345Z","iopub.status.idle":"2022-03-29T17:06:45.445306Z","shell.execute_reply.started":"2022-03-29T17:06:45.436317Z","shell.execute_reply":"2022-03-29T17:06:45.444614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing the Data**\n\nTo preprocess the train and test data, we take a look at each entry in the excerpt variable and remove foreign texts, unnecessary blanks, convert the text to lowercase, tokenize the words using nltk, remove any non-english verbiage, and use a word lemmatizer.\n\nThe variable then becomes defined as \"excerpt_preprocessed\", which indicates the newly cleaned excerpt variable which we will be working for our regressions.","metadata":{}},{"cell_type":"code","source":"train_df[\"excerpt_preprocessed\"] = preprocess(train_df)\ntest_df[\"excerpt_preprocessed\"] = preprocess(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:06:45.44633Z","iopub.execute_input":"2022-03-29T17:06:45.446817Z","iopub.status.idle":"2022-03-29T17:07:54.037846Z","shell.execute_reply.started":"2022-03-29T17:06:45.446784Z","shell.execute_reply":"2022-03-29T17:07:54.036751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Excerpt\nprint(train_df.iloc[0,3])","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:07:54.039139Z","iopub.execute_input":"2022-03-29T17:07:54.039434Z","iopub.status.idle":"2022-03-29T17:07:54.045858Z","shell.execute_reply.started":"2022-03-29T17:07:54.039404Z","shell.execute_reply":"2022-03-29T17:07:54.044698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exerpt Preprocessed\nprint(train_df.iloc[0,6])","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:07:54.048324Z","iopub.execute_input":"2022-03-29T17:07:54.048633Z","iopub.status.idle":"2022-03-29T17:07:54.060035Z","shell.execute_reply.started":"2022-03-29T17:07:54.048602Z","shell.execute_reply":"2022-03-29T17:07:54.059242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploratory Data Analysis** \\\nWe explore the following variables: \\\nTarget \\\nStandard Error \\\nExcerpt ","metadata":{}},{"cell_type":"code","source":"#View target in depth\n#Title#\nprint(\"target Variable\")\nprint(\"----------\")\n\n#Mean#\ntarget_mean = train_df[\"target\"].mean()\nprint(f\"Mean: {target_mean}\")\n\n#Median#\ntarget_median = train_df[\"target\"].median()\nprint(f\"Median: {target_median}\")\n\n#Standard Deviation\ntarget_std = train_df[\"target\"].std()\nprint(f\"Standard Deviation: {target_std}\")\n\n#Minimum Value\ntarget_min = train_df[\"target\"].min()\nprint(f\"Minimum Value: {target_min}\")\n\n#25th Percentile\ntarget_25 = np.percentile(train_df[\"target\"],25)\nprint(f\"25th Percentile: {target_25}\")\n\n#50th Percentile\ntarget_50 = np.percentile(train_df[\"target\"],50)\nprint(f\"50th Percentile: {target_50}\")\n\n#75th Percentile\ntarget_75 = np.percentile(train_df[\"target\"],75)\nprint(f\"75th Percentile: {target_75}\")\n\n#Maximum Value\ntarget_max = train_df[\"target\"].max()\nprint(f\"Maximum Value: {target_max}\")\n\n#Skew\ntarget_skew = train_df[\"target\"].skew(axis= 0, skipna = True)\nprint(f\"Skew: {target_skew}\")\n\n#Plot 'target' variable\n\nplt.hist(train_df['target'],edgecolor = 'black', bins=50, density=True)\nmn, mx = plt.xlim()\nplt.xlim(mn, mx)\nkde_xs = np.linspace(mn, mx, 300)\nkde = st.gaussian_kde(train_df['target'])\nplt.plot(kde_xs, kde.pdf(kde_xs), label=\"PDF\")\nplt.ylabel('Frequency')\nplt.xlabel('Target')\nplt.title(\"Target Distribution\");","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:07:54.061619Z","iopub.execute_input":"2022-03-29T17:07:54.061961Z","iopub.status.idle":"2022-03-29T17:07:54.511469Z","shell.execute_reply.started":"2022-03-29T17:07:54.06192Z","shell.execute_reply":"2022-03-29T17:07:54.510383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Target Variable** \\\nOf the values mentioned, the most intriguing values are the minimum and maximum values in the target variable, which are -3.676267773 and 1.711389827 respectively.\n\nThe mean value of the target variable is -0.9596573929279933, which indicates that on average most text is on the difficult end.\n\nAfterwards, the distribution is plotted, showing a bell curve.","metadata":{}},{"cell_type":"code","source":"#View standard_error in depth\n#Title#\nprint(\"standard_error Variable\")\nprint(\"----------\")\n\n#Mean#\nstandard_error_mean = train_df[\"standard_error\"].mean()\nprint(f\"Mean: {standard_error_mean}\")\n\n#Median#\nstandard_error_median = train_df[\"standard_error\"].median()\nprint(f\"Median: {standard_error_median}\")\n\n#Standard Deviation\nstandard_error_std = train_df[\"standard_error\"].std()\nprint(f\"Standard Deviation: {standard_error_std}\")\n\n#Minimum Value\nstandard_error_min = train_df[\"standard_error\"].min()\nprint(f\"Minimum Value: {standard_error_min}\")\n\n#25th Percentile\nstandard_error_25 = np.percentile(train_df[\"standard_error\"],25)\nprint(f\"25th Percentile: {standard_error_25}\")\n\n#50th Percentile\nstandard_error_50 = np.percentile(train_df[\"standard_error\"],50)\nprint(f\"50th Percentile: {standard_error_50}\")\n\n#75th Percentile\nstandard_error_75 = np.percentile(train_df[\"standard_error\"],75)\nprint(f\"75th Percentile: {standard_error_75}\")\n\n#Maximum Value\nstandard_error_max = train_df[\"standard_error\"].max()\nprint(f\"Maximum Value: {standard_error_max}\")\n\n#Skew\nstandard_error_skew = train_df[\"target\"].skew(axis= 0, skipna = True)\nprint(f\"Skew: {standard_error_skew}\")\n\n#Plot 'standard_error' variable\n\nplt.hist(train_df['standard_error'],edgecolor = 'black', bins=50, density=True)\nmn, mx = plt.xlim()\nplt.xlim(mn, mx)\nkde_xs = np.linspace(mn, mx, 300)\nkde = st.gaussian_kde(train_df['standard_error'])\nplt.plot(kde_xs, kde.pdf(kde_xs), label=\"PDF\")\nplt.ylabel('Frequency')\nplt.xlabel('Standard Error')\nplt.title(\"Standard Error Distribution\");","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:07:54.512739Z","iopub.execute_input":"2022-03-29T17:07:54.513031Z","iopub.status.idle":"2022-03-29T17:07:54.788543Z","shell.execute_reply.started":"2022-03-29T17:07:54.513002Z","shell.execute_reply":"2022-03-29T17:07:54.787755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text = text.lower().split()\n    return [word for word in text if word not in stopwords.words('english')]\n\ntrain_df['temp'] = train_df[\"excerpt\"].apply(lambda x : clean_text(x))\n\ntop = Counter([word for words in train_df['temp'] for word in words])\n\nwordlist = pd.DataFrame(top.most_common(20),columns = ['Word','Frequency'])\n\n#Bar Chart For Word Frequency\nwordlist.plot.bar(x='Word',y='Frequency')\nplt.ylabel('Frequency')\nplt.xlabel('Word')\nplt.title(\"Top 20 Words Frequency Distribution - Excerpt\");","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:07:54.789772Z","iopub.execute_input":"2022-03-29T17:07:54.790058Z","iopub.status.idle":"2022-03-29T17:08:51.659852Z","shell.execute_reply.started":"2022-03-29T17:07:54.790032Z","shell.execute_reply":"2022-03-29T17:08:51.658839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text = text.lower().split()\n    return [word for word in text if word not in stopwords.words('english')]\n\ntrain_df['temp'] = train_df[\"excerpt_preprocessed\"].apply(lambda x : clean_text(x))\n\ntop = Counter([word for words in train_df['temp'] for word in words])\n\nwordlist = pd.DataFrame(top.most_common(20),columns = ['Word','Frequency'])\n\n#Bar Chart For Word Frequency\nwordlist.plot.bar(x='Word',y='Frequency', color='purple')\nplt.ylabel('Frequency')\nplt.xlabel('Word')\nplt.title(\"Top 20 Words Frequency Distribution - Excerpt Preprocessed\");","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:08:51.661096Z","iopub.execute_input":"2022-03-29T17:08:51.661364Z","iopub.status.idle":"2022-03-29T17:09:20.891014Z","shell.execute_reply.started":"2022-03-29T17:08:51.661338Z","shell.execute_reply":"2022-03-29T17:09:20.890067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Excerpt**","metadata":{}},{"cell_type":"code","source":"def avg_word_len(text):\n    avg_len = text.str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n    return avg_len\n\nfig, ax = plt.subplots(4, 1, figsize=(10,15))\n\ntrain_df['text_len'] = train_df['excerpt'].str.split().map(lambda x: len(x))\nsns.scatterplot(x='text_len', y='target', data=train_df, color='blue', ax=ax[0])\nax[0].set_title(\"Word Count vs Target - Excerpt\", fontweight =\"bold\")\n\navg_len = avg_word_len(train_df['excerpt'])\ntrain_df['avg_word_len'] = avg_len\nsns.scatterplot(x='avg_word_len', y='target', data=train_df, color='blue', ax=ax[1])\nax[1].set_title(\"Average Word Length vs Target - Excerpt\", fontweight =\"bold\")\n\ntrain_df['no_sents'] = train_df['excerpt'].apply(lambda x : len(x.split('\\n')))\nsns.scatterplot(x='no_sents', y='target', data=train_df, color='blue', ax=ax[2])\nax[2].set_title(\"Sentence Count vs Target - Excerpt\", fontweight =\"bold\")\n\ntrain_df['chr_len'] = train_df['excerpt'].str.len()\nsns.scatterplot(x='chr_len', y='target', data=train_df, color='blue', ax=ax[3])\nax[3].set_title(\"Character Count vs Target - Excerpt\", fontweight =\"bold\")\n\nplt.subplots_adjust(hspace=0.35)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:09:20.892147Z","iopub.execute_input":"2022-03-29T17:09:20.892401Z","iopub.status.idle":"2022-03-29T17:09:21.799854Z","shell.execute_reply.started":"2022-03-29T17:09:20.892376Z","shell.execute_reply":"2022-03-29T17:09:21.799018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def min_max_mean_sentence_length(text):\n\n    tokened_sent = sent_tokenize(text)\n    main_dict = {}\n    for item in tokened_sent:\n        item1 = list(item.split(\" \"))\n        item2 = [' '.join(item1)]\n        Length = []\n        Length.append(len(item1))\n        mydict = dict(zip(item2, Length))\n        main_dict.update(mydict)\n\n    return max(main_dict.values()), min(main_dict.values()), round(mean(main_dict.values()),3)\n\ntrain_df[['max_len_sent','min_len_sent','avg_len_sent']] = train_df.apply(lambda x: min_max_mean_sentence_length(x['excerpt']),axis=1, result_type='expand')\n\nfig, ax = plt.subplots(3, 1, figsize=(10,15))\nsns.scatterplot(x='max_len_sent', y='target', data=train_df, color='blue', ax=ax[0])\nax[0].set_title(\"Maximum Sentence Length in Excerpt vs Target - Excerpt\", fontweight =\"bold\")\n\nsns.scatterplot(x='min_len_sent', y='target', data=train_df, color='blue', ax=ax[1])\nax[1].set_title(\"Minimum Sentence Length in Excerpt vs Target - Excerpt\", fontweight =\"bold\")\n\nsns.scatterplot(x='avg_len_sent', y='target', data=train_df, color='blue', ax=ax[2])\nax[2].set_title(\"Average Sentence Length in Excerpt vs Target - Excerpt\", fontweight =\"bold\")","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:09:21.801367Z","iopub.execute_input":"2022-03-29T17:09:21.801666Z","iopub.status.idle":"2022-03-29T17:09:23.901499Z","shell.execute_reply.started":"2022-03-29T17:09:21.801634Z","shell.execute_reply":"2022-03-29T17:09:23.900393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Excerpt Preprocessed**","metadata":{}},{"cell_type":"code","source":"def avg_word_len(text):\n    avg_len = text.str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n    return avg_len\n\nfig, ax = plt.subplots(4, 1, figsize=(10,15))\n\ntrain_df['text_len_pre'] = train_df['excerpt_preprocessed'].str.split().map(lambda x: len(x))\nsns.scatterplot(x='text_len_pre', y='target', data=train_df, color='purple', ax=ax[0])\nax[0].set_title(\"Word Count vs Target - Excerpt Preprocessed\", fontweight =\"bold\")\n\navg_len_pre = avg_word_len(train_df['excerpt_preprocessed'])\ntrain_df['avg_word_len_pre'] = avg_len_pre\nsns.scatterplot(x='avg_word_len_pre', y='target', data=train_df, color='purple', ax=ax[1])\nax[1].set_title(\"Average Word Length vs Target - Excerpt Preprocessed\", fontweight =\"bold\")\n\ntrain_df['no_sents_pre'] = train_df['excerpt_preprocessed'].apply(lambda x : len(x.split('\\n')))\nsns.scatterplot(x='no_sents_pre', y='target', data=train_df, color='purple', ax=ax[2])\nax[2].set_title(\"Sentence Count vs Target - Excerpt Preprocessed\", fontweight =\"bold\")\n\ntrain_df['chr_len_pre'] = train_df['excerpt_preprocessed'].str.len()\nsns.scatterplot(x='chr_len_pre', y='target', data=train_df, color='purple', ax=ax[3])\nax[3].set_title(\"Character Count vs Target - Excerpt Preprocessed\", fontweight =\"bold\")\n\nplt.subplots_adjust(hspace=0.35)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:09:23.902935Z","iopub.execute_input":"2022-03-29T17:09:23.903229Z","iopub.status.idle":"2022-03-29T17:09:24.634165Z","shell.execute_reply.started":"2022-03-29T17:09:23.903199Z","shell.execute_reply":"2022-03-29T17:09:24.633427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Combined**","metadata":{}},{"cell_type":"code","source":"def avg_word_len(text):\n    avg_len = text.str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n    return avg_len\n\nfig, ax = plt.subplots(4, 1, figsize=(10,15))\n\ntrain_df['text_len'] = train_df['excerpt'].str.split().map(lambda x: len(x))\ntrain_df['text_len_pre'] = train_df['excerpt_preprocessed'].str.split().map(lambda x: len(x))\nsns.scatterplot(x='text_len', y='target', data=train_df, color='blue', ax=ax[0])\nsns.scatterplot(x='text_len_pre', y='target', data=train_df, color='purple', ax=ax[0])\nax[0].set_title(\"Word Count vs Target - Combined\", fontweight =\"bold\")\n\navg_len = avg_word_len(train_df['excerpt'])\navg_len_pre = avg_word_len(train_df['excerpt_preprocessed'])\ntrain_df['avg_word_len'] = avg_len\ntrain_df['avg_word_len_pre'] = avg_len_pre\nsns.scatterplot(x='avg_word_len', y='target', data=train_df, color='blue', ax=ax[1])\nsns.scatterplot(x='avg_word_len_pre', y='target', data=train_df, color='purple', ax=ax[1])\nax[1].set_title(\"Average Word Length vs Target - Combined\", fontweight =\"bold\")\n\ntrain_df['no_sents'] = train_df['excerpt'].apply(lambda x : len(x.split('\\n')))\ntrain_df['no_sents_pre'] = train_df['excerpt_preprocessed'].apply(lambda x : len(x.split('\\n')))\nsns.scatterplot(x='no_sents', y='target', data=train_df, color='blue', ax=ax[2])\nsns.scatterplot(x='no_sents_pre', y='target', data=train_df, color='purple', ax=ax[2])\nax[2].set_title(\"Sentence Count vs Target - Combined\", fontweight =\"bold\")\n\ntrain_df['chr_len'] = train_df['excerpt'].str.len()\ntrain_df['chr_len_pre'] = train_df['excerpt_preprocessed'].str.len()\nsns.scatterplot(x='chr_len', y='target', data=train_df, color='blue', ax=ax[3])\nsns.scatterplot(x='chr_len_pre', y='target', data=train_df, color='purple', ax=ax[3])\nax[3].set_title(\"Character Count vs Target - Combined\", fontweight =\"bold\")\n\nplt.subplots_adjust(hspace=0.35)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:09:24.635182Z","iopub.execute_input":"2022-03-29T17:09:24.635547Z","iopub.status.idle":"2022-03-29T17:09:25.812296Z","shell.execute_reply.started":"2022-03-29T17:09:24.635519Z","shell.execute_reply":"2022-03-29T17:09:25.811316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploring Excerpt Data** \\\nWorking with the excerpts, we look at the following and graph it: \\\nWord Count = Total number of words in the excerpt \\\nAverage Word Length = Average character length per word in excerpt \\\nSentence Count = Total number of sentences in excerpt \\\nCharacter Length = Total number of characters in excerpt \\\nMaximum Sentence Length = Maximum length of sentence in excerpt \\\nMinimum Sentence Length = Minimum length of sentence in excerpt \\\nAverage Sentence Length = Average length of sentence in excerpt\n\nExploring the excerpt, we look at excerpt and the preprocessed version of excerpt.  \\\nIn my exploratory data analysis of excerpt, I graphed both excerpt and the preprocessed version of excerpt for each of the categories for excerpt for comparison purposes. \\\nWe want to know how each of the categories affects the target variable. Looking at the target variable, we also want to know how preprocessing the data changes the target variable's distribution in a scatterplot.\n","metadata":{}},{"cell_type":"code","source":"def training(model, X_train, y_train, X_test, y_test, model_name):\n    \n    model = make_pipeline(\n        TfidfVectorizer(binary=True, ngram_range=(1,1)),\n        model,\n    )\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    MSE = mse(y_test, y_pred)\n    \n    print(\"Model:\", model_name)\n    print(\"Mean Squared Error:\", MSE)\n    \n    plt.bar(model_name, MSE, align = 'center', alpha = 0.5)\n    plt.xticks(rotation='vertical')\n    plt.ylabel('Mean Squared Error')\n    plt.xlabel('Regression Model')\n    plt.title(\"Regression Model: Mean Squared Error\");\n\nrfr = RandomForestRegressor()\ngbr = GradientBoostingRegressor()\nsvr = SVR()\nabr = AdaBoostRegressor()\nxg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 100)\nridge = Ridge()\nlr = LinearRegression()\nm = [rfr,gbr,svr,abr,xg,ridge,lr]\nmn = [\"Random Forest Regression\",\"Gradient Boosting Regression\",\"Support Vector Regression\",\"AdaBoost Regression\",\"XGBoost Regression\",\"Ridge Regression\",\"Linear Regression\"]\n\nX = train_df[\"excerpt_preprocessed\"].values\ny = train_df['target'].values\n    \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \nfor i in range(0,len(m)):\n    training(model=m[i], X_train=X_train, y_train=y_train, X_test=X_test,y_test=y_test, model_name= mn[i])","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:09:25.813531Z","iopub.execute_input":"2022-03-29T17:09:25.813829Z","iopub.status.idle":"2022-03-29T17:11:28.187467Z","shell.execute_reply.started":"2022-03-29T17:09:25.813798Z","shell.execute_reply":"2022-03-29T17:11:28.186367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training Model**\n\n**Mean Squared Error** \\\nBased on the results, it would appear that Ridge Regression has the best results since it has the lowest mean squared error. \\\nXGBoost had very similar results regardless of when the n estimator is 100 or 1000.","metadata":{}},{"cell_type":"code","source":"def training_all(model,X,y):\n    \n    model = make_pipeline(\n        TfidfVectorizer(binary=True, ngram_range=(1,1)),\n        model,\n    )\n    model.fit(X, y)\n    y_pred = model.predict(test_df[\"excerpt_preprocessed\"])\n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:11:28.18913Z","iopub.execute_input":"2022-03-29T17:11:28.189881Z","iopub.status.idle":"2022-03-29T17:11:28.196076Z","shell.execute_reply.started":"2022-03-29T17:11:28.189833Z","shell.execute_reply":"2022-03-29T17:11:28.195275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results**","metadata":{}},{"cell_type":"code","source":"#Random Forest Regression Results\ntest_pred = training_all(rfr,X,y)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:11:28.197318Z","iopub.execute_input":"2022-03-29T17:11:28.197921Z","iopub.status.idle":"2022-03-29T17:14:32.613323Z","shell.execute_reply.started":"2022-03-29T17:11:28.197876Z","shell.execute_reply":"2022-03-29T17:14:32.611945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gradient Boosting Regression Results\ntest_pred = training_all(gbr,X,y)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:14:32.614801Z","iopub.execute_input":"2022-03-29T17:14:32.615128Z","iopub.status.idle":"2022-03-29T17:14:39.628326Z","shell.execute_reply.started":"2022-03-29T17:14:32.615098Z","shell.execute_reply":"2022-03-29T17:14:39.627416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Support Vector Regression Results\ntest_pred = training_all(svr,X,y)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:14:39.629381Z","iopub.execute_input":"2022-03-29T17:14:39.629631Z","iopub.status.idle":"2022-03-29T17:14:46.032295Z","shell.execute_reply.started":"2022-03-29T17:14:39.629606Z","shell.execute_reply":"2022-03-29T17:14:46.031243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AdaBoost Regression Results\ntest_pred = training_all(abr,X,y)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:14:46.033497Z","iopub.execute_input":"2022-03-29T17:14:46.033837Z","iopub.status.idle":"2022-03-29T17:14:49.818581Z","shell.execute_reply.started":"2022-03-29T17:14:46.033794Z","shell.execute_reply":"2022-03-29T17:14:49.817592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGBoost Regression Results\ntest_pred = training_all(xg,X,y)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:14:49.821924Z","iopub.execute_input":"2022-03-29T17:14:49.822329Z","iopub.status.idle":"2022-03-29T17:14:52.032446Z","shell.execute_reply.started":"2022-03-29T17:14:49.822297Z","shell.execute_reply":"2022-03-29T17:14:52.031786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ridge Regression Results\ntest_pred = training_all(ridge,X,y)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:14:52.033573Z","iopub.execute_input":"2022-03-29T17:14:52.033864Z","iopub.status.idle":"2022-03-29T17:14:52.349069Z","shell.execute_reply.started":"2022-03-29T17:14:52.033836Z","shell.execute_reply":"2022-03-29T17:14:52.347671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Linear Regression Results\ntest_pred = training_all(lr,X,y)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:14:52.351141Z","iopub.execute_input":"2022-03-29T17:14:52.351667Z","iopub.status.idle":"2022-03-29T17:14:52.810192Z","shell.execute_reply.started":"2022-03-29T17:14:52.351565Z","shell.execute_reply":"2022-03-29T17:14:52.80905Z"},"trusted":true},"execution_count":null,"outputs":[]}]}