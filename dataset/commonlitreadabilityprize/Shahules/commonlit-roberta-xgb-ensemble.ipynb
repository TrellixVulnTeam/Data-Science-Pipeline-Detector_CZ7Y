{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nThe goal of this competition is to predict the text difficulty for a given text.The text difficulty here ranges from **-3.67 to 1.71**,the latter one being most difficult.\nHere in this notebook I am doing an **Exploratory Data Analysis** and baseline model using **tensorflow and roberta** ( huggingface). Hope you will learn something new from this notebook.\n\n<img src=\"https://media.giphy.com/media/WoWm8YzFQJg5i/giphy.gif\" alt=\"Paris\" class=\"center\" >\n","metadata":{}},{"cell_type":"markdown","source":"### Importing imporant libraries","metadata":{}},{"cell_type":"code","source":"!pip install ../input/textstat-pypi/Pyphen-0.9.3-py2.py3-none-any.whl\n!pip install ../input/textstat-pypi/textstat-0.7.0-py3-none-any.whl\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-05-23T14:01:47.397988Z","iopub.execute_input":"2021-05-23T14:01:47.398324Z","iopub.status.idle":"2021-05-23T14:02:40.106948Z","shell.execute_reply.started":"2021-05-23T14:01:47.398282Z","shell.execute_reply":"2021-05-23T14:02:40.105963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tokenizers\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nimport matplotlib.colors as mcolors\nimport matplotlib.colors as mcolors\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nimport gensim,pyLDAvis\nfrom collections import Counter\nimport pyLDAvis.gensim_models as gensimvis\nfrom nltk.tokenize import word_tokenize\nfrom warnings import filterwarnings\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import *\nfrom sklearn.model_selection import train_test_split,KFold\nfrom tqdm import tqdm\nfrom tensorflow.keras.layers import Dense, Input,GlobalAveragePooling1D,Dropout,Average\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom IPython.core.display import display, HTML\nimport xgboost as xgb\nROOT = '../input/tf-meta-features-bert/'","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T14:02:40.109944Z","iopub.execute_input":"2021-05-23T14:02:40.110292Z","iopub.status.idle":"2021-05-23T14:02:49.012858Z","shell.execute_reply.started":"2021-05-23T14:02:40.110252Z","shell.execute_reply":"2021-05-23T14:02:49.012007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class color:\n    BOLD = '\\033[1m' + '\\033[93m'\n    END = '\\033[0m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    \nsns.set(style='whitegrid')\n   \nfilterwarnings(\"ignore\")\ndisplay(HTML(\"<style>.container { max-width:100% !important; }</style>\"))\ndisplay(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\ndisplay(HTML(\"<style>.output_area { max-width:100% !important; }</style>\"))\ndisplay(HTML(\"<style>.input_area { max-width:100% !important; }</style>\"))\npyLDAvis.enable_notebook()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T14:02:49.015063Z","iopub.execute_input":"2021-05-23T14:02:49.015455Z","iopub.status.idle":"2021-05-23T14:02:49.032408Z","shell.execute_reply.started":"2021-05-23T14:02:49.015395Z","shell.execute_reply":"2021-05-23T14:02:49.031448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\",nrows=300)\ndf_test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n\nprint(f\"Train data has {df_train.shape[0]} rows as {df_train.shape[1]} columns\")\nprint(f\"Test data has {df_test.shape[0]} rows as {df_test.shape[1]} columns\")","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:02:49.034316Z","iopub.execute_input":"2021-05-23T14:02:49.034889Z","iopub.status.idle":"2021-05-23T14:02:49.292977Z","shell.execute_reply.started":"2021-05-23T14:02:49.034851Z","shell.execute_reply":"2021-05-23T14:02:49.292124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:02:49.294203Z","iopub.execute_input":"2021-05-23T14:02:49.294713Z","iopub.status.idle":"2021-05-23T14:02:49.315732Z","shell.execute_reply.started":"2021-05-23T14:02:49.294674Z","shell.execute_reply":"2021-05-23T14:02:49.314879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"### Let's look at the target distribution first.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 15))\nsns.displot(df_train['target'],kde=True)\nplt.title(\"Target Distribution\", size=20)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T14:02:49.316926Z","iopub.execute_input":"2021-05-23T14:02:49.317258Z","iopub.status.idle":"2021-05-23T14:02:49.708895Z","shell.execute_reply.started":"2021-05-23T14:02:49.317224Z","shell.execute_reply":"2021-05-23T14:02:49.708117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This looks like a normal distrbution with values ranging from ~ -3 to ~-1. Let's look at the minimum and maximum value of the target here.","metadata":{}},{"cell_type":"code","source":"print(f\"The min value of target is {color.BOLD} {df_train['target'].min()} {color.END} and max value of target is {color.BOLD} {df_train['target'].max()}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:02:49.71013Z","iopub.execute_input":"2021-05-23T14:02:49.710474Z","iopub.status.idle":"2021-05-23T14:02:49.715611Z","shell.execute_reply.started":"2021-05-23T14:02:49.710417Z","shell.execute_reply":"2021-05-23T14:02:49.714801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Standard Error","metadata":{}},{"cell_type":"markdown","source":"There is a column names **standard_error**. What is this?\nMultiple coders rate each texts and they might disagree on the readability of texts. This measure of spread of scores among multiple raters for each text.\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 15))\nsns.displot(df_train['standard_error'],kde=True)\nplt.title(\"Target Distribution\", size=20)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T14:02:49.718341Z","iopub.execute_input":"2021-05-23T14:02:49.718929Z","iopub.status.idle":"2021-05-23T14:02:50.172918Z","shell.execute_reply.started":"2021-05-23T14:02:49.718891Z","shell.execute_reply":"2021-05-23T14:02:50.172064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that the standard error is very left skewed, this implies that there are many instances in which the coders ( people who rate the text) have disagreed.","metadata":{}},{"cell_type":"markdown","source":"### Binning and segment Analysis","metadata":{}},{"cell_type":"code","source":"bins = 3\nw = (df_train['target'].max() - df_train['target'].min())/bins\nvals = df_train['target'].min()+w,df_train['target'].min()+2*w,df_train['target'].min()+3*w\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:02:50.174824Z","iopub.execute_input":"2021-05-23T14:02:50.175367Z","iopub.status.idle":"2021-05-23T14:02:50.18156Z","shell.execute_reply.started":"2021-05-23T14:02:50.175324Z","shell.execute_reply":"2021-05-23T14:02:50.1807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_binning(x,vals=vals):\n    \n    if x<=vals[0]:\n        return \"Low\"\n    elif x>vals[0] and x<=vals[1]:\n        return \"Medium\"\n    else:\n        return \"High\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T14:02:50.184357Z","iopub.execute_input":"2021-05-23T14:02:50.184667Z","iopub.status.idle":"2021-05-23T14:02:50.192062Z","shell.execute_reply.started":"2021-05-23T14:02:50.184639Z","shell.execute_reply":"2021-05-23T14:02:50.191268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['segment'] = df_train['target'].map(lambda x : do_binning(x))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:02:50.194639Z","iopub.execute_input":"2021-05-23T14:02:50.195005Z","iopub.status.idle":"2021-05-23T14:02:50.202579Z","shell.execute_reply.started":"2021-05-23T14:02:50.194972Z","shell.execute_reply":"2021-05-23T14:02:50.201703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nx=df_train.segment.value_counts().index\ny=df_train.segment.value_counts().values\nsns.barplot(x,y)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T14:02:50.203832Z","iopub.execute_input":"2021-05-23T14:02:50.204184Z","iopub.status.idle":"2021-05-23T14:02:50.338083Z","shell.execute_reply.started":"2021-05-23T14:02:50.204149Z","shell.execute_reply":"2021-05-23T14:02:50.337373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Level Analysis","metadata":{}},{"cell_type":"markdown","source":"### WordCloud","metadata":{}},{"cell_type":"code","source":"stop=set(stopwords.words('english'))\n\ndef preprocess(df):\n    corpus=[]\n    stem=PorterStemmer()\n    lem=WordNetLemmatizer()\n    for news in df['excerpt']:\n        words=[w for w in word_tokenize(news) if (w not in stop)]\n        \n        words=[lem.lemmatize(w) for w in words if len(w)>2]\n        \n        corpus.append(words)\n    return corpus\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T14:02:50.339252Z","iopub.execute_input":"2021-05-23T14:02:50.339603Z","iopub.status.idle":"2021-05-23T14:02:50.362343Z","shell.execute_reply.started":"2021-05-23T14:02:50.339569Z","shell.execute_reply":"2021-05-23T14:02:50.361636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_wordcloud(data):\n    data = \" \".join([ i for texts in data for i in texts])\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stop,\n        max_words=150,\n        max_font_size=50,\n        scale=3,\n        random_state=1,width = 800, height = 800)\n   \n    wordcloud=wordcloud.generate(str(data))\n\n    return wordcloud\n\n\nfig,ax = plt.subplots(1,3,figsize=(16,6))\n\nfor i,segment in enumerate(['Low','Medium','High']):\n    \n    corpus = preprocess(df_train.query(f'segment==\"{segment}\"'))\n    wordcloud = show_wordcloud(corpus)\n    ax[i].imshow(wordcloud)\n    ax[i].set_title(segment,fontweight='bold')\n    ax[i].axis('off')\n    \n    \nplt.show()\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:02:50.363482Z","iopub.execute_input":"2021-05-23T14:02:50.363971Z","iopub.status.idle":"2021-05-23T14:02:58.254883Z","shell.execute_reply.started":"2021-05-23T14:02:50.363936Z","shell.execute_reply":"2021-05-23T14:02:58.249172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline","metadata":{}},{"cell_type":"markdown","source":"## Tokenizer \n\nHere I am using ByterLevelBPEtokenizer. ","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 256\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab=PATH+'vocab-roberta-base.json', \n    merges=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:02:58.256238Z","iopub.execute_input":"2021-05-23T14:02:58.256709Z","iopub.status.idle":"2021-05-23T14:02:58.396609Z","shell.execute_reply.started":"2021-05-23T14:02:58.256639Z","shell.execute_reply":"2021-05-23T14:02:58.395805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_encode_texts(texts,tokenizer,batch_size=256):\n\n    \n    \n    all_ids = []\n    all_masks = []\n    \n    for batch in tqdm(range(0,len(texts),batch_size)):\n        encoder = tokenizer.encode_batch(texts[batch:batch+batch_size],add_special_tokens=False)\n        all_ids.extend([([0]+enc.ids[:MAX_LEN-2]+[2])+[0]*(MAX_LEN-(len(enc.ids)+2)) for enc in encoder])\n        all_masks.extend([([1]+mask.attention_mask[:MAX_LEN-2]+[1])+[0]*(MAX_LEN-(len(mask.attention_mask)+2)) for mask in encoder])\n        \n    return np.array(all_ids),np.array(all_masks)\n\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:02:58.399523Z","iopub.execute_input":"2021-05-23T14:02:58.399791Z","iopub.status.idle":"2021-05-23T14:02:58.410819Z","shell.execute_reply.started":"2021-05-23T14:02:58.399756Z","shell.execute_reply":"2021-05-23T14:02:58.40994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Meta-Features","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import MinMaxScaler\nimport spacy\nfrom textstat.textstat import textstatistics, legacy_round\nimport textstat","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-23T14:02:58.413573Z","iopub.execute_input":"2021-05-23T14:02:58.413807Z","iopub.status.idle":"2021-05-23T14:02:59.878525Z","shell.execute_reply.started":"2021-05-23T14:02:58.413785Z","shell.execute_reply":"2021-05-23T14:02:59.877688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/english-common-words/20k.txt\",'r') as file:\n    common_words = file.readlines()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:02:59.879756Z","iopub.execute_input":"2021-05-23T14:02:59.880077Z","iopub.status.idle":"2021-05-23T14:02:59.905921Z","shell.execute_reply.started":"2021-05-23T14:02:59.880044Z","shell.execute_reply":"2021-05-23T14:02:59.905139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stops = np.array(stopwords.words('english'))\nwith open(\"../input/english-common-words/20k.txt\",'r') as file:\n    common_words = file.readlines()\n    \nNLP = spacy.load('en')\n\n\ndef make_sentences(text):\n    doc = NLP(text)\n    return [x for x in doc.sents]\n  \n    \ndef get_meta_features(df,col):\n    \n    tfidf = TfidfVectorizer()\n    #tsvd = TruncatedSVD(n_components = 50)\n    \n    df['word_count'] = df[col].map(lambda x : len(x.split()))\n    df['unique_words'] = df[col].map(lambda x : len(np.unique(x.split())))\n    df['char_len'] = df[col].str.len()\n    df['stop_words'] = df[col].map(lambda x : len(np.intersect1d(x.split(),stops)))\n    df['sentence_count'] = df[col].map(lambda x : len(make_sentences(x)))\n    df['common_words'] = df[col].map(lambda x : len(np.intersect1d(x.split(),common_words)))\n    df['syllable_count'] = df[col].map(lambda x : textstatistics().syllable_count(x))\n    df['average_syllable_word'] = df['word_count']/df['syllable_count']\n    df['average_sentence_len'] = df['word_count']/df['sentence_count']\n    df['flesch_reading_ease'] = df[col].map(lambda x : textstat.flesch_reading_ease(x) )\n    df['smog_index'] = df[col].map(lambda x : textstat.smog_index(x))\n    df['difficult_words'] = df[col].map(lambda x : textstat.difficult_words(x))\n    \n    \n    vectors = tfidf.fit_transform(df[col].values).toarray()\n    \n    #vectors = tsvd.fit_transform(vectors)\n    count_features = df[['word_count','unique_words','char_len','stop_words','sentence_count',\n                         'common_words','syllable_count','average_syllable_word','average_sentence_len',\n                        'flesch_reading_ease','smog_index','difficult_words']].values\n    return np.hstack([count_features,vectors])\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:02:59.907091Z","iopub.execute_input":"2021-05-23T14:02:59.907416Z","iopub.status.idle":"2021-05-23T14:03:00.938292Z","shell.execute_reply.started":"2021-05-23T14:02:59.907382Z","shell.execute_reply":"2021-05-23T14:03:00.937493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\nThis is a very simple model that averages the 786 dim output and passes it through a linear layer to produce teh output.","metadata":{}},{"cell_type":"code","source":"def commonlit_model(max_len=256):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    attention_masks = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_masks\")\n    \n    config = RobertaConfig.from_pretrained('../input/tf-roberta/config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained('../input/tf-roberta/pretrained-roberta-base.h5',config=config)\n    output = bert_model(input_word_ids,attention_mask = attention_masks)[0]\n    \n    pool = GlobalAveragePooling1D()(output)\n    \n    dense=[]\n    FC = Dense(32,activation='relu')\n    for p in np.linspace(0.1,0.5,5):\n        x=Dropout(p)(pool)\n#         x=FC(x)\n        x=Dense(1)(x)\n        dense.append(x)\n    \n    out = Average()(dense)\n    model = Model(inputs=[input_word_ids,attention_masks], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:03:00.939691Z","iopub.execute_input":"2021-05-23T14:03:00.940034Z","iopub.status.idle":"2021-05-23T14:03:00.947948Z","shell.execute_reply.started":"2021-05-23T14:03:00.939999Z","shell.execute_reply":"2021-05-23T14:03:00.946894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE =16","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:03:00.949474Z","iopub.execute_input":"2021-05-23T14:03:00.949907Z","iopub.status.idle":"2021-05-23T14:03:00.96161Z","shell.execute_reply.started":"2021-05-23T14:03:00.949867Z","shell.execute_reply":"2021-05-23T14:03:00.960702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KFold Model","metadata":{}},{"cell_type":"markdown","source":"I am using 10 fold split, only using the first 5 folds.\n","metadata":{}},{"cell_type":"code","source":"SEED=42\n\ndef train_nn(df_train,n_splits):\n    \n    train_ids,train_masks = batch_encode_texts(df_train['excerpt'].values,tokenizer)\n    target = df_train['target'].values\n    \n    skf = KFold(n_splits=n_splits,shuffle=True,random_state=SEED)\n    train_metics,valid_metrics=[],[]\n\n    for fold,(train_idx,valid_idx) in enumerate(skf.split(df_train)):\n\n\n            x_train_ids,x_train_masks,y_train = train_ids[train_idx],train_masks[train_idx],target[train_idx]\n            x_valid_ids,x_valid_masks,y_valid = train_ids[valid_idx],train_masks[valid_idx],target[valid_idx]\n\n\n\n            train_dataset = (\n            tf.data.Dataset\n            .from_tensor_slices(((x_train_ids,x_train_masks,train_meta),y_train))\n            .repeat()\n            .shuffle(2048)\n            .batch(BATCH_SIZE)\n            .prefetch(AUTO)\n            )\n\n            valid_dataset = (\n                tf.data.Dataset\n                .from_tensor_slices(((x_valid_ids,x_valid_masks),y_valid))\n                .batch(BATCH_SIZE)\n                .cache()\n                .prefetch(AUTO)\n            )\n\n            checkpoint = tf.keras.callbacks.ModelCheckpoint(\n                f'commonlit_fold{fold}.h5', monitor='val_root_mean_squared_error', verbose=0, save_best_only=True,\n                save_weights_only=True, mode='auto', save_freq='epoch')\n\n            model = commonlit_model()\n\n            n_steps = len(train_idx)// BATCH_SIZE\n            valid_steps = len(valid_idx)// BATCH_SIZE\n\n            print(color.OKCYAN,f\"Eval fold {fold}...\")\n            train_history = model.evaluate(valid_dataset)\n            print(color.OKBLUE,f\"Average train RMSE of fold {fold} = {np.mean(train_history.history['root_mean_squared_error']):.4f}\")\n            print(color.OKBLUE,f\"Average validation RMSE of fold {fold} = {np.mean(train_history.history['val_root_mean_squared_error']):.4f}\")\n\n            train_metics.append(np.mean(train_history.history['root_mean_squared_error']))\n            valid_metrics.append(np.mean(train_history.history['val_root_mean_squared_error']))\n        \n       \n            \n       \n    \n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:03:00.962939Z","iopub.execute_input":"2021-05-23T14:03:00.963346Z","iopub.status.idle":"2021-05-23T14:03:00.97537Z","shell.execute_reply.started":"2021-05-23T14:03:00.963312Z","shell.execute_reply":"2021-05-23T14:03:00.97453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_xgb(df_train,n_splits):\n    \n\n    train_meta_features = get_meta_features(df_train,'excerpt')\n\n    skf = KFold(n_splits=n_splits,shuffle=True,random_state=SEED)\n    best_iterations=[]\n    oof_rmses,train_rmses=[],[]\n\n    for fold,(train_idx,valid_idx) in enumerate(skf.split(df_train)):\n\n        dtrain = xgb.DMatrix(train_meta_features[train_idx],target[train_idx])\n        dvalid =  xgb.DMatrix(train_meta_features[valid_idx],target[valid_idx])\n\n        evals_result = dict()\n        booster = xgb.train(params,\n                            dtrain,\n                            evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                            num_boost_round=300,\n                            early_stopping_rounds=20,\n                            evals_result=evals_result,\n                            verbose_eval=False)\n\n        best_iteration = np.argmin(evals_result['valid']['rmse'])\n        best_iterations.append(best_iteration)\n        oof_rmse = evals_result['valid']['rmse'][best_iteration]\n        train_rmse = evals_result['train']['rmse'][best_iteration]\n        oof_rmses.append(oof_rmse)\n        train_rmses.append(train_rmse)\n\n    evals_df = pd.DataFrame()\n    evals_df['fold'] = range(1, skf.n_splits+1)\n    evals_df['best_iteration'] = best_iterations\n    evals_df['oof_rmse'] = oof_rmses\n    evals_df['train_rmse'] = train_rmses\n\n    display(evals_df)\n    print('mean oof rmse = {}'.format(np.mean(oof_rmses)))\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:03:00.978748Z","iopub.execute_input":"2021-05-23T14:03:00.978988Z","iopub.status.idle":"2021-05-23T14:03:00.99004Z","shell.execute_reply.started":"2021-05-23T14:03:00.978963Z","shell.execute_reply":"2021-05-23T14:03:00.989234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Inference","metadata":{}},{"cell_type":"code","source":"import joblib\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:05:39.353889Z","iopub.execute_input":"2021-05-23T14:05:39.354229Z","iopub.status.idle":"2021-05-23T14:05:39.359521Z","shell.execute_reply.started":"2021-05-23T14:05:39.3542Z","shell.execute_reply":"2021-05-23T14:05:39.358497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_nn(df_test,n_splits=7):\n    \n    \n    test_ids,test_masks = batch_encode_texts(df_test['excerpt'].values,tokenizer)\n    y_test = np.zeros((len(df_test)))\n\n    test_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices(((test_ids,test_masks),y_test))\n        .batch(BATCH_SIZE)\n    )\n    \n    preds_nn = np.zeros((len(test_ids),1))\n    model = commonlit_model()\n    for fold in range(n_splits):\n        model.load_weights(ROOT+f'commonlit_fold{fold}.h5')\n        print(color.OKGREEN,f\"Inference fold {fold}...\")\n        preds_nn += model.predict(test_dataset, verbose=1)/ n_splits\n\n        \n    return preds_nn\n\n \ndef inference_xgb(df_test,n_splits=7):\n    \n    test_meta_features = get_meta_features(df_test,'excerpt')\n    \n    preds_xgb = np.zeros((len(test_meta_features)))\n    for fold in tqdm(range(n_splits)):\n        booster = joblib.load(ROOT+f'xgb_fold{fold}')\n        preds_xgb += booster.predict(xgb.DMatrix(test_meta_features)) / n_splits\n        \n    return preds_xgb\n\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:05:54.998154Z","iopub.execute_input":"2021-05-23T14:05:54.998591Z","iopub.status.idle":"2021-05-23T14:05:55.006657Z","shell.execute_reply.started":"2021-05-23T14:05:54.998545Z","shell.execute_reply":"2021-05-23T14:05:55.005669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_nn = inference_nn(df_test)\npreds_xgb = inference_xgb(df_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:05:57.476969Z","iopub.execute_input":"2021-05-23T14:05:57.477292Z","iopub.status.idle":"2021-05-23T14:05:58.103703Z","shell.execute_reply.started":"2021-05-23T14:05:57.477262Z","shell.execute_reply":"2021-05-23T14:05:58.103017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.average([preds_nn.flatten(),preds_xgb.flatten()],axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:07:32.573684Z","iopub.execute_input":"2021-05-23T14:07:32.574017Z","iopub.status.idle":"2021-05-23T14:07:32.578012Z","shell.execute_reply.started":"2021-05-23T14:07:32.573987Z","shell.execute_reply":"2021-05-23T14:07:32.577178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsub['target'] = preds\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T14:03:51.744634Z","iopub.status.idle":"2021-05-23T14:03:51.745291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Work in Progress! I hope you liked it :) ","metadata":{}}]}