{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentence Embeddings for Regression with RAPIDS","metadata":{}},{"cell_type":"markdown","source":"## Summary :\n1. Obtaining Sentence Embeddings from Transformers\n2. Using it for Regression","metadata":{}},{"cell_type":"markdown","source":"### Installing RAPIDS and other requirements","metadata":{}},{"cell_type":"markdown","source":"[RAPIDS](https://rapids.ai) enable you to perform every numpy, pandas or sklearn manipulation & modeling, entirely on GPU for higher performance.","metadata":{}},{"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cudf as pd # pandas on GPU\nimport cupy as np # numpy on GPU\nfrom cuml.decomposition import PCA # scikit-learn on GPU\n!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer # PyTorch supported\nimport gc\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading and Processing Text Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ndf_test  = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will concatenate text data from train and test database in order to process them globally.","metadata":{}},{"cell_type":"code","source":"n0 = df_train.shape[0]\n# text is upper case in source data, but models are lower case\n# txt = [sent3.lower() for df in (df_train, df_test) for t in df.excerpt.to_array() for sent in t.split('\\n') for sent2 in sent.split('.') for sent3 in sent2.split(';')]\ntxt = [t.lower() for df in (df_train, df_test) for t in df.excerpt.to_array()]\nids = [id_ for df in (df_train, df_test) for id_ in df.id.to_array() ]\ntxt[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IDS = []\nTXT = []\nfor t, id_ in zip(txt, ids):\n    sent = [w.replace('\\\\', '') for w in t.split('\\n')]\n    TXT += sent\n    for _ in range(len(sent)):\n        IDS.append(id_)\nTXT[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt = []\nids = []\nfor t, id_ in zip(TXT, IDS):\n    sent = [t]\n    for char in ['...', '.', ';', '!', '?', '\"']:\n        sent = [w for t_ in sent for w in t_.split(char)]\n    sent = list(filter(lambda w: len(w)>1, sent))\n    txt += sent\n    for _ in range(len(sent)):\n        ids.append(id_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# txt, ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformers are a very efficient way of getting optimal text embeddings.\nI will compute raw sentence embeddings based on the paraphrase-trained DistilRoberta. You can see more on this model [here](https://github.com/UKPLab/sentence-transformers) or [here](https://www.sbert.net).","metadata":{}},{"cell_type":"markdown","source":"### Model Preparation","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available(): # check if GPU enabled kernel\"\n    print('Cuda !')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer('paraphrase-distilroberta-base-v1', device='cuda')\nprint(f'Initial sequence length in paraphrase distilroberta : {model.max_seq_length}')\nprint(f'First sentence : {txt[0]}\\nCorresponding tokens : {model.tokenizer(txt[0])}')\nprint(f\"Maximal sequence length in our text data : {max([len(model.tokenizer(t)['input_ids']) for t in txt])}\")\nmodel.max_seq_length = 150\nprint(f'Resized sequence length in paraphrase distilroberta : {model.max_seq_length}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Raw Roberta embeddings","metadata":{}},{"cell_type":"code","source":"txt_encoded = np.array(model.encode(txt, normalize_embeddings=True))\ntxt_encoded.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(np.var(txt_encoded, axis=0).get(), bins=100)\nplt.title('Variance on the 768 Embedding Coordinates')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ids = [i for i in ids if i in df_train.id.to_pandas().values]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n0 = len(train_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test = txt_encoded[:n0, :], txt_encoded[n0:, :]\nx_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = np.array([df_train.loc[df_train.id==i, 'target'].values[0, 0] for i in train_ids])\ntargets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple predictive neuron","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.utils.data as Data\n\ntorch.manual_seed(1)    # reproducible\n\nnet = torch.nn.Sequential(\n        torch.nn.Linear(768, 200),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(200, 200),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(200, 100),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(100, 1),\n    )\nnet.cuda()\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\nloss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n\nBATCH_SIZE = 64\nEPOCH = 10\n\ntorch_dataset = Data.TensorDataset(torch.tensor(x_train, device='cuda'), torch.tensor(targets, device='cuda'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nloader = Data.DataLoader(\n    dataset=torch_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True)\n# start training\nl = []\nfor epoch in tqdm(range(EPOCH)):\n    total_loss = 0\n    for step, (b_x, b_y) in enumerate(loader): # for each training step\n        \n#         b_x = Variable(batch_x)\n#         b_y = Variable(batch_y)\n\n        prediction = net(b_x)     # input x and predict based on x\n\n        loss = loss_func(prediction.float(), b_y.float())     # must be (1. nn output, 2. target)\n        with torch.no_grad():\n            total_loss += loss.item() / len(loader)\n        optimizer.zero_grad()   # clear gradients for next train\n        loss.backward()         # backpropagation, compute gradients\n        optimizer.step()        # apply gradients\n    l.append(total_loss)\nplt.plot(l)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = net(torch.tensor(x_test, device='cuda'))\npred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's not great... Predicts almost a constant ! We'll have to improve that !","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ids = [i for i in ids if i in df_test.id.to_pandas().values]\nsub = pd.DataFrame({'id':test_ids, 'target':pred.detach().cpu()}).groupby('id').mean()\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=True)","metadata":{},"execution_count":null,"outputs":[]}]}