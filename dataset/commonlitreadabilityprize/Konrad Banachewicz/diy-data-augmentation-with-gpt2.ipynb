{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport pandas as pd\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\nimport numpy as np\nimport time\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG: \n    model_name = 'gpt2' # obviously, moving to a larger model helps -> e.g. \"gpt2-large\"\n    # for the sake speedy demonstration, keep one possible output - \n    # but an obvious extension is generating more and selecting the one most similar (in readability score) to the input\n    nof_outputs = 1 \n    seed = 34\n    MAX_LEN = 70\n    nof_rows = 10\n\ntf.random.set_seed(CFG.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"# quick score for comparison\ndef syllable_count(word):\n    count = 0\n    vowels = \"aeiouy\"\n    if word[0] in vowels:\n        count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            count += 1\n            if word.endswith(\"e\"):\n                count -= 1\n    if count == 0:\n        count += 1\n    return count\n\ndef fleisch(passage):\n    nof_char = len(passage)\n    nof_words = len(passage.split(' '))\n    nof_sent = passage.count('.')\n    nof_syl = syllable_count(passage)\n\n    fl = 206  - ((0.1 + nof_words) / (0.1 + nof_sent)) - 84 * (nof_syl / nof_words)\n    return fl\n\ndef generate_paragraph(xinput):\n    lix = len(xinput)\n\n    input_ids = tokenizer.encode(xinput, return_tensors='tf')\n    sample_outputs = GPT2.generate(input_ids, do_sample = True,  max_length = lix, \n                                   temperature = .85, \n                                   top_k = 50, \n                                   top_p = 0.85, \n    #                               num_return_sequences = CFG.nof_outputs\n                                  )\n    xoutput = tokenizer.decode(sample_outputs[0], skip_special_tokens = True)[lix:  2 * lix]\n\n\n    return xoutput","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(CFG.model_name)\nGPT2 = TFGPT2LMHeadModel.from_pretrained(CFG.model_name, pad_token_id=tokenizer.eos_token_id)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# load the training data\nxtrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv', encoding = 'utf8')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # process single paragraph by sentence - HORRIBLY SLOW ATM\n\n# xinput = xtrain['excerpt'][0]\n\n# xoutput = ''\n\n# nof_sentences = len(xinput.split('.'))\n\n# for ii in range(nof_sentences):\n\n#     input_sequence = xinput.split('.')[ii]\n\n#     len_inp = len(input_sequence)\n\n#     input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n\n#     len_inp = len(input_sequence)\n\n#     # only process non-empty sentences\n#     if len_inp:\n#         ## topk + top p\n#         sample_outputs = GPT2.generate(input_ids, do_sample = True,  max_length = 2*len_inp, temperature = .7, top_k = 50, top_p = 0.85, num_return_sequences = 3)\n\n#         print('input: ' + input_sequence)\n\n#         fin_score = 10 ** 10\n#         fin_output = ''\n\n#         for i, sample_output in enumerate(sample_outputs):\n#             outtext = tokenizer.decode(sample_output, skip_special_tokens = True)[(len_inp + 1): (2 * len_inp  + 1) ]\n#             dist = np.abs(fleisch(input_sequence) - fleisch(outtext))\n#             if dist < fin_score:\n#                 fin_score = dist\n#                 fin_output = outtext\n#             print('')\n\n#         print('output: ' + fin_output)\n#         xoutput += '. ' + fin_output\n#         print('---')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n# for testing purposes, run on a small subset\n\nxaug = xtrain.loc[0:CFG.nof_rows]\n\nxaug['new_xrc'] = xaug['excerpt'].apply(generate_paragraph)\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xaug.to_csv('train_augmented.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}