{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport string\n\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, Flatten\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom transformers import RobertaTokenizer, TFRobertaModel\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-16T21:16:02.224424Z","iopub.execute_input":"2021-07-16T21:16:02.225009Z","iopub.status.idle":"2021-07-16T21:16:10.366235Z","shell.execute_reply.started":"2021-07-16T21:16:02.224914Z","shell.execute_reply":"2021-07-16T21:16:10.365071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\nprint(df_train.shape)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:10.367484Z","iopub.execute_input":"2021-07-16T21:16:10.367748Z","iopub.status.idle":"2021-07-16T21:16:10.545148Z","shell.execute_reply.started":"2021-07-16T21:16:10.367721Z","shell.execute_reply":"2021-07-16T21:16:10.544018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nprint(df_test.shape)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:10.547394Z","iopub.execute_input":"2021-07-16T21:16:10.5478Z","iopub.status.idle":"2021-07-16T21:16:10.572341Z","shell.execute_reply.started":"2021-07-16T21:16:10.547756Z","shell.execute_reply":"2021-07-16T21:16:10.571239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_scatter_plot(x, y, x_label, y_label, plot_color='blue'):\n    fig=plt.figure()\n    ax=fig.add_axes([0,0,1,1])\n    ax.scatter(x, y, color=plot_color, alpha=0.3)\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n    ax.set_title(\"{} vs {}\".format(x_label, y_label))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:10.573849Z","iopub.execute_input":"2021-07-16T21:16:10.574154Z","iopub.status.idle":"2021-07-16T21:16:10.579839Z","shell.execute_reply.started":"2021-07-16T21:16:10.574124Z","shell.execute_reply":"2021-07-16T21:16:10.579033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = df_train['target'].to_numpy()\nstandard_err = df_train['standard_error'].to_numpy()\nshow_scatter_plot(target, standard_err, \"Target\", \"Standar Error\")","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:10.581222Z","iopub.execute_input":"2021-07-16T21:16:10.581524Z","iopub.status.idle":"2021-07-16T21:16:10.98263Z","shell.execute_reply.started":"2021-07-16T21:16:10.581495Z","shell.execute_reply":"2021-07-16T21:16:10.981566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Outlier based on scatter plot\noutlier_scatplot = df_train.loc[(df_train['standard_error'] < 0.4)]\noutlier_scatplot.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:10.983991Z","iopub.execute_input":"2021-07-16T21:16:10.984295Z","iopub.status.idle":"2021-07-16T21:16:11.014695Z","shell.execute_reply.started":"2021-07-16T21:16:10.984265Z","shell.execute_reply":"2021-07-16T21:16:11.01364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove outlier\ndf_train.drop(outlier_scatplot.index, inplace=True)\ndf_train.reset_index(drop=True, inplace=True)\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:11.015961Z","iopub.execute_input":"2021-07-16T21:16:11.016268Z","iopub.status.idle":"2021-07-16T21:16:11.023805Z","shell.execute_reply.started":"2021-07-16T21:16:11.016237Z","shell.execute_reply":"2021-07-16T21:16:11.022738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_MODEL = '../input/huggingface-roberta/roberta-base'","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:11.026142Z","iopub.execute_input":"2021-07-16T21:16:11.026459Z","iopub.status.idle":"2021-07-16T21:16:11.034322Z","shell.execute_reply.started":"2021-07-16T21:16:11.02643Z","shell.execute_reply":"2021-07-16T21:16:11.033243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_standardization(text):\n    text = text.lower() # if encoder is uncased\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.strip()\n    return text\n\ndef get_dataset(pandas_df, tokenizer, labeled=True, ordered=False, repeated=False, \n                batch_size=32, seq_len=128):\n    \"\"\"\n        Return a Tensorflow dataset ready for training or inference.\n    \"\"\"\n    text = [custom_standardization(text) for text in pandas_df['excerpt']]\n    \n    # Tokenize inputs\n    tokenized_inputs = tokenizer(text, max_length=seq_len, truncation=True, \n                                 padding='max_length', return_tensors='tf')\n    \n    if labeled:\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids']}, \n#                                                       'attention_mask': tokenized_inputs['attention_mask']}, \n                                                      (pandas_df['target'])))\n        \n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids']}) \n#                                                       'attention_mask': tokenized_inputs['attention_mask']})\n        \n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:11.036176Z","iopub.execute_input":"2021-07-16T21:16:11.036636Z","iopub.status.idle":"2021-07-16T21:16:11.049034Z","shell.execute_reply.started":"2021-07-16T21:16:11.036573Z","shell.execute_reply":"2021-07-16T21:16:11.047862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def base_model(encoder, seq_len=256):\n    input_ids = Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n#     input_attention_mask = Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    transformer = encoder({'input_ids': input_ids})\n#                       'attention_mask': input_attention_mask})\n    \n    lstm = LSTM(32, return_sequences=True, name=\"lstm_layer\")(transformer.last_hidden_state)\n    \n    dropout1 = Dropout(0.3, name=\"dropout_layer1\")(lstm)\n    \n    dense = Dense(16, name=\"dense_layer\")(dropout1)\n    \n    dropout2 = Dropout(0.5, name=\"dropout_layer2\")(dense)\n    \n    flatten = Flatten(name=\"flatten_layer\")(dropout2)\n    \n    output = Dense(1, activation=\"linear\", name=\"output_layer\")(flatten)\n    \n    model = Model(inputs=[input_ids], outputs=output)\n    \n#     model = Model(inputs=[input_ids, input_attention_mask], outputs=output)\n    \n#     model.summary()\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:11.050857Z","iopub.execute_input":"2021-07-16T21:16:11.051193Z","iopub.status.idle":"2021-07-16T21:16:11.067731Z","shell.execute_reply.started":"2021-07-16T21:16:11.05115Z","shell.execute_reply":"2021-07-16T21:16:11.066552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:11.068989Z","iopub.execute_input":"2021-07-16T21:16:11.069425Z","iopub.status.idle":"2021-07-16T21:16:16.641465Z","shell.execute_reply.started":"2021-07-16T21:16:11.06938Z","shell.execute_reply":"2021-07-16T21:16:16.639564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ntokenizer = RobertaTokenizer.from_pretrained(BASE_MODEL)\n\nSEQ_LEN = 256\nBATCH_SIZE = 8 * REPLICAS\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfold = 1\n\nbest_fold = []\n\nfor train_idx, val_idx in kf.split(df_train):\n\n    dataset_train = get_dataset(df_train.loc[train_idx], tokenizer, repeated=True, \n                                          batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n\n    dataset_val = get_dataset(df_train.loc[val_idx], tokenizer, ordered=True, \n                                          batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n\n    with strategy.scope():\n        encoder = TFRobertaModel.from_pretrained(BASE_MODEL)\n        model = base_model(encoder)\n        model.compile(optimizer = SGD(learning_rate=0.001),\n                      loss = 'mse', \n                      metrics = [tf.keras.metrics.RootMeanSquaredError()])\n        \n#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n    mc = ModelCheckpoint('model_fold{}.h5'.format(fold), monitor='val_loss',mode='min',\n                         save_weights_only=True, save_best_only=True,verbose=1)\n    \n    history = model.fit(\n                dataset_train,\n                validation_data=(dataset_val),\n                batch_size=BATCH_SIZE,\n                steps_per_epoch=len(df_train.loc[train_idx])//BATCH_SIZE,\n                epochs=50,\n                verbose=1,\n                callbacks=[mc]\n    )\n    \n    plt.figure()\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.plot(history.history['root_mean_squared_error'])\n    plt.plot(history.history['val_root_mean_squared_error'])\n    plt.ylabel('loss/error')\n    plt.xlabel('epochs')\n    plt.title(\"Training Loss and Error\")\n    plt.legend(['train_loss', 'val_loss', 'train_rmse', 'val_rmse'], loc='upper right')\n    plt.show()\n    \n    lowest_idx = np.argmin(history.history['val_loss'])\n    best_fold.append([fold, history.history['val_loss'][lowest_idx],\n                      history.history['loss'][lowest_idx]])\n    \n    fold += 1","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:16:16.643299Z","iopub.execute_input":"2021-07-16T21:16:16.643662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result = pd.DataFrame(\n    best_fold,\n    columns=['fold', 'val_loss', 'train_loss']).sort_values(by='val_loss', ascending=True)\ndf_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}