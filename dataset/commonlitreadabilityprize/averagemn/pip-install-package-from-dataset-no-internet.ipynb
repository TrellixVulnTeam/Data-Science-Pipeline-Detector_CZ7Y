{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing a Package from a Dataset when Internet is not allowed in a competition\n\nSometimes you have a specific Python package that is not included in Kaggle's standard notebook install but you would like to use it. Disabling internet makes the standard Pip install impossible as the Pypi repository cannot be accessed. \n\nThis notebook shows how to convert a package into a dataset that can be installed in an offline kernel. In this notebook I am using the [Keras Self-Attention](https://github.com/CyberZHG/keras-self-attention) package.\n\nIf the dataset requires downloading external data (e.g., Huggingface models), those need to be handled separately. Overall, this approach should work for pip packages though.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:18:08.783472Z","iopub.execute_input":"2021-06-06T11:18:08.783861Z","iopub.status.idle":"2021-06-06T11:18:09.884292Z","shell.execute_reply.started":"2021-06-06T11:18:08.783824Z","shell.execute_reply":"2021-06-06T11:18:09.883231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The files in the pre-created dataset\n\nAs I created the package dataset, it ended up for some reason having a directory structure starting with \"d\". It would be simple to fix by trimming the dataset structure, but it actually makes for a better example to illustrate all the issues like this. So why not.","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:14:46.52291Z","iopub.execute_input":"2021-06-06T09:14:46.523498Z","iopub.status.idle":"2021-06-06T09:14:46.528704Z","shell.execute_reply.started":"2021-06-06T09:14:46.523446Z","shell.execute_reply":"2021-06-06T09:14:46.527374Z"}}},{"cell_type":"code","source":"!ls ../input\n#the d below is where it is at for this dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:18:09.886054Z","iopub.execute_input":"2021-06-06T11:18:09.886455Z","iopub.status.idle":"2021-06-06T11:18:10.604094Z","shell.execute_reply.started":"2021-06-06T11:18:09.886413Z","shell.execute_reply":"2021-06-06T11:18:10.602965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../input/d/donkeys/kerasselfattention/mypip\n#and going a bit deeper, below is listing of the actual dataset contents. \n#the keras-self-attention-0.49.0 directory should actually be a tar.gz file but Kaggle seems to unzip it from the dataset\n#so have to convert it later back to tar.gz to install it","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:18:10.606129Z","iopub.execute_input":"2021-06-06T11:18:10.606454Z","iopub.status.idle":"2021-06-06T11:18:11.333307Z","shell.execute_reply.started":"2021-06-06T11:18:10.606419Z","shell.execute_reply":"2021-06-06T11:18:11.331962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First a general look at the data in this competition:","metadata":{}},{"cell_type":"code","source":"DATA_PATH = \"../input/commonlitreadabilityprize\"\n\ndf_t = pd.read_csv(f\"{DATA_PATH}/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:18:11.335061Z","iopub.execute_input":"2021-06-06T11:18:11.335367Z","iopub.status.idle":"2021-06-06T11:18:11.464832Z","shell.execute_reply.started":"2021-06-06T11:18:11.335334Z","shell.execute_reply":"2021-06-06T11:18:11.46398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_t.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:18:11.466421Z","iopub.execute_input":"2021-06-06T11:18:11.467155Z","iopub.status.idle":"2021-06-06T11:18:11.501115Z","shell.execute_reply.started":"2021-06-06T11:18:11.467105Z","shell.execute_reply":"2021-06-06T11:18:11.49993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This will be a traditional Keras LSTM model with word embeddings from Glove etc. so lets get those out of the way first:","metadata":{}},{"cell_type":"code","source":"def load_word_vectors(glove_dir):\n    print('Indexing word vectors.')\n\n    embeddings_index = {}\n    glove_path = os.path.join(glove_dir, 'glove.6B.300d.txt')\n    len(list(open(glove_path)))\n    with open(glove_path, encoding='utf8') as f:\n        for line in tqdm(f, total=400_000):\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n\n    print('Found %s word vectors.' % len(embeddings_index))\n    return embeddings_index\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:18:11.502959Z","iopub.execute_input":"2021-06-06T11:18:11.503413Z","iopub.status.idle":"2021-06-06T11:18:11.512274Z","shell.execute_reply.started":"2021-06-06T11:18:11.503365Z","shell.execute_reply":"2021-06-06T11:18:11.510889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef tokenize_text(vocab_size, texts, seq_length):\n    tokenizer = Tokenizer(num_words=vocab_size)\n    tokenizer.fit_on_texts(texts)\n    sequences = tokenizer.texts_to_sequences(texts)\n\n    word_index = tokenizer.word_index\n    print('Found %s unique tokens.' % len(word_index))\n\n    X = pad_sequences(sequences, maxlen=seq_length)\n    print('Shape of data tensor:', X.shape)\n\n    return X, tokenizer\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:18:11.514659Z","iopub.execute_input":"2021-06-06T11:18:11.515271Z","iopub.status.idle":"2021-06-06T11:18:18.160785Z","shell.execute_reply.started":"2021-06-06T11:18:11.51522Z","shell.execute_reply":"2021-06-06T11:18:18.159653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def embedding_index_to_matrix(embeddings_index, vocab_size, embedding_dim, word_index):\n    global num_words\n    print('Preparing embedding matrix.')\n\n    # prepare embedding matrix\n    #+1 because have to match num_words to zero-index embedding matrix, as the tokenizer word-index starts at 1\n    num_words = min(vocab_size, len(word_index)+1)\n    embedding_matrix = np.zeros((num_words, embedding_dim))\n    for word, i in word_index.items():\n        if i >= vocab_size:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:18:18.162656Z","iopub.execute_input":"2021-06-06T11:18:18.163074Z","iopub.status.idle":"2021-06-06T11:18:18.169937Z","shell.execute_reply.started":"2021-06-06T11:18:18.163036Z","shell.execute_reply":"2021-06-06T11:18:18.168601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_callbacks(name):\n    checkpoint_callback = ModelCheckpoint(filepath=f\"./model-weights-\" + name + \".hdf5\",\n                                          monitor='val_loss', verbose=1, save_best_only=True)\n    return [checkpoint_callback]","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:18:18.171777Z","iopub.execute_input":"2021-06-06T11:18:18.172372Z","iopub.status.idle":"2021-06-06T11:18:18.183025Z","shell.execute_reply.started":"2021-06-06T11:18:18.172325Z","shell.execute_reply":"2021-06-06T11:18:18.181864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove_dir = \"../input/glove6b\"\nembeddings_index = load_word_vectors(glove_dir)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:18:18.185882Z","iopub.execute_input":"2021-06-06T11:18:18.186293Z","iopub.status.idle":"2021-06-06T11:19:33.257439Z","shell.execute_reply.started":"2021-06-06T11:18:18.186206Z","shell.execute_reply":"2021-06-06T11:19:33.2437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = df_t[\"excerpt\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:33.260014Z","iopub.execute_input":"2021-06-06T11:19:33.260347Z","iopub.status.idle":"2021-06-06T11:19:33.279593Z","shell.execute_reply.started":"2021-06-06T11:19:33.260319Z","shell.execute_reply":"2021-06-06T11:19:33.274772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 30000\nseq_length = 256 #seems there is a max of about 230 tokens in an excerpt in the training data, so I just use 256\nX, tokenizer = tokenize_text(vocab_size, features.astype(str), seq_length)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:33.281627Z","iopub.execute_input":"2021-06-06T11:19:33.282145Z","iopub.status.idle":"2021-06-06T11:19:34.359402Z","shell.execute_reply.started":"2021-06-06T11:19:33.282094Z","shell.execute_reply":"2021-06-06T11:19:34.358215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 300\nembedding_matrix = embedding_index_to_matrix(embeddings_index=embeddings_index,\n                                                 vocab_size=vocab_size,\n                                                 embedding_dim=embedding_dim,\n                                                 word_index=tokenizer.word_index)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:34.360974Z","iopub.execute_input":"2021-06-06T11:19:34.361431Z","iopub.status.idle":"2021-06-06T11:19:34.442553Z","shell.execute_reply.started":"2021-06-06T11:19:34.36135Z","shell.execute_reply":"2021-06-06T11:19:34.441357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = df_t[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:34.443788Z","iopub.execute_input":"2021-06-06T11:19:34.444077Z","iopub.status.idle":"2021-06-06T11:19:34.448046Z","shell.execute_reply.started":"2021-06-06T11:19:34.44405Z","shell.execute_reply":"2021-06-06T11:19:34.447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, targets, test_size=.2, random_state=6969)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:34.449391Z","iopub.execute_input":"2021-06-06T11:19:34.449854Z","iopub.status.idle":"2021-06-06T11:19:34.465339Z","shell.execute_reply.started":"2021-06-06T11:19:34.449817Z","shell.execute_reply":"2021-06-06T11:19:34.464057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_training_history(history):\n    # list all data in history\n    print(history.history.keys())\n    # summarize history for accuracy\n    plt.plot(history.history['MSE'])\n    plt.plot(history.history['val_MSE'])\n    plt.title('model MSE')\n    plt.ylabel('MSE')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:34.466968Z","iopub.execute_input":"2021-06-06T11:19:34.467373Z","iopub.status.idle":"2021-06-06T11:19:34.476214Z","shell.execute_reply.started":"2021-06-06T11:19:34.467292Z","shell.execute_reply":"2021-06-06T11:19:34.475028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Installing the external package from dataset\n\nAs I noted before, the keras-self-attention-0.49.0 directory from the dataset is unzipped by Kaggle, so I will restore it to tar.gz to be able to do pip install:","metadata":{"execution":{"iopub.status.busy":"2021-06-06T08:55:05.234629Z","iopub.execute_input":"2021-06-06T08:55:05.235114Z","iopub.status.idle":"2021-06-06T08:55:05.239927Z","shell.execute_reply.started":"2021-06-06T08:55:05.23507Z","shell.execute_reply":"2021-06-06T08:55:05.238607Z"}}},{"cell_type":"code","source":"!ls /kaggle/input/d/donkeys/kerasselfattention/mypip\n!pwd","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:34.477557Z","iopub.execute_input":"2021-06-06T11:19:34.477864Z","iopub.status.idle":"2021-06-06T11:19:36.013033Z","shell.execute_reply.started":"2021-06-06T11:19:34.477836Z","shell.execute_reply":"2021-06-06T11:19:36.011649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kaggle seems to have created this strange nested directory with the file path containing the directory multiple times. It does not matter as I am copying its contents elsewhere, but was a bit confusing.\nThe contents of this directory are what I need for the tar.gz file:","metadata":{}},{"cell_type":"code","source":"!ls -l /kaggle/input/d/donkeys/kerasselfattention/mypip/keras-self-attention-0.49.0/keras-self-attention-0.49.0","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:36.017786Z","iopub.execute_input":"2021-06-06T11:19:36.018392Z","iopub.status.idle":"2021-06-06T11:19:36.802658Z","shell.execute_reply.started":"2021-06-06T11:19:36.01834Z","shell.execute_reply":"2021-06-06T11:19:36.801452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#on Kaggle, the /kaggle/working directory (where we usually are located) is generally writable\n#however, the dataset directories cannot be written to, so have to copy them to workspace to create the tar.gz for the installable version\n!rm -r mypip\n!mkdir mypip\n#this create the tar.gz file under the /kaggle/working/mypip directory\n!tar -czvf mypip/keras-self-attention-0.49.0.tar.gz -C /kaggle/input/d/donkeys/kerasselfattention/mypip/keras-self-attention-0.49.0/keras-self-attention-0.49.0 .","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:36.80457Z","iopub.execute_input":"2021-06-06T11:19:36.804888Z","iopub.status.idle":"2021-06-06T11:19:41.723076Z","shell.execute_reply.started":"2021-06-06T11:19:36.804854Z","shell.execute_reply":"2021-06-06T11:19:41.721616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -l mypip\n#there is the tar.gz just created","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:41.727412Z","iopub.execute_input":"2021-06-06T11:19:41.727841Z","iopub.status.idle":"2021-06-06T11:19:42.471802Z","shell.execute_reply.started":"2021-06-06T11:19:41.727801Z","shell.execute_reply":"2021-06-06T11:19:42.470564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar tvf mypip/keras-self-attention-0.49.0.tar.gz\n#and it has the contents we need","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:42.47577Z","iopub.execute_input":"2021-06-06T11:19:42.476119Z","iopub.status.idle":"2021-06-06T11:19:43.228943Z","shell.execute_reply.started":"2021-06-06T11:19:42.476085Z","shell.execute_reply":"2021-06-06T11:19:43.227762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remember, the downloaded package dataset also contains a number of .whl files for installing all the dependencies of our external package.\nNeed to copy those as well so that the pip install command does not need to check the internet for them either.","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/d/donkeys/kerasselfattention/mypip\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:43.233027Z","iopub.execute_input":"2021-06-06T11:19:43.233404Z","iopub.status.idle":"2021-06-06T11:19:43.982631Z","shell.execute_reply.started":"2021-06-06T11:19:43.233364Z","shell.execute_reply":"2021-06-06T11:19:43.981655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /kaggle/input/d/donkeys/kerasselfattention/mypip/*.whl mypip","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:43.983941Z","iopub.execute_input":"2021-06-06T11:19:43.984256Z","iopub.status.idle":"2021-06-06T11:19:46.973013Z","shell.execute_reply.started":"2021-06-06T11:19:43.984222Z","shell.execute_reply":"2021-06-06T11:19:46.971829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls mypip","metadata":{"execution":{"iopub.status.busy":"2021-06-06T11:19:46.974551Z","iopub.execute_input":"2021-06-06T11:19:46.975086Z","iopub.status.idle":"2021-06-06T11:19:47.725752Z","shell.execute_reply.started":"2021-06-06T11:19:46.975037Z","shell.execute_reply":"2021-06-06T11:19:47.724402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, this is the actual command to install the package from the mypip directory that was just created above:","metadata":{}},{"cell_type":"code","source":"!pip install --no-index --find-links file:///kaggle/working/mypip keras-self-attention","metadata":{"execution":{"iopub.status.busy":"2021-06-06T07:47:27.286721Z","iopub.execute_input":"2021-06-06T07:47:27.28717Z","iopub.status.idle":"2021-06-06T07:47:33.808309Z","shell.execute_reply.started":"2021-06-06T07:47:27.287018Z","shell.execute_reply":"2021-06-06T07:47:33.807065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using the installed package\n\nIt is just the same as any python package once the install is success:","metadata":{}},{"cell_type":"code","source":"from keras_self_attention import SeqSelfAttention, SeqWeightedAttention","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:01:50.991465Z","iopub.execute_input":"2021-06-06T09:01:50.991949Z","iopub.status.idle":"2021-06-06T09:01:50.998272Z","shell.execute_reply.started":"2021-06-06T09:01:50.991907Z","shell.execute_reply":"2021-06-06T09:01:50.996856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Dense, Input, GlobalMaxPooling1D, Bidirectional\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\nfrom keras.models import Model\nfrom keras.callbacks import TensorBoard, ModelCheckpoint\nfrom keras.layers import LSTM#, CuDNNLSTM \n\ndef build_model_lstm_attention(vocab_size, embedding_dim, embedding_matrix, sequence_length, embeddings_trainable):\n    input = Input(shape=(sequence_length,), name=\"Input\")\n    if embedding_matrix is None:\n        embedding = Embedding(input_dim=num_words, \n                              output_dim=embedding_dim, \n                              input_length=sequence_length,\n                              trainable=embeddings_trainable,\n                              name=\"embedding\")(input)\n    else:\n        embedding = Embedding(input_dim=num_words, \n                              weights=[embedding_matrix],\n                              output_dim=embedding_dim, \n                              input_length=sequence_length,\n                              trainable=embeddings_trainable,\n                              name=\"embedding\")(input)\n    lstm1_bi1 = Bidirectional(LSTM(128, return_sequences=True, name='lstm1'), name=\"lstm-bi1\")(embedding)\n    attention1 = SeqSelfAttention(attention_width=attention_width)(lstm1_bi1)\n    lstm2_bi2 = Bidirectional(LSTM(64, return_sequences=True, name='lstm2'), name=\"lstm-bi2\")(attention1)\n    attention2 = SeqWeightedAttention()(lstm2_bi2)\n\n    dense64 = Dense(64, activation = 'relu')(attention2)\n    dense32 = Dense(32, activation = 'relu')(dense64)\n    output = Dense(1, activation = 'linear')(dense32)\n\n    model = Model(inputs=input, outputs=output)\n    model.compile(optimizer='adam', loss='MSE', metrics=['MSE'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:01:56.476019Z","iopub.execute_input":"2021-06-06T09:01:56.476488Z","iopub.status.idle":"2021-06-06T09:01:56.492601Z","shell.execute_reply.started":"2021-06-06T09:01:56.476453Z","shell.execute_reply":"2021-06-06T09:01:56.491424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attention_width = None\nbatch_size = 32\nepochs = 4\n\ndef train_model_attention(seed, embedding_dim, embedding_matrix, X_train, y_train, X_val, y_val, embeddings_trainable):\n    callbacks = create_callbacks(f\"{seed}\")\n\n    model = build_model_lstm_attention(vocab_size=vocab_size,\n                        embedding_dim=embedding_dim,\n                        sequence_length=seq_length,\n                        embedding_matrix=embedding_matrix,\n                        embeddings_trainable=embeddings_trainable) \n\n    history = model.fit(X_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(X_val, y_val),\n              callbacks=callbacks)\n\n    #the callback should have saved the model at best epoch, so just use it\n    print(f\"loading model weights: ./model-weights-{seed}.hdf5\")\n    model.load_weights(f\"./model-weights-{seed}.hdf5\")\n\n    return history, model\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:04:26.815988Z","iopub.execute_input":"2021-06-06T09:04:26.816581Z","iopub.status.idle":"2021-06-06T09:04:26.827639Z","shell.execute_reply.started":"2021-06-06T09:04:26.816533Z","shell.execute_reply":"2021-06-06T09:04:26.826261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history, model = train_model_attention(6969, embedding_dim, embedding_matrix, X_train, y_train, X_test, y_test, True)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:04:33.871Z","iopub.execute_input":"2021-06-06T09:04:33.871502Z","iopub.status.idle":"2021-06-06T09:12:46.005854Z","shell.execute_reply.started":"2021-06-06T09:04:33.871463Z","shell.execute_reply":"2021-06-06T09:12:46.004929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training_history(history)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T09:12:46.00755Z","iopub.execute_input":"2021-06-06T09:12:46.008059Z","iopub.status.idle":"2021-06-06T09:12:46.381687Z","shell.execute_reply.started":"2021-06-06T09:12:46.007997Z","shell.execute_reply":"2021-06-06T09:12:46.380517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n#x_test = X_test\ntest_features = test_df[\"excerpt\"]\nsequences = tokenizer.texts_to_sequences(test_features.astype(str))\nx_test = pad_sequences(sequences, maxlen=seq_length)\nresults = model.predict(x_test)\nids = test_df['id']\nresults = pd.Series(np.squeeze(results, 1))\nsubmission = pd.concat([ids, results], axis = 1)\nsubmission.rename({0:'target'}, axis = 1, inplace = True)\nsubmission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T08:06:28.526558Z","iopub.execute_input":"2021-06-06T08:06:28.52721Z","iopub.status.idle":"2021-06-06T08:06:30.253714Z","shell.execute_reply.started":"2021-06-06T08:06:28.52717Z","shell.execute_reply":"2021-06-06T08:06:30.252671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T08:06:30.255141Z","iopub.execute_input":"2021-06-06T08:06:30.255436Z","iopub.status.idle":"2021-06-06T08:06:30.267596Z","shell.execute_reply.started":"2021-06-06T08:06:30.255409Z","shell.execute_reply":"2021-06-06T08:06:30.266891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the zip file for the dataset","metadata":{}},{"cell_type":"markdown","source":"The following commands will download the package and its dependencies, and zip them. The resulting zip file should be downloadable from the kernel output, and can then be added to a dataset. \n\n","metadata":{}},{"cell_type":"code","source":"!pip download -d mydataset keras-self-attention\n!zip -r mydataset.zip mydataset\n","metadata":{},"execution_count":null,"outputs":[]}]}