{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook uses below given notebooks to make predictions.\n\n1. Pretrain Roberta Model: https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n2. Finetune Roberta Model: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune\n3. Inference Notebook: https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference\n4. Roberta + SVM: this notebook","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification)\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:15:26.634327Z","iopub.execute_input":"2021-07-25T15:15:26.635304Z","iopub.status.idle":"2021-07-25T15:15:34.182842Z","shell.execute_reply.started":"2021-07-25T15:15:26.635039Z","shell.execute_reply":"2021-07-25T15:15:34.18185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\n#対数分だけnum_binsに分割\n#ビニング処理,データをbin数でグループ分けする\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()\n\n#平均二条誤差\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:15:34.184628Z","iopub.execute_input":"2021-07-25T15:15:34.184981Z","iopub.status.idle":"2021-07-25T15:15:34.307931Z","shell.execute_reply.started":"2021-07-25T15:15:34.184938Z","shell.execute_reply":"2021-07-25T15:15:34.307105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'batch_size':128,\n    'max_len':256,\n    'nfolds':10,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:15:34.309522Z","iopub.execute_input":"2021-07-25T15:15:34.30985Z","iopub.status.idle":"2021-07-25T15:15:34.319638Z","shell.execute_reply.started":"2021-07-25T15:15:34.309819Z","shell.execute_reply":"2021-07-25T15:15:34.318807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:15:34.321153Z","iopub.execute_input":"2021-07-25T15:15:34.321511Z","iopub.status.idle":"2021-07-25T15:15:34.329426Z","shell.execute_reply.started":"2021-07-25T15:15:34.321475Z","shell.execute_reply":"2021-07-25T15:15:34.328578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:15:34.330632Z","iopub.execute_input":"2021-07-25T15:15:34.330982Z","iopub.status.idle":"2021-07-25T15:15:34.342017Z","shell.execute_reply.started":"2021-07-25T15:15:34.330945Z","shell.execute_reply":"2021-07-25T15:15:34.341203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained('../input/clrp-pytorch-roberta-pretrain-roberta-large/clrp_roberta_large/')    \n        #changed attentionHead Dimension from 768 to 1024 by changing model from roberta-base to roberta-large\n        self.head = AttentionHead(1024,1024,1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.head.out_features,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:18:47.173769Z","iopub.execute_input":"2021-07-25T15:18:47.174107Z","iopub.status.idle":"2021-07-25T15:18:47.182358Z","shell.execute_reply.started":"2021-07-25T15:18:47.174077Z","shell.execute_reply":"2021-07-25T15:18:47.181582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ここが新規\ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n    #cuda使えたら使う構文\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained('../input/clrp-pytorch-roberta-pretrain-roberta-large/clrp_roberta_large/')\n    \n    ds = CLRPDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    #以下でpredictionsを抽出するために使った構文を使ってembeddingsをreturnしている.\n    #SVMの手法とは、embeddingsの意味は？\n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:18:57.213089Z","iopub.execute_input":"2021-07-25T15:18:57.213428Z","iopub.status.idle":"2021-07-25T15:18:57.222552Z","shell.execute_reply.started":"2021-07-25T15:18:57.213399Z","shell.execute_reply":"2021-07-25T15:18:57.221236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train/testでembeddingsを取得している\ntrain_embeddings1 =  get_embeddings(train_data,'../input/clrp-pytorch-roberta-finetune-roberta-large/model0/model0.bin')\ntest_embeddings1 = get_embeddings(test_data,'../input/clrp-pytorch-roberta-finetune-roberta-large/model0/model0.bin')\n\ntrain_embeddings2 =  get_embeddings(train_data,'../input/clrp-pytorch-roberta-finetune-roberta-large/model1/model1.bin')\ntest_embeddings2 = get_embeddings(test_data,'../input/clrp-pytorch-roberta-finetune-roberta-large/model1/model1.bin')\n\ntrain_embeddings3 =  get_embeddings(train_data,'../input/clrp-pytorch-roberta-finetune-roberta-large/model2/model2.bin')\ntest_embeddings3 = get_embeddings(test_data,'../input/clrp-pytorch-roberta-finetune-roberta-large/model2/model2.bin')\n\ntrain_embeddings4 =  get_embeddings(train_data,'../input/clrp-pytorch-roberta-finetune-roberta-large/model3/model3.bin')\ntest_embeddings4 = get_embeddings(test_data,'../input/clrp-pytorch-roberta-finetune-roberta-large/model3/model3.bin')\n\ntrain_embeddings5 =  get_embeddings(train_data,'../input/clrp-pytorch-roberta-finetune-roberta-large/model4/model4.bin')\ntest_embeddings5 = get_embeddings(test_data,'../input/clrp-pytorch-roberta-finetune-roberta-large/model4/model4.bin')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:19:00.979183Z","iopub.execute_input":"2021-07-25T15:19:00.979517Z","iopub.status.idle":"2021-07-25T15:20:33.942693Z","shell.execute_reply.started":"2021-07-25T15:19:00.979485Z","shell.execute_reply":"2021-07-25T15:20:33.940535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## svm","metadata":{}},{"cell_type":"code","source":"#SVMをアンサンブル処理している\ndef get_preds_svm(X,y,X_test,bins=bins,nfolds=10,C=10,kernel='rbf'):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    \n    kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        model = SVR(C=C,kernel=kernel,gamma='auto')\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)/nfolds","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:16:11.541032Z","iopub.status.idle":"2021-07-25T15:16:11.541401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\nsvm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\nsvm_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3)\nsvm_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\nsvm_preds5 = get_preds_svm(train_embeddings5,target,test_embeddings5)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:16:11.542513Z","iopub.status.idle":"2021-07-25T15:16:11.543047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5)/5","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:16:11.54405Z","iopub.status.idle":"2021-07-25T15:16:11.544824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.target = svm_preds\nsample.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:16:11.545912Z","iopub.status.idle":"2021-07-25T15:16:11.546524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:16:11.547797Z","iopub.status.idle":"2021-07-25T15:16:11.548481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}