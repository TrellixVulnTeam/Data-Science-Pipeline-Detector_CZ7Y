{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This note book is Modified for Haruki's learning. \n# by Version2 Model is changed from roberta-base to distilroberta-base\n# By disabling CPU to finish distilroberta-base.\n\nThis notebooks shows how to pretrain any language model easily\n\n\n1. Pretrain Roberta Model: this notebook\n2. Finetune Roberta Model: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune<br/>\n   Finetune Roberta Model [TPU]: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune-tpu\n3. Inference Notebook: https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference\n4. Roberta + SVM: https://www.kaggle.com/maunish/clrp-roberta-svm\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom transformers import (AutoModel,AutoModelForMaskedLM, \n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-24T14:13:29.730445Z","iopub.execute_input":"2021-07-24T14:13:29.730905Z","iopub.status.idle":"2021-07-24T14:13:37.690307Z","shell.execute_reply.started":"2021-07-24T14:13:29.730819Z","shell.execute_reply":"2021-07-24T14:13:37.68944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"「Transformer」は、過去に自然言語処理分野で多く使われていた「RNN」（Recurrent Neural Network）や「CNN」（Convolutional Neural Network）を「Self-Attention Layer」に入れ替えたモデルで、発表以来、多くの自然言語処理モデルが「Transformer」で再構築され、過去を上回る結果を上げました。","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\ndata = pd.concat([train_data,test_data])\n\n#data['excerpt']に対して改行をなくす。\ndata['excerpt'] = data['excerpt'].apply(lambda x: x.replace('\\n',''))\n\n#改行をなくしたexcerptを改行で全結合、textに保存.\ntext  = '\\n'.join(data.excerpt.tolist())\n\nwith open('text.txt','w') as f:\n    f.write(text)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:13:37.691744Z","iopub.execute_input":"2021-07-24T14:13:37.692098Z","iopub.status.idle":"2021-07-24T14:13:37.785559Z","shell.execute_reply.started":"2021-07-24T14:13:37.692064Z","shell.execute_reply":"2021-07-24T14:13:37.784599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pretrainモデルであるroberta-baseをmodelとして指定.\nmodel_name = 'roberta-large'\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\n\n#Tokenizer\n#Tokenizerは保存しておく(なんのために？)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained('./clrp_roberta_large');","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:13:37.787434Z","iopub.execute_input":"2021-07-24T14:13:37.787828Z","iopub.status.idle":"2021-07-24T14:15:01.734632Z","shell.execute_reply.started":"2021-07-24T14:13:37.787791Z","shell.execute_reply":"2021-07-24T14:15:01.733688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#この段階ではtrain/validともに同じ全結合されたデータが元になったdataset状態。\n#tokenizerを指定して、file_pathの文字列を区切ったものになっている？\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\", #mention train text file here\n    block_size=256)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\", #mention valid text file here\n    block_size=256)\n\n#data_collator \n#Data collators are objects that will form a batch by using a list of \n#dataset elements as input. \n#These elements are of the same type as the elements of train_dataset or eval_dataset.\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\n#training_argsはtrainerで指定するハイパーパラメータのことで、TrainingArguments関数で一式を必要なフォーマットにて与える\ntraining_args = TrainingArguments(\n    output_dir=\"./clrp_roberta_large_chk\", #select model path for checkpoint\n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy= 'steps',\n    save_total_limit=2,\n    eval_steps=200,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end =True,\n    prediction_loss_only=True,\n    report_to = \"none\")\n\n#プリトレイニングのクラスを作成。\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:19:02.446075Z","iopub.execute_input":"2021-07-24T14:19:02.446435Z","iopub.status.idle":"2021-07-24T14:19:09.000436Z","shell.execute_reply.started":"2021-07-24T14:19:02.446402Z","shell.execute_reply":"2021-07-24T14:19:08.999147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:19:14.144458Z","iopub.execute_input":"2021-07-24T14:19:14.144765Z","iopub.status.idle":"2021-07-24T14:19:14.151624Z","shell.execute_reply.started":"2021-07-24T14:19:14.144738Z","shell.execute_reply":"2021-07-24T14:19:14.15078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#今回のお題になっているexcerptをroberta_baseモデルにプレ学習させて、\n#clrp_robertaモデルとして保存している.\n#以下の１行を動作させようとするとメモリが15GBit超えてSTOPする.\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:19:26.904717Z","iopub.execute_input":"2021-07-24T14:19:26.905034Z","iopub.status.idle":"2021-07-24T15:16:21.01304Z","shell.execute_reply.started":"2021-07-24T14:19:26.905004Z","shell.execute_reply":"2021-07-24T15:16:21.012045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrainer.save_model(f'./clrp_roberta_large')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:16:33.277349Z","iopub.execute_input":"2021-07-24T15:16:33.277825Z","iopub.status.idle":"2021-07-24T15:16:37.86481Z","shell.execute_reply.started":"2021-07-24T15:16:33.277785Z","shell.execute_reply":"2021-07-24T15:16:37.863512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}