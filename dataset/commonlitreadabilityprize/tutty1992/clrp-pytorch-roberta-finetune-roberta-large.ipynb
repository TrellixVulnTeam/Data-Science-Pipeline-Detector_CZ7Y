{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This note book is Modified for Haruki's learning. \nThis notebook uses the model created in pretrain any model notebook.\n\n1. Pretrain Roberta Model: https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n2. Finetune Roberta Model: this notebook, <br/>\n   Finetune Roberta Model TPU: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune-tpu\n3. Inference Notebook: https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference\n4. Roberta + SVM: https://www.kaggle.com/maunish/clrp-roberta-svm","metadata":{}},{"cell_type":"code","source":"!pip install accelerate","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-25T06:12:57.546015Z","iopub.execute_input":"2021-07-25T06:12:57.546327Z","iopub.status.idle":"2021-07-25T06:13:05.41685Z","shell.execute_reply.started":"2021-07-25T06:12:57.546257Z","shell.execute_reply":"2021-07-25T06:13:05.415862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n#garbage collectionメモリの開放\nimport gc\n\nimport sys\nimport math\nimport time\n\n#pythonでシークバーを表示する\nimport tqdm\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\n\n#Stratified KFold -> 層化抽出K分割公差検証\n#目的変数の偏りが保持されるようにK分割を実施する\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom accelerate import Accelerator\nfrom transformers import (AutoModel,AutoConfig,\n                          AutoTokenizer,get_cosine_schedule_with_warmup)\n\n#色関係?EDA用?\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T06:13:05.418553Z","iopub.execute_input":"2021-07-25T06:13:05.418886Z","iopub.status.idle":"2021-07-25T06:13:12.808453Z","shell.execute_reply.started":"2021-07-25T06:13:05.418854Z","shell.execute_reply":"2021-07-25T06:13:12.807527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ここから再開\ntrain_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\n#print(train_data.head())\n#exerptの改行情報を削除\ntrain_data['excerpt'] = train_data['excerpt'].apply(lambda x: x.replace('\\n',''))\n\n#np.floorは少数点以下を四捨五入する関数. train dataのレコード数の対数+1を整数化\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\n\n#binsで指定した数の群にtargetによって分割する\n#labels=Falseでは出力の値域は非表示となり, 0始まりの群のidのみがoutputされる\n#loc methodは[行,列]要素の順番で指定してdataframeの要素を抽出する.\n#以下の例だと、すべての行のbins列に対して、群番号を(新規列)binsに格納している\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\n#bins/targetを夫々numpyクラスに変換\nbins = train_data.bins.to_numpy()\ntarget = train_data.target.to_numpy()\n\n#print(\"bins\", type(bins))\n#print(\"target\", target)\n\n#二乗和平均誤差関数を定義\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T06:13:12.816249Z","iopub.execute_input":"2021-07-25T06:13:12.816617Z","iopub.status.idle":"2021-07-25T06:13:12.920414Z","shell.execute_reply.started":"2021-07-25T06:13:12.816578Z","shell.execute_reply":"2021-07-25T06:13:12.919484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.head())","metadata":{"execution":{"iopub.status.busy":"2021-07-25T06:13:12.921764Z","iopub.execute_input":"2021-07-25T06:13:12.922123Z","iopub.status.idle":"2021-07-25T06:13:12.934119Z","shell.execute_reply.started":"2021-07-25T06:13:12.922085Z","shell.execute_reply":"2021-07-25T06:13:12.932929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#HP設定, HPの各詳細もあとで追いかける\nconfig = {\n    'lr': 2e-5,\n    'wd':0.01,\n    'batch_size':8,\n    'valid_step':10,\n    'max_len':256,\n    'epochs':3,\n    'nfolds':5,\n    'seed':42,\n    'model_path':'../input/clrp-pytorch-roberta-pretrain-roberta-large/clrp_roberta_large',\n}\n\n#nfolds分の格納用dirを作成\nfor i in range(config['nfolds']):\n    os.makedirs(f'model{i}',exist_ok=True)\n\n#seed設定の関数\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n#seedを42に設定\nseed_everything(seed=config['seed'])\n\n#FOLD列を追加して仮に-1を一律格納\ntrain_data['Fold'] = -1\n#層化抽出khold, シャッフル有効, seed固定, \n#kfoldクラスを使ったid抽出は以下が分かりやすい.\n#https://blog.amedama.jp/entry/2018/06/21/235951\n#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold.split\n#Stratified Kfoldにおけるsplitにおいてyは層化抽出に必須のパラメタ, binsを層化抽出する\nkfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n    train_data.loc[valid_idx,'Fold'] = k","metadata":{"execution":{"iopub.status.busy":"2021-07-25T06:13:12.936128Z","iopub.execute_input":"2021-07-25T06:13:12.936591Z","iopub.status.idle":"2021-07-25T06:13:12.965576Z","shell.execute_reply.started":"2021-07-25T06:13:12.936541Z","shell.execute_reply":"2021-07-25T06:13:12.964534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T06:13:12.968295Z","iopub.execute_input":"2021-07-25T06:13:12.968671Z","iopub.status.idle":"2021-07-25T06:13:12.993323Z","shell.execute_reply.started":"2021-07-25T06:13:12.968632Z","shell.execute_reply":"2021-07-25T06:13:12.992416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(dpi=100)\n#seabornはこの形だけでプロットできるのすごい...\n#binsで区切った適正年齢群は、ほぼシグマプロットの様な形状をしている.\nsns.countplot(train_data.bins);","metadata":{"execution":{"iopub.status.busy":"2021-07-25T06:13:12.994892Z","iopub.execute_input":"2021-07-25T06:13:12.995344Z","iopub.status.idle":"2021-07-25T06:13:13.202368Z","shell.execute_reply.started":"2021-07-25T06:13:12.995297Z","shell.execute_reply":"2021-07-25T06:13:13.20128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataset Class定義、初期値としてexcerptとtargets、max_len, tokenizerを持つ。\n#Class定義におけるダブルアンダーバーに特別な意味があったか.\n\nclass CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n    #get itemでは、tokenizerに割り当てられる番号とtarget値が出てくる？\n    #getitemについてhttps://qiita.com/gyu-don/items/bde192b129a7b1b8c532\n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        target = torch.tensor(self.targets[idx],dtype=torch.float) \n        return encode, target\n    \n    #excerptの長さ\n    def __len__(self):\n        return len(self.excerpt)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T06:13:13.204031Z","iopub.execute_input":"2021-07-25T06:13:13.204453Z","iopub.status.idle":"2021-07-25T06:13:13.21281Z","shell.execute_reply.started":"2021-07-25T06:13:13.204414Z","shell.execute_reply":"2021-07-25T06:13:13.211613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AttentionHeadクラスの定義\nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n    #forwardではattentionを計算して、context_vectorを返す\n    def forward(self, features):\n        #W(argument)でargumentに対してnn.linearを実施, hidden_dimに変換して、tanh変換\n        att = torch.tanh(self.W(features))\n        #入力=>隠れ層へ全結合=>tanh変換をscoreに線形変換\n        score = self.V(att)\n        #attention重みをsoftmax関数で作成\n        attention_weights = torch.softmax(score, dim=1)\n        #context_vectorはvalueにattentionをつけたもの\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","metadata":{"execution":{"iopub.status.busy":"2021-07-25T06:13:13.214481Z","iopub.execute_input":"2021-07-25T06:13:13.214932Z","iopub.status.idle":"2021-07-25T06:13:13.227251Z","shell.execute_reply.started":"2021-07-25T06:13:13.214862Z","shell.execute_reply":"2021-07-25T06:13:13.226071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    #Model本体は一つ前のnotebookで作成したmodelをpathで参照する形式\n    def __init__(self,path):\n        super(Model,self).__init__()\n        #robertaモデルを読み込み\n        self.roberta = AutoModel.from_pretrained(path)\n        #configを読み込み？\n        self.config = AutoConfig.from_pretrained(path)\n        #attentionHeadを読み込み\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        #Dropoutを0.1に設定\n        self.dropout = nn.Dropout(0.1)\n        #1次元への全結合層?\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self,**xb):\n        #xbをモデルに入れる**が引数にあるのは任意の数の引数を指定できる意味\n        #input xbをrobertaモデルに解釈させて、headを取って、dropoutさせて、全結合してxを返す\n        #forwardはattention自体を返す=予測値？\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-07-25T06:13:13.22901Z","iopub.execute_input":"2021-07-25T06:13:13.229498Z","iopub.status.idle":"2021-07-25T06:13:13.239413Z","shell.execute_reply.started":"2021-07-25T06:13:13.229459Z","shell.execute_reply":"2021-07-25T06:13:13.238072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fold数を指定して、runする？\ndef run(fold,verbose=True):\n    #loss関数の設定\n    def loss_fn(outputs,targets):\n        outputs = outputs.view(-1)\n        targets = targets.view(-1)\n        return torch.sqrt(nn.MSELoss()(outputs,targets))\n    \n    #そのまま？\n    def train_and_evaluate_loop(train_loader,valid_loader,model,loss_fn,optimizer,epoch,fold,best_loss,valid_step=10,lr_scheduler=None):\n        train_loss = 0\n        for i, (inputs1,targets1) in enumerate(train_loader):\n            #modelをtrainする宣言\n            model.train()\n            #gradienｔの初期化\n            optimizer.zero_grad()\n            #inputs1のkey valを変形してinputs1に再代入\n            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n            #outputはmodelで処理したinputs1\n            outputs1 = model(**inputs1)\n            #loss1は誤差関数\n            loss1 = loss_fn(outputs1,targets1)\n            #損失関数に関して、伝搬への影響を微分計算\n            loss1.backward()\n            #学習率と最適化手法に基づいて学習を実施する\n            optimizer.step()\n            \n            #loss1を代入 書く train_loader要素に対してforループを回しているので.\n            train_loss += loss1.item()\n            \n            #学習率を動的に変化させるscheduler.\n            if lr_scheduler:\n                lr_scheduler.step()\n            \n            #evaluating for every valid_step\n            #特定のタイミングで評価, valid_stepまたはループのケツ\n            if (i % valid_step == 0) or ((i + 1) == len(train_loader)):\n                model.eval()\n                valid_loss = 0\n                with torch.no_grad():\n                    for j, (inputs2,targets2) in enumerate(valid_loader):\n                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n                        outputs2 = model(**inputs2)\n                        loss2 = loss_fn(outputs2,targets2)\n                        valid_loss += loss2.item()\n                     \n                    valid_loss /= len(valid_loader)\n                    if valid_loss <= best_loss:\n                        #評価タイミングで比較して, inputされたbest_lossに対して,\n                        #best_lossより同等か小さいvalid_lossが得られたらそのモデルとtokenizerをモデルとして保存する\n                        if verbose:\n                            print(f\"epoch:{epoch} | Train Loss:{train_loss/(i+1)} | Validation loss:{valid_loss}\")\n                            print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n\n                        best_loss = valid_loss\n                        torch.save(model.state_dict(),f'./model{fold}/model{fold}.bin')\n                        tokenizer.save_pretrained(f'./model{fold}')\n                        \n        return best_loss\n    \n    accelerator = Accelerator()\n    print(f\"{accelerator.device} is used\")\n    \n    #foldで指定されたfoldナンバーがvalid用, そのほかがtrain用になっている\n    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n    \n    #tokenizerモデルを読み込む\n    tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\n    #modelを読み込む\n    model = Model(config['model_path'])\n\n    #読み込んだtokenizerでx_trainを変換してdatasetとする\n    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n    #datasetをdataLoaderに読み込ませてtrain_dl(what dl stand for?)する.\n    train_dl = DataLoader(train_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=True,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n    \n    #validも同様に処理\n    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n    valid_dl = DataLoader(valid_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    #optimizerの設定\n    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n    #学習率スケジューラ設定\n    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n\n    #acceleratorでいい感じに処理できるようにする\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    print(f\"Fold: {fold}\")\n    \n    #best_lossでかいところから初めて\n    #runの中で定義したtrain_and_evaluate_loop関数で学習をepoch数だけ繰り返す\n    \n    best_loss = 9999\n    for epoch in range(config[\"epochs\"]):\n        print(f\"Epoch Started:{epoch}\")\n        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n                                            optimizer,epoch,fold,best_loss,\n                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T06:13:13.24159Z","iopub.execute_input":"2021-07-25T06:13:13.242249Z","iopub.status.idle":"2021-07-25T06:13:13.264827Z","shell.execute_reply.started":"2021-07-25T06:13:13.242124Z","shell.execute_reply":"2021-07-25T06:13:13.263915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fold数だけ実施する\nfor f in range(config['nfolds']):\n    run(f)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T06:13:13.266582Z","iopub.execute_input":"2021-07-25T06:13:13.267489Z","iopub.status.idle":"2021-07-25T07:28:12.797635Z","shell.execute_reply.started":"2021-07-25T06:13:13.267447Z","shell.execute_reply":"2021-07-25T07:28:12.794516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}