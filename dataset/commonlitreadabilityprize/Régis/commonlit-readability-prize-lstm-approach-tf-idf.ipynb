{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LSTM model for NLP\n\nWe want for a given text, we want to rate the reading complexity of this one. Here, we face a regression problem.\n\nIn this kernel, we are going to use an LSTM model in order to predict the complixity of the current text.\n\nDon't hesitate if have question or if you see some improvement that can be made.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-29T15:31:10.013859Z","iopub.execute_input":"2021-05-29T15:31:10.014333Z","iopub.status.idle":"2021-05-29T15:31:16.340818Z","shell.execute_reply.started":"2021-05-29T15:31:10.014219Z","shell.execute_reply":"2021-05-29T15:31:16.339792Z"}}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Embedding, Input, Dropout, SpatialDropout1D, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.layers.wrappers import TimeDistributed\nfrom tensorflow.python.keras.layers.recurrent import LSTM\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom sklearn.model_selection import train_test_split\nfrom os import path","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:15:49.910918Z","iopub.execute_input":"2021-05-30T20:15:49.914867Z","iopub.status.idle":"2021-05-30T20:15:56.721597Z","shell.execute_reply.started":"2021-05-30T20:15:49.913982Z","shell.execute_reply":"2021-05-30T20:15:56.720509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ndf_test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ndf_sample = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:15:56.723137Z","iopub.execute_input":"2021-05-30T20:15:56.723455Z","iopub.status.idle":"2021-05-30T20:15:56.887276Z","shell.execute_reply.started":"2021-05-30T20:15:56.723424Z","shell.execute_reply":"2021-05-30T20:15:56.88632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize some data","metadata":{}},{"cell_type":"code","source":"df_train['excerpt'][0]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:15:56.889742Z","iopub.execute_input":"2021-05-30T20:15:56.890179Z","iopub.status.idle":"2021-05-30T20:15:56.907118Z","shell.execute_reply.started":"2021-05-30T20:15:56.890133Z","shell.execute_reply":"2021-05-30T20:15:56.905845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(columns=['url_legal', 'license'])\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:15:56.909368Z","iopub.execute_input":"2021-05-30T20:15:56.909799Z","iopub.status.idle":"2021-05-30T20:15:56.940585Z","shell.execute_reply.started":"2021-05-30T20:15:56.909742Z","shell.execute_reply":"2021-05-30T20:15:56.939367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop(columns=['url_legal', 'license'])\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:15:56.94196Z","iopub.execute_input":"2021-05-30T20:15:56.942283Z","iopub.status.idle":"2021-05-30T20:15:56.953265Z","shell.execute_reply.started":"2021-05-30T20:15:56.94224Z","shell.execute_reply":"2021-05-30T20:15:56.952477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Max length of phrases - Useful for the input of our model.\n\nmax_length_training = max(df_train.apply(lambda x : len(x[\"excerpt\"]), axis=1))\nmax_length_testing = max(df_test.apply(lambda x : len(x[\"excerpt\"]), axis=1))\n\nprint(max_length_training, max_length_testing)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:15:56.95464Z","iopub.execute_input":"2021-05-30T20:15:56.955148Z","iopub.status.idle":"2021-05-30T20:15:57.012122Z","shell.execute_reply.started":"2021-05-30T20:15:56.955097Z","shell.execute_reply":"2021-05-30T20:15:57.011104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess the data\n\nIn order to preprocess the data, we are going to :\n\n- Word tokenize : we want to break down the sentence to get the words that compose it.\n- To lower case : normalize each word.\n- Remove punctuations/digits.\n- (optional) Remove stopwords : remove non significative words.\n- (optional) Stemming : get the word stem, the root form of the word. (Example : fishing, fished, fisher => fish)\n- Lemmatized : Get the lemma of the word.\n\nIn this approach, I wanted to keep the context of the phrase and also the sequences of words used. \nI think that the complexity of the phrase is also referred by all the words used.\nThat for this reason, I didn't remove the stopwords, but maybe I wrong on this one.\nAlso, I prefer to keep the lemmatization instead of the stemming.","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words(\"english\"))\nporter = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_text(text):\n    \n    # Extract all the words in the phrase : get a list \n    tokens = word_tokenize(text)\n    \n    # Lowercase the words\n    tokens = [word.lower() for word in tokens]\n    \n    # Remove all tokens that are not alphabetic\n    words = [word for word in tokens if word.isalpha()]\n    \n    # Remove word in the stop word\n    # words = [word for word in words if not word in stop_words]\n\n    # Get the root of the word \n    # stemmed = [porter.stem(word) for word in words]\n    \n    # Lematize the word\n    lematized = [lemmatizer.lemmatize(word) for word in words]\n\n    return \" \".join(lematized)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:15:57.013238Z","iopub.execute_input":"2021-05-30T20:15:57.013508Z","iopub.status.idle":"2021-05-30T20:15:57.800322Z","shell.execute_reply.started":"2021-05-30T20:15:57.013482Z","shell.execute_reply":"2021-05-30T20:15:57.799469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['preprocess_text'] = df_train.excerpt.apply(preprocess_text)\ndf_test['preprocess_text'] = df_test.excerpt.apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:15:57.801457Z","iopub.execute_input":"2021-05-30T20:15:57.801918Z","iopub.status.idle":"2021-05-30T20:16:09.017006Z","shell.execute_reply.started":"2021-05-30T20:15:57.801873Z","shell.execute_reply":"2021-05-30T20:16:09.016268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_words = list(df_train.preprocess_text.str.split(' ', expand=True).stack().unique())","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:16:09.019123Z","iopub.execute_input":"2021-05-30T20:16:09.019667Z","iopub.status.idle":"2021-05-30T20:16:09.458973Z","shell.execute_reply.started":"2021-05-30T20:16:09.019617Z","shell.execute_reply":"2021-05-30T20:16:09.457957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(unique_words))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:16:09.460531Z","iopub.execute_input":"2021-05-30T20:16:09.460847Z","iopub.status.idle":"2021-05-30T20:16:09.465883Z","shell.execute_reply.started":"2021-05-30T20:16:09.460816Z","shell.execute_reply":"2021-05-30T20:16:09.464768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:16:09.467682Z","iopub.execute_input":"2021-05-30T20:16:09.468109Z","iopub.status.idle":"2021-05-30T20:16:09.48669Z","shell.execute_reply.started":"2021-05-30T20:16:09.468065Z","shell.execute_reply":"2021-05-30T20:16:09.485691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize our text","metadata":{}},{"cell_type":"code","source":"MAX_INPUT_LENGTH = max(max_length_training, max_length_testing)\nDICTIONARY_SIZE = len(unique_words)\nEMBEDDING_SIZE = 100","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:16:30.387474Z","iopub.execute_input":"2021-05-30T20:16:30.38783Z","iopub.status.idle":"2021-05-30T20:16:30.392059Z","shell.execute_reply.started":"2021-05-30T20:16:30.3878Z","shell.execute_reply":"2021-05-30T20:16:30.391251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For each text, we fit them to Tokenizer.\ntokenizer = Tokenizer(num_words=DICTIONARY_SIZE, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df_train['preprocess_text'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\n# Then, we \nX = tokenizer.texts_to_sequences(df_train['preprocess_text'].values)\nX = pad_sequences(X, maxlen=MAX_INPUT_LENGTH)\nprint('Shape of data tensor:', X.shape)\n\nX_test = tokenizer.texts_to_sequences(df_test['preprocess_text'].values)\nX_test = pad_sequences(X_test, maxlen=MAX_INPUT_LENGTH)\n\n# Get the value \nY = df_train['target'].values\n\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:16:31.821853Z","iopub.execute_input":"2021-05-30T20:16:31.822351Z","iopub.status.idle":"2021-05-30T20:16:32.647527Z","shell.execute_reply.started":"2021-05-30T20:16:31.82232Z","shell.execute_reply":"2021-05-30T20:16:32.646446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Do a simple LSTM model","metadata":{}},{"cell_type":"markdown","source":"With this LSTM model, we are going to stack two bidirectional LSTM.\nRegarding the output, as we want to return a rate, we put a single neuron with a linear activation function.","metadata":{}},{"cell_type":"code","source":"def create_lstm_model():\n    model = Sequential()\n    model.add(Input(shape=(MAX_INPUT_LENGTH,)))\n    \n    model.add(Embedding(DICTIONARY_SIZE, EMBEDDING_SIZE))\n    model.add(SpatialDropout1D(0.2))\n    \n    model.add(Bidirectional(LSTM(128, recurrent_dropout=0.2, return_sequences=True)))\n    model.add(Dropout(0.5))\n    \n    model.add(Bidirectional(LSTM(128)))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1, activation='linear'))\n    \n    # Possibly of changing the learning rate of Adam\n    model.compile(loss=\"mean_squared_error\", optimizer=Adam(), metrics=['mse', 'mae', RootMeanSquaredError()])\n    \n    return model\n\nmodel = create_lstm_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:16:34.505951Z","iopub.execute_input":"2021-05-30T20:16:34.506391Z","iopub.status.idle":"2021-05-30T20:16:35.224388Z","shell.execute_reply.started":"2021-05-30T20:16:34.506354Z","shell.execute_reply":"2021-05-30T20:16:35.223329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import LearningRateScheduler\n\nMODEL_PATH = './bi_lstm_stack'\n\ndef lr_time_based_decay(epoch, lr):\n    decay_rate = 0.1\n    return lr * 1 / (1 + decay_rate * epoch)\n\nif path.exists(MODEL_PATH):\n    print(\"[*] Load pretrained model !\")\n    \n    model = keras.models.load_model(MODEL_PATH)\nelse:\n    print(\"[*] Train the model !\")\n    \n    model.fit(\n        X_train, \n        y_train,\n        validation_data=(X_val, y_val),\n        batch_size=16,\n        epochs=8,\n        verbose=1,\n        callbacks=[LearningRateScheduler(lr_time_based_decay, verbose=1)]\n    )\n    \n    model.save(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:16:39.689522Z","iopub.execute_input":"2021-05-30T20:16:39.689884Z","iopub.status.idle":"2021-05-30T20:18:12.198171Z","shell.execute_reply.started":"2021-05-30T20:16:39.689855Z","shell.execute_reply":"2021-05-30T20:18:12.196685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\n\ndf_sample['target'] = y_pred\n\ndf_sample.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:16:09.905002Z","iopub.status.idle":"2021-05-30T20:16:09.905424Z"},"trusted":true},"execution_count":null,"outputs":[]}]}