{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Embedding, Input, Dropout\nfrom tensorflow.python.keras.layers.wrappers import TimeDistributed\nfrom tensorflow.python.keras.layers.recurrent import LSTM","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-27T15:41:21.884319Z","iopub.execute_input":"2021-05-27T15:41:21.88481Z","iopub.status.idle":"2021-05-27T15:41:21.891598Z","shell.execute_reply.started":"2021-05-27T15:41:21.884756Z","shell.execute_reply":"2021-05-27T15:41:21.890527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ndf_test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ndf_sample = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:21.89354Z","iopub.execute_input":"2021-05-27T15:41:21.894291Z","iopub.status.idle":"2021-05-27T15:41:21.965481Z","shell.execute_reply.started":"2021-05-27T15:41:21.89424Z","shell.execute_reply":"2021-05-27T15:41:21.964172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize some data","metadata":{}},{"cell_type":"code","source":"df_train['excerpt'][0]","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:21.96721Z","iopub.execute_input":"2021-05-27T15:41:21.967522Z","iopub.status.idle":"2021-05-27T15:41:21.974255Z","shell.execute_reply.started":"2021-05-27T15:41:21.967493Z","shell.execute_reply":"2021-05-27T15:41:21.972662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(columns=['url_legal', 'license'])\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:21.976186Z","iopub.execute_input":"2021-05-27T15:41:21.976752Z","iopub.status.idle":"2021-05-27T15:41:21.998163Z","shell.execute_reply.started":"2021-05-27T15:41:21.976708Z","shell.execute_reply":"2021-05-27T15:41:21.997383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop(columns=['url_legal', 'license'])\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:21.999411Z","iopub.execute_input":"2021-05-27T15:41:21.999845Z","iopub.status.idle":"2021-05-27T15:41:22.02208Z","shell.execute_reply.started":"2021-05-27T15:41:21.999798Z","shell.execute_reply":"2021-05-27T15:41:22.020993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Max length of phrases\n\nmax_length_training = max(df_train.apply(lambda x : len(x[\"excerpt\"]), axis=1))\nmax_length_testing = max(df_test.apply(lambda x : len(x[\"excerpt\"]), axis=1))\n\nprint(\"Max length of the sentences :\")\nprint(\"Training : \", max_length_training, \" - Testing : \", max_length_testing)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:22.023296Z","iopub.execute_input":"2021-05-27T15:41:22.023731Z","iopub.status.idle":"2021-05-27T15:41:22.086738Z","shell.execute_reply.started":"2021-05-27T15:41:22.023695Z","shell.execute_reply":"2021-05-27T15:41:22.085652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess the data\n\nIn order to preprocess the data, we are going to :\n\n- Word tokenize : we want to break down the sentence to get the words that compose it.\n- To lower case : normalize each word.\n- Remove punctuations/digits.\n- Remove stopwords : remove non significative words.\n- Stemming : get the word stem, the root form of the word. (Example : fishing, fished, fisher => fish)\n- Lemmatized : get the lemma of the word.","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words(\"english\"))\nporter = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_text(text):\n    \n    # Extract all the words in the phrase : get a list \n    tokens = word_tokenize(text)\n    \n    # Lowercase the words\n    tokens = [word.lower() for word in tokens]\n    \n    # Remove all tokens that are not alphabetic\n    words = [word for word in tokens if word.isalpha()]\n    \n    # Remove word in the stop word\n    words = [word for word in words if not word in stop_words]\n\n    # Get the root of the word \n    stemmed = [porter.stem(word) for word in words]\n    \n    # Lematize the word\n    lematized = [lemmatizer.lemmatize(word) for word in stemmed]\n\n    return \" \".join(lematized)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:22.088704Z","iopub.execute_input":"2021-05-27T15:41:22.089051Z","iopub.status.idle":"2021-05-27T15:41:22.899608Z","shell.execute_reply.started":"2021-05-27T15:41:22.089019Z","shell.execute_reply":"2021-05-27T15:41:22.898558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['preprocess_text'] = df_train.excerpt.apply(preprocess_text)\ndf_test['preprocess_text'] = df_test.excerpt.apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:22.902415Z","iopub.execute_input":"2021-05-27T15:41:22.902743Z","iopub.status.idle":"2021-05-27T15:41:40.696084Z","shell.execute_reply.started":"2021-05-27T15:41:22.902714Z","shell.execute_reply":"2021-05-27T15:41:40.694906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:40.697486Z","iopub.execute_input":"2021-05-27T15:41:40.697806Z","iopub.status.idle":"2021-05-27T15:41:40.711654Z","shell.execute_reply.started":"2021-05-27T15:41:40.697773Z","shell.execute_reply":"2021-05-27T15:41:40.710633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF ","metadata":{}},{"cell_type":"markdown","source":"The idea of TF-IDF (term frequency-inverse document frequency) is to describe a document by his vocabulary. For exemple, we can assume that the more complexe vocabulary we use, the more complex the document could be. In TF-IDF, we found : \n\n- Terme frequency : given a document we compute the number of occurence of the word.\n- Inverse document frequency : indicates how common or rare a word is in the entire document set. The close it is to 0, the more common a word is. It can be compute by taking the number of documents, dividing the number of doucments that contain a word, and calculating the logarithm. So if the word is very common in each document, we don't really want to keep it. \n\nWe get a TF-IDF score by multiplying these two results.\n\n* https://monkeylearn.com/blog/what-is-tf-idf/","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:40.713524Z","iopub.execute_input":"2021-05-27T15:41:40.713989Z","iopub.status.idle":"2021-05-27T15:41:40.730473Z","shell.execute_reply.started":"2021-05-27T15:41:40.713943Z","shell.execute_reply":"2021-05-27T15:41:40.729241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_all = pd.concat([df_train[\"preprocess_text\"], df_test[\"preprocess_text\"]])\n\ntfidf = TfidfVectorizer(stop_words = 'english')\ntfidf.fit(X_all)\n\nX = tfidf.transform(df_train[\"preprocess_text\"])\nX_test = tfidf.transform(df_test[\"preprocess_text\"])\n\n\nX_train, X_val, y_train, y_val = train_test_split(X, df_train[\"target\"], test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:40.731865Z","iopub.execute_input":"2021-05-27T15:41:40.732477Z","iopub.status.idle":"2021-05-27T15:41:41.389844Z","shell.execute_reply.started":"2021-05-27T15:41:40.73243Z","shell.execute_reply":"2021-05-27T15:41:41.388805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM - Hyperparameters of SVR \n\nIn this approach, we are going to use an SVM. In order to tuned that model, we need to understand his hyperparameter.\n\n> Note : For a regression, we use SVR, but for a classification, we have to use SVC !\n\n- C parameter adds a penalty for each misclassified data point. If C is small, the penalty for misclassified points is low so a decision boundary with a large margin is chosen at the expense of a greater number of misclassifications.\n\n- Gamma controls the distance of influence of a single training point. Low values indicates a large similarity radius, which results in more points being grouped together. And, for high values of gamma, the points need to be very close to each other to be considered n the same group. \n\n- Espilon defines a margin of tolerance where no penalty is given to errors.\n\n\n0.1 < C < 100\n\n0.0001 < gamma < 10\n\n* https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167\n* https://stats.stackexchange.com/questions/259018/meaning-of-epsilon-in-svm-regression","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\n\n# If you want to search the best hyperparameter, change this variable.\nFULL_PARAMETER_SEARCH = False\n\n# At the moment, best search obtained from full parameter search : \n# {'C': 100, 'epsilon': 0.1, 'gamma': 0.005, 'kernel': 'rbf'}\n\nif FULL_PARAMETER_SEARCH : \n    parameters = { \n        'kernel': ['rbf'], \n        'C': [0.1, 1, 10, 100],\n        'epsilon': [0.0001, 0.001, 0.01, 0.1],\n        'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n    }\nelse:\n    parameters = { \n        'kernel': ['rbf'], \n        'C': [100],\n        'epsilon': [0.1],\n        'gamma': [0.005]\n    }","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:41.391228Z","iopub.execute_input":"2021-05-27T15:41:41.39156Z","iopub.status.idle":"2021-05-27T15:41:41.398129Z","shell.execute_reply.started":"2021-05-27T15:41:41.391528Z","shell.execute_reply":"2021-05-27T15:41:41.397331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We use our training set and validation.\nif FULL_PARAMETER_SEARCH :\n    model = GridSearchCV(\n        SVR(), \n        parameters,\n        cv=5, \n        scoring='neg_mean_squared_error',\n        n_jobs=-1, \n        verbose=1).fit(X_train, y_train)\n    \n    # See on our validation data our score.\n    y_val_pred = model.predict(X_val)\n    print(\"Error on validation set : \", mean_squared_error(y_val, y_val_pred))\n    \nelse :\n    # Train on all the data \n    model = GridSearchCV(\n        SVR(), \n        parameters,\n        cv=5, \n        scoring='neg_mean_squared_error',\n        n_jobs=-1, \n        verbose=1).fit(X, df_train[\"target\"]) \n    \nprint(model.cv_results_['params'][model.best_index_])","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:41.399223Z","iopub.execute_input":"2021-05-27T15:41:41.399507Z","iopub.status.idle":"2021-05-27T15:41:58.737921Z","shell.execute_reply.started":"2021-05-27T15:41:41.399474Z","shell.execute_reply":"2021-05-27T15:41:58.736679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\n\ndf_sample['target'] = y_pred\n\ndf_sample.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:58.741547Z","iopub.execute_input":"2021-05-27T15:41:58.742008Z","iopub.status.idle":"2021-05-27T15:41:58.769438Z","shell.execute_reply.started":"2021-05-27T15:41:58.741965Z","shell.execute_reply":"2021-05-27T15:41:58.768335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bayesian Ridge model\n\nWith our TF-IDF representation, we can use other model instead of SVM. We can use Bayesian Ridge.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import BayesianRidge\n\nclf = BayesianRidge()\nclf.fit(X_train.toarray(), y_train)\n\n# See on our validation data our score.\ny_val_pred = clf.predict(X_val)\n\nprint(\"Error on validation : \", mean_squared_error(y_val, y_val_pred))","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:41:58.771318Z","iopub.execute_input":"2021-05-27T15:41:58.771762Z","iopub.status.idle":"2021-05-27T15:42:52.757417Z","shell.execute_reply.started":"2021-05-27T15:41:58.771715Z","shell.execute_reply":"2021-05-27T15:42:52.756276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample_bay = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n\n# Train on all our data\nclf = BayesianRidge()\nclf.fit(X.toarray(), df_train[\"target\"])\n\n# Make the prediction and save the file.\ny_pred = clf.predict(X_test)\n\ndf_sample_bay['target'] = y_pred\n\ndf_sample_bay.to_csv(\"submission_bayes.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:42:52.759397Z","iopub.execute_input":"2021-05-27T15:42:52.760215Z","iopub.status.idle":"2021-05-27T15:43:58.58013Z","shell.execute_reply.started":"2021-05-27T15:42:52.760143Z","shell.execute_reply":"2021-05-27T15:43:58.578574Z"},"trusted":true},"execution_count":null,"outputs":[]}]}