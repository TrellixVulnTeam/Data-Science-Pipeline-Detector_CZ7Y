{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import KFold\n\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['id','excerpt', 'target']].to_csv('vals.csv', index = False, header = None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport sys\nimport csv\ncsv.field_size_limit(sys.maxsize)\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn import metrics\nimport numpy as np\n\ndef get_evaluation(y_true, y_prob, list_metrics):\n    y_pred = y_prob\n    output = {}\n    if 'accuracy' in list_metrics:\n        output['accuracy'] = metrics.accuracy_score(y_true, y_pred)\n    if 'loss' in list_metrics:\n        output['loss'] = metrics.mean_squared_error(y_true, y_prob, squared=False)\n    if 'confusion_matrix' in list_metrics:\n        output['confusion_matrix'] = str(metrics.confusion_matrix(y_true, y_pred))\n    return output\n\ndef matrix_mul(input, weight, bias=False):\n    feature_list = []\n    for feature in input:\n        feature = torch.mm(feature, weight)\n        if isinstance(bias, torch.nn.parameter.Parameter):\n            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n        feature = torch.tanh(feature).unsqueeze(0)\n        feature_list.append(feature)\n\n    return torch.cat(feature_list, 0).squeeze()\n\ndef element_wise_mul(input1, input2):\n\n    feature_list = []\n    for feature_1, feature_2 in zip(input1, input2):\n        feature_2 = feature_2.unsqueeze(1).expand_as(feature_1)\n        feature = feature_1 * feature_2\n        feature_list.append(feature.unsqueeze(0))\n    output = torch.cat(feature_list, 0)\n\n    return torch.sum(output, 0).unsqueeze(0)\n\ndef get_max_lengths(data_path):\n    word_length_list = []\n    sent_length_list = []\n    with open(data_path) as csv_file:\n        reader = csv.reader(csv_file, quotechar='\"')\n        for idx, line in enumerate(reader):\n            text = \"\"\n            for tx in line[1:]:\n                text += tx.lower()\n                text += \" \"\n            sent_list = sent_tokenize(text)\n            sent_length_list.append(len(sent_list))\n\n            for sent in sent_list:\n                word_list = word_tokenize(sent)\n                word_length_list.append(len(word_list))\n\n        sorted_word_length = sorted(word_length_list)\n        sorted_sent_length = sorted(sent_length_list)\n\n    return sorted_word_length[int(0.8*len(sorted_word_length))], sorted_sent_length[int(0.8*len(sorted_sent_length))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nimport csv\n\nclass WordAttNet(nn.Module):\n    def __init__(self, word2vec_path, hidden_size=20):\n        super(WordAttNet, self).__init__()\n        dict = pd.read_csv(filepath_or_buffer=word2vec_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE).values[:, 1:]\n        dict_len, embed_size = dict.shape\n        dict_len += 1\n        unknown_word = np.zeros((1, embed_size))\n        dict = torch.from_numpy(np.concatenate([unknown_word, dict], axis=0).astype(np.float))\n\n        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n\n        self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n        self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True)\n        self._create_weights(mean=0.0, std=0.05)\n\n    def _create_weights(self, mean=0.0, std=0.05):\n\n        self.word_weight.data.normal_(mean, std)\n        self.context_weight.data.normal_(mean, std)\n\n    def forward(self, input, hidden_state):\n\n        output = self.lookup(input)\n        f_output, h_output = self.gru(output.float(), hidden_state)  # feature output and hidden state output\n        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n        output = matrix_mul(output, self.context_weight).permute(1,0)\n        \n#         output = F.softmax(output)\n        output = element_wise_mul(f_output,output.permute(1,0))\n\n        return output, h_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SentAttNet(nn.Module):\n    def __init__(self, sent_hidden_size=50, word_hidden_size=50, num_classes=14):\n        super(SentAttNet, self).__init__()\n\n        self.sent_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 2 * sent_hidden_size))\n        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * sent_hidden_size))\n        self.context_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 1))\n\n        self.gru = nn.GRU(2 * word_hidden_size, sent_hidden_size, bidirectional=True)\n        self.fc = nn.Linear(2 * sent_hidden_size, num_classes)\n        # self.sent_softmax = nn.Softmax()\n        # self.fc_softmax = nn.Softmax()\n        self._create_weights(mean=0.0, std=0.05)\n\n    def _create_weights(self, mean=0.0, std=0.05):\n        self.sent_weight.data.normal_(mean, std)\n        self.context_weight.data.normal_(mean, std)\n\n    def forward(self, input, hidden_state):\n\n        f_output, h_output = self.gru(input, hidden_state)\n        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n        output = matrix_mul(output, self.context_weight).permute(1, 0)\n#         output = F.softmax(output)\n        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n        output = self.fc(output)\n        \n        return output, h_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass HierAttNet(nn.Module):\n    def __init__(self, word_hidden_size, sent_hidden_size, batch_size, num_classes, pretrained_word2vec_path,\n                 max_sent_length, max_word_length):\n        super(HierAttNet, self).__init__()\n        self.batch_size = batch_size\n        self.word_hidden_size = word_hidden_size\n        self.sent_hidden_size = sent_hidden_size\n        self.max_sent_length = max_sent_length\n        self.max_word_length = max_word_length\n        self.word_att_net = WordAttNet(pretrained_word2vec_path, word_hidden_size)\n        self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size, num_classes)\n        self._init_hidden_state()\n\n    def _init_hidden_state(self, last_batch_size=None):\n        if last_batch_size:\n            batch_size = last_batch_size\n        else:\n            batch_size = self.batch_size\n        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n        if torch.cuda.is_available():\n            self.word_hidden_state = self.word_hidden_state.cuda()\n            self.sent_hidden_state = self.sent_hidden_state.cuda()\n\n    def forward(self, input):\n\n        output_list = []\n        input = input.permute(1, 0, 2)\n        for i in input:\n            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0), self.word_hidden_state)\n            output_list.append(output)\n\n        output = torch.cat(output_list, 0)\n        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data.dataset import Dataset\nimport csv\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport numpy as np\n\n\nclass MyDataset(Dataset):\n\n    def __init__(self, data_path, dict_path, max_length_sentences=30, max_length_word=35):\n        super(MyDataset, self).__init__()\n\n        texts, labels = [], []\n        try:\n            with open(data_path) as csv_file:\n                reader = csv.reader(csv_file, quotechar='\"')\n                for idx, line in enumerate(reader):\n                    text = \"\"\n                    for tx in line[1:]:\n                        text += tx.lower()\n                        text += \" \"\n                    label = np.double(line[-1])\n                    texts.append(text)\n                    labels.append(label)\n        except:\n            pass\n\n        self.texts = texts\n        self.labels = labels\n        self.dict = pd.read_csv(filepath_or_buffer=dict_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE,\n                                usecols=[0]).values\n        self.dict = [word[0] for word in self.dict]\n        self.max_length_sentences = max_length_sentences\n        self.max_length_word = max_length_word\n        self.num_classes = 1\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        label = self.labels[index]\n        text = self.texts[index]\n        document_encode = [\n            [self.dict.index(word) if word in self.dict else -1 for word in word_tokenize(text=sentences)] for sentences\n            in\n            sent_tokenize(text=text)]\n\n        for sentences in document_encode:\n            if len(sentences) < self.max_length_word:\n                extended_words = [-1 for _ in range(self.max_length_word - len(sentences))]\n                sentences.extend(extended_words)\n\n        if len(document_encode) < self.max_length_sentences:\n            extended_sentences = [[-1 for _ in range(self.max_length_word)] for _ in\n                                  range(self.max_length_sentences - len(document_encode))]\n            document_encode.extend(extended_sentences)\n\n        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n                          :self.max_length_sentences]\n\n        document_encode = np.stack(arrays=document_encode, axis=0)\n        document_encode += 1\n\n        return document_encode.astype(np.int64), label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir trained_models\n!mkdir tensorboard","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['fold'] = -1\ngkf = KFold(n_splits=5) #, shuffle = True, random_state = 42)\nfor fold, (train, val) in enumerate(gkf.split(train_df.excerpt, train_df.target)):\n    train_df.loc[val,'fold']=fold\n\nfold = 0\nvalidation_df = train_df[train_df.fold==fold].reset_index(drop=True)\ntrain_df = train_df[train_df.fold!=fold].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[['id','excerpt', 'target']].to_csv('train.csv', index = False, header = None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df[['id','excerpt', 'target']].to_csv('test.csv', index = False, header = None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tensorboardX import SummaryWriter\nimport argparse\nimport shutil\nimport numpy as np\n\nclass opt:\n    batch_size = 128\n    num_epoches = 30\n    lr = 0.001\n    momentum = 0.9\n    word_hidden_size = 10\n    sent_hidden_size = 10\n    es_min_delta = 0.0\n    es_patience = 5\n    train_set = './train.csv'\n    test_set = './test.csv'\n    test_interval = 1\n    word2vec_path = '../input/glove6b/glove.6B.50d.txt'\n    log_path = './tensorboard'\n    saved_path = './trained_models'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    torch.cuda.manual_seed(123)\nelse:\n    torch.manual_seed(123)\noutput_file = open(opt.saved_path + os.sep + \"logs.txt\", \"w\")\noutput_file.write(\"Model's parameters: {}\".format(vars(opt)))\ntraining_params = {\"batch_size\": opt.batch_size,\n                   \"shuffle\": True,\n                   \"drop_last\": True}\ntest_params = {\"batch_size\": opt.batch_size,\n               \"shuffle\": False,\n               \"drop_last\": False}\n\nmax_word_length, max_sent_length = get_max_lengths(opt.train_set)\ntraining_set = MyDataset(opt.train_set, opt.word2vec_path, max_sent_length, max_word_length)\ntraining_generator = DataLoader(training_set, **training_params)\ntest_set = MyDataset(opt.test_set, opt.word2vec_path, max_sent_length, max_word_length)\ntest_generator = DataLoader(test_set, **test_params)\n\nmodel = HierAttNet(opt.word_hidden_size, opt.sent_hidden_size, opt.batch_size, training_set.num_classes,\n                   opt.word2vec_path, max_sent_length, max_word_length)\n\n\nif os.path.isdir(opt.log_path):\n    shutil.rmtree(opt.log_path)\nos.makedirs(opt.log_path)\nwriter = SummaryWriter(opt.log_path)\n# writer.add_graph(model, torch.zeros(opt.batch_size, max_sent_length, max_word_length))\n\nif torch.cuda.is_available():\n    model.cuda()\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=opt.lr)\nbest_loss = 1e5\nbest_epoch = 0\nmodel.train()\nnum_iter_per_epoch = len(training_generator)\nfor epoch in range(opt.num_epoches):\n    for iter, (feature, label) in enumerate(training_generator):\n        if torch.cuda.is_available():\n            feature = feature.cuda()\n            label = label.cuda()\n        optimizer.zero_grad()\n        model._init_hidden_state()\n        predictions = model(feature)\n        predictions = predictions.reshape((-1,1))\n        label = label.reshape((-1,1))\n        loss = criterion(predictions.float(), label.float())\n        loss.backward()\n        optimizer.step()\n        training_metrics = get_evaluation(label.cpu().numpy(), predictions.cpu().detach().numpy(), list_metrics=[\"loss\"])\n        print(\"Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss: {}\".format(\n            epoch + 1,\n            opt.num_epoches,\n            iter + 1,\n            num_iter_per_epoch,\n            optimizer.param_groups[0]['lr'],\n            np.sqrt(loss.detach().cpu().numpy()), training_metrics[\"loss\"]))\n        writer.add_scalar('Train/Loss', loss, epoch * num_iter_per_epoch + iter)\n        writer.add_scalar('Train/Accuracy', training_metrics[\"loss\"], epoch * num_iter_per_epoch + iter)\n    if epoch % opt.test_interval == 0:\n        model.eval()\n        loss_ls = []\n        te_label_ls = []\n        te_pred_ls = []\n        for te_feature, te_label in test_generator:\n            num_sample = len(te_label)\n            if torch.cuda.is_available():\n                te_feature = te_feature.cuda()\n                te_label = te_label.cuda().reshape((-1,1))\n            with torch.no_grad():\n                model._init_hidden_state(num_sample)\n                te_predictions = model(te_feature).reshape((-1,1))\n            te_loss = criterion(te_predictions, te_label)\n            loss_ls.append(te_loss * num_sample)\n            te_label_ls.extend(te_label.clone().cpu())\n            te_pred_ls.append(te_predictions.clone().cpu())\n        te_loss = sum(loss_ls) / test_set.__len__()\n        te_pred = torch.cat(te_pred_ls, 0)\n        te_label = np.array(te_label_ls)\n        test_metrics = get_evaluation(te_label, te_pred.numpy(), list_metrics=[\"loss\"])\n        output_file.write(\n            \"Epoch: {}/{} \\nTest loss: {} Test rmse: {} \\n\\n\".format(\n                epoch + 1, opt.num_epoches,\n                np.sqrt(te_loss.detach().cpu().numpy()),\n                test_metrics[\"loss\"]))\n        print(\"Epoch: {}/{} \\nTest loss: {} Test rmse: {} \\n\\n\".format(\n                epoch + 1, opt.num_epoches,\n                te_loss,\n                test_metrics[\"loss\"]))\n        writer.add_scalar('Test/Loss', te_loss, epoch)\n        model.train()\n        if te_loss + opt.es_min_delta < best_loss:\n            best_loss = te_loss\n            best_epoch = epoch\n            torch.save(model, opt.saved_path + os.sep + \"whole_model_han\")\n\n        # Early stopping\n        if epoch - best_epoch > opt.es_patience > 0:\n            print(\"Stop training at epoch {}. The lowest loss achieved is {}\".format(epoch, np.sqrt(te_loss)))\n            break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}