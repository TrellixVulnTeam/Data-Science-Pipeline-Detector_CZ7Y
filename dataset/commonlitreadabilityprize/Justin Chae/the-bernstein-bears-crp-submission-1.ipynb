{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The Bernstein Bears CRP Submission 1\n\n## Theme: Scaled TunA with PyTorch-Lightining, Optuna, and LGBMRegressor\n\n## The Gist\n1. Fine tune a roberta for sequence classification model from huggingface\n2. Scale target values with SKLearn StandardScaler\n3. Pytorch for model framework (tried PL but still buggy)\n4. Optional: Find best hyperparameters for roberta model with Optuna\n5. Optional: Run each optuna trial through k-fold validation and return mean score as trial score\n6. Train roberta model with best params on all train data and run inference\n7. Join roberta inference with original text data and apply textstat library for new features\n8. Run feature selection with support vector machine (SVR) from sklearn\n9. Select the top features (should be the roberta logits) and manually select the text category standard \n10. Scale inputs and targets again as a new LGBM dataset\n11. Run LGBM dataset through LGBMRegressor hyperparameter search with section optuna study\n12. Apply best params to LGBMRegressor and apply to all train data for predictions\n13. Strip extra fields from test data and apply LGBMRegressor to test data\n14. Apply the inverse transformation scaler to target values\n15. Submit final predictions as submission.csv\n\n## Some Key Learnings\n1. scaling values for both inputs and targets is important\n2. the text standard category is quite powerful in a lgbm regressor as a categorical feature\n3. most of the textstat features seem to be not great for predicting the target value\n4. kfold validation inside the optuna study works nicely for ML model\n5. kfold validation inside optuna for deep learning works but consumes a ton of memory\n6. to reduce memory footprint, ensure to set output hidden states to false for roberta\n7. also try not to do deep copy or copy while avoiding set with copy issues in pandas df","metadata":{}},{"cell_type":"code","source":"%%capture\n\n# install necessary libraries from input\n# import progressbar library for offline usage\n!ls ../input/progresbar2local\n!pip install progressbar2 --no-index --find-links=file:///kaggle/input/progresbar2local/progressbar2\n\n# import text stat library for additional ml data prep\n!ls ../input/textstat-local\n!pip install textstat --no-index --find-links=file:///kaggle/input/textstat-local/textstat ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T12:11:31.567899Z","iopub.execute_input":"2021-08-02T12:11:31.568407Z","iopub.status.idle":"2021-08-02T12:11:49.836449Z","shell.execute_reply.started":"2021-08-02T12:11:31.568302Z","shell.execute_reply":"2021-08-02T12:11:49.835046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FAST_DEV_RUN = False\nUSE_CHECKPOINT = True\nUSE_HIDDEN_IN_RGR = False\nN_FEATURES_TO_USE_HEAD = 7\nN_FEATURES_TO_USE_TAIL = None\n# in this kernel, run train on all data to maximize score on held out data but use what we learned about optimal parameters\n# set to 16 bit precision to cut compute requirements/increase batch size capacity\nUSE_16_BIT_PRECISION = True\n# set a seed value for consistent experimentation; optional, else leave as None\nSEED_VAL = 42\n# set a train-validation split, .7 means 70% of train data and 30% to validation set\nTRAIN_VALID_SPLIT = .8 # if None, then don't split\n# set hyperparameters learned from tuning: https://www.kaggle.com/justinchae/tune-roberta-pytorch-lightning-optuna\nMAX_EPOCHS = 4\nBATCH_SIZE = 16\nGRADIENT_CLIP_VAL = 0.18318092164684585\nLEARNING_RATE = 3.613894271216525e-05\nTOKENIZER_MAX_LEN = 363\nWARMUP_STEPS = 292\nWEIGHT_DECAY = 0.004560699842170359","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:11:49.839048Z","iopub.execute_input":"2021-08-02T12:11:49.839513Z","iopub.status.idle":"2021-08-02T12:11:49.849668Z","shell.execute_reply.started":"2021-08-02T12:11:49.839464Z","shell.execute_reply":"2021-08-02T12:11:49.848215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kaggle_config\nfrom kaggle_config import (WORKFLOW_ROOT, DATA_PATH, CACHE_PATH, FIG_PATH, \n                           MODEL_PATH, ANALYSIS_PATH, KAGGLE_INPUT, \n                           CHECKPOINTS_PATH, LOGS_PATH)\n\nINPUTS, DEVICE = kaggle_config.run()\nKAGGLE_TRAIN_PATH = kaggle_config.get_train_path(INPUTS)\nKAGGLE_TEST_PATH = kaggle_config.get_test_path(INPUTS)\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers as pl_loggers\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.tuner.batch_size_scaling import scale_batch_size\nfrom pytorch_lightning.tuner.lr_finder import _LRFinder, lr_find\n\nimport torchmetrics\n\nimport optuna\nfrom optuna.integration import PyTorchLightningPruningCallback\nfrom optuna.samplers import TPESampler, RandomSampler, CmaEsSampler\nfrom optuna.visualization import (plot_intermediate_values\n                                  , plot_optimization_history\n                                  , plot_param_importances)\n\nimport optuna.integration.lightgbm as lgb\nimport lightgbm as lgm\n\nfrom sklearn.model_selection import KFold, cross_val_score, RepeatedKFold, train_test_split\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import RFE, f_regression, mutual_info_regression, SequentialFeatureSelector\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\nimport math\n\nimport textstat\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataset import random_split\n\nimport tensorflow as tf\n\nfrom transformers import (RobertaForSequenceClassification\n                          , RobertaTokenizer\n                          , AdamW\n                          , get_linear_schedule_with_warmup)\n\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport gc\nfrom functools import partial\n\nfrom typing import List, Dict\nfrom typing import Optional\nfrom argparse import ArgumentParser\n\nimport random\n\nif SEED_VAL:\n    random.seed(SEED_VAL)\n    np.random.seed(SEED_VAL)\n    seed_everything(SEED_VAL)\n    \nNUM_DATALOADER_WORKERS = os.cpu_count()\n\ntry: \n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    n_tpus = len(tf.config.list_logical_devices('TPU'))\nexcept ValueError:\n    n_tpus = 0\n\nACCELERATOR_TYPE = {}\nACCELERATOR_TYPE.update({'gpus': torch.cuda.device_count() if torch.cuda.is_available() else None})\nACCELERATOR_TYPE.update({'tpu_cores': n_tpus if n_tpus > 0 else None})\n# still debugging how to best toggle between tpu and gpu; there's too much code to configure to work simply\nprint(\"ACCELERATOR_TYPE:\\n\", ACCELERATOR_TYPE)\n\nPRETTRAINED_ROBERTA_BASE_MODEL_PATH = \"/kaggle/input/pre-trained-roberta-base\"\nPRETRAINED_ROBERTA_BASE_TOKENIZER_PATH = \"/kaggle/input/tokenizer-roberta\"\nPRETRAINED_ROBERTA_BASE_TOKENIZER = RobertaTokenizer.from_pretrained(PRETRAINED_ROBERTA_BASE_TOKENIZER_PATH)\n\nTUNED_CHECKPOINT_PATH = \"/kaggle/input/best-crp-ckpt-4/crp_roberta_trial_4.ckpt\"\n# from: https://www.kaggle.com/justinchae/crp-regression-with-roberta-and-lightgbm\nTUNED_BEST_ROBERTA_PATH = \"/kaggle/input/my-best-tuned-roberta\"\n# from: https://www.kaggle.com/justinchae/the-bernstein-bears-crp-submission-2\nMODEL_42 = \"/kaggle/input/model-42\"\nMODEL_0 = \"/kaggle/input/model-0\"\nMODEL_21 = \"/kaggle/input/model-21\"","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:11:49.853025Z","iopub.execute_input":"2021-08-02T12:11:49.853877Z","iopub.status.idle":"2021-08-02T12:12:03.189759Z","shell.execute_reply.started":"2021-08-02T12:11:49.853789Z","shell.execute_reply":"2021-08-02T12:12:03.188447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Implementing Lightning instead of torch.nn.Module\n\"\"\"\nclass LitRobertaLogitRegressor(pl.LightningModule):\n    def __init__(self, pre_trained_path: str\n                     , output_hidden_states: bool = False\n                     , num_labels: int = 1\n                     , layer_1_output_size: int = 64\n                     , layer_2_output_size: int = 1\n                     , learning_rate: float = 1e-5\n                     , task_name: Optional[str] = None\n                     , warmup_steps: int = 100\n                     , weight_decay: float = 0.0\n                     , adam_epsilon: float = 1e-8\n                     , batch_size: Optional[int] = None\n                     , train_size: Optional[int] = None\n                     , max_epochs: Optional[int] = None\n                     , n_gpus: Optional[int] = 0\n                     , n_tpus: Optional[int] = 0 \n                     , accumulate_grad_batches = None\n                     , tokenizer = None\n                     , do_decode = False\n                ):\n        \"\"\"refactored from: https://www.kaggle.com/justinchae/my-bert-tuner and https://www.kaggle.com/justinchae/roberta-tuner\n        \"\"\"\n        super(LitRobertaLogitRegressor, self).__init__()\n        \n        # this saves class params as self.hparams\n        self.save_hyperparameters()\n        \n        self.model = RobertaForSequenceClassification.from_pretrained(self.hparams.pre_trained_path\n                                                                      , output_hidden_states=self.hparams.output_hidden_states\n                                                                       , num_labels=self.hparams.num_labels\n                                                                        )\n\n        self.accelerator_multiplier = n_gpus if n_gpus > 0 else 1\n        \n        self.config = self.model.config\n        self.parameters = self.model.parameters\n        self.save_pretrained = self.model.save_pretrained\n        # these layers are not currently used, tbd in future iteration\n        self.layer_1 = torch.nn.Linear(768, layer_1_output_size)\n        self.layer_2 = torch.nn.Linear(layer_1_output_size, layer_2_output_size)\n        self.tokenizer = tokenizer\n        self.do_decode = do_decode\n        self.output_hidden_states = output_hidden_states\n        \n        def rmse_loss(x, y):\n            criterion = F.mse_loss\n            loss = torch.sqrt(criterion(x, y))\n            return loss\n        \n        # TODO: enable toggle for various loss funcs and torchmetrics package\n        self.loss_func = rmse_loss\n#         self.eval_func = rmse_loss   \n        \n    def setup(self, stage=None) -> None:\n        if stage == 'fit':\n            # when this class is called by trainer.fit, this stage runs and so on\n            # Calculate total steps\n            tb_size = self.hparams.batch_size * self.accelerator_multiplier\n            ab_size = self.hparams.accumulate_grad_batches * float(self.hparams.max_epochs)\n            self.total_steps = (self.hparams.train_size // tb_size) // ab_size\n        \n    def extract_logit_only(self, input_ids, attention_mask) -> float:\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        logit = output.logits\n        logit = logit.cpu().numpy().astype(float)\n        return logit\n    \n    def extract_hidden_only(self, input_ids, attention_mask) -> np.array:\n        output = self.model(input_ids=input_ids, attention_mask=input_ids)\n        hidden_states = output.hidden_states\n        x = torch.stack(hidden_states[-4:]).sum(0)\n        m1 = torch.nn.Sequential(self.layer_1\n                                 , self.layer_2\n                                 , torch.nn.Flatten())\n        x = m1(x)\n        x = torch.squeeze(x).cpu().numpy()\n        \n        return x\n        \n    def forward(self, input_ids, attention_mask) -> torch.Tensor:\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        x = output.logits\n        return x\n    \n    def training_step(self, batch, batch_idx: int) -> float:\n        # refactored from: https://www.kaggle.com/justinchae/epoch-utils\n        labels, encoded_batch, kaggle_ids = batch\n        input_ids = encoded_batch['input_ids']\n        attention_mask = encoded_batch['attention_mask']\n        # per docs, keep train step separate from forward call\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        y_hat = output.logits\n        # quick reshape to align labels to predictions\n        labels = labels.view(-1, 1)\n        loss = self.loss_func(y_hat, labels)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx: int) -> float:\n        # refactored from: https://www.kaggle.com/justinchae/epoch-utils\n        labels, encoded_batch, kaggle_ids = batch\n        input_ids = encoded_batch['input_ids']\n        attention_mask = encoded_batch['attention_mask']\n        # this self call is calling the forward method\n        y_hat = self(input_ids, attention_mask)\n        # quick reshape to align labels to predictions\n        labels = labels.view(-1, 1)\n        loss = self.loss_func(y_hat, labels)\n        self.log('val_loss', loss)\n        return loss\n    \n    def predict(self, batch, batch_idx: int, dataloader_idx: int = None):\n        # creating this predict method overrides the pl predict method\n        target, encoded_batch, kaggle_ids = batch\n        \n        input_ids = encoded_batch['input_ids']\n        attention_mask = encoded_batch['attention_mask']\n        # this self call is calling the forward method\n        y_hat = self(input_ids, attention_mask)\n        # convert to numpy then list like struct to zip with ids\n        y_hat = y_hat.cpu().numpy().ravel()\n        # customizing the predict behavior to account for unique ids\n        if self.tokenizer is not None and self.do_decode:\n            target = target.cpu().numpy().ravel() if len(target) > 0 else None\n            \n            excerpt = self.tokenizer.batch_decode(input_ids.cpu().numpy()\n                                            , skip_special_tokens=True\n                                            , clean_up_tokenization_spaces=True)\n            if self.output_hidden_states:   \n                hidden_states = self.extract_hidden_only(input_ids=input_ids\n                                                             , attention_mask=attention_mask)\n            else:\n                hidden_states = None\n            \n            if target is not None:\n                predictions = list(zip(kaggle_ids\n                                       , target\n                                       , y_hat\n#                                        , hidden_states\n                                      ))\n                predictions = pd.DataFrame(predictions, columns=['id'\n                                                                 , 'target'\n                                                                 , 'logit'\n#                                                                  , 'hidden_states'\n                                                                ])\n            else:\n                predictions = list(zip(kaggle_ids\n                                       , y_hat\n#                                        , hidden_states\n                                      ))\n                predictions = pd.DataFrame(predictions, columns=['id'\n                                                                 , 'logit'\n#                                                                  , 'hidden_states'\n                                                                ])\n                \n        else:\n            predictions = list(zip(kaggle_ids, y_hat))\n            predictions = pd.DataFrame(predictions, columns=['id', 'target'])\n\n        return predictions\n    \n    def configure_optimizers(self) -> torch.optim.Optimizer:\n        # Reference: https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html\n        model = self.model\n        \n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        \n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": self.hparams.weight_decay,\n            },\n            {\n                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        \n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n        \n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.total_steps\n        )\n        scheduler = {'scheduler': scheduler, 'interval': 'step', 'frequency': 1}\n        \n        return [optimizer], [scheduler]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:12:03.192202Z","iopub.execute_input":"2021-08-02T12:12:03.192891Z","iopub.status.idle":"2021-08-02T12:12:03.228134Z","shell.execute_reply.started":"2021-08-02T12:12:03.192832Z","shell.execute_reply":"2021-08-02T12:12:03.226887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_collate_fn(batch\n                 , tokenizer\n                 , max_length: int = 100\n                 , return_tensors: str = 'pt'\n                 , padding: str = \"max_length\"\n                 , truncation: bool = True\n                 ):\n    # source: https://www.kaggle.com/justinchae/nn-utils\n    labels = []\n    batch_texts = []\n    kaggle_ids = []\n\n    for (_label, batch_text, kaggle_id) in batch:\n        if _label is not None:\n            labels.append(_label)\n        \n        batch_texts.append(batch_text)\n        kaggle_ids.append(kaggle_id)\n    \n            \n    if _label is not None:\n        labels = torch.tensor(labels, dtype=torch.float)\n    \n    encoded_batch = tokenizer(batch_texts\n                              , return_tensors=return_tensors\n                              , padding=padding\n                              , max_length=max_length\n                              , truncation=truncation)\n\n    return labels, encoded_batch, kaggle_ids\n\n\nclass CommonLitDataset(Dataset):\n    def __init__(self\n                 , df\n                 , text_col: str = 'excerpt'\n                 , label_col: str = 'target'\n                 , kaggle_id: str = 'id'\n                 , sample_size: Optional[str] = None\n                ):\n        self.df = df if sample_size is None else df.sample(sample_size)\n        self.text_col = text_col\n        self.label_col = label_col\n        self.kaggle_id = kaggle_id\n        self.num_labels = len(df[label_col].unique()) if label_col in df.columns else None\n        # source: https://www.kaggle.com/justinchae/nn-utils\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        result = None\n        text = self.df.iloc[idx][self.text_col]\n        kaggle_id = self.df.iloc[idx][self.kaggle_id]\n        \n        if 'target' in self.df.columns:\n            target = self.df.iloc[idx][self.label_col]\n            return target, text, kaggle_id     \n        else:\n            return None, text, kaggle_id\n\n\nclass CommonLitDataModule(pl.LightningDataModule):\n    def __init__(self\n                 , tokenizer\n                 , train_path\n                 , collate_fn=None\n                 , max_length: int = 280\n                 , batch_size: int = 16\n                 , valid_path: Optional[str] = None\n                 , test_path: Optional[str] = None\n                 , train_valid_split: float = .6\n                 , dtypes=None\n                 , shuffle_dataloader: bool = True\n                 , num_dataloader_workers: int = NUM_DATALOADER_WORKERS\n                 , kfold: Optional[dict] = None):\n        super(CommonLitDataModule, self).__init__()\n        self.tokenizer = tokenizer\n        self.train_path = train_path\n        self.valid_path = valid_path\n        self.test_path = test_path\n        self.train_valid_split = train_valid_split\n        self.dtypes = {'id': str} if dtypes is None else dtypes\n        self.train_size = None\n        self.train_df, self.train_data = None, None\n        self.valid_df, self.valid_data = None, None\n        self.test_df, self.test_data = None, None\n        if collate_fn is not None:\n            self.collate_fn = partial(collate_fn\n                                      , tokenizer=tokenizer\n                                      , max_length=max_length) \n        else:\n            \n            self.collate_fn = partial(my_collate_fn\n                                      , batch=batch_size\n                                      , tokenizer=tokenizer)\n            \n        self.shuffle_dataloader = shuffle_dataloader\n        self.batch_size = batch_size\n        self.num_dataloader_workers = num_dataloader_workers\n        # refactored from: https://www.kaggle.com/justinchae/nn-utils\n    \n    def _strip_extraneous(self, df):\n        strip_cols = ['url_legal', 'license']\n        if all(col in df.columns for col in strip_cols):\n            extraneous_data = strip_cols\n            return df.drop(columns=extraneous_data)\n        else: \n            return df\n    \n    def prepare(self, prep_type=None):\n        if prep_type == 'train':\n            # creates just an instance of the train data as a pandas df\n            self.train_df = self.train_path if isinstance(self.train_path, pd.DataFrame) else pd.read_csv(self.train_path, dtype=self.dtypes)\n            self.train_df = self._strip_extraneous(self.train_df)\n            \n        if prep_type == 'train_stage_2':\n            self.train_df = self.train_path if isinstance(self.train_path, pd.DataFrame) else pd.read_csv(self.train_path, dtype=self.dtypes)\n            self.train_df = self._strip_extraneous(self.train_df)\n            self.train_size = int(len(self.train_df))\n            self.train_data = CommonLitDataset(df=self.train_df)\n        \n    def setup(self, stage: Optional[str] = None) -> None:\n        if stage == 'fit':\n            # when this class is called by trainer.fit, this stage runs and so on\n            self.train_df = self.train_path if isinstance(self.train_path, pd.DataFrame) else pd.read_csv(self.train_path, dtype=self.dtypes)\n            self.train_df = self._strip_extraneous(self.train_df)\n            self.train_size = int(len(self.train_df))\n            self.train_data = CommonLitDataset(df=self.train_df)\n        \n            if self.train_valid_split is not None and self.valid_path is None:\n                self.train_size = int(len(self.train_df) * self.train_valid_split)\n                self.train_data, self.valid_data = random_split(self.train_data, [self.train_size, len(self.train_df) - self.train_size])\n            elif self.valid_path is not None:\n                self.valid_df = self.valid_path if isinstance(self.valid_path, pd.DataFrame) else pd.read_csv(self.valid_path, dtype=self.dtypes)\n                self.valid_data = CommonLitDataset(df=self.valid_df)\n            \n        if stage == 'predict':           \n            self.test_df = self.test_path if isinstance(self.test_path, pd.DataFrame) else pd.read_csv(self.test_path, dtype=self.dtypes)\n            self.test_df = self._strip_extraneous(self.test_df)\n            self.test_data = CommonLitDataset(df=self.test_df)\n            \n            self.train_df = self.train_path if isinstance(self.train_path, pd.DataFrame) else pd.read_csv(self.train_path, dtype=self.dtypes)\n            self.train_df = self._strip_extraneous(self.train_df)\n            self.train_size = int(len(self.train_df))\n            self.train_data = CommonLitDataset(df=self.train_df)\n    \n    def kfold_data(self):\n        # TODO: wondering how to integrate kfolds into the datamodule\n        pass\n    \n    def train_dataloader(self) -> DataLoader:\n        return DataLoader(self.train_data\n                          , batch_size=self.batch_size\n                          , shuffle=self.shuffle_dataloader\n                          , collate_fn=self.collate_fn\n                          , num_workers=self.num_dataloader_workers\n                          , pin_memory=True\n                          )\n    def val_dataloader(self) -> DataLoader:\n        if self.valid_data is None:\n            return None\n        else:\n            return DataLoader(self.valid_data\n                              , batch_size=self.batch_size\n                              , shuffle=False\n                              , collate_fn=self.collate_fn\n                              , num_workers=self.num_dataloader_workers\n                              , pin_memory=True\n                              )\n    def predict_dataloader(self) -> DataLoader:\n        if self.test_data is None:\n            return None\n        else:\n            return DataLoader(self.test_data\n                              , batch_size=self.batch_size\n                              , shuffle=False\n                              , collate_fn=self.collate_fn\n                              , num_workers=self.num_dataloader_workers\n                              , pin_memory=True\n                              ) ","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:12:03.229743Z","iopub.execute_input":"2021-08-02T12:12:03.23028Z","iopub.status.idle":"2021-08-02T12:12:03.270445Z","shell.execute_reply.started":"2021-08-02T12:12:03.230235Z","shell.execute_reply":"2021-08-02T12:12:03.268547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_textstat_features(df):\n    # adding the text standard seems to boost the accuracy score a bit\n    df['text_standard'] = df['excerpt'].apply(lambda x: textstat.text_standard(x))\n    df['text_standard_category'] = df['text_standard'].astype('category').cat.codes\n    \n    # counting ratio of difficult words by lexicon count\n    df['difficult_words_ratio'] = df['excerpt'].apply(lambda x: textstat.difficult_words(x))\n    df['difficult_words_ratio'] = df.apply(lambda x: x['difficult_words_ratio'] / textstat.lexicon_count(x['excerpt']), axis=1)\n                                           \n    df['syllable_ratio'] = df['excerpt'].apply(lambda x: textstat.syllable_count(x))\n    df['syllable_ratio'] = df.apply(lambda x: x['syllable_ratio'] / textstat.lexicon_count(x['excerpt']), axis=1) \n                                    \n\n    ### You can add/remove any feature below and it will be used in training and test\n    df['coleman_liau_index'] = df['excerpt'].apply(lambda x: textstat.coleman_liau_index(x))\n    df['flesch_reading_ease'] = df['excerpt'].apply(lambda x: textstat.flesch_reading_ease(x))\n    df['smog_index'] = df['excerpt'].apply(lambda x: textstat.smog_index(x))\n    df['gunning_fog'] = df['excerpt'].apply(lambda x: textstat.gunning_fog(x))\n    df['flesch_kincaid_grade'] = df['excerpt'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n    df['automated_readability_index'] = df['excerpt'].apply(lambda x: textstat.automated_readability_index(x))\n    df['dale_chall_readability_score'] = df['excerpt'].apply(lambda x: textstat.dale_chall_readability_score(x))\n    df['linsear_write_formula'] = df['excerpt'].apply(lambda x: textstat.linsear_write_formula(x))\n    ###\n    df = df.drop(columns=['excerpt', 'text_standard'])\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:12:03.272771Z","iopub.execute_input":"2021-08-02T12:12:03.27337Z","iopub.status.idle":"2021-08-02T12:12:03.289179Z","shell.execute_reply.started":"2021-08-02T12:12:03.273324Z","shell.execute_reply":"2021-08-02T12:12:03.287856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_hidden_states(df, drop_hidden_states=False):\n    # for convenience, moving hidden states to the far right of the df\n    if drop_hidden_states:\n        df.drop(columns=['hidden_states'], inplace=True)\n        return df\n    \n    elif \"hidden_states\" in df.columns:\n        df['hidden_state'] = df['hidden_states']\n        df.drop(columns=['hidden_states'], inplace=True)\n\n        temp = df['hidden_state'].apply(pd.Series)\n        temp = temp.rename(columns = lambda x: 'hidden_state_' + str(x))\n        df = pd.concat([df, temp], axis=1)\n        df.drop(columns=['hidden_state'], inplace=True)\n\n        return df\n    else:\n        print(\"hidden_states not found in dataframe, skipping process_hidden_states\")\n        return df","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:12:03.291114Z","iopub.execute_input":"2021-08-02T12:12:03.29162Z","iopub.status.idle":"2021-08-02T12:12:03.314281Z","shell.execute_reply.started":"2021-08-02T12:12:03.291549Z","shell.execute_reply":"2021-08-02T12:12:03.313022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datamodule = CommonLitDataModule(collate_fn=my_collate_fn\n                                         , tokenizer=PRETRAINED_ROBERTA_BASE_TOKENIZER\n                                         , train_path=KAGGLE_TRAIN_PATH\n                                         , test_path=KAGGLE_TEST_PATH\n                                         , max_length=TOKENIZER_MAX_LEN\n                                         , batch_size=BATCH_SIZE\n                                         , train_valid_split=TRAIN_VALID_SPLIT\n                                          )\n# manually calling this stage since we need some params to set up model initially\ndatamodule.setup(stage='fit')\n\nif USE_CHECKPOINT:\n    \n#     model = LitRobertaLogitRegressor.load_from_checkpoint(TUNED_CHECKPOINT_PATH)\n    trainer = pl.Trainer(gpus=ACCELERATOR_TYPE['gpus']\n                     , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                     )\n    \n    model = LitRobertaLogitRegressor(pre_trained_path=TUNED_BEST_ROBERTA_PATH\n                                          , train_size=datamodule.train_size\n                                          , batch_size=datamodule.batch_size\n                                          , output_hidden_states=USE_HIDDEN_IN_RGR\n                                          , n_gpus=ACCELERATOR_TYPE['gpus']\n                                          , accumulate_grad_batches=trainer.accumulate_grad_batches\n                                          , learning_rate=LEARNING_RATE\n                                          , warmup_steps=WARMUP_STEPS\n                                          , max_epochs=MAX_EPOCHS\n                                          , tokenizer=datamodule.tokenizer\n                                          )\n    \n    trainer = pl.Trainer(gpus=ACCELERATOR_TYPE['gpus']\n                     , tpu_cores=ACCELERATOR_TYPE['tpu_cores'])\n    \nelse:\n\n    checkpoint_filename = f'crp_roberta_trial_main'\n    checkpoint_save = ModelCheckpoint(dirpath=CHECKPOINTS_PATH\n                                      , filename=checkpoint_filename\n                                      )\n\n    early_stopping_callback = EarlyStopping(monitor='val_loss'\n                                            , patience=2\n                                            )\n\n    trainer = pl.Trainer(max_epochs=MAX_EPOCHS\n                         , gpus=ACCELERATOR_TYPE['gpus']\n                         , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                         , precision=16 if USE_16_BIT_PRECISION else 32\n                         , default_root_dir=CHECKPOINTS_PATH\n                         , gradient_clip_val=GRADIENT_CLIP_VAL\n                         , stochastic_weight_avg=True\n                         , callbacks=[checkpoint_save\n                                     , early_stopping_callback\n                                     ]\n                         , fast_dev_run=FAST_DEV_RUN\n                        )\n\n    model = LitRobertaLogitRegressor(pre_trained_path=PRETTRAINED_ROBERTA_BASE_MODEL_PATH\n                                          , train_size=datamodule.train_size\n                                          , batch_size=datamodule.batch_size\n                                          , n_gpus=trainer.gpus\n                                          , n_tpus=trainer.tpu_cores\n                                          , max_epochs=trainer.max_epochs\n                                          , accumulate_grad_batches=trainer.accumulate_grad_batches\n                                          , learning_rate=LEARNING_RATE\n                                          , warmup_steps=WARMUP_STEPS\n                                          , tokenizer=datamodule.tokenizer\n                                          )\n\n    trainer.fit(model, datamodule=datamodule)\n\n    # let's also save the tuned roberta state which our model wraps around \n    model_file_name = f\"tuned_roberta_model\"\n    model_file_path = os.path.join(MODEL_PATH, model_file_name)\n    model.save_pretrained(model_file_path)\n\n    # clean up memory\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:12:03.316108Z","iopub.execute_input":"2021-08-02T12:12:03.316673Z","iopub.status.idle":"2021-08-02T12:12:13.689146Z","shell.execute_reply.started":"2021-08-02T12:12:03.316615Z","shell.execute_reply":"2021-08-02T12:12:13.687788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freeze the model for prediction\nmodel.eval()\nmodel.freeze()\ndatamodule.shuffle_dataloader = False\ndatamodule.setup(stage='predict')\n\nmodel.do_decode = True\n\n# run predict on the test data\ntrain_data_stage_20 = trainer.predict(model=model, dataloaders=datamodule.train_dataloader())\ntrain_data_stage_20 = pd.concat(train_data_stage_20).reset_index(drop=True)\n\ntrain_data_stage_20 = pd.merge(left=train_data_stage_20 \n                                , right=datamodule.train_df.drop(columns=['standard_error', 'target'])\n                                , left_on='id'\n                                , right_on='id')\n\nprint(train_data_stage_20)\n# TODO: test whether we need to save and upload the fine-tuned state of roberta or if pytorch lightning checkpoints take care of it all","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:12:13.693613Z","iopub.execute_input":"2021-08-02T12:12:13.694077Z","iopub.status.idle":"2021-08-02T12:14:24.349983Z","shell.execute_reply.started":"2021-08-02T12:12:13.694031Z","shell.execute_reply":"2021-08-02T12:14:24.348824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def free_mem():                          \n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:14:24.353997Z","iopub.execute_input":"2021-08-02T12:14:24.35434Z","iopub.status.idle":"2021-08-02T12:14:24.359629Z","shell.execute_reply.started":"2021-08-02T12:14:24.354307Z","shell.execute_reply":"2021-08-02T12:14:24.357905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"free_mem()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:14:24.361267Z","iopub.execute_input":"2021-08-02T12:14:24.361799Z","iopub.status.idle":"2021-08-02T12:14:24.651042Z","shell.execute_reply.started":"2021-08-02T12:14:24.361751Z","shell.execute_reply":"2021-08-02T12:14:24.649721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply other models\n\ntrainer = pl.Trainer(gpus=ACCELERATOR_TYPE['gpus']\n                 , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                 )\n\nmodel_42 = LitRobertaLogitRegressor(pre_trained_path=MODEL_42\n                                      , train_size=datamodule.train_size\n                                      , batch_size=datamodule.batch_size\n                                      , output_hidden_states=USE_HIDDEN_IN_RGR\n                                      , n_gpus=ACCELERATOR_TYPE['gpus']\n                                      , accumulate_grad_batches=trainer.accumulate_grad_batches\n                                      , learning_rate=LEARNING_RATE\n                                      , warmup_steps=WARMUP_STEPS\n                                      , max_epochs=MAX_EPOCHS\n                                      , tokenizer=datamodule.tokenizer\n                                      )\n\nmodel_42.do_decode = True\n# freeze the model for prediction\nmodel_42.eval()\nmodel_42.freeze()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:14:24.653125Z","iopub.execute_input":"2021-08-02T12:14:24.653631Z","iopub.status.idle":"2021-08-02T12:14:35.400222Z","shell.execute_reply.started":"2021-08-02T12:14:24.653584Z","shell.execute_reply":"2021-08-02T12:14:35.399061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run predict on the test data\ntrain_data_stage_21 = trainer.predict(model=model_42, dataloaders=datamodule.train_dataloader())\ntrain_data_stage_21 = pd.concat(train_data_stage_21).reset_index(drop=True)\ntrain_data_stage_21.rename(columns={'logit': 'logit_42'}, inplace=True)\n\ntrain_data_stage_two = pd.merge(left=train_data_stage_20 \n                                , right=train_data_stage_21[['id', 'logit_42']]\n                                , left_on='id'\n                                , right_on='id')\n\nprint(train_data_stage_two)\n# TODO: test whether we need to save and upload the fine-tuned state of roberta or if pytorch lightning checkpoints take care of it all","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:14:35.401912Z","iopub.execute_input":"2021-08-02T12:14:35.402375Z","iopub.status.idle":"2021-08-02T12:16:38.094888Z","shell.execute_reply.started":"2021-08-02T12:14:35.402332Z","shell.execute_reply":"2021-08-02T12:16:38.093752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"free_mem()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:16:38.098221Z","iopub.execute_input":"2021-08-02T12:16:38.098556Z","iopub.status.idle":"2021-08-02T12:16:38.389385Z","shell.execute_reply.started":"2021-08-02T12:16:38.098523Z","shell.execute_reply":"2021-08-02T12:16:38.388064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply two other models\n\ntrainer = pl.Trainer(gpus=ACCELERATOR_TYPE['gpus']\n                 , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                 )\n\nmodel_0 = LitRobertaLogitRegressor(pre_trained_path=MODEL_0\n                                      , train_size=datamodule.train_size\n                                      , batch_size=datamodule.batch_size\n                                      , output_hidden_states=USE_HIDDEN_IN_RGR\n                                      , n_gpus=ACCELERATOR_TYPE['gpus']\n                                      , accumulate_grad_batches=trainer.accumulate_grad_batches\n                                      , learning_rate=LEARNING_RATE\n                                      , warmup_steps=WARMUP_STEPS\n                                      , max_epochs=MAX_EPOCHS\n                                      , tokenizer=datamodule.tokenizer\n                                      )\n\nmodel_0.do_decode = True\n# freeze the model for prediction\nmodel_0.eval()\nmodel_0.freeze()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:16:38.391168Z","iopub.execute_input":"2021-08-02T12:16:38.391645Z","iopub.status.idle":"2021-08-02T12:16:49.059713Z","shell.execute_reply.started":"2021-08-02T12:16:38.391584Z","shell.execute_reply":"2021-08-02T12:16:49.058556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run predict on the test data\ntrain_data_stage_22 = trainer.predict(model=model_0, dataloaders=datamodule.train_dataloader())\ntrain_data_stage_22 = pd.concat(train_data_stage_22).reset_index(drop=True)\ntrain_data_stage_22.rename(columns={'logit': 'logit_0'}, inplace=True)\n\ntrain_data_stage_two = pd.merge(left=train_data_stage_two \n                                , right=train_data_stage_22[['id', 'logit_0']]\n                                , left_on='id'\n                                , right_on='id')\n\nprint(train_data_stage_two)\n# TODO: test whether we need to save and upload the fine-tuned state of roberta or if pytorch lightning checkpoints take care of it all","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:16:49.061318Z","iopub.execute_input":"2021-08-02T12:16:49.061803Z","iopub.status.idle":"2021-08-02T12:18:51.095386Z","shell.execute_reply.started":"2021-08-02T12:16:49.06176Z","shell.execute_reply":"2021-08-02T12:18:51.093904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"free_mem()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:18:51.097428Z","iopub.execute_input":"2021-08-02T12:18:51.097913Z","iopub.status.idle":"2021-08-02T12:18:51.392293Z","shell.execute_reply.started":"2021-08-02T12:18:51.097863Z","shell.execute_reply":"2021-08-02T12:18:51.390054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply two other models\n\ntrainer = pl.Trainer(gpus=ACCELERATOR_TYPE['gpus']\n                 , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                 )\n\nmodel_21 = LitRobertaLogitRegressor(pre_trained_path=MODEL_21\n                                      , train_size=datamodule.train_size\n                                      , batch_size=datamodule.batch_size\n                                      , output_hidden_states=USE_HIDDEN_IN_RGR\n                                      , n_gpus=ACCELERATOR_TYPE['gpus']\n                                      , accumulate_grad_batches=trainer.accumulate_grad_batches\n                                      , learning_rate=LEARNING_RATE\n                                      , warmup_steps=WARMUP_STEPS\n                                      , max_epochs=MAX_EPOCHS\n                                      , tokenizer=datamodule.tokenizer\n                                      )\n\nmodel_21.do_decode = True\n# freeze the model for prediction\nmodel_21.eval()\nmodel_21.freeze()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:18:51.394557Z","iopub.execute_input":"2021-08-02T12:18:51.395183Z","iopub.status.idle":"2021-08-02T12:19:02.153698Z","shell.execute_reply.started":"2021-08-02T12:18:51.395134Z","shell.execute_reply":"2021-08-02T12:19:02.152432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run predict on the test data\ntrain_data_stage_23 = trainer.predict(model=model_21, dataloaders=datamodule.train_dataloader())\ntrain_data_stage_23 = pd.concat(train_data_stage_23).reset_index(drop=True)\ntrain_data_stage_23.rename(columns={'logit': 'logit_21'}, inplace=True)\n\ntrain_data_stage_two = pd.merge(left=train_data_stage_two \n                                , right=train_data_stage_23[['id', 'logit_21']]\n                                , left_on='id'\n                                , right_on='id')\n\nprint(train_data_stage_two)\n# TODO: test whether we need to save and upload the fine-tuned state of roberta or if pytorch lightning checkpoints take care of it all","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:19:02.155368Z","iopub.execute_input":"2021-08-02T12:19:02.155836Z","iopub.status.idle":"2021-08-02T12:21:04.374777Z","shell.execute_reply.started":"2021-08-02T12:19:02.155789Z","shell.execute_reply":"2021-08-02T12:21:04.373464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"free_mem()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:21:04.377147Z","iopub.execute_input":"2021-08-02T12:21:04.377475Z","iopub.status.idle":"2021-08-02T12:21:04.779626Z","shell.execute_reply.started":"2021-08-02T12:21:04.377444Z","shell.execute_reply":"2021-08-02T12:21:04.778279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_stage_three = add_textstat_features(train_data_stage_two)\n\nlabel_data = train_data_stage_three[['id']].copy(deep=True)\n\ntrain_data = train_data_stage_three.drop(columns=['id', 'target', 'text_standard_category']).copy(deep=True)\n\ntrain_data_cols =  list(train_data.columns)\n\ntarget_data = train_data_stage_three[['target']].copy(deep=True)\n\nscaler = StandardScaler()\ntrain_data_scaled = scaler.fit_transform(train_data)\ntrain_data_scaled = pd.DataFrame(train_data_scaled, columns=train_data_cols)\n\nTARGET_SCALER = StandardScaler()\ntarget_data_scaled = TARGET_SCALER.fit_transform(target_data)\ntarget_data_scaled = pd.DataFrame(target_data_scaled, columns=['target'])\n\nregr = SVR(kernel='linear')\nregr.fit(train_data_scaled, target_data_scaled['target'])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:21:04.781564Z","iopub.execute_input":"2021-08-02T12:21:04.781998Z","iopub.status.idle":"2021-08-02T12:21:26.624892Z","shell.execute_reply.started":"2021-08-02T12:21:04.781955Z","shell.execute_reply":"2021-08-02T12:21:26.62361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"   Assessment of Features   \")\nprint(\"R2 Score: \", regr.score(train_data_scaled, target_data_scaled['target']))\nprint(\"RSME Score: \", math.sqrt(mean_squared_error(target_data_scaled['target'], regr.predict(train_data_scaled))))\n\n# regr.coef_ is a array of n, 1\nfeats_coef = list(zip(train_data_cols, regr.coef_[0]))\nfeature_analysis = pd.DataFrame(feats_coef\n                               , columns=['feature_col', 'coef_val'])\n\nfeature_analysis['coef_val'] = feature_analysis['coef_val']#.abs()\nfeature_analysis = feature_analysis.sort_values('coef_val',ascending = False)\n\nfeature_analysis.plot.barh(x='feature_col', y='coef_val', title=\"Comparison of Features and Importance\")\n\n# select the top n features for use in final regression approach\nbest_n_features = feature_analysis.head(N_FEATURES_TO_USE_HEAD)['feature_col'].to_list()\n# the opposite\nif N_FEATURES_TO_USE_TAIL is not None:\n    worst_n_features = feature_analysis.tail(N_FEATURES_TO_USE_TAIL)['feature_col'].to_list()\n    best_n_features.extend(worst_n_features)\n\n# manually adding this categorical feature in\nif 'text_standard_category' not in best_n_features:\n    best_n_features.append('text_standard_category')\n    \n# manually adding this categorical feature in\nif 'logit_0' not in best_n_features:\n    best_n_features.append('logit_0')\n    \n# manually adding this categorical feature in\nif 'logit_21' not in best_n_features:\n    best_n_features.append('logit_21')\n\nbest_n_features = list(set(best_n_features))\ntrain_data = train_data_stage_three[best_n_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:29:44.023381Z","iopub.execute_input":"2021-08-02T12:29:44.023849Z","iopub.status.idle":"2021-08-02T12:29:44.831541Z","shell.execute_reply.started":"2021-08-02T12:29:44.023819Z","shell.execute_reply":"2021-08-02T12:29:44.830378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET = train_data.copy(deep=True)\nDATASET['target'] = target_data_scaled['target']\nDATASET['id'] = label_data['id']\n\nif 'text_standard_category' in best_n_features:\n    drop_cols = ['id', 'target', 'text_standard_category']\nelse:\n    drop_cols = ['id', 'target']\n    \ntemp_cols = list(DATASET.drop(columns=drop_cols).columns)\n\n\nDATASET_scaled = DATASET[temp_cols]\n\nscaler = StandardScaler()\nDATASET_scaled = scaler.fit_transform(DATASET_scaled)\nDATASET_scaled = pd.DataFrame(DATASET_scaled, columns=temp_cols)\n\nDATASET_scaled[drop_cols] = DATASET[drop_cols] \nprint(DATASET_scaled)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:29:44.83333Z","iopub.execute_input":"2021-08-02T12:29:44.833625Z","iopub.status.idle":"2021-08-02T12:29:44.867258Z","shell.execute_reply.started":"2021-08-02T12:29:44.833595Z","shell.execute_reply":"2021-08-02T12:29:44.866196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258\n# https://www.kaggle.com/corochann/optuna-tutorial-for-hyperparameter-optimization\n\nRGR_MODELS = []\n\ndef objective(trial: optuna.trial.Trial\n              , n_folds=10\n              , shuffle=True\n             ):\n    \n    params = {'metric': 'rmse'\n              , 'boosting_type': 'gbdt'\n              , 'verbose': -1\n              , 'num_leaves': trial.suggest_int('num_leaves', 4, 512)\n              , 'max_depth': trial.suggest_int('max_depth', 4, 512)\n              , 'max_bin': trial.suggest_int('max_bin', 4, 512)\n              , 'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 64, 512)\n              , \"bagging_fraction\": trial.suggest_uniform('bagging_fraction', 0.1, 1.0)\n              , \"bagging_freq\": trial.suggest_int('max_bin', 5, 10)\n              , \"feature_fraction\": trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n              , 'learning_rate': trial.suggest_float(\"bagging_fraction\", .0005, .01)\n              , 'n_estimators': trial.suggest_int('num_leaves', 10, 10000)\n              , 'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0)\n              , 'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0)\n             }\n     \n    fold = KFold(n_splits=n_folds\n               , shuffle=shuffle\n               , random_state=SEED_VAL if shuffle else None\n                )\n    \n    valid_score = 0\n    \n    best_model_tracker = {}\n\n    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(range(len(DATASET_scaled)))):\n        train_data = DATASET_scaled.iloc[train_idx].drop(columns=['id', 'target']).copy(deep=True)\n        train_target = DATASET_scaled[['target']].iloc[train_idx].copy(deep=True)\n\n        valid_data = DATASET_scaled.iloc[valid_idx].drop(columns=['id', 'target']).copy(deep=True)\n        valid_target = DATASET_scaled[['target']].iloc[valid_idx].copy(deep=True)\n\n        lgbm_train = lgm.Dataset(train_data, label=train_target\n                                 , categorical_feature=['text_standard_category'] if 'text_standard_category' in best_n_features else None\n                                )\n        lgbm_valid = lgm.Dataset(valid_data, label=valid_target\n                                 , categorical_feature=['text_standard_category'] if 'text_standard_category' in best_n_features else None\n                                )\n        \n        curr_model = lgm.train(params,\n                          train_set=lgbm_train,\n                          valid_sets=[lgbm_train, lgbm_valid],\n                          verbose_eval=-1,\n                         )\n        \n        valid_pred = curr_model.predict(valid_data, num_iteration=curr_model.best_iteration)\n        \n        best_score = curr_model.best_score['valid_1']['rmse']\n        \n        best_model_tracker.update({best_score: curr_model})\n            \n        valid_score += best_score\n    \n#     best_model_score = min([k for k, v in best_model_tracker.items()])\n#     best_model = best_model_tracker[best_model_score]\n\n#     RGR_MODELS.append(best_model) \n    \n#     RGR_MODELS.append({best_model_score: best_model})\n    \n#     worst_rgr_model_idx = max([d.keys[0] for d in RGR_MODELS])\n    \n#     RGR_MODELS[worst_rgr_model_idx] = {best_model_score: None}\n    \n    score = valid_score / n_folds\n    return score","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:29:44.870493Z","iopub.execute_input":"2021-08-02T12:29:44.870918Z","iopub.status.idle":"2021-08-02T12:29:44.938924Z","shell.execute_reply.started":"2021-08-02T12:29:44.870879Z","shell.execute_reply":"2021-08-02T12:29:44.937403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(storage='sqlite:///lgm-study.db')\nstudy.optimize(objective, n_trials=256)\n\nplot_optimization_history(study).show()\nprint(\"Best Trial: \", study.best_trial, '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:29:44.941147Z","iopub.execute_input":"2021-08-02T12:29:44.941671Z","iopub.status.idle":"2021-08-02T12:35:34.524271Z","shell.execute_reply.started":"2021-08-02T12:29:44.94162Z","shell.execute_reply":"2021-08-02T12:35:34.522673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use the study parameters to create and train a lgbm regressor\nlgm_train_data = DATASET_scaled.drop(columns=['id']).copy(deep=True)\nx_features = lgm_train_data.loc[:, lgm_train_data.columns != 'target']\ny_train = lgm_train_data[['target']]\n\nlgm_train_set_full = lgm.Dataset(data=x_features\n                                 , categorical_feature=['text_standard_category'] if 'text_standard_category' in best_n_features else None\n                                 , label=y_train)\n\ngbm = lgm.train(study.best_trial.params,\n                lgm_train_set_full,\n                )","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:35:34.526061Z","iopub.execute_input":"2021-08-02T12:35:34.526819Z","iopub.status.idle":"2021-08-02T12:35:34.654315Z","shell.execute_reply.started":"2021-08-02T12:35:34.526766Z","shell.execute_reply":"2021-08-02T12:35:34.653037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.do_decode = True\n\ntrainer = pl.Trainer(gpus=ACCELERATOR_TYPE['gpus']\n                     , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                     )\n\n# run predict on the test data\nsubmission_stage_10 = trainer.predict(model=model\n                                     , dataloaders=datamodule.predict_dataloader())\n\nsubmission_stage_10 = pd.concat(submission_stage_10).reset_index(drop=True)\nprint(\"   Submission Stage 10: After RoBERTA\\n\")\nprint(submission_stage_10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:35:34.657183Z","iopub.execute_input":"2021-08-02T12:35:34.657598Z","iopub.status.idle":"2021-08-02T12:36:14.590874Z","shell.execute_reply.started":"2021-08-02T12:35:34.657556Z","shell.execute_reply":"2021-08-02T12:36:14.589135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 'logit_42' in best_n_features:\n    model_42.do_decode = True\n\n    trainer = pl.Trainer(gpus=ACCELERATOR_TYPE['gpus']\n                         , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                         )\n\n    # run predict on the test data\n    submission_stage_11 = trainer.predict(model=model_42\n                                         , dataloaders=datamodule.predict_dataloader())\n\n    submission_stage_11 = pd.concat(submission_stage_11).reset_index(drop=True)\n    submission_stage_11.rename(columns={'logit': 'logit_42'}, inplace=True)\n\n    submission_stage_1 = pd.merge(left=submission_stage_10\n                                 , right=submission_stage_11\n                                 , left_on='id'\n                                 , right_on='id'\n                                 , how='left')\n\n    print(\"   Submission Stage 11: After RoBERTA\\n\")\n    print(submission_stage_1)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:36:14.593054Z","iopub.execute_input":"2021-08-02T12:36:14.593547Z","iopub.status.idle":"2021-08-02T12:36:54.217944Z","shell.execute_reply.started":"2021-08-02T12:36:14.593497Z","shell.execute_reply":"2021-08-02T12:36:54.214472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if \"logit_0\" in best_n_features:\n\n    model_0.do_decode = True\n\n    trainer = pl.Trainer(gpus=ACCELERATOR_TYPE['gpus']\n                         , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                         )\n\n    # run predict on the test data\n    submission_stage_12 = trainer.predict(model=model_0\n                                         , dataloaders=datamodule.predict_dataloader())\n\n    submission_stage_12 = pd.concat(submission_stage_12).reset_index(drop=True)\n    submission_stage_12.rename(columns={'logit': 'logit_0'}, inplace=True)\n\n    submission_stage_1 = pd.merge(left=submission_stage_1\n                                 , right=submission_stage_12\n                                 , left_on='id'\n                                 , right_on='id'\n                                 , how='left')\n\n    print(\"   Submission Stage 12: After RoBERTA\\n\")\n    print(submission_stage_1)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:36:54.221857Z","iopub.execute_input":"2021-08-02T12:36:54.22235Z","iopub.status.idle":"2021-08-02T12:36:54.235646Z","shell.execute_reply.started":"2021-08-02T12:36:54.222305Z","shell.execute_reply":"2021-08-02T12:36:54.23439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if \"logit_21\" in best_n_features:\n    model_21.do_decode = True\n\n    trainer = pl.Trainer(gpus=ACCELERATOR_TYPE['gpus']\n                         , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                         )\n\n    # run predict on the test data\n    submission_stage_13 = trainer.predict(model=model_21\n                                         , dataloaders=datamodule.predict_dataloader())\n\n    submission_stage_13 = pd.concat(submission_stage_13).reset_index(drop=True)\n    submission_stage_13.rename(columns={'logit': 'logit_21'}, inplace=True)\n\n    submission_stage_1 = pd.merge(left=submission_stage_1\n                                 , right=submission_stage_13\n                                 , left_on='id'\n                                 , right_on='id'\n                                 , how='left')\n\n    print(\"   Submission Stage 13: After RoBERTA\\n\")\n    print(submission_stage_1)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:36:54.237707Z","iopub.execute_input":"2021-08-02T12:36:54.238463Z","iopub.status.idle":"2021-08-02T12:36:54.257892Z","shell.execute_reply.started":"2021-08-02T12:36:54.23842Z","shell.execute_reply":"2021-08-02T12:36:54.256531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_stage_2 = pd.merge(left=submission_stage_1\n                             , right=datamodule.test_df\n                             , left_on='id'\n                             , right_on='id'\n                             , how='left')\n\nsubmission_stage_2 = add_textstat_features(submission_stage_2)\n\nfeature_cols = list(submission_stage_2.drop(columns=['id']).copy(deep=True).columns)\n\npredict_data = submission_stage_2.drop(columns=['id']).copy(deep=True)\npredict_data = predict_data[best_n_features]\n\nif 'text_standard_category' in best_n_features:\n    temp_cols = list(predict_data.drop(columns=['text_standard_category']).columns)\n\npredict_data_scaled = predict_data[temp_cols]\npredict_data_scaled = scaler.transform(predict_data_scaled)\npredict_data_scaled = pd.DataFrame(predict_data_scaled, columns=temp_cols)\nif 'text_standard_category' in best_n_features:\n    predict_data_scaled['text_standard_category'] = predict_data['text_standard_category']","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:36:54.261775Z","iopub.execute_input":"2021-08-02T12:36:54.263357Z","iopub.status.idle":"2021-08-02T12:36:54.356998Z","shell.execute_reply.started":"2021-08-02T12:36:54.263313Z","shell.execute_reply":"2021-08-02T12:36:54.355569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = submission_stage_2[['id']].copy(deep=True)\n\nsubmission['target'] = gbm.predict(predict_data_scaled)\nsubmission['target'] = TARGET_SCALER.inverse_transform(submission['target'])\n\nprint(\"   Final Stage After LGBM\\n\")\nprint(submission)\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:36:54.363249Z","iopub.execute_input":"2021-08-02T12:36:54.366072Z","iopub.status.idle":"2021-08-02T12:36:54.415864Z","shell.execute_reply.started":"2021-08-02T12:36:54.366018Z","shell.execute_reply":"2021-08-02T12:36:54.414694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Helpful Resources\n\n* Optuna Docs: [https://optuna.readthedocs.io/en/stable/index.html](https://optuna.readthedocs.io/en/stable/index.html)\n\n* PyTorch Lightning Docs: [https://pytorch-lightning.readthedocs.io/en/latest/](https://pytorch-lightning.readthedocs.io/en/latest/)\n\n* For learning rate tuning: [https://medium.com/pytorch/using-optuna-to-optimize-pytorch-hyperparameters-990607385e36](https://medium.com/pytorch/using-optuna-to-optimize-pytorch-hyperparameters-990607385e36)\n\n* For PyTorch Lightning Precision: [https://pytorch-lightning.readthedocs.io/en/stable/advanced/amp.html](https://pytorch-lightning.readthedocs.io/en/stable/advanced/amp.html)\n\n* For PyTorch Lightning Early Stopping: [https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html](https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html)\n\n* For PyTorch Lightning Checkpointing: [https://pytorch-lightning.readthedocs.io/en/stable/common/weights_loading.html](https://pytorch-lightning.readthedocs.io/en/stable/common/weights_loading.html)\n\n* BERT Example from PyTorch Lighting: [https://pytorch-lightning.readthedocs.io/en/stable/advanced/transfer_learning.html](https://pytorch-lightning.readthedocs.io/en/stable/advanced/transfer_learning.html)\n\n* Fine-Tuning a Transformer from PyTorch Lightning: [https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html)\n\n* Example of Optuna with PyTorch Lightning: [https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_lightning_simple.py](https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_lightning_simple.py)\n\n* For PyTorch Lightning Logging: [https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html](https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html)\n\n* For Predict Mode with PyTorch Lightning: [https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html](https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html)\n\n* Restoring Checkpoints and Continuing Training: [https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html?highlight=checkpoint#checkpoint-loading](https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html?highlight=checkpoint#checkpoint-loading)\n\n* Gradient Clipping in PyTorch Lightning: [https://pytorch-lightning.readthedocs.io/en/stable/advanced/training_tricks.html?highlight=memory#advanced-gpu-optimizations](https://pytorch-lightning.readthedocs.io/en/stable/advanced/training_tricks.html?highlight=memory#advanced-gpu-optimizations)\n\n* How to approach trial suggestions in Optuna: [https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html?highlight=suggest#optuna.trial.Trial.suggest_int](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html?highlight=suggest#optuna.trial.Trial.suggest_int)\n\n* Guidance on Early Stopping Callbacks with PyTorch Lightning: [https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.callbacks.early_stopping.html#pytorch_lightning.callbacks.early_stopping.EarlyStopping](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.callbacks.early_stopping.html#pytorch_lightning.callbacks.early_stopping.EarlyStopping)\n\n* For Reproducible Optuna Studies: [https://optuna.readthedocs.io/en/stable/faq.html#how-can-i-obtain-reproducible-optimization-results](https://optuna.readthedocs.io/en/stable/faq.html#how-can-i-obtain-reproducible-optimization-results)\n\n* Guidance on Optuna Pruners: [https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.HyperbandPruner.html#optuna.pruners.HyperbandPruner](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.HyperbandPruner.html#optuna.pruners.HyperbandPruner)\n\n* More guidance on which Optuna Pruners to use based on ML task: [https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html?highlight=memory#activating-pruners](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html?highlight=memory#activating-pruners)\n\n* TPUs [https://www.kaggle.com/justusschock/pytorch-on-tpu-with-pytorch-lightning](https://www.kaggle.com/justusschock/pytorch-on-tpu-with-pytorch-lightning)\n\n* For Neptune to PyTorch Lightning Integration: [https://docs.neptune.ai/integrations-and-supported-tools/model-training/pytorch-lightning](https://docs.neptune.ai/integrations-and-supported-tools/model-training/pytorch-lightning)","metadata":{}}]}