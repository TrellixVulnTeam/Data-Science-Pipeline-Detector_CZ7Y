{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# End to End Example: Fine Tuning RoBERTa for Sequence Classification with PyTorch Lightning and Optuna","metadata":{}},{"cell_type":"markdown","source":"## Why\n### I needed to learn how to optimize and organize networks with greater efficiency. To that end, I put together this project to share some generic learnings on optimizing deep networks with Optuna and PyTorch Lightning based on the [CommonLit Readability Prize](https://www.kaggle.com/c/commonlitreadabilityprize) data. To really learn these tools, I went through some pain to refactor a kernel that I originally created for the same competition [from here](https://www.kaggle.com/justinchae/crp-regression-with-roberta-and-lightgbm). Specifically, in this kernel, I leverage some neat code organization with [PyTorch Lightning](https://www.pytorchlightning.ai/) and efficiency in automating the tuning process with [Optuna](https://optuna.org/). Although this kernel is nlp-centric, it should demonstrate the general framework for fine-tuning hyperparamters in a transfer learning approach to deep learning.\n\n## You may be interested in this kernel if...\n\n### - You are in the CommonLit Readability Competition and need some help fine tuning your networks\n### - You want to know how to refactor a PyTorch project into a PyTorch Lightning Project\n### - You need to see some working examples of fine tuning a BERT model with PyTorch Lightning\n### - You are looking for some integrated project samples that combine Optuna with PyTorch Lightning on an open dataset\n### - You want to combine k-fold cross validation with PyTorch-Lightining and Optuna optimization\n\n## Bottom Line Results\n\n### With all other things equal, after I learned the optimal parameters by running this kernel, I applied them to a prior submission that scored .497 on the CommonLit Public Leaderboard. The Results? After applying the optimal hyperparameters from this Optuna-PyTorch Lightning integration, the public score of my [prior kernel improved](https://www.kaggle.com/justinchae/crp-regression-with-roberta-and-lightgbm) from .497 to .491. Although a gain of .006 is not a huge number, it still shows the utility of being able to automate the hyperparameter tuning process to improve a model's peformance without fundamentally doing something different with your data. For context, the gain in performance is about 1%, as a result, the question to consider how much value a 1% gain in model performance represents to your specific problem set. I should note that since I created this kernel to learn how to use new tools, the kernel does not perform at a comptetive level on its own with a public score of about .52; instead, this forms a solid foundation to search for hyperparameters and gain a slight edge as part of a bigger prediction scheme.","metadata":{}},{"cell_type":"code","source":"%%capture\n\n# install necessary libraries from input\n# import progressbar library for offline usage\n!ls ../input/progresbar2local\n!pip install progressbar2 --no-index --find-links=file:///kaggle/input/progresbar2local/progressbar2\n\n# import text stat library for additional ml data prep\n!ls ../input/textstat-local\n!pip install textstat --no-index --find-links=file:///kaggle/input/textstat-local/textstat ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set to 16 bit precision to cut compute requirements/increase batch size capacity\nUSE_16_BIT_PRECISION = True\n# set a seed value for consistent experimentation; optional, else leave as None\nSEED_VAL = 42\n# set a train-validation split, .7 means 70% of train data and 30% to validation set\nTRAIN_VALID_SPLIT = .8\n# set some hyperparameters as global variables here\nN_OPTUNA_TRIALS = 12\nMAX_EPOCHS = 4\nBATCH_SIZE = 16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if running with TPU, uncomment this cell and run to install \n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev\n# TODO: make it easy to toggle between gpu and tpu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kaggle_config\nfrom kaggle_config import (WORKFLOW_ROOT, DATA_PATH, CACHE_PATH, FIG_PATH, \n                           MODEL_PATH, ANALYSIS_PATH, KAGGLE_INPUT, \n                           CHECKPOINTS_PATH, LOGS_PATH)\n\nINPUTS, DEVICE = kaggle_config.run()\nKAGGLE_TRAIN_PATH = kaggle_config.get_train_path(INPUTS)\nKAGGLE_TEST_PATH = kaggle_config.get_test_path(INPUTS)\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers as pl_loggers\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.tuner.batch_size_scaling import scale_batch_size\nfrom pytorch_lightning.tuner.lr_finder import _LRFinder, lr_find\n\nimport torchmetrics\n\nimport optuna\nfrom optuna.integration import PyTorchLightningPruningCallback\nfrom optuna.samplers import TPESampler, RandomSampler, CmaEsSampler\nfrom optuna.visualization import (plot_intermediate_values\n                                  , plot_optimization_history\n                                  , plot_param_importances)\n\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataset import random_split\n\nimport tensorflow as tf\n\nfrom transformers import (RobertaForSequenceClassification\n                          , RobertaTokenizer\n                          , AdamW\n                          , get_linear_schedule_with_warmup)\n\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport gc\nfrom functools import partial\n\nfrom typing import List, Dict\nfrom typing import Optional\nfrom argparse import ArgumentParser\n\nimport random\n\nif SEED_VAL:\n    random.seed(SEED_VAL)\n    np.random.seed(SEED_VAL)\n    seed_everything(SEED_VAL)\n    \nNUM_DATALOADER_WORKERS = os.cpu_count()\n\ntry: \n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    n_tpus = len(tf.config.list_logical_devices('TPU'))\nexcept ValueError:\n    n_tpus = 0\n\nACCELERATOR_TYPE = {}\nACCELERATOR_TYPE.update({'gpus': torch.cuda.device_count() if torch.cuda.is_available() else None})\nACCELERATOR_TYPE.update({'tpu_cores': n_tpus if n_tpus > 0 else None})\n# still debugging how to best toggle between tpu and gpu; there's too much code to configure to work simply\nprint(\"ACCELERATOR_TYPE:\\n\", ACCELERATOR_TYPE)\n\nPRETTRAINED_ROBERTA_BASE_MODEL_PATH = \"/kaggle/input/pre-trained-roberta-base\"\nPRETRAINED_ROBERTA_BASE_TOKENIZER_PATH = \"/kaggle/input/tokenizer-roberta\"\nPRETRAINED_ROBERTA_BASE_TOKENIZER = RobertaTokenizer.from_pretrained(PRETRAINED_ROBERTA_BASE_TOKENIZER_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%capture\n# !pip install 'neptune-client[pytorch-lightning]'\n# !pip install neptune-client\n# from neptune.new.integrations.pytorch_lightning import NeptuneLogger\n# from pytorch_lightning.loggers.neptune import NeptuneLogger\n\n# neptune_api_token = \"\"\"<token>\"\"\"\n# neptune_project=\"\"\"justinhchae/kaggle-crp-pytorchlightning-plus-optuna\"\"\"\n# neptune_name='test-run'\n\n# NEPTUNE_LOGGER = NeptuneLogger(api_token=neptune_api_token\n#                                , project=neptune_project\n#                                , name=neptune_name)\n\n# debugging this neptune implementation; seems to have compatibility issues","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Implementing Lightning instead of torch.nn.Module\n\"\"\"\nclass LitRobertaLogitRegressor(pl.LightningModule):\n    def __init__(self, pre_trained_path: str\n                     , output_hidden_states: bool = False\n                     , num_labels: int = 1\n                     , layer_1_output_size: int = 64\n                     , layer_2_output_size: int = 1\n                     , learning_rate: float = 1e-5\n                     , task_name: Optional[str] = None\n                     , warmup_steps: int = 100\n                     , weight_decay: float = 0.0\n                     , adam_epsilon: float = 1e-8\n                     , batch_size: Optional[int] = None\n                     , train_size: Optional[int] = None\n                     , max_epochs: Optional[int] = None\n                     , n_gpus: Optional[int] = 0\n                     , n_tpus: Optional[int] = 0 \n                     , accumulate_grad_batches = None\n                ):\n        \"\"\"refactored from: https://www.kaggle.com/justinchae/my-bert-tuner and https://www.kaggle.com/justinchae/roberta-tuner\n        \"\"\"\n        super(LitRobertaLogitRegressor, self).__init__()\n        \n        # this saves class params as self.hparams\n        self.save_hyperparameters()\n        \n        self.model = RobertaForSequenceClassification.from_pretrained(self.hparams.pre_trained_path\n                                                                        , output_hidden_states=self.hparams.output_hidden_states\n                                                                        , num_labels=self.hparams.num_labels\n                                                                        )\n\n        self.accelerator_multiplier = n_gpus if n_gpus > 0 else 1\n        \n        self.config = self.model.config\n        self.parameters = self.model.parameters\n        self.save_pretrained = self.model.save_pretrained\n        # these layers are not currently used, tbd in future iteration\n        self.layer_1 = torch.nn.Linear(768, layer_1_output_size)\n        self.layer_2 = torch.nn.Linear(layer_1_output_size, layer_2_output_size)\n        \n        def rmse_loss(x, y):\n            criterion = F.mse_loss\n            loss = torch.sqrt(criterion(x, y))\n            return loss\n        \n        # TODO: enable toggle for various loss funcs and torchmetrics package\n        self.loss_func = rmse_loss\n#         self.eval_func = rmse_loss   \n        \n    def setup(self, stage=None) -> None:\n        if stage == 'fit':\n            # when this class is called by trainer.fit, this stage runs and so on\n            # Calculate total steps\n            tb_size = self.hparams.batch_size * self.accelerator_multiplier\n            ab_size = self.hparams.accumulate_grad_batches * float(self.hparams.max_epochs)\n            self.total_steps = (self.hparams.train_size // tb_size) // ab_size\n        \n    def extract_logit_only(self, input_ids, attention_mask) -> float:\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        logit = output.logits\n        logit = logit.cpu().numpy().astype(float)\n        return logit\n    \n    def extract_hidden_only(self, input_ids, attention_mask) -> np.array:\n        output = self.model(input_ids=input_ids, attention_mask=input_ids)\n        hidden_states = output.hidden_states\n        x = torch.stack(hidden_states[-4:]).sum(0)\n        m1 = torch.nn.Sequential(self.layer_1\n                                 , self.layer_2\n                                 , torch.nn.Flatten())\n        x = m1(x)\n        x = torch.squeeze(x).cpu().numpy()\n        \n        return x\n        \n    def forward(self, input_ids, attention_mask) -> torch.Tensor:\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        x = output.logits\n        return x\n    \n    def training_step(self, batch, batch_idx: int) -> float:\n        # refactored from: https://www.kaggle.com/justinchae/epoch-utils\n        labels, encoded_batch, kaggle_ids = batch\n        input_ids = encoded_batch['input_ids']\n        attention_mask = encoded_batch['attention_mask']\n        # per docs, keep train step separate from forward call\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        y_hat = output.logits\n        # quick reshape to align labels to predictions\n        labels = labels.view(-1, 1)\n        loss = self.loss_func(y_hat, labels)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx: int) -> float:\n        # refactored from: https://www.kaggle.com/justinchae/epoch-utils\n        labels, encoded_batch, kaggle_ids = batch\n        input_ids = encoded_batch['input_ids']\n        attention_mask = encoded_batch['attention_mask']\n        # this self call is calling the forward method\n        y_hat = self(input_ids, attention_mask)\n        # quick reshape to align labels to predictions\n        labels = labels.view(-1, 1)\n        loss = self.loss_func(y_hat, labels)\n        self.log('val_loss', loss)\n        return loss\n    \n    def predict(self, batch, batch_idx: int, dataloader_idx: int = None):\n        # creating this predict method overrides the pl predict method\n        _, encoded_batch, kaggle_ids = batch\n        \n        input_ids = encoded_batch['input_ids']\n        attention_mask = encoded_batch['attention_mask']\n        # this self call is calling the forward method\n        y_hat = self(input_ids, attention_mask)\n        # convert to numpy then list like struct to zip with ids\n        y_hat = y_hat.cpu().numpy().ravel()\n        # customizing the predict behavior to account for unique ids\n        predictions = list(zip(kaggle_ids, y_hat))\n        predictions = pd.DataFrame(predictions, columns=['id', 'target'])\n        \n        return predictions\n    \n    def configure_optimizers(self) -> torch.optim.Optimizer:\n        # Reference: https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html\n        model = self.model\n        \n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        \n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": self.hparams.weight_decay,\n            },\n            {\n                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        \n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n        \n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.total_steps\n        )\n        scheduler = {'scheduler': scheduler, 'interval': 'step', 'frequency': 1}\n        \n        return [optimizer], [scheduler]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_collate_fn(batch\n                 , tokenizer\n                 , max_length: int = 100\n                 , return_tensors: str = 'pt'\n                 , padding: str = \"max_length\"\n                 , truncation: bool = True\n                 ):\n    # source: https://www.kaggle.com/justinchae/nn-utils\n    labels = []\n    batch_texts = []\n    kaggle_ids = []\n\n    for (_label, batch_text, kaggle_id) in batch:\n        if _label is not None:\n            labels.append(_label)\n        \n        batch_texts.append(batch_text)\n        kaggle_ids.append(kaggle_id)\n    \n            \n    if _label is not None:\n        labels = torch.tensor(labels, dtype=torch.float)\n    \n    encoded_batch = tokenizer(batch_texts\n                              , return_tensors=return_tensors\n                              , padding=padding\n                              , max_length=max_length\n                              , truncation=truncation)\n\n    return labels, encoded_batch, kaggle_ids\n\n\nclass CommonLitDataset(Dataset):\n    def __init__(self\n                 , df\n                 , text_col: str = 'excerpt'\n                 , label_col: str = 'target'\n                 , kaggle_id: str = 'id'\n                 , sample_size: Optional[str] = None\n                ):\n        self.df = df if sample_size is None else df.sample(sample_size)\n        self.text_col = text_col\n        self.label_col = label_col\n        self.kaggle_id = kaggle_id\n        self.num_labels = len(df[label_col].unique()) if label_col in df.columns else None\n        # source: https://www.kaggle.com/justinchae/nn-utils\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        result = None\n        text = self.df.iloc[idx][self.text_col]\n        kaggle_id = self.df.iloc[idx][self.kaggle_id]\n        \n        if 'target' in self.df.columns:\n            target = self.df.iloc[idx][self.label_col]\n            return target, text, kaggle_id     \n        else:\n            return None, text, kaggle_id\n\n\nclass CommonLitDataModule(pl.LightningDataModule):\n    def __init__(self\n                 , tokenizer\n                 , train_path\n                 , collate_fn=None\n                 , max_length: int = 280\n                 , batch_size: int = 16\n                 , valid_path: Optional[str] = None\n                 , test_path: Optional[str] = None\n                 , train_valid_split: float = .6\n                 , dtypes=None\n                 , shuffle_dataloader: bool = True\n                 , num_dataloader_workers: int = NUM_DATALOADER_WORKERS\n                 , kfold: Optional[dict] = None):\n        super(CommonLitDataModule, self).__init__()\n        self.train_path = train_path\n        self.valid_path = valid_path\n        self.test_path = test_path\n        self.train_valid_split = train_valid_split\n        self.dtypes = {'id': str} if dtypes is None else dtypes\n        self.train_size = None\n        self.train_df, self.train_data = None, None\n        self.valid_df, self.valid_data = None, None\n        self.test_df, self.test_data = None, None\n        if collate_fn is not None:\n            self.collate_fn = partial(collate_fn\n                                      , tokenizer=tokenizer\n                                      , max_length=max_length) \n        else:\n            \n            self.collate_fn = partial(my_collate_fn\n                                      , batch=batch_size\n                                      , tokenizer=tokenizer)\n            \n        self.shuffle_dataloader = shuffle_dataloader\n        self.batch_size = batch_size\n        self.num_dataloader_workers = num_dataloader_workers\n        # refactored from: https://www.kaggle.com/justinchae/nn-utils\n    \n    def _strip_extraneous(self, df):\n        strip_cols = ['url_legal', 'license']\n        if all(col in df.columns for col in strip_cols):\n            extraneous_data = strip_cols\n            return df.drop(columns=extraneous_data)\n        else: \n            return df\n    \n    def prepare(self, prep_type=None):\n        if prep_type == 'train':\n            # creates just an instance of the train data as a pandas df\n            self.train_df = self.train_path if isinstance(self.train_path, pd.DataFrame) else pd.read_csv(self.train_path, dtype=self.dtypes)\n            self.train_df = self._strip_extraneous(self.train_df)\n        \n    def setup(self, stage: Optional[str] = None) -> None:\n        if stage == 'fit':\n            # when this class is called by trainer.fit, this stage runs and so on\n            self.train_df = self.train_path if isinstance(self.train_path, pd.DataFrame) else pd.read_csv(self.train_path, dtype=self.dtypes)\n            self.train_df = self._strip_extraneous(self.train_df)\n            self.train_size = int(len(self.train_df))\n            self.train_data = CommonLitDataset(df=self.train_df)\n        \n            if self.train_valid_split is not None and self.valid_path is None:\n                self.train_size = int(len(self.train_df) * self.train_valid_split)\n                self.train_data, self.valid_data = random_split(self.train_data, [self.train_size, len(self.train_df) - self.train_size])\n            elif self.valid_path is not None:\n                self.valid_df = self.valid_path if isinstance(self.valid_path, pd.DataFrame) else pd.read_csv(self.valid_path, dtype=self.dtypes)\n                self.valid_data = CommonLitDataset(df=self.valid_df)\n            \n        if stage == 'predict':           \n            self.test_df = self.test_path if isinstance(self.test_path, pd.DataFrame) else pd.read_csv(self.test_path, dtype=self.dtypes)\n            self.test_df = self._strip_extraneous(self.test_df)\n            self.test_data = CommonLitDataset(df=self.test_df)\n    \n    def kfold_data(self):\n        # TODO: wondering how to integrate kfolds into the datamodule\n        pass\n    \n    def train_dataloader(self) -> DataLoader:\n        return DataLoader(self.train_data\n                          , batch_size=self.batch_size\n                          , shuffle=self.shuffle_dataloader\n                          , collate_fn=self.collate_fn\n                          , num_workers=self.num_dataloader_workers\n                          , pin_memory=True\n                          )\n    def val_dataloader(self) -> DataLoader:\n        if self.valid_data is None:\n            return None\n        else:\n            return DataLoader(self.valid_data\n                              , batch_size=self.batch_size\n                              , shuffle=False\n                              , collate_fn=self.collate_fn\n                              , num_workers=self.num_dataloader_workers\n                              , pin_memory=True\n                              )\n    def predict_dataloader(self) -> DataLoader:\n        if self.test_data is None:\n            return None\n        else:\n            return DataLoader(self.test_data\n                              , batch_size=self.batch_size\n                              , shuffle=False\n                              , collate_fn=self.collate_fn\n                              , num_workers=self.num_dataloader_workers\n                              , pin_memory=True\n                              ) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial: optuna.trial.Trial\n              , logger=None\n              , datamodule=None\n              , train_df=None\n              , valid_df=None) -> float:\n    \"\"\"Reference: https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_lightning_simple.py#L81\n    \"\"\"\n\n    # propose params to study\n    batch_size = BATCH_SIZE #trial.suggest_int(\"batch_size\", 8, 16)\n    # tried tuning batch size but ended up with better results by setting it to 16; ideally with larger capacity we go higher to 32\n    tokenizer_max_len = trial.suggest_int(\"tokenizer_max_len\", 128, 512)\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n    warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 1000)\n    max_epochs = MAX_EPOCHS\n    # also tried tuning epochs but seems to be too much to tune when considering everything else\n    gradient_clip_val = trial.suggest_float(\"gradient_clip_val\", 0, .5)\n    weight_decay = trial.suggest_float(\"weight_decay\", 0, 1e-2)\n    # we can add more things to study here...\n    \n    checkpoint_filename = f'crp_roberta_trial_{trial.number}'\n    checkpoint_save = ModelCheckpoint(dirpath=CHECKPOINTS_PATH\n                                          , filename=checkpoint_filename\n                                         )\n    \n    early_stopping_callback = EarlyStopping(monitor='val_loss'\n                                            , patience=2\n                                            )\n    \n    logger = pl.loggers.TensorBoardLogger(save_dir=LOGS_PATH) if logger is None else logger\n    \n    trainer = pl.Trainer(max_epochs=max_epochs\n                         , logger=logger\n                         , gpus=ACCELERATOR_TYPE['gpus']\n                         , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                         , callbacks = [PyTorchLightningPruningCallback(trial, monitor='val_loss') \n                                        , checkpoint_save\n                                        # come back and decide to use early stopping mid-epoch\n#                                         , early_stopping_callback\n                                        ]\n                         , precision=16 if USE_16_BIT_PRECISION else 32\n                         , default_root_dir=CHECKPOINTS_PATH\n                         , gradient_clip_val=gradient_clip_val\n                         , stochastic_weight_avg=True\n                         # TODO: debug how to ensure trainer consumes stopped state\n#                          , val_check_interval=.33\n                         # TODO: debug how to auto scale batch search with optuna-lightning\n#                          , auto_scale_batch_size='binsearch'\n                        )\n    # default to using the data module but allow for cross validation via train and valid loader objects\n    if datamodule is None and train_df is None and valid_df is None:\n        datamodule = CommonLitDataModule(collate_fn=my_collate_fn\n                                         , tokenizer=PRETRAINED_ROBERTA_BASE_TOKENIZER\n                                         , train_path=KAGGLE_TRAIN_PATH\n                                         , test_path=KAGGLE_TEST_PATH\n                                         , max_length=tokenizer_max_len\n                                         , batch_size=batch_size\n                                         , train_valid_split=TRAIN_VALID_SPLIT\n                                          )\n        # manually calling this stage since we need some params to set up model initially\n        datamodule.setup(stage='fit')\n        \n    elif datamodule is None and train_df is not None and valid_df is not None:\n        datamodule = CommonLitDataModule(collate_fn=my_collate_fn\n                                         , tokenizer=PRETRAINED_ROBERTA_BASE_TOKENIZER\n                                         , train_path=train_df\n                                         , valid_path=valid_df\n                                         , test_path=KAGGLE_TEST_PATH\n                                         , max_length=tokenizer_max_len\n                                         , batch_size=batch_size\n                                         , train_valid_split=TRAIN_VALID_SPLIT\n                                          )\n        datamodule.setup(stage='fit')\n    else:\n        return False\n        \n    model = LitRobertaLogitRegressor(pre_trained_path=PRETTRAINED_ROBERTA_BASE_MODEL_PATH\n                                      , train_size=datamodule.train_size\n                                      , batch_size=datamodule.batch_size\n                                      , n_gpus=trainer.gpus\n                                      , n_tpus=trainer.tpu_cores\n                                      , max_epochs=trainer.max_epochs\n                                      , accumulate_grad_batches=trainer.accumulate_grad_batches\n                                      , learning_rate=learning_rate\n                                      , warmup_steps=warmup_steps\n                                      )\n    \n    hyperparameters = dict(learning_rate=learning_rate\n                           , warmup_steps=warmup_steps\n                           , max_epochs=max_epochs\n                           , gradient_clip_val=gradient_clip_val\n                           , weight_decay=weight_decay\n                           , batch_size=batch_size\n                           , tokenizer_max_len=tokenizer_max_len\n                          )\n    \n    trainer.logger.log_hyperparams(hyperparameters)\n    trainer.fit(model, datamodule=datamodule)\n    \n    # saving the fine-tuned states of roberta transformers\n#     model_file_name = f\"trial_{trial.number}_tuned_roberta\"\n#     model_file_path = os.path.join(MODEL_PATH, model_file_name)\n#     model.save_pretrained(model_file_path)\n    # it turns out that we don't actually need to save the RoBERTa model weights seperately, that's all in the checkpoint\n\n    curr_loss = trainer.callback_metrics['val_loss'].item()\n        \n    return curr_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective_cv(trial):\n\n    # wrap a cross validation dataset around the objective function\n    # source: https://stackoverflow.com/questions/63224426/how-can-i-cross-validate-by-pytorch-and-optuna\n    # we can call this function instead of objective to run kfolds in each trial\n    # FIXME: run k folds within each trial without incremending the objective counter\n    # Issue: Each iteration of a trial increases disk usage and with kaggle we run out of memory\n    \n    datamodule = CommonLitDataModule(tokenizer=PRETRAINED_ROBERTA_BASE_TOKENIZER\n                                    , train_path=KAGGLE_TRAIN_PATH\n                                     )\n    \n    datamodule.prepare(prep_type='train')\n    train_df = datamodule.train_df\n\n    fold = KFold(n_splits=3, shuffle=True, random_state=SEED_VAL)\n    scores = []\n    \n    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(range(len(train_df)))):\n        # clean up memory\n        torch.cuda.empty_cache()\n        gc.collect()\n\n        train_data = train_df.iloc[train_idx]\n        valid_data = train_df.iloc[valid_idx]\n        # pass data objects to objective and return average of losses\n        losses = objective(trial\n                             , train_df=train_data\n                             , valid_df=valid_data)\n        print(f\"=== kfold: {fold_idx + 1}: val_loss: {accuracy} \\n\")\n        scores.append(losses)\n        \n    return np.mean(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    # necessary to clean up space when running cells repeatedly in kernel\n    torch.cuda.empty_cache()\n    gc.collect()\n    # testing the hyperbandpruner which is supposed to work better\n#     pruner = optuna.pruners.HyperbandPruner(min_resource=2)\n    pruner = optuna.pruners.MedianPruner(n_startup_trials=2\n                                         # steps are epochs, this ensures trials are not pruned until second epoch\n                                        , n_warmup_steps=2\n                                        )\n    sampler = TPESampler(multivariate=True\n                         , seed=SEED_VAL)\n    # testing some other samplers which requires some trial and error\n#     sampler = RandomSampler(seed=SEED_VAL)\n    # params per docs, re CMA-ES sampler with hyperband pruner\n#     sampler = CmaEsSampler(consider_pruned_trials=True\n#                            , n_startup_trials=\n#                            , seed=SEED_VAL\n#                            , restart_strategy='ipop')\n    \n    study = optuna.create_study(study_name=\"crp-roberta-tuning\"\n                                , direction=\"minimize\"\n                                , pruner=pruner\n                                , sampler=sampler\n                                # testing study storage in rdb instead of memory\n                                , storage='sqlite:///crp-study.db'\n                               )\n    \n    study.optimize(objective\n                   # or objective_cv for cross validator\n                   # exceeding memory constraints past 10-12 trials in kaggle\n                   , n_trials=N_OPTUNA_TRIALS\n                   # if using timeout, trials end at time instead of running all expected trials\n#                    , timeout=600\n#                    , gc_after_trial=True\n                   # test if callbacks with lambda manages memory instead\n                   , callbacks=[lambda study, trial: gc.collect()]\n                  )\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    best_trial = study.best_trial\n\n    print(\"  Value: {}\".format(best_trial.value))\n\n    print(\"  Params: \")\n    for key, value in best_trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n        \n    print(\"  Full Summary of Trials:  \")\n    print(study.trials_dataframe())\n    \n    plot_optimization_history(study).show()\n    plot_intermediate_values(study).show()\n    try:\n        plot_param_importances(study).show()\n    except ValueError:\n        pass\n        \n    #TODO: link logs to offline graphs, i.e. tensorboard UI\n    #TODO: link logs to neptune for real time  awareness\n    #TODO: figure out an easy way to toggle TPU and GPU usage\n    #TODO: debug early stopping - early stop during optuna seems to work but the state of early stop does not seem to persist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing restoring from checkpoints (buggy)\n# clean up memory\ntorch.cuda.empty_cache()\ngc.collect()\n\n# # recall the best trial checkpoint\nbest_trial_checkpoint = os.path.join(CHECKPOINTS_PATH, f'crp_roberta_trial_{best_trial.number}.ckpt')\n\n# we can recall the roberta fine tuned state if we want\n# tuned_roberta_file_name = f\"trial_{best_trial.number}_tuned_roberta\"\n# tuned_roberta_file_path = os.path.join(MODEL_PATH, tuned_roberta_file_name)\n\n# TODO: organize code so we don't have to call it in optuna study and then again here\ncrp_data = CommonLitDataModule(collate_fn=my_collate_fn\n                               , tokenizer=PRETRAINED_ROBERTA_BASE_TOKENIZER\n                               , train_path=KAGGLE_TRAIN_PATH\n                               , test_path=KAGGLE_TEST_PATH\n                               , max_length=best_trial.params['tokenizer_max_len']\n                               , batch_size=BATCH_SIZE #best_trial.params['batch_size']\n                              )\n\ncrp_data.setup(stage='predict')\n# restore the model checkpoint of the best trial\nmodel = LitRobertaLogitRegressor.load_from_checkpoint(best_trial_checkpoint\n                                                    # so here, we don't have to load the tuned roberta model, its all in the checkpoint file\n#                                                        , pre_trained_path=tuned_roberta_file_path\n                                                     )\n# freeze the model for prediction\nmodel.eval()\nmodel.freeze()\n\n# set up a new trainer object to run prediction\ntrainer = pl.Trainer(gpus=ACCELERATOR_TYPE['gpus']\n                     , tpu_cores=ACCELERATOR_TYPE['tpu_cores']\n                     )\n\n# run predict on the test data\npredictions = trainer.predict(model=model, datamodule=crp_data)\n\nsubmission = pd.concat(predictions).reset_index(drop=True)\n\nprint(submission)\nsubmission.to_csv('submission.csv', index=False)\n\n# TODO: test whether we need to save and upload the fine-tuned state of roberta or if pytorch lightning checkpoints take care of it all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Helpful Resources\n\n* Optuna Docs: [https://optuna.readthedocs.io/en/stable/index.html](https://optuna.readthedocs.io/en/stable/index.html)\n\n* PyTorch Lightning Docs: [https://pytorch-lightning.readthedocs.io/en/latest/](https://pytorch-lightning.readthedocs.io/en/latest/)\n\n* For learning rate tuning: [https://medium.com/pytorch/using-optuna-to-optimize-pytorch-hyperparameters-990607385e36](https://medium.com/pytorch/using-optuna-to-optimize-pytorch-hyperparameters-990607385e36)\n\n* For PyTorch Lightning Precision: [https://pytorch-lightning.readthedocs.io/en/stable/advanced/amp.html](https://pytorch-lightning.readthedocs.io/en/stable/advanced/amp.html)\n\n* For PyTorch Lightning Early Stopping: [https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html](https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html)\n\n* For PyTorch Lightning Checkpointing: [https://pytorch-lightning.readthedocs.io/en/stable/common/weights_loading.html](https://pytorch-lightning.readthedocs.io/en/stable/common/weights_loading.html)\n\n* BERT Example from PyTorch Lighting: [https://pytorch-lightning.readthedocs.io/en/stable/advanced/transfer_learning.html](https://pytorch-lightning.readthedocs.io/en/stable/advanced/transfer_learning.html)\n\n* Fine-Tuning a Transformer from PyTorch Lightning: [https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html)\n\n* Example of Optuna with PyTorch Lightning: [https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_lightning_simple.py](https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_lightning_simple.py)\n\n* For PyTorch Lightning Logging: [https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html](https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html)\n\n* For Predict Mode with PyTorch Lightning: [https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html](https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html)\n\n* Restoring Checkpoints and Continuing Training: [https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html?highlight=checkpoint#checkpoint-loading](https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html?highlight=checkpoint#checkpoint-loading)\n\n* Gradient Clipping in PyTorch Lightning: [https://pytorch-lightning.readthedocs.io/en/stable/advanced/training_tricks.html?highlight=memory#advanced-gpu-optimizations](https://pytorch-lightning.readthedocs.io/en/stable/advanced/training_tricks.html?highlight=memory#advanced-gpu-optimizations)\n\n* How to approach trial suggestions in Optuna: [https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html?highlight=suggest#optuna.trial.Trial.suggest_int](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html?highlight=suggest#optuna.trial.Trial.suggest_int)\n\n* Guidance on Early Stopping Callbacks with PyTorch Lightning: [https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.callbacks.early_stopping.html#pytorch_lightning.callbacks.early_stopping.EarlyStopping](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.callbacks.early_stopping.html#pytorch_lightning.callbacks.early_stopping.EarlyStopping)\n\n* For Reproducible Optuna Studies: [https://optuna.readthedocs.io/en/stable/faq.html#how-can-i-obtain-reproducible-optimization-results](https://optuna.readthedocs.io/en/stable/faq.html#how-can-i-obtain-reproducible-optimization-results)\n\n* Guidance on Optuna Pruners: [https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.HyperbandPruner.html#optuna.pruners.HyperbandPruner](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.HyperbandPruner.html#optuna.pruners.HyperbandPruner)\n\n* More guidance on which Optuna Pruners to use based on ML task: [https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html?highlight=memory#activating-pruners](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html?highlight=memory#activating-pruners)\n\n* TPUs [https://www.kaggle.com/justusschock/pytorch-on-tpu-with-pytorch-lightning](https://www.kaggle.com/justusschock/pytorch-on-tpu-with-pytorch-lightning)\n\n* For Neptune to PyTorch Lightning Integration: [https://docs.neptune.ai/integrations-and-supported-tools/model-training/pytorch-lightning](https://docs.neptune.ai/integrations-and-supported-tools/model-training/pytorch-lightning)","metadata":{}}]}