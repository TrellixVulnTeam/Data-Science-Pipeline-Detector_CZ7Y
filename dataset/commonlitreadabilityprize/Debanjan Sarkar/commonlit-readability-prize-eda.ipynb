{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 id=\"heading\">\n\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/deb009/commonlit-readability-prize-eda/notebook#heading\">¶</a>\n</h1>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"your-secret-label\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install textfeatures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install textstat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import datasets\nfrom sklearn import model_selection\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport missingno as msno\nimport re\nimport string\nfrom pprint import pprint\nimport textstat\nimport textfeatures\nimport nltk\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVR\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample_submision = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submision.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train = train.drop([\"url_legal\",\"license\"], axis=1)\n#test = test.drop([\"url_legal\",\"license\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing values","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">I am using [missingno](http://pypi.org/project/missingno/) library for visualizing missing values.</font>\n","metadata":{}},{"cell_type":"code","source":"msno.bar(train, sort=\"ascending\", figsize=(10,5), fontsize=12)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.histplot(train['target'])\nplt.title('Target distribution')\nplt.show()\nprint(train.target.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.xlim(-5,2)\nplt.ylabel('target')\nsns.boxplot(x=train['target'])\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">We can infer from the above graphs that :</font>\n1. Most of the values are less then 0.\n2. There are no outliers in the column.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.histplot(train['standard_error'])\nplt.title('Target distribution')\nplt.show()\nprint(train.standard_error.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.xlim(-0.05,0.7)\nplt.ylabel('standard_error')\nsns.boxplot(x=train['standard_error'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">It looks like standard error column has outliers around 0 and more than 0.5.</font>\n","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x=train['target'], y=train['standard_error'], kind='hex',height=8)\nplt.suptitle(\"Target vs Standard error\")\nplt.subplots_adjust(top=0.94)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">We will try to check the values for excerpt when the target is lowest and highest.</font>","metadata":{}},{"cell_type":"markdown","source":"Excerpt Values for Lowest Target Values","metadata":{}},{"cell_type":"code","source":"sort_by_target = train.sort_values(['target'])\nsort_by_target_lowest = sort_by_target[['excerpt','target']].head(5)\nfor label, row in sort_by_target_lowest.iterrows():\n    print(row[\"excerpt\"][:400])\n    print(row[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excerpt Values for Highest Target Values","metadata":{}},{"cell_type":"code","source":"sort_by_target_highest = sort_by_target[['excerpt','target']].tail(5)\nfor label, row in sort_by_target_highest.iterrows():\n    print(row[\"excerpt\"][:400])\n    print(row[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\"> So, from the above two cells we are able to figure out that the target is inversly propotional to complexity of the sentences</font>","metadata":{}},{"cell_type":"markdown","source":"# Wordcloud","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">Wordcloud for target lowest till 5 rows each</font>","metadata":{}},{"cell_type":"code","source":"text = \" \".join(excerpt for excerpt in sort_by_target_lowest.excerpt)\n# Create stopword list:\nstopwords = set(STOPWORDS)\n\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">Wordcloud for highest target till 5 rows each</font>","metadata":{}},{"cell_type":"code","source":"text = \" \".join(excerpt for excerpt in sort_by_target_highest.excerpt)\n# Create stopword list:\nstopwords = set(STOPWORDS)\n\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Processing and Feature Engineering","metadata":{}},{"cell_type":"code","source":"df_train = train.copy()\ndf_test = test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will add a new feature called [Dale–Chall readability formula](http://https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula).\nThe Dale–Chall readability formula is a readability test that provides a numeric gauge of the comprehension difficulty that readers come upon when reading a text. It uses a list of 3000 words that groups of fourth-grade American students could reliably understand, considering any word not on that list to be difficult.\n\nYou will be able to find more about the formula in the hyperlink.\n\nWe will add some other features also.","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    df['character_count'] = df['excerpt'].apply(lambda x: len(str(x)))\n    df['digit_count'] = df['excerpt'].apply(lambda x: np.sum(([int(word.isdigit()) for word in str(x).split()])))\n    df['word_count'] = df['excerpt'].apply(textstat.lexicon_count)\n    df['unique_word_count'] = df['excerpt'].apply(lambda x: len(set(str(x).split())))\n    df['mean_word_length'] = df['excerpt'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\n    df['syllable_count'] = df['excerpt'].apply(textstat.syllable_count)\n    df['sentence_count'] = df['excerpt'].apply(textstat.sentence_count)\n    df['flesch_reading_ease'] = df['excerpt'].apply(textstat.flesch_reading_ease)\n    df['flesch_kincaid_grade'] = df['excerpt'].apply(textstat.flesch_kincaid_grade)\n    df['automated_readability_index'] = df['excerpt'].apply(textstat.automated_readability_index)\n    df['coleman_liau_index'] = df['excerpt'].apply(textstat.coleman_liau_index)\n    df['linsear_write_formula'] = df['excerpt'].apply(textstat.linsear_write_formula)\n    df['difficult_words']= df['excerpt'].apply(lambda x: textstat.difficult_words(x))\n    df['avg_sentence_length']= df['excerpt'].apply(lambda x: textstat.avg_sentence_length(x))\n    df['reading_time']=df['excerpt'].apply(lambda x: textstat.reading_time(x))\n    df['dc_readability_score'] = df['excerpt'].apply(lambda x: textstat.dale_chall_readability_score(x))\n\n    return df\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = feature_engineering(df_train)\ndf_test = feature_engineering(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['character_count', 'digit_count', 'word_count', 'unique_word_count',\n       'mean_word_length', 'syllable_count', 'sentence_count',\n       'flesch_reading_ease', 'flesch_kincaid_grade',\n       'automated_readability_index', 'coleman_liau_index',\n       'linsear_write_formula', 'difficult_words', 'avg_sentence_length',\n       'reading_time', 'dc_readability_score']\n\ndf_temp = df_train[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will try to figure out the collinearity of the columns which we have created now.\n\n","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 15), dpi=100)\nmatrix = np.triu(df_train[columns + ['target']].corr())\nsns.heatmap(df_train[columns + ['target']].corr(), annot=True, mask=matrix)\nplt.title('New Features and Target Correlations', size=20, pad=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature(feature):\n\n    fig, axes = plt.subplots(ncols=2, figsize=(32, 6))\n\n    sns.regplot(x=df_train['target'], y=df_train[feature], line_kws={'color': 'red'}, ax=axes[0])\n    sns.kdeplot(df_train[feature], fill=True, ax=axes[1])\n\n    axes[0].set_xlabel(f'target', size=18)\n    axes[0].set_ylabel(feature, size=18)\n    axes[1].set_xlabel('')\n    axes[1].set_ylabel('')\n    axes[1].legend(prop={'size': 15})\n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=15)\n        axes[i].tick_params(axis='y', labelsize=15)\n    axes[0].set_title(f'target vs {feature}', size=20, pad=20)\n    axes[1].set_title(f'{feature} Distribution', size=20, pad=20)\n\n    plt.show()\n    \nfor feature in columns:\n    plot_feature(feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only flesch_reading_grade and dc_readability_score has more than 50 percent coorelation with target.We will try to plot each column with target and see there distribution.","metadata":{}},{"cell_type":"markdown","source":"# Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"clean() function of textfeatures can be used to clean the document.","metadata":{}},{"cell_type":"code","source":"textfeatures.clean(df_train,\"excerpt\",\"clean_excerpt\")\ntextfeatures.clean(df_test,\"excerpt\",\"clean_excerpt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train['clean_excerpt']=df_train['clean_excerpt'].apply(lambda x: clean(x))\n#df_test['clean_excerpt']=df_test['clean_excerpt'].apply(lambda x: clean(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalization","metadata":{}},{"cell_type":"code","source":"df_train['clean_excerpt'] = df_train['clean_excerpt'].astype(str)\ndf_test['clean_excerpt'] = df_test['clean_excerpt'].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stemming\nfrom nltk.stem.porter import PorterStemmer\n\ndef get_stemmed_text(corpus):\n    stemmer = PorterStemmer()\n    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n\ndf_train['clean_excerpt'] = get_stemmed_text(df_train['clean_excerpt'])\ndf_test['clean_excerpt'] = get_stemmed_text(df_test['clean_excerpt'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lemmatization\nfrom nltk.stem import WordNetLemmatizer\ndef get_lemmatized_text(corpus):\n    lemmatizer = WordNetLemmatizer()\n    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n\ndf_train['clean_excerpt'] = get_lemmatized_text(df_train['clean_excerpt'])\ndf_test['clean_excerpt'] = get_lemmatized_text(df_test['clean_excerpt'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenization","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(use_idf=True)\n\ntrain_v = vectorizer.fit_transform(df_train['clean_excerpt'])\ntest_v = vectorizer.transform(df_test['clean_excerpt'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nscaler = StandardScaler()\n\n\nfor col in columns:\n    df_train[col] = scaler.fit_transform(df_train[col].values.reshape(-1, 1))\n    df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standard Scalar","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n\nfor col in columns:\n    df_train[col] = scaler.fit_transform(df_train[col].values.reshape(-1, 1))\n    df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X1 = df_train[columns]\ntest_X1 = df_test[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X_Title = hstack([train_v, csr_matrix(train_X1.values)])\ntest_X_Title = hstack([test_v, csr_matrix(test_X1.values)])\ny1 = train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LinearSVR model\n\nX_train, X_test, y_train, y_test = train_test_split(train_X_Title, y1, test_size=0.20, random_state=42)\n\nclf1 = LinearSVR(C=0.2)\nclf1.fit(X_train, y_train)\n\ny_pred1 = clf1.predict(X_test)\nmae1 = mean_absolute_error(y_pred1, y_test)\nprint('MAE:', sqrt(mae1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = clf1.predict(test_X_Title)\ntest['target'] = pred\ntest.drop(['url_legal','license'],axis=1,inplace=True)\ntest.drop(['excerpt'],axis=1,inplace=True)\ntest.reset_index(drop=True, inplace=True)\ntest.head()\ntest.to_csv('output.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}