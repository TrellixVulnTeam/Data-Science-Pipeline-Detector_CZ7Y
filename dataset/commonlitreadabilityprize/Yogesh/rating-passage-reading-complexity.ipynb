{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit Readability Prize\n1. Problem statement: Develop **algorithm to rate the complexity of reading passage** for grade 3-12 classroom use.\n2. Submissions are scored on the root mean squared error. \n\nCompetetion Link: https://www.kaggle.com/c/commonlitreadabilityprize/discussion/241029\n\n\nSolution: \n\n#### Steps implemented:\n1. <a href='#Prep'>Preparing input data</a>\n2. <a href='#EDA'>EDA</a> \n3. <a href='#ModelDevelopment'>Model(s) Development stage one</a>\n4. <a href='#SubmitFinalOutput'>Submit Final Output</a>\n5. <a href='#SaveFinalModel'>Save final Model</a>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T12:47:52.087613Z","iopub.execute_input":"2021-08-02T12:47:52.087999Z","iopub.status.idle":"2021-08-02T12:47:52.09954Z","shell.execute_reply.started":"2021-08-02T12:47:52.087962Z","shell.execute_reply":"2021-08-02T12:47:52.098561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install textstat  # install textstat","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:47:21.487228Z","iopub.execute_input":"2021-08-02T12:47:21.487808Z","iopub.status.idle":"2021-08-02T12:47:32.350477Z","shell.execute_reply.started":"2021-08-02T12:47:21.48776Z","shell.execute_reply":"2021-08-02T12:47:32.34917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm, linear_model, metrics\n\nimport math\nimport numpy as np\nimport re\n\nfrom pandas import read_csv, set_option, DataFrame, concat\nset_option('display.max_rows', 4)\nset_option('display.max_colWidth', 20)\n\nimport gc; \ngc.enable()\n\nimport seaborn as sns\nsns.set_theme()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef seed_everything(seed=10):\n    #random.seed(seed)\n    #os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    #torch.manual_seed(seed)\n    #torch.cuda.manual_seed(seed)\n    #torch.backends.cudnn.deterministic = True\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:47:32.35217Z","iopub.execute_input":"2021-08-02T12:47:32.352484Z","iopub.status.idle":"2021-08-02T12:47:33.836286Z","shell.execute_reply.started":"2021-08-02T12:47:32.352452Z","shell.execute_reply":"2021-08-02T12:47:33.834973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdInputTrainData = read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\npdInputTrainData","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:48:24.537725Z","iopub.execute_input":"2021-08-02T12:48:24.538144Z","iopub.status.idle":"2021-08-02T12:48:24.688572Z","shell.execute_reply.started":"2021-08-02T12:48:24.538113Z","shell.execute_reply":"2021-08-02T12:48:24.687455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Prep'></a>\n# 1. Preparing input data\n1. Calculation of existing standard readability indexes\n2. Normalization\n3. Outliers treatment\n4. Derive two new params","metadata":{}},{"cell_type":"code","source":"dictCorrValues = {} # updated during training\n\ndef return_all_index_framework(pdInputData, strType):\n    \n    global dictCorrValues\n    \n    pdFeatures = pdInputData.copy()\n    \n    # 1. derive readability indexes\n    pdFeatures = readability_indexes(pdFeatures[['excerpt']], 'excerpt')\n    pdFeatures.TextStd=pdFeatures.TextStd.apply( lambda x: get_grade_number(x) ) \n    \n    # 2. Normalization\n    lstInputFeatures=['Flesch', 'SMOG', 'FleschKincaid', 'ColemanLiau', \n                      'Automated', 'DaleChall', 'DifficultWords', 'LinsearWrite',\n                       'GunningFog', 'TextStd', 'FernandezHuerta', 'SzigrisztPazos',\n                       'GutierrezPolini', 'CrawFord']\n    \n    X=pdFeatures[lstInputFeatures].copy()\n    X=z_score(X)\n    \n    \n    # 3. Compute outliers\n    X_treated = outlier_treatment(X[lstInputFeatures], lstInputFeatures)\n    \n    # 4. Derive two new params from the existing ones\n    # Derive corelation values for each parameter during training to use it as wt.s and calculate mean of selected ones\n    \n    if(strType=='training'): # Find Corelation value, cal mean, check rmse\n        \n        X_treated = concat([ X_treated, pdInputData[['target']] ], axis=1)\n        \n        dictCorrValues= get_feature_corr_index(X_treated, lstInputFeatures)\n        \n        \n        lstFeaturesPos=['Flesch', 'SzigrisztPazos', 'FernandezHuerta', 'GutierrezPolini']\n        X_treated['PosMean']=calculate_index_mean(X_treated, dictCorrValues, lstFeaturesPos)\n        print(\"\\nPosMean:\")\n        calculate_rmse(pdInputData['target'], X_treated['PosMean'])\n\n        lstFeaturesNeg=['DifficultWords', 'DaleChall', 'CrawFord', 'SMOG']\n        X_treated['NegMean']=calculate_index_mean(X_treated, dictCorrValues, lstFeaturesNeg)\n        print(\"\\nNegMean:\")\n        calculate_rmse(pdInputData['target'], X_treated['NegMean'])\n        \n        sns.pairplot(data=X_treated, y_vars='PosMean', x_vars='target', kind='reg')\n        sns.pairplot(data=X_treated, y_vars='NegMean', x_vars='target', kind='reg')\n\n    \n    else: # Use save corelation value for each parameters during training\n        \n        lstFeaturesPos=['Flesch', 'SzigrisztPazos', 'FernandezHuerta', 'GutierrezPolini']        \n        X_treated['PosMean']=calculate_index_mean(X_treated, dictCorrValues, lstFeaturesPos)\n\n        lstFeaturesNeg=['DifficultWords', 'DaleChall', 'CrawFord', 'SMOG']\n        X_treated['NegMean']=calculate_index_mean(X_treated, dictCorrValues, lstFeaturesNeg)\n    \n    \n    return X_treated\n\ndef readability_indexes(pdInput, strExcerptColName):\n    \n    from textstat import flesch_reading_ease, smog_index, flesch_kincaid_grade,\\\n    coleman_liau_index, automated_readability_index, dale_chall_readability_score, difficult_words, \\\n    linsear_write_formula, gunning_fog, text_standard, fernandez_huerta, szigriszt_pazos, \\\n    gutierrez_polini, crawford\n\n    lstAllIndexes = ['Flesch', 'SMOG', 'FleschKincaid', 'ColemanLiau',\\\n                     'Automated','DaleChall', 'DifficultWords', 'LinsearWrite',\\\n                    'GunningFog', 'TextStd', 'FernandezHuerta','SzigrisztPazos',\\\n                     'GutierrezPolini','CrawFord']\n    for Formula in lstAllIndexes:\n        pdInput[Formula] = ''\n        \n    def compute_all_indexes(row, strExcerptColName):\n        \n        strExcerpt=row[strExcerptColName]\n\n        row['Flesch']=flesch_reading_ease(strExcerpt)\n        row['SMOG']=smog_index(strExcerpt)\n        row['FleschKincaid']=flesch_kincaid_grade(strExcerpt)\n        row['ColemanLiau']=coleman_liau_index(strExcerpt)\n        row['Automated']=automated_readability_index(strExcerpt)\n        row['DaleChall']=dale_chall_readability_score(strExcerpt)\n        row['DifficultWords']=difficult_words(strExcerpt)\n        row['LinsearWrite']=linsear_write_formula(strExcerpt)\n        row['GunningFog']=gunning_fog(strExcerpt)\n        row['TextStd']=text_standard(strExcerpt)\n        row['FernandezHuerta']=fernandez_huerta(strExcerpt)\n        row['SzigrisztPazos']=szigriszt_pazos(strExcerpt)\n        row['GutierrezPolini']=gutierrez_polini(strExcerpt)\n        row['CrawFord']=crawford(strExcerpt)\n    \n        return row\n    \n    pdFeatures=pdInput.apply(lambda row: compute_all_indexes(row, strExcerptColName), axis=1 )\n\n    return pdFeatures\n\ndef get_grade_number(num_string):\n    import re\n    pattern=\"\\\\W|\\\\d+\" # num_string can be '8th and 9th grade' or '-8th and -9th grade'\n    str_list=re.findall(pattern,num_string)  #['8', '', '9'] or ['-', '8', ' ', ' -', '9', ' ']\n    \n    # combine -ve sign and number\n    if(str_list[0] == '-'): \n        str_list= [str_list[0]+str_list[1]]\n        #print(num_string, str_list)\n    return int(str_list[0])  # take lower grade number in the range e.g. 8\n\n\ndef calculate_index_mean(pdFeatures, dictCorrValues, lstFeatures):\n    pdInput = pdFeatures.copy()\n    for item in dictCorrValues:\n        #print(\"\\n\", item, dictCorrValues[item])\n        pdInput[item]=dictCorrValues[item] * pdInput[item]\n        #calculate_rmse(pdInput['target'], pdInput[item])\n    \n    return pdInput[lstFeatures].apply(lambda row: np.mean(row), axis=1)\n\n\ndef get_feature_corr_index(pdInput, lstFeatures):\n    dictCorrValues = {} \n    for item in lstFeatures:\n        dictCorrValues[item]=pdInput.corr(method ='pearson')[['target']].loc[item]['target'] \n    print(\"\\nTraining: Calculated corelation values: \", dictCorrValues)\n    return dictCorrValues\n\n# Normalise data: the z-score method in Pandas same can be done using sklearn lib in one line code\ndef z_score(df):\n    lstColumns = df.columns\n    for column in lstColumns: df[column] = (df[column] - df[column].mean()) / df[column].std()\n    return df\n\n# Compute Outliers\ndef get_summary_statistics(dataset):\n    \n    mean = np.round(np.mean(dataset), 2)\n    median = np.round(np.median(dataset), 2)\n    min_value = np.round(dataset.min(), 2)\n    max_value = np.round(dataset.max(), 2)\n    quartile_1 = np.round(dataset.quantile(0.25), 2)\n    quartile_3 = np.round(dataset.quantile(0.75), 2)\n    \n    pdHoldValues = DataFrame(columns={'Min', 'Max', 'Mean', 'Q1_25', 'Median', 'Q3_75th', 'IQR'})\n    # Interquartile range\n    iqr = np.round(quartile_3 - quartile_1, 2)\n    \n    pdHoldValues['Min']    = min_value\n    pdHoldValues['Mean']   = mean\n    pdHoldValues['Max']    = max_value\n    pdHoldValues['Q1_25']  = quartile_1\n    pdHoldValues['Q3_75th']   = quartile_3\n    pdHoldValues['Median'] = median\n    pdHoldValues['IQR']    = iqr\n    \n    return pdHoldValues\n\n# Compute values for the outliers\ndef treate_outliers(X, lstInputFeatures, pdIQRValues, strTreatmentType):\n    \n    if(strTreatmentType == 'LowerBound'):\n            \n        for colName in lstInputFeatures:\n            \n            floatLowerLim = pdIQRValues[ pdIQRValues['index']==colName ]['floatLowerLim'].values[0]\n            #print(colName, \"imputing lowest value as: \", floatLowerLim)\n            #print(\"Before imputing: min:\",round(X[colName].min(), 5), \"mean:\",round(X[colName].mean(), 5), \"max:\",round(X[colName].max(),5))\n            \n            X.loc[ X[colName] < floatLowerLim, colName ] = floatLowerLim\n            #print(\"After imputing: min:\", X[colName].min(), \"mean:\",X[colName].mean(), \"max:\",X[colName].max())\n\n    \n    elif(strTreatmentType=='UpperBound'):\n        \n        for colName in lstInputFeatures:\n            floatUpperLim = pdIQRValues[ pdIQRValues['index']==colName ]['floatUpperLim'].values[0]\n            #print(\"\\n\",colName, \"imputing highest value as: \", floatUpperLim)\n            #print(\"Before: \",X[colName].min(), X[colName].mean(), X[colName].max())\n            X.loc[ X[colName] >= floatUpperLim, colName ] = floatUpperLim\n            #print(\"After: \", X[colName].min(), X[colName].mean(), X[colName].max())\n        \n    return X\n\ndef outlier_treatment(X, lstInputFeatures):\n    pdIQRValues = get_summary_statistics(X)\n    pdIQRValues['floatLowerLim'] = pdIQRValues['Q1_25'] - pdIQRValues['IQR'] * 1.5\n    pdIQRValues['floatUpperLim'] = pdIQRValues['Q3_75th'] + pdIQRValues['IQR'] * 1.5\n    pdIQRValues = pdIQRValues[['floatLowerLim', 'floatUpperLim']].reset_index()\n\n    \n    plt.figure(figsize=(17, 7))\n    plt.title(f'Before computing for the outlier values for each para:')\n    sns.boxplot(data=X, orient=\"h\", palette=\"Set1\")\n    plt.show()\n        \n    X = treate_outliers(X, lstInputFeatures, pdIQRValues, \"LowerBound\")\n    X = treate_outliers(X, lstInputFeatures, pdIQRValues, \"UpperBound\")\n    \n    plt.figure(figsize=(17, 7))\n    plt.title(f'After computing for the outlier values for each para:')\n    sns.boxplot(data=X, orient=\"h\", palette=\"Set1\")\n    plt.show()\n    \n    return X\n\ndef calculate_rmse( serActual, serPredicted ):\n    MSE = metrics.mean_squared_error(serActual, serPredicted)\n    print(\"MSE: \", MSE, \"   RMSE: \", round(math.sqrt(MSE), 5) )","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:48:29.663872Z","iopub.execute_input":"2021-08-02T12:48:29.664247Z","iopub.status.idle":"2021-08-02T12:48:29.703456Z","shell.execute_reply.started":"2021-08-02T12:48:29.664207Z","shell.execute_reply":"2021-08-02T12:48:29.702134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdFeatures = return_all_index_framework(pdInputTrainData, 'training')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:51:13.128913Z","iopub.execute_input":"2021-08-02T12:51:13.129299Z","iopub.status.idle":"2021-08-02T12:51:22.916647Z","shell.execute_reply.started":"2021-08-02T12:51:13.129269Z","shell.execute_reply":"2021-08-02T12:51:22.915507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='EDA'></a>\n# 2. EDA\n1. Find corlation between derived parameters and target if present any.","metadata":{}},{"cell_type":"code","source":"plt.figure( figsize=(17,8) )\nsns.heatmap( pdFeatures.corr(method ='pearson').sort_values('target'), annot=True, fmt=\"f\" )\nplt.xticks( rotation=45 )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:51:22.918194Z","iopub.execute_input":"2021-08-02T12:51:22.918521Z","iopub.status.idle":"2021-08-02T12:51:25.319285Z","shell.execute_reply.started":"2021-08-02T12:51:22.918457Z","shell.execute_reply":"2021-08-02T12:51:25.317722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstInputFeatures=['Flesch', 'SMOG', 'FleschKincaid', 'ColemanLiau', 'Automated', 'DaleChall', 'DifficultWords', 'LinsearWrite',\n       'GunningFog', 'TextStd', 'FernandezHuerta', 'SzigrisztPazos',\n       'GutierrezPolini', 'CrawFord', 'PosMean', 'NegMean']\npdFeatures = pdFeatures[lstInputFeatures + ['target']]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:51:25.321558Z","iopub.execute_input":"2021-08-02T12:51:25.322015Z","iopub.status.idle":"2021-08-02T12:51:25.330767Z","shell.execute_reply.started":"2021-08-02T12:51:25.321965Z","shell.execute_reply":"2021-08-02T12:51:25.32928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data=pdFeatures, y_vars='target', kind='reg')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:51:25.509038Z","iopub.execute_input":"2021-08-02T12:51:25.509437Z","iopub.status.idle":"2021-08-02T12:51:35.419464Z","shell.execute_reply.started":"2021-08-02T12:51:25.509405Z","shell.execute_reply":"2021-08-02T12:51:35.418346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdFeatures.plot(figsize=(17, 6))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:51:35.421373Z","iopub.execute_input":"2021-08-02T12:51:35.421819Z","iopub.status.idle":"2021-08-02T12:51:36.884062Z","shell.execute_reply.started":"2021-08-02T12:51:35.421771Z","shell.execute_reply":"2021-08-02T12:51:36.883231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdFeatures[['target', 'PosMean', 'NegMean']].plot(figsize=(17, 6))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:51:36.885643Z","iopub.execute_input":"2021-08-02T12:51:36.885957Z","iopub.status.idle":"2021-08-02T12:51:37.376823Z","shell.execute_reply.started":"2021-08-02T12:51:36.885923Z","shell.execute_reply":"2021-08-02T12:51:37.375739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='ModelDevelopment'></a>\n# 3. Multi Linear Model training-testing to finalise the model(s)","metadata":{}},{"cell_type":"code","source":"def linear_models(X_train, y_train, X_test, y_test):\n    \n    lstClassifiers = [\n                        svm.SVR(kernel='poly', degree=2, max_iter=-1), # SVR: Epsilon-Support Vector Regression.\n                        linear_model.SGDRegressor(penalty='l1', learning_rate='adaptive'),\n                        linear_model.BayesianRidge(),\n                        #linear_model.LassoLars(),\n                        linear_model.ARDRegression(),\n                        linear_model.PassiveAggressiveRegressor(),\n                        linear_model.TheilSenRegressor(),\n                        linear_model.LinearRegression()\n    ]\n    \n    pdModelsOp=DataFrame()      \n    \n    # Train models\n    for clf in lstClassifiers:\n        clf.fit(X_train.values, y_train.values)\n        \n        strModelName=str(clf)\n        \n        pdModelsOp[strModelName]=clf.predict(X_test)\n        print(strModelName)\n        calculate_rmse( y_test, pdModelsOp[strModelName] )\n        \n        \n    pdModelsOp['Actual'] = y_test.values\n    \n    pdModelsOp=pdModelsOp.rename(columns={\\\n                \"\"\"SVR(degree=2, kernel='poly')\"\"\":'SVR',\\\n                \"\"\"SGDRegressor(learning_rate='adaptive', penalty='l1')\"\"\":'SGDR',\\\n                'BayesianRidge()':'BR',\n                'ARDRegression()':'ARDR', \n                'PassiveAggressiveRegressor()':'PAR',\\\n                'TheilSenRegressor(max_subpopulation=10000)':'TSR',  \\\n                'LinearRegression()':'LR',\n                #'LassoLars()':'LL', \n                'Actual':'target'})\n    \n    return pdModelsOp","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:51:37.378186Z","iopub.execute_input":"2021-08-02T12:51:37.378478Z","iopub.status.idle":"2021-08-02T12:51:37.388742Z","shell.execute_reply.started":"2021-08-02T12:51:37.378449Z","shell.execute_reply":"2021-08-02T12:51:37.387593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### OBSERVATION:\n- After rying out different combinations of parameters and cmparing their RMSE finalised parameters to use. ","metadata":{}},{"cell_type":"code","source":"X, Y = pdFeatures[lstInputFeatures], pdFeatures['target']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=333)\npdModelsOp=linear_models(X_train, y_train, X_test, y_test)\nplt.figure( figsize=(17,8) )\nsns.heatmap( pdModelsOp.corr(method ='pearson').sort_values('target'), annot=True, fmt=\"f\" )\nplt.xticks( rotation=45 )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:54:31.094551Z","iopub.execute_input":"2021-08-02T12:54:31.094924Z","iopub.status.idle":"2021-08-02T12:54:34.061968Z","shell.execute_reply.started":"2021-08-02T12:54:31.094891Z","shell.execute_reply":"2021-08-02T12:54:34.060919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdModelsOp[['SGDR', 'BR', 'ARDR', 'TSR', 'LR', 'target']].plot(figsize=(17,7))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:57:09.77832Z","iopub.execute_input":"2021-08-02T12:57:09.778807Z","iopub.status.idle":"2021-08-02T12:57:10.155778Z","shell.execute_reply.started":"2021-08-02T12:57:09.778772Z","shell.execute_reply":"2021-08-02T12:57:10.154827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### OBSERVATION: Model selected \n- After comparing performance of all the linear models I am selecting SGDRegressor is selected for the final prediction","metadata":{}},{"cell_type":"code","source":"# Train SVR model on training data for the prediction on test data\nSGDRegressor=linear_model.SGDRegressor(penalty='l1', learning_rate='adaptive')\nX, Y = pdFeatures[lstInputFeatures], pdFeatures['target']\nSGDRegressor.fit(X.values, Y.values)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:58:02.239053Z","iopub.execute_input":"2021-08-02T12:58:02.239464Z","iopub.status.idle":"2021-08-02T12:58:02.26927Z","shell.execute_reply.started":"2021-08-02T12:58:02.239428Z","shell.execute_reply":"2021-08-02T12:58:02.268226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='SubmitFinalOutput'></a>\n# 4. Submit Final Output\n1. Preparing Test data to predict possible readability value ","metadata":{}},{"cell_type":"code","source":"pdTest=read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\npdSample=read_csv('/kaggle/input/commonlitreadabilityprize/sample_submission.csv')\n%time pdTest=return_all_index_framework(pdTest, 'testing')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:58:15.94793Z","iopub.execute_input":"2021-08-02T12:58:15.948329Z","iopub.status.idle":"2021-08-02T12:58:17.032689Z","shell.execute_reply.started":"2021-08-02T12:58:15.948293Z","shell.execute_reply":"2021-08-02T12:58:17.031726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = SGDRegressor.predict(pdTest)\npdSample['target']=np.round(y_pred, 6)\npdSample","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:21:39.300676Z","iopub.execute_input":"2021-08-02T13:21:39.301221Z","iopub.status.idle":"2021-08-02T13:21:39.318367Z","shell.execute_reply.started":"2021-08-02T13:21:39.301184Z","shell.execute_reply":"2021-08-02T13:21:39.317465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdSample.to_csv(\"submission.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:21:44.696704Z","iopub.execute_input":"2021-08-02T13:21:44.697078Z","iopub.status.idle":"2021-08-02T13:21:44.717246Z","shell.execute_reply.started":"2021-08-02T13:21:44.69705Z","shell.execute_reply":"2021-08-02T13:21:44.716414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='SubmitFinalOutput'></a>\n# 5. Save Final Model","metadata":{}},{"cell_type":"code","source":"# To save and reuse the model use following code        \ndef save_model(model, strModelName):\n    import pickle\n    # save the model to disk\n    #filename = 'finalized_model.sav'\n    filename = strModelName + '.sav'\n    pickle.dump(model, open(filename, 'wb'))\n    \n    return \n\n#save_model(clf, strModelName)\n\ndef load_model(strModelName):\n    \n    filename = strModelName + '.sav'\n    # load the model from disk\n    loaded_model = pickle.load(open(filename, 'rb'))\n    \n    return loaded_model\n\n\n\n#loaded_model = load_model(strModelName)\n#result = loaded_model.score(X_test, y_test)\n#print(result)","metadata":{},"execution_count":null,"outputs":[]}]}