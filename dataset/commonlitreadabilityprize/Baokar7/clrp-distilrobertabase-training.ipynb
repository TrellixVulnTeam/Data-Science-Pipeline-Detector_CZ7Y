{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-05T16:15:47.615184Z","iopub.execute_input":"2021-07-05T16:15:47.615555Z","iopub.status.idle":"2021-07-05T16:15:47.625181Z","shell.execute_reply.started":"2021-07-05T16:15:47.615476Z","shell.execute_reply":"2021-07-05T16:15:47.62404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport re\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tqdm.notebook import tqdm\nimport transformers\nimport torch\nimport torch.nn as nn\nfrom transformers import get_linear_schedule_with_warmup, AdamW, get_cosine_schedule_with_warmup\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup\nfrom transformers import get_polynomial_decay_schedule_with_warmup\nfrom sklearn.metrics import mean_squared_error\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:15:48.035093Z","iopub.execute_input":"2021-07-05T16:15:48.035486Z","iopub.status.idle":"2021-07-05T16:15:55.133549Z","shell.execute_reply.started":"2021-07-05T16:15:48.035439Z","shell.execute_reply":"2021-07-05T16:15:55.132757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:15:55.136316Z","iopub.execute_input":"2021-07-05T16:15:55.136678Z","iopub.status.idle":"2021-07-05T16:15:55.237714Z","shell.execute_reply.started":"2021-07-05T16:15:55.136642Z","shell.execute_reply":"2021-07-05T16:15:55.236951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained('distilroberta-base')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:15:57.510101Z","iopub.execute_input":"2021-07-05T16:15:57.51041Z","iopub.status.idle":"2021-07-05T16:16:26.664638Z","shell.execute_reply.started":"2021-07-05T16:15:57.510381Z","shell.execute_reply":"2021-07-05T16:16:26.663835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 256\ntokenizer(train.excerpt[0], return_tensors='pt', max_length=256, padding='max_length')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:16:26.666046Z","iopub.execute_input":"2021-07-05T16:16:26.666361Z","iopub.status.idle":"2021-07-05T16:16:26.715235Z","shell.execute_reply.started":"2021-07-05T16:16:26.666327Z","shell.execute_reply":"2021-07-05T16:16:26.714385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer(train.excerpt[0], return_tensors='pt', max_length=256, padding='max_length')\nclass train_valid_dataset():\n    def __init__(self, excerpt, target):\n        self.excerpt = excerpt\n        self.target = target\n        self.maxlen = MAX_LEN\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n    def __getitem__(self, index):\n        excerpt = str(self.excerpt[index])\n        \n        tokenizer_dict = tokenizer(excerpt,\n                                  return_tensors='pt',\n                                  max_length=self.maxlen,\n                                  padding='max_length',\n                                  truncation=True)\n        \n        ids = tokenizer_dict['input_ids']\n        mask = tokenizer_dict['attention_mask']\n        \n        return {'ids': torch.tensor(ids, dtype=torch.long),\n               'mask': torch.tensor(mask, dtype=torch.long),\n               'target': torch.tensor(self.target[index], dtype=torch.float)}","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:16:29.531285Z","iopub.execute_input":"2021-07-05T16:16:29.531625Z","iopub.status.idle":"2021-07-05T16:16:29.538886Z","shell.execute_reply.started":"2021-07-05T16:16:29.531593Z","shell.execute_reply":"2021-07-05T16:16:29.537932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = transformers.AutoModelForSequenceClassification.from_pretrained('distilroberta-base',\n                                                                        num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:16:38.241112Z","iopub.execute_input":"2021-07-05T16:16:38.24144Z","iopub.status.idle":"2021-07-05T16:17:12.059934Z","shell.execute_reply.started":"2021-07-05T16:16:38.24141Z","shell.execute_reply":"2021-07-05T16:17:12.059255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_function(dataloader, model, device, optimizer, scheduler):\n    model.train()\n    sum_loss = 0.0\n    total = 0\n    iterator = tqdm(enumerate(dataloader),\n                   total=len(dataloader))\n    for index, data in iterator:\n        ids = data['ids']\n        mask = data['mask']\n        targets = data['target']\n        \n        ids = ids.squeeze()\n        mask = mask.squeeze()\n        \n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(ids,\n                       mask)\n        \n        outputs = outputs.logits.squeeze()\n        \n        loss = F.mse_loss(outputs, targets)\n        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        sqrt_loss = np.sqrt(loss.detach().cpu().numpy())\n        \n        sum_loss += sqrt_loss*targets.shape[0]\n        \n        total += targets.shape[0]\n    \n    return sum_loss/total","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:17:12.061996Z","iopub.execute_input":"2021-07-05T16:17:12.062349Z","iopub.status.idle":"2021-07-05T16:17:12.071037Z","shell.execute_reply.started":"2021-07-05T16:17:12.06231Z","shell.execute_reply":"2021-07-05T16:17:12.069874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation_function(dataloader, model, device):\n    model.eval()\n    sum_loss = 0.0\n    total = 0\n    iterator = tqdm(enumerate(dataloader),\n                   total=len(dataloader))\n    for index, data in iterator:\n        ids = data['ids']\n        mask = data['mask']\n        targets = data['target']\n        \n        ids = ids.squeeze()\n        mask = mask.squeeze()\n        \n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        \n        with torch.no_grad():\n            outputs = model(ids,\n                           mask)\n        \n        outputs = outputs.logits.squeeze()\n        \n        loss = F.mse_loss(outputs, targets)\n        \n        sqrt_loss = np.sqrt(loss.detach().cpu().numpy())\n        \n        sum_loss += sqrt_loss*targets.shape[0]\n        \n        total += targets.shape[0]\n    \n    return sum_loss/total","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:17:12.072566Z","iopub.execute_input":"2021-07-05T16:17:12.072953Z","iopub.status.idle":"2021-07-05T16:17:12.085249Z","shell.execute_reply.started":"2021-07-05T16:17:12.072914Z","shell.execute_reply":"2021-07-05T16:17:12.084302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_bins = int(np.floor(1 + np.log2(len(train))))\ntrain.loc[:, 'bins'] = pd.cut(train['target'], bins=num_bins, labels=False)\nbins = train.bins.to_numpy()\nskfold = StratifiedKFold(n_splits=5)\nnew = skfold.split(train, bins)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:17:12.086547Z","iopub.execute_input":"2021-07-05T16:17:12.086885Z","iopub.status.idle":"2021-07-05T16:17:12.101573Z","shell.execute_reply.started":"2021-07-05T16:17:12.086848Z","shell.execute_reply":"2021-07-05T16:17:12.100664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\ndevice = torch.device('cuda')\n\nfor k, (train_idx, valid_idx) in enumerate(new):\n    print(\"******************** FOLD: %d ********************\" % (k+1))\n    x_train, y_train = train.iloc[train_idx], train.target.iloc[train_idx]\n    x_valid, y_valid = train.iloc[valid_idx], train.target.iloc[valid_idx]\n    \n    x_train = x_train.reset_index(drop=True)\n    y_train = y_train.reset_index(drop=True)\n    x_valid = x_valid.reset_index(drop=True)\n    y_valid = y_valid.reset_index(drop=True)\n    \n    model = transformers.AutoModelForSequenceClassification.from_pretrained('distilroberta-base',\n                                                                            num_labels=1)\n    model.to(device)\n\n    train_dataset = train_valid_dataset(excerpt=x_train.excerpt,\n                                       target=y_train)\n    train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                                  batch_size=16)\n    valid_dataset = train_valid_dataset(excerpt=x_valid.excerpt,\n                                       target=y_valid)\n    valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,\n                                                  batch_size=8)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0},]\n    optimizer = AdamW(optimizer_parameters, lr=0.00002)\n    training_steps = int(len(train_dataloader)*epochs)\n#     num_warmup_steps = int(training_steps*0.1)\n    num_warmup_steps = 0\n    scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=num_warmup_steps,\n                                                num_training_steps=training_steps)\n    \n    for epoch in range(epochs):\n        print(\"#################### EPOCH: %d ####################\" % (epoch+1))\n        train_loss = training_function(train_dataloader, model, device, optimizer, scheduler)\n        valid_loss = validation_function(valid_dataloader, model, device)\n        print(\"Training Loss: %f, Validation Loss: %f\" % (train_loss, valid_loss))\n    model.save_pretrained('./model_'+str(k+1))\n    tokenizer.save_pretrained('./model_'+str(k+1))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:17:49.284195Z","iopub.execute_input":"2021-07-05T16:17:49.284517Z","iopub.status.idle":"2021-07-05T16:33:27.704321Z","shell.execute_reply.started":"2021-07-05T16:17:49.284483Z","shell.execute_reply":"2021-07-05T16:33:27.703442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}