{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We will create a baseline BERT model following the [excellent notebook](https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub) written by @xhlulu. This work also has been recently implemented [here](https://www.kaggle.com/jeongyoonlee/tf-keras-bert-baseline-training-inference)","metadata":{}},{"cell_type":"code","source":"debug = False\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's first import few librarie.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport os\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nfrom tensorflow.keras.metrics import RootMeanSquaredError\n\nimport tokenization\nfrom sklearn.manifold import TSNE\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\ntf.random.set_seed(seed)\nimport plotly\nimport plotly.graph_objs as go\nfrom plotly.graph_objs import FigureWidget\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:08:47.952347Z","iopub.execute_input":"2021-05-20T13:08:47.952678Z","iopub.status.idle":"2021-05-20T13:08:48.225365Z","shell.execute_reply.started":"2021-05-20T13:08:47.952648Z","shell.execute_reply":"2021-05-20T13:08:48.223714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create our training data by randomly selecting 70% of records and our test/validation set by the rest of the records","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n\nif debug == True:\n    data = data.tail(100)\n\nshuffle_df = data.sample(frac=1)\n\ntrain_size = int(0.7 * len(data))\n\n\ntrain = shuffle_df[:train_size]\ntest = shuffle_df[train_size:]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:46:22.77563Z","iopub.execute_input":"2021-05-20T12:46:22.775962Z","iopub.status.idle":"2021-05-20T12:46:22.85872Z","shell.execute_reply.started":"2021-05-20T12:46:22.775932Z","shell.execute_reply":"2021-05-20T12:46:22.857791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:41:41.818826Z","iopub.execute_input":"2021-05-20T12:41:41.81919Z","iopub.status.idle":"2021-05-20T12:42:16.027199Z","shell.execute_reply.started":"2021-05-20T12:41:41.819158Z","shell.execute_reply":"2021-05-20T12:42:16.026206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=205, first=True):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        if first == True:\n            text = text[:max_len-2]\n        else: \n            text = text[-(max_len-2):]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:47:03.577203Z","iopub.execute_input":"2021-05-20T12:47:03.577551Z","iopub.status.idle":"2021-05-20T12:47:03.586306Z","shell.execute_reply.started":"2021-05-20T12:47:03.57752Z","shell.execute_reply":"2021-05-20T12:47:03.585398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(bert_layer, max_len=205):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='linear')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:46:49.120753Z","iopub.execute_input":"2021-05-20T12:46:49.121074Z","iopub.status.idle":"2021-05-20T12:46:49.128145Z","shell.execute_reply.started":"2021-05-20T12:46:49.121045Z","shell.execute_reply":"2021-05-20T12:46:49.127315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain_input = bert_encode(train.excerpt.values, tokenizer, first=True)\ntest_input = bert_encode(test.excerpt.values, tokenizer, first=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:47:06.185836Z","iopub.execute_input":"2021-05-20T12:47:06.186146Z","iopub.status.idle":"2021-05-20T12:47:06.594505Z","shell.execute_reply.started":"2021-05-20T12:47:06.186117Z","shell.execute_reply":"2021-05-20T12:47:06.593616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels = train.target.values\ntest_labels = test.target.values","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:47:14.773646Z","iopub.execute_input":"2021-05-20T12:47:14.773961Z","iopub.status.idle":"2021-05-20T12:47:14.777684Z","shell.execute_reply.started":"2021-05-20T12:47:14.773932Z","shell.execute_reply":"2021-05-20T12:47:14.776848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model(bert_layer, max_len=205)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:47:21.915917Z","iopub.execute_input":"2021-05-20T12:47:21.916255Z","iopub.status.idle":"2021-05-20T12:47:23.33614Z","shell.execute_reply.started":"2021-05-20T12:47:21.916222Z","shell.execute_reply":"2021-05-20T12:47:23.335447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_first = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\ntrain_history_first = model.fit(train_input, train_labels,validation_data=(test_input, test_labels),epochs=10,callbacks=[checkpoint_first],batch_size=8)\ntest_pred_first = model.predict(test_input)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:47:31.789621Z","iopub.execute_input":"2021-05-20T12:47:31.789952Z","iopub.status.idle":"2021-05-20T12:48:46.214545Z","shell.execute_reply.started":"2021-05-20T12:47:31.789919Z","shell.execute_reply":"2021-05-20T12:48:46.213672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, our model development is complete. We now look at the predictions that it generated in the validation set","metadata":{}},{"cell_type":"code","source":"print(\"validation rmse is\", mean_squared_error(test.target, test_pred_first, squared=False))\nplt.hist(test_pred_first, bins = 50)\nplt.title(\"Distribution of predictions on validation set\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:49:50.729908Z","iopub.execute_input":"2021-05-20T12:49:50.730242Z","iopub.status.idle":"2021-05-20T12:49:50.950166Z","shell.execute_reply.started":"2021-05-20T12:49:50.730212Z","shell.execute_reply":"2021-05-20T12:49:50.949352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we extract the features from the penultimate layer of the BERT model that we developed. ","metadata":{}},{"cell_type":"code","source":"extract = Model(model.inputs, model.layers[-2].output)\nfeatures_last = extract.predict(test_input)\nfeatures_last.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:58:42.01134Z","iopub.execute_input":"2021-05-20T12:58:42.011669Z","iopub.status.idle":"2021-05-20T12:58:45.590679Z","shell.execute_reply.started":"2021-05-20T12:58:42.01164Z","shell.execute_reply":"2021-05-20T12:58:45.589747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we perform TSNE on these features to visualize these embeddings in smaller dimension. We create 2 tsne components and use the target variable to plot the scatter diagram in 3 dimensions.  ","metadata":{}},{"cell_type":"code","source":"tsne = TSNE(n_components=2 , random_state=0)\ndata_tsne = tsne.fit_transform(features_last)\n\ndata_tsne\n\ndata_tsne = pd.DataFrame(data_tsne , columns=['tsne1' , 'tsne2'])\ndata_tsne.head()\n\ndata_tsne[\"target\"] = test[\"target\"].values\n\ntraces = go.Scatter3d(\n    x=data_tsne['tsne1'],\n    y=data_tsne['tsne2'],\n    z=data_tsne['target'],\n    mode='markers',\n    marker=dict(\n        size=4,\n        opacity=0.2,\n        colorscale='Viridis',\n     )\n)\n\nlayout = go.Layout(\n    autosize=True,\n    showlegend=True,\n    width=800,\n    height=1000,\n)\n\nFigureWidget(data=[traces], layout=layout)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T12:58:49.368962Z","iopub.execute_input":"2021-05-20T12:58:49.369305Z","iopub.status.idle":"2021-05-20T12:58:50.978876Z","shell.execute_reply.started":"2021-05-20T12:58:49.369252Z","shell.execute_reply":"2021-05-20T12:58:50.97801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we need to look carefully by rotating the plot, do we see the linear pattern between the two t-sne components and the target variable from any plane? Also, if there is non-linearity at some places, we see how to use a [tree based model](https://www.kaggle.com/maunish/clrp-roberta-svm) on top of these embeddings. We also check if the texts with high standard error and low standard errors are somehow separate or not. We first create an indicator in the test data which would indicate if the texts are corresponding to high standard error or low. We use the mean value of training set standard error to avoid any information leakage.","metadata":{}},{"cell_type":"code","source":"test['std'] = np.where(test['standard_error']>train.standard_error.mean(), 1, 0)\ntest['std'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:08:50.488104Z","iopub.execute_input":"2021-05-20T13:08:50.488454Z","iopub.status.idle":"2021-05-20T13:08:50.504543Z","shell.execute_reply.started":"2021-05-20T13:08:50.488421Z","shell.execute_reply":"2021-05-20T13:08:50.503869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traces = go.Scatter3d(\n    x=data_tsne['tsne1'],\n    y=data_tsne['tsne2'],\n    z=data_tsne['target'],\n    mode='markers',\n    marker=dict(\n        size=4,\n        opacity=0.2,\n        colorscale='Viridis',\n        color = test['std'].values\n     )\n)\n\nlayout = go.Layout(\n    autosize=True,\n    showlegend=True,\n    width=800,\n    height=1000,\n)\n\nFigureWidget(data=[traces], layout=layout)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:09:30.001549Z","iopub.execute_input":"2021-05-20T13:09:30.002002Z","iopub.status.idle":"2021-05-20T13:09:30.066069Z","shell.execute_reply.started":"2021-05-20T13:09:30.00195Z","shell.execute_reply":"2021-05-20T13:09:30.065192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above plot, we just coloured points of the first plot. Points with yellow colours are corresponding to texts with high standard deviation. Are the yellow and non-yellow points following any pattern or they are randomly spaced in the space (with respect to x and y axes)? If their presence is random in nature, then it could be a good news becasue the bert embedding space does not face a big problem (while predicting tharget) between high and low standard deviation texts. However, if we look closely, we see a pattern. If we keep z axis vertically with respect to our screen, then we see that yellow points are more concentrated towrds two tails of z (that is the target column) axis, this is expected, as we earlier observed any many EDAs that the target and standard error in this data have U-shaped relationship. That means targets with high and low values have high standard error values. ","metadata":{}}]}