{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import AdaBoostRegressor as Ada\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.neural_network import MLPRegressor as MP\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T04:03:31.719545Z","iopub.execute_input":"2021-06-02T04:03:31.719862Z","iopub.status.idle":"2021-06-02T04:03:31.725197Z","shell.execute_reply.started":"2021-06-02T04:03:31.71983Z","shell.execute_reply":"2021-06-02T04:03:31.724042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load in the train and test data....","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T04:03:31.726913Z","iopub.execute_input":"2021-06-02T04:03:31.727589Z","iopub.status.idle":"2021-06-02T04:03:31.767825Z","shell.execute_reply.started":"2021-06-02T04:03:31.72755Z","shell.execute_reply":"2021-06-02T04:03:31.767128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Brief EDA...we know that some is potentially irrelevant ie: copyright and error, as they're not included in the testing/scoring sets.","metadata":{}},{"cell_type":"code","source":"train.head()\nprint(\"Train length \",len(train))\ntest.head()\nprint(\"Test length \", len(test))\ntarget = train[\"target\"]\n#Create submission file\nsub = pd.DataFrame(test[\"id\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T04:03:31.781735Z","iopub.execute_input":"2021-06-02T04:03:31.781983Z","iopub.status.idle":"2021-06-02T04:03:31.788649Z","shell.execute_reply.started":"2021-06-02T04:03:31.781959Z","shell.execute_reply":"2021-06-02T04:03:31.787756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setup a simple Ada Boost training","metadata":{}},{"cell_type":"code","source":"A = Ada(random_state = 42)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T04:03:31.790256Z","iopub.execute_input":"2021-06-02T04:03:31.790769Z","iopub.status.idle":"2021-06-02T04:03:31.797978Z","shell.execute_reply.started":"2021-06-02T04:03:31.790732Z","shell.execute_reply":"2021-06-02T04:03:31.797027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Simple Feature Engineer","metadata":{}},{"cell_type":"code","source":"train['exerpt_len'] = train.excerpt.str.len()\n\ntest['exerpt_len'] = test.excerpt.str.len()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T04:03:31.799574Z","iopub.execute_input":"2021-06-02T04:03:31.799939Z","iopub.status.idle":"2021-06-02T04:03:31.811632Z","shell.execute_reply.started":"2021-06-02T04:03:31.799891Z","shell.execute_reply":"2021-06-02T04:03:31.810752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"C=CV(strip_accents='unicode')\n\ntrain_dict = C.fit_transform(train.excerpt.append(test.excerpt))\nnames = C.get_feature_names()\n#Drop out all the possible conflicting column headings....\nnew_test = test.drop([\"url_legal\",\"license\",\"id\",\"excerpt\"],axis=1)\nnew_train = train.drop([\"url_legal\",\"license\",\"id\",\"excerpt\",\"target\",\"standard_error\"],axis=1)\n#...join together\ntrain_dict=pd.DataFrame(train_dict.toarray(),columns=names)\ntest_dict=pd.DataFrame(C.transform(test.excerpt).toarray(),columns=names)\n\nname_len = pd.DataFrame(names)\nname_len[\"len\"] = name_len[0].str.len()\nndw=[]\nawl=[]\ngreater_than_6 = []\nmost =[]\n\n#Looking at word complexity etc...\nfor i in range(len(train)): #Subtly different length....\n    check = train_dict.iloc[i]      #Get the line\n    num_dist_word = check[check>0]  #Anything with a count >0\n    ndw.append(len(num_dist_word))  #How long is that list\n    len_ndw = num_dist_word.index.str.len()   #How long is each word in that list\n    awl.append(sum(len_ndw) / len(len_ndw)) \n    junk = len_ndw>6    #test to see how many words are >6 ??\n    greater_than_6.append(len(junk.nonzero()[0]))#Count TRUE\n    stuff = len_ndw.value_counts()#Find the most popular word length\n    most.append(stuff.index[0])\ntrain['num_dist_word'] = ndw\ntrain['avg_word_len'] = awl\ntrain['g_t_6'] = greater_than_6\ntrain['most_pop_len'] = most\n#repetition...could have made a func???\nndw=[]\nawl=[]\ngreater_than_6 = []\nmost =[]\n\nfor i in range(len(test_dict)):\n    check = test_dict.iloc[i]\n    num_dist_word = check[check>0]\n    ndw.append(len(num_dist_word))\n    len_ndw = num_dist_word.index.str.len()\n    awl.append(sum(len_ndw) / len(len_ndw))\n    junk = len_ndw>6\n    greater_than_6.append(len(junk.nonzero()[0]))\n    stuff = len_ndw.value_counts()\n    most.append(stuff.index[0])\ntest['num_dist_word'] = ndw\ntest['avg_word_len'] = awl\ntest['g_t_6'] = greater_than_6\ntest['most_pop_len'] = most\n#new_test = new_test.join(test_dict)\n#new_train = new_train.join(train_dict)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T04:03:31.814751Z","iopub.execute_input":"2021-06-02T04:03:31.815115Z","iopub.status.idle":"2021-06-02T04:03:37.029523Z","shell.execute_reply.started":"2021-06-02T04:03:31.815081Z","shell.execute_reply":"2021-06-02T04:03:37.028677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train and test....","metadata":{}},{"cell_type":"code","source":"#Drop out all the possible conflicting column headings....\nnew_test = test.drop([\"url_legal\",\"license\",\"id\",\"excerpt\"],axis=1)\nnew_train = train.drop([\"url_legal\",\"license\",\"id\",\"excerpt\",\"standard_error\"],axis=1)\n\n#Treat target separate so we can convert back easier??\ntarget = new_train.pop('target')\n#Normalisation\n# copy the data\nnew_train_scaled = new_train.copy()\n  \n# apply normalization techniques\nfor column in new_train_scaled.columns:\n    new_train_scaled[column] = (new_train_scaled[column] - new_train_scaled[column].min()) / (new_train_scaled[column].max() - new_train_scaled[column].min())    \n\n# copy the data\nnew_test_scaled = new_test.copy()\n  \n# apply normalization\nfor column in new_test_scaled.columns:\n    #Use the original new_train values\n    new_test_scaled[column] = (new_test_scaled[column] - new_train[column].min()) / (new_train[column].max() - new_train[column].min())    \n\nt_min = target.min()\nt_max = target.max()\n\ntarget_scaled = (target - t_min) / (t_max - t_min)    \n\n\nA = Ada(random_state = 42)\n\nA.fit(new_train,target_scaled)\n\n\"\"\"\n#Create Keras model\nlen_in = new_train_scaled.shape[1]\nlen_in_2 = (len_in*2) *2\n\nmodel = Sequential()\nmodel.add(layers.Dense(len_in_2,activation=\"relu\",input_dim=len_in))\nmodel.add(layers.Dense(len_in_2,activation=\"relu\"))\n#model.add(layers.SimpleRNN(len_in_2))\n#model.add(layers.Dense(len_in_2,activation=\"relu\"))\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer='Adam',loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\nmodel.fit(new_train_scaled,target_scaled,batch_size=1, epochs=10, shuffle=False)\n\"\"\"\nmodel = MP(activation='tanh',random_state = 42, hidden_layer_sizes=(100,100,25),verbose=1,solver='adam',)\nmodel.fit(new_train_scaled, target_scaled)\n\nA_pred = A.predict(new_test_scaled)\n\nNN_pred = model.predict(new_test_scaled)\n#Stack them??\nNN_pred = (A_pred + NN_pred)/2\n\nNN_pred_rescale = (NN_pred * (t_max - t_min)) + t_min\n#**Need to rescale output to remove normalisation\n\n#sub[\"target\"] = A.predict(new_test)\nsub[\"target\"] = NN_pred_rescale\n\n\nsub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T04:03:37.031126Z","iopub.execute_input":"2021-06-02T04:03:37.031462Z","iopub.status.idle":"2021-06-02T04:03:38.102882Z","shell.execute_reply.started":"2021-06-02T04:03:37.031427Z","shell.execute_reply":"2021-06-02T04:03:38.10144Z"},"trusted":true},"execution_count":null,"outputs":[]}]}