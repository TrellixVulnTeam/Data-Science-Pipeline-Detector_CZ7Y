{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit: Detailed Guide\n\n<a id=\"section-zero\"></a>\n\n<p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Learning NLP </p>\n# TABLE OF CONTENTS\n\n\n* [Library Importations](#section-library-importations)\n* [Loading Datasets](#section-loading-datasets)\n* [Exploratory Data Analysis](#section-EDA)\n* [Data Preprocessing](#section-preprocessing)\n    - [Data Cleaning](#subsection-datacleaning) \n    - [Stemming](#subsection-stemming) \n    - [Lemmatization](#subsection-lemmatization)\n* [Part-of-Speech Tagging](#section-pos)\n* [Named Entity Recognition](#section-ner)\n* [Bag of Words + Models](#section-bow)\n    - [Linear Regression](#subsection-bow-lr) \n    - [Ridge Regression](#subsection-bow-ridge)  \n    - [Extreme Gradient Boosting](#subsection-bow-xgb)  \n* [TD IDF + Models](#section-tdidf)\n    - [Linear Regression](#section-tdidf) \n    - [Ridge Regression](#section-tdidf)  \n    - [Extreme Gradient Boosting](#section-tdidf)  \n    - [Lasso Regression](#section-tdidf) \n    - [Tweedie Regression](#section-tdidf)  \n    - [Huber Regression](#section-tdidf)  \n* [Embedding + Models](#section-wordembeddings)\n    - [Simple Embedding](#subsection-embedding)\n    - [Convolutional Neural Networks](#subsection-CNN)\n    - [Gated Recurrent Units](#subsection-GRU)\n    - [Single Long Short Term Memory](#subsection-LSTM)\n    - [Multiple Long Short Term Memory](#subsection-multiple-LSTM)\n* [Hyper Parameters Tuning](#section-hyperparametertuning)\n    - [Random Search](#subsection-randomsearch)\n    - [Hyperband](#subsection-hyperband)\n* [Glove Embeddings](#section-gloveembeddings)\n    - [Extreme Gradient Boosting](#subsection-glovexgb)\n    - [Stacked LSTM](#subsection-gloveLSTM)\n* [BERT Huggingface Transformer](#section-bert)\n* [RoBerta HuggingFace Transformer](#section-robertabase)\n* [Submission](#section-submission)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-library-importations\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Library Importations </p>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport string\nimport re\nimport math\n\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, TweedieRegressor,HuberRegressor\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import mean_squared_error as mse\n\n\nimport xgboost as xgb\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n\nimport kerastuner as kt\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.io import curdoc, show, output_notebook\noutput_notebook()\n\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nstop_words = stopwords.words('english')\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')\nfrom spacy import displacy\n\nimport transformers\nfrom transformers import BertTokenizer, TFBertModel, RobertaTokenizer, TFRobertaModel","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:06:20.341467Z","iopub.execute_input":"2021-06-09T06:06:20.341856Z","iopub.status.idle":"2021-06-09T06:06:36.439871Z","shell.execute_reply.started":"2021-06-09T06:06:20.341754Z","shell.execute_reply":"2021-06-09T06:06:36.439026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-loading-datasets\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Loading Datasets </p>","metadata":{}},{"cell_type":"markdown","source":"Use pandas's read_csv function to read dataframe and print it's shape.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ndf_test = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\ndf_submission = pd.read_csv('/kaggle/input/commonlitreadabilityprize/sample_submission.csv')\n\nprint(\" Training dataset shape : \" + str(df_train.shape))\nprint(\" Testing dataset shape : \" + str(df_test.shape))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:06:51.031886Z","iopub.execute_input":"2021-06-09T06:06:51.032198Z","iopub.status.idle":"2021-06-09T06:06:51.114034Z","shell.execute_reply.started":"2021-06-09T06:06:51.032168Z","shell.execute_reply":"2021-06-09T06:06:51.113246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['excerpt'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-EDA\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Exploratory Data Analysis </p>","metadata":{}},{"cell_type":"markdown","source":"Only url_legal and license columns appear to be having missing values","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Examples with the 5 lowest target values","metadata":{}},{"cell_type":"code","source":"display(df_train.sort_values(by=['target']).head())","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:04:21.447197Z","iopub.execute_input":"2021-06-01T16:04:21.447542Z","iopub.status.idle":"2021-06-01T16:04:21.4691Z","shell.execute_reply.started":"2021-06-01T16:04:21.447511Z","shell.execute_reply":"2021-06-01T16:04:21.468266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Examples with the 5 highest target values","metadata":{}},{"cell_type":"code","source":"display(df_train.sort_values(by=['target'], ascending=False).head())","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:06:11.297203Z","iopub.execute_input":"2021-06-01T16:06:11.297561Z","iopub.status.idle":"2021-06-01T16:06:11.313756Z","shell.execute_reply.started":"2021-06-01T16:06:11.297531Z","shell.execute_reply":"2021-06-01T16:06:11.312555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View target distribution","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(df_train['target'], ax=ax, color ='green')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:16:36.485127Z","iopub.execute_input":"2021-06-01T16:16:36.485542Z","iopub.status.idle":"2021-06-01T16:16:36.705341Z","shell.execute_reply.started":"2021-06-01T16:16:36.485501Z","shell.execute_reply":"2021-06-01T16:16:36.704389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View std_error distribution","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n#sns.distplot(df_train['standard_error'], ax=ax, color ='green')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:10:25.200306Z","iopub.execute_input":"2021-06-01T16:10:25.200639Z","iopub.status.idle":"2021-06-01T16:10:25.476682Z","shell.execute_reply.started":"2021-06-01T16:10:25.200609Z","shell.execute_reply":"2021-06-01T16:10:25.475592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def msv_1(data, color = 'yellow', edgecolor = 'black', height = 3, width = 15):\n    \n    plt.figure(figsize = (width, height))\n    percentage = (data.isnull().mean()) * 100\n    percentage.sort_values(ascending = False).plot.bar(color = color, edgecolor = edgecolor)\n\n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()\nmsv_1(df_train, color=sns.color_palette('flare',15))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:06:51.828226Z","iopub.execute_input":"2021-06-01T16:06:51.828575Z","iopub.status.idle":"2021-06-01T16:06:52.040955Z","shell.execute_reply.started":"2021-06-01T16:06:51.828544Z","shell.execute_reply":"2021-06-01T16:06:52.040177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Count number of words in excerpts and maximum count","metadata":{}},{"cell_type":"code","source":"count = df_train['excerpt'].str.split().str.len()\nprint(\"Number of words in excerpts:\\n\",count)\nprint(\"Max word count from excerpt: \", max(count))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding two columns to the train dataset: \n* *excerpt_len* \n> Length of the excerpt \n* *excerpt_word_count* \n> Count of number of words in the excerpt","metadata":{}},{"cell_type":"code","source":"df_train['excerpt_len'] = df_train['excerpt'].apply(\n    lambda x : len(x)\n)\ndf_train['excerpt_word_count'] = df_train['excerpt'].apply(\n    lambda x : len(x.split(' '))\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:11:29.779388Z","iopub.execute_input":"2021-06-01T16:11:29.779784Z","iopub.status.idle":"2021-06-01T16:11:29.815368Z","shell.execute_reply.started":"2021-06-01T16:11:29.779755Z","shell.execute_reply":"2021-06-01T16:11:29.814412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig= plt.subplots(1, 1, figsize=(20, 6))\nsns.kdeplot(df_train['excerpt_len'],  color = 'green').set_title('Excerpt Len')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:19:49.785959Z","iopub.execute_input":"2021-06-01T16:19:49.786276Z","iopub.status.idle":"2021-06-01T16:19:49.966386Z","shell.execute_reply.started":"2021-06-01T16:19:49.786248Z","shell.execute_reply":"2021-06-01T16:19:49.96544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.kdeplot(df_train['excerpt_word_count'], ax=ax, color = 'green').set_title('Excerpt Word Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:19:26.431811Z","iopub.execute_input":"2021-06-01T16:19:26.432132Z","iopub.status.idle":"2021-06-01T16:19:26.61332Z","shell.execute_reply.started":"2021-06-01T16:19:26.432103Z","shell.execute_reply":"2021-06-01T16:19:26.612289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generate a word cloud","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=500).generate(\" \".join(df_train['excerpt']))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title('Excerpts',fontsize=40);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"License Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nsns.countplot(y=\"license\",data=df_train,palette=\"crest\",linewidth=3)\nplt.title(\"License Distribution\",font=\"Serif\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:21:39.366262Z","iopub.execute_input":"2021-06-01T16:21:39.366601Z","iopub.status.idle":"2021-06-01T16:21:39.592003Z","shell.execute_reply.started":"2021-06-01T16:21:39.366573Z","shell.execute_reply":"2021-06-01T16:21:39.591054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will find out vocab size count,i.e. total number of words used. We will use Counter class from collections.","metadata":{}},{"cell_type":"code","source":"\nresults = Counter()\ndf_train['excerpt'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find out the longest word and it's length","metadata":{}},{"cell_type":"code","source":"longest = max(str(results.keys()).split(), key=len)\nprint(longest)\nprint(len(longest))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will see the excerpt with minimum target value","metadata":{}},{"cell_type":"code","source":"df_train['target'].min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[df_train['target'] == df_train['target'].min()].excerpt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for word in df_train.loc[df_train['target'] == df_train['target'].min()].excerpt:\n    print(word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will see the excerpt with maximum target value","metadata":{}},{"cell_type":"code","source":"df_train['target'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[df_train['target'] == df_train['target'].max()].excerpt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for word in df_train.loc[df_train['target'] == df_train['target'].max()].excerpt:\n    print(word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Functions to get top unigrams and bigrams","metadata":{}},{"cell_type":"code","source":"def get_top_n_words(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' unigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus) \n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] \n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return freq_sorted[:n]\n\ndef get_top_n_bigram(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' bigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer(ngram_range = (2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return freq_sorted[:n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-preprocessing\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Data Preprocessing </p>","metadata":{}},{"cell_type":"markdown","source":"Data preprocessing is the process of converting raw data into a well-readable format to be used by a machine learning model.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-datacleaning\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Data Cleaning </p>","metadata":{}},{"cell_type":"markdown","source":"Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset\n\nWe will create a 'clean' function which comprises of various cleaning function such as removal of punctuations etc","metadata":{}},{"cell_type":"code","source":"def removeStopwords(text):\n    doc = nlp(text)\n    clean_text = ' '\n    for txt in doc:\n        if (txt.is_stop == False):\n            clean_text = clean_text + \" \" + str(txt)        \n    \n    return clean_text\n\nprint(\"\\033[1mText before removeStopwords function: \\033[0m\" + df_train['excerpt'][1])\nprint(\"\\033[1mText after removeStopwords function: \\033[0m\" + removeStopwords(df_train['excerpt'][1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:07:18.180979Z","iopub.execute_input":"2021-06-09T06:07:18.181311Z","iopub.status.idle":"2021-06-09T06:07:18.247238Z","shell.execute_reply.started":"2021-06-09T06:07:18.181278Z","shell.execute_reply":"2021-06-09T06:07:18.24636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removePunctuations(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\nprint(\"\\033[1mText before removePunctuations function: \\033[0m\" + df_train['excerpt'][1])\nprint(\"\\n\")\nprint(\"\\033[1mText after removePunctuations function: \\033[0m\" + removePunctuations(df_train['excerpt'][1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:07:22.226275Z","iopub.execute_input":"2021-06-09T06:07:22.226673Z","iopub.status.idle":"2021-06-09T06:07:22.238295Z","shell.execute_reply.started":"2021-06-09T06:07:22.226638Z","shell.execute_reply":"2021-06-09T06:07:22.23728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeLinks(text):\n    clean_text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    #https? will match both http and https\n    #A|B, where A and B can be arbitrary REs, creates a regular expression that will match either A or B.\n    #\\S Matches any character which is not a whitespace character.\n    #+ Causes the resulting RE to match 1 or more repetitions of the preceding RE. ab+ will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’.\n    return clean_text\n\ntest_string = \"http://www.youtube.com/ and https://www.youtube.com/ should be removed \"\n(test_string,removeLinks(test_string))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:07:24.731218Z","iopub.execute_input":"2021-06-09T06:07:24.731541Z","iopub.status.idle":"2021-06-09T06:07:24.741358Z","shell.execute_reply.started":"2021-06-09T06:07:24.731509Z","shell.execute_reply":"2021-06-09T06:07:24.740498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeNumbers(text):\n    clean_text = re.sub(r'\\d+', '', text)\n    return clean_text\n\ntest_string = \"Hi 🙈 99 girls are running\"\n(test_string,removeNumbers(test_string))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:07:27.569148Z","iopub.execute_input":"2021-06-09T06:07:27.569478Z","iopub.status.idle":"2021-06-09T06:07:27.576079Z","shell.execute_reply.started":"2021-06-09T06:07:27.569448Z","shell.execute_reply":"2021-06-09T06:07:27.575245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(text):\n    text = text.lower() #Lets make it lowercase\n    text = removeStopwords(text)\n    text = removePunctuations(text)\n    text = removeNumbers(text)\n    text = removeLinks(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:07:29.906589Z","iopub.execute_input":"2021-06-09T06:07:29.906939Z","iopub.status.idle":"2021-06-09T06:07:29.911538Z","shell.execute_reply.started":"2021-06-09T06:07:29.906904Z","shell.execute_reply":"2021-06-09T06:07:29.910424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['excerpt_clean'] = df_train['excerpt'].apply(clean)\ndf_test['excerpt_clean'] = df_test['excerpt'].apply(clean)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:07:33.761653Z","iopub.execute_input":"2021-06-09T06:07:33.762008Z","iopub.status.idle":"2021-06-09T06:08:49.534843Z","shell.execute_reply.started":"2021-06-09T06:07:33.761975Z","shell.execute_reply":"2021-06-09T06:08:49.533998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After cleaning, see size of vocabulary:","metadata":{}},{"cell_type":"code","source":"results = Counter()\ndf_train['excerpt_clean'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.excerpt_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_unigram = get_top_n_words(df_train['excerpt_clean'], 20)\nwords = [i[0] for i in top_unigram]\ncount = [i[1] for i in top_unigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#6baed6'] * 20))\n\np = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Unigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\ncurdoc().theme = 'dark_minimal'\np.xgrid.grid_line_color = None\np.y_range.start = 0\np.title.align = 'center'\np.xaxis.major_label_orientation = \"vertical\"\nshow(p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_bigram = get_top_n_bigram(df_train['excerpt_clean'], 20)\nwords = [i[0] for i in top_bigram]\ncount = [i[1] for i in top_bigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#a1dab4'] * 20))\n\np1 = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Bigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np1.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\n# curdoc().theme = 'dark_minimal'\np1.xgrid.grid_line_color = None\np1.title.align = 'center'\np1.y_range.start = 0\np1.xaxis.major_label_orientation = \"vertical\"\nshow(p1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-stemming\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Stemming </p>","metadata":{}},{"cell_type":"markdown","source":"We will use NLTK for stemming since Spacy doesn't contain any function for stemming as it relies on lemmatization only There are two types of stemmers in NLTK: Porter Stemmer and Snowball stemmers. Snowball stemmer is a slightly improved version of the Porter stemmer and is usually preferred over the latter. So we will use that.\n\nStemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster.","metadata":{}},{"cell_type":"code","source":"stemmer = SnowballStemmer(language='english')\n\ntokens = df_train['excerpt'][1].split()\nclean_text = ' '\n\nfor token in tokens:\n    print(token + ' --> ' + stemmer.stem(token))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stemWord(text):\n    stemmer = SnowballStemmer(language='english')\n    tokens = text.split()\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + stemmer.stem(token)      \n    \n    return clean_text\n\nprint(\"\\033[1mText before stemWord function: \\033[0m\" + df_train['excerpt'][1])\nprint(\"\\033[1mText after stemWord function: \\033[0m\" + stemWord(df_train['excerpt'][1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['excerpt_clean'] = df_train['excerpt_clean'].apply(stemWord)\ndf_test['excerpt_clean'] = df_test['excerpt_clean'].apply(stemWord)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See vocabulary size now","metadata":{}},{"cell_type":"code","source":"results = Counter()\ndf_train['excerpt_clean'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-lemmatization\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Lemmatization </p>","metadata":{}},{"cell_type":"markdown","source":"Though we could not perform stemming with spaCy, we can perform lemmatization using spaCy. This is a time consuming process.\n\nOutput of lemmatization is an actual word in English unlike Stemming. (word.lemma_ will print word's lemma in SPacy)","metadata":{}},{"cell_type":"code","source":"doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n#for token in doc:\n   # print(token.lemma_)\nfor noun in doc.noun_chunks:\n    print(noun.text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for word in doc:\n    print(word.text,  word.lemma_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatizeWord(text):\n    tokens=nlp(text)\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + token.lemma_      \n    \n    return clean_text\n\nprint(\"Text before lemmatizeWord function: \" + df_train['excerpt'][1])\nprint(\"Text after lemmatizeWord function: \" + lemmatizeWord(df_train['excerpt'][1]))\n\ndoc = \"Apple is looking at buying U.K. startup for $1 billion\"\nlemmatizeWord(doc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"Lets define Root Mean Squared Error","metadata":{}},{"cell_type":"code","source":"rmse = lambda y_true, y_pred: np.sqrt(mse(y_true, y_pred))\nrmse_loss = lambda Estimator, X, y: rmse(y, Estimator.predict(X))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into train and test sets\n\n\nx = df_train['excerpt_clean']\ny = df_train['target']\n\nprint(len(x), len(y))\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\nprint(len(x_train), len(y_train))\nprint(len(x_test), len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-pos\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Part-of-Speech Tagging </p>","metadata":{}},{"cell_type":"markdown","source":"> In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.","metadata":{}},{"cell_type":"markdown","source":"[Wiki link](https://en.wikipedia.org/wiki/Part-of-speech_tagging)","metadata":{}},{"cell_type":"code","source":"df_train['pos_tags'] = df_train['excerpt_clean'].str.split().map(pos_tag)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:09:55.093417Z","iopub.execute_input":"2021-06-09T06:09:55.093758Z","iopub.status.idle":"2021-06-09T06:10:08.8987Z","shell.execute_reply.started":"2021-06-09T06:09:55.093723Z","shell.execute_reply":"2021-06-09T06:10:08.897827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['pos_tags']","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:10:19.514351Z","iopub.execute_input":"2021-06-09T06:10:19.514673Z","iopub.status.idle":"2021-06-09T06:10:19.615844Z","shell.execute_reply.started":"2021-06-09T06:10:19.514643Z","shell.execute_reply":"2021-06-09T06:10:19.61487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a function count_tags to count the number of pos_tags and add it as a column to the dataframe.","metadata":{}},{"cell_type":"code","source":"def count_tags(pos_tags):\n    tag_count = {}\n    for word,tag in pos_tags:\n        if tag in tag_count:\n            tag_count[tag] += 1\n        else:\n            tag_count[tag] = 1\n    return tag_count\n\ndf_train['tag_counts'] = df_train['pos_tags'].map(count_tags)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:10:38.362528Z","iopub.execute_input":"2021-06-09T06:10:38.362868Z","iopub.status.idle":"2021-06-09T06:10:38.413468Z","shell.execute_reply.started":"2021-06-09T06:10:38.36283Z","shell.execute_reply":"2021-06-09T06:10:38.412647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['tag_counts']","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:10:46.051364Z","iopub.execute_input":"2021-06-09T06:10:46.051679Z","iopub.status.idle":"2021-06-09T06:10:46.06345Z","shell.execute_reply.started":"2021-06-09T06:10:46.051649Z","shell.execute_reply":"2021-06-09T06:10:46.06258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add columns for differnt tags","metadata":{}},{"cell_type":"markdown","source":"Here are some of the different tags:","metadata":{}},{"cell_type":"markdown","source":"* CC coordinating conjunction\n* CD cardinal digit\n* DT determiner\n* EX existential there (like: “there is” … think of it like “there exists”)\n* FW foreign word\n* IN preposition/subordinating conjunction\n* JJ adjective ‘big’\n* JJR adjective, comparative ‘bigger’\n* JJS adjective, superlative ‘biggest’\n* LS list marker 1)\n* MD modal could, will\n* NN noun, singular ‘desk’\n* NNS noun plural ‘desks’\n* NNP proper noun, singular ‘Harrison’\n* NNPS proper noun, plural ‘Americans’\n* PDT predeterminer ‘all the kids’\n* POS possessive ending parent‘s \n* PRP personal pronoun I, he, she\n* RB adverb very, silently,\n* RBR adverb, comparative better\n* RBS adverb, superlative best\n* RP particle give up\n* TO to go ‘to‘ the store.\n* UH interjection errrrrrrrm\n* VB verb, base form take\n* VBD verb, past tense took\n* VBG verb, gerund/present participle taking\n* VBN verb, past participle taken\n* VBP verb, sing. present, non-3d take\n* VBZ verb, 3rd person sing. present takes\n* WDT wh-determiner which\n* WP wh-pronoun who, what\n* WP$ possessive wh-pronoun whose\n* WRB wh-abverb where, when","metadata":{}},{"cell_type":"code","source":"set_pos = set([tag for tags in df_train['tag_counts'] for tag in tags])\ntag_cols = list(set_pos)\n\nfor tag in tag_cols:\n    df_train[tag] = df_train['tag_counts'].map(lambda x: x.get(tag, 0))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:12:40.322534Z","iopub.execute_input":"2021-06-09T06:12:40.322857Z","iopub.status.idle":"2021-06-09T06:12:40.403297Z","shell.execute_reply.started":"2021-06-09T06:12:40.322827Z","shell.execute_reply":"2021-06-09T06:12:40.402518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View df_train now","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:14:18.238465Z","iopub.execute_input":"2021-06-09T06:14:18.238799Z","iopub.status.idle":"2021-06-09T06:14:18.362055Z","shell.execute_reply.started":"2021-06-09T06:14:18.238753Z","shell.execute_reply":"2021-06-09T06:14:18.361117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Refer https://seaborn.pydata.org/tutorial/color_palettes.html for seaborn palette\n\nPlot POS tag frequency for df_train's tag_cols.\nset_yscale as log so that smaller values also get displayed.","metadata":{}},{"cell_type":"code","source":"pos = df_train[tag_cols].sum().sort_values(ascending = False)\nplt.figure(figsize=(16,10))\nax = sns.barplot(x=pos.index, y=pos.values,palette=\"flare\")\nplt.xticks(rotation = 50)\nax.set_yscale('log')\nplt.title('Part-Of-Speech tags frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:22:16.206875Z","iopub.execute_input":"2021-06-09T06:22:16.207217Z","iopub.status.idle":"2021-06-09T06:22:17.04063Z","shell.execute_reply.started":"2021-06-09T06:22:16.207186Z","shell.execute_reply":"2021-06-09T06:22:17.039698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use displacy to render the excerpt with the largest target value","metadata":{}},{"cell_type":"code","source":"df_train.loc[df_train['target'] == df_train['target'].max()].excerpt.to_string()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:53:14.296743Z","iopub.execute_input":"2021-06-09T06:53:14.297096Z","iopub.status.idle":"2021-06-09T06:53:14.315518Z","shell.execute_reply.started":"2021-06-09T06:53:14.297066Z","shell.execute_reply":"2021-06-09T06:53:14.314744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent = str()\nfor word in df_train.loc[df_train['target'] == df_train['target'].max()].excerpt:\n    sent = sent  + word\nsent","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:54:53.009999Z","iopub.execute_input":"2021-06-09T06:54:53.010317Z","iopub.status.idle":"2021-06-09T06:54:53.020423Z","shell.execute_reply.started":"2021-06-09T06:54:53.010289Z","shell.execute_reply":"2021-06-09T06:54:53.019419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc1 = nlp(sent)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:55:47.790479Z","iopub.execute_input":"2021-06-09T06:55:47.790807Z","iopub.status.idle":"2021-06-09T06:55:47.822496Z","shell.execute_reply.started":"2021-06-09T06:55:47.790759Z","shell.execute_reply":"2021-06-09T06:55:47.821756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displacy.render(doc1, style=\"dep\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:55:50.630142Z","iopub.execute_input":"2021-06-09T06:55:50.630472Z","iopub.status.idle":"2021-06-09T06:55:51.616013Z","shell.execute_reply.started":"2021-06-09T06:55:50.630444Z","shell.execute_reply":"2021-06-09T06:55:51.61516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-ner\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Named Entity Recognition </p>","metadata":{}},{"cell_type":"markdown","source":"spaCy features an extremely fast statistical entity recognition system, that assigns labels to contiguous spans of tokens. The default trained pipelines can indentify a variety of named and numeric entities, including companies, locations, organizations and products. You can add arbitrary classes to the entity recognition system, and update the model with new examples.","metadata":{}},{"cell_type":"markdown","source":"A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\n\nNamed entities are available as the ents property of a Doc","metadata":{}},{"cell_type":"code","source":"doc1 = nlp(df_train['excerpt'][22])","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:44:38.95264Z","iopub.execute_input":"2021-06-09T06:44:38.952973Z","iopub.status.idle":"2021-06-09T06:44:38.987221Z","shell.execute_reply.started":"2021-06-09T06:44:38.952943Z","shell.execute_reply":"2021-06-09T06:44:38.986467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use displacy to render with style ent","metadata":{}},{"cell_type":"code","source":"displacy.render(doc1, style=\"ent\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:44:41.16065Z","iopub.execute_input":"2021-06-09T06:44:41.160993Z","iopub.status.idle":"2021-06-09T06:44:41.166811Z","shell.execute_reply.started":"2021-06-09T06:44:41.160962Z","shell.execute_reply":"2021-06-09T06:44:41.165867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# document level\nents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc1.ents]\nprint(ents)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:44:46.581279Z","iopub.execute_input":"2021-06-09T06:44:46.581595Z","iopub.status.idle":"2021-06-09T06:44:46.586244Z","shell.execute_reply.started":"2021-06-09T06:44:46.581564Z","shell.execute_reply":"2021-06-09T06:44:46.585407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On preprocessing, some NER information is lost.","metadata":{}},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-bow\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Bag of Words (BoW) </p>","metadata":{}},{"cell_type":"markdown","source":"A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n* A vocabulary of known words.\n* A measure of the presence of known words.\nIt is called a “*bag*” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.","metadata":{}},{"cell_type":"markdown","source":"An n-gram is a contiguous sequence of n items from a given sample of text or speech\n\nAn n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\".","metadata":{}},{"cell_type":"markdown","source":"Click [here](https://en.wikipedia.org/wiki/Bag-of-words_model) for more information on Bag-of-Words model","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-bow-lr\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Linear Regression </p>","metadata":{}},{"cell_type":"markdown","source":"# Unigram Only","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for CountVectorizer(1,1): {val_score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"# Bi-grams only","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(2,2)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for CountVectorizer(2,2): {val_score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"# Unigrams + Bi-grams","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,2)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for CountVectorizer(1,2): {val_score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"# Unigrams + Bi-grams + Tri-grams","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,3)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for CountVectorizer(1,3): {val_score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-bow-ridge\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Ridge Regression</p>","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    Ridge(),\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for Ridge Regression: {val_score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-bow-xgb\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Extreme Gradient Boosting</p>","metadata":{}},{"cell_type":"markdown","source":"Change ngram_range and experiment","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    xgb.XGBRegressor() ,\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for Extreme Gradient Boosting: {val_score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-tdidf\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> TD IDF </p>","metadata":{}},{"cell_type":"markdown","source":"TF-IDF (stands for Term-Frequency-Inverse-Document Frequency) weights down the common words occuring in almost all the documents and give more importance to the words that appear in a subset of documents. TF-IDF works by penalising these common words by assigning them lower weights while giving importance to some rare words in a particular document.","metadata":{}},{"cell_type":"markdown","source":"Use sklearn.feature_extraction.text's TfidfVectorizer and make a pipeline comprising TfidfVectorizer and our models","metadata":{}},{"cell_type":"code","source":"def training(model, X_train, y_train, X_test, y_test, model_name, ngram_range):\n    t1 = time.time()\n    \n    model = make_pipeline(\n        TfidfVectorizer(binary=True, ngram_range=ngram_range),\n        model,\n    )\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    MSE = mse(y_test, y_pred)\n    \n    t2 = time.time()\n    training_time = t2-t1 \n    \n    print(\"--- Model:\", model_name,\"---\")\n    print(\"MSE: \",MSE)\n    print(\"Training time:\",training_time)\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will run different models at the same time.\n1. Ridge Regression\n2. Linear Regression\n3. Extreme Gradient Boosting","metadata":{}},{"cell_type":"markdown","source":"**fit_intercept** bool, default=True\n    Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations (i.e. X and y are expected to be centered).\n    \n**normalizebool**, default=False\n    This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use StandardScaler before calling fit on an estimator with normalize=False","metadata":{}},{"cell_type":"code","source":"ridge = Ridge(fit_intercept = True, normalize = False)\nlr = LinearRegression()\nxgbr = xgb.XGBRegressor()\nlasso = Lasso(alpha=0.1)\ntr = TweedieRegressor()\nhr = HuberRegressor(max_iter = 300)\nmodels = [ridge,lr,xgbr,lasso,tr,hr]\n\nmodelnames = [\"Ridge Regression\",\"Linear Regression\",\"Extreme Gradient Boosting\", \"Lasso Regression\",\"Tweedie Regressor\",\"Huber Regressor\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use train_test_split to split data into training and validation","metadata":{}},{"cell_type":"code","source":"X = df_train[\"excerpt_clean\"]\ny = df_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nn_gram_dict = { \"Unigram\" : (1,1), \"Unigrams + Bigrams\": (1,2), \"Bigrams alone\": (2,2), \"Unigrams + Bigrams + Trigrams\": (1,3)}\n\nfor n_gram in n_gram_dict.keys():\n    print(\"\\033[1m \" + n_gram + \" \\n \\033[0m\")\n    for i in range(0,len(models)):\n        training(model=models[i], X_train=X_train, y_train=y_train, X_test=X_test,y_test=y_test, model_name=modelnames[i],ngram_range=n_gram_dict[n_gram])\n    print(\"*\" * 40)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-wordembeddings\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Embeddings </p>","metadata":{}},{"cell_type":"markdown","source":"Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. \n\nAn embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. \n\nA higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.","metadata":{}},{"cell_type":"markdown","source":"Before we start, let us define some callbacks, variables and other helper functions","metadata":{}},{"cell_type":"markdown","source":"# Callbacks","metadata":{}},{"cell_type":"markdown","source":"We will define some callbacks to be used with the model's fit function:-\n* **Learning rate reduction** - \n\n        tf.keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_loss\",\n        factor=0.1,\n        patience=10,\n        verbose=0,\n        mode=\"auto\",\n        min_delta=0.0001,\n        cooldown=0,\n        min_lr=0,\n        **kwargs\n        )\nReduce learning rate when a metric has stopped improving.\n\nModels often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.\n\nWe will use that to prevent/reduce overfitting.\n\n* **Early Stopping**\n\n       tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        min_delta=0,\n        patience=0,\n        verbose=0,\n        mode=\"auto\",\n        baseline=None,\n        restore_best_weights=False,\n        )\nStop training when a monitored metric has stopped improving.\n\nAssuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates.\n\nThe quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.compile().\n\nWe will use that to prevent/reduce overfitting.","metadata":{}},{"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_root_mean_squared_error', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\n\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=5, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting and Predicting Helper Functions","metadata":{}},{"cell_type":"code","source":"\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_complexity(model, excerpt):\n  # Create the sequences\n  padding_type='post'\n  sample_sequences = tokenizer.texts_to_sequences(excerpt)\n  excerpt_padded = pad_sequences(sample_sequences, padding=padding_type, \n                                 maxlen=max_length) \n  classes = model.predict(excerpt_padded)\n  for x in range(len(excerpt_padded)):\n    print(excerpt[x])\n    print(classes[x])\n    print('\\n')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#text = df_train.excerpt\ntext = df_train.excerpt_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A question we should ask ourselves is what values to give for vocab_size, max_length, embedding dimension etc.\nVocab_size I have given as 51308 here which is the total number of words (We found this in [Exploratory Data Analysis](#section-EDA))","metadata":{}},{"cell_type":"code","source":"vocab_size = 51038\nembedding_dim = 64\nmax_length = 50\ntrunc_type='post'\npad_type='post'\noov_tok = \"<OOV>\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The preprocessing library in TensorFlow Keras provides a number of extremely useful tools to prepare data for machine learning. One of these is a\nTokenizer that will allow you to take words and turn them into tokens.\n","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(text)\nword_index = tokenizer.word_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"texts_to_sequences will convert our excerpts to corresponding sequences. We use padding so that each sequence is same (max_length). Commented out validtion part because I am using validation_split in model's fit function instead.","metadata":{}},{"cell_type":"code","source":"training_sequences = tokenizer.texts_to_sequences(text)\ntraining_padded = pad_sequences(training_sequences,maxlen=max_length, \n                                truncating=trunc_type, padding=pad_type)\n\n#validation_sequences = tokenizer.texts_to_sequences(text[800:])\n#validation_padded = pad_sequences(validation_sequences,maxlen=max_length)\n\ntraining_labels_final = np.array(df_train.target)\n#validation_labels_final = np.array(df_train[800:].target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_padded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(training_padded.shape)\nprint(training_labels_final.shape)\n#print(validation_labels_final.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),  \n    tf.keras.layers.Dense(1)\n])\n#model.compile(loss='binary_crossentropy',optimizer='adam',)#metrics=[rmse])\nmodel.compile(loss='mean_squared_error', metrics=[RootMeanSquaredError()])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 35\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our model is over-fitting now.","metadata":{}},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"# Optimizing the Model","metadata":{}},{"cell_type":"markdown","source":"Let's optimize this model and then run in next section.","metadata":{}},{"cell_type":"markdown","source":"*Earlier values:*\nvocab_size = 51038\nembedding_dim = 64\nmax_length = 50\ntrunc_type='post'\npad_type='post'\noov_tok = \"<OOV>\"","metadata":{}},{"cell_type":"markdown","source":"**Exploring embedding dimensions**","metadata":{}},{"cell_type":"markdown","source":"Best practice for embedding size is to have it be the fourth root of the vocab size. \n\n","metadata":{}},{"cell_type":"code","source":"\ndef f(num):\n    return math.sqrt(math.sqrt(num))\n\nf(51308)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f(16662)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 12","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using regularization**\nRegularization is a technique that helps prevent overfitting by reducing the polariza‐\ntion of weights. If the weights on some of the neurons are too heavy, regularization\neffectively punishes them. Broadly speaking, there are two types of regularization: L1\nand L2.\n* L1 regularization is often called lasso (least absolute shrinkage and selection operator)\n    regularization. It effectively helps us ignore the zero or close-to-zero weights when\n    calculating a result in a layer.\n* L2 regularization is often called ridge regression because it pushes values apart by taking their squares. This tends to amplify the differences between nonzero  values and zero or close-to-zero ones, creating a ridge effect.\n\nFor NLP problems like the one we’re considering, L2 is most commonly used. It can be added as an attribute to the Dense layer using the kernel_regularizers property,\nand takes a floating-point value as the regularization factor. This is another hyperparameter that you can experiment with to improve your model.","metadata":{}},{"cell_type":"markdown","source":"**Max length Optimization** We have arbitarly set max_length as 50 earlier. ","metadata":{}},{"cell_type":"code","source":"xs=[]\nys=[]\ncurrent_item=1\nfor item in text:\n xs.append(current_item)\n current_item=current_item+1\n ys.append(len(item))\nnewys = sorted(ys)\nplt.xlabel('Excerpt')\nplt.ylabel('Word Length')\nplt.title('Length of Words in Excerpt')\nplt.plot(xs,newys)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most excerpts have 800 words or less, so we use that value instead.","metadata":{}},{"cell_type":"code","source":"max_length = 800","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create a simple model using an embedding after applying all the optimizations we learned.","metadata":{}},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-embedding\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Create the model using an Embedding </p>","metadata":{}},{"cell_type":"markdown","source":"Let's define a simple model with embedding layer as the first layer\nFor regression to arbitary values problem, dont give last layer activations","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu', kernel_regularizer = tf.keras.regularizers.l2(0.01)),\n    tf.keras.layers.Dense(1)\n])\nmodel.compile(loss='mean_squared_error',optimizer='adam', metrics=[RootMeanSquaredError()])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The num of params of the embedding layer will be (vocab_size) * (embedding_dim).\nThe average pooling layer has 0 trainable parameters, as it’s just averaging the parameters in the embedding layer before it.","metadata":{}},{"cell_type":"markdown","source":"Using validation split as 0.1 so that 10% of training data is used for validation purpose.","metadata":{}},{"cell_type":"code","source":"num_epochs = 100\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us see the predictions for df_test","metadata":{}},{"cell_type":"code","source":"\npredict_complexity(model, df_test['excerpt'])\nplot_graphs(history, \"root_mean_squared_error\")\nplot_graphs(history, \"loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-CNN\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Convolutional Neural Network (CNN/ConvNet) </p>","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(embedding_dim, 5, activation='relu'),\n    tf.keras.layers.GlobalMaxPooling1D(), \n    tf.keras.layers.Dense(24, activation='relu', kernel_regularizer = tf.keras.regularizers.l2(0.01)),\n    tf.keras.layers.Dense(1)\n])\n\n# Default learning rate for the Adam optimizer is 0.001\n# Let's slow down the learning rate by 10.\nlearning_rate = 0.0001\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"commonlitmodel.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-GRU\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Gated Recurrent Units  RNN(GRU) </p>","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n    tf.keras.layers.Dense(1)\n])\n\n# Default learning rate for the Adam optimizer is 0.001\n# Let's slow down the learning rate by 10.\nlearning_rate = 0.00003\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 35\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-LSTM\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Bidirectional Long Short Term Memory (LSTM) </p>","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)), \n    tf.keras.layers.Dense(1)\n])\n\n# Default learning rate for the Adam optimizer is 0.001\n# Let's slow down the learning rate by 10.\nlearning_rate = 0.00003\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 35\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-multiple-LSTM\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Multiple Bidirectional Long Short Term Memory (LSTM) </p>","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, \n                                                       return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n    tf.keras.layers.Dense(1)\n])\n\n# Default learning rate for the Adam optimizer is 0.001\n# Let's slow down the learning rate by 10.\nlearning_rate = 0.00003\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 35\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-hyperparametertuning\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Exploring Hyper Parameter Tuning with Keras </p>","metadata":{}},{"cell_type":"markdown","source":"Let's define a function to build our model. Try optimizing units and learning rate.","metadata":{}},{"cell_type":"code","source":"def model_builder(hp):\n  model = keras.Sequential()\n  model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n\n  # Tune the number of units in the first Dense layer\n  # Choose an optimal value between 32-512\n  hp_units = hp.Int('units', min_value=16, max_value=256, step=8)\n  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n  model.add(keras.layers.Dense(1))\n\n  # Tune the learning rate for the optimizer\n  # Choose an optimal value from 0.01, 0.001, or 0.0001\n  hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n\n  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.MeanSquaredError(),\n                metrics=[RootMeanSquaredError()])\n\n  return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Search","metadata":{}},{"cell_type":"markdown","source":"kerastuner.tuners.randomsearch.RandomSearch(hypermodel, objective, max_trials, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs)","metadata":{}},{"cell_type":"code","source":"tuner_search=kt.RandomSearch(model_builder,\n                       objective = kt.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                       max_trials=5,directory='output',project_name=\"nlp\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uncomment this to run","metadata":{}},{"cell_type":"code","source":"#tuner_search.search(training_padded,training_labels_final,epochs=10,validation_split=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperband","metadata":{}},{"cell_type":"markdown","source":"kerastuner.tuners.hyperband.Hyperband(hypermodel, objective, max_epochs, factor=3, hyperband_iterations=1, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs)","metadata":{}},{"cell_type":"code","source":"tuner = kt.Hyperband(model_builder,\n                     max_epochs=10,\n                     objective = kt.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                     factor=3,\n                     directory='my_dir',\n                     project_name='intro_to_kt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uncomment this to run. ","metadata":{}},{"cell_type":"code","source":"\n#tuner.search(training_padded, training_labels_final, epochs=5, validation_split=0.1, callbacks=[stop_early])\n\n# Get the optimal hyperparameters\n#best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\n#print(f\"\"\"The hyperparameter search is complete. The optimal number of units in the first densely-connected layer is {best_hps.get('units')} and the optimal learning rate for the optimizeris {best_hps.get('learning_rate')}.\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-gloveembeddings\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Glove Embeddings </p>","metadata":{}},{"cell_type":"markdown","source":"What if, instead of learning the embeddings for yourself, you could instead use prelearned embeddings, where researchers have already done the hard work of turning words into vectors and those vectors are proven? One example of this is the GloVe (Global Vectors for WordRepresentation) model developed by Jeffrey Pennington, Richard Socher, and Christopher Manning at Stanford","metadata":{}},{"cell_type":"markdown","source":"Pretrained Word Embeddings are the embeddings learned in one task that are used for solving another similar task. These embeddings are trained on large datasets, saved, and then used for solving other tasks. That’s why pretrained word embeddings are a form of Transfer Learning.","metadata":{}},{"cell_type":"code","source":"glove_embeddings = dict()\nf = open('/kaggle/input/glove6b/glove.6B.50d.txt')\nfor line in f:\n values = line.split()\n word = values[0]\n coefs = np.asarray(values[1:], dtype='float32')\n glove_embeddings[word] = coefs\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove_embeddings['frog']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(glove_embeddings[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(50)\n    return v / np.sqrt((v ** 2).sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-glovexgb\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Extreme Gradient Boosting Regressor </p>","metadata":{}},{"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(df_train.excerpt_clean, df_train.target, \n \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create sentence vectors using the above function for training and validation set\nxtrain_glove = [sent2vec(x) for x in xtrain]\nxvalid_glove = [sent2vec(x) for x in xvalid]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBRegressor(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict(xvalid_glove)\n\nprint (\"MSE: %f \" % mse(yvalid, predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-gloveLSTM\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> GloVe  Stacked LSTM </p>","metadata":{}},{"cell_type":"code","source":"embedding_dim = 50\nvocab_size = 51308","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n if index > vocab_size - 1:\n     break\n else:\n     embedding_vector = glove_embeddings.get(word)\n if embedding_vector is not None:\n     embedding_matrix[index] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False),\n tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,\n return_sequences=True)),\n tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n tf.keras.layers.Dense(24, activation='relu'),\n tf.keras.layers.Dense(1)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.00003\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.3,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs=[]\nys=[]\ncumulative_x=[]\ncumulative_y=[]\ntotal_y=0\nfor word, index in tokenizer.word_index.items():\n xs.append(index)\n cumulative_x.append(index)\n if glove_embeddings.get(word) is not None:\n     total_y = total_y + 1\n     ys.append(1)\n else:\n     ys.append(0)\n cumulative_y.append(total_y / index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,2))\nax.spines['top'].set_visible(False)\nplt.margins(x=0, y=None, tight=True)\n#plt.axis([13000, 14000, 0, 1])\nplt.fill(ys)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(cumulative_x, cumulative_y)\nplt.axis([0, 25000, .915, .985])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-bert\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> HuggingFace TFBertModel </p>","metadata":{}},{"cell_type":"markdown","source":"The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It’s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.","metadata":{}},{"cell_type":"markdown","source":"The abstract from the paper is the following:\n\n> We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n> \n> BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n> \nTips:\n\n* BERT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n\n* BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation.","metadata":{}},{"cell_type":"code","source":"\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T12:59:58.948016Z","iopub.execute_input":"2021-06-01T12:59:58.948392Z","iopub.status.idle":"2021-06-01T13:00:03.088216Z","shell.execute_reply.started":"2021-06-01T12:59:58.948362Z","shell.execute_reply":"2021-06-01T13:00:03.087209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BERT Encoding**\n\n\nData is encoded according to BERT requirement.There is a very helpful function called encode_plus provided in the Tokenizer class. It can seamlessly perform the following operations:\n\n* Tokenize the text\n\n* Add special tokens - [CLS] and [SEP]\n\n* Create token IDs\n\n* Pad the sentences to a common length\n\n* Create attention masks for the above PAD tokens","metadata":{}},{"cell_type":"markdown","source":"Use on data excerpt or excerpt_clean?","metadata":{}},{"cell_type":"code","source":"def bert_encode(data,maximum_length) :\n  input_ids = []\n  attention_masks = []\n  \n\n  for i in range(len(data.excerpt)):\n      encoded = tokenizer.encode_plus(\n        \n        data.excerpt[i],\n        add_special_tokens=True,\n        max_length=maximum_length,\n        pad_to_max_length=True,\n        \n        return_attention_mask=True,\n        \n      )\n      \n      input_ids.append(encoded['input_ids'])\n      attention_masks.append(encoded['attention_mask'])\n  return np.array(input_ids),np.array(attention_masks)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T15:54:59.355622Z","iopub.execute_input":"2021-06-01T15:54:59.355947Z","iopub.status.idle":"2021-06-01T15:54:59.363201Z","shell.execute_reply.started":"2021-06-01T15:54:59.355913Z","shell.execute_reply":"2021-06-01T15:54:59.362243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encode both train and test dataset","metadata":{}},{"cell_type":"code","source":"train_input_ids,train_attention_masks = bert_encode(df_train,60)\ntest_input_ids,test_attention_masks = bert_encode(df_test,60)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T15:55:02.310102Z","iopub.execute_input":"2021-06-01T15:55:02.310438Z","iopub.status.idle":"2021-06-01T15:55:07.683912Z","shell.execute_reply.started":"2021-06-01T15:55:02.31038Z","shell.execute_reply":"2021-06-01T15:55:07.683049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a function to create our model. We will add Dense layer(s) as output layer.\n\nAdd DropOut if overfitting","metadata":{}},{"cell_type":"code","source":"def create_model(bert_model):\n  input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n  attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n  \n  output = bert_model([input_ids,attention_masks])\n  output = output[1]\n  #output = tf.keras.layers.Dense(32,activation='relu')(output)\n  #output = tf.keras.layers.Dropout(0.2)(output)\n\n  output = tf.keras.layers.Dense(1)(output)\n  model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n  model.compile(tf.keras.optimizers.Adam(lr=6e-6), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n  return model","metadata":{"execution":{"iopub.status.busy":"2021-06-01T13:00:27.031613Z","iopub.execute_input":"2021-06-01T13:00:27.032034Z","iopub.status.idle":"2021-06-01T13:00:27.040312Z","shell.execute_reply.started":"2021-06-01T13:00:27.031993Z","shell.execute_reply":"2021-06-01T13:00:27.038978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Click [TFBertModel](https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) for more information","metadata":{}},{"cell_type":"code","source":"\nbert_model = TFBertModel.from_pretrained('bert-large-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T13:00:27.042502Z","iopub.execute_input":"2021-06-01T13:00:27.042968Z","iopub.status.idle":"2021-06-01T13:01:30.374685Z","shell.execute_reply.started":"2021-06-01T13:00:27.042929Z","shell.execute_reply":"2021-06-01T13:01:30.373651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model(bert_model)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T13:01:30.377975Z","iopub.execute_input":"2021-06-01T13:01:30.378278Z","iopub.status.idle":"2021-06-01T13:01:39.819363Z","shell.execute_reply.started":"2021-06-01T13:01:30.378242Z","shell.execute_reply":"2021-06-01T13:01:39.818367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change epochs number","metadata":{}},{"cell_type":"code","source":"history = model.fit([train_input_ids,train_attention_masks],df_train.target,validation_split=0.3, epochs=2,batch_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T13:01:39.821872Z","iopub.execute_input":"2021-06-01T13:01:39.82233Z","iopub.status.idle":"2021-06-01T13:05:50.80063Z","shell.execute_reply.started":"2021-06-01T13:01:39.822272Z","shell.execute_reply":"2021-06-01T13:05:50.79959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T13:06:39.686895Z","iopub.execute_input":"2021-06-01T13:06:39.687275Z","iopub.status.idle":"2021-06-01T13:06:39.90693Z","shell.execute_reply.started":"2021-06-01T13:06:39.687244Z","shell.execute_reply":"2021-06-01T13:06:39.905541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-robertabase\"></a>\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> roberta-base Hugging Face Transformer </p>","metadata":{}},{"cell_type":"markdown","source":"Follow the same steps as the above section","metadata":{}},{"cell_type":"markdown","source":"https://huggingface.co/roberta-base","metadata":{}},{"cell_type":"code","source":"\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nBASE_MODEL = TFRobertaModel.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T15:53:47.972276Z","iopub.execute_input":"2021-06-01T15:53:47.972607Z","iopub.status.idle":"2021-06-01T15:54:31.695523Z","shell.execute_reply.started":"2021-06-01T15:53:47.972577Z","shell.execute_reply":"2021-06-01T15:54:31.694738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef create_model(bert_model):\n  input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n  attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n  \n  output = bert_model([input_ids,attention_masks])\n  output = output[1]\n  output = tf.keras.layers.Dense(32,activation='relu')(output)\n  output = tf.keras.layers.Dropout(0.2)(output)\n\n  output = tf.keras.layers.Dense(1)(output)\n  model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n  \n  return model\n\n\n\nmodel = create_model(BASE_MODEL)\nmodel.compile(tf.keras.optimizers.Adam(lr=6e-6), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:01:47.765936Z","iopub.execute_input":"2021-06-01T16:01:47.766278Z","iopub.status.idle":"2021-06-01T16:01:49.197073Z","shell.execute_reply.started":"2021-06-01T16:01:47.766249Z","shell.execute_reply":"2021-06-01T16:01:49.196303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit([train_input_ids,train_attention_masks],df_train.target,validation_split=0.3, epochs=2,batch_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:01:53.013777Z","iopub.execute_input":"2021-06-01T16:01:53.014121Z","iopub.status.idle":"2021-06-01T16:03:01.872899Z","shell.execute_reply.started":"2021-06-01T16:01:53.014092Z","shell.execute_reply":"2021-06-01T16:03:01.872112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-submission\"></a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Making the Submission </p>","metadata":{}},{"cell_type":"code","source":"def submission(submission_file_path,model,excerpt):\n    padding_type='post'\n    sample_sequences = tokenizer.texts_to_sequences(excerpt)\n    excerpt_padded = pad_sequences(sample_sequences, padding=padding_type, \n                                 maxlen=max_length) \n    classes = model.predict(excerpt_padded)\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = classes\n    sample_submission.to_csv(\"submission.csv\", index=False)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-01T13:05:50.83541Z","iopub.status.idle":"2021-06-01T13:05:50.835931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission_file_path = \"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\"\n\n#submission(submission_file_path,model,df_test['excerpt'])","metadata":{"execution":{"iopub.status.busy":"2021-06-01T13:05:50.837912Z","iopub.status.idle":"2021-06-01T13:05:50.838686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Back to Top](#section-zero)\n","metadata":{}}]}