{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install ../input/tartardown/gdown-4.2.0.tar","metadata":{"execution":{"iopub.status.busy":"2021-12-07T19:04:26.298982Z","iopub.execute_input":"2021-12-07T19:04:26.299648Z","iopub.status.idle":"2021-12-07T19:04:44.950933Z","shell.execute_reply.started":"2021-12-07T19:04:26.299612Z","shell.execute_reply":"2021-12-07T19:04:44.950064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !gdown https://drive.google.com/uc?id=1-FppEnP33A1NWVdQegwYD4J3QAxtHurc","metadata":{"execution":{"iopub.status.busy":"2021-12-07T17:47:15.493465Z","iopub.execute_input":"2021-12-07T17:47:15.494192Z","iopub.status.idle":"2021-12-07T17:48:50.895504Z","shell.execute_reply.started":"2021-12-07T17:47:15.494151Z","shell.execute_reply":"2021-12-07T17:48:50.894662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !tar -xvf model.tar.gz","metadata":{"execution":{"iopub.status.busy":"2021-12-07T17:49:21.894156Z","iopub.execute_input":"2021-12-07T17:49:21.894955Z","iopub.status.idle":"2021-12-07T17:51:49.279967Z","shell.execute_reply.started":"2021-12-07T17:49:21.894903Z","shell.execute_reply":"2021-12-07T17:51:49.279113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom os import path\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nfrom sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import DataLoader, Dataset\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:45:02.27361Z","iopub.execute_input":"2021-12-23T01:45:02.273858Z","iopub.status.idle":"2021-12-23T01:45:05.90381Z","shell.execute_reply.started":"2021-12-23T01:45:02.27383Z","shell.execute_reply":"2021-12-23T01:45:05.902869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install textstat","metadata":{"execution":{"iopub.status.busy":"2021-12-07T17:55:29.12689Z","iopub.execute_input":"2021-12-07T17:55:29.127154Z","iopub.status.idle":"2021-12-07T17:55:37.096685Z","shell.execute_reply.started":"2021-12-07T17:55:29.127122Z","shell.execute_reply":"2021-12-07T17:55:37.095781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import subprocess\n\nwhls = [\n    \"../input/textstat/textstat-0.7.2-py3-none-any.whl\",\n    \"../input/library/Pyphen-0.10.0-py3-none-any.whl\",\n    \"../input/library/huggingface_hub-0.2.1-py3-none-any.whl\",\n    \"../input/library/transformers-4.12.5-py3-none-any.whl\"\n]\n\nfor w in whls:\n    print(\"Installing\", w)\n    subprocess.call([\"pip\", \"install\", w, \"--no-deps\", \"--upgrade\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:45:05.905927Z","iopub.execute_input":"2021-12-23T01:45:05.906186Z","iopub.status.idle":"2021-12-23T01:46:30.779618Z","shell.execute_reply.started":"2021-12-23T01:45:05.906151Z","shell.execute_reply":"2021-12-23T01:46:30.778718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import textstat\nimport itertools\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AdamW,\n)\nfrom transformers.models.auto.tokenization_auto import AutoTokenizer\n\nimport transformers\n\nprint(\"Transformers version:\", transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:46:30.781381Z","iopub.execute_input":"2021-12-23T01:46:30.781653Z","iopub.status.idle":"2021-12-23T01:46:35.549705Z","shell.execute_reply.started":"2021-12-23T01:46:30.781615Z","shell.execute_reply":"2021-12-23T01:46:35.548939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionBlock(nn.Module):\n    def __init__(self, in_features, middle_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = middle_features\n        self.out_features = out_features\n        self.W = nn.Linear(in_features, middle_features)\n        self.V = nn.Linear(middle_features, out_features)\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n        return context_vector\n\n\nclass CommonLitModel(pl.LightningModule):\n    def __init__(\n        self,\n        model_name: str = \"roberta-base\",\n        lr: float = 0.001,\n        weight_decay: float = 0,\n        pretrained: bool = False,\n        betas: tuple = (0.9, 0.999),\n        eps: float = 1e-6,\n        kl_loss: bool = False,\n        warmup: int = 100,\n        hf_config=None,\n        pooled: bool = False,\n        use_hidden: bool = False,\n        **kwargs,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        if hf_config is None:\n            if pretrained:\n                model_path = OUTPUT_PATH / \"pretraining\" / model_name\n                print(\"Using pretrained from\", model_path)\n                self.config = AutoConfig.from_pretrained(model_name)\n                self.transformer = AutoModel.from_pretrained(\n                    model_path, output_hidden_states=True\n                )\n            else:\n                self.config = AutoConfig.from_pretrained(\n                    model_name,\n                    cache_dir=MODEL_CACHE / model_name,\n                )\n                self.transformer = AutoModel.from_pretrained(\n                    model_name,\n                    cache_dir=MODEL_CACHE / model_name,\n                    output_hidden_states=True,\n                )\n        else:\n            self.config = hf_config\n            self.config.output_hidden_states = True\n            self.transformer = AutoModel.from_config(hf_config)\n\n        # self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n        # Multi sample Dropout\n        # self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n        # self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        # self.regressor = nn.Linear(self.config.hidden_size, 2)\n        # self._init_weights(self.layer_norm)\n        # self._init_weights(self.regressor)\n\n        if use_hidden:\n            n_hidden = self.config.hidden_size * 2\n        else:\n            n_hidden = self.config.hidden_size\n\n        self.seq_attn_head = nn.Sequential(\n            nn.LayerNorm(n_hidden),\n            # nn.Dropout(0.1),\n            AttentionBlock(n_hidden, n_hidden, 1),\n            # nn.Dropout(0.1),\n            # nn.Linear(self.config.hidden_size, 2 if kl_loss else 1),\n        )\n\n        self.regressor = nn.Linear(n_hidden + 2, 2 if kl_loss else 1)\n\n        self.loss_fn = nn.MSELoss()\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, features, **kwargs):\n        # out = self.transformer(**kwargs)[\"logits\"]\n\n        model_out = self.transformer(**kwargs)  # 0=seq_output, 1=pooler_output\n        # x = self.layer_norm(x)\n        # for i, dropout in enumerate(self.dropouts):\n        #     if i == 0:\n        #         out = self.regressor(dropout(x))\n        #     else:\n        #         out += self.regressor(dropout(x))\n        # out /= len(self.dropouts)\n\n        if self.hparams.use_hidden:\n            states = model_out[2]\n            out = torch.stack(\n                tuple(states[-i - 1] for i in range(self.config.num_hidden_layers)),\n                dim=0,\n            )\n            out_mean = torch.mean(out, dim=0)\n            out_max, _ = torch.max(out, dim=0)\n            out = torch.cat((out_mean, out_max), dim=-1)\n        else:\n            out = model_out[0]\n\n        out = self.seq_attn_head(out)\n        out = torch.cat([out, features], -1)\n        out = self.regressor(out)\n\n        if out.shape[1] == 1:\n            return out, None\n        else:\n            mean = out[:, 0].view(-1, 1)\n            log_var = out[:, 1].view(-1, 1)\n            return mean, log_var\n\n    def training_step(self, batch, batch_idx):\n        inputs, labels, features = batch\n        mean, log_var = self.forward(features, **inputs)\n        if self.hparams.kl_loss:\n            p = torch.distributions.Normal(mean, torch.exp(log_var))\n            q = torch.distributions.Normal(labels[\"target\"], labels[\"error\"])\n            loss = torch.distributions.kl_divergence(p, q).mean()\n        else:\n            loss = self.loss_fn(mean, labels[\"target\"])\n        self.log_dict({\"loss/train_step\": loss})\n        return {\"loss\": loss}\n\n    def training_epoch_end(self, training_step_outputs):\n        avg_loss = torch.stack([x[\"loss\"] for x in training_step_outputs]).mean()\n        self.log(\"loss/train\", avg_loss, sync_dist=True)\n\n    def validation_step(self, batch, batch_idx):\n        inputs, labels, features = batch\n        mean, log_var = self.forward(features, **inputs)\n        if self.hparams.kl_loss:\n            p = torch.distributions.Normal(mean, torch.exp(log_var))\n            q = torch.distributions.Normal(labels[\"target\"], labels[\"error\"])\n            loss = torch.distributions.kl_divergence(p, q).mean()\n        else:\n            loss = self.loss_fn(mean, labels[\"target\"])\n\n        return {\n            \"val_loss\": loss,\n            \"y_pred\": mean,\n            \"y_true\": labels[\"target\"],\n        }\n\n    def validation_epoch_end(self, outputs):\n        loss_val = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        y_pred = torch.cat([x[\"y_pred\"] for x in outputs])\n        y_true = torch.cat([x[\"y_true\"] for x in outputs])\n\n        rmse = torch.sqrt(self.loss_fn(y_pred, y_true))\n\n        self.log_dict(\n            {\n                \"loss/valid\": loss_val,\n                \"rmse\": rmse,\n            },\n            prog_bar=True,\n            sync_dist=True,\n        )\n\n    # learning rate warm-up\n    def optimizer_step(\n        self,\n        epoch,\n        batch_idx,\n        optimizer,\n        optimizer_idx,\n        optimizer_closure,\n        on_tpu=False,\n        using_native_amp=False,\n        using_lbfgs=False,\n    ):\n        # Warm-up the first 100 steps\n        if self.trainer.global_step < self.hparams.warmup:\n            lr_scale = min(\n                1.0, float(self.trainer.global_step + 1) / self.hparams.warmup\n            )\n            for pg in optimizer.param_groups:\n                pg[\"lr\"] = lr_scale * self.hparams.lr\n\n        # update params\n        optimizer.step(closure=optimizer_closure)\n\n    def configure_optimizers(self):\n        parameters = add_weight_decay(\n            self,\n            self.hparams.weight_decay,\n            skip_list=[\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"],\n        )\n\n        opt = AdamW(\n            parameters,\n            lr=self.hparams.lr,\n            betas=self.hparams.betas,\n            eps=self.hparams.eps,\n        )\n\n        sch = torch.optim.lr_scheduler.CosineAnnealingLR(\n            opt, T_max=1000, eta_min=self.hparams.lr / 10\n        )\n\n        return {\n            \"optimizer\": opt,\n            \"lr_scheduler\": {\"scheduler\": sch, \"interval\": \"step\"},\n        }\n\n\n# utils.py\ndef add_weight_decay(model, weight_decay=1e-5, skip_list=()):\n    decay = []\n    no_decay = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if len(param.shape) == 1 or any(s in name for s in skip_list):\n            no_decay.append(param)\n        else:\n            decay.append(param)\n    return [\n        {\"params\": no_decay, \"weight_decay\": 0.0},\n        {\"params\": decay, \"weight_decay\": weight_decay},\n    ]\n\n\n# datasets.py\nclass CommonLitDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=256):\n        self.df = df.reset_index(drop=True)\n        self.excerpt = self.df[\"excerpt\"]\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        inputs = self.tokenizer(\n            str(row[\"excerpt\"]),\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            # add_special_tokens=True  # not sure what this does\n        )\n\n        input_dict = {\n            \"input_ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n            \"attention_mask\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n        }\n\n        if \"target\" in self.df.columns:\n            labels = {\n                \"target\": torch.tensor([row[\"target\"]], dtype=torch.float32),\n                \"error\": torch.tensor([row[\"standard_error\"]], dtype=torch.float32),\n            }\n\n            # For id 436ce79fe\n            if labels[\"error\"] <= 0:\n                labels[\"error\"] += 0.5\n\n            labels[\"target_stoch\"] = torch.normal(\n                mean=labels[\"target\"], std=labels[\"error\"]\n            )\n        else:\n            labels = 0\n\n        # Add addtional features\n        features = self.generate_features(str(row[\"excerpt\"]))\n\n        return input_dict, labels, features\n\n    def generate_features(self, text):\n        means = torch.tensor([67.742121, 10.308363])\n        stds = torch.tensor([17.530230, 3.298237])\n        features = torch.tensor(\n            [\n                # textstat.sentence_count(text),\n                # textstat.lexicon_count(text),\n                textstat.flesch_reading_ease(text),\n                textstat.smog_index(text),\n            ]\n        )\n        return (features - means) / stds\n\n\n# infer.py\ndef infer(model, dataset, batch_size=32, device=\"cuda\"):\n    model.to(device)\n    model.eval()\n    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4)\n\n    predictions = []\n    with torch.no_grad():\n        for input_dict, _, features in loader:\n            input_dict = {k: v.to(device) for k, v in input_dict.items()}\n            mean, log_var = model(features.to(device), **input_dict)\n            predictions.append(mean.cpu())\n\n    return torch.cat(predictions, 0)\n\n\n# https://kaggler.readthedocs.io/en/latest/_modules/kaggler/ensemble/linear.html#netflix\ndef netflix(es, ps, e0, l=0.0001):\n    \"\"\"Combine predictions with the optimal weights to minimize RMSE.\n\n    Ref: Töscher, A., Jahrer, M., & Bell, R. M. (2009). The bigchaos solution to the netflix grand prize.\n\n    Args:\n        es (list of float): RMSEs of predictions\n        ps (list of np.array): predictions\n        e0 (float): RMSE of all zero prediction\n        l (float): lambda as in the ridge regression\n\n    Returns:\n        (tuple):\n\n            - (np.array): ensemble predictions\n            - (np.array): weights for input predictions\n    \"\"\"\n    m = len(es)\n    n = len(ps[0])\n\n    X = np.stack(ps).T\n    pTy = 0.5 * (n * e0 ** 2 + (X ** 2).sum(axis=0) - n * np.array(es) ** 2)\n\n    w = np.linalg.pinv(X.T.dot(X) + l * n * np.eye(m)).dot(pTy)\n\n    return X.dot(w), w\n\n\ndef create_folds(data, y, n_splits=5, random_state=None):\n    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    data.loc[:, \"bins\"] = pd.cut(y, bins=num_bins, labels=False)\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    splits = []\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        splits.append((t_, v_))\n    return splits\n\n\ndef add_properties(oofs,pred_cols ):\n    for col in pred_cols:\n        oofs[\"log-\"+col] = np.log(oofs[col].abs())\n        oofs[\"square-\"+col] = oofs[col]**2\n        oofs[\"sroot-\"+col] = np.sqrt(oofs[col].abs())\n        \n#     for col in list(itertools.combinations(pred_cols,2)):\n#         new_col = \"-\".join(col)\n# #         oofs[\"subtract-\" + new_col] = oofs[list(col)].iloc[:,1] - oofs[list(col)].iloc[:,0]\n# #         oofs[\"mul-\"+new_col] = oofs[list(col)].iloc[:,1] * oofs[list(col)].iloc[:,0]\n# #         oofs[\"sum-\" +new_col] = oofs[list(col)].iloc[:,1] + oofs[list(col)].iloc[:,0]\n# #         oofs[\"div-\" +new_col] = oofs[list(col)].iloc[:,1] / oofs[list(col)].iloc[:,0]\n#         oofs[\"mean-\"+new_col] = oofs[list(col)].mean(axis = 1)\n#         oofs[\"std-\"+new_col] = oofs[list(col)].mean(axis = 1)\n#         oofs[\"max-\"+new_col] = oofs[list(col)].max(axis = 1)\n#         oofs[\"min-\"+new_col] = oofs[list(col)].min(axis = 1)\n    \n#     for col in list(itertools.combinations(pred_cols,3)):\n#         new_col = \"-\".join(col)\n#         oofs[\"mean-\"+new_col] = oofs[list(col)].mean(axis = 1)\n#         oofs[\"std-\"+new_col] = oofs[list(col)].std(axis = 1)\n#         oofs[\"median-\"+new_col] = oofs[list(col)].median(axis = 1)\n#         oofs[\"max-\"+new_col] = oofs[list(col)].max(axis = 1)\n#         oofs[\"min-\"+new_col] = oofs[list(col)].min(axis = 1)\n\n    oofs[\"mean\"] = oofs[pred_cols].mean(axis =1)\n    oofs[\"std\"] = oofs[pred_cols].std(axis =1)\n    oofs[\"median\"] = oofs[pred_cols].median(axis =1)\n    oofs[\"max\"] = oofs[pred_cols].max(axis =1)\n    oofs[\"min\"] = oofs[pred_cols].max(axis =1)\n    \n    pred_cols = list(oofs.columns[2:])\n    return oofs, pred_cols","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:53:25.946738Z","iopub.execute_input":"2021-12-23T01:53:25.947Z","iopub.status.idle":"2021-12-23T01:53:26.01497Z","shell.execute_reply.started":"2021-12-23T01:53:25.946971Z","shell.execute_reply":"2021-12-23T01:53:26.014301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = Path(\"../input/commonlitreadabilityprize\")\n# OUTPUT_PATH = Path(f\"output/{COMP_NAME}/\")\n# CONFIG_PATH = Path(f\"{COMP_NAME}/hyperparams.yml\")\nMODEL_CACHE = None","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:46:35.622833Z","iopub.execute_input":"2021-12-23T01:46:35.623268Z","iopub.status.idle":"2021-12-23T01:46:35.642974Z","shell.execute_reply.started":"2021-12-23T01:46:35.623227Z","shell.execute_reply":"2021-12-23T01:46:35.642179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_path = Path(\"../input/oofrmse\")\noof_paths = []\nfor p in results_path.glob(f\"*.csv\"):\n    oof_paths.extend([p])\n\nresults_path = Path(\"../input/oofsv2\")\nfor p in results_path.glob(f\"*.csv\"):\n    oof_paths.extend([p])\n\nprint(f\"{len(oof_paths)} OOFs found\")\n\noofs = pd.read_csv(INPUT_PATH / \"train.csv\", usecols=[\"id\", \"target\"]).sort_values(\n    by=\"id\"\n)\nfor i, p in enumerate(oof_paths):\n    print(p)\n    x = pd.read_csv(p).sort_values(by=\"id\")\n    oofs[f\"model_{i}\"] = x[\"prediction\"].values\n\npred_cols = [f\"model_{i}\" for i in range(len(oof_paths))]\n\n\n# add properties \noofs , pred_cols_added= add_properties(oofs,pred_cols)\nprint(f\"have {len(pred_cols_added)} properties\")\n\nprint(pred_cols)\n\n#RidgeCv\nreg = RidgeCV(alphas=(0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50, 100, 500, 1000), normalize=True)\nreg.fit(oofs[pred_cols_added], oofs[\"target\"])\nprint(f\"Best RMSE: {np.sqrt(-reg.best_score_):0.5f}. Alpha {reg.alpha_}\")  \n\n# LassoCV\n# reg = LassoCV(max_iter=5000, random_state=48, n_jobs=-1, normalize=True)\n# reg.fit(oofs[pred_cols], oofs[\"target\"])\n# print(f\"Weights: {reg.coef_}, bias: {reg.intercept_}\")\n# print(f\"Best RMSE: {np.sqrt(reg.mse_path_)[-1].mean():0.5f}\")  # Lasso\n\n#     # Stack using Netflix method\n# oof_preds = [oofs[c].values for c in pred_cols_added]\n# rmses = [np.sqrt(mean_squared_error(p, oofs[\"target\"])) for p in oof_preds]\n# ensemble, weights = netflix(rmses, oof_preds, 1.4100)\n# score = np.sqrt(mean_squared_error(ensemble, oofs[\"target\"]))\n# print(f\"Best RMSE: {score:0.5f}\")\n\n\n# Netflix with 3-seed CV\n# X, y = oofs[pred_cols_added], oofs[\"target\"]\n# scores = []\n# weights_agg = 0\n# for seed in [48, 42, 3]:\n#     for fold, (trn_idx, val_idx) in enumerate(create_folds(X, y, random_state=seed)):\n#         train_oofs = X.loc[trn_idx]\n#         valid_oofs = X.loc[val_idx]\n#         valid_target = y.loc[val_idx]\n\n#         train_preds = [train_oofs[c].values for c in X.columns]\n#         rmses = [np.sqrt(mean_squared_error(X[c], y)) for c in X.columns]\n#         _, weights = netflix(rmses, train_preds, 1.4100)\n\n#         val_pred = valid_oofs @ weights\n#         score = np.sqrt(mean_squared_error(val_pred, valid_target))\n#         scores.append(score)\n#         weights_agg += weights\n\n# weights_agg /= len(scores)\n# print(f\"CV RMSE: {np.mean(scores):0.5f}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:53:26.849059Z","iopub.execute_input":"2021-12-23T01:53:26.849461Z","iopub.status.idle":"2021-12-23T01:53:27.911007Z","shell.execute_reply.started":"2021-12-23T01:53:26.849424Z","shell.execute_reply":"2021-12-23T01:53:27.910226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for col in list(itertools.combinations(pred_cols,2)):\n#     new_col = \"-\".join(col)\n#     oofs[new_col] = oofs[list(col)].iloc[:,1] * oofs[list(col)].iloc[:,0]\n#     oofs[\"mean-\"+new_col] = oofs[list(col)].mean(axis = 1)\n#     oofs[\"std-\"+new_col] = oofs[list(col)].mean(axis = 1)\n#     oofs[\"max-\"+new_col] = oofs[list(col)].max(axis = 1)\n#     oofs[\"min-\"+new_col] = oofs[list(col)].min(axis = 1)\n    \n# for col in list(itertools.combinations(pred_cols,3)):\n#     new_col = \"-\".join(col)\n#     oofs[\"mean-\"+new_col] = oofs[list(col)].mean(axis = 1)\n#     oofs[\"std-\"+new_col] = oofs[list(col)].std(axis = 1)\n#     oofs[\"median-\"+new_col] = oofs[list(col)].median(axis = 1)\n#     oofs[\"max-\"+new_col] = oofs[list(col)].max(axis = 1)\n#     oofs[\"min-\"+new_col] = oofs[list(col)].min(axis = 1)\n\n# oofs[\"mean\"] = oofs[pred_cols].mean(axis =1)\n# oofs[\"std\"] = oofs[pred_cols].std(axis =1)\n# oofs[\"median\"] = oofs[pred_cols].median(axis =1)\n# oofs[\"max\"] = oofs[pred_cols].max(axis =1)\n# oofs[\"min\"] = oofs[pred_cols].max(axis =1)\n# print(oofs.columns)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T09:43:10.497851Z","iopub.execute_input":"2021-12-22T09:43:10.498162Z","iopub.status.idle":"2021-12-22T09:43:10.577487Z","shell.execute_reply.started":"2021-12-22T09:43:10.498127Z","shell.execute_reply":"2021-12-22T09:43:10.576637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nmodel_name = [\"rbtabasefold\",\"debertafold\",\"distilrbtafold\",\"rbsquad2fold\", \"debertav2fold\", \"robertabasev2fold\", \"distilv2fold\"]\npath_model  = Path(\"../input/\")\ngroup_model = [] \nfor name in model_name:\n    temp = []\n    for i in  path_model.rglob(f\"*.ckpt\"):\n        if(name in str(i)):\n            temp.append(i)\n    group_model.append(temp)    ","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:43:59.663838Z","iopub.execute_input":"2021-12-22T16:43:59.664112Z","iopub.status.idle":"2021-12-22T16:44:00.07951Z","shell.execute_reply.started":"2021-12-22T16:43:59.664083Z","shell.execute_reply":"2021-12-22T16:44:00.078492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\"\ndf = pd.read_csv(INPUT_PATH / \"test.csv\")\noutput = 0\n\nfor i, group in enumerate(group_model):\n    output = 0\n    for j, p in enumerate(group):\n        ckpt_size = p.stat().st_size / 1024 ** 3\n        bs = 32 if ckpt_size > 1.5 else 64\n        print(f\"{i} {p}, Size: {ckpt_size:0.2f}\")\n\n        if j == 0:\n            config = AutoConfig.from_pretrained(str(p.parent))\n            tokenizer = AutoTokenizer.from_pretrained(str(p.parent))\n            dataset = CommonLitDataset(df, tokenizer)\n\n        model = CommonLitModel.load_from_checkpoint(p, hf_config=config)\n        output += infer(model, dataset, batch_size=bs, device=device)\n\n        del model\n\n    del dataset\n    del tokenizer\n    gc.collect()\n\n    df[f\"model_{i}\"] = output.squeeze().numpy() / len(group)\n\n# add properties\ndf , *_ = add_properties(df,pred_cols)\npred_cols =  pred_cols_added\n\ndf[\"target\"] = reg.predict(df[pred_cols])\n# df[\"target\"] = df[pred_cols] @ weights #  netflix\n# df[\"target\"] = df[pred_cols].mean(1) # using mean\n# df[\"target\"] = df[pred_cols] @ weights_agg\ndf[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:09:50.5768Z","iopub.execute_input":"2021-12-22T16:09:50.577353Z","iopub.status.idle":"2021-12-22T16:13:45.765446Z","shell.execute_reply.started":"2021-12-22T16:09:50.577318Z","shell.execute_reply":"2021-12-22T16:13:45.764472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def make_predictions(dataset_paths, device=\"cuda\"):\n#     mpaths, oof_paths = [], []\n#     for p in dataset_paths:\n#         mpaths.append(sorted(list(p.rglob(f\"*.ckpt\"))))\n#         oof_paths.extend(sorted(list(p.glob(f\"*.csv\"))))\n\n#     print(\n#         f\"{len([item for sublist in mpaths for item in sublist])} models found.\",\n#         f\"{len(oof_paths)} OOFs found\",\n#     )\n\n#     # Construct OOF df\n#     oofs = pd.read_csv(INPUT_PATH / \"train.csv\", usecols=[\"id\", \"target\"]).sort_values(\n#         by=\"id\"\n#     )\n#     for i, p in enumerate(oof_paths):\n#         x = pd.read_csv(p).sort_values(by=\"id\")\n#         oofs[f\"model_{i}\"] = x[\"prediction\"].values\n\n#     df = pd.read_csv(INPUT_PATH / \"test.csv\")\n#     output = 0\n\n#     for i, group in enumerate(mpaths):\n#         output = 0\n#         for j, p in enumerate(group):\n#             ckpt_size = p.stat().st_size / 1024 ** 3\n#             bs = 32 if ckpt_size > 1.5 else 64\n#             print(f\"{i} {p}, Size: {ckpt_size:0.2f}\")\n\n#             if j == 0:\n#                 config = AutoConfig.from_pretrained(str(p.parent))\n#                 tokenizer = AutoTokenizer.from_pretrained(str(p.parent))\n#                 dataset = CommonLitDataset(df, tokenizer)\n\n#             model = CommonLitModel.load_from_checkpoint(p, hf_config=config)\n#             output += infer(model, dataset, batch_size=bs, device=device)\n\n#             del model\n\n#         del dataset\n#         del tokenizer\n#         gc.collect()\n\n#         df[f\"model_{i}\"] = output.squeeze().numpy() / len(group)\n\n#     pred_cols = [f\"model_{i}\" for i in range(len(mpaths))]\n\n#     # Use mean\n#     # df[\"target\"] = df[pred_cols].mean(1)\n\n#     # Stack using linear regression\n# #     print(oofs[pred_cols].corr())\n#     reg = RidgeCV(alphas=(0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50, 100, 500, 1000), normalize=True)\n# #     reg = LassoCV(max_iter=5000, random_state=48, n_jobs=-1, normalize=True)\n#     reg.fit(oofs[pred_cols], oofs[\"target\"])\n# #     print(f\"Weights: {reg.coef_}, bias: {reg.intercept_}\")\n#     print(f\"Best RMSE: {np.sqrt(-reg.best_score_):0.5f}. Alpha {reg.alpha_}\")  # Ridge\n# #     print(f\"Best RMSE: {np.sqrt(reg.mse_path_)[-1].mean():0.5f}\")  # Lasso\n#     df[\"target\"] = reg.predict(df[pred_cols])\n    \n#     # Stack using Netflix method\n# #     oof_preds = [oofs[c].values for c in pred_cols]\n# #     rmses = [np.sqrt(mean_squared_error(p, oofs[\"target\"])) for p in oof_preds]\n# #     ensemble, weights = netflix(rmses, oof_preds, 1.4100)\n# #     score = np.sqrt(mean_squared_error(ensemble, oofs[\"target\"]))\n# #     print(f\"Best RMSE: {score:0.5f}\")\n# #     df[\"target\"] = df[pred_cols] @ weights\n\n#     # Netflix with 3-seed CV\n#     #     X, y = oofs[pred_cols], oofs[\"target\"]\n#     #     scores = []\n#     #     weights_agg = 0\n#     #     for seed in [48, 42, 3]:\n#     #         for fold, (trn_idx, val_idx) in enumerate(create_folds(X, y, random_state=seed)):\n#     #             train_oofs = X.loc[trn_idx]\n#     #             valid_oofs = X.loc[val_idx]\n#     #             valid_target = y.loc[val_idx]\n\n#     #             train_preds = [train_oofs[c].values for c in X.columns]\n#     #             rmses = [np.sqrt(mean_squared_error(X[c], y)) for c in X.columns]\n#     #             _, weights = netflix(rmses, train_preds, 1.4100)\n\n#     #             val_pred = valid_oofs @ weights\n#     #             score = np.sqrt(mean_squared_error(val_pred, valid_target))\n#     #             scores.append(score)\n#     #             weights_agg += weights\n\n#     #     weights_agg /= len(scores)\n#     #     print(f\"CV RMSE: {np.mean(scores):0.5f}\")\n#     #     df[\"target\"] = df[pred_cols] @ weights_agg\n\n#     df[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)\n    \n# if __name__ == \"__main__\":\n\n#     model_folders = [\n#         \"20211204-023300\",\n#         \"20211205-062042\",\n#         \"20211205-134729\",\n#         \"20211206-015956\"\n#     ]\n\n\n#     dataset_paths = [OUTPUT_PATH / f for f in model_folders]\n\n#     predictions = make_predictions(dataset_paths, device=\"cuda\")","metadata":{},"execution_count":null,"outputs":[]}]}