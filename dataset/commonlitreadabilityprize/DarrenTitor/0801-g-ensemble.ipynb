{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hill-climbing","metadata":{}},{"cell_type":"code","source":"name_list = [\n             '0712-b-stratified-large',\n             '0713-a-stratified-large',\n             '0718-a-stratified', \n             '0718-b-stratified', \n             '0718-c-stratified', \n             '0718-d-stratified', \n             '0724-a-stratified',\n             '0726-a-stratified-large',\n             '0726-b-stratified',\n             '0726-c-stratified-large',\n             '0727-b-stratified-large',\n             '0727-c-stratified-large',\n             '0727-f-stratified-large',\n             '0728-c-stratified',\n             '0728-d-stratified',\n             '0728-e-stratified',\n            '0729-a-stratified-large',\n            '0729-b-stratified-large',\n            '0729-d-stratified-large',\n            '0730-b-stratified',\n            '0730-c-stratified-large',\n            '0731-a-stratified-large',\n            '0731-b-stratified-large',\n            '0731-e-stratified-large',\n            '0801-c-stratified',\n            ]","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:27.757386Z","iopub.execute_input":"2021-08-01T14:16:27.757752Z","iopub.status.idle":"2021-08-01T14:16:27.770421Z","shell.execute_reply.started":"2021-08-01T14:16:27.757674Z","shell.execute_reply":"2021-08-01T14:16:27.768286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\npd.options.display.max_colwidth = None\noof_path_list = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print('---------------')\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        print(path)\n        if 'oof' in path:\n            oof_path_list.append(path)\noof_path_list.sort()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:28.237029Z","iopub.execute_input":"2021-08-01T14:16:28.237361Z","iopub.status.idle":"2021-08-01T14:16:29.962626Z","shell.execute_reply.started":"2021-08-01T14:16:28.237329Z","shell.execute_reply":"2021-08-01T14:16:29.961817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_path_list","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:29.964088Z","iopub.execute_input":"2021-08-01T14:16:29.964452Z","iopub.status.idle":"2021-08-01T14:16:29.978435Z","shell.execute_reply.started":"2021-08-01T14:16:29.964413Z","shell.execute_reply":"2021-08-01T14:16:29.977247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_df = pd.DataFrame(oof_path_list, columns=['oof_path']).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:29.981809Z","iopub.execute_input":"2021-08-01T14:16:29.982048Z","iopub.status.idle":"2021-08-01T14:16:29.994145Z","shell.execute_reply.started":"2021-08-01T14:16:29.982025Z","shell.execute_reply":"2021-08-01T14:16:29.993212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_df","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:29.995381Z","iopub.execute_input":"2021-08-01T14:16:29.997345Z","iopub.status.idle":"2021-08-01T14:16:30.019165Z","shell.execute_reply.started":"2021-08-01T14:16:29.997286Z","shell.execute_reply":"2021-08-01T14:16:30.018363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_file_list = []\nfor name in name_list:\n    oof_cat = pd.DataFrame([])\n    for path in path_df['oof_path']:\n        if name in path:\n            oof_i_df = pd.read_csv(path)\n            oof_cat = pd.concat([oof_cat, oof_i_df],ignore_index=True)\n    oof_cat.to_csv(f'cat_{name}.csv')\n    print(f'cat_{name}.csv')\n    cat_file_list.append(oof_cat)\n\n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:30.020298Z","iopub.execute_input":"2021-08-01T14:16:30.020609Z","iopub.status.idle":"2021-08-01T14:16:31.645117Z","shell.execute_reply.started":"2021-08-01T14:16:30.02058Z","shell.execute_reply":"2021-08-01T14:16:31.644033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cal_rmse(pred_values, target_values):\n# np.sqrt(nn.MSELoss(reduction=\"mean\")(torch.tensor(a.pred.values), torch.tensor(a.target.values)).item())\n    return np.sqrt(nn.MSELoss(reduction=\"mean\")(torch.tensor(pred_values), torch.tensor(target_values)).item())","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:31.646474Z","iopub.execute_input":"2021-08-01T14:16:31.646974Z","iopub.status.idle":"2021-08-01T14:16:31.654671Z","shell.execute_reply.started":"2021-08-01T14:16:31.646923Z","shell.execute_reply":"2021-08-01T14:16:31.650855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in cat_file_list:\n    print(cal_rmse(file.pred.values, file.target.values))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:33.170154Z","iopub.execute_input":"2021-08-01T14:16:33.170558Z","iopub.status.idle":"2021-08-01T14:16:33.227385Z","shell.execute_reply.started":"2021-08-01T14:16:33.170525Z","shell.execute_reply":"2021-08-01T14:16:33.226165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# oof_path_list\n# path_df\n# name_list\n# cat_file_list","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:37.29852Z","iopub.execute_input":"2021-08-01T14:16:37.298845Z","iopub.status.idle":"2021-08-01T14:16:37.303833Z","shell.execute_reply.started":"2021-08-01T14:16:37.298816Z","shell.execute_reply":"2021-08-01T14:16:37.302893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_hillclimb():\n    best_ensemble = []\n    best_score = 1.5\n    \n    return best_ensemble, best_score","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:37.671977Z","iopub.execute_input":"2021-08-01T14:16:37.672279Z","iopub.status.idle":"2021-08-01T14:16:37.676492Z","shell.execute_reply.started":"2021-08-01T14:16:37.672251Z","shell.execute_reply":"2021-08-01T14:16:37.675482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_best_improvement(ensemble, cat_file_list):\n    best_score    = 1.5\n    best_ensemble = [] \n    # 寻找到label对应的能带来最大均值提升的那个文件\n    for i in range(0,len(cat_file_list)):\n        ensemble = ensemble + [i] \n        score = score_ensemble(ensemble, cat_file_list) # 加入均值集成，计算分数\n        \n        if score < best_score:\n            best_score    = score\n            best_ensemble = ensemble\n            \n        ensemble = ensemble[:-1]\n    \n    return best_ensemble, best_score","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:38.078414Z","iopub.execute_input":"2021-08-01T14:16:38.078732Z","iopub.status.idle":"2021-08-01T14:16:38.0846Z","shell.execute_reply.started":"2021-08-01T14:16:38.078704Z","shell.execute_reply":"2021-08-01T14:16:38.083312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_ensemble(ensemble, cat_file_list):\n    blend_preds = np.zeros(cat_file_list[0].shape[0]) \n    for model_idx in ensemble:\n        blend_preds += cat_file_list[model_idx]['pred']\n        \n    blend_preds = blend_preds/len(ensemble)\n    score = cal_rmse(blend_preds, cat_file_list[0]['target'].values)\n    \n    return score","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:38.505674Z","iopub.execute_input":"2021-08-01T14:16:38.506082Z","iopub.status.idle":"2021-08-01T14:16:38.515884Z","shell.execute_reply.started":"2021-08-01T14:16:38.506043Z","shell.execute_reply":"2021-08-01T14:16:38.515031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def climb(best_ensemble, best_score):\n    best_ensemble, best_score = find_best_improvement(best_ensemble, cat_file_list)\n    return best_ensemble, best_score","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:38.910334Z","iopub.execute_input":"2021-08-01T14:16:38.910645Z","iopub.status.idle":"2021-08-01T14:16:38.914482Z","shell.execute_reply.started":"2021-08-01T14:16:38.910619Z","shell.execute_reply":"2021-08-01T14:16:38.913548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'mean stacking: {score_ensemble(list(range(len(cat_file_list))), cat_file_list)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:39.648162Z","iopub.execute_input":"2021-08-01T14:16:39.64852Z","iopub.status.idle":"2021-08-01T14:16:39.684521Z","shell.execute_reply.started":"2021-08-01T14:16:39.648491Z","shell.execute_reply":"2021-08-01T14:16:39.683655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_optimal_weights(best_ensemble):\n    weights = {}\n    # 获取最优权重\n    for num in set(best_ensemble):\n        weights[num] = best_ensemble.count(num)/len(best_ensemble)\n    return weights","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:40.926253Z","iopub.execute_input":"2021-08-01T14:16:40.926609Z","iopub.status.idle":"2021-08-01T14:16:40.930975Z","shell.execute_reply.started":"2021-08-01T14:16:40.92658Z","shell.execute_reply":"2021-08-01T14:16:40.930175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_ensemble, best_score = init_hillclimb()\n\n# Run hillclimb\nfor i in range(100):\n    print(\"-------------\")\n    print(\"Step\", i)    \n    best_ensemble, best_score = climb(best_ensemble, best_score)\n    print(\"Best ensemble:\")\n    print(best_ensemble)\n    print(\"Best score:\")\n    print(best_score)\n\n# Get optimal weights\nopt_w = get_optimal_weights(best_ensemble)\nprint(\"Optimal weights:\")\nprint(opt_w)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:16:42.037817Z","iopub.execute_input":"2021-08-01T14:16:42.038128Z","iopub.status.idle":"2021-08-01T14:17:38.840646Z","shell.execute_reply.started":"2021-08-01T14:16:42.038098Z","shell.execute_reply":"2021-08-01T14:17:38.839847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range((len(cat_file_list))):\n    if i not in opt_w:\n        opt_w[i] = 0","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:17:56.540273Z","iopub.execute_input":"2021-08-01T14:17:56.540659Z","iopub.status.idle":"2021-08-01T14:17:56.545586Z","shell.execute_reply.started":"2021-08-01T14:17:56.540629Z","shell.execute_reply":"2021-08-01T14:17:56.544469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt_w","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:17:56.832782Z","iopub.execute_input":"2021-08-01T14:17:56.83312Z","iopub.status.idle":"2021-08-01T14:17:56.842653Z","shell.execute_reply.started":"2021-08-01T14:17:56.83309Z","shell.execute_reply":"2021-08-01T14:17:56.841488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import GradScaler\nfrom torch.cuda.amp import autocast\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\nfrom transformers import logging\nlogging.set_verbosity_error()\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport gc\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:59:56.156985Z","iopub.execute_input":"2021-07-30T13:59:56.157505Z","iopub.status.idle":"2021-07-30T14:00:03.625867Z","shell.execute_reply.started":"2021-07-30T13:59:56.157462Z","shell.execute_reply":"2021-07-30T14:00:03.624721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\n# example: print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:03.627492Z","iopub.execute_input":"2021-07-30T14:00:03.627964Z","iopub.status.idle":"2021-07-30T14:00:03.635354Z","shell.execute_reply.started":"2021-07-30T14:00:03.627917Z","shell.execute_reply":"2021-07-30T14:00:03.633876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:39:46.370251Z","iopub.execute_input":"2021-07-30T11:39:46.370624Z","iopub.status.idle":"2021-07-30T11:39:46.379101Z","shell.execute_reply.started":"2021-07-30T11:39:46.370588Z","shell.execute_reply":"2021-07-30T11:39:46.378206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n            # 注意这里要把 pred移到cpu上\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:39:46.380406Z","iopub.execute_input":"2021-07-30T11:39:46.380791Z","iopub.status.idle":"2021-07-30T11:39:46.388295Z","shell.execute_reply.started":"2021-07-30T11:39:46.380754Z","shell.execute_reply":"2021-07-30T11:39:46.387374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FOLDS = 5\nNUM_EPOCHS = 6\nSEED = 21\nBATCH_SIZE = 2\nMAX_LEN = 248\nNUM_WORKERS = 0\nIS_TARGET_SAMPLING = False\nIS_FREEZE_EMBEDDING = False\nIS_BERT_ADAM = True\nIS_STRATIFIED_KFOLD = True\nIS_USING_AMP = True\nROBERTA_HIDDEN_LEN = 768 # 768 or 1024\nRANDOM_FACTOR_OF_TARGET_SAMPLING = 0.1\nEARLY_STOPPING_PATIENCE = 15\nEARLY_STOPPING_LOSS_THRES = 0.485\nWEIGHT_DECAY = 0.01\n\nEVAL_SCHEDULE = [(0.50, 16), (0.495, 10), (0.49, 8), (0.485, 6),(0.48, 4), (0.47, 2), (-1., 1)]\n# ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nset_random_seed(SEED+NUM_FOLDS-1)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:39:46.39124Z","iopub.execute_input":"2021-07-30T11:39:46.391617Z","iopub.status.idle":"2021-07-30T11:39:46.441832Z","shell.execute_reply.started":"2021-07-30T11:39:46.391581Z","shell.execute_reply":"2021-07-30T11:39:46.440928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass LitDataset(Dataset):\n    def __init__(self, df, tokenizer, inference_only=False, is_target_sampling=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n         \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)    \n            self.err_std = torch.tensor(df.standard_error.values, dtype=torch.float32)   \n        self.is_target_sampling = is_target_sampling\n        self.encoded = tokenizer(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            # 注意这里要手动把return_attention_mask打开\n            return_attention_mask=True\n#             return_tensors='pt'\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        \n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            if self.is_target_sampling:\n                err_std = self.err_std[index]\n            # 这里对std可以有多种映射，这里先只尝试std=err_std\n                \n                sampled_target = torch.normal(mean=target, std=RANDOM_FACTOR_OF_TARGET_SAMPLING*err_std, \n                                              size=(1,1)).item()\n                sampled_target = torch.tensor([sampled_target], dtype=torch.float32)[0]\n                return (input_ids, attention_mask, sampled_target)\n            else:\n                return (input_ids, attention_mask, target)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:39:46.443986Z","iopub.execute_input":"2021-07-30T11:39:46.444592Z","iopub.status.idle":"2021-07-30T11:39:46.455902Z","shell.execute_reply.started":"2021-07-30T11:39:46.444554Z","shell.execute_reply":"2021-07-30T11:39:46.455086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:49:26.25802Z","iopub.execute_input":"2021-07-30T11:49:26.258375Z","iopub.status.idle":"2021-07-30T11:49:26.269894Z","shell.execute_reply.started":"2021-07-30T11:49:26.258338Z","shell.execute_reply":"2021-07-30T11:49:26.269107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH):\n\n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n\n    test_dataset = LitDataset(test_df, tokenizer, inference_only=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                             drop_last=False, shuffle=False, num_workers=NUM_WORKERS)\n\n    for index in range(NUM_FOLDS):            \n        print(f\"\\nUsing {model_path}\")\n\n        model = LitModel()\n        model.load_state_dict(torch.load(f'{input_path}/{model_path}_FOLD{index+1}.pth'))    \n        model.to(DEVICE)\n\n        temp_pred = predict(model, test_loader)\n        print(temp_pred)\n        result_sub = np.zeros(len(temp_pred)) \n        result_sub[:] = temp_pred.flatten()\n        result_sub_df = pd.DataFrame(result_sub, columns=['pred'])\n        result_sub_df = result_sub_df.to_csv(f'sub_{model_path}_FOLD{index+1}.csv', columns=['pred'])\n\n        del model, temp_pred, result_sub, result_sub_df\n        gc.collect()\n        torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:39:46.481669Z","iopub.execute_input":"2021-07-30T11:39:46.482003Z","iopub.status.idle":"2021-07-30T11:39:46.489718Z","shell.execute_reply.started":"2021-07-30T11:39:46.481969Z","shell.execute_reply":"2021-07-30T11:39:46.488606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_1","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\n    \nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n    \nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n    \nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(abs(self.layer_start), 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.conv_neck = CNNHead(abs(self.layer_start), 1024)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        hidden_state_images = torch.stack(roberta_output.hidden_states)\n        \n        hidden_state_images = hidden_state_images[self.layer_start:, :, :, :]\n        hidden_state_images = hidden_state_images.permute(1, 0, 2, 3)\n#         hidden_state_images : size(batch_size * |layer_start| * seq_len * hidden_num)\n        context_features = self.conv_neck(hidden_state_images)\n        return self.regressor(context_features)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:39:46.49117Z","iopub.execute_input":"2021-07-30T11:39:46.491537Z","iopub.status.idle":"2021-07-30T11:39:46.515399Z","shell.execute_reply.started":"2021-07-30T11:39:46.491503Z","shell.execute_reply":"2021-07-30T11:39:46.514533Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0718-a-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0718a\"\n\nTOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n# \nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:39:46.516683Z","iopub.execute_input":"2021-07-30T11:39:46.517022Z","iopub.status.idle":"2021-07-30T11:39:46.526021Z","shell.execute_reply.started":"2021-07-30T11:39:46.51699Z","shell.execute_reply":"2021-07-30T11:39:46.525339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_2","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n\nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n\n### Body\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -9\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.conv_neck = CNNHead(abs(self.layer_start), ROBERTA_HIDDEN_LEN)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)  \n\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:39:46.528999Z","iopub.execute_input":"2021-07-30T11:39:46.529307Z","iopub.status.idle":"2021-07-30T11:39:46.549434Z","shell.execute_reply.started":"2021-07-30T11:39:46.529274Z","shell.execute_reply":"2021-07-30T11:39:46.54845Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0718-b-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0718b\"\n\nTOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:39:46.550836Z","iopub.execute_input":"2021-07-30T11:39:46.5512Z","iopub.status.idle":"2021-07-30T11:40:34.599573Z","shell.execute_reply.started":"2021-07-30T11:39:46.551164Z","shell.execute_reply":"2021-07-30T11:40:34.598749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_3","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * abs(self.layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n\nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n\n### Body\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.conv_neck = CNNHead(abs(self.layer_start), ROBERTA_HIDDEN_LEN)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        sequence_output = self.pooler(roberta_output.hidden_states)\n        weights = self.attention(sequence_output)\n        context_vector = torch.sum(weights * sequence_output, dim=1)  \n\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.602266Z","iopub.execute_input":"2021-07-30T11:40:34.602541Z","iopub.status.idle":"2021-07-30T11:40:34.62269Z","shell.execute_reply.started":"2021-07-30T11:40:34.602513Z","shell.execute_reply":"2021-07-30T11:40:34.62187Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0718-c-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0718c\"\n\nTOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.623986Z","iopub.execute_input":"2021-07-30T11:40:34.624556Z","iopub.status.idle":"2021-07-30T11:40:34.63647Z","shell.execute_reply.started":"2021-07-30T11:40:34.624517Z","shell.execute_reply":"2021-07-30T11:40:34.635682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_4","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * abs(self.layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n\nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n\n### Body\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -9\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.conv_neck = CNNHead(abs(self.layer_start), ROBERTA_HIDDEN_LEN)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        sequence_output = self.pooler(roberta_output.hidden_states)\n        weights = self.attention(sequence_output)\n        context_vector = torch.sum(weights * sequence_output, dim=1)  \n\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.637771Z","iopub.execute_input":"2021-07-30T11:40:34.63819Z","iopub.status.idle":"2021-07-30T11:40:34.659303Z","shell.execute_reply.started":"2021-07-30T11:40:34.638153Z","shell.execute_reply":"2021-07-30T11:40:34.658561Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0718-d-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0718d\"","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.660541Z","iopub.execute_input":"2021-07-30T11:40:34.661051Z","iopub.status.idle":"2021-07-30T11:40:34.671952Z","shell.execute_reply.started":"2021-07-30T11:40:34.661013Z","shell.execute_reply":"2021-07-30T11:40:34.671127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.674657Z","iopub.execute_input":"2021-07-30T11:40:34.674911Z","iopub.status.idle":"2021-07-30T11:40:34.68246Z","shell.execute_reply.started":"2021-07-30T11:40:34.674889Z","shell.execute_reply":"2021-07-30T11:40:34.681687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_5","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n\nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n\n### Body\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -9\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(abs(self.layer_start), 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.conv_neck = CNNHead(abs(self.layer_start), 1024)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        hidden_state_images = torch.stack(roberta_output.hidden_states)\n        \n        hidden_state_images = hidden_state_images[self.layer_start:, :, :, :]\n        hidden_state_images = hidden_state_images.permute(1, 0, 2, 3)\n#         hidden_state_images : size(batch_size * |layer_start| * seq_len * hidden_num)\n        context_features = self.conv_neck(hidden_state_images)\n        return self.regressor(context_features)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.683876Z","iopub.execute_input":"2021-07-30T11:40:34.684344Z","iopub.status.idle":"2021-07-30T11:40:34.706296Z","shell.execute_reply.started":"2021-07-30T11:40:34.684288Z","shell.execute_reply":"2021-07-30T11:40:34.705549Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0724-a-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0724a\"","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.707496Z","iopub.execute_input":"2021-07-30T11:40:34.707836Z","iopub.status.idle":"2021-07-30T11:40:34.716869Z","shell.execute_reply.started":"2021-07-30T11:40:34.707801Z","shell.execute_reply":"2021-07-30T11:40:34.716063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.722407Z","iopub.execute_input":"2021-07-30T11:40:34.722708Z","iopub.status.idle":"2021-07-30T11:40:34.727989Z","shell.execute_reply.started":"2021-07-30T11:40:34.722666Z","shell.execute_reply":"2021-07-30T11:40:34.727153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_6","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\n\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n#         print(last_layer_hidden_states.shape)\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.730326Z","iopub.execute_input":"2021-07-30T11:40:34.730681Z","iopub.status.idle":"2021-07-30T11:40:34.747305Z","shell.execute_reply.started":"2021-07-30T11:40:34.730648Z","shell.execute_reply":"2021-07-30T11:40:34.746395Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0713-a-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0713a\"","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.74856Z","iopub.execute_input":"2021-07-30T11:40:34.748982Z","iopub.status.idle":"2021-07-30T11:40:34.758404Z","shell.execute_reply.started":"2021-07-30T11:40:34.748947Z","shell.execute_reply":"2021-07-30T11:40:34.75758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:40:34.761068Z","iopub.execute_input":"2021-07-30T11:40:34.761339Z","iopub.status.idle":"2021-07-30T11:42:16.240427Z","shell.execute_reply.started":"2021-07-30T11:40:34.761298Z","shell.execute_reply":"2021-07-30T11:42:16.239381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_7","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n    \nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n    \nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 16),\n            nn.ReLU(),\n            nn.Linear(16, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n#         print(last_layer_hidden_states.shape)\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.24194Z","iopub.execute_input":"2021-07-30T11:42:16.242208Z","iopub.status.idle":"2021-07-30T11:42:16.261575Z","shell.execute_reply.started":"2021-07-30T11:42:16.242183Z","shell.execute_reply":"2021-07-30T11:42:16.260704Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0726-a-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0726a\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.262938Z","iopub.execute_input":"2021-07-30T11:42:16.26329Z","iopub.status.idle":"2021-07-30T11:42:16.271997Z","shell.execute_reply.started":"2021-07-30T11:42:16.263255Z","shell.execute_reply":"2021-07-30T11:42:16.271184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_8","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n\nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n\n\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -9\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.conv_neck = CNNHead(abs(self.layer_start), ROBERTA_HIDDEN_LEN)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 16),\n            nn.ReLU(),\n            nn.Linear(16, 1)\n        )\n        \n\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)  \n\n        return self.regressor(context_vector)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.273402Z","iopub.execute_input":"2021-07-30T11:42:16.273758Z","iopub.status.idle":"2021-07-30T11:42:16.29468Z","shell.execute_reply.started":"2021-07-30T11:42:16.273722Z","shell.execute_reply":"2021-07-30T11:42:16.293857Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0726-b-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0726b\"\n\nTOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.29592Z","iopub.execute_input":"2021-07-30T11:42:16.296419Z","iopub.status.idle":"2021-07-30T11:42:16.305069Z","shell.execute_reply.started":"2021-07-30T11:42:16.296383Z","shell.execute_reply":"2021-07-30T11:42:16.30425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_9","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Linear(16, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n#         print(last_layer_hidden_states.shape)\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.306453Z","iopub.execute_input":"2021-07-30T11:42:16.306815Z","iopub.status.idle":"2021-07-30T11:42:16.323921Z","shell.execute_reply.started":"2021-07-30T11:42:16.306782Z","shell.execute_reply":"2021-07-30T11:42:16.322884Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0726-c-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0726c\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.325424Z","iopub.execute_input":"2021-07-30T11:42:16.325868Z","iopub.status.idle":"2021-07-30T11:42:16.333542Z","shell.execute_reply.started":"2021-07-30T11:42:16.325801Z","shell.execute_reply":"2021-07-30T11:42:16.3328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_10","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.PReLU(),\n            nn.Linear(512, 256),\n            nn.PReLU(),\n            nn.Linear(256, 64),\n            nn.PReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n#         print(last_layer_hidden_states.shape)\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.334621Z","iopub.execute_input":"2021-07-30T11:42:16.334859Z","iopub.status.idle":"2021-07-30T11:42:16.35299Z","shell.execute_reply.started":"2021-07-30T11:42:16.334837Z","shell.execute_reply":"2021-07-30T11:42:16.352264Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0727-b-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0727b\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.353798Z","iopub.execute_input":"2021-07-30T11:42:16.354039Z","iopub.status.idle":"2021-07-30T11:42:16.362889Z","shell.execute_reply.started":"2021-07-30T11:42:16.354015Z","shell.execute_reply":"2021-07-30T11:42:16.362182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_11","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n#         print(last_layer_hidden_states.shape)\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.364157Z","iopub.execute_input":"2021-07-30T11:42:16.364544Z","iopub.status.idle":"2021-07-30T11:42:16.382707Z","shell.execute_reply.started":"2021-07-30T11:42:16.364508Z","shell.execute_reply":"2021-07-30T11:42:16.381977Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0727-c-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0727c\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.384885Z","iopub.execute_input":"2021-07-30T11:42:16.385695Z","iopub.status.idle":"2021-07-30T11:42:16.393194Z","shell.execute_reply.started":"2021-07-30T11:42:16.385652Z","shell.execute_reply":"2021-07-30T11:42:16.392374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_12","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.layer_norm = nn.LayerNorm(1024)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_layer_hidden_states.size()).float()\n        sum_embeddings = torch.sum(last_layer_hidden_states * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        norm_mean_embeddings = self.layer_norm(mean_embeddings)\n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(norm_mean_embeddings)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.396075Z","iopub.execute_input":"2021-07-30T11:42:16.396335Z","iopub.status.idle":"2021-07-30T11:42:16.413919Z","shell.execute_reply.started":"2021-07-30T11:42:16.396289Z","shell.execute_reply":"2021-07-30T11:42:16.413094Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0727-f-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0727f\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:42:16.41529Z","iopub.execute_input":"2021-07-30T11:42:16.415718Z","iopub.status.idle":"2021-07-30T11:44:11.342068Z","shell.execute_reply.started":"2021-07-30T11:42:16.415684Z","shell.execute_reply":"2021-07-30T11:44:11.341283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_13","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * abs(self.layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n\nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n\n# Body\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -2\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.conv_neck = CNNHead(abs(self.layer_start), ROBERTA_HIDDEN_LEN)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        sequence_output = self.pooler(roberta_output.hidden_states)\n        weights = self.attention(sequence_output)\n        context_vector = torch.sum(weights * sequence_output, dim=1)  \n\n        return self.regressor(context_vector)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.343534Z","iopub.execute_input":"2021-07-30T11:44:11.343874Z","iopub.status.idle":"2021-07-30T11:44:11.365053Z","shell.execute_reply.started":"2021-07-30T11:44:11.343837Z","shell.execute_reply":"2021-07-30T11:44:11.363785Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0728-c-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0728c\"\n\nTOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.366505Z","iopub.execute_input":"2021-07-30T11:44:11.367028Z","iopub.status.idle":"2021-07-30T11:44:11.378387Z","shell.execute_reply.started":"2021-07-30T11:44:11.366992Z","shell.execute_reply":"2021-07-30T11:44:11.377485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_14","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * abs(self.layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n\nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n\n# Body\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -3\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.conv_neck = CNNHead(abs(self.layer_start), ROBERTA_HIDDEN_LEN)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        sequence_output = self.pooler(roberta_output.hidden_states)\n        weights = self.attention(sequence_output)\n        context_vector = torch.sum(weights * sequence_output, dim=1)  \n\n        return self.regressor(context_vector)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.379916Z","iopub.execute_input":"2021-07-30T11:44:11.380483Z","iopub.status.idle":"2021-07-30T11:44:11.407026Z","shell.execute_reply.started":"2021-07-30T11:44:11.380291Z","shell.execute_reply":"2021-07-30T11:44:11.406029Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0728-d-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0728d\"\n\nTOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.408654Z","iopub.execute_input":"2021-07-30T11:44:11.409123Z","iopub.status.idle":"2021-07-30T11:44:11.416813Z","shell.execute_reply.started":"2021-07-30T11:44:11.409082Z","shell.execute_reply":"2021-07-30T11:44:11.416072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_15","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n\nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n\n# Body\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -2\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(abs(self.layer_start), 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.conv_neck = CNNHead(abs(self.layer_start), 1024)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        hidden_state_images = torch.stack(roberta_output.hidden_states)\n        \n        hidden_state_images = hidden_state_images[self.layer_start:, :, :, :]\n        hidden_state_images = hidden_state_images.permute(1, 0, 2, 3)\n#         hidden_state_images : size(batch_size * |layer_start| * seq_len * hidden_num)\n        context_features = self.conv_neck(hidden_state_images)\n        return self.regressor(context_features)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.418258Z","iopub.execute_input":"2021-07-30T11:44:11.418667Z","iopub.status.idle":"2021-07-30T11:44:11.440857Z","shell.execute_reply.started":"2021-07-30T11:44:11.418631Z","shell.execute_reply":"2021-07-30T11:44:11.439742Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0728-e-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0728e\"\n\nTOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.442548Z","iopub.execute_input":"2021-07-30T11:44:11.442941Z","iopub.status.idle":"2021-07-30T11:44:11.450565Z","shell.execute_reply.started":"2021-07-30T11:44:11.442905Z","shell.execute_reply":"2021-07-30T11:44:11.449782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_16","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        \n        \n        \n        \n        \n        \n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n#         print(last_layer_hidden_states.shape)\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.451622Z","iopub.execute_input":"2021-07-30T11:44:11.451915Z","iopub.status.idle":"2021-07-30T11:44:11.468067Z","shell.execute_reply.started":"2021-07-30T11:44:11.451891Z","shell.execute_reply":"2021-07-30T11:44:11.467015Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0712-b-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0712b\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.469554Z","iopub.execute_input":"2021-07-30T11:44:11.470056Z","iopub.status.idle":"2021-07-30T11:44:11.480824Z","shell.execute_reply.started":"2021-07-30T11:44:11.470016Z","shell.execute_reply":"2021-07-30T11:44:11.47979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_17","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.PReLU(),\n            nn.Linear(512, 256),\n            nn.PReLU(),\n            nn.Linear(256, 128),\n            nn.PReLU(),\n            nn.Linear(128, 64),\n            nn.PReLU(),\n            nn.Linear(64, 16),\n            nn.PReLU(),\n            nn.Linear(16, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n#         print(last_layer_hidden_states.shape)\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.482387Z","iopub.execute_input":"2021-07-30T11:44:11.482867Z","iopub.status.idle":"2021-07-30T11:44:11.504409Z","shell.execute_reply.started":"2021-07-30T11:44:11.482827Z","shell.execute_reply":"2021-07-30T11:44:11.503522Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0729-a-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0727b\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.507279Z","iopub.execute_input":"2021-07-30T11:44:11.507771Z","iopub.status.idle":"2021-07-30T11:44:11.516992Z","shell.execute_reply.started":"2021-07-30T11:44:11.507735Z","shell.execute_reply":"2021-07-30T11:44:11.516238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_18","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.PReLU(),\n            nn.Linear(512, 256),\n            nn.PReLU(),\n            nn.Linear(256, 128),\n            nn.PReLU(),\n            nn.Linear(128, 128),\n            nn.PReLU(),\n            nn.Linear(128, 64),\n            nn.PReLU(),\n            nn.Linear(64, 16),\n            nn.PReLU(),\n            nn.Linear(16, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n#         print(last_layer_hidden_states.shape)\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.51856Z","iopub.execute_input":"2021-07-30T11:44:11.519037Z","iopub.status.idle":"2021-07-30T11:44:11.537074Z","shell.execute_reply.started":"2021-07-30T11:44:11.519Z","shell.execute_reply":"2021-07-30T11:44:11.536106Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0729-b-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0727b\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.538428Z","iopub.execute_input":"2021-07-30T11:44:11.538846Z","iopub.status.idle":"2021-07-30T11:44:11.547018Z","shell.execute_reply.started":"2021-07-30T11:44:11.538808Z","shell.execute_reply":"2021-07-30T11:44:11.546136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_19","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.LeakyReLU(0.1),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, 64),\n            nn.LeakyReLU(0.1),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n#         print(last_layer_hidden_states.shape)\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.549852Z","iopub.execute_input":"2021-07-30T11:44:11.550137Z","iopub.status.idle":"2021-07-30T11:44:11.56662Z","shell.execute_reply.started":"2021-07-30T11:44:11.550113Z","shell.execute_reply":"2021-07-30T11:44:11.565877Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0729-d-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0729d\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:11.567889Z","iopub.execute_input":"2021-07-30T11:44:11.56823Z","iopub.status.idle":"2021-07-30T11:44:11.57655Z","shell.execute_reply.started":"2021-07-30T11:44:11.568196Z","shell.execute_reply":"2021-07-30T11:44:11.575664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_20","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n\nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n\n# Body\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -6\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(abs(self.layer_start), 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.conv_neck = CNNHead(abs(self.layer_start), 1024)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.PReLU(),\n            nn.Linear(512, 256),\n            nn.PReLU(),\n            nn.Linear(256, 128),\n            nn.PReLU(),\n            nn.Linear(128, 64),\n            nn.PReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        hidden_state_images = torch.stack(roberta_output.hidden_states)\n        \n        hidden_state_images = hidden_state_images[self.layer_start:, :, :, :]\n        hidden_state_images = hidden_state_images.permute(1, 0, 2, 3)\n#         hidden_state_images : size(batch_size * |layer_start| * seq_len * hidden_num)\n        context_features = self.conv_neck(hidden_state_images)\n        return self.regressor(context_features)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:44:59.906301Z","iopub.execute_input":"2021-07-30T11:44:59.906747Z","iopub.status.idle":"2021-07-30T11:44:59.941155Z","shell.execute_reply.started":"2021-07-30T11:44:59.90671Z","shell.execute_reply":"2021-07-30T11:44:59.94018Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0730-b-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0730b\"","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:45:00.310812Z","iopub.execute_input":"2021-07-30T11:45:00.311126Z","iopub.status.idle":"2021-07-30T11:45:00.317707Z","shell.execute_reply.started":"2021-07-30T11:45:00.311095Z","shell.execute_reply":"2021-07-30T11:45:00.316878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:45:02.623632Z","iopub.execute_input":"2021-07-30T11:45:02.623961Z","iopub.status.idle":"2021-07-30T11:45:35.231165Z","shell.execute_reply.started":"2021-07-30T11:45:02.623933Z","shell.execute_reply":"2021-07-30T11:45:35.230343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_21","metadata":{}},{"cell_type":"code","source":"\n\nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.layer_norm = nn.LayerNorm(1024)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.PReLU(),\n            nn.Linear(512, 256),\n            nn.PReLU(),\n            nn.Linear(256, 128),\n            nn.PReLU(),\n            nn.Linear(128, 64),\n            nn.PReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_layer_hidden_states.size()).float()\n        sum_embeddings = torch.sum(last_layer_hidden_states * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        norm_mean_embeddings = self.layer_norm(mean_embeddings)\n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(norm_mean_embeddings)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:47:42.252837Z","iopub.execute_input":"2021-07-30T11:47:42.253214Z","iopub.status.idle":"2021-07-30T11:47:42.274453Z","shell.execute_reply.started":"2021-07-30T11:47:42.253176Z","shell.execute_reply":"2021-07-30T11:47:42.27317Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0730-c-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0730c\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:47:46.852385Z","iopub.execute_input":"2021-07-30T11:47:46.852705Z","iopub.status.idle":"2021-07-30T11:49:26.256571Z","shell.execute_reply.started":"2021-07-30T11:47:46.852676Z","shell.execute_reply":"2021-07-30T11:49:26.255734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_22","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.layer_norm = nn.LayerNorm(1024)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.PReLU(),\n            nn.Linear(512, 256),\n            nn.PReLU(),\n            nn.Linear(256, 256),\n            nn.PReLU(),\n            nn.Linear(256, 128),\n            nn.PReLU(),\n            nn.Linear(128, 64),\n            nn.PReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_layer_hidden_states.size()).float()\n        sum_embeddings = torch.sum(last_layer_hidden_states * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        norm_mean_embeddings = self.layer_norm(mean_embeddings)\n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(norm_mean_embeddings)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:47:42.252837Z","iopub.execute_input":"2021-07-30T11:47:42.253214Z","iopub.status.idle":"2021-07-30T11:47:42.274453Z","shell.execute_reply.started":"2021-07-30T11:47:42.253176Z","shell.execute_reply":"2021-07-30T11:47:42.27317Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0731-a-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0731a\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:47:46.852385Z","iopub.execute_input":"2021-07-30T11:47:46.852705Z","iopub.status.idle":"2021-07-30T11:49:26.256571Z","shell.execute_reply.started":"2021-07-30T11:47:46.852676Z","shell.execute_reply":"2021-07-30T11:49:26.255734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_23","metadata":{}},{"cell_type":"code","source":"\n\nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * abs(self.layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.PReLU(),\n            nn.Linear(512, 256),\n            nn.PReLU(),\n            nn.Linear(256, 256),\n            nn.PReLU(),\n            nn.Linear(256, 128),\n            nn.PReLU(),\n            nn.Linear(128, 64),\n            nn.PReLU(),\n            nn.Linear(64, 16),\n            nn.PReLU(),\n            nn.Linear(16, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        sequence_output = self.pooler(roberta_output.hidden_states)\n        weights = self.attention(sequence_output)\n        context_vector = torch.sum(weights * sequence_output, dim=1)  \n\n        return self.regressor(context_vector)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:47:42.252837Z","iopub.execute_input":"2021-07-30T11:47:42.253214Z","iopub.status.idle":"2021-07-30T11:47:42.274453Z","shell.execute_reply.started":"2021-07-30T11:47:42.253176Z","shell.execute_reply":"2021-07-30T11:47:42.27317Z"},"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0731-b-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0731b\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:47:46.852385Z","iopub.execute_input":"2021-07-30T11:47:46.852705Z","iopub.status.idle":"2021-07-30T11:49:26.256571Z","shell.execute_reply.started":"2021-07-30T11:47:46.852676Z","shell.execute_reply":"2021-07-30T11:49:26.255734Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_24","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * abs(self.layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        self.layer_start = -8\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=self.layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 512),\n            nn.PReLU(),\n            nn.Linear(512, 256),\n            nn.PReLU(),\n            nn.Linear(256, 256),\n            nn.PReLU(),\n            nn.Linear(256, 128),\n            nn.PReLU(),\n            nn.Linear(128, 64),\n            nn.PReLU(),\n            nn.Linear(64, 16),\n            nn.PReLU(),\n            nn.Linear(16, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)   \n        sequence_output = self.pooler(roberta_output.hidden_states)\n        weights = self.attention(sequence_output)\n        context_vector = torch.sum(weights * sequence_output, dim=1)  \n\n        return self.regressor(context_vector)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:47:42.252837Z","iopub.execute_input":"2021-07-30T11:47:42.253214Z","iopub.status.idle":"2021-07-30T11:47:42.274453Z","shell.execute_reply.started":"2021-07-30T11:47:42.253176Z","shell.execute_reply":"2021-07-30T11:47:42.27317Z"},"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\ninput_path = '../input/0731-e-stratified-large-roberta/'\nmodel_path = f\"Roberta_large_model_0731e\"\n\nTOKENIZER_PATH = \"../input/0712-a-pretrain-large-pipeline/my_roberta_large_pretrained_0711a\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:47:46.852385Z","iopub.execute_input":"2021-07-30T11:47:46.852705Z","iopub.status.idle":"2021-07-30T11:49:26.256571Z","shell.execute_reply.started":"2021-07-30T11:47:46.852676Z","shell.execute_reply":"2021-07-30T11:49:26.255734Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_25","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * abs(self.layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_layer_embeddings_input):\n#         print('num',self.num_hidden_layers )\n        ft_all_layers = all_layer_embeddings_input\n        # these teo lines convert 'tuple of tensors' into 'tensor', and then slice on it\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n#         print('h',all_layer_embedding.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n\nclass ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n\nclass Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n\nclass CNNHead(nn.Module):\n    def __init__(self, in_channel, out_num, kernel_size=10):\n        super().__init__() \n        self.head = nn.Sequential(\n            nn.Conv2d(in_channel, 8, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((64, 256)),\n            nn.BatchNorm2d(8),\n            nn.Conv2d(8, 16, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((16, 64)),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            nn.AdaptiveMaxPool2d((4, 16)),\n            nn.BatchNorm2d(32),\n            # batch_size * 32 * 4 * 16\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 16, out_num)\n        )\n        \n    def forward(self, x):\n        return self.head(x)\n\n# Body\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7,\n                       'output_hidden_states':True})   \n        \n        freeze_embedding = IS_FREEZE_EMBEDDING\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config, )  \n        self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)    \n        \n        \n#         self.config_hidden_size = config.hidden_size\n        \n        layer_start = 4\n        self.pooler = WeightedLayerPooling(\n            config.num_hidden_layers, \n            layer_start=layer_start, layer_weights=None\n        )\n        self.arcface = ArcFaceClassifier(emb_size=512,output_classes=1)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),            \n            nn.Tanh(),                       \n            self.arcface\n        )        \n        self.layer_norm = nn.LayerNorm(ROBERTA_HIDDEN_LEN)\n        self.regressor = nn.Sequential(                        \n            nn.Linear(ROBERTA_HIDDEN_LEN, 512),\n            nn.PReLU(),\n            nn.Linear(512, 256),\n            nn.PReLU(),\n            nn.Linear(256, 128),\n            nn.PReLU(),\n            nn.Linear(128, 64),\n            nn.PReLU(),\n            nn.Linear(64, 1)\n        )\n#         self.fc = nn.Linear(config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        \n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_layer_hidden_states.size()).float()\n        sum_embeddings = torch.sum(last_layer_hidden_states * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        norm_mean_embeddings = self.layer_norm(mean_embeddings)\n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(norm_mean_embeddings)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:47:42.252837Z","iopub.execute_input":"2021-07-30T11:47:42.253214Z","iopub.status.idle":"2021-07-30T11:47:42.274453Z","shell.execute_reply.started":"2021-07-30T11:47:42.253176Z","shell.execute_reply":"2021-07-30T11:47:42.27317Z"},"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\ninput_path = '../input/0801-c-stratified-maunish-roberta'\nmodel_path = f\"Roberta_maunish_model_0801c\"\n\nTOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n\nmake_submission(ROBERTA_PATH, input_path, model_path, TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:47:46.852385Z","iopub.execute_input":"2021-07-30T11:47:46.852705Z","iopub.status.idle":"2021-07-30T11:49:26.256571Z","shell.execute_reply.started":"2021-07-30T11:47:46.852676Z","shell.execute_reply":"2021-07-30T11:49:26.255734Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"model_path_list = [\n    f\"Roberta_large_model_0712b\",\n    f\"Roberta_large_model_0713a\",\n    f\"Roberta_maunish_model_0718a\",\n    f\"Roberta_maunish_model_0718b\",\n    f\"Roberta_maunish_model_0718c\",\n    f\"Roberta_maunish_model_0718d\",\n    f\"Roberta_maunish_model_0724a\",\n    f\"Roberta_large_model_0726a\",\n    f\"Roberta_maunish_model_0726b\",\n    f\"Roberta_large_model_0726c\",\n    f\"Roberta_large_model_0727b\",\n    f\"Roberta_large_model_0727c\",\n    f\"Roberta_large_model_0727f\",\n    f\"Roberta_maunish_model_0728c\",\n    f\"Roberta_maunish_model_0728d\",\n    f\"Roberta_maunish_model_0728e\",\n    f\"Roberta_large_model_0727b\", # 29a\n    f\"Roberta_large_model_0727b\", # 29b\n    f\"Roberta_large_model_0729d\",\n    f\"Roberta_maunish_model_0730b\",\n    f\"Roberta_large_model_0730c\",\n    f\"Roberta_large_model_0731a\",\n    f\"Roberta_large_model_0731b\",\n    f\"Roberta_large_model_0731e\",\n    f\"Roberta_maunish_model_0801c\",\n                  ]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:49:26.271542Z","iopub.execute_input":"2021-07-30T11:49:26.271791Z","iopub.status.idle":"2021-07-30T11:49:26.276025Z","shell.execute_reply.started":"2021-07-30T11:49:26.271768Z","shell.execute_reply":"2021-07-30T11:49:26.275274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt_w","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:49:26.277209Z","iopub.execute_input":"2021-07-30T11:49:26.277698Z","iopub.status.idle":"2021-07-30T11:49:26.286927Z","shell.execute_reply.started":"2021-07-30T11:49:26.277663Z","shell.execute_reply":"2021-07-30T11:49:26.286049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model_idx, model_path in enumerate(model_path_list):\n    for index in range(NUM_FOLDS):            \n        print(f\"\\nUsing {model_path}\")\n\n        sub_i_df = pd.read_csv(f'sub_{model_path}_FOLD{index+1}.csv')\n        submission_df.target += sub_i_df.pred * opt_w[model_idx] / NUM_FOLDS\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:49:31.877438Z","iopub.execute_input":"2021-07-30T11:49:31.877761Z","iopub.status.idle":"2021-07-30T11:49:31.953153Z","shell.execute_reply.started":"2021-07-30T11:49:31.877732Z","shell.execute_reply":"2021-07-30T11:49:31.952368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:49:32.893831Z","iopub.execute_input":"2021-07-30T11:49:32.89415Z","iopub.status.idle":"2021-07-30T11:49:32.899501Z","shell.execute_reply.started":"2021-07-30T11:49:32.894119Z","shell.execute_reply":"2021-07-30T11:49:32.898423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}