{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Visualization\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Text processing\nimport string\nfrom nltk.corpus import stopwords\n\nimport numpy as np\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-18T19:05:18.684962Z","iopub.execute_input":"2021-06-18T19:05:18.68559Z","iopub.status.idle":"2021-06-18T19:05:19.460328Z","shell.execute_reply.started":"2021-06-18T19:05:18.685552Z","shell.execute_reply":"2021-06-18T19:05:19.458947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.462222Z","iopub.execute_input":"2021-06-18T19:05:19.462596Z","iopub.status.idle":"2021-06-18T19:05:19.487742Z","shell.execute_reply.started":"2021-06-18T19:05:19.462563Z","shell.execute_reply":"2021-06-18T19:05:19.486801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\nprint(f'Train shape: {df_train.shape}\\n')\nprint('Train info:')\ndf_train.info()\nprint()\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.489847Z","iopub.execute_input":"2021-06-18T19:05:19.490156Z","iopub.status.idle":"2021-06-18T19:05:19.562522Z","shell.execute_reply.started":"2021-06-18T19:05:19.490128Z","shell.execute_reply":"2021-06-18T19:05:19.561589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nprint(f'Test shape: {df_test.shape}\\n')\nprint('Test info:')\ndf_test.info()\nprint()\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.564124Z","iopub.execute_input":"2021-06-18T19:05:19.564413Z","iopub.status.idle":"2021-06-18T19:05:19.592438Z","shell.execute_reply.started":"2021-06-18T19:05:19.564385Z","shell.execute_reply":"2021-06-18T19:05:19.591182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.593878Z","iopub.execute_input":"2021-06-18T19:05:19.594222Z","iopub.status.idle":"2021-06-18T19:05:19.640624Z","shell.execute_reply.started":"2021-06-18T19:05:19.594192Z","shell.execute_reply":"2021-06-18T19:05:19.639472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.642196Z","iopub.execute_input":"2021-06-18T19:05:19.642563Z","iopub.status.idle":"2021-06-18T19:05:19.663361Z","shell.execute_reply.started":"2021-06-18T19:05:19.642522Z","shell.execute_reply":"2021-06-18T19:05:19.662005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check cardinality of features","metadata":{}},{"cell_type":"code","source":"for feature in df_train.columns:\n    print(f\"Cardinality of {feature.upper()} in train dataset: {df_train[feature].nunique()}\")\nprint()\nfor feature in df_test.columns:\n    print(f\"Cardinality of {feature.upper()} in test dataset: {df_test[feature].nunique()}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.66529Z","iopub.execute_input":"2021-06-18T19:05:19.66579Z","iopub.status.idle":"2021-06-18T19:05:19.697186Z","shell.execute_reply.started":"2021-06-18T19:05:19.665746Z","shell.execute_reply":"2021-06-18T19:05:19.696044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's impute all missing values","metadata":{}},{"cell_type":"code","source":"df_train['url_legal'] = df_train['url_legal'].fillna('Missing')\ndf_train['license'] = df_train['license'].fillna('Missing')\n\ndf_test['url_legal'] = df_test['url_legal'].fillna('Missing')\ndf_test['license'] = df_test['license'].fillna('Missing')","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.698553Z","iopub.execute_input":"2021-06-18T19:05:19.698848Z","iopub.status.idle":"2021-06-18T19:05:19.708773Z","shell.execute_reply.started":"2021-06-18T19:05:19.698819Z","shell.execute_reply":"2021-06-18T19:05:19.707251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['target'].sort_values(ascending=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.711502Z","iopub.execute_input":"2021-06-18T19:05:19.711964Z","iopub.status.idle":"2021-06-18T19:05:19.731902Z","shell.execute_reply.started":"2021-06-18T19:05:19.711915Z","shell.execute_reply":"2021-06-18T19:05:19.73111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Value of target feature corresponds to complexity of text. Larger value means that text is easier, i.e. text with target value -3.676268 is the most complex. Text appropriate 1.711390 value is the easiest.","metadata":{}},{"cell_type":"markdown","source":"Let's check this","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 300) # That allow us to check first 300 symbols of text in cell (replace 300 with None to see whole text)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.73318Z","iopub.execute_input":"2021-06-18T19:05:19.733474Z","iopub.status.idle":"2021-06-18T19:05:19.74179Z","shell.execute_reply.started":"2021-06-18T19:05:19.733445Z","shell.execute_reply":"2021-06-18T19:05:19.740953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.sort_values('target')['excerpt'].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.743222Z","iopub.execute_input":"2021-06-18T19:05:19.743527Z","iopub.status.idle":"2021-06-18T19:05:19.762714Z","shell.execute_reply.started":"2021-06-18T19:05:19.743498Z","shell.execute_reply":"2021-06-18T19:05:19.761489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.sort_values('target')['excerpt'].tail()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.764318Z","iopub.execute_input":"2021-06-18T19:05:19.764622Z","iopub.status.idle":"2021-06-18T19:05:19.777951Z","shell.execute_reply.started":"2021-06-18T19:05:19.764593Z","shell.execute_reply":"2021-06-18T19:05:19.776741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that suggestion was prooved","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 50)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.779851Z","iopub.execute_input":"2021-06-18T19:05:19.780233Z","iopub.status.idle":"2021-06-18T19:05:19.787631Z","shell.execute_reply.started":"2021-06-18T19:05:19.7802Z","shell.execute_reply":"2021-06-18T19:05:19.786499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engeneering","metadata":{}},{"cell_type":"code","source":"df_train_new = df_train.copy()\ndf_test_new = df_test.copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.789317Z","iopub.execute_input":"2021-06-18T19:05:19.789681Z","iopub.status.idle":"2021-06-18T19:05:19.803576Z","shell.execute_reply.started":"2021-06-18T19:05:19.789649Z","shell.execute_reply":"2021-06-18T19:05:19.802164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_new = df_train_new.sort_values(by='target', ascending=False)\ndf_train_new","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.804907Z","iopub.execute_input":"2021-06-18T19:05:19.805296Z","iopub.status.idle":"2021-06-18T19:05:19.831223Z","shell.execute_reply.started":"2021-06-18T19:05:19.805263Z","shell.execute_reply":"2021-06-18T19:05:19.830276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With creating new features we can improve our understanding of undependent features","metadata":{}},{"cell_type":"code","source":"df_train_new['word_count'] = df_train_new['excerpt'].apply(lambda x: len(str(x).split()))\ndf_test_new['word_count'] = df_test_new['excerpt'].apply(lambda x: len(str(x).split()))\n\ndf_train_new['unique_word_count'] = df_train_new['excerpt'].apply(lambda x: len(set(str(x).split())))   \ndf_test_new['unique_word_count'] = df_test_new['excerpt'].apply(lambda x: len(set(str(x).split())))\n\ndf_train_new['stop_words_count'] = df_train_new['excerpt'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\ndf_test_new['stop_words_count'] = df_test_new['excerpt'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n\ndf_train_new['mean_word_length'] = df_train_new['excerpt'].apply(lambda x: np.mean(len(str(x).split())))\ndf_test_new['mean_word_length'] = df_test_new['excerpt'].apply(lambda x:  np.mean(len(str(x).split())))\n\ndf_train_new['char_count'] = df_train_new['excerpt'].apply(lambda x: len(str(x)))\ndf_test_new['char_count'] = df_test_new['excerpt'].apply(lambda x: len(str(x)))\n\ndf_train_new['punctuation_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ndf_test_new['punctuation_count'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n\n# Individual punctuation marks can also be important\ndf_train_new['question_mark_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '?']))\ndf_test_new['question_mark_count'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '?']))\n\ndf_train_new['exclamation_mark_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '!']))\ndf_test_new['exclamation_mark_count'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '!']))\n\ndf_train_new['comma_mark_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == ',']))\ndf_test_new['comma_mark_count'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == ',']))\n\ndf_train_new['point_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '.']))\ndf_test_new['pointCount'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '.']))\n\ndf_train_new['ellipsis_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '...']))\ndf_test_new['ellipsis_count'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '...']))\n\n\n# I guess that in texts for elementary school number of pronouns is more because sentences are easier\nmy_stopwords = ['i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'it', 'its', 'we', 'our', 'they', 'their']\n\ndf_train_new['pronoun_count'] = df_train_new['excerpt'].apply(lambda x: len([w for w in str(x).lower().split() if w in my_stopwords]))\ndf_test_new['pronoun_count'] = df_test_new['excerpt'].apply(lambda x: len([w for w in str(x).lower().split() if w in my_stopwords]))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:05:19.832267Z","iopub.execute_input":"2021-06-18T19:05:19.832526Z","iopub.status.idle":"2021-06-18T19:06:19.570668Z","shell.execute_reply.started":"2021-06-18T19:05:19.832499Z","shell.execute_reply":"2021-06-18T19:06:19.569473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins = 100\nplt.figure(figsize=(8,6))\nplt.hist(df_train_new['target'], bins, alpha=0.5, label='target')\nplt.title('Target distribution')\nplt.legend(loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T20:43:35.96207Z","iopub.execute_input":"2021-06-18T20:43:35.962458Z","iopub.status.idle":"2021-06-18T20:43:36.315846Z","shell.execute_reply.started":"2021-06-18T20:43:35.962425Z","shell.execute_reply":"2021-06-18T20:43:36.314438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target feature has normal distribution","metadata":{}},{"cell_type":"code","source":"meta_features = ['standard_error', 'word_count', 'unique_word_count', 'stop_words_count', 'mean_word_length', 'char_count', 'punctuation_count', 'question_mark_count',\n                'exclamation_mark_count', 'comma_mark_count', 'point_count', 'ellipsis_count', 'pronoun_count']\n\nfig, axs = plt.subplots(ncols=2, nrows=len(meta_features), figsize=(20, 50), dpi=100)\n\nfor i, feature in enumerate(meta_features):\n    sns.histplot(df_train_new[feature], label=f'{feature} distribution in Training dataset', ax=axs[i][0], kde=True)\n    sns.regplot(data=df_train_new, x='target', y=feature, ax=axs[i][1])\n    \n    for j in range(2):\n        axs[i][j].set_xlabel('')\n        axs[i][j].tick_params(axis='x', labelsize=12)\n        axs[i][j].tick_params(axis='y', labelsize=12)\n        axs[i][j].legend()\n        \n    axs[i][0].set_title(f'{feature} distribution in Training dataset', fontsize=13)\n    axs[i][1].set_title('Target distribution', fontsize=13)\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:59:03.585542Z","iopub.execute_input":"2021-06-18T19:59:03.586149Z","iopub.status.idle":"2021-06-18T19:59:14.388879Z","shell.execute_reply.started":"2021-06-18T19:59:03.58611Z","shell.execute_reply":"2021-06-18T19:59:14.387275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen word_count, unique_word_count, unique_word_count, stop_words_count, mean_word_length, char_count and comma_mark_count decreases while target feature increases. But **word_count, unique_word_count and char_count decreases** more than others. It is logical because vocabulary in primary school is small, words are simple, not very long and they often repeated.\nSuch features as **point_count and pronoun_count** increases while target feature increases. That's because in primary school sentences are short -> more sentences -> more points. As I suggested pronoun_count is more in primary school than in high school, because when text consists of small sentences noun quantity is small and author have to replace nouns with pronouns.\n**standard_error** is higher for primary and high school.\nAll other features (that I haven't highlighted in bold) are not representative.","metadata":{}}]}