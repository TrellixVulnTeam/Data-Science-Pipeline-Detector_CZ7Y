{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# -*- coding: utf-8 -*-\n\"\"\"`tldextract` accurately separates the gTLD or ccTLD (generic or country code\ntop-level domain) from the registered domain and subdomains of a URL.\n    >>> import tldextract\n    >>> tldextract.extract('http://forums.news.cnn.com/')\n    ExtractResult(subdomain='forums.news', domain='cnn', suffix='com')\n    >>> tldextract.extract('http://forums.bbc.co.uk/') # United Kingdom\n    ExtractResult(subdomain='forums', domain='bbc', suffix='co.uk')\n    >>> tldextract.extract('http://www.worldbank.org.kg/') # Kyrgyzstan\n    ExtractResult(subdomain='www', domain='worldbank', suffix='org.kg')\n`ExtractResult` is a namedtuple, so it's simple to access the parts you want.\n    >>> ext = tldextract.extract('http://forums.bbc.co.uk')\n    >>> (ext.subdomain, ext.domain, ext.suffix)\n    ('forums', 'bbc', 'co.uk')\n    >>> # rejoin subdomain and domain\n    >>> '.'.join(ext[:2])\n    'forums.bbc'\n    >>> # a common alias\n    >>> ext.registered_domain\n    'bbc.co.uk'\nNote subdomain and suffix are _optional_. Not all URL-like inputs have a\nsubdomain or a valid suffix.\n    >>> tldextract.extract('google.com')\n    ExtractResult(subdomain='', domain='google', suffix='com')\n    >>> tldextract.extract('google.notavalidsuffix')\n    ExtractResult(subdomain='google', domain='notavalidsuffix', suffix='')\n    >>> tldextract.extract('http://127.0.0.1:8080/deployed/')\n    ExtractResult(subdomain='', domain='127.0.0.1', suffix='')\nIf you want to rejoin the whole namedtuple, regardless of whether a subdomain\nor suffix were found:\n    >>> ext = tldextract.extract('http://127.0.0.1:8080/deployed/')\n    >>> # this has unwanted dots\n    >>> '.'.join(ext)\n    '.127.0.0.1.'\n    >>> # join part only if truthy\n    >>> '.'.join(part for part in ext if part)\n    '127.0.0.1'\n\"\"\"\n\nimport collections\nimport logging\nimport os\nfrom functools import wraps\n\nimport idna\n\n\"\"\"Helpers \"\"\"\nimport errno\nimport hashlib\nimport json\nimport logging\nimport os\nimport os.path\nimport sys\nfrom hashlib import md5\n\nfrom filelock import FileLock\n\nLOG = logging.getLogger(__name__)\n\n_DID_LOG_UNABLE_TO_CACHE = False\n\n\ndef get_pkg_unique_identifier():\n    \"\"\"\n    Generate an identifier unique to the python version, tldextract version, and python instance\n\n    This will prevent interference between virtualenvs and issues that might arise when installing\n    a new version of tldextract\n    \"\"\"\n    try:\n        # pylint: disable=import-outside-toplevel\n        from tldextract._version import version\n    except ImportError:\n        version = \"dev\"\n\n    tldextract_version = \"tldextract-\" + version\n    python_env_name = os.path.basename(sys.prefix)\n    # just to handle the edge case of two identically named python environments\n    python_binary_path_short_hash = hashlib.md5(sys.prefix.encode(\"utf-8\")).hexdigest()[:6]\n    python_version = \".\".join([str(v) for v in sys.version_info[:-1]])\n    identifier_parts = [\n        python_version,\n        python_env_name,\n        python_binary_path_short_hash,\n        tldextract_version\n    ]\n    pkg_identifier = \"__\".join(identifier_parts)\n\n    return pkg_identifier\n\n\ndef get_cache_dir():\n    \"\"\"\n    Get a cache dir that we have permission to write to\n\n    Try to follow the XDG standard, but if that doesn't work fallback to the package directory\n    http://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html\n    \"\"\"\n    cache_dir = os.environ.get(\"TLDEXTRACT_CACHE\", None)\n    if cache_dir is not None:\n        return cache_dir\n\n    xdg_cache_home = os.getenv(\"XDG_CACHE_HOME\", None)\n    if xdg_cache_home is None:\n        user_home = os.getenv(\"HOME\", None)\n        if user_home:\n            xdg_cache_home = os.path.join(user_home, \".cache\")\n\n    if xdg_cache_home is not None:\n        return os.path.join(xdg_cache_home, \"python-tldextract\", get_pkg_unique_identifier())\n\n    # fallback to trying to use package directory itself\n    return os.path.join(os.path.dirname(__file__), \".suffix_cache/\")\n\n\nclass DiskCache:\n    \"\"\"Disk _cache that only works for jsonable values\"\"\"\n\n    def __init__(self, cache_dir, lock_timeout=20):\n        self.enabled = bool(cache_dir)\n        self.cache_dir = os.path.expanduser(str(cache_dir) or \"\")\n        self.lock_timeout = lock_timeout\n        # using a unique extension provides some safety that an incorrectly set cache_dir\n        # combined with a call to `.clear()` wont wipe someones hard drive\n        self.file_ext = \".tldextract.json\"\n\n    def get(self, namespace, key):\n        \"\"\"Retrieve a value from the disk cache\"\"\"\n        if not self.enabled:\n            raise KeyError(\"Cache is disabled\")\n        cache_filepath = self._key_to_cachefile_path(namespace, key)\n\n        if not os.path.isfile(cache_filepath):\n            raise KeyError(\"namespace: \" + namespace + \" key: \" + repr(key))\n        try:\n            with open(cache_filepath) as cache_file:\n                return json.load(cache_file)\n        except (OSError, ValueError) as exc:\n            LOG.error(\"error reading TLD cache file %s: %s\", cache_filepath, exc)\n            raise KeyError(  # pylint: disable=raise-missing-from\n                \"namespace: \" + namespace + \" key: \" + repr(key)\n            )\n\n    def set(self, namespace, key, value):\n        \"\"\"Set a value in the disk cache\"\"\"\n        if not self.enabled:\n            return False\n        cache_filepath = self._key_to_cachefile_path(namespace, key)\n\n        try:\n            _make_dir(cache_filepath)\n            with open(cache_filepath, \"w\") as cache_file:\n                json.dump(value, cache_file)\n        except OSError as ioe:\n            global _DID_LOG_UNABLE_TO_CACHE  # pylint: disable=global-statement\n            if not _DID_LOG_UNABLE_TO_CACHE:\n                LOG.warning(\n                    (\n                        \"unable to cache %s.%s in %s. This could refresh the \"\n                        \"Public Suffix List over HTTP every app startup. \"\n                        \"Construct your `TLDExtract` with a writable `cache_dir` or \"\n                        \"set `cache_dir=False` to silence this warning. %s\"\n                    ),\n                    namespace,\n                    key,\n                    cache_filepath,\n                    ioe,\n                )\n                _DID_LOG_UNABLE_TO_CACHE = True\n\n        return None\n\n    def clear(self):\n        \"\"\"Clear the disk cache\"\"\"\n        for root, _, files in os.walk(self.cache_dir):\n            for filename in files:\n                if filename.endswith(self.file_ext) or filename.endswith(\n                    self.file_ext + \".lock\"\n                ):\n                    try:\n                        os.unlink(os.path.join(root, filename))\n                    except FileNotFoundError:\n                        pass\n                    except OSError as exc:\n                        # errno.ENOENT == \"No such file or directory\"\n                        # https://docs.python.org/2/library/errno.html#errno.ENOENT\n                        if exc.errno != errno.ENOENT:\n                            raise\n\n    def _key_to_cachefile_path(self, namespace, key):\n        namespace_path = os.path.join(self.cache_dir, namespace)\n        hashed_key = _make_cache_key(key)\n\n        cache_path = os.path.join(namespace_path, hashed_key + self.file_ext)\n\n        return cache_path\n\n    def run_and_cache(self, func, namespace, kwargs, hashed_argnames):\n        \"\"\"Get a url but cache the response\"\"\"\n        if not self.enabled:\n            return func(**kwargs)\n\n        key_args = {k: v for k, v in kwargs.items() if k in hashed_argnames}\n        cache_filepath = self._key_to_cachefile_path(namespace, key_args)\n        lock_path = cache_filepath + \".lock\"\n        try:\n            _make_dir(cache_filepath)\n        except OSError as ioe:\n            global _DID_LOG_UNABLE_TO_CACHE  # pylint: disable=global-statement\n            if not _DID_LOG_UNABLE_TO_CACHE:\n                LOG.warning(\n                    (\n                        \"unable to cache %s.%s in %s. This could refresh the \"\n                        \"Public Suffix List over HTTP every app startup. \"\n                        \"Construct your `TLDExtract` with a writable `cache_dir` or \"\n                        \"set `cache_dir=False` to silence this warning. %s\"\n                    ),\n                    namespace,\n                    key_args,\n                    cache_filepath,\n                    ioe,\n                )\n                _DID_LOG_UNABLE_TO_CACHE = True\n\n            return func(**kwargs)\n\n        with FileLock(lock_path, timeout=self.lock_timeout):\n            try:\n                result = self.get(namespace=namespace, key=key_args)\n            except KeyError:\n                result = func(**kwargs)\n                self.set(namespace=\"urls\", key=key_args, value=result)\n\n            return result\n\n    def cached_fetch_url(self, session, url, timeout):\n        \"\"\"Get a url but cache the response\"\"\"\n        return self.run_and_cache(\n            func=_fetch_url,\n            namespace=\"urls\",\n            kwargs={\"session\": session, \"url\": url, \"timeout\": timeout},\n            hashed_argnames=[\"url\"],\n        )\n\n\ndef _fetch_url(session, url, timeout):\n\n    response = session.get(url, timeout=timeout)\n    response.raise_for_status()\n    text = response.text\n\n    if not isinstance(text, str):\n        text = str(text, \"utf-8\")\n\n    return text\n\n\ndef _make_cache_key(inputs):\n    key = repr(inputs)\n    try:\n        key = md5(key).hexdigest()\n    except TypeError:\n        key = md5(key.encode(\"utf8\")).hexdigest()\n    return key\n\n\ndef _make_dir(filename):\n    \"\"\"Make a directory if it doesn't already exist\"\"\"\n    if not os.path.exists(os.path.dirname(filename)):\n        try:\n            os.makedirs(os.path.dirname(filename))\n        except OSError as exc:  # Guard against race condition\n            if exc.errno != errno.EEXIST:\n                raise\n'tldextract helpers for testing and fetching remote resources.'\n\nimport re\nimport socket\n\nfrom urllib.parse import scheme_chars\n\n\nIP_RE = re.compile(\n    r'^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$')  # pylint: disable=line-too-long\n\nSCHEME_RE = re.compile(r'^([' + scheme_chars + ']+:)?//')\n\n\ndef looks_like_ip(maybe_ip):\n    \"\"\"Does the given str look like an IP address?\"\"\"\n    if not maybe_ip[0].isdigit():\n        return False\n\n    try:\n        socket.inet_aton(maybe_ip)\n        return True\n    except (AttributeError, UnicodeError):\n        if IP_RE.match(maybe_ip):\n            return True\n    except socket.error:\n        return False\n\"tldextract helpers for testing and fetching remote resources.\"\n\nimport logging\nimport pkgutil\nimport re\n\nimport requests\nfrom requests_file import FileAdapter\n\nLOG = logging.getLogger(\"tldextract\")\n\nPUBLIC_SUFFIX_RE = re.compile(r\"^(?P<suffix>[.*!]*\\w[\\S]*)\", re.UNICODE | re.MULTILINE)\nPUBLIC_PRIVATE_SUFFIX_SEPARATOR = \"// ===BEGIN PRIVATE DOMAINS===\"\n\n\nclass SuffixListNotFound(LookupError):\n    \"\"\"A recoverable error while looking up a suffix list. Recoverable because\n    you can specify backups, or use this library's bundled snapshot.\"\"\"\n\n\ndef find_first_response(cache, urls, cache_fetch_timeout=None):\n    \"\"\"Decode the first successfully fetched URL, from UTF-8 encoding to\n    Python unicode.\n    \"\"\"\n    with requests.Session() as session:\n        session.mount(\"file://\", FileAdapter())\n\n        for url in urls:\n            try:\n                return cache.cached_fetch_url(\n                    session=session, url=url, timeout=cache_fetch_timeout\n                )\n            except requests.exceptions.RequestException:\n                LOG.exception(\"Exception reading Public Suffix List url %s\", url)\n    raise SuffixListNotFound(\n        \"No Public Suffix List found. Consider using a mirror or constructing \"\n        \"your TLDExtract with `suffix_list_urls=None`.\"\n    )\n\n\ndef extract_tlds_from_suffix_list(suffix_list_text):\n    \"\"\"Parse the raw suffix list text for its different designations of\n    suffixes.\"\"\"\n    public_text, _, private_text = suffix_list_text.partition(\n        PUBLIC_PRIVATE_SUFFIX_SEPARATOR\n    )\n\n    public_tlds = [m.group(\"suffix\") for m in PUBLIC_SUFFIX_RE.finditer(public_text)]\n    private_tlds = [m.group(\"suffix\") for m in PUBLIC_SUFFIX_RE.finditer(private_text)]\n    return public_tlds, private_tlds\n\n\ndef get_suffix_lists(cache, urls, cache_fetch_timeout, fallback_to_snapshot):\n    \"\"\"Fetch, parse, and cache the suffix lists\"\"\"\n    return cache.run_and_cache(\n        func=_get_suffix_lists,\n        namespace=\"publicsuffix.org-tlds\",\n        kwargs={\n            \"cache\": cache,\n            \"urls\": urls,\n            \"cache_fetch_timeout\": cache_fetch_timeout,\n            \"fallback_to_snapshot\": fallback_to_snapshot,\n        },\n        hashed_argnames=[\"urls\", \"fallback_to_snapshot\"],\n    )\n\n\ndef _get_suffix_lists(cache, urls, cache_fetch_timeout, fallback_to_snapshot):\n    \"\"\"Fetch, parse, and cache the suffix lists\"\"\"\n\n    try:\n        text = find_first_response(cache, urls, cache_fetch_timeout=cache_fetch_timeout)\n    except SuffixListNotFound as exc:\n        if fallback_to_snapshot:\n            text = pkgutil.get_data(\"tldextract\", \".tld_set_snapshot\")\n            if not isinstance(text, str):\n                text = str(text, \"utf-8\")\n        else:\n            raise exc\n\n    public_tlds, private_tlds = extract_tlds_from_suffix_list(text)\n\n    return public_tlds, private_tlds\n\nLOG = logging.getLogger(\"tldextract\")\n\n\nCACHE_TIMEOUT = os.environ.get(\"TLDEXTRACT_CACHE_TIMEOUT\")\n\nPUBLIC_SUFFIX_LIST_URLS = (\n    \"https://publicsuffix.org/list/public_suffix_list.dat\",\n    \"https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat\",\n)\n\n\nclass ExtractResult(collections.namedtuple(\"ExtractResult\", \"subdomain domain suffix\")):\n    \"\"\"namedtuple of a URL's subdomain, domain, and suffix.\"\"\"\n\n    # Necessary for __dict__ member to get populated in Python 3+\n    __slots__ = ()\n\n    @property\n    def registered_domain(self):\n        \"\"\"\n        Joins the domain and suffix fields with a dot, if they're both set.\n        >>> extract('http://forums.bbc.co.uk').registered_domain\n        'bbc.co.uk'\n        >>> extract('http://localhost:8080').registered_domain\n        ''\n        \"\"\"\n        if self.domain and self.suffix:\n            return self.domain + \".\" + self.suffix\n        return \"\"\n\n    @property\n    def fqdn(self):\n        \"\"\"\n        Returns a Fully Qualified Domain Name, if there is a proper domain/suffix.\n        >>> extract('http://forums.bbc.co.uk/path/to/file').fqdn\n        'forums.bbc.co.uk'\n        >>> extract('http://localhost:8080').fqdn\n        ''\n        \"\"\"\n        if self.domain and self.suffix:\n            # self is the namedtuple (subdomain domain suffix)\n            return \".\".join(i for i in self if i)\n        return \"\"\n\n    @property\n    def ipv4(self):\n        \"\"\"\n        Returns the ipv4 if that is what the presented domain/url is\n        >>> extract('http://127.0.0.1/path/to/file').ipv4\n        '127.0.0.1'\n        >>> extract('http://127.0.0.1.1/path/to/file').ipv4\n        ''\n        >>> extract('http://256.1.1.1').ipv4\n        ''\n        \"\"\"\n        if not (self.suffix or self.subdomain) and IP_RE.match(self.domain):\n            return self.domain\n        return \"\"\n\n\nclass TLDExtract:\n    \"\"\"A callable for extracting, subdomain, domain, and suffix components from\n    a URL.\"\"\"\n\n    # TODO: Agreed with Pylint: too-many-arguments\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        cache_dir=get_cache_dir(),\n        suffix_list_urls=PUBLIC_SUFFIX_LIST_URLS,\n        fallback_to_snapshot=True,\n        include_psl_private_domains=False,\n        extra_suffixes=(),\n        cache_fetch_timeout=CACHE_TIMEOUT,\n    ):\n        \"\"\"\n        Constructs a callable for extracting subdomain, domain, and suffix\n        components from a URL.\n        Upon calling it, it first checks for a JSON in `cache_dir`.\n        By default, the `cache_dir` will live in the tldextract directory.\n        You can disable the caching functionality of this module  by setting `cache_dir` to False.\n        If the cached version does not exist (such as on the first run), HTTP request the URLs in\n        `suffix_list_urls` in order, until one returns public suffix list data. To disable HTTP\n        requests, set this to something falsy.\n        The default list of URLs point to the latest version of the Mozilla Public Suffix List and\n        its mirror, but any similar document could be specified. Local files can be specified by\n        using the `file://` protocol. (See `urllib2` documentation.)\n        If there is no cached version loaded and no data is found from the `suffix_list_urls`,\n        the module will fall back to the included TLD set snapshot. If you do not want\n        this behavior, you may set `fallback_to_snapshot` to False, and an exception will be\n        raised instead.\n        The Public Suffix List includes a list of \"private domains\" as TLDs,\n        such as blogspot.com. These do not fit `tldextract`'s definition of a\n        suffix, so these domains are excluded by default. If you'd like them\n        included instead, set `include_psl_private_domains` to True.\n        You can pass additional suffixes in `extra_suffixes` argument without changing list URL\n        cache_fetch_timeout is passed unmodified to the underlying request object\n        per the requests documentation here:\n        http://docs.python-requests.org/en/master/user/advanced/#timeouts\n        cache_fetch_timeout can also be set to a single value with the\n        environment variable TLDEXTRACT_CACHE_TIMEOUT, like so:\n        TLDEXTRACT_CACHE_TIMEOUT=\"1.2\"\n        When set this way, the same timeout value will be used for both connect\n        and read timeouts\n        \"\"\"\n        suffix_list_urls = suffix_list_urls or ()\n        self.suffix_list_urls = tuple(\n            url.strip() for url in suffix_list_urls if url.strip()\n        )\n\n        self.fallback_to_snapshot = fallback_to_snapshot\n        if not (self.suffix_list_urls or cache_dir or self.fallback_to_snapshot):\n            raise ValueError(\n                \"The arguments you have provided disable all ways for tldextract \"\n                \"to obtain data. Please provide a suffix list data, a cache_dir, \"\n                \"or set `fallback_to_snapshot` to `True`.\"\n            )\n\n        self.include_psl_private_domains = include_psl_private_domains\n        self.extra_suffixes = extra_suffixes\n        self._extractor = None\n\n        self.cache_fetch_timeout = cache_fetch_timeout\n        self._cache = DiskCache(cache_dir)\n        if isinstance(self.cache_fetch_timeout, str):\n            self.cache_fetch_timeout = float(self.cache_fetch_timeout)\n\n    def __call__(self, url, include_psl_private_domains=None):\n        \"\"\"\n        Takes a string URL and splits it into its subdomain, domain, and\n        suffix (effective TLD, gTLD, ccTLD, etc.) component.\n        >>> extract = TLDExtract()\n        >>> extract('http://forums.news.cnn.com/')\n        ExtractResult(subdomain='forums.news', domain='cnn', suffix='com')\n        >>> extract('http://forums.bbc.co.uk/')\n        ExtractResult(subdomain='forums', domain='bbc', suffix='co.uk')\n        \"\"\"\n\n        netloc = (\n            SCHEME_RE.sub(\"\", url)\n            .partition(\"/\")[0]\n            .partition(\"?\")[0]\n            .partition(\"#\")[0]\n            .split(\"@\")[-1]\n            .partition(\":\")[0]\n            .strip()\n            .rstrip(\".\")\n        )\n\n        labels = netloc.split(\".\")\n\n        translations = [_decode_punycode(label) for label in labels]\n        suffix_index = self._get_tld_extractor().suffix_index(\n            translations, include_psl_private_domains=include_psl_private_domains\n        )\n\n        suffix = \".\".join(labels[suffix_index:])\n        if not suffix and netloc and looks_like_ip(netloc):\n            return ExtractResult(\"\", netloc, \"\")\n\n        subdomain = \".\".join(labels[: suffix_index - 1]) if suffix_index else \"\"\n        domain = labels[suffix_index - 1] if suffix_index else \"\"\n        return ExtractResult(subdomain, domain, suffix)\n\n    def update(self, fetch_now=False):\n        \"\"\"Force fetch the latest suffix list definitions.\"\"\"\n        self._extractor = None\n        self._cache.clear()\n        if fetch_now:\n            self._get_tld_extractor()\n\n    @property\n    def tlds(self):\n        \"\"\"\n        Returns the list of tld's used by default\n        This will vary based on `include_psl_private_domains` and `extra_suffixes`\n        \"\"\"\n        return list(self._get_tld_extractor().tlds())\n\n    def _get_tld_extractor(self):\n        \"\"\"Get or compute this object's TLDExtractor. Looks up the TLDExtractor\n        in roughly the following order, based on the settings passed to\n        __init__:\n        1. Memoized on `self`\n        2. Local system _cache file\n        3. Remote PSL, over HTTP\n        4. Bundled PSL snapshot file\"\"\"\n\n        if self._extractor:\n            return self._extractor\n\n        public_tlds, private_tlds = get_suffix_lists(\n            cache=self._cache,\n            urls=self.suffix_list_urls,\n            cache_fetch_timeout=self.cache_fetch_timeout,\n            fallback_to_snapshot=self.fallback_to_snapshot,\n        )\n\n        if not any([public_tlds, private_tlds, self.extra_suffixes]):\n            raise ValueError(\"No tlds set. Cannot proceed without tlds.\")\n\n        self._extractor = _PublicSuffixListTLDExtractor(\n            public_tlds=public_tlds,\n            private_tlds=private_tlds,\n            extra_tlds=list(self.extra_suffixes),\n            include_psl_private_domains=self.include_psl_private_domains,\n        )\n        return self._extractor\n\n\nTLD_EXTRACTOR = TLDExtract()\n\n\n@wraps(TLD_EXTRACTOR.__call__)\ndef extract(\n    url, include_psl_private_domains=False\n):  # pylint: disable=missing-function-docstring\n    return TLD_EXTRACTOR(url, include_psl_private_domains=include_psl_private_domains)\n\n\n@wraps(TLD_EXTRACTOR.update)\ndef update(*args, **kwargs):  # pylint: disable=missing-function-docstring\n    return TLD_EXTRACTOR.update(*args, **kwargs)\n\n\nclass _PublicSuffixListTLDExtractor:\n    \"\"\"Wrapper around this project's main algo for PSL\n    lookups.\n    \"\"\"\n\n    def __init__(\n        self, public_tlds, private_tlds, extra_tlds, include_psl_private_domains=False\n    ):\n        # set the default value\n        self.include_psl_private_domains = include_psl_private_domains\n        self.public_tlds = public_tlds\n        self.private_tlds = private_tlds\n        self.tlds_incl_private = frozenset(public_tlds + private_tlds + extra_tlds)\n        self.tlds_excl_private = frozenset(public_tlds + extra_tlds)\n\n    def tlds(self, include_psl_private_domains=None):\n        \"\"\"Get the currently filtered list of suffixes.\"\"\"\n        if include_psl_private_domains is None:\n            include_psl_private_domains = self.include_psl_private_domains\n\n        return (\n            self.tlds_incl_private\n            if include_psl_private_domains\n            else self.tlds_excl_private\n        )\n\n    def suffix_index(self, lower_spl, include_psl_private_domains=None):\n        \"\"\"Returns the index of the first suffix label.\n        Returns len(spl) if no suffix is found\n        \"\"\"\n        tlds = self.tlds(include_psl_private_domains)\n        length = len(lower_spl)\n        for i in range(length):\n            maybe_tld = \".\".join(lower_spl[i:])\n            exception_tld = \"!\" + maybe_tld\n            if exception_tld in tlds:\n                return i + 1\n\n            if maybe_tld in tlds:\n                return i\n\n            wildcard_tld = \"*.\" + \".\".join(lower_spl[i + 1 :])\n            if wildcard_tld in tlds:\n                return i\n\n        return length\n\n\ndef _decode_punycode(label):\n    lowered = label.lower()\n    looks_like_puny = lowered.startswith(\"xn--\")\n    if looks_like_puny:\n        try:\n            return idna.decode(label.encode(\"ascii\")).lower()\n        except (UnicodeError, IndexError):\n            pass\n    return lowered","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing Packages\nimport pandas as pd\nimport numpy as np\nimport spacy\nimport sys\nsys.path = [\n    '../input/readability-package',\n] + sys.path\nimport readability\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\nfrom urllib.parse import urlparse\nimport re\nfrom tldextract import extract\n\nfrom sklearn import metrics, preprocessing, model_selection\nimport lightgbm as lgb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading Data\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Readability feature","metadata":{}},{"cell_type":"code","source":"# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef readability_measurements(passage: str):\n    \"\"\"\n    This function uses the readability library for feature engineering.\n    It includes textual statistics, readability scales and metric, and some pos stats\n    \"\"\"\n    results = readability.getmeasures(passage, lang='en')\n    \n    chars_per_word = results['sentence info']['characters_per_word']\n    syll_per_word = results['sentence info']['syll_per_word']\n    words_per_sent = results['sentence info']['words_per_sentence']\n    \n    kincaid = results['readability grades']['Kincaid']\n    ari = results['readability grades']['ARI']\n    coleman_liau = results['readability grades']['Coleman-Liau']\n    flesch = results['readability grades']['FleschReadingEase']\n    gunning_fog = results['readability grades']['GunningFogIndex']\n    lix = results['readability grades']['LIX']\n    smog = results['readability grades']['SMOGIndex']\n    rix = results['readability grades']['RIX']\n    dale_chall = results['readability grades']['DaleChallIndex']\n    \n    tobeverb = results['word usage']['tobeverb']\n    auxverb = results['word usage']['auxverb']\n    conjunction = results['word usage']['conjunction']\n    pronoun = results['word usage']['pronoun']\n    preposition = results['word usage']['preposition']\n    nominalization = results['word usage']['nominalization']\n    \n    pronoun_b = results['sentence beginnings']['pronoun']\n    interrogative = results['sentence beginnings']['interrogative']\n    article = results['sentence beginnings']['article']\n    subordination = results['sentence beginnings']['subordination']\n    conjunction_b = results['sentence beginnings']['conjunction']\n    preposition_b = results['sentence beginnings']['preposition']\n\n    \n    return [chars_per_word, syll_per_word, words_per_sent,\n            kincaid, ari, coleman_liau, flesch, gunning_fog, lix, smog, rix, dale_chall,\n            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## spacy feature","metadata":{}},{"cell_type":"code","source":"# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef spacy_features(df: pd.DataFrame):\n    \"\"\"\n    This function generates features using spacy en_core_wb_lg\n    I learned about this from these resources:\n    https://www.kaggle.com/konradb/linear-baseline-with-cv\n    https://www.kaggle.com/anaverageengineer/comlrp-baseline-for-complete-beginners\n    \"\"\"\n    \n    nlp = spacy.load('en_core_web_lg')\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in df.excerpt])\n        \n    return vectors\n\ndef get_spacy_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n        \n    return names","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## pos tag features","metadata":{}},{"cell_type":"code","source":"# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef pos_tag_features(passage: str):\n    \"\"\"\n    This function counts the number of times different parts of speech occur in an excerpt\n    \"\"\"\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    \n    tags = pos_tag(word_tokenize(passage))\n    tag_list= list()\n    \n    for tag in pos_tags:\n        tag_list.append(len([i[0] for i in tags if i[1] == tag]))\n    \n    return tag_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Others","metadata":{}},{"cell_type":"code","source":"# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef generate_other_features(passage: str):\n    \"\"\"\n    This function is where I test miscellaneous features\n    This is experimental\n    \"\"\"\n    # punctuation count\n    periods = passage.count(\".\")\n    commas = passage.count(\",\")\n    semis = passage.count(\";\")\n    exclaims = passage.count(\"!\")\n    questions = passage.count(\"?\")\n    \n    # Some other stats\n    num_char = len(passage)\n    num_words = len(passage.split(\" \"))\n    unique_words = len(set(passage.split(\" \") ))\n    word_diversity = unique_words/num_words\n    \n    word_len = [len(w) for w in passage.split(\" \")]\n    longest_word = np.max(word_len)\n    avg_len_word = np.mean(word_len)\n    \n    return [periods, commas, semis, exclaims, questions,\n            num_char, num_words, unique_words, word_diversity,\n            longest_word, avg_len_word]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combine All","metadata":{}},{"cell_type":"code","source":"def extract_features(df):\n\n    scores_df = pd.DataFrame(df[\"excerpt\"].apply(lambda p : readability_measurements(p)).tolist(), \n                                 columns=[\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n                                          \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"])\n    df = pd.merge(df, scores_df, left_index=True, right_index=True)\n    \n    spacy_df = pd.DataFrame(spacy_features(df), columns=get_spacy_col_names())\n    df = pd.merge(df, spacy_df, left_index=True, right_index=True)\n    \n    pos_df = pd.DataFrame(df[\"excerpt\"].apply(lambda p : pos_tag_features(p)).tolist(),\n                            columns=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                                    \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                                    \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"])\n    df = pd.merge(df, pos_df, left_index=True, right_index=True)\n    \n    other_df = pd.DataFrame(df[\"excerpt\"].apply(lambda p : generate_other_features(p)).tolist(),\n                            columns=[\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                        \"num_char\", \"num_words\", \"unique_words\", \"word_diversity\",\n                                        \"longest_word\", \"avg_len_word\"])\n    df = pd.merge(df, other_df, left_index=True, right_index=True)\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"extract_features(test)","metadata":{}},{"cell_type":"markdown","source":"## URL Features","metadata":{}},{"cell_type":"code","source":"from urllib.parse import urlparse\nimport re\nfrom tldextract import extract","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_url_license_feat(df):\n    \n    temp = pd.DataFrame()\n#     temp['path'] = df['url_legal'].apply(lambda x : x if x is np.nan else urlparse(x).path)\n    temp['article_year'] = df['url_legal'].apply(lambda x : x if x is np.nan else re.search('(2\\d{3})|$', urlparse(x).path).group())\n    temp['subdomain'] = df['url_legal'].apply(lambda x : x if x is np.nan else extract(x)[0])\n    temp['domain'] = df['url_legal'].apply(lambda x : x if x is np.nan else extract(x)[1])\n    temp['suffix'] = df['url_legal'].apply(lambda x : x if x is np.nan else extract(x)[2])\n    temp['is_pdf'] = df['url_legal'].apply(lambda x : 1 if '.pdf' in str(x) else 0)\n    \n    temp['is_cc'] = df['license'].apply(lambda x : 1 if 'CC' in str(x) else 0)\n    temp['is_by'] = df['license'].apply(lambda x : 1 if 'BY' in str(x) else 0)\n    temp['is_sa'] = df['license'].apply(lambda x : 1 if 'SA' in str(x) else 0)\n    temp['is_nc'] = df['license'].apply(lambda x : 1 if 'NC' in str(x) else 0)\n    temp['is_nd'] = df['license'].apply(lambda x : 1 if 'ND' in str(x) else 0)\n    temp['is_gnu'] = df['license'].apply(lambda x : 1 if 'GNU' in str(x) else 0)\n    temp['license_version'] = df['license'].apply(lambda x : x if x is np.nan else re.search('([0-9][.][0-9])|$', urlparse(x).path).group())\n    \n    df = pd.concat([df, temp], axis = 1)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract Features","metadata":{}},{"cell_type":"code","source":"train_feat = extract_features(train)\ntrain_feat = extract_url_license_feat(train_feat)\ntrain_feat.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feat = extract_features(test)\ntest_feat = extract_url_license_feat(test_feat)\ntest_feat.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train_feat = pd.read_csv('../input/features/train_feat.csv')\ntest_feat = pd.read_csv('../input/features/test_feat.csv')","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ignore_cols = ['id','url_legal','license','excerpt', 'standard_error', 'target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in train_feat.select_dtypes('object').columns.tolist():\n    if col not in ignore_cols:\n        train_feat[col] = train_feat[col].replace(np.nan, '')\n        test_feat[col] = test_feat[col].replace(np.nan, '')\n        lbl = LabelEncoder()\n        train_feat[col] = lbl.fit_transform(train_feat[col])\n        test_feat[col] = lbl.transform(test_feat[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feat.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_feat[[i for i in train_feat.columns if i not in ignore_cols]]\ny_train = train_feat['target']\ntest_X = test_feat[[i for i in test_feat.columns if i not in ignore_cols]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[i for i in train_feat.columns if i not in test_feat.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(test_X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standard Error Weighting","metadata":{}},{"cell_type":"code","source":"std_error = train_feat['standard_error']\nstd_error = 1/std_error\nstd_error = std_error.replace(np.inf, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nstd_error = scaler.fit_transform(std_error.values.reshape(-1, 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"std_error = std_error.reshape(-1)\nstd_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Build","metadata":{}},{"cell_type":"markdown","source":"### LighGBM Regression","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics, preprocessing, model_selection\nimport lightgbm as lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def runLGB_reg(train_X, train_y, test_X, sample_weight, test_y=None, test_X2=None, dep=8, seed=0, data_leaf=50, rounds=20000):\n    params = {}\n    params[\"objective\"] = \"regression\"\n    params['metric'] = 'rmse'\n    params[\"max_depth\"] = dep\n    params[\"num_leaves\"] = 30\n    params[\"min_data_in_leaf\"] = data_leaf\n    #     params[\"min_sum_hessian_in_leaf\"] = 50\n    params[\"learning_rate\"] = 0.01\n    params[\"bagging_fraction\"] = 0.8\n    params[\"feature_fraction\"] = 0.2\n    params[\"feature_fraction_seed\"] = seed\n    params[\"bagging_freq\"] = 1\n    params[\"bagging_seed\"] = seed\n    params[\"lambda_l2\"] = 3\n    params[\"lambda_l1\"] = 3\n    params[\"verbosity\"] = -1\n    print(sample_weight)\n#     params[\"sample_weight\"] = sample_weight\n    num_rounds = rounds\n\n    plst = list(params.items())\n    lgtrain = lgb.Dataset(train_X, label=train_y)\n\n    if test_y is not None:\n        lgtest = lgb.Dataset(test_X, label=test_y)\n        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=200, verbose_eval=500)\n\n    #         model = lgb.LGBMRegressor()\n    else:\n        lgtest = lgb.DMatrix(test_X)\n        model = lgb.train(params, lgtrain, num_rounds)\n\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    #imps = model.feature_importance()\n    #names = model.feature_name()\n    #for fi, fn in enumerate(names):\n    #    print(fn, imps[fi])\n\n    loss = 0\n    if test_y is not None:\n        loss = np.sqrt(metrics.mean_squared_error(test_y, pred_test_y))\n        print(loss)\n        return model, loss, pred_test_y, pred_test_y2\n    else:\n        return model, loss, pred_test_y, pred_test_y2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Building model..\")\ncv_scores = []\npred_test_full = 0\npred_train = np.zeros(X_train.shape[0])\nn_splits = 5\nkf = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=7988)\n# gkf = model_selection.GroupKFold(n_splits=n_splits)\nmodel_name = \"lgb\"\nfor dev_index, val_index in kf.split(X_train, y_train):\n    dev_X, val_X = X_train.iloc[dev_index,:], X_train.iloc[val_index,:]\n    dev_y, val_y = y_train[dev_index], y_train[val_index]\n    std_error_x = std_error[dev_index]\n\n    pred_val = 0\n    pred_test = 0\n    n_models = 0.\n\n    model, loss, pred_v, pred_t = runLGB_reg(dev_X, dev_y, val_X, std_error_x, val_y, test_X, dep=6, data_leaf=200, seed=2019)\n    pred_val += pred_v\n    pred_test += pred_t\n    n_models += 1\n    \n    model, loss, pred_v, pred_t = runLGB_reg(dev_X, dev_y, val_X, std_error_x, val_y, test_X,  dep=7, data_leaf=180, seed=9873)\n    pred_val += pred_v\n    pred_test += pred_t\n    n_models += 1\n    \n#     model, loss, pred_v, pred_t = runLGB(dev_X, dev_y, val_X, val_y, test_X, dep=7, data_leaf=200, seed=4568)\n#     pred_val += pred_v\n#     pred_test += pred_t\n#     n_models += 1\n    \n    \n    pred_val /= n_models\n    pred_test /= n_models\n    \n    loss = np.sqrt(metrics.mean_squared_error(val_y, pred_val))\n        \n    pred_train[val_index] = pred_val\n    pred_test_full += pred_test / n_splits\n    cv_scores.append(loss)\n    print(cv_scores)\n#     break\nprint(np.mean(cv_scores))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'id': test.id, 'target': pred_test})\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}