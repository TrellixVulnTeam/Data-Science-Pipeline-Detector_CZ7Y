{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport gc\n\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n                                      ReduceLROnPlateau)\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification,get_constant_schedule_with_warmup)\nfrom transformers import PreTrainedModel, RobertaTokenizerFast, RobertaConfig, RobertaModel, AdamW\n\nscaler = torch.cuda.amp.GradScaler()\n\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Configs/Constants\n\nclass config:\n    \n    SEED = 42\n    MAX_LEN = 256\n    TRAIN_BATCH_SIZE = 16\n    VAL_BATCH_SIZE = 64\n    ROBERTA_MODEL_PATH = '../input/roberta-base'\n    EPOCHS = 3\n    LR = 1e-5\n    TEXT_COLUMN = 'excerpt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 0):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nrandom_state = set_seed(config.SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_kfolds(df,target_col, seed):\n\n    df[\"kfold\"] = -1\n\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=seed)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=df)):\n        print(len(train_idx), len(val_idx))\n        df.loc[val_idx, 'kfold'] = fold\n\n    return df\n\ndef create_Stratkfolds(df,target_col, seed):\n\n    df[\"kfold\"] = -1\n\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    ### This was taken from https://www.kaggle.com/abhishek/step-1-create-folds\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(df))))\n    \n    # bin targets\n    df.loc[:, \"bins\"] = pd.cut(\n        df[target_col], bins=num_bins, labels=False\n    )\n\n    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y = df.bins.values)):\n        print(len(train_idx), len(val_idx))\n        df.loc[val_idx, 'kfold'] = fold\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading Data\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\ntarget = train['target'].to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model with classifier layers on top of RoBERTa\nclass ReadabilityModel(torch.nn.Module):\n    def __init__(self, conf, dropout_rate=0.3):\n        super(ReadabilityModel, self).__init__()\n        \n        self.roberta = RobertaModel.from_pretrained(config.ROBERTA_MODEL_PATH, config = conf)\n        self.dropout=nn.Dropout(dropout_rate)\n        self.linear=nn.Linear(768,1)\n        \n    def forward(self, ids, mask):\n        output1 = self.roberta(input_ids=ids, attention_mask=mask)\n        output1 = output1.hidden_states\n        output1 = output1[-1]\n        xlnet_output=self.dropout(output1)\n        \n        out = torch.mean(xlnet_output, 1, False)\n        final_output=self.linear(out)\n        final_outputs = final_output.squeeze(-1).squeeze(-1)\n        \n        return final_outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear GPU Memory Unused\n\n# gc.collect()\ntorch.cuda.empty_cache()\n# torch.cuda.clear_memory_allocated()  # entirely clear all allocated memory","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = config.ROBERTA_MODEL_PATH\ntokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n\nmodel_config = RobertaConfig.from_pretrained(model_name)\nmodel_config.output_hidden_states = True\n\nmodel = ReadabilityModel(conf = model_config ,dropout_rate=0.4)\nmodel = model.to(device)\n\noptimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=0.01)\nscheduler = get_constant_schedule_with_warmup(optimizer, 100)\n\ndef loss_fn(output,target):\n    return torch.sqrt(nn.MSELoss()(output,target))\n\nepochs = config.EPOCHS\n\nFOLD_MAPPPING = {\n    0: [1, 2, 3, 4],\n    1: [0, 2, 3, 4],\n    2: [0, 1, 3, 4],\n    3: [0, 1, 2, 4],\n    4: [0, 1, 2, 3]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create FOLDS\ntrain = create_Stratkfolds(train,'target', config.SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReadabiltyDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.sentences = data[config.TEXT_COLUMN].to_numpy()\n        self.target = data['target']\n        self.tokenizer = tokenizer\n        self.max_len = config.MAX_LEN\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, item):\n        sentences = str(self.sentences[item])\n        sentences = \" \".join(sentences.split())\n\n        inputs = self.tokenizer.encode_plus(\n            sentences,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"targets\": torch.tensor(self.target[item], dtype=torch.float),\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taken from https://www.kaggle.com/chumajin/pytorch-bert-beginner-s-room\ndef training(train_dataloader,model,optimizer,scheduler):\n    \n    model.train()\n    torch.backends.cudnn.benchmark = True\n\n    allpreds = []\n    alltargets = []\n\n    for a in tqdm(train_dataloader):\n\n        losses = []\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n\n            ids = a[\"ids\"].to(device,non_blocking=True)\n            mask = a[\"mask\"].to(device,non_blocking=True)\n\n            output = model(ids,mask)\n\n            target = a[\"targets\"].to(device,non_blocking=True)\n\n            loss = loss_fn(output,target)\n\n\n            # For scoring\n            losses.append(loss.item())\n            allpreds.append(output.detach().cpu().numpy())\n            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n\n        scaler.scale(loss).backward() # backwards of loss\n        scaler.step(optimizer) # Update optimizer\n        scaler.update() # scaler update\n\n        scheduler.step() # Update learning rate schedule\n\n        # Combine dataloader minutes\n\n    allpreds = np.concatenate(allpreds)\n    alltargets = np.concatenate(alltargets)\n\n    # I don't use loss, but I collect it\n\n    losses = np.mean(losses)\n\n    # Score with rmse\n    train_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n\n    return losses,train_rme_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validating(valid_dataloader, model):\n    \n    model.eval()\n\n    allpreds = []\n    alltargets = []\n\n    for a in valid_dataloader:\n\n        losses = []\n\n        with torch.no_grad():\n\n            ids = a[\"ids\"].to(device,non_blocking=True)\n            mask = a[\"mask\"].to(device,non_blocking=True)\n\n            output = model(ids,mask)\n\n            target = a[\"targets\"].to(device,non_blocking=True)\n\n            loss = loss_fn(output,target)\n\n\n            # For scoring\n            losses.append(loss.item())\n            allpreds.append(output.detach().cpu().numpy())\n            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n\n\n    # Combine dataloader minutes\n\n    allpreds = np.concatenate(allpreds)\n    alltargets = np.concatenate(alltargets)\n\n    # I don't use loss, but I collect it\n\n    losses = np.mean(losses)\n\n    # Score with rmse\n    valid_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n\n    return allpreds,losses,valid_rme_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for FOLD in FOLD_MAPPPING.keys():\n    \n    print(\" Fold Number : {0}\".format(str(FOLD)))\n    \n    train_df = train[(train.kfold.isin(FOLD_MAPPPING.get(FOLD)))].reset_index(drop=True)\n    valid_df = train[(train.kfold==FOLD)].reset_index(drop=True)\n    \n    train_data = ReadabiltyDataset(data = train_df, tokenizer = tokenizer) \n    train_loader = DataLoader(dataset = train_data, shuffle=True, batch_size = config.TRAIN_BATCH_SIZE, num_workers=4,pin_memory=True)\n\n    val_data = ReadabiltyDataset(data = valid_df, tokenizer = tokenizer) \n    val_loader = DataLoader(dataset = val_data, shuffle=False, batch_size = config.VAL_BATCH_SIZE, num_workers=4,pin_memory=True)\n    \n    train_steps = int(len(train_df)/config.TRAIN_BATCH_SIZE * config.EPOCHS)\n\n    num_steps = int(train_steps*0.1)\n    \n    trainlosses = []\n    vallosses = []\n    bestscore = None\n\n    trainscores = []\n    validscores = []\n\n    for epoch in range(epochs):\n    \n        print(\"---------------\" + str(epoch) + \"start-------------\")\n\n        trainloss,trainscore = training(train_loader,model,optimizer,scheduler)\n\n        trainlosses.append(trainloss)\n        trainscores.append(trainscore)\n\n        print(\"trainscore is \" + str(trainscore))\n\n        preds,validloss,valscore=validating(val_loader,model)\n\n        vallosses.append(validloss)\n        validscores.append(valscore)\n\n\n        print(\"valscore is \" + str(valscore))\n\n        if bestscore is None:\n            bestscore = valscore\n\n            print(\"Save first model\")\n\n            state = {\n                            'state_dict': model.state_dict(),\n                            'optimizer_dict': optimizer.state_dict(),\n                            \"bestscore\":bestscore\n                        }\n\n\n            torch.save(state, \"roberta_model_fold{0}.pth\".format(FOLD))\n\n        elif bestscore > valscore:\n\n            bestscore = valscore\n\n            print(\"found better point\")\n\n            state = {\n                            'state_dict': model.state_dict(),\n                            'optimizer_dict': optimizer.state_dict(),\n                            \"bestscore\":bestscore\n                        }\n\n\n            torch.save(state, \"roberta_model_fold{0}.pth\".format(FOLD))\n\n        else:\n            pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ndel train_dataset,valid_dataset,train_dataloader,valid_dataloader\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReadabiltyInfDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.sentences = data[config.TEXT_COLUMN].to_numpy()\n        self.tokenizer = tokenizer\n        self.max_len = config.MAX_LEN\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, item):\n        sentences = str(self.sentences[item])\n        sentences = \" \".join(sentences.split())\n\n        inputs = self.tokenizer.encode_plus(\n            sentences,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = ReadabiltyInfDataset(test, tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataloader = DataLoader(test_dataset,batch_size=config.VAL_BATCH_SIZE,shuffle = False,num_workers=4,pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pthes = [os.path.join(\"./\",s) for s in os.listdir(\"./\") if \".pth\" in s]\n# pthes = ['./model0.pth']\npthes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"states = [torch.load(s) for s in pthes]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predicting(test_dataloader, model, states):\n\n    allpreds = []\n    \n    for state in states:\n        model.load_state_dict(state[\"state_dict\"])\n        model.to(device)\n        model.eval()\n    \n    \n        preds = []\n        allvalloss=0\n\n        with torch.no_grad():\n\n\n            for a in tqdm(test_dataloader):\n\n                ids = a[\"ids\"].to(device)\n                mask = a[\"mask\"].to(device)\n\n               # output = model(ids,mask,tokentype)\n                output = model(ids,mask)\n\n                preds.append(output.cpu().numpy())\n\n            preds = np.concatenate(preds)\n            \n            allpreds.append(preds)\n\n    return allpreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"allpreds = predicting(test_dataloader,model,states)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"findf = pd.DataFrame(allpreds)\nfindf = findf.T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"findf = findf.mean(axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'id': test.id, 'target': findf})\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]}]}