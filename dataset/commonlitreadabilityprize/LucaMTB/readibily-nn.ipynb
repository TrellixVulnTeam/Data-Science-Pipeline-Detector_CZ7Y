{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tensorflow import keras\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-22T15:52:58.465006Z","iopub.execute_input":"2021-05-22T15:52:58.465329Z","iopub.status.idle":"2021-05-22T15:52:58.472253Z","shell.execute_reply.started":"2021-05-22T15:52:58.4653Z","shell.execute_reply":"2021-05-22T15:52:58.471387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntext","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:23:49.945734Z","iopub.execute_input":"2021-05-22T14:23:49.946014Z","iopub.status.idle":"2021-05-22T14:23:50.044646Z","shell.execute_reply.started":"2021-05-22T14:23:49.945967Z","shell.execute_reply":"2021-05-22T14:23:50.043759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom keras import regularizers, optimizers\nfrom keras.layers.experimental.preprocessing import TextVectorization\nfrom keras.layers import Embedding, Dense, Dropout, Input, LSTM, GlobalMaxPool1D\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nimport tensorflow as tf\nimport spacy\n\n# download and import the large english model.\n#!python -m spacy download en_core_web_lg\n#import en_core_web_lg\n\nnlp = spacy.load('en_core_web_lg')\nVectorizer = TextVectorization()\n\n#load the data \ntext = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n\n#fit the vectorizer on the text and extract the corpus vocabulary\nVectorizer.adapt(text.excerpt.to_numpy())\nvocab = Vectorizer.get_vocabulary()\n\n#generate the embedding matrix\nnum_tokens = len(vocab)\nembedding_dim = len(nlp('The').vector)\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor i, word in enumerate(vocab):\n    embedding_matrix[i] = nlp(word).vector\n\n#Load the embedding matrix as the weights matrix for the embedding layer and set trainable to False\nEmbedding_layer=Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=Constant(embedding_matrix),\n    trainable=False)\n\n#build the model.  This is a bigger one, but it works well on this problem.\n\n\n#fit the model\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T15:55:18.992692Z","iopub.execute_input":"2021-05-22T15:55:18.993018Z","iopub.status.idle":"2021-05-22T15:58:17.797659Z","shell.execute_reply.started":"2021-05-22T15:55:18.992988Z","shell.execute_reply":"2021-05-22T15:58:17.796058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_callbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=5,verbose=1,monitor='val_loss'),\n    #tf.keras.callbacks.ModelCheckpoint(filepath='Read_model',verbose=1,monitor='val_loss',save_best_only=True),\n    #tf.keras.callbacks.LearningRateScheduler(lr_schedul,verbose=1)\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.9,patience=2,min_lr=1e-30,mode='min',verbose=1,)\n    \n]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:40:14.658177Z","iopub.execute_input":"2021-05-22T16:40:14.658504Z","iopub.status.idle":"2021-05-22T16:40:14.663604Z","shell.execute_reply.started":"2021-05-22T16:40:14.658475Z","shell.execute_reply":"2021-05-22T16:40:14.662531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"times = 7","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(times):\n    \n    model = Sequential()\n    model.add(Input(shape=(1,), dtype=tf.string))\n    model.add(Vectorizer)\n    model.add(Embedding_layer)\n    model.add(LSTM(25, return_sequences=True))\n    model.add(GlobalMaxPool1D())\n    model.add(Dropout(0.5))\n    model.add(Dense(32, activation='tanh', \n                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n    model.add(Dropout(0.5))\n    model.add(Dense(32, activation='tanh', \n                    kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))    \n    model.add(Dense(1))\n\n    adam = optimizers.Adam(learning_rate=.01)\n    model.compile(optimizer = adam, loss = 'mean_squared_error', metrics = None)\n    \n    \n    model.fit(text.excerpt,\n          text.target,\n          batch_size = 500,\n          epochs = 80,\n          callbacks=my_callbacks,\n          validation_split=.2)\n    \n    model.save('Read_model_'+ str(i))\n\n#print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:40:19.80543Z","iopub.execute_input":"2021-05-22T16:40:19.805764Z","iopub.status.idle":"2021-05-22T16:42:26.809298Z","shell.execute_reply.started":"2021-05-22T16:40:19.805736Z","shell.execute_reply":"2021-05-22T16:42:26.808481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adam = optimizers.Adam(learning_rate=.001, decay=1e-2)\n#model.compile(optimizer = adam, loss = 'mean_squared_error', metrics = None)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:03:48.388861Z","iopub.execute_input":"2021-05-22T16:03:48.389226Z","iopub.status.idle":"2021-05-22T16:03:48.404374Z","shell.execute_reply.started":"2021-05-22T16:03:48.389195Z","shell.execute_reply":"2021-05-22T16:03:48.403341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model1 = keras.models.load_model('./Read_model')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T15:54:01.014772Z","iopub.execute_input":"2021-05-22T15:54:01.015101Z","iopub.status.idle":"2021-05-22T15:54:38.178914Z","shell.execute_reply.started":"2021-05-22T15:54:01.015071Z","shell.execute_reply":"2021-05-22T15:54:38.178136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:42:26.811757Z","iopub.execute_input":"2021-05-22T16:42:26.812027Z","iopub.status.idle":"2021-05-22T16:42:26.824184Z","shell.execute_reply.started":"2021-05-22T16:42:26.812002Z","shell.execute_reply":"2021-05-22T16:42:26.8235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = []\n\nfor i in range(times):\n    \n    model1 = keras.models.load_model('./Read_model_' + str(i))\n    a = model1.predict(test.excerpt)\n    pred.append(a)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:42:37.709008Z","iopub.execute_input":"2021-05-22T16:42:37.709342Z","iopub.status.idle":"2021-05-22T16:43:56.417475Z","shell.execute_reply.started":"2021-05-22T16:42:37.709314Z","shell.execute_reply":"2021-05-22T16:43:56.413678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(pred,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:44:45.309768Z","iopub.execute_input":"2021-05-22T16:44:45.310133Z","iopub.status.idle":"2021-05-22T16:44:45.315753Z","shell.execute_reply.started":"2021-05-22T16:44:45.310101Z","shell.execute_reply":"2021-05-22T16:44:45.314888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['target'] = np.mean(pred,axis=0)\n\ntest[['id','target']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:47:16.579211Z","iopub.execute_input":"2021-05-22T14:47:16.579582Z","iopub.status.idle":"2021-05-22T14:47:16.765578Z","shell.execute_reply.started":"2021-05-22T14:47:16.579551Z","shell.execute_reply":"2021-05-22T14:47:16.764812Z"},"trusted":true},"execution_count":null,"outputs":[]}]}