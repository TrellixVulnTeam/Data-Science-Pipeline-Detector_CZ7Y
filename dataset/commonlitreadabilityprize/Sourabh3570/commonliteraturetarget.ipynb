{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T16:33:54.295124Z","iopub.execute_input":"2021-07-30T16:33:54.295485Z","iopub.status.idle":"2021-07-30T16:33:54.305222Z","shell.execute_reply.started":"2021-07-30T16:33:54.295453Z","shell.execute_reply":"2021-07-30T16:33:54.304302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import spatial\n\nfrom sklearn.metrics import mean_squared_error, auc, mean_absolute_error \nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import cross_val_score, train_test_split, cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nimport time\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.linear_model import LogisticRegression,SGDRegressor, LinearRegression,Ridge\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n%matplotlib inline\nimport seaborn as sns\nsns.set_theme()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:55:10.934817Z","iopub.execute_input":"2021-07-30T16:55:10.935163Z","iopub.status.idle":"2021-07-30T16:55:10.946922Z","shell.execute_reply.started":"2021-07-30T16:55:10.935133Z","shell.execute_reply":"2021-07-30T16:55:10.945928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = \"/kaggle/input/commonlitreadabilityprize\"\ntrain_path = os.path.join(train_dir, \"train.csv\")\ntest_path = os.path.join(train_dir, \"test.csv\")\n\nglove_files = [\"glove.6B.300d.txt\",\"glove.6B.200d.txt\", \"glove.6B.100d.txt\", \"glove.6B.50d.txt\"]\nglove_path_dir = \"../input/glove6b/\"\nsample = os.path.join(train_dir, \"sample_submission.csv\")\ndf = pd.read_csv(train_path)\ndf.head()\n\ndf_test = pd.read_csv(test_path)\n\ndf_sample = pd.read_csv(sample)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:34:57.925124Z","iopub.execute_input":"2021-07-30T16:34:57.925673Z","iopub.status.idle":"2021-07-30T16:34:58.01964Z","shell.execute_reply.started":"2021-07-30T16:34:57.925641Z","shell.execute_reply":"2021-07-30T16:34:58.01885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# array  =[]; glove_embedding = {}; count=0\n\n# sentence = \"example sentence for word emnedding\"\n\n# with open(glove_path_dir+glove_files[-1], \"r\") as f:\n#     for line in f:\n#         k = line.split()\n#         if k[0].isalpha():\n#             glove_embedding[k[0]] = np.asarray(k[1:], np.float32)\n            ","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:00.741057Z","iopub.execute_input":"2021-07-30T16:35:00.741443Z","iopub.status.idle":"2021-07-30T16:35:09.506352Z","shell.execute_reply.started":"2021-07-30T16:35:00.741398Z","shell.execute_reply":"2021-07-30T16:35:09.505386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # \n# def find_closest_embeddings(embedding, embeddings_dict= glove_embedding):\n#     return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))[:10]\n\n# embed_list = find_closest_embeddings(glove_embedding['king'])\n# embed_list","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:17.710068Z","iopub.execute_input":"2021-07-30T16:35:17.710437Z","iopub.status.idle":"2021-07-30T16:35:22.724121Z","shell.execute_reply.started":"2021-07-30T16:35:17.710405Z","shell.execute_reply":"2021-07-30T16:35:22.723133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_results_original(results,original ):\n    #  Convert the results to a string, and word-wrap them.\n    results = re.sub(r\"(.{,70})\\s\", r\"\\1\\n\", results + \" \").rstrip()\n\n    # Convert the original to a string, and word wrap it.\n    original = re.sub(r\"(.{,70})\\s\", r\"\\1\\n\", original + \" \").rstrip()\n\n    # Print the results.\n    print(\"Original \" ,len(original))\n    print(\"Reults \",len(results))\n    print(\"-Original-\".center(70).replace(\" \", \"*\").replace(\"-\", \" \"))\n    print(original)\n    print(\"-Results-\".center(70).replace(\" \", \"*\").replace(\"-\", \" \"))\n    print(results)\n    print(\"*\" * 70)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:23.956232Z","iopub.execute_input":"2021-07-30T16:35:23.956855Z","iopub.status.idle":"2021-07-30T16:35:23.96478Z","shell.execute_reply.started":"2021-07-30T16:35:23.956804Z","shell.execute_reply":"2021-07-30T16:35:23.963869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from nltk import wordnet \n# import regex as re\n# from nltk import stem\n\n# stemmer = stem.PorterStemmer()\n# treebank = df.iloc[:,3]\n# stemmed = []\n\n# for i in range(df.shape[0]):\n#     for word in df.iloc[i,3]:\n#         stemmed.append(stemmer.stem(word))\n#     results = \"\".join(stemmed)\n#     if len(df.iloc[:,3])>len(results):\n#         if i<1:\n#             print_results_original(results, df.iloc[i,3])\n#         df.iloc[i,3]= results\n#     stemmed = []\n# #","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:35:29.041771Z","iopub.execute_input":"2021-07-30T16:35:29.042117Z","iopub.status.idle":"2021-07-30T16:35:33.664206Z","shell.execute_reply.started":"2021-07-30T16:35:29.042087Z","shell.execute_reply":"2021-07-30T16:35:33.663337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.iloc[:,3] = df.iloc[:,3].str.replace(\"[:;.,<>/-_~`!@#$%^&*?[]{}|-+0-9]\", \"\", regex = True)\ndf.iloc[16, 3]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:40:35.529642Z","iopub.execute_input":"2021-07-30T16:40:35.530271Z","iopub.status.idle":"2021-07-30T16:40:35.750731Z","shell.execute_reply.started":"2021-07-30T16:40:35.530191Z","shell.execute_reply":"2021-07-30T16:40:35.749747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_graph(data):\n    tar_s = np.unique(data['target']).shape\n\n    fig,ax = plt.subplots(1,2,figsize=(15,8))\n    sns.scatterplot(x = np.arange(tar_s[0]), y =data['target'],ax=ax[0] )\n    sns.scatterplot(x = np.arange(tar_s[0]), y =data['standard_error'],ax=ax[1] )\n    return fig, ax\n# plot_graph(df)\n    # A outlier could easily be drop with IQR and pandas dataframe filter method\n    # There is one outlier in standard error at index = 0","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:41:00.598403Z","iopub.execute_input":"2021-07-30T16:41:00.598879Z","iopub.status.idle":"2021-07-30T16:41:00.604724Z","shell.execute_reply.started":"2021-07-30T16:41:00.598838Z","shell.execute_reply":"2021-07-30T16:41:00.603668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_validate_data(X, y):\n    X_train, X_test, y_train,y_test = train_test_split(X, y, test_size= 0.2)\n    print(X_train.shape, X_test.shape, y_train.shape,y_test.shape)\n    return X_train, X_test, y_train,y_test\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:41:03.456249Z","iopub.execute_input":"2021-07-30T16:41:03.456792Z","iopub.status.idle":"2021-07-30T16:41:03.461322Z","shell.execute_reply.started":"2021-07-30T16:41:03.456748Z","shell.execute_reply":"2021-07-30T16:41:03.460488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_describe = df.describe()\nQ3 = df_describe.loc['75%']; Q1 = df_describe.loc['25%']\nIQR = Q3 - Q1\n\nupper_iqr = Q3 +1.5*IQR\nlower_iqr  = Q1 - 1.5*IQR\n\n# print(f\"Condition for outliers is {df['target']<=  upper_iqr and df['target']<= lower_iqr}\")\ndf['target']\nm = map(lambda x: x>= lower_iqr, df['target'])\n\n\ndf_upper = df.groupby(\"target\").filter(lambda x: (x['standard_error']> upper_iqr['standard_error'] ))\ndf_lower = df.groupby(\"target\").filter(lambda x: (x['standard_error']< lower_iqr['standard_error'] ))\n\ndf_upper2 = df.groupby(\"standard_error\").filter(lambda x: (x['target']> upper_iqr['target'] ))\ndf_lower2 = df.groupby(\"standard_error\").filter(lambda x: (x['target']< lower_iqr['target'] ))\n\ndf.drop(index= df_upper.index, axis=0, inplace=True)\ndf.drop(index = df_lower.index, axis=0, inplace=True)\ndf.drop(index = df_upper2.index, axis=0, inplace=True)\ndf.drop(index = df_lower2.index, axis=0, inplace=True)\n# ploting target data after removing outlier","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:41:32.862241Z","iopub.execute_input":"2021-07-30T16:41:32.862663Z","iopub.status.idle":"2021-07-30T16:41:36.572897Z","shell.execute_reply.started":"2021-07-30T16:41:32.862628Z","shell.execute_reply":"2021-07-30T16:41:36.572183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_graph(df)\n\ndf.head(5)\n\ndf['target_std'] = np.divide(df.iloc[:, 4], df.iloc[:, 5])\ndf['target_std_m'] = np.subtract(df.iloc[:, 4], df.iloc[:, 5])\n\nmean_err = np.mean(df.iloc[:, 5])\ndf['target_std_mean'] = np.divide(df.iloc[:, 4], mean_err)\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:47:15.200622Z","iopub.execute_input":"2021-07-30T16:47:15.200964Z","iopub.status.idle":"2021-07-30T16:47:15.22316Z","shell.execute_reply.started":"2021-07-30T16:47:15.200936Z","shell.execute_reply":"2021-07-30T16:47:15.22216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse_score(model, X, y):\n    y_pred = model.predict(X)\n    score = cross_val_score(model, X, y, n_jobs= -1, cv= 5); m = mean_squared_error(y, y_pred)\n    print(f\"Cross_val_Score for X_train prediction:\\t{score}\\nmean squared error:\\t{m}\")\n    return score, m ","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:55:40.901322Z","iopub.execute_input":"2021-07-30T17:55:40.901661Z","iopub.status.idle":"2021-07-30T17:55:40.907125Z","shell.execute_reply.started":"2021-07-30T17:55:40.901628Z","shell.execute_reply":"2021-07-30T17:55:40.906069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n                                 stop_words='english')\nd = vectorizer.fit_transform(df['excerpt'])\n\ny_train = df.iloc[:, 4]\ny_train1 = df.iloc[:, 6]\ny_train2 = df.iloc[:, 7]\ny_train3 = df.iloc[:, 8]\n\nX_train = d.toarray()\n\nX_train_, X_test_, y_train,y_test = get_validate_data(X_train, y_train)\nX_train1, X_test1, y_train1,y_test1 = get_validate_data(X_train, y_train1)\nX_train2, X_test2, y_train2,y_test2 = get_validate_data(X_train, y_train2)\nX_train3, X_test3, y_train3,y_test3 = get_validate_data(X_train, y_train3)\n\nX_train.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:49:05.895121Z","iopub.execute_input":"2021-07-30T17:49:05.895491Z","iopub.status.idle":"2021-07-30T17:49:07.678945Z","shell.execute_reply.started":"2021-07-30T17:49:05.895456Z","shell.execute_reply":"2021-07-30T17:49:07.678203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_dict = {}\n# words = vectorizer.get_feature_names()\n# skip_word = 0; X_arr = np.array([])\n# for i in words:\n#     try:\n# #         X_arr = np.append(X_arr, glove_embedding[i])\n#         word_dict[i] = (glove_embedding[i])\n#     except KeyError:\n#         skip_word +=1\n# print(\"Skip word\\t%s\"%skip_word)\n# print(\"len of word dict\\t%s\"%len(word_dict))\n# print(\"X array shape\\t%s\"%X_arr.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:29:01.372622Z","iopub.execute_input":"2021-07-30T17:29:01.372942Z","iopub.status.idle":"2021-07-30T17:29:01.419294Z","shell.execute_reply.started":"2021-07-30T17:29:01.372915Z","shell.execute_reply":"2021-07-30T17:29:01.418414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_array = np.array(list(word_dict.values()))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:32:44.235737Z","iopub.execute_input":"2021-07-30T17:32:44.236041Z","iopub.status.idle":"2021-07-30T17:32:44.259132Z","shell.execute_reply.started":"2021-07-30T17:32:44.236014Z","shell.execute_reply":"2021-07-30T17:32:44.25845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nIt is slow  model with accuracy of (rmse ) is 0.75, which is not good to enough to get good rank under 200\n'''\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\nmodel = GaussianProcessRegressor(n_restarts_optimizer= 10 )\n\nmodel.fit(X_train_, y_train)\n\n\ntest_score, test_mean = rmse_score(model, X_test_,y_test)\ntrain_score, train_mean = rmse_score(model, X_train_,y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:54:55.731181Z","iopub.execute_input":"2021-07-30T17:54:55.73166Z","iopub.status.idle":"2021-07-30T17:54:55.734982Z","shell.execute_reply.started":"2021-07-30T17:54:55.73163Z","shell.execute_reply":"2021-07-30T17:54:55.734348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it has low accuracy than gauss, 0.756\n\n# ridge_estimator = Ridge(alpha= 0.5)\n\n# ridge_estimator.fit(X_train_, y_train)\n\n# test_score_, test_mean_ = rmse_score(ridge_estimator, X_test_,y_test)\n# train_score_, train_mean_ = rmse_score(ridge_estimator, X_train_,y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:10:38.609907Z","iopub.execute_input":"2021-07-30T18:10:38.610298Z","iopub.status.idle":"2021-07-30T18:10:47.625135Z","shell.execute_reply.started":"2021-07-30T18:10:38.610254Z","shell.execute_reply":"2021-07-30T18:10:47.623626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Guassian estimator rmse score\",rmse_score(model, X_test_, y_test))\n\n# print(\"Linear Model estimator score\", rmse_score(ridge_estimator, X_train_, y_train))\n# print(\"Linear Model estimator score\", rmse_score(ridge_estimator, X_test_, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:03:57.669033Z","iopub.execute_input":"2021-07-30T18:03:57.669411Z","iopub.status.idle":"2021-07-30T18:04:50.750689Z","shell.execute_reply.started":"2021-07-30T18:03:57.669375Z","shell.execute_reply":"2021-07-30T18:04:50.749996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(1,2, figsize= (15, 8))\n# ax[0].plot(test_score_, 'y')\n# ax[0].plot(train_score_, 'r')\n# ax[0].set_title(\"cross_val_score\")\n\n# ax[0].annotate(\"Validation data\",xy= (2.0,0.410), arrowprops=dict(facecolor='red', shrink=5),fontsize=16,horizontalalignment='left')\n# ax[0].annotate(\"training data\",xy= (2.0,0.30), arrowprops=dict(facecolor='red', shrink=5),fontsize=16,horizontalalignment='left')\n\n# ax[1].plot(test_score, 'y')\n# ax[1].plot(train_score, 'r')\n# ax[1].set_title(\"cross_val_score\")\n\n# ax[1].annotate(\"Validation data\",xy= (2.0,0.410), arrowprops=dict(facecolor='red', shrink=5),fontsize=16,horizontalalignment='left')\n# ax[1].annotate(\"training data\",xy= (2.0,0.30), arrowprops=dict(facecolor='red', shrink=5),fontsize=16,horizontalalignment='left')\n\n# # from graph, it is clear, our model is overfitting data very high, could be fix with l1 or l2 ","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:11:39.97619Z","iopub.execute_input":"2021-07-30T18:11:39.976673Z","iopub.status.idle":"2021-07-30T18:11:40.437846Z","shell.execute_reply.started":"2021-07-30T18:11:39.976642Z","shell.execute_reply":"2021-07-30T18:11:40.43694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_from_gauss(model, data, num):\n    p = data.iloc[num]['excerpt']\n    p = vectorizer.transform([p])\n    pred = model.predict(p.toarray())\n    print(pred)\n    return pred[0]\n\ndef predict_traget_(model, data, save_file=False):\n    data['target']=0\n    for i in range(data.shape[0]):\n        data.iloc[i, -1] = predict_from_gauss(model, data, i)\n    for i in ['url_legal','license','excerpt']:\n        try:\n            df_test.drop(i, inplace=True, axis=1)\n        except KeyError:\n            pass\n    if save_file:\n        data.to_csv(\"submission.csv\", index=False)\n    return f\"Successfully test data submitted or written in output directory\\tsubmission.csv\"\n    \n\npredict_traget_(model, df_test, save_file=True )","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:14:25.961733Z","iopub.execute_input":"2021-07-30T18:14:25.962142Z","iopub.status.idle":"2021-07-30T18:14:26.021132Z","shell.execute_reply.started":"2021-07-30T18:14:25.962105Z","shell.execute_reply":"2021-07-30T18:14:26.020161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:14:43.615032Z","iopub.execute_input":"2021-07-30T18:14:43.615437Z","iopub.status.idle":"2021-07-30T18:14:43.632484Z","shell.execute_reply.started":"2021-07-30T18:14:43.615401Z","shell.execute_reply":"2021-07-30T18:14:43.631514Z"},"trusted":true},"execution_count":null,"outputs":[]}]}