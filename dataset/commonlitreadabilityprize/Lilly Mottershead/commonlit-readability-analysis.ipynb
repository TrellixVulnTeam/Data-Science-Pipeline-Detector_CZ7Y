{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-03T14:44:45.122628Z","iopub.execute_input":"2021-06-03T14:44:45.123159Z","iopub.status.idle":"2021-06-03T14:44:45.133312Z","shell.execute_reply.started":"2021-06-03T14:44:45.123105Z","shell.execute_reply":"2021-06-03T14:44:45.132479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overview\n\nIn this competition, the goal is to develop a reading ease classification model for literature excerpts drawn from a wide range of time periods.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:45.175454Z","iopub.execute_input":"2021-06-03T14:44:45.176021Z","iopub.status.idle":"2021-06-03T14:44:45.18066Z","shell.execute_reply.started":"2021-06-03T14:44:45.175975Z","shell.execute_reply":"2021-06-03T14:44:45.180008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train.csv","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\nRECORDS, COLUMNS = df.shape\nprint(f'Shape: {(RECORDS, COLUMNS)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:45.21977Z","iopub.execute_input":"2021-06-03T14:44:45.220371Z","iopub.status.idle":"2021-06-03T14:44:45.26287Z","shell.execute_reply.started":"2021-06-03T14:44:45.220321Z","shell.execute_reply":"2021-06-03T14:44:45.261877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train.csv has 6 columns and 2834 records. Lets see what the columns look like.","metadata":{}},{"cell_type":"markdown","source":"### train.csv feature information","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:45.289432Z","iopub.execute_input":"2021-06-03T14:44:45.289957Z","iopub.status.idle":"2021-06-03T14:44:45.305105Z","shell.execute_reply.started":"2021-06-03T14:44:45.289903Z","shell.execute_reply":"2021-06-03T14:44:45.304128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('dtypes for each column:')\ndtypes = {}\nfor column in df.columns:\n    print(f'{column}: {str(df[column].dtype)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:45.340189Z","iopub.execute_input":"2021-06-03T14:44:45.34053Z","iopub.status.idle":"2021-06-03T14:44:45.346768Z","shell.execute_reply.started":"2021-06-03T14:44:45.3405Z","shell.execute_reply":"2021-06-03T14:44:45.345903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 6 features are: id, url_legal, license, excerpt, target, and standard_error. Target and standard_error are floats, and all other features are strings.","metadata":{}},{"cell_type":"code","source":"null_count = df.isnull().sum().rename('null_count')\nnull_proportion_total = (null_count/RECORDS).rename('proportion_total')\n\npd.concat([null_count, null_proportion_total], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:45.415163Z","iopub.execute_input":"2021-06-03T14:44:45.415515Z","iopub.status.idle":"2021-06-03T14:44:45.431831Z","shell.execute_reply.started":"2021-06-03T14:44:45.415484Z","shell.execute_reply":"2021-06-03T14:44:45.430931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are numerous null values for url_legal and license, but those probably won't be useful in our model, so we don't need to worry about filling them.","metadata":{}},{"cell_type":"code","source":"unique_count = df.nunique().rename('unique_count')\nproportion_non_null = (unique_count/(RECORDS-null_count)).rename('proportion_non_null')\nproportion_total = (unique_count/RECORDS).rename('proportion_total')\n\npd.concat([unique_count, proportion_non_null, proportion_total], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:45.480618Z","iopub.execute_input":"2021-06-03T14:44:45.48099Z","iopub.status.idle":"2021-06-03T14:44:45.508647Z","shell.execute_reply.started":"2021-06-03T14:44:45.480948Z","shell.execute_reply":"2021-06-03T14:44:45.507655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Every record has unique values for id (as expected), excerpt, target, and standard_error. Among the non-null records for url_legal and license, there are numerous duplicates, especially for license.","metadata":{}},{"cell_type":"markdown","source":"## Target and standard error distributions","metadata":{}},{"cell_type":"code","source":"df[['target', 'standard_error']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:45.560483Z","iopub.execute_input":"2021-06-03T14:44:45.560815Z","iopub.status.idle":"2021-06-03T14:44:45.581277Z","shell.execute_reply.started":"2021-06-03T14:44:45.560785Z","shell.execute_reply":"2021-06-03T14:44:45.580485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(\n    2,\n    1,\n    figsize=(10, 6),\n)\nfor i, column in enumerate(['target', 'standard_error']):\n    plot = sns.boxenplot(\n        data=df, \n        x=column,\n        ax=ax[i],\n        linewidth=1,\n        width=.5,\n        palette=sns.color_palette('deep')[:1],\n    )\n    plot.set_xlabel(column, fontsize=14)\n    if i > 0:\n        plot.set(ylabel=None)\nfig.suptitle('Distributions of target and standard_error', fontsize=16)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:45.618978Z","iopub.execute_input":"2021-06-03T14:44:45.619302Z","iopub.status.idle":"2021-06-03T14:44:45.904143Z","shell.execute_reply.started":"2021-06-03T14:44:45.619273Z","shell.execute_reply":"2021-06-03T14:44:45.903113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"target ranges from -3.677 to 1.711. standard_error ranges from 0 to 0.650, although it should be noted that the minimum is a significant outlier. Both target and standard_error contain outliers.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(\n    1,\n    1,\n    figsize=(6, 6),\n)\nplot = sns.scatterplot(\n    data=df, \n    x='target',\n    y='standard_error',\n    linewidth=0,\n    color=sns.color_palette('deep')[0],\n    alpha=.5,\n)\nplt.ylim(.4, None)\nfig.suptitle('Scatterplot of target v. standard_error', fontsize=16)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:45.905564Z","iopub.execute_input":"2021-06-03T14:44:45.905829Z","iopub.status.idle":"2021-06-03T14:44:46.144655Z","shell.execute_reply.started":"2021-06-03T14:44:45.905802Z","shell.execute_reply":"2021-06-03T14:44:46.143967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generally, the more extreme values of target have a larger standard error. The above scatterplot has a y minimum of 0.4 to more clearly show the trend, but doing so removes the clear standard_error outlier at 0.","metadata":{}},{"cell_type":"markdown","source":"# Pre-processing Excerpt","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nimport string\nfrom nltk.tokenize import sent_tokenize, regexp_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:46.145975Z","iopub.execute_input":"2021-06-03T14:44:46.146345Z","iopub.status.idle":"2021-06-03T14:44:46.150409Z","shell.execute_reply.started":"2021-06-03T14:44:46.146318Z","shell.execute_reply":"2021-06-03T14:44:46.14952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Special characters","metadata":{}},{"cell_type":"code","source":"lowercase = [c for c in string.ascii_lowercase]\ndigits = [c for c in string.digits]\npunc = [c for c in string.punctuation]\nextra = [' ', '\\n', '—', '–']\nlowercase_digits_punc = lowercase + digits + punc + extra\n\ncharacter_counter = Counter([character.lower() for excerpt in df['excerpt'] for character in excerpt])\ncharacters_df = pd.DataFrame(sorted(character_counter.items(), key=lambda x: -x[1]))\ncharacters_df.columns = ['character', 'count']\n\nspecial_characters_df = pd.DataFrame(characters_df.loc[~characters_df['character'].isin(lowercase_digits_punc)])\n\ntotal_character_count = characters_df['count'].sum()\nprint(f'Total number of characters: {total_character_count}')","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:46.152138Z","iopub.execute_input":"2021-06-03T14:44:46.152522Z","iopub.status.idle":"2021-06-03T14:44:46.850876Z","shell.execute_reply.started":"2021-06-03T14:44:46.152464Z","shell.execute_reply":"2021-06-03T14:44:46.849948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nplot = sns.barplot(\n    data=special_characters_df,\n    x='character',\n    y='count',\n    color=sns.color_palette('deep')[0],\n)\nplot.set_xticklabels(plot.get_xticklabels(), size=14)\nplot.set_title('Barplot of special character counts', fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:46.85205Z","iopub.execute_input":"2021-06-03T14:44:46.852327Z","iopub.status.idle":"2021-06-03T14:44:47.265155Z","shell.execute_reply.started":"2021-06-03T14:44:46.8523Z","shell.execute_reply":"2021-06-03T14:44:47.26425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are instances of the less common characters, but they are relatively rare. There are also special characters, such as accented characters, which likely appear in the middle of words.","metadata":{}},{"cell_type":"markdown","source":"## Sentence cleaning function","metadata":{}},{"cell_type":"code","source":"def clean(sentence, remove_stopwords=True, lemmatize=True):\n    \"\"\"\n    Takes a raw sentence and processes it into a list of lowercase, lemmatized words without stopwords.\n    \n    Keyword arguments:\n        sentence: the raw sentence as a string\n        remove_stopwords: True if stopwords should be removed from the sentence, false otherwise\n        lemmatize: True if the sentence tokens should be lemmatized, false otherwise\n    \n    Returns:\n        sentence (list): the cleaned list of tokens in the sentence\n    \"\"\"\n    \n    # Transform to lowercase\n    sentence = sentence.lower()\n    \n    # Separate tokens\n    # This method is fast, but will split words containing accented characters and other special characters may be split strangely\n    sentence = regexp_tokenize(sentence, pattern='\\w+')\n\n    # Remove stopwords\n    if remove_stopwords:\n        sentence = [word for word in sentence if word not in stopwords.words('english')]\n\n    # Lemmatize\n    if lemmatize:\n        lemmatizer = WordNetLemmatizer()\n        sentence = [lemmatizer.lemmatize(word) for word in sentence]\n    \n    return sentence","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:47.266349Z","iopub.execute_input":"2021-06-03T14:44:47.266637Z","iopub.status.idle":"2021-06-03T14:44:47.272747Z","shell.execute_reply.started":"2021-06-03T14:44:47.26661Z","shell.execute_reply":"2021-06-03T14:44:47.271788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example with the first excerpt","metadata":{}},{"cell_type":"markdown","source":"Before pre-processing:","metadata":{}},{"cell_type":"code","source":"print(df['excerpt'][0])","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:47.27397Z","iopub.execute_input":"2021-06-03T14:44:47.274239Z","iopub.status.idle":"2021-06-03T14:44:47.289248Z","shell.execute_reply.started":"2021-06-03T14:44:47.274213Z","shell.execute_reply":"2021-06-03T14:44:47.288204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After pre-processing, with sentences separated by '.' and tokens separated by ' ':","metadata":{}},{"cell_type":"code","source":"res = sent_tokenize(df['excerpt'][0])\nres = [' '.join(clean(sentence)) for sentence in res]\nres = ' . '.join(res)\nprint(res)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:47.291749Z","iopub.execute_input":"2021-06-03T14:44:47.292058Z","iopub.status.idle":"2021-06-03T14:44:47.327468Z","shell.execute_reply.started":"2021-06-03T14:44:47.292031Z","shell.execute_reply":"2021-06-03T14:44:47.326455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning all excerpts","metadata":{}},{"cell_type":"code","source":"CLEANED_EXCERPTS = df['excerpt'].transform(lambda excerpt: [clean(sentence) for sentence in sent_tokenize(excerpt)])\nCLEANED_EXCERPTS","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:44:47.329448Z","iopub.execute_input":"2021-06-03T14:44:47.329712Z","iopub.status.idle":"2021-06-03T14:45:50.694698Z","shell.execute_reply.started":"2021-06-03T14:44:47.329686Z","shell.execute_reply":"2021-06-03T14:45:50.693781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import cmudict\n\nfrom nltk import pos_tag as pos_tag","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:59:02.378917Z","iopub.execute_input":"2021-06-03T14:59:02.379364Z","iopub.status.idle":"2021-06-03T14:59:02.382905Z","shell.execute_reply.started":"2021-06-03T14:59:02.379335Z","shell.execute_reply":"2021-06-03T14:59:02.382176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Excerpt, sentence, word, and syllable metadata","metadata":{}},{"cell_type":"code","source":"meta_df = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:45:56.976653Z","iopub.execute_input":"2021-06-03T14:45:56.977084Z","iopub.status.idle":"2021-06-03T14:45:56.982849Z","shell.execute_reply.started":"2021-06-03T14:45:56.977036Z","shell.execute_reply":"2021-06-03T14:45:56.98195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Excerpt length","metadata":{}},{"cell_type":"code","source":"meta_df['char_count'] = CLEANED_EXCERPTS.transform(lambda excerpt: sum([len(word) for sentence in excerpt for word in sentence]))\nmeta_df['word_count'] = CLEANED_EXCERPTS.transform(lambda excerpt: len([word for sentence in excerpt for word in sentence]))\nmeta_df['sent_count'] = CLEANED_EXCERPTS.transform(len)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:45:56.983977Z","iopub.execute_input":"2021-06-03T14:45:56.984259Z","iopub.status.idle":"2021-06-03T14:45:57.052645Z","shell.execute_reply.started":"2021-06-03T14:45:56.98423Z","shell.execute_reply":"2021-06-03T14:45:57.051643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mean words per sentence","metadata":{}},{"cell_type":"code","source":"def mean_words_per_sent(excerpt):\n    \"\"\"\n    Takes a cleaned excerpt and returns the mean number of words per sentence\n    \n    Keyword Parameters:\n        excerpt: cleaned excerpt\n        \n    Returns:\n        mean_len (float): the mean number of words in all sentences\n    \"\"\"\n    sentence_lens = [len(sentence) for sentence in excerpt]\n    return sum(sentence_lens)/float(len(sentence_lens))\nmeta_df['mean_words_per_sent'] = CLEANED_EXCERPTS.transform(mean_words_per_sent)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:45:57.054059Z","iopub.execute_input":"2021-06-03T14:45:57.054469Z","iopub.status.idle":"2021-06-03T14:45:57.068107Z","shell.execute_reply.started":"2021-06-03T14:45:57.054428Z","shell.execute_reply":"2021-06-03T14:45:57.06702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mean characters per word","metadata":{}},{"cell_type":"code","source":"def mean_chars_per_word(excerpt):\n    \"\"\"\n    Takes a cleaned excerpt and returns the mean word length\n    \n    Keyword Parameters:\n        excerpt: cleaned excerpt\n        \n    Returns:\n        mean_len (float): the mean word length\n    \"\"\"\n    word_lens = [len(word) for sentence in excerpt for word in sentence]\n    return sum(word_lens)/float(len(word_lens))\nmeta_df['mean_chars_per_word'] = CLEANED_EXCERPTS.transform(mean_chars_per_word)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:45:57.069331Z","iopub.execute_input":"2021-06-03T14:45:57.069699Z","iopub.status.idle":"2021-06-03T14:45:57.116192Z","shell.execute_reply.started":"2021-06-03T14:45:57.06967Z","shell.execute_reply":"2021-06-03T14:45:57.115217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mean syllables per word\n\nCredit to [Syllapy](https://github.com/mholtzscher/syllapy), Michael Holtzscher for syllabication.","metadata":{}},{"cell_type":"code","source":"# referred from datascience.stackexchange.com/questions/23376/how-to-get-the-number-of-syllables-in-a-word/24262\nd = cmudict.dict()\ndef nsyl(word):\n    try:\n        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word]][0]\n    except KeyError:\n        # if word not found in cmudict\n        return syllables(word)\ndef syllables(word):\n    count = 0\n    vowels = 'aeiouy'\n    word = word.lower()\n    if word[0] in vowels:\n        count +=1\n    for index in range(1,len(word)):\n        if word[index] in vowels and word[index-1] not in vowels:\n            count +=1\n    if word.endswith('e'):\n        count -= 1\n    if word.endswith('le'):\n        count += 1\n    if count == 0:\n        count += 1\n    return count\n\ndef mean_syllable_count_per_word(excerpt):\n    \"\"\"\n    Takes a cleaned excerpt and returns the mean syllable count per word\n    \n    Keyword Parameters:\n        excerpt: cleaned excerpt\n        \n    Returns:\n        mean_len (float): the mean syllable count per word\n    \"\"\"\n    # If 0 syllables are counted for a word, the word is likely jargon and shouldn't be counted\n    syllable_counts = [nsyl(word) for sentence in excerpt for word in sentence if nsyl(word) > 0]\n    return sum(syllable_counts)/float(len(syllable_counts))\nmeta_df['mean_syllables'] = CLEANED_EXCERPTS.transform(mean_syllable_count_per_word)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T15:03:22.736759Z","iopub.execute_input":"2021-06-03T15:03:22.737113Z","iopub.status.idle":"2021-06-03T15:03:25.019852Z","shell.execute_reply.started":"2021-06-03T15:03:22.73708Z","shell.execute_reply":"2021-06-03T15:03:25.018862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Polysyllables count","metadata":{}},{"cell_type":"code","source":"def polysyllables(excerpt):\n    polysyllables = [word for sentence in excerpt for word in sentence if nsyl(word) >= 3]\n    return len(polysyllables)\nmeta_df['polysyllables'] = CLEANED_EXCERPTS.transform(polysyllables)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T15:03:34.705101Z","iopub.execute_input":"2021-06-03T15:03:34.705484Z","iopub.status.idle":"2021-06-03T15:03:35.348808Z","shell.execute_reply.started":"2021-06-03T15:03:34.705444Z","shell.execute_reply":"2021-06-03T15:03:35.347855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizations","metadata":{}},{"cell_type":"code","source":"meta_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T15:03:38.987067Z","iopub.execute_input":"2021-06-03T15:03:38.987403Z","iopub.status.idle":"2021-06-03T15:03:38.998849Z","shell.execute_reply.started":"2021-06-03T15:03:38.987361Z","shell.execute_reply":"2021-06-03T15:03:38.997988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, len(meta_df.columns), figsize=(16,6))\nfor i, column in enumerate(meta_df.columns):\n    plot = sns.boxenplot(\n        data=meta_df,\n        y=column,\n        ax=ax[i],\n        linewidth=1,\n        width=.5,\n    )\n    plot.set_ylabel(f'\\n{column}', fontsize=14)\nfig.suptitle('Boxenplots for excerpt metadata', fontsize=12)\nfig.tight_layout()\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T15:03:43.104211Z","iopub.execute_input":"2021-06-03T15:03:43.104574Z","iopub.status.idle":"2021-06-03T15:03:43.820325Z","shell.execute_reply.started":"2021-06-03T15:03:43.104539Z","shell.execute_reply":"2021-06-03T15:03:43.819475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that there are outliers across all the features, especially for mean words per sentence, with a few overly long sentences. Also, these features will need to be scaled before being used in any model, considering the variance in their ranges.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(12,8), sharey=True)\nfor i, column in enumerate(meta_df.columns):\n    plot = sns.scatterplot(\n        data=meta_df,\n        x=column,\n        y=df['target'],\n        ax=ax[i//3, i%3],\n        linewidth=0,\n        alpha=0.2,\n    )\n    plot.set_xlabel(f'{column}', fontsize=14)\n    \nax[2,1].axis('off')\nax[2,2].axis('off')\nfig.suptitle('Scatterplots for excerpt metadata against target', fontsize=16)\nfig.tight_layout()\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T15:03:50.179681Z","iopub.execute_input":"2021-06-03T15:03:50.180039Z","iopub.status.idle":"2021-06-03T15:03:51.241994Z","shell.execute_reply.started":"2021-06-03T15:03:50.180004Z","shell.execute_reply":"2021-06-03T15:03:51.241108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pd.concat([meta_df, df['target']], axis=1).corr().drop('target', axis=1).drop(meta_df.columns, axis=0)\nfig = plt.figure(figsize=(10,3))\nplot = sns.heatmap(\n    data=corr,\n    vmin=-1,\n    vmax=1,\n    linewidth=2,\n    annot=True,\n    square=True,\n)\nplot.set_xlabel('Excerpt metadata', size=14)\nplot.set_title('Excerpt metadata correlation with target', size=16)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T15:03:56.199982Z","iopub.execute_input":"2021-06-03T15:03:56.200485Z","iopub.status.idle":"2021-06-03T15:03:56.475818Z","shell.execute_reply.started":"2021-06-03T15:03:56.200433Z","shell.execute_reply":"2021-06-03T15:03:56.475148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Several of these features have a negative correlation with the target. `word_count` and `sentence_count` do not have as strong a correlation, which is reasonable given that overall excerpt length was probably somewhat engineered to be standard across all records.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(6,6))\nplot = sns.scatterplot(\n    data=df,\n    x=df['excerpt'].transform(lambda excerpt: len(regexp_tokenize(excerpt, pattern='\\w+'))),\n    y=df['target'],\n    linewidth=0,\n)\nplot.set_xlabel('raw excerpt word count')\nplot.set_title('Raw excerpt length v. target', size=16)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:00.863842Z","iopub.execute_input":"2021-06-03T14:46:00.864237Z","iopub.status.idle":"2021-06-03T14:46:01.210817Z","shell.execute_reply.started":"2021-06-03T14:46:00.864195Z","shell.execute_reply":"2021-06-03T14:46:01.209896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This figure supports the previous note, as the raw excerpt word count has no visual correlation with the target.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10,10))\nmask = np.triu(np.ones_like(meta_df.corr(), dtype=np.bool))\nplot = sns.heatmap(\n    data=meta_df.corr(),\n    mask=mask,\n    vmin=-1,\n    vmax=1,\n    annot=True,\n    linewidth=2,\n    square=True,\n)\nplot.set_title('Correlation heatmap of excerpt metadata', fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:01.211965Z","iopub.execute_input":"2021-06-03T14:46:01.212233Z","iopub.status.idle":"2021-06-03T14:46:01.570571Z","shell.execute_reply.started":"2021-06-03T14:46:01.212206Z","shell.execute_reply":"2021-06-03T14:46:01.569442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes:\n* `char_count` is highly positively correlated with `word_count` and `polysyllables`.\n* `mean_syllables`, `mean_chars_per_word`, and `polysyllables` are all highly positively correlated with each other.","metadata":{}},{"cell_type":"markdown","source":"## Parts of speech","metadata":{}},{"cell_type":"markdown","source":"Parts of speech tagging is best done without any lemmatization or removal of stop words, so excerpts from `CLEANED_EXCERPTS` shouldn't be used.","metadata":{}},{"cell_type":"markdown","source":"### Tagging function","metadata":{}},{"cell_type":"code","source":"def excerpt_pos_tag(excerpt):\n    \"\"\"\n    Take a raw excerpt and return a list of sentences of parts of speech tags\n    \n    Keyword arguments:\n        excerpt: the raw excerpt\n        \n    Returns:\n        pos_tags (list): list of each sentence in the excerpt as a list of parts of speech tags\n    \"\"\"\n    clean_curried = lambda sentence: clean(sentence, remove_stopwords=False, lemmatize=False)\n    return [[tag[1] for tag in pos_tag(clean_curried(sentence))] for sentence in sent_tokenize(excerpt)]","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:01.575764Z","iopub.execute_input":"2021-06-03T14:46:01.576065Z","iopub.status.idle":"2021-06-03T14:46:01.580931Z","shell.execute_reply.started":"2021-06-03T14:46:01.576037Z","shell.execute_reply":"2021-06-03T14:46:01.58025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Example with the first excerpt","metadata":{}},{"cell_type":"code","source":"pos_tags = excerpt_pos_tag(df['excerpt'][0])\nfor sentence in pos_tags:\n    print(sentence)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:01.582783Z","iopub.execute_input":"2021-06-03T14:46:01.58322Z","iopub.status.idle":"2021-06-03T14:46:01.615108Z","shell.execute_reply.started":"2021-06-03T14:46:01.583172Z","shell.execute_reply":"2021-06-03T14:46:01.614171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tag all excerpts","metadata":{}},{"cell_type":"code","source":"POS_TAGGED_EXCERPTS = df['excerpt'].transform(excerpt_pos_tag)\nPOS_TAGGED_EXCERPTS","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:01.617035Z","iopub.execute_input":"2021-06-03T14:46:01.617318Z","iopub.status.idle":"2021-06-03T14:46:31.429372Z","shell.execute_reply.started":"2021-06-03T14:46:01.617289Z","shell.execute_reply":"2021-06-03T14:46:31.428658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tag counts","metadata":{}},{"cell_type":"code","source":"def tag_count(pos_tags):\n    \"\"\"\n    Takes a list of tags separated into sentences, and returns a counter of all tags\n    \n    Keyword Arguments:\n        pos_tags: a list of each sentence as a list of tags.\n        \n    Returns:\n        tag_count_dict: a dict with counts for each tag\n    \"\"\"\n    return dict(Counter([tag for sentence in pos_tags for tag in sentence]))","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:31.430331Z","iopub.execute_input":"2021-06-03T14:46:31.430712Z","iopub.status.idle":"2021-06-03T14:46:31.434566Z","shell.execute_reply.started":"2021-06-03T14:46:31.430683Z","shell.execute_reply":"2021-06-03T14:46:31.433907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An example tag count with the first excerpt","metadata":{}},{"cell_type":"code","source":"tag_count(POS_TAGGED_EXCERPTS[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:31.435411Z","iopub.execute_input":"2021-06-03T14:46:31.435756Z","iopub.status.idle":"2021-06-03T14:46:31.450663Z","shell.execute_reply.started":"2021-06-03T14:46:31.43573Z","shell.execute_reply":"2021-06-03T14:46:31.450021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now count tags for all excerpts, with a column for each tag.","metadata":{}},{"cell_type":"code","source":"POS_TAG_COUNTS = POS_TAGGED_EXCERPTS.transform(tag_count)\npos_tag_counts_df = POS_TAG_COUNTS.apply(pd.Series).fillna(0).astype(int)\npos_tag_counts_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:31.451537Z","iopub.execute_input":"2021-06-03T14:46:31.451908Z","iopub.status.idle":"2021-06-03T14:46:32.544756Z","shell.execute_reply.started":"2021-06-03T14:46:31.451878Z","shell.execute_reply":"2021-06-03T14:46:32.543884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_zero_pos_tag_counts_df = pd.DataFrame((RECORDS - (pos_tag_counts_df == 0).astype(int).sum(axis=0)))\nnon_zero_pos_tag_counts_df = non_zero_pos_tag_counts_df.sort_values(0, axis=0, ascending=False)\nnon_zero_pos_tag_counts_df = non_zero_pos_tag_counts_df.reset_index()\nnon_zero_pos_tag_counts_df.columns = ['tag', 'non_zero_count']\nfig = plt.figure(figsize=(16,6))\nplot = sns.barplot(\n    data=non_zero_pos_tag_counts_df,\n    x= 'tag',\n    y='non_zero_count',\n    color=sns.color_palette('deep')[0],\n)\nfor index, row in non_zero_pos_tag_counts_df.iterrows():\n    plot.text(index,row.non_zero_count+20, row.non_zero_count, color='black', ha='center')\nplot.set_xticklabels(plot.get_xticklabels(), size=14, rotation=45)\nplot.set_title('Number of non zero counts for parts of speech tags', fontsize=16)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:32.545903Z","iopub.execute_input":"2021-06-03T14:46:32.546186Z","iopub.status.idle":"2021-06-03T14:46:33.093536Z","shell.execute_reply.started":"2021-06-03T14:46:32.54616Z","shell.execute_reply":"2021-06-03T14:46:33.092654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some tags are overwhelmingly absent in most excerpts, and likely shouldn't be included as counts in the modelling. Also note the SYM is missing, a result of our tokenization process removing punctuation.","metadata":{}},{"cell_type":"code","source":"USEFUL_TAGS = non_zero_pos_tag_counts_df['tag'].head(25)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:33.09475Z","iopub.execute_input":"2021-06-03T14:46:33.095048Z","iopub.status.idle":"2021-06-03T14:46:33.098981Z","shell.execute_reply.started":"2021-06-03T14:46:33.095018Z","shell.execute_reply":"2021-06-03T14:46:33.098118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\nmask = np.triu(np.ones_like(pos_tag_counts_df.corr(), dtype=np.bool))\nplot = sns.heatmap(\n    data=pos_tag_counts_df.corr(),\n    mask=mask,\n    vmin=-1,\n    vmax=1,\n    linewidth=0,\n    square=True,\n)\nplot.set_title('Correlation heatmap of POS tag counts', size=16)\nfig.tight_layout()\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:33.099815Z","iopub.execute_input":"2021-06-03T14:46:33.100077Z","iopub.status.idle":"2021-06-03T14:46:33.746397Z","shell.execute_reply.started":"2021-06-03T14:46:33.100052Z","shell.execute_reply":"2021-06-03T14:46:33.74543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pd.concat([meta_df, pos_tag_counts_df], axis=1).corr().drop(pos_tag_counts_df.columns, axis=0).drop(meta_df.columns, axis=1)\nfig = plt.figure(figsize=(12,3))\nplot = sns.heatmap(\n    data=corr,\n    vmin=-1,\n    vmax=1,\n    linewidth=0,\n    square=True,\n)\nplot.set_xlabel('POS tag counts', size=14)\nplot.set_ylabel('Excerpt metadata', size=14)\nplot.set_title('Heatmap of POS tag count correlation with excerpt metadata', size=16)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:33.747478Z","iopub.execute_input":"2021-06-03T14:46:33.747749Z","iopub.status.idle":"2021-06-03T14:46:34.353258Z","shell.execute_reply.started":"2021-06-03T14:46:33.747714Z","shell.execute_reply":"2021-06-03T14:46:34.352544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pd.concat([pos_tag_counts_df, df['target']], axis=1).corr().drop('target', axis=1).drop(pos_tag_counts_df.columns, axis=0)\nfig = plt.figure(figsize=(12,2))\nplot = sns.heatmap(\n    data=corr,\n    vmin=-1,\n    vmax=1,\n    linewidth=2,\n    annot=True,\n    annot_kws={'rotation': 90}\n)\nplot.set_xlabel('POS tag count', size=14)\nplot.set_title('POS tag count correlation with target', size=16)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:34.35431Z","iopub.execute_input":"2021-06-03T14:46:34.354708Z","iopub.status.idle":"2021-06-03T14:46:34.914187Z","shell.execute_reply.started":"2021-06-03T14:46:34.354667Z","shell.execute_reply":"2021-06-03T14:46:34.913516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some POS tag counts are slightly correlated with target, but none are significant.","metadata":{}},{"cell_type":"markdown","source":"## Readability Formulas","metadata":{}},{"cell_type":"code","source":"readability_df = pd.DataFrame()\n\n# Coleman-Liau Index\nreadability_df['cli'] = (0.0588 * meta_df['mean_chars_per_word']*100) - (0.296 * 100/meta_df['mean_words_per_sent']) - 15.8\n\n# Flesch-kincaid Grade Level\nreadability_df['fkgl'] = (0.39 * meta_df['mean_words_per_sent']) + (11.8 * meta_df['mean_syllables']) - 15.59\n\n# Flesch Reading Ease\nreadability_df['fre'] = 206.835 - (84.6 * (meta_df['mean_syllables'] / meta_df['mean_words_per_sent'])) - (1.015 * (meta_df['mean_words_per_sent'] / meta_df['sent_count']))\n\n# Simple Measure of Gobbledygook\nreadability_df['smog'] = (1.043 * np.sqrt(meta_df['polysyllables'] * (30 / meta_df['sent_count']))) + 3.1291\n\n# Gunning Fog Index\nreadability_df['gfi'] = 0.4 * ((meta_df['word_count'] / meta_df['sent_count']) + ((meta_df['polysyllables'] / meta_df['word_count']) * 100))","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:34.915224Z","iopub.execute_input":"2021-06-03T14:46:34.915596Z","iopub.status.idle":"2021-06-03T14:46:34.93253Z","shell.execute_reply.started":"2021-06-03T14:46:34.915566Z","shell.execute_reply":"2021-06-03T14:46:34.931636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizations","metadata":{}},{"cell_type":"code","source":"readability_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:34.933749Z","iopub.execute_input":"2021-06-03T14:46:34.934035Z","iopub.status.idle":"2021-06-03T14:46:34.953353Z","shell.execute_reply.started":"2021-06-03T14:46:34.934007Z","shell.execute_reply":"2021-06-03T14:46:34.952543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, len(readability_df.columns), figsize=(16,6))\nfor i, column in enumerate(readability_df.columns):\n    plot = sns.boxenplot(\n        data=readability_df,\n        y=column,\n        ax=ax[i],\n        linewidth=1,\n        width=.5,\n    )\n    plot.set_ylabel(f'\\n{column}', fontsize=14)\nfig.suptitle('Boxenplots for readability formulas', fontsize=12)\nfig.tight_layout()\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:34.954881Z","iopub.execute_input":"2021-06-03T14:46:34.955353Z","iopub.status.idle":"2021-06-03T14:46:35.491639Z","shell.execute_reply.started":"2021-06-03T14:46:34.955311Z","shell.execute_reply":"2021-06-03T14:46:35.490763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 3, figsize=(12,8), sharey=True)\nfor i, column in enumerate(readability_df.columns):\n    plot = sns.scatterplot(\n        data=readability_df,\n        x=column,\n        y=df['target'],\n        ax=ax[i//3, i%3],\n        linewidth=0,\n        alpha=0.2,\n    )\n    plot.set_xlabel(f'{column}', fontsize=14)\nax[1,2].axis('off')\nfig.suptitle('Scatterplots for readability formulas against target', fontsize=16)\nfig.tight_layout()\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:35.49286Z","iopub.execute_input":"2021-06-03T14:46:35.493143Z","iopub.status.idle":"2021-06-03T14:46:36.32097Z","shell.execute_reply.started":"2021-06-03T14:46:35.493114Z","shell.execute_reply":"2021-06-03T14:46:36.319945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,5))\nmask = np.triu(np.ones_like(readability_df.corr(), dtype=np.bool))\nplot = sns.heatmap(\n    data=readability_df.corr(),\n    mask=mask,\n    vmin=-1,\n    vmax=1,\n    annot=True,\n    linewidth=2,\n    square=True,\n)\nplot.set_title('Correlation heatmap of readability formulas', size=16)\nfig.tight_layout()\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:36.322468Z","iopub.execute_input":"2021-06-03T14:46:36.322891Z","iopub.status.idle":"2021-06-03T14:46:36.686251Z","shell.execute_reply.started":"2021-06-03T14:46:36.322844Z","shell.execute_reply":"2021-06-03T14:46:36.685348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The various readability formulas are highly correlated with eachother, except for `fre`.","metadata":{}},{"cell_type":"code","source":"corr = pd.concat([meta_df, readability_df], axis=1).corr().drop(readability_df.columns, axis=0).drop(meta_df.columns, axis=1)\nfig = plt.figure(figsize=(10,10))\nplot = sns.heatmap(\n    data=corr,\n    vmin=-1,\n    vmax=1,\n    linewidth=2,\n    annot=True,\n    square=True,\n)\nplot.set_xlabel('Readability formulas', size=14)\nplot.set_ylabel('Excerpt metadata', size=14)\nplot.set_title('Heatmap of readability formula correlation with excerpt metadata', size=16)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:36.687452Z","iopub.execute_input":"2021-06-03T14:46:36.687711Z","iopub.status.idle":"2021-06-03T14:46:37.26571Z","shell.execute_reply.started":"2021-06-03T14:46:36.687685Z","shell.execute_reply":"2021-06-03T14:46:37.264848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the readability formulas use the excerpt metadata, it would be expected that they are highly correlated with each other. This is especially the case for `mean_chars_per_word`, `mean_syllables`, and `polysyllables` with all readability formulas except `fre`. `fre` is highly negatively correlated with `sent_count`.","metadata":{}},{"cell_type":"markdown","source":"# Final Data Preparation","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:37.266861Z","iopub.execute_input":"2021-06-03T14:46:37.267142Z","iopub.status.idle":"2021-06-03T14:46:37.270814Z","shell.execute_reply.started":"2021-06-03T14:46:37.267112Z","shell.execute_reply":"2021-06-03T14:46:37.269877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating training and test set","metadata":{}},{"cell_type":"code","source":"X = pd.concat([meta_df, pos_tag_counts_df, readability_df], axis=1).copy()\n# Remove features that are too dependent on excerpt length\nX = X.drop(['word_count', 'sent_count'], axis=1)\n\n# Remove tags that don't appear in many excerpts\ndropped_tags = [tag for tag in non_zero_pos_tag_counts_df['tag'] if tag not in list(USEFUL_TAGS)]\nX = X.drop(dropped_tags, axis=1)\n\ny = pd.DataFrame(df['target'])\n\n# Scale features in X\nscaler = RobustScaler()\nX[X.columns] = scaler.fit_transform(X[X.columns])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=75)\nfor name, split in {\n    'X_train': X_train, \n    'y_train': y_train, \n    'X_test': X_test, \n    'y_test': y_test,\n}.items():\n    descripter = f'The shape of {name} is:'\n    print(f'{descripter:<24} {split.shape}')\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:37.271972Z","iopub.execute_input":"2021-06-03T14:46:37.272233Z","iopub.status.idle":"2021-06-03T14:46:37.368628Z","shell.execute_reply.started":"2021-06-03T14:46:37.272209Z","shell.execute_reply":"2021-06-03T14:46:37.367635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:37.370194Z","iopub.execute_input":"2021-06-03T14:46:37.370617Z","iopub.status.idle":"2021-06-03T14:46:37.375239Z","shell.execute_reply.started":"2021-06-03T14:46:37.370574Z","shell.execute_reply":"2021-06-03T14:46:37.374591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling Helper Function","metadata":{}},{"cell_type":"code","source":"def fit_score_model(regressor):\n    \"\"\"\n    Fits and scores a model by predicting the test set and calculating accuracy as RMSE\n    \n    Keyword Parameters:\n        regressor: the model to use\n        \n    Returns:\n        result (dict): a dictionary with the model and accuracy\n    \"\"\"\n    model = regressor.fit(X_train, np.ravel(y_train))\n\n    y_pred = model.predict(X_test)\n    accuracy = mean_squared_error(y_test, y_pred, squared=False)\n\n    return {'model': model, 'accuracy': accuracy}","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:46:37.376121Z","iopub.execute_input":"2021-06-03T14:46:37.376361Z","iopub.status.idle":"2021-06-03T14:46:37.388219Z","shell.execute_reply.started":"2021-06-03T14:46:37.376327Z","shell.execute_reply":"2021-06-03T14:46:37.387413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_results(name, regressor, param_grid=None):\n    model = fit_score_model(regressor)\n    print(f'{name} RMSE no tuning: {round(model[\"accuracy\"], 3)}')\n\n    # Parameter tuning\n    if param_grid is not None:\n        regressor = GridSearchCV(\n            regressor, \n            param_grid,\n        )\n        tuned_model = fit_score_model(regressor)\n        print(f'{name} RMSE: {round(tuned_model[\"accuracy\"], 3)}')\n        return tuned_model\n    else:\n        return model","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:30.382365Z","iopub.execute_input":"2021-06-03T14:47:30.382766Z","iopub.status.idle":"2021-06-03T14:47:30.388357Z","shell.execute_reply.started":"2021-06-03T14:47:30.382731Z","shell.execute_reply":"2021-06-03T14:47:30.387424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"linear_model = model_results('Linear Regression', LinearRegression())","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:30.427837Z","iopub.execute_input":"2021-06-03T14:47:30.42817Z","iopub.status.idle":"2021-06-03T14:47:30.455481Z","shell.execute_reply.started":"2021-06-03T14:47:30.428141Z","shell.execute_reply":"2021-06-03T14:47:30.454467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ridge","metadata":{}},{"cell_type":"code","source":"parameters = {\n    'alpha': [0.01, 0.1, 1.0, 10.0],\n}\nridge_model = model_results('Ridge', Ridge(), parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:30.465541Z","iopub.execute_input":"2021-06-03T14:47:30.466118Z","iopub.status.idle":"2021-06-03T14:47:30.796146Z","shell.execute_reply.started":"2021-06-03T14:47:30.466069Z","shell.execute_reply":"2021-06-03T14:47:30.79496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lasso","metadata":{}},{"cell_type":"code","source":"parameters = {\n    'alpha': [0.01, 0.1, 1.0, 10.0],\n}\nlasso_model = model_results('Lasso', Lasso(), parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:30.798143Z","iopub.execute_input":"2021-06-03T14:47:30.800264Z","iopub.status.idle":"2021-06-03T14:47:31.602636Z","shell.execute_reply.started":"2021-06-03T14:47:30.800217Z","shell.execute_reply":"2021-06-03T14:47:31.601466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Elastic-Net","metadata":{}},{"cell_type":"code","source":"parameters = {\n    'alpha': [0.01, 0.1, 1.0, 10.0],\n    'l1_ratio': [0.125, 0.25, 0.5, 0.75, 0.875],\n}\nelastic_model = model_results('Elastic-Net', ElasticNet(), parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:31.605214Z","iopub.execute_input":"2021-06-03T14:47:31.607712Z","iopub.status.idle":"2021-06-03T14:47:34.250267Z","shell.execute_reply.started":"2021-06-03T14:47:31.607665Z","shell.execute_reply":"2021-06-03T14:47:34.249212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stochastic Gradient Descent","metadata":{}},{"cell_type":"code","source":"param_grid = [\n    {'penalty': ['l2', 'l1'], 'alpha': [0.01, 0.1, 1.0, 10.0],},\n    {'penalty': ['elasticnet'], 'alpha': [0.01, 0.1, 1.0, 10.0], 'l1_ratio': [0.125, 0.25, 0.5, 0.75, 0.875],}\n]\nsgd_model = model_results('Stochastic Gradient Descent', SGDRegressor(random_state=54), param_grid)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:34.252096Z","iopub.execute_input":"2021-06-03T14:47:34.254078Z","iopub.status.idle":"2021-06-03T14:47:37.533363Z","shell.execute_reply.started":"2021-06-03T14:47:34.254037Z","shell.execute_reply":"2021-06-03T14:47:37.532231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Support Vector Machine","metadata":{}},{"cell_type":"code","source":"linear_svm_model = model_results('Linear Support Vector Machine', SVR(kernel='linear'))","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:37.535068Z","iopub.execute_input":"2021-06-03T14:47:37.535504Z","iopub.status.idle":"2021-06-03T14:47:38.517693Z","shell.execute_reply.started":"2021-06-03T14:47:37.53546Z","shell.execute_reply":"2021-06-03T14:47:38.516769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = {\n    'degree': [2,3,4,5],\n    'gamma': ['scale', 'auto'],\n}\npoly_svm_model = model_results('Poly Support Vector Machine', SVR(kernel='poly'), parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:38.51894Z","iopub.execute_input":"2021-06-03T14:47:38.519214Z","iopub.status.idle":"2021-06-03T14:47:47.44549Z","shell.execute_reply.started":"2021-06-03T14:47:38.519185Z","shell.execute_reply":"2021-06-03T14:47:47.44448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"parameters = {\n    'n_neighbors': [1,2,4,8,16,32,64],\n    'weights': ['uniform', 'distance'],\n}\nknearest_model = model_results('K-Nearest Neighbors', KNeighborsRegressor(), parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:47.446621Z","iopub.execute_input":"2021-06-03T14:47:47.447039Z","iopub.status.idle":"2021-06-03T14:47:49.34177Z","shell.execute_reply.started":"2021-06-03T14:47:47.447Z","shell.execute_reply":"2021-06-03T14:47:49.340731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The currently best performing model is Ridge, with an RMSE of 0.756.","metadata":{}},{"cell_type":"code","source":"SELECTED_MODEL = ridge_model","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:49.344608Z","iopub.execute_input":"2021-06-03T14:47:49.345695Z","iopub.status.idle":"2021-06-03T14:47:49.351685Z","shell.execute_reply.started":"2021-06-03T14:47:49.345646Z","shell.execute_reply":"2021-06-03T14:47:49.350657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating submission file","metadata":{}},{"cell_type":"code","source":"submission_df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\n\nSUBMISSION_CLEANED_EXCERPTS = submission_df['excerpt'].transform(lambda excerpt: [clean(sentence) for sentence in sent_tokenize(excerpt)])\n\nsubmission_df['char_count'] = SUBMISSION_CLEANED_EXCERPTS.transform(lambda excerpt: sum([len(word) for sentence in excerpt for word in sentence]))\nsubmission_df['word_count'] = SUBMISSION_CLEANED_EXCERPTS.transform(lambda excerpt: len([word for sentence in excerpt for word in sentence]))\nsubmission_df['sent_count'] = SUBMISSION_CLEANED_EXCERPTS.transform(len)\nsubmission_df['mean_words_per_sent'] = SUBMISSION_CLEANED_EXCERPTS.transform(mean_words_per_sent)\nsubmission_df['mean_chars_per_word'] = SUBMISSION_CLEANED_EXCERPTS.transform(mean_chars_per_word)\nsubmission_df['mean_syllables'] = SUBMISSION_CLEANED_EXCERPTS.transform(mean_syllable_count_per_word)\nsubmission_df['polysyllables'] = SUBMISSION_CLEANED_EXCERPTS.transform(polysyllables)\n\n\nSUBMISSION_POS_TAGGED_EXCERPTS = submission_df['excerpt'].transform(excerpt_pos_tag)\nSUBMISSION_POS_TAG_COUNTS = SUBMISSION_POS_TAGGED_EXCERPTS.transform(tag_count)\nsubmission_tag_counts_df = SUBMISSION_POS_TAG_COUNTS.apply(pd.Series).fillna(0).astype(int)\nsubmission_tag_counts_df = submission_tag_counts_df.drop([tag for tag in submission_tag_counts_df.columns if tag not in list(USEFUL_TAGS)], axis=1)\nsubmission_df = pd.concat([submission_df, submission_tag_counts_df], axis=1)\n\n\nsubmission_df['cli'] = (0.0588 * submission_df['mean_chars_per_word']*100) - (0.296 * 100/submission_df['mean_words_per_sent']) - 15.8\nsubmission_df['fkgl'] = (0.39 * submission_df['mean_words_per_sent']) + (11.8 * submission_df['mean_syllables']) - 15.59\nsubmission_df['fre'] = 206.835 - (84.6 * (submission_df['mean_syllables'] / submission_df['mean_words_per_sent'])) - (1.015 * (submission_df['mean_words_per_sent'] / submission_df['sent_count']))\nsubmission_df['smog'] = (1.043 * np.sqrt(submission_df['polysyllables'] * (30 / submission_df['sent_count']))) + 3.1291\nsubmission_df['gfi'] = 0.4 * ((submission_df['word_count'] / submission_df['sent_count']) + ((submission_df['polysyllables'] / submission_df['word_count']) * 100))\n\nsubmission_df = submission_df.drop(['url_legal', 'license', 'excerpt'], axis=1)\nsubmission_df = submission_df.drop(['word_count', 'sent_count'], axis=1)\n\nids = submission_df['id']\nsubmission_df = submission_df.drop('id', axis=1)\n\nsubmission_df[submission_df.columns] = scaler.transform(submission_df[submission_df.columns])\n\nsubmission_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:47:49.356936Z","iopub.execute_input":"2021-06-03T14:47:49.357746Z","iopub.status.idle":"2021-06-03T14:47:49.682402Z","shell.execute_reply.started":"2021-06-03T14:47:49.357696Z","shell.execute_reply":"2021-06-03T14:47:49.68147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = pd.DataFrame([ids, SELECTED_MODEL['model'].predict(submission_df)])\nfinal = final.transpose()\nfinal.columns = ['id', 'target']\nfinal.index= final['id']\nfinal = final.drop('id', axis=1)\nfinal","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:50:06.773903Z","iopub.execute_input":"2021-06-03T14:50:06.774267Z","iopub.status.idle":"2021-06-03T14:50:06.789187Z","shell.execute_reply.started":"2021-06-03T14:50:06.774233Z","shell.execute_reply":"2021-06-03T14:50:06.788345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:50:14.931577Z","iopub.execute_input":"2021-06-03T14:50:14.93197Z","iopub.status.idle":"2021-06-03T14:50:14.937717Z","shell.execute_reply.started":"2021-06-03T14:50:14.931934Z","shell.execute_reply":"2021-06-03T14:50:14.936668Z"},"trusted":true},"execution_count":null,"outputs":[]}]}