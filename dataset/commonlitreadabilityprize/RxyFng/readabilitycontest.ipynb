{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-20T00:48:55.210576Z","iopub.execute_input":"2021-06-20T00:48:55.210927Z","iopub.status.idle":"2021-06-20T00:48:55.238588Z","shell.execute_reply.started":"2021-06-20T00:48:55.21089Z","shell.execute_reply":"2021-06-20T00:48:55.237777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nimport sys\nimport warnings\nimport csv\nimport collections\nimport re\nimport nltk\nimport time\nimport random\nimport os\nimport matplotlib.pyplot as plt\n\nfrom transformers import *\n# sys.stderr = open('stderr_output.txt', 'w')\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:55.241348Z","iopub.execute_input":"2021-06-20T00:48:55.241591Z","iopub.status.idle":"2021-06-20T00:48:55.250039Z","shell.execute_reply.started":"2021-06-20T00:48:55.241566Z","shell.execute_reply":"2021-06-20T00:48:55.24918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/commonlitreadabilityprize/train.csv'\nTEST_PATH = '/kaggle/input/commonlitreadabilityprize/test.csv'\n\nSEED=777\n\nrandom.seed(SEED)\nos.environ['PYTHONASSEED'] = str(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:55.253123Z","iopub.execute_input":"2021-06-20T00:48:55.253376Z","iopub.status.idle":"2021-06-20T00:48:55.261804Z","shell.execute_reply.started":"2021-06-20T00:48:55.253353Z","shell.execute_reply":"2021-06-20T00:48:55.261102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ENGLISTH_STOPWORDS = set(nltk.corpus.stopwords.words('english')) \ndef preprocess_text(txt):\n    # code for removing stop words, but to be honest, would only not be that important...\n    filtered_sentence = txt.lower()\n#     word_tokens = nltk.tokenize.word_tokenize(filtered_sentence) \n#     filtered_sentence = ' '.join([w.lower() for w in word_tokens if not w.lower() in ENGLISTH_STOPWORDS])\n\n#     # remove digits\n    filtered_sentence = ''.join(chr for chr in filtered_sentence if not chr.isdigit())\n   \n    return filtered_sentence\n  \n\n\nall_data = [] # (id, sentence, score, stderr)\nall_tests = [] # (id, sentence)\n\nwith open(TRAIN_PATH) as f_dt:\n    dt_reader = csv.DictReader(f_dt)\n    for dt_row in dt_reader:\n        all_data.append((dt_row['id'], preprocess_text(dt_row['excerpt']), dt_row['target'], dt_row['standard_error']))\n\n\nwith open(TEST_PATH) as f_tst:\n    tst_reader = csv.DictReader(f_tst)\n    for tst_row in tst_reader:\n        all_tests.append((tst_row['id'], preprocess_text(tst_row['excerpt'])))\n\n        \nn_sent_data = [len(nltk.tokenize.sent_tokenize(dt_i[1])) for dt_i in all_data]\nn_sent_tests = [len(nltk.tokenize.sent_tokenize(tst_i[1])) for tst_i in all_tests]\n        \nprint(np.mean(n_sent_data), max(n_sent_data), min(n_sent_data), len(all_data))\nprint(np.mean(n_sent_tests), max(n_sent_tests), min(n_sent_tests), len(all_tests))\n\n\ntotal_data_pts = len(all_data)\nnp.random.shuffle(all_data)\ntraining_data = all_data[: int(0.9 * total_data_pts)]\nval_data = all_data[int(0.9 * total_data_pts): ]\nprint(len(training_data), len(val_data))\nprint(len(all_data))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:55.264874Z","iopub.execute_input":"2021-06-20T00:48:55.265118Z","iopub.status.idle":"2021-06-20T00:48:56.7455Z","shell.execute_reply.started":"2021-06-20T00:48:55.265094Z","shell.execute_reply":"2021-06-20T00:48:56.744578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # debug code\n# sample_sentences = [\n#     'i am training something on a contest', \n#     'which might make me feel good', \n#     'but to be honest i am a bit worried'\n# ]\n\n\n# sample_tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased')\n# sample_model = BertForSequenceClassification.from_pretrained('/kaggle/input/bert-base-uncased')\n\n# for snm, spr in sample_model.named_parameters():\n#     print(snm)\n\n# sample_result = sample_tokenizer(sample_sentences, padding=True, return_tensors='pt')\n# print(sample_result)\n# print(sample_model(sample_result['input_ids'], attention_mask=sample_result['attention_mask']))\n# print(sample_model(sample_result['input_ids'], attention_mask=None))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.746931Z","iopub.execute_input":"2021-06-20T00:48:56.747441Z","iopub.status.idle":"2021-06-20T00:48:56.75142Z","shell.execute_reply.started":"2021-06-20T00:48:56.747402Z","shell.execute_reply":"2021-06-20T00:48:56.750556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fold_divide(data, val_fold_idx, n_folds):\n    total_data = len(data)\n    fold_stride = (total_data // n_folds) + 1\n    fold_low, fold_high = val_fold_idx * fold_stride, (val_fold_idx + 1) * fold_stride\n    return [*data[:fold_low], *data[fold_high:]], data[fold_low: fold_high]\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.752677Z","iopub.execute_input":"2021-06-20T00:48:56.753006Z","iopub.status.idle":"2021-06-20T00:48:56.768783Z","shell.execute_reply.started":"2021-06-20T00:48:56.752973Z","shell.execute_reply":"2021-06-20T00:48:56.767905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProgressBar:\n    def __init__(self, total_dt, tabs):\n        self.total_data = total_dt\n        self.done_data = 0\n        self.printed = 0\n        \n        self.tabs = tabs\n    \n    def print_init(self):\n        print('-' * self.tabs)\n    \n    def update(self, num):\n        self.done_data += num\n        while self.printed < int((self.done_data / self.total_data) * self.tabs):\n            print('>', end='')\n            self.printed += 1\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.770617Z","iopub.execute_input":"2021-06-20T00:48:56.770897Z","iopub.status.idle":"2021-06-20T00:48:56.779988Z","shell.execute_reply.started":"2021-06-20T00:48:56.770875Z","shell.execute_reply":"2021-06-20T00:48:56.779247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextReorderBatchLoader:\n    # default no re-order\n    def __init__(self, data_lst, batch_size, **overrides):\n        sample_params = {\n            'trunc_lower': 1.0,\n            'trunc_higher': 1.0,\n            'trunc_min': 1,\n            'trunc_shuffle': False,\n            'iter_percentage': 1\n        }\n        sample_params = {\n            **sample_params,\n            **overrides\n        }\n        \n        self.sample_lower = sample_params['trunc_lower']\n        self.sample_range = sample_params['trunc_higher'] - self.sample_lower\n        self.sample_min = sample_params['trunc_min']\n        self.sample_shuffle = sample_params['trunc_shuffle']\n        \n        self.iter_percentage=sample_params['iter_percentage']\n        \n        self.data_list = [*data_lst]\n        self.batch_size = batch_size\n        \n        np.random.shuffle(self.data_list)\n    \n    def __iter__(self):\n        \n        np.random.shuffle(self.data_list)\n        # since randomly shuffled each time so only need to worry about only sampling the front\n        for idx in range(0, len(self), self.batch_size):\n            all_texts = []\n            all_scores = []\n            all_stds = []\n            for itm_i in self.data_list[idx: idx + self.batch_size]:\n                txt_i, scr_i, std_i = itm_i[1], float(itm_i[2]), float(itm_i[3])\n                all_texts.append(self.random_sample_text(txt_i))\n                all_scores.append(scr_i)\n                all_stds.append(std_i)\n            \n            yield all_texts, all_scores, all_stds\n    def random_sample_text(self, s):\n        # len_kept = int((np.random.random() * self.sample_range + self.sample_lower) * len(s))\n        # len_start = np.random.randint(0, high=len(s) - len_kept)\n        # return s[len_start: len_start+len_kept]\n        s_sentences = nltk.tokenize.sent_tokenize(s)\n        sentences_kept = max([\n            int((np.random.random() * self.sample_range + self.sample_lower) * len(s_sentences)), \n            self.sample_min\n        ])\n        if self.sample_shuffle:\n            np.random.shuffle(s_sentences) \n        \n        sentences_kept_start = np.random.randint(0, high = max([1, len(s_sentences) - sentences_kept]))\n        return ' '.join(s_sentences[sentences_kept_start: sentences_kept_start + sentences_kept])\n    \n    def __len__(self):\n        return int(self.iter_percentage * len(self.data_list))\n                                            ","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.783766Z","iopub.execute_input":"2021-06-20T00:48:56.783999Z","iopub.status.idle":"2021-06-20T00:48:56.798894Z","shell.execute_reply.started":"2021-06-20T00:48:56.783977Z","shell.execute_reply":"2021-06-20T00:48:56.798081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# debug code\n# sample_tokenizer = PARAMS['tokenizer_class'].from_pretrained(PARAMS['token_path'])\n# sample_loader = FullReorderBatchLoader(val_data, 10, sample_tokenizer)\n\n# for s_txt_tsr, s_scr, s_std in sample_loader:\n#     print(s_txt_tsr, s_scr, s_std, sep='\\n')\n#     break","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.800779Z","iopub.execute_input":"2021-06-20T00:48:56.801126Z","iopub.status.idle":"2021-06-20T00:48:56.810764Z","shell.execute_reply.started":"2021-06-20T00:48:56.801092Z","shell.execute_reply":"2021-06-20T00:48:56.809835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ContinuousWrapper(torch.nn.Module):\n    def __init__(self, \n                 pre_trained_cls, \n                 pretrained_model_path,\n                 freeze_param_condition=lambda name: False):\n        super().__init__()\n        self.pre_model = pre_trained_cls.from_pretrained(pretrained_model_path, num_labels = 1)\n        for pre_model_param_name, pre_param in self.pre_model.named_parameters():\n            if freeze_param_condition(pre_model_param_name):\n                pre_param.requires_grad = False\n                print('freezing {0}'.format(pre_model_param_name))\n        \n    def forward(self, x, mask=None):\n        return self.pre_model(x, attention_mask=mask).logits\n    \n    def loss(self, x, scores, mask=None, weights=None):\n        if weights is None:\n            weights = torch.ones(scores.shape).to(scores.device)\n        s_pred = torch.reshape(self(x, mask), scores.shape)\n        loss_pred = torch.sqrt(torch.mean(torch.pow((scores - s_pred) * weights, 2)))\n        return loss_pred\n    \n    def make_prediction(self, x, mask=None, strategy='none', *strategy_args):\n        return torch.reshape(self(x, mask), (x.shape[0],))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.813553Z","iopub.execute_input":"2021-06-20T00:48:56.814093Z","iopub.status.idle":"2021-06-20T00:48:56.824596Z","shell.execute_reply.started":"2021-06-20T00:48:56.814048Z","shell.execute_reply":"2021-06-20T00:48:56.8237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class REstimator(ContinuousWrapper):\n    def __init__(self, \n                 pre_trained_cls, \n                 pretrained_model_path,\n                 freeze_param_condition=lambda name: False):\n        super().__init__(pre_trained_cls, \n                 pretrained_model_path,\n                 freeze_param_condition)\n        self.shrink_and_expand = torch.nn.Sequential(\n                                    torch.nn.Tanh(),\n                                    torch.nn.Linear(1, 1)\n                                )\n        \n    def forward(self, x, mask=None):\n        raw_v = super().forward(x, mask)\n        \n        return self.shrink_and_expand(raw_v)\n    \n    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.825803Z","iopub.execute_input":"2021-06-20T00:48:56.82616Z","iopub.status.idle":"2021-06-20T00:48:56.83457Z","shell.execute_reply.started":"2021-06-20T00:48:56.826121Z","shell.execute_reply":"2021-06-20T00:48:56.833782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiscreteEstimator(torch.nn.Module):\n    def __init__(self, \n                 pre_trained_cls, \n                 pretrained_model_path, \n                 pred_range, \n                 divisions, \n                 freeze_param_condition=lambda name: False\n                ):\n        super().__init__()\n        self.pre_model = pre_trained_cls.from_pretrained(pretrained_model_path, num_labels = divisions)\n        \n        for pre_model_param_name, pre_param in self.pre_model.named_parameters():\n            if freeze_param_condition(pre_model_param_name):\n                pre_param.requires_grad = False\n                print('freezing {0}'.format(pre_model_param_name))\n        \n        \n        # not a parameter, thus tensor should not be propagated\n        self.value_match_vec = torch.linspace(pred_range[0], pred_range[1], divisions)\n    \n    # this will only return the weights before softmax\n    def forward(self, x, mask=None):\n        return self.pre_model(x, attention_mask=mask).logits\n    \n    \n    def loss(self, x, scores, mask=None, weights=None):\n        match_vec = self.value_match_vec.to(x.device)\n        pred_x = self(x, mask)\n        \n        scores_match_shape = torch.cat([torch.reshape(scores, (*scores.shape, 1))] * match_vec.shape[0], dim=1)\n        score_distance = torch.pow(scores_match_shape - match_vec, 2)\n        score_label = torch.argmin(score_distance, dim=1)\n        \n        return torch.nn.functional.cross_entropy(pred_x, score_label, weight=weights)\n        \n    def make_prediction(self, x, mask=None, strategy='avg', *strategy_args):\n        match_vec = self.value_match_vec.to(x.device)\n        pred_x = self(x, mask)\n        \n        if strategy == 'avg':\n            softmax_coe = 1.0 if len(strategy_args) == 0 else strategy_args[0]\n            avg_terms = torch.nn.functional.softmax(pred_x * softmax_coe, dim=1) * match_vec\n            avg_result = torch.sum(avg_terms, dim=1)\n            return avg_result\n        elif strategy == 'argmax':\n            match_idxs = torch.argmax(pred_x, dim=1)\n            argmax_result = match_vec[match_idxs]\n            return argmax_result\n        \n        else:\n            raise ValueError('unrecognized strategy {0}'.format(strategy))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.836059Z","iopub.execute_input":"2021-06-20T00:48:56.836708Z","iopub.status.idle":"2021-06-20T00:48:56.850479Z","shell.execute_reply.started":"2021-06-20T00:48:56.836672Z","shell.execute_reply":"2021-06-20T00:48:56.849635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimTokenizer:\n    def __call__(self, text_inputs, *args, **kwargs):\n        return {\n            'input_ids': self.parse(text_inputs),\n            'mask': None\n        }\n    \n    def parse(self, text_inputs):\n        raise NotImplemented\n        \n# inspired by https://www.kaggle.com/weka511/naive-readability\nclass LRTokenizer(SimTokenizer):\n    stop_words = set(nltk.corpus.stopwords.words('english')) \n    \n    @classmethod\n    def from_pretrained(cls, freq_file_path):\n        return cls(freq_file_path)\n    \n    def __init__(self, freq_file_path):\n        self.word_freqs = {}\n        with open(freq_file_path) as f_freq:\n            freq_reader = csv.DictReader(f_freq)\n            for freq_data in freq_reader:\n                self.word_freqs[freq_data['word'].lower()] = int(freq_data['count'])\n                \n        mx_freq, mn_freq = max(self.word_freqs[wrd_i] for wrd_i in self.word_freqs), min(self.word_freqs[wrd_i] for wrd_i in self.word_freqs)\n        for word_i in self.word_freqs:\n            self.word_freqs[word_i] = float(self.word_freqs[word_i] - mn_freq) / float(mx_freq - mn_freq)\n    \n    def parse(self, text_inputs):\n        return torch.tensor([self.get_features(txt_pc) for txt_pc in text_inputs])\n    \n    def get_features(self, txt_ipt):\n        txt_wrds = nltk.tokenize.word_tokenize(txt_ipt)\n        txt_sents = nltk.tokenize.sent_tokenize(txt_ipt)\n        \n        n_sents = float(len(txt_sents))\n        len_sents = float(len(txt_wrds)) / n_sents\n        syllables_per_sents = float(self.count_syllables(txt_wrds)) / n_sents\n        freq_avg = self.freq_cnt(txt_wrds)\n        \n        stop_word_cnts_per_sent = (np.array(self.count_stop_words(txt_wrds)) / n_sents).tolist()\n        \n        return [\n            n_sents, \n            len_sents, \n            syllables_per_sents, \n            freq_avg,\n            *stop_word_cnts_per_sent\n        ]\n        \n        \n    def count_syllables(self, txt_wrds):\n        vowels = set(['a', 'e', 'i', 'o', 'u', 'y'])\n        \n        cnt = 0\n        for wrd in txt_wrds:\n            last_vowel = False\n            for wc in wrd:\n                found_vowel = wc in vowels\n                if found_vowel:\n                    if not last_vowel:\n                        cnt += 1\n                        \n                    last_vowel = True\n                    \n                else:\n                    last_vowel = False\n            if len(wrd) > 1 and wrd[-1] == 'e':\n                cnt -= 1\n        return cnt\n    \n    def freq_cnt(self, all_words):\n        all_freqs = [self.word_freqs[wrd_i] for wrd_i in all_words if wrd_i in self.word_freqs]\n        return float(sum(all_freqs)) / float(len(all_freqs))\n        \n    def count_stop_words(self, all_words, stop_word_list=None):\n        if stop_word_list is None:\n            stop_word_list = self.stop_words\n        \n        cnter = collections.Counter(all_words)\n        return [float(cnter[sw_i]) for sw_i in stop_word_list]\n        \n\n        \nclass FeatureModel(torch.nn.Module):\n    def __init__(self, dims, mask=None):\n        super().__init__()\n        dims = [*dims]\n        if len(dims) < 2:\n            raise ValueError('At least provide the dim for input and output')\n        \n        if mask is None:\n            mask = []\n        input_dim = dims[0]\n        dims[0] = dims[0] - len(mask)\n        \n        lyrs = []\n        for i in range(1, len(dims)):\n            lyrs.append(torch.nn.Linear(dims[i-1], dims[i]))\n            if i < len(dims) - 1:\n                lyrs.append(torch.nn.Tanhshrink())\n        self.feature_model = torch.nn.Sequential(*lyrs)\n        \n        self.feature_indices = [i for i in range(0, input_dim) if i not in mask]\n    \n    def forward(self, x, mask=None):\n        x = x[:, self.feature_indices]\n        return self.feature_model(x)\n    \n    def loss(self, x, scores, mask=None, weights=None):\n        if weights is None:\n            weights = torch.ones(scores.shape).to(scores.device)\n        s_pred = torch.reshape(self(x), scores.shape)\n        loss_pred = torch.sqrt(torch.mean(torch.pow((scores - s_pred) * weights, 2)))\n        return loss_pred\n    \n    def make_prediction(self, x, mask=None, strategy='none', *strategy_args):\n        return torch.reshape(self(x), (x.shape[0],))\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.851788Z","iopub.execute_input":"2021-06-20T00:48:56.85216Z","iopub.status.idle":"2021-06-20T00:48:56.87752Z","shell.execute_reply.started":"2021-06-20T00:48:56.852099Z","shell.execute_reply":"2021-06-20T00:48:56.876706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# debug code\n# sample_lrp = LRTokenizer.from_pretrained('/kaggle/input/english-word-frequency/unigram_freq.csv')\n\n# sample_lrp([\n#     'hi i have no idea what is happening. but i will try.', \n#     'you guess what happend? i have no idea. just asking. you sure?', \n#     'hey you are a bitch.', \n#     'you are done, i am saying this. you are done. no more words.'\n# ])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.878796Z","iopub.execute_input":"2021-06-20T00:48:56.879184Z","iopub.status.idle":"2021-06-20T00:48:56.889825Z","shell.execute_reply.started":"2021-06-20T00:48:56.879119Z","shell.execute_reply":"2021-06-20T00:48:56.888994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelScaledWrapper(torch.nn.Module):\n    def __init__(self, wrapped_model, coefficient):\n        super().__init__()\n        self.wrapped = wrapped_model\n        self.coefficient = coefficient\n        \n    def make_prediction(self, *args):\n        return self.coefficient * self.wrapped.make_prediction(*args)\n    \n    def forward(self, *args):\n        return self.coefficient * self.wrapped(*args)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.891118Z","iopub.execute_input":"2021-06-20T00:48:56.891553Z","iopub.status.idle":"2021-06-20T00:48:56.900054Z","shell.execute_reply.started":"2021-06-20T00:48:56.891515Z","shell.execute_reply":"2021-06-20T00:48:56.899275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GradBoostModel(torch.nn.Module):\n    def __init__(self, device='cpu'):\n        super().__init__()\n        self.sub_modules = torch.nn.ModuleList([])\n        self.pred_params = []\n        self.tokenizers = []\n        self.model_device = device\n        self.to(self.model_device)\n    \n    def add_model(self, sub_model, sub_model_pred_param, sub_model_tokenizer, additional_args):\n        coefficient = self.fit_coefficient(\n            sub_model, \n            sub_model_tokenizer, \n            sub_model_pred_param, \n            additional_args['dt_loader']\n        ) * additional_args['shrinkage']\n        \n        self.sub_modules.append(ModelScaledWrapper(sub_model, coefficient).to(self.model_device))\n        self.pred_params.append(sub_model_pred_param)\n        self.tokenizers.append(sub_model_tokenizer)\n        \n        \n    def forward(self, txt_input, model_num=0):\n        if len(self) > 0:\n            while model_num <= 0:\n                model_num = model_num + len(self)\n            \n            rt_array = []\n            for sub_md, sub_pred_prm, sub_tk in zip(\n                        self.sub_modules[0: model_num],\n                        self.pred_params[0: model_num],\n                        self.tokenizers[0: model_num]\n                    ):\n                sub_tokenized = sub_tk(txt_input, padding=True, return_tensors='pt')\n                rt_array.append(\n                    sub_md.make_prediction(\n                        sub_tokenized['input_ids'].to(self.model_device), \n                        sub_tokenized['attention_mask'].to(self.model_device), \n                        *sub_pred_prm\n                    ) \n                )\n            rt = sum(rt_array)\n        else:\n            rt = torch.zeros((len(txt_input), )).to(self.model_device)\n        return rt\n    \n    def loss(self, txt_input, score, *forward_args):\n        scr_pred = self(txt_input, *forward_args)\n        loss_pred = torch.sqrt(torch.mean(torch.pow(scores - scr_pred, 2)))\n        return loss_pred\n    \n    def next_model_loss(self, t, md, x, msk, scr):\n        with torch.no_grad():\n            curr_md_target = (scr - self(t))\n        return md.loss(x, curr_md_target, msk) \n    \n    def last_model_added(self, *args):\n        pass\n    \n    def to(self, device):\n        self.model_device = device\n        return super().to(self.model_device)\n        \n    def __len__(self):\n        return len(self.sub_modules)\n    \n    def fit_coefficient(self, md, md_tk, md_pred_args, fit_data_loader):\n        base_md = self\n        base_scores = []\n        md_scores = []\n        ans_scores = []\n        \n        with torch.no_grad():\n            for txt_f, scr_f, _ in fit_data_loader:\n                txt_tk = md_tk(txt_f, padding=True, return_tensors='pt')\n                txt_vec_f = txt_tk['input_ids'].to(PARAMS['device'])\n                txt_msk_f = txt_tk['attention_mask'].to(PARAMS['device'])\n                scr_f = torch.tensor(scr_f).to(PARAMS['device'])\n\n                base_md_pred = base_md(txt_f)\n                md_pred = md.make_prediction(txt_vec_f, txt_msk_f, *md_pred_args)\n\n                base_scores.append(base_md_pred)\n                md_scores.append(md_pred)\n                ans_scores.append(scr_f)\n\n        yb = torch.cat(base_scores, dim=0)\n        ym = torch.cat(md_scores, dim=0)\n        ya = torch.cat(ans_scores, dim=0)\n\n        # find c such that c minimize [ya - (yb + c * ym)]^2\n        cm = (torch.sum((ya - yb) * ym) / torch.sum(ym * ym)).detach().cpu().numpy()\n        return cm\n            ","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.902738Z","iopub.execute_input":"2021-06-20T00:48:56.903086Z","iopub.status.idle":"2021-06-20T00:48:56.92251Z","shell.execute_reply.started":"2021-06-20T00:48:56.903048Z","shell.execute_reply":"2021-06-20T00:48:56.921672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WeightHeadModel(torch.nn.Module):\n    def __init__(self, pred_model_num, w_model_features, output_dims):\n        super().__init__()\n        \n        output_dims.append(pred_model_num) # assume output_dims only contain internal dims\n        \n        self.bi_ln_interpolate = torch.nn.Bilinear(pred_model_num, w_model_features, output_dims[0])\n        \n        inner_layers = []\n        for lyr_i in range(1, len(output_dims)):\n            inner_layers.append(torch.nn.Tanhshrink())\n            inner_layers.append(torch.nn.Linear(output_dims[lyr_i - 1], output_dims[lyr_i]))\n        \n        self.ln_model = torch.nn.Sequential(\n            *inner_layers\n        )\n        \n    def forward(self, preds, features):\n        return self.ln_model(self.bi_ln_interpolate(preds, features))\n\n\nclass AvgWeightHeadModel(torch.nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        \n    def forward(self, preds, features):\n        return (torch.ones(preds.shape) / float(preds.shape[1])).to(preds.device)\n    \n\nclass WeightedAvgModel(torch.nn.Module):\n    def __init__(self, weight_tokenizer, weighted_model, weight_head_cls, weight_head_params, device='cpu'):\n        super().__init__()\n        self.sub_modules = torch.nn.ModuleList([])\n        self.pred_params = []\n        self.tokenizers = []\n        \n        self.weight_model_tokenizer = weight_tokenizer\n        self.weight_model = weighted_model\n        self.weight_head_params = weight_head_params\n        self.weight_head_cls = weight_head_cls\n        self.weight_model_head = None\n        \n        self.model_device = device\n        self.to(self.model_device)\n    \n    def add_model(self, sub_model, sub_model_pred_param, sub_model_tokenizer, additional_args):\n        self.sub_modules.append(sub_model.to(self.model_device))\n        self.pred_params.append(sub_model_pred_param)\n        self.tokenizers.append(sub_model_tokenizer)\n    \n    def loss(self, txt_input, scores, *forward_args):\n        scr_pred = self(txt_input, *forward_args)\n        loss_pred = torch.sqrt(torch.mean(torch.pow(scores - scr_pred, 2)))\n        return loss_pred\n    \n    def next_model_loss(self, t, md, x, msk, scr):\n        return md.loss(x, scr, msk) \n    \n    def last_model_added(self, tr_dt_loader, v_dt_loader, w_lr, w_epochs):\n        self.weight_model_head = self.weight_head_cls(len(self), *self.weight_head_params).to(self.model_device)\n        \n        w_opt = torch.optim.AdamW(\n            [\n                {'params': self.weight_model.parameters()},\n                {'params': self.weight_model_head.parameters()}\n            ],\n            lr = w_lr\n        )\n        for w_epoch_i in range(0, w_epochs):\n            print('-----epoch {0}------'.format(w_epoch_i + 1))\n            tr_losses = []\n            v_losses = []\n            \n            self.train()\n            for txt_w_train, scr_w_train, _ in tr_dt_loader:\n                scr_w_train = torch.tensor(scr_w_train).to(self.model_device)\n                w_opt.zero_grad()\n                \n                train_w_loss = self.loss(txt_w_train, scr_w_train)\n                train_w_loss.backward()\n                w_opt.step()\n                tr_losses.extend([train_w_loss.detach().cpu().numpy()] * len(txt_w_train))\n            \n            self.eval()\n            for txt_w_val, scr_w_val, _ in v_dt_loader:\n                scr_w_val = torch.tensor(scr_w_val).to(self.model_device)\n                with torch.no_grad():\n                    val_w_loss = self.loss(txt_w_val, scr_w_val)\n                    v_losses.extend([val_w_loss.detach().cpu().numpy()] * len(txt_w_val))\n            \n            print('weight train loss {0}'.format(np.mean(tr_losses)))\n            print('weight val loss {0}'.format(np.mean(v_losses)))\n            print()\n        \n        \n    def forward(self, txt_input):\n        if self.weight_model_head is None:\n            raise ValueError('Training not finished, cannot predict')\n            \n        if len(self) == 0:\n            rt = torch.zeros((len(txt_input), )).to(self.model_device)\n        else:\n            with torch.no_grad():\n                score_preds = []\n                \n                for sub_md, sub_pred_prm, sub_tk in zip(\n                            self.sub_modules,\n                            self.pred_params,\n                            self.tokenizers\n                        ):\n                    sub_tokenized = sub_tk(txt_input, padding=True, return_tensors='pt')\n                    score_preds.append(\n                        torch.reshape(\n                            sub_md.make_prediction(\n                                sub_tokenized['input_ids'].to(self.model_device), \n                                sub_tokenized['attention_mask'].to(self.model_device), \n                                *sub_pred_prm\n                            ),\n                            (len(txt_input), 1)\n                        )\n                    )\n                \n            score_preds = torch.cat(score_preds, dim=1)\n            \n            weight_txt_vec = self.weight_model_tokenizer(txt_input, padding=True, return_tensors='pt')['input_ids'].to(self.model_device)\n            score_weights = self.weight_model_head(score_preds, self.weight_model(weight_txt_vec))\n            \n            final_pred = self.weight_collapse(score_preds, score_weights)\n            return final_pred\n            \n    @classmethod\n    def weight_collapse(cls, preds, ws):\n        # write it as a func so that it can be called during training as well\n        wf = torch.nn.functional.softmax(ws, dim=1)\n        ret = torch.sum(\n            preds * wf,\n            dim=1\n        )\n        return ret\n        \n    def to(self, device):\n        self.model_device = device\n        return super().to(self.model_device)\n        \n    def __len__(self):\n        return len(self.sub_modules)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.925149Z","iopub.execute_input":"2021-06-20T00:48:56.92553Z","iopub.status.idle":"2021-06-20T00:48:56.951731Z","shell.execute_reply.started":"2021-06-20T00:48:56.925499Z","shell.execute_reply":"2021-06-20T00:48:56.950796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#debug code\n# mlst = torch.nn.ModuleList([torch.nn.Linear(3, 3), torch.nn.Linear(5, 3), torch.nn.Linear(3, 4)])\n# mlst[0:-1]\n# gbs = GradBoostModel()\n# print(len(gbs))\n# gbs.add_model(torch.nn.Linear(3, 4), 'a', None)\n# print(len(gbs))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.952844Z","iopub.execute_input":"2021-06-20T00:48:56.953324Z","iopub.status.idle":"2021-06-20T00:48:56.965642Z","shell.execute_reply.started":"2021-06-20T00:48:56.953289Z","shell.execute_reply":"2021-06-20T00:48:56.964859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PARAMS = {\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    \n    'sent_reorder_loader_params': {\n        'trunc_lower': 1.0,\n        'trunc_higher': 1.0,\n        'trunc_min': 2,\n        'trunc_shuffle': False,\n        'iter_percentage': 0.7\n    },\n    \n    'main_model_class': WeightedAvgModel,\n    'main_model_construct_args': [\n        LRTokenizer('/kaggle/input/english-word-frequency/unigram_freq.csv'),\n        FeatureModel([183, 50, 30]),\n        AvgWeightHeadModel,\n        [],\n#         WeightHeadModel,\n#         [30, [15, 7]]\n    ],\n    \n    'sub_models': [\n#         {\n#             'model_class': ContinuousWrapper,\n#             'construct_args': [\n#                 BertForSequenceClassification, \n#                 '/kaggle/input/bert-base-uncased', \n#                 lambda p_name: sum([1 if non_freeze_str in p_name else 0 for \n#                                                         non_freeze_str in [\n#                                                           'classifier',\n#                                                           'pooler',\n#                                                           'encoder.layer'\n# #                                                             '.'\n#                                                         ]\n#                                     ]) == 0,\n#             ],\n#             'tokenizer_class': BertTokenizer,\n#             'tokenizer_path': '/kaggle/input/bert-base-uncased',\n\n#             'pred_args': ['none'],\n\n#             'epochs': 12,\n#             'lr': 0.00001,\n#             'weight_decay': 0.07,\n#             'random_sample_decay': 1.0,\n#             'add_model_params': {}\n#         },\n#         {\n#             'model_class': DiscreteEstimator,\n#             'construct_args': [\n#                 RobertaForSequenceClassification, \n#                '/kaggle/input/roberta-transformers-pytorch/distilroberta-base', \n#                 (-5.0, 3.0),\n#                 9,\n#                 lambda p_name: sum([1 if non_freeze_str in p_name else 0 for \n#                                                       non_freeze_str in [\n# #                                                           'classifier',\n# #                                                           'pooler',\n# #                                                           'encoder.layer'\n#                                                           '.'\n#                                                       ]\n#                                                  ]) == 0,\n#             ],\n#             'tokenizer_class': RobertaTokenizer,\n#             'tokenizer_path': '/kaggle/input/roberta-transformers-pytorch/distilroberta-base',\n            \n#             'pred_args': ['avg', 5],\n            \n#             'epochs': 9,\n#             'lr': 0.00001,\n#             'weight_decay': 0.01,\n#             'random_sample_decay': 1.0,\n#             'add_model_params': {\n# #                 'dt_loader': TextReorderBatchLoader(training_data, 10), \n# #                 'shrinkage': 1.0\n#             }\n#         },\n            \n        {\n            'model_class': ContinuousWrapper,\n            'construct_args': [\n                RobertaForSequenceClassification, \n               '/kaggle/input/roberta-transformers-pytorch/roberta-large-mnli', \n                lambda p_name: sum([1 if non_freeze_str in p_name else 0 for \n                                                        non_freeze_str in [\n                                                          'classifier',\n                                                          'pooler',\n                                                          *['encoder.layer.{0}'.format(unf_lyr) for unf_lyr in range(16, 24)],\n#                                                           '.'\n                                                        ]\n                                    ]) == 0,\n            ],\n            'tokenizer_class': RobertaTokenizer,\n            'tokenizer_path': '/kaggle/input/roberta-transformers-pytorch/roberta-base',\n\n            'pred_args': ['none'],\n\n            'epochs': 8,\n            'lr': 0.00001,\n            'weight_decay': 0.5,\n            'random_sample_decay': 1.0,\n            'add_model_params': {\n#                 'dt_loader': TextReorderBatchLoader(training_data, 10), \n#                 'shrinkage': 1.0\n            }\n        },\n        {\n            'model_class': ContinuousWrapper,\n            'construct_args': [\n                RobertaForSequenceClassification, \n               '/kaggle/input/roberta-transformers-pytorch/roberta-large', \n                lambda p_name: sum([1 if non_freeze_str in p_name else 0 for \n                                                        non_freeze_str in [\n                                                          'classifier',\n                                                          'pooler',\n                                                          *['encoder.layer.{0}'.format(unf_lyr) for unf_lyr in range(12, 24)],\n#                                                           '.'\n                                                        ]\n                                    ]) == 0,\n            ],\n            'tokenizer_class': RobertaTokenizer,\n            'tokenizer_path': '/kaggle/input/roberta-transformers-pytorch/roberta-base',\n\n            'pred_args': ['none'],\n\n            'epochs': 12,\n            'lr': 0.00001,\n            'weight_decay': 0.5,\n            'random_sample_decay': 1.0,\n            'add_model_params': {\n#                 'dt_loader': TextReorderBatchLoader(training_data, 10), \n#                 'shrinkage': 1.0\n            }\n        },\n        {\n            'model_class': ContinuousWrapper,\n            'construct_args': [\n                RobertaForSequenceClassification, \n               '/kaggle/input/roberta-transformers-pytorch/roberta-base', \n                lambda p_name: sum([1 if non_freeze_str in p_name else 0 for \n                                                        non_freeze_str in [\n                                                          'classifier',\n                                                          'pooler',\n                                                          'encoder.layer',\n#                                                           '.'\n                                                        ]\n                                    ]) == 0,\n            ],\n            'tokenizer_class': RobertaTokenizer,\n            'tokenizer_path': '/kaggle/input/roberta-transformers-pytorch/roberta-base',\n\n            'pred_args': ['none'],\n\n            'epochs': 16,\n            'lr': 0.00001,\n            'weight_decay': 0.5,\n            'random_sample_decay': 1.0,\n            'add_model_params': {\n#                 'dt_loader': TextReorderBatchLoader(training_data, 10), \n#                 'shrinkage': 1.0\n            }\n        },\n        \n#         {\n#             'model_class': ContinuousWrapper,\n#             'construct_args': [\n#                 RobertaForSequenceClassification, \n#                '/kaggle/input/roberta-transformers-pytorch/distilroberta-base', \n#                 lambda p_name: sum([1 if non_freeze_str in p_name else 0 for \n#                                                         non_freeze_str in [\n#                                                           'classifier',\n#                                                           'pooler',\n#                                                           'encoder.layer.5',\n#                                                           'encoder.layer.4',\n#                                                           'encoder.layer.3',\n#                                                         ]\n#                                     ]) == 0,\n#             ],\n#             'tokenizer_class': RobertaTokenizer,\n#             'tokenizer_path': '/kaggle/input/roberta-transformers-pytorch/distilroberta-base',\n\n#             'pred_args': ['none'],\n\n#             'epochs': 15,\n#             'lr': 0.0000001,\n#             'weight_decay': 0.1,\n#             'random_sample_decay': 1.0,\n#             'grad_boost_shrinkage': 1.0\n#         },\n    ],\n    \n    'batch_size': 10,\n    'val_batch_size': 30,\n    \n    'on_finish_params': [\n        TextReorderBatchLoader(\n            training_data,\n            10\n        ),\n        TextReorderBatchLoader(\n            val_data,\n            10\n        ),\n        0.0001,\n        0\n    ]\n    \n    \n}\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:56.96725Z","iopub.execute_input":"2021-06-20T00:48:56.967646Z","iopub.status.idle":"2021-06-20T00:48:58.487715Z","shell.execute_reply.started":"2021-06-20T00:48:56.967609Z","shell.execute_reply":"2021-06-20T00:48:58.486821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# model = DiscreteEstimator(\n#     PARAMS['model_class'],\n#     PARAMS['model_path'],\n#     PARAMS['discrete_pred_range'],\n#     PARAMS['discrete_classes'],\n#     PARAMS['freeze_param_condition']\n# ).to(PARAMS['device'])\n# model = ContinuousWrapper(\n#     PARAMS['model_class'],\n#     PARAMS['model_path'],\n#     PARAMS['freeze_param_condition']\n# ).to(PARAMS['device'])\nmodel = PARAMS['main_model_class'](*PARAMS['main_model_construct_args']).to(PARAMS['device'])\n\n# pre_tokenizer = PARAMS['tokenizer_class'].from_pretrained(PARAMS['token_path'])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:58.490726Z","iopub.execute_input":"2021-06-20T00:48:58.490992Z","iopub.status.idle":"2021-06-20T00:48:58.541028Z","shell.execute_reply.started":"2021-06-20T00:48:58.490966Z","shell.execute_reply":"2021-06-20T00:48:58.540271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer = torch.optim.AdamW(model.parameters(), lr=PARAMS['lr'], weight_decay=PARAMS['weight_decay'])\n# print(list(model.parameters()))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:58.542554Z","iopub.execute_input":"2021-06-20T00:48:58.542878Z","iopub.status.idle":"2021-06-20T00:48:58.547345Z","shell.execute_reply.started":"2021-06-20T00:48:58.542842Z","shell.execute_reply":"2021-06-20T00:48:58.546345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unreg_loss_fn = lambda scr1, scr2: torch.sqrt(torch.mean(torch.pow(scr2 - scr1, 2)))\n# reg_loss_fn = lambda scr1, scr2, std: torch.sqrt(torch.mean(\n#                                             torch.pow(scr2 - scr1, 2) / torch.maximum(\n#                                                                             torch.pow(std, 2), \n#                                                                             0.01 * torch.ones(std.shape).to(std.device)\n#                                                                         )\n#                                         ))\n\n# loss_md_fn = lambda base_md, t, diff_c, md, x, scr: md.loss(x, (scr - base_md(t)) * diff_c)\n# def loss_md_fn(base_md, t, md, x, scr):\n#     with torch.no_grad():\n#         curr_md_target = (scr - base_md(t))\n#     return md.loss(x, curr_md_target)\n            \n\n# for cycle_num in range(1, PARAMS['cycles'] + 1):\n#     for fold_num in range(0, PARAMS['folds']):\nfor _ in range(0, 1):\n    for sub_model_idx in range(0, len(PARAMS['sub_models'])):\n        print('training sub model {0}'.format(sub_model_idx + 1))\n        sub_train_loader = TextReorderBatchLoader(\n            training_data, \n            PARAMS['batch_size'], \n            **PARAMS['sent_reorder_loader_params'],\n            \n        )\n        sub_val_loader = TextReorderBatchLoader(val_data, PARAMS['val_batch_size'])\n        \n        curr_md = PARAMS['sub_models'][sub_model_idx]['model_class'](\n                        *PARAMS['sub_models'][sub_model_idx]['construct_args']\n                    ).to(PARAMS['device'])\n        curr_tk = PARAMS['sub_models'][sub_model_idx]['tokenizer_class'].from_pretrained(\n            PARAMS['sub_models'][sub_model_idx]['tokenizer_path']\n        )\n        \n        curr_pred_params = PARAMS['sub_models'][sub_model_idx]['pred_args']\n        \n        curr_opt = torch.optim.AdamW(\n            curr_md.parameters(), \n            lr=PARAMS['sub_models'][sub_model_idx]['lr'], \n            weight_decay=PARAMS['sub_models'][sub_model_idx]['weight_decay']\n        )\n        \n        curr_model_epochs = PARAMS['sub_models'][sub_model_idx]['epochs']\n        \n        curr_add_param = PARAMS['sub_models'][sub_model_idx]['add_model_params']\n        for epoch in range(1, curr_model_epochs + 1):\n            prog_bar = ProgressBar(len(sub_train_loader), 100)\n            prog_bar.print_init()\n            print('epoch {0} / {1}'.format(epoch, curr_model_epochs))\n\n            loss_stats = {\n                'train_md': [],\n                'val_md': []\n            }\n        \n            model.eval()\n            curr_md.train()\n            for text_train, score_train, std_train in sub_train_loader:\n                torch.cuda.empty_cache() \n                curr_opt.zero_grad()\n                text_train_tk = curr_tk(text_train, padding=True, return_tensors='pt')\n                text_train_vec = text_train_tk['input_ids'].to(PARAMS['device'])\n                text_train_msk = text_train_tk['attention_mask'].to(PARAMS['device'])\n                \n                score_train = torch.tensor(score_train).to(PARAMS['device'])\n                std_train = torch.tensor(std_train).to(PARAMS['device'])\n                \n                d_score_train = torch.normal(\n                    torch.zeros(std_train.shape).to(PARAMS['device']),\n                    std_train\n                ) * PARAMS['sub_models'][sub_model_idx]['random_sample_decay']\n\n\n                train_loss_md = model.next_model_loss(\n                    text_train,\n                    curr_md, \n                    text_train_vec, \n                    text_train_msk,\n                    score_train + d_score_train\n                )\n                train_loss_md.backward()\n                curr_opt.step()\n        #         scheduler.step()\n\n\n\n                loss_stats['train_md'].append(train_loss_md.detach().cpu().numpy())\n\n                prog_bar.update(score_train.shape[0])\n    \n            model.eval()\n            curr_md.eval()\n            with torch.no_grad():\n                for text_val, score_val, std_val in sub_val_loader:\n                    torch.cuda.empty_cache() \n                    text_val_tk = curr_tk(text_val, padding=True, return_tensors='pt')\n                    text_val_vec = text_val_tk['input_ids'].to(PARAMS['device'])\n                    text_val_msk = text_val_tk['attention_mask'].to(PARAMS['device'])\n                    score_val = torch.tensor(score_val).to(PARAMS['device'])\n                    std_val = torch.tensor(std_val).to(PARAMS['device'])\n\n\n                    val_loss_md = model.next_model_loss( \n                                    text_val, \n                                    curr_md, \n                                    text_val_vec, \n                                    text_val_msk,\n                                    score_val\n                                )\n                    loss_stats['val_md'].append(val_loss_md.detach().cpu().numpy())\n\n            print()\n\n            for loss_name in loss_stats:\n                print('{0}: {1}'.format(loss_name, np.mean(loss_stats[loss_name])))\n        \n        model.add_model(\n            curr_md, \n            curr_pred_params, \n            curr_tk, \n            curr_add_param\n        )\n        print()\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:48:58.551402Z","iopub.execute_input":"2021-06-20T00:48:58.55171Z","iopub.status.idle":"2021-06-20T01:59:58.900966Z","shell.execute_reply.started":"2021-06-20T00:48:58.551678Z","shell.execute_reply":"2021-06-20T01:59:58.899146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.last_model_added(*PARAMS['on_finish_params'])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:59:58.902706Z","iopub.status.idle":"2021-06-20T01:59:58.90336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# can be different from the ones in training\n\nvalidation_loss_func = lambda scr1, scr2: torch.sqrt(torch.mean(torch.pow(scr2 - scr1, 2)))\nvalidation_results = []\n\nval_loader = TextReorderBatchLoader(val_data, len(val_data))\n\n\nbase_scores = []\npred_scores = []\npred_losses = []\n\nmodel.eval()\nwith torch.no_grad():\n    for text_val, score_val, std_val in val_loader:\n        torch.cuda.empty_cache() \n#         text_val = pre_tokenizer(text_val, padding=True, return_tensors='pt')['input_ids'].to(PARAMS['device'])\n        score_val = torch.tensor(score_val).to(PARAMS['device'])\n        std_val = torch.tensor(std_val).to(PARAMS['device'])\n\n        score_pred_val = model(text_val)\n        val_loss = validation_loss_func(score_val, score_pred_val)\n        \n        validation_results.extend([val_loss.detach().cpu().numpy()] * score_val.shape[0])\n        \n        base_scores.extend(\n            score_val.detach().cpu().numpy()\n        )\n        pred_scores.extend(\n            score_pred_val.detach().cpu().numpy()\n        )\n        pred_losses.extend(\n            torch.sqrt(torch.pow(score_val - score_pred_val, 2)).detach().cpu().numpy()\n        )\n        \n        \n        \nprint('final validation loss {0}'.format(np.mean(validation_results)))\n\nplt.plot(base_scores, pred_scores, 'b.')\nplt.show()\nplt.plot(base_scores, pred_losses, 'r.')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:59:58.90466Z","iopub.status.idle":"2021-06-20T01:59:58.905279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ntest_preds = []\neval_batch_size = 10\nwith torch.no_grad():\n#     for test_id, test_str in all_tests:\n    for test_idx in range(0, len(all_tests), eval_batch_size):\n        torch.cuda.empty_cache() \n#         pred = model(pre_tokenizer([test_str], padding=True, return_tensors='pt')['input_ids'].to(PARAMS['device'])).logits[0, 0]\n#         pred = model.make_prediction(\n#             pre_tokenizer([test_str], padding=True, return_tensors='pt')['input_ids'].to(PARAMS['device']),\n#             PARAMS['pred_strategy'],\n#             *PARAMS['pred_strategy_args']\n#         )[0]\n        test_ids = [t_id for t_id, _ in all_tests[test_idx: test_idx + eval_batch_size]]\n        test_strs = [t_txt for _, t_txt in all_tests[test_idx: test_idx + eval_batch_size]]\n        pred = model(test_strs).detach().cpu().numpy()\n        for test_id, target_value in zip(test_ids, pred):\n            test_preds.append({\n                'id': test_id,\n                'target': target_value\n            })\n    with open('submission.csv', 'w') as output_file:\n        fieldnames = ['id', 'target']\n        result_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n\n        result_writer.writeheader()\n        for test_row in test_preds:\n            result_writer.writerow(test_row)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:59:58.906478Z","iopub.status.idle":"2021-06-20T01:59:58.907031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat submission.csv","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:59:58.908115Z","iopub.status.idle":"2021-06-20T01:59:58.908694Z"},"trusted":true},"execution_count":null,"outputs":[]}]}