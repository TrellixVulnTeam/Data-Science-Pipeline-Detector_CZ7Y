{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Network for Estimating Text Readability\n\nPretrained DistilBERT model is initialized for classification task with just one class and compiled with regression metrics to produce readability scores. The model is fine-tuned with decreasing learning rate on the train samples until validation RMSE stops improving.\n\nOriginal text samples are split into train and validation subsets 90%/10%. Texts are not preprocessed in any way before passing them to the tokenizer of cased model. When splitting data length of text samples is taken into account to observe proportional distribution of short, medium and long excerpts in train and validation sets because text length strongly correlates with readability score.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport os\nimport gc\n\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-31T08:40:44.186724Z","iopub.execute_input":"2021-07-31T08:40:44.187116Z","iopub.status.idle":"2021-07-31T08:40:44.197051Z","shell.execute_reply.started":"2021-07-31T08:40:44.187086Z","shell.execute_reply":"2021-07-31T08:40:44.195872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plots display settings\nplt.rcParams['figure.figsize'] = 12, 8\nplt.rcParams.update({'font.size': 14})","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:40:04.399737Z","iopub.execute_input":"2021-07-31T08:40:04.400214Z","iopub.status.idle":"2021-07-31T08:40:04.408155Z","shell.execute_reply.started":"2021-07-31T08:40:04.40016Z","shell.execute_reply":"2021-07-31T08:40:04.407112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seed everything\nseed_value = 5\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:40:04.410965Z","iopub.execute_input":"2021-07-31T08:40:04.411486Z","iopub.status.idle":"2021-07-31T08:40:04.419163Z","shell.execute_reply.started":"2021-07-31T08:40:04.411446Z","shell.execute_reply":"2021-07-31T08:40:04.417644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Paths to original data\ntrain_path = '../input/commonlitreadabilityprize/train.csv'\ntest_path = '../input/commonlitreadabilityprize/test.csv'","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:40:04.421247Z","iopub.execute_input":"2021-07-31T08:40:04.421791Z","iopub.status.idle":"2021-07-31T08:40:04.429888Z","shell.execute_reply.started":"2021-07-31T08:40:04.421732Z","shell.execute_reply":"2021-07-31T08:40:04.428741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pretrained model from HugginFace Hub\n# Loaded from Kaggle dataset at https://www.kaggle.com/sauravmaheshkar/huggingface-bert-variants\nmodel_path = '../input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased'","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:40:04.431514Z","iopub.execute_input":"2021-07-31T08:40:04.432026Z","iopub.status.idle":"2021-07-31T08:40:04.441247Z","shell.execute_reply.started":"2021-07-31T08:40:04.431967Z","shell.execute_reply":"2021-07-31T08:40:04.439972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TensorFlow settings\nBATCH_SIZE = 32\nEPOCHS = 20\nSTART_LR = 1e-5\nEND_LR = 1e-7\nPATIENCE = 3\n\n# NLP model settings\nMAX_LEN = 256  # Number of words per text\n\n# Portion of data for validation\nVAL_SIZE = 0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:40:04.442969Z","iopub.execute_input":"2021-07-31T08:40:04.443537Z","iopub.status.idle":"2021-07-31T08:40:04.45329Z","shell.execute_reply.started":"2021-07-31T08:40:04.443495Z","shell.execute_reply":"2021-07-31T08:40:04.452164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"code","source":"def get_data(path: str) -> pd.DataFrame:\n    \"\"\"Function loads data from the csv file\n    and creates a column with total number of characters.\n    :param path: Path to csv file with original data\n    :return: DataFrame with original data and generated features\n    \"\"\"\n    df = pd.read_csv(path)\n    df['n_chars'] = df['excerpt'].apply(len)\n\n    return df\n\n\ndef tokenize_texts(tokenizer, texts: pd.Series, labels):\n    \"\"\"Function converts texts into tokenized and batched datasets for the model.\n    Returns dataset with or without labels.\n    :param tokenizer: Tokenizer instance from transformers library\n    :param texts: pd.Series with raw texts\n    :param labels: Series of target values or None, if labels are not available\n    :return: Tensorflow Dataset object\n    \"\"\"\n    if labels is not None:  # Data is labeled\n        ds = tokenizer(texts.values.tolist(),\n                       return_tensors='tf', max_length=MAX_LEN,\n                       padding='max_length', truncation=True)\n        ds = tf.data.Dataset.from_tensor_slices(\n            (ds['input_ids'], ds['attention_mask'], labels.values)\n        ).map(lambda x1, x2, y: ({'input_ids': x1, 'attention_mask': x2}, y))\\\n            .batch(BATCH_SIZE)\n\n    else:  # If no labels are provided\n        ds = tokenizer(texts.values.tolist(),\n                       return_tensors='tf', max_length=MAX_LEN,\n                       padding='max_length', truncation=True)\n        ds = tf.data.Dataset.from_tensor_slices(\n            (ds['input_ids'], ds['attention_mask'])\n        ).map(lambda x1, x2: {'input_ids': x1, 'attention_mask': x2})\\\n            .batch(BATCH_SIZE)\n\n    return ds\n\n\ndef train_and_forecast() -> np.array:\n    \"\"\"Function fine-tunes pretrained transformer model\n    and produces a forecast for the test data.\n    :returns: Numpy array with readability scores for test data\n    \"\"\"\n    # Initialize pretrained model for classification task with 1 class\n    model = TFAutoModelForSequenceClassification.from_pretrained(model_path, num_labels=1)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    # Tokenize test texts and convert into tensorflow Dataset object\n    test_ds = tokenize_texts(tokenizer, data_test['excerpt'], labels=None)\n    print(f'Number of test text samples: {len(data_test)}')\n    print(f'Test texts tokenized. Number of batches: {len(test_ds)}')\n\n    # Split the samples of various text length proportionally\n    # between train and validation sets\n    groups = pd.cut(data_train['n_chars'],\n                    bins=[0, 800, 900, 1000, 1100, 1200, 1350],\n                    labels=[1, 2, 3, 4, 5, 6])\n\n    train_texts, val_texts, train_scores, val_scores = train_test_split(\n        data_train['excerpt'], data_train['target'], stratify=groups,\n        test_size=VAL_SIZE, shuffle=True, random_state=0)\n\n    # Tokenize train and validation texts\n    train_ds = tokenize_texts(tokenizer, train_texts, train_scores)\n    valid_ds = tokenize_texts(tokenizer, val_texts, val_scores)\n\n    print(f'Number of train text samples: {len(train_texts)}')\n    print(f'Train texts tokenized. Number of batches: {len(train_ds)}')\n\n    print(f'Number of validation text samples: {len(val_texts)}')\n    print(f'Validation texts tokenized. Number of batches: {len(valid_ds)}')\n\n    # Linearly decreasing learning rate\n    lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n        initial_learning_rate=START_LR,\n        end_learning_rate=END_LR,\n        decay_steps=EPOCHS * len(train_ds)\n    )\n\n    # Compile the model with regression metrics\n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=lr_scheduler, clipnorm=1.0),\n                  loss=tf.keras.losses.MeanSquaredError(),\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n    # To stop training when per-epoch validation RMSE starts growing\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error',\n                                                  patience=PATIENCE,\n                                                  restore_best_weights=True)\n\n    history = model.fit(train_ds, validation_data=valid_ds,\n                        epochs=EPOCHS, verbose=2, callbacks=[early_stop],\n                        use_multiprocessing=True, workers=2)\n    plot_history(history)\n\n    loss, rmse = model.evaluate(valid_ds)\n    print(f'Training completed. Validation loss (MSE) = {loss}\\nValidation RMSE: {rmse}')\n\n    # Make a forecast for test data\n    forecast = model.predict(test_ds).logits.flatten()\n\n    gc.collect()\n    tf.keras.backend.clear_session()\n\n    return forecast\n\n\ndef plot_history(hist):\n    \"\"\"Function plots a chart with training and validation metrics.\n    :param hist: Tensorflow history object from model.fit()\n    \"\"\"\n    # Losses and metrics\n    mse = hist.history['loss']\n    val_mse = hist.history['val_loss']\n    rmse = hist.history['root_mean_squared_error']\n    val_rmse = hist.history['val_root_mean_squared_error']\n\n    # Epochs to plot along x axis\n    x_axis = range(1, len(mse) + 1)\n\n    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n\n    ax1.plot(x_axis, mse, 'bo', label='Training')\n    ax1.plot(x_axis, val_mse, 'ro', label='Validation')\n    ax1.set_title('Training and validation MSE')\n    ax1.set_ylabel('Loss (MSE)')\n    ax1.legend()\n\n    ax2.plot(x_axis, rmse, 'bo', label='Training')\n    ax2.plot(x_axis, val_rmse, 'ro', label='Validation')\n    ax2.set_title('Training and validation RMSE')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('RMSE')\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:49:20.983198Z","iopub.execute_input":"2021-07-31T08:49:20.983573Z","iopub.status.idle":"2021-07-31T08:49:21.008739Z","shell.execute_reply.started":"2021-07-31T08:49:20.983544Z","shell.execute_reply":"2021-07-31T08:49:21.007462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"# Load train and test data\ndata_train = get_data(train_path)\ndata_test = get_data(test_path)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:40:04.486554Z","iopub.execute_input":"2021-07-31T08:40:04.487137Z","iopub.status.idle":"2021-07-31T08:40:04.59075Z","shell.execute_reply.started":"2021-07-31T08:40:04.487081Z","shell.execute_reply":"2021-07-31T08:40:04.589832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train neural network and produce a forecast for the test set\ndata_test['target'] = train_and_forecast()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:49:27.019856Z","iopub.execute_input":"2021-07-31T08:49:27.020321Z","iopub.status.idle":"2021-07-31T08:56:21.342012Z","shell.execute_reply.started":"2021-07-31T08:49:27.020291Z","shell.execute_reply":"2021-07-31T08:56:21.340849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the result\ndata_test[['id', 'target']].to_csv('submission.csv', index=False)\nprint(data_test[['id', 'target']].head())","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:40:20.801947Z","iopub.status.idle":"2021-07-31T08:40:20.802437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}