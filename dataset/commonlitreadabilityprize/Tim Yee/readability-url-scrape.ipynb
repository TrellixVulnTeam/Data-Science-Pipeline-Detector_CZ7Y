{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Scrape URLs for CommonLit Readability Prize competition \n\n1. There contains over 600 URLs to scrape. I only scrape ~570 URLs from 3-4 separate domains. \n2. Wikipedia was the most annoying to scrape cleanly. \n3. There may be some undetected artifacts in the text so use with caution.\n3. You should perform your own exploratory data analysis to discover any remaining artifacts that occured during scraping.\n4. I created a [notebook](https://www.kaggle.com/teeyee314/readability-external-data-eda)  with additional preparation for use with competition training.\n\nYou're welcome :)","metadata":{}},{"cell_type":"code","source":"!pip install -q bs4","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nBASE_DIR = '../input/commonlitreadabilityprize'\n\nprint(os.listdir(BASE_DIR))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(os.path.join(BASE_DIR, 'train.csv'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select rows that have urls\nhas_text = train[~train['url_legal'].isnull()]\n\n# grab the domain name\nhas_text['domain'] = has_text['url_legal'].apply(lambda x: x.split('/')[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list all reference urls by frequency in descending order\nhas_text['url_legal'].apply(lambda x: x.split('/')[2]).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"| Count | Url |\n|--- | --- |\n| 196 | simple.wikipedia.org |  \n| 191 | kids.frontiersin.org |\n| 176 | en.wikipedia.org |\n| 8 | en.wikibooks.org |\n| 571 | Total |\n| 95 | Missing | ","metadata":{}},{"cell_type":"code","source":"def show_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n\n    words = []\n\n    for paragraph in soup.find_all('p'):\n        if paragraph.sup:\n            for support in paragraph.find_all('sup'):\n                support.decompose()\n        words.append(paragraph.get_text())\n\n    return words\n\ndef clean_newline(soup=''):\n    return re.sub(r'\\n', '', soup)\n\ndef clean_http(soup=''):\n    soup = list(map(lambda x: '' if re.search('http',x) else x, soup))\n    soup = list(filter(lambda x: x != '', soup))\n    return soup\n\ndef clean_frontiersin(soup=''):\n    soup = list(map(lambda x: '0' if re.search('\\n', x) else x, soup))\n    soup = list(map(lambda y: '1' if re.search('↑', y) else y, soup))\n    soup = list(filter(lambda x: x != '0', soup))\n    soup = list(map(clean_brackets, soup))\n    soup = list(map(remove_http_url, soup))\n    try:\n        soup = soup[:soup.index('1')]\n        \n    except Exception as e:\n        pass\n    \n    return soup\n\ndef remove_copyright(soup=''):\n    text = ['The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.',\n            'The author declares that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.']\n    for t in text:\n        try:\n            soup = soup[:soup.index(t)]\n        except Exception as e:\n            pass\n    return soup\n\n# remove some artifacts not present in competition data\ndef clean_brackets(text):\n    cleaned = re.sub(r'\\[([a-zA-Z0-9]+)\\]', '', text)\n    cleaned = re.sub(r'\\((Figure(s) .+)\\)', '', cleaned)\n    cleaned = re.sub(r'\\((see Figure .+)\\)', '', cleaned)\n    cleaned = re.sub(r'\\[([\\w\\d\\s\\W]+)\\]', '', cleaned)\n    return cleaned\n\ndef remove_http_url(soup):\n    soup = re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", '', soup)\n    return soup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# kids.frontiersin.org","metadata":{}},{"cell_type":"code","source":"frontier = has_text[has_text['domain'] == 'kids.frontiersin.org'].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrontier_text = frontier['url_legal'].map(requests.get)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frontier_soup = frontier_text.apply(lambda x: x.text)\nfrontier_soup = frontier_soup.map(show_html)\nfrontier_soup = frontier_soup.map(clean_frontiersin)\nfrontier_soup = frontier_soup.map(remove_copyright)\nfrontier_soup = frontier_soup.map(lambda x: '\\n'.join(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frontier['external_text'] = frontier_soup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# en.wikibooks.org","metadata":{}},{"cell_type":"code","source":"wikibooks = has_text[has_text['domain'] == 'en.wikibooks.org'].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwikibooks_text = wikibooks['url_legal'].map(requests.get)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikibooks_soup = wikibooks_text.apply(lambda x: x.text)\nwikibooks_soup = wikibooks_soup.map(show_html)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikibooks_soup[0] = list(filter(lambda x: x != '\\n', clean_http(wikibooks_soup[0])[:-5]))\nwikibooks_soup[1] = list(filter(lambda x: x != '\\n',clean_http(wikibooks_soup[1])[:-1]))\nwikibooks_soup[2] = list(filter(lambda x: x != '\\n', wikibooks_soup[2]))\nwikibooks_soup[6] = wikibooks_soup[6][:7] + wikibooks_soup[6][9:]\nwikibooks_soup = wikibooks_soup.map(lambda x: ''.join(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikibooks['external_text'] = wikibooks_soup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# simple.wikipedia.org","metadata":{}},{"cell_type":"code","source":"def show_html_wiki(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    words = []\n    \n    # remove tables\n    for table in soup.find_all('table'):\n        table.decompose()\n    \n    # remove spans\n    for span in soup.find_all('span'):\n        span.decompose()\n        \n    # remove un-ordered lists\n    for ul in soup.find_all('ul'):\n        ul.decompose()\n        \n    # remove ordered lists\n    for ol in soup.find_all('ol'):\n        ol.decompose()\n\n    for paragraph in soup.find_all('p'):\n        # remove sup tags\n        if paragraph.sup:\n            for support in paragraph.find_all('sup'):\n                support.decompose()\n        cleaned = remove_ufeff(paragraph.get_text())\n        cleaned = remove_xa0(cleaned)\n        words.append(cleaned)\n    \n    return words\n\n#  remove artifact from using requests library on wikipedia\ndef remove_ufeff(text):\n    return re.sub(r'\\ufeff', '', text)\n\n# remove another artifact\ndef remove_xa0(text):\n    return re.sub(r'\\xa0', '', text)\n\ndef filter_newline(text):\n    text = text.split('\\n')\n    return '\\n'.join(list(filter(lambda x: x != \"\", text)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_wiki = has_text[has_text['domain'] == 'simple.wikipedia.org'].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsimple_wiki_text = simple_wiki['url_legal'].map(requests.get)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_wiki_soup = simple_wiki_text.apply(lambda x: x.text)\nsimple_wiki_soup = simple_wiki_soup.map(show_html_wiki)\nsimple_wiki_soup = simple_wiki_soup.map(lambda x: ''.join(x))\nsimple_wiki_soup = simple_wiki_soup.map(filter_newline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_wiki['external_text'] = simple_wiki_soup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# en.wikipedia.org","metadata":{}},{"cell_type":"code","source":"wiki = has_text[has_text['domain'] == 'en.wikipedia.org'].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwiki_text = wiki['url_legal'].map(requests.get)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki_soup = wiki_text.apply(lambda x: x.text)\nwiki_soup = wiki_soup.map(show_html_wiki)\nwiki_soup = wiki_soup.map(lambda x: ''.join(x))\nwiki_soup = wiki_soup.map(filter_newline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki['external_text'] = wiki_soup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"external = pd.concat([wiki, simple_wiki, wikibooks, frontier])\nexternal.to_csv('external.csv', index=False)\nexternal","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}