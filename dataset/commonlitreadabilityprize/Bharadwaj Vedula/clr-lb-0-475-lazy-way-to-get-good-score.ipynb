{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìù Abstract\n\n* This Notebook shows how to leverage model trained by others to get a decent score.\n* With the amount of quality kaggle kernels published by kagglers one can easily use finetuned model to get decent score. \n* This Notebook uses [Ragnar's](https://www.kaggle.com/ragnar123) finetuned model weights as a Feature Extractor, kindly check out his [Training Notebook](https://www.kaggle.com/ragnar123/commonlit-readability-roberta-tf) and [Inference Notebook](https://www.kaggle.com/ragnar123/commonlit-readability-roberta-tf-inference/data) \n\n\n### Future Work\n* Hyperparameter optimization of Ridge model\n* Using other Feature Extractors\n\n### Versions\n* Version 3 : CV- 0.37 LB- 0.475\n* Version 4 : Setting seed = 123 (same as ragnars seed to avoid data leak) CV - 0.3702 LB- 0.474 ","metadata":{}},{"cell_type":"markdown","source":"# üöö Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport pickle\nfrom tqdm import tqdm\nimport gc\n\nfrom sklearn.model_selection import KFold,StratifiedKFold,train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Input,LSTM,Bidirectional,Embedding,Dense, Conv1D, Dropout , MaxPool1D , MaxPooling1D, GlobalAveragePooling2D , GlobalAveragePooling1D , GlobalMaxPooling1D , concatenate , Flatten\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.models import Model,load_model,save_model , model_from_json\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\n\nfrom transformers import TFBertModel, BertTokenizerFast , BertTokenizer , RobertaTokenizerFast , TFRobertaModel , RobertaConfig , TFAutoModel , AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:16:47.206408Z","iopub.execute_input":"2021-06-26T00:16:47.20681Z","iopub.status.idle":"2021-06-26T00:16:54.705569Z","shell.execute_reply.started":"2021-06-26T00:16:47.20673Z","shell.execute_reply":"2021-06-26T00:16:54.704553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu,True)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:16:54.710205Z","iopub.execute_input":"2021-06-26T00:16:54.712324Z","iopub.status.idle":"2021-06-26T00:16:54.780411Z","shell.execute_reply.started":"2021-06-26T00:16:54.712284Z","shell.execute_reply":"2021-06-26T00:16:54.779519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚öôÔ∏è Parameters","metadata":{}},{"cell_type":"code","source":"max_len = 250\nbatch_size = 32\nAUTOTUNE = tf.data.AUTOTUNE\nSEED = 123\n\nMODEL=['bert-base-uncased' , 'roberta-base']\n\nmodel_name = MODEL[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:16:54.785753Z","iopub.execute_input":"2021-06-26T00:16:54.78829Z","iopub.status.idle":"2021-06-26T00:16:54.79457Z","shell.execute_reply.started":"2021-06-26T00:16:54.788232Z","shell.execute_reply":"2021-06-26T00:16:54.793617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path=[\n    \"../input/commonlitreadabilityprize/sample_submission.csv\",\n    \"../input/commonlitreadabilityprize/test.csv\",\n    \"../input/commonlitreadabilityprize/train.csv\"\n]\n\ndf_train = pd.read_csv(path[2])\ndf_test = pd.read_csv(path[1])\ndf_ss = pd.read_csv(path[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:16:54.799403Z","iopub.execute_input":"2021-06-26T00:16:54.800012Z","iopub.status.idle":"2021-06-26T00:16:54.912856Z","shell.execute_reply.started":"2021-06-26T00:16:54.799974Z","shell.execute_reply":"2021-06-26T00:16:54.912056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(['url_legal','license','standard_error'],axis='columns')\ndf_test = df_test.drop(['url_legal','license'],axis='columns')","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:16:54.914046Z","iopub.execute_input":"2021-06-26T00:16:54.914375Z","iopub.status.idle":"2021-06-26T00:16:54.92778Z","shell.execute_reply.started":"2021-06-26T00:16:54.914338Z","shell.execute_reply":"2021-06-26T00:16:54.92629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= df_train['excerpt']\ny=df_train['target'].values\n\nX_test = df_test['excerpt']","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:16:54.929445Z","iopub.execute_input":"2021-06-26T00:16:54.929947Z","iopub.status.idle":"2021-06-26T00:16:54.936264Z","shell.execute_reply.started":"2021-06-26T00:16:54.929908Z","shell.execute_reply":"2021-06-26T00:16:54.93522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer1 = AutoTokenizer.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\")\ntokenizer1","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:16:54.937941Z","iopub.execute_input":"2021-06-26T00:16:54.938628Z","iopub.status.idle":"2021-06-26T00:16:55.089582Z","shell.execute_reply.started":"2021-06-26T00:16:54.938444Z","shell.execute_reply":"2021-06-26T00:16:55.088636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"print('tokenization')\ntrain_embeddings = tokenizer1(X.to_list(), truncation = True , padding = 'max_length' , max_length=max_len)\ntest_embeddings = tokenizer1(X_test.to_list() , truncation = True , padding = 'max_length' , max_length = max_len)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:16:55.092425Z","iopub.execute_input":"2021-06-26T00:16:55.092752Z","iopub.status.idle":"2021-06-26T00:16:56.585054Z","shell.execute_reply.started":"2021-06-26T00:16:55.092724Z","shell.execute_reply":"2021-06-26T00:16:56.58407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìä Dataset Preperation","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef map_function(encodings):\n    input_ids = encodings['input_ids']\n    \n    return {'input_word_ids': input_ids}\n\nprint(\"generating train and test\")    \ntrain = tf.data.Dataset.from_tensor_slices((train_embeddings))\ntrain = (\n            train\n            .map(map_function, num_parallel_calls=AUTOTUNE)\n            .batch(16)\n            .prefetch(AUTOTUNE)\n        )\n\n\ntest = tf.data.Dataset.from_tensor_slices((test_embeddings))\ntest = (\n        test\n        .map(map_function, num_parallel_calls = AUTOTUNE)\n        .batch(16)\n        .prefetch(AUTOTUNE)\n    )","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:16:56.589387Z","iopub.execute_input":"2021-06-26T00:16:56.589684Z","iopub.status.idle":"2021-06-26T00:17:06.458451Z","shell.execute_reply.started":"2021-06-26T00:16:56.589655Z","shell.execute_reply":"2021-06-26T00:17:06.457637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üß† Modelling","metadata":{}},{"cell_type":"markdown","source":"#### Replicating Ragnars model architecture ","metadata":{}},{"cell_type":"code","source":"def build_roberta_base_model(max_len=max_len ):\n    \n    transformer = TFAutoModel.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\")\n    \n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    \n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    \n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = output)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:17:06.459759Z","iopub.execute_input":"2021-06-26T00:17:06.460087Z","iopub.status.idle":"2021-06-26T00:17:06.466441Z","shell.execute_reply.started":"2021-06-26T00:17:06.460051Z","shell.execute_reply":"2021-06-26T00:17:06.465375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ragnar_model = build_roberta_base_model()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:17:06.467911Z","iopub.execute_input":"2021-06-26T00:17:06.468583Z","iopub.status.idle":"2021-06-26T00:17:20.971292Z","shell.execute_reply.started":"2021-06-26T00:17:06.468543Z","shell.execute_reply":"2021-06-26T00:17:20.970398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ragnar_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:17:20.972594Z","iopub.execute_input":"2021-06-26T00:17:20.972953Z","iopub.status.idle":"2021-06-26T00:17:20.994769Z","shell.execute_reply.started":"2021-06-26T00:17:20.972918Z","shell.execute_reply":"2021-06-26T00:17:20.993904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction Model","metadata":{}},{"cell_type":"code","source":"def feature_extractor(path):\n    print(\"loading weights\")\n    ragnar_model.load_weights(path)\n    x= ragnar_model.layers[-3].output\n    model = Model(inputs = ragnar_model.inputs , outputs = x)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:17:20.995954Z","iopub.execute_input":"2021-06-26T00:17:20.996291Z","iopub.status.idle":"2021-06-26T00:17:21.001186Z","shell.execute_reply.started":"2021-06-26T00:17:20.996254Z","shell.execute_reply":"2021-06-26T00:17:21.000109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds(model,train,test):\n    print(\"Extracting Features from train data\")\n    train_features = model.predict( train , verbose =1)\n    train_features = train_features.last_hidden_state\n    train_features = train_features[: , 0 , :]\n    print(\"Extracting Features from train data\")\n    test_features = model.predict( test , verbose =1)\n    test_features = test_features.last_hidden_state\n    test_features = test_features[: , 0 , :]\n    \n    return np.array(train_features , dtype= np.float16) , np.array(test_features , dtype= np.float16) ","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:17:21.002712Z","iopub.execute_input":"2021-06-26T00:17:21.003367Z","iopub.status.idle":"2021-06-26T00:17:21.010739Z","shell.execute_reply.started":"2021-06-26T00:17:21.00333Z","shell.execute_reply":"2021-06-26T00:17:21.009937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üé£ Feature Extraction","metadata":{}},{"cell_type":"code","source":"#model weight paths\npaths=[\"../input/commonlit-readability-roberta-base/Roberta_Base_123_1.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_2.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_3.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_4.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_5.h5\"\n      ]","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:17:21.011942Z","iopub.execute_input":"2021-06-26T00:17:21.012296Z","iopub.status.idle":"2021-06-26T00:17:21.022342Z","shell.execute_reply.started":"2021-06-26T00:17:21.012259Z","shell.execute_reply":"2021-06-26T00:17:21.021518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#1\nextraction_model = feature_extractor(paths[0])\ntrain_embeddings1 , test_embeddings1 = get_preds(extraction_model , train , test)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:17:21.02367Z","iopub.execute_input":"2021-06-26T00:17:21.024033Z","iopub.status.idle":"2021-06-26T00:18:03.719073Z","shell.execute_reply.started":"2021-06-26T00:17:21.023994Z","shell.execute_reply":"2021-06-26T00:18:03.718288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2\nextraction_model = feature_extractor(paths[1])\ntrain_embeddings2 , test_embeddings2 = get_preds(extraction_model , train , test)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:18:03.721717Z","iopub.execute_input":"2021-06-26T00:18:03.721973Z","iopub.status.idle":"2021-06-26T00:18:47.210898Z","shell.execute_reply.started":"2021-06-26T00:18:03.721947Z","shell.execute_reply":"2021-06-26T00:18:47.209941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3\nextraction_model = feature_extractor(paths[2])\ntrain_embeddings3 , test_embeddings3 = get_preds(extraction_model , train , test)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:18:47.212323Z","iopub.execute_input":"2021-06-26T00:18:47.212709Z","iopub.status.idle":"2021-06-26T00:19:30.527418Z","shell.execute_reply.started":"2021-06-26T00:18:47.212653Z","shell.execute_reply":"2021-06-26T00:19:30.526612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4\nextraction_model = feature_extractor(paths[3])\ntrain_embeddings4 , test_embeddings4 = get_preds(extraction_model , train , test)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:19:30.528738Z","iopub.execute_input":"2021-06-26T00:19:30.529217Z","iopub.status.idle":"2021-06-26T00:20:12.892817Z","shell.execute_reply.started":"2021-06-26T00:19:30.529169Z","shell.execute_reply":"2021-06-26T00:20:12.891791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#5\nextraction_model = feature_extractor(paths[4])\ntrain_embeddings5 , test_embeddings5 = get_preds(extraction_model , train , test)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:20:12.896986Z","iopub.execute_input":"2021-06-26T00:20:12.897352Z","iopub.status.idle":"2021-06-26T00:20:56.158151Z","shell.execute_reply.started":"2021-06-26T00:20:12.897316Z","shell.execute_reply":"2021-06-26T00:20:56.157351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üîÑ Kfold Training","metadata":{}},{"cell_type":"markdown","source":"#### Using Ridge","metadata":{}},{"cell_type":"code","source":"def get_preds(train_embeddings , test_embeddings):\n    scores=[]\n    kfold = KFold(n_splits=5, shuffle= True , random_state= SEED)\n    iteration=1\n    preds = np.zeros((test_embeddings.shape[0]))\n    for train_idx, test_idx in kfold.split(train_embeddings,y):\n        print(f'running iteration {iteration}')\n        X_train = train_embeddings[train_idx]\n        X_test = train_embeddings[test_idx]\n        y_train = y[train_idx]\n        y_test = y[test_idx]\n\n        regression_model = Ridge()\n        \n        regression_model.fit(X_train,y_train)\n        y_pred = regression_model.predict(X_test)\n\n        score = np.sqrt(mse(y_pred,y_test))\n        scores.append(score)\n        print(f'Fold {iteration} , rmse score: {score}')\n        y_preds = regression_model.predict(test_embeddings)\n        y_preds=y_preds.reshape(-1)\n        preds+=y_preds  \n        iteration += 1\n\n    print(f\"the average rmse is {np.mean(scores)}\")\n    return np.array(preds)/5  ","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:20:56.159512Z","iopub.execute_input":"2021-06-26T00:20:56.159862Z","iopub.status.idle":"2021-06-26T00:20:56.167528Z","shell.execute_reply.started":"2021-06-26T00:20:56.159825Z","shell.execute_reply":"2021-06-26T00:20:56.166551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"***********predicting***********\")\npreds1 = get_preds(train_embeddings1,test_embeddings1)\nprint(\"***********predicting***********\")\npreds2 = get_preds(train_embeddings2,test_embeddings2)\nprint(\"***********predicting***********\")\npreds3 = get_preds(train_embeddings3,test_embeddings3)\nprint(\"***********predicting***********\")\npreds4 = get_preds(train_embeddings4,test_embeddings4)\nprint(\"***********predicting***********\")\npreds5 = get_preds(train_embeddings5,test_embeddings5)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:20:56.169043Z","iopub.execute_input":"2021-06-26T00:20:56.169622Z","iopub.status.idle":"2021-06-26T00:20:59.108992Z","shell.execute_reply.started":"2021-06-26T00:20:56.169572Z","shell.execute_reply":"2021-06-26T00:20:59.108094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üíØ Submission","metadata":{}},{"cell_type":"code","source":"preds=(preds1+preds2+preds3+preds4+preds5)/5\npreds = preds.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:20:59.115826Z","iopub.execute_input":"2021-06-26T00:20:59.118415Z","iopub.status.idle":"2021-06-26T00:20:59.125211Z","shell.execute_reply.started":"2021-06-26T00:20:59.118373Z","shell.execute_reply":"2021-06-26T00:20:59.124524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.DataFrame({'id':df_ss['id'],'target':preds})\nsub.to_csv('submission.csv',index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T00:20:59.12896Z","iopub.execute_input":"2021-06-26T00:20:59.132046Z","iopub.status.idle":"2021-06-26T00:20:59.41818Z","shell.execute_reply.started":"2021-06-26T00:20:59.131998Z","shell.execute_reply":"2021-06-26T00:20:59.41742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Thanks for viewing, drop your suggestions down in the comments below. üôÇ","metadata":{}}]}