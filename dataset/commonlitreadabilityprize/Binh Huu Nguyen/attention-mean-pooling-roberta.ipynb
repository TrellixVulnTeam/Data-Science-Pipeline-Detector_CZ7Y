{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (AutoModel, AutoTokenizer,AutoConfig,\n                          get_cosine_schedule_with_warmup)\n\nfrom transformers import (AutoModelForMaskedLM, \n                            LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","metadata":{"execution":{"iopub.status.busy":"2021-07-25T11:11:01.078838Z","iopub.execute_input":"2021-07-25T11:11:01.079132Z","iopub.status.idle":"2021-07-25T11:11:08.076303Z","shell.execute_reply.started":"2021-07-25T11:11:01.079063Z","shell.execute_reply":"2021-07-25T11:11:08.075347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nconfig = {\n    'lr': 2e-5,\n    'wd':0.01,\n    'batch_size':16,\n    'valid_step':20,\n    'max_len':300,\n    'epochs':5,\n    'nfolds':5,\n    'seed':42,\n\n    'model_path':'../input/clrp-roberta-base/clrp_roberta_base',\n}\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-25T11:11:08.077957Z","iopub.execute_input":"2021-07-25T11:11:08.078347Z","iopub.status.idle":"2021-07-25T11:11:08.100091Z","shell.execute_reply.started":"2021-07-25T11:11:08.078295Z","shell.execute_reply":"2021-07-25T11:11:08.099242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRPTestDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len=256):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        return encode # encode has format {'input_ids': tensor, 'attention_mask'}\n    \n    def __len__(self):\n        return len(self.excerpt)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T11:11:08.102125Z","iopub.execute_input":"2021-07-25T11:11:08.102553Z","iopub.status.idle":"2021-07-25T11:11:08.109793Z","shell.execute_reply.started":"2021-07-25T11:11:08.102509Z","shell.execute_reply":"2021-07-25T11:11:08.108411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","metadata":{"execution":{"iopub.status.busy":"2021-07-25T11:11:08.111906Z","iopub.execute_input":"2021-07-25T11:11:08.112301Z","iopub.status.idle":"2021-07-25T11:11:08.123487Z","shell.execute_reply.started":"2021-07-25T11:11:08.112259Z","shell.execute_reply":"2021-07-25T11:11:08.122507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionModel(nn.Module):\n    def __init__(self,path):\n        super(AttentionModel,self).__init__()\n        self.roberta = AutoModel.from_pretrained(path)  \n        self.config = AutoConfig.from_pretrained(path)\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size) # self attention\n        self.mlp = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(self.config.hidden_size,128),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(128,1)\n        )\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0] # last_hidden_state \n        x = self.head(x)\n        x = self.mlp(x)\n        return x\n    \n\nclass MeanPoolingModel(nn.Module):\n    \n    def __init__(self, path):\n        super(MeanPoolingModel,self).__init__()\n        \n        self.config = AutoConfig.from_pretrained(path)\n        self.roberta = AutoModel.from_pretrained(path)\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n        \n    def forward(self, **xb):\n        attention_mask = xb['attention_mask']\n        \n        outputs = self.roberta(**xb)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        norm_mean_embeddings = self.layer_norm(mean_embeddings)\n        logits = self.linear(norm_mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        return preds.view(-1).float()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T11:11:08.125045Z","iopub.execute_input":"2021-07-25T11:11:08.125456Z","iopub.status.idle":"2021-07-25T11:11:08.14022Z","shell.execute_reply.started":"2021-07-25T11:11:08.125393Z","shell.execute_reply":"2021-07-25T11:11:08.139365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\ntest_ds = CLRPTestDataset(test_data,tokenizer,config['max_len'])\nvalid_dl = DataLoader(test_ds,\n                    batch_size = 16,\n                    shuffle=False,\n                    num_workers = 4,\n                    pin_memory=True,\n                    drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T11:11:08.14167Z","iopub.execute_input":"2021-07-25T11:11:08.142033Z","iopub.status.idle":"2021-07-25T11:11:08.380913Z","shell.execute_reply.started":"2021-07-25T11:11:08.141998Z","shell.execute_reply":"2021-07-25T11:11:08.380066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrain_folder = '../input/pytorch-roberta-meanpooling-attention'","metadata":{"execution":{"iopub.status.busy":"2021-07-25T11:11:08.382189Z","iopub.execute_input":"2021-07-25T11:11:08.382701Z","iopub.status.idle":"2021-07-25T11:11:08.387323Z","shell.execute_reply.started":"2021-07-25T11:11:08.382663Z","shell.execute_reply":"2021-07-25T11:11:08.386278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ntotal_preds = np.zeros((test_data.shape[0], 1))\nmodel_type = 'attention'\nfor i in range(config['nfolds']):\n    \n    model_name = f'model{i}'\n    model_path = f'{pretrain_folder}/{model_type}/{model_name}/{model_name}.bin'\n    model = None\n    if model_type == 'attention':\n        model = AttentionModel(config['model_path'])\n    elif model_type == 'mean':\n        model = MeanPoolingModel(config['model_path'])\n    model.load_state_dict(torch.load(model_path))\n    preds = []\n    for valid_enum in valid_dl:\n        x_test = {key:val.reshape(val.shape[0],-1) for key,val in valid_enum.items()}\n        preds.append(model(**x_test).detach().numpy())\n        gc.collect()\n    preds = np.concatenate(preds).reshape(test_data.shape[0], 1)\n    total_preds += preds\n    gc.collect()\nfinal_preds = total_preds/config['nfolds']","metadata":{"execution":{"iopub.status.busy":"2021-07-25T11:12:00.896128Z","iopub.execute_input":"2021-07-25T11:12:00.896556Z","iopub.status.idle":"2021-07-25T11:13:23.326773Z","shell.execute_reply.started":"2021-07-25T11:12:00.896501Z","shell.execute_reply":"2021-07-25T11:13:23.325843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissions = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\nsubmissions['target'] = final_preds\nsubmissions.to_csv('submission.csv', index = False)\nsubmissions","metadata":{"execution":{"iopub.status.busy":"2021-07-25T11:13:23.328421Z","iopub.execute_input":"2021-07-25T11:13:23.328749Z","iopub.status.idle":"2021-07-25T11:13:23.358121Z","shell.execute_reply.started":"2021-07-25T11:13:23.328715Z","shell.execute_reply":"2021-07-25T11:13:23.356944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}