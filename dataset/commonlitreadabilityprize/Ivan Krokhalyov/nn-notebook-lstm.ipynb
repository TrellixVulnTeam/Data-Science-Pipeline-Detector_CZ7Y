{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:00.94955Z","iopub.execute_input":"2022-01-03T11:31:00.950202Z","iopub.status.idle":"2022-01-03T11:31:00.955466Z","shell.execute_reply.started":"2022-01-03T11:31:00.950164Z","shell.execute_reply":"2022-01-03T11:31:00.95461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Загрузка данных","metadata":{}},{"cell_type":"code","source":"input_path = \"/kaggle/input/commonlitreadabilityprize/\"\ntrain = pd.read_csv(input_path+\"train.csv\", usecols = [\"excerpt\",\"target\"])\ntest = pd.read_csv(input_path+\"test.csv\", usecols=[\"excerpt\"])\nsub = pd.read_csv(input_path+\"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:00.95732Z","iopub.execute_input":"2022-01-03T11:31:00.957785Z","iopub.status.idle":"2022-01-03T11:31:01.009292Z","shell.execute_reply.started":"2022-01-03T11:31:00.957716Z","shell.execute_reply":"2022-01-03T11:31:01.008566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport re\nfrom collections import Counter\nimport string","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:01.011324Z","iopub.execute_input":"2022-01-03T11:31:01.01176Z","iopub.status.idle":"2022-01-03T11:31:01.016163Z","shell.execute_reply.started":"2022-01-03T11:31:01.011723Z","shell.execute_reply":"2022-01-03T11:31:01.015344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Подготовка текста (токенизация, создание словаря)","metadata":{}},{"cell_type":"code","source":"tok = spacy.load('en_core_web_sm')\ndef tokenize (text):\n    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n    nopunct = regex.sub(\" \", text.lower())\n    return [token.text for token in tok.tokenizer(nopunct)]","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:01.018537Z","iopub.execute_input":"2022-01-03T11:31:01.018933Z","iopub.status.idle":"2022-01-03T11:31:01.594486Z","shell.execute_reply.started":"2022-01-03T11:31:01.018895Z","shell.execute_reply":"2022-01-03T11:31:01.593767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count number of occurences of each word\ncounts = Counter()\nfor text in list(train['excerpt']):\n    counts.update(tokenize(text))","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:01.596016Z","iopub.execute_input":"2022-01-03T11:31:01.596285Z","iopub.status.idle":"2022-01-03T11:31:05.923799Z","shell.execute_reply.started":"2022-01-03T11:31:01.596249Z","shell.execute_reply":"2022-01-03T11:31:05.922918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deleting infrequent words\nprint(\"num_words before:\",len(counts.keys()))\nfor word in list(counts):\n    if counts[word] < 2:\n        del counts[word]\nprint(\"num_words after:\",len(counts.keys()))","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:05.925015Z","iopub.execute_input":"2022-01-03T11:31:05.925435Z","iopub.status.idle":"2022-01-03T11:31:05.968293Z","shell.execute_reply.started":"2022-01-03T11:31:05.925398Z","shell.execute_reply":"2022-01-03T11:31:05.967571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating vocabulary\nvocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:05.969564Z","iopub.execute_input":"2022-01-03T11:31:05.969929Z","iopub.status.idle":"2022-01-03T11:31:05.990279Z","shell.execute_reply.started":"2022-01-03T11:31:05.969898Z","shell.execute_reply":"2022-01-03T11:31:05.989654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_sentence(text, vocab2index, N=200):\n    tokenized = tokenize(text)\n    encoded = np.zeros(N, dtype=int)\n    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n    length = min(N, len(enc1))\n    encoded[:length] = enc1[:length]\n    return encoded, length","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:05.991555Z","iopub.execute_input":"2022-01-03T11:31:05.991987Z","iopub.status.idle":"2022-01-03T11:31:05.998178Z","shell.execute_reply.started":"2022-01-03T11:31:05.991949Z","shell.execute_reply":"2022-01-03T11:31:05.997475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['encoded'] = train['excerpt'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:05.999405Z","iopub.execute_input":"2022-01-03T11:31:05.999934Z","iopub.status.idle":"2022-01-03T11:31:09.290859Z","shell.execute_reply.started":"2022-01-03T11:31:05.9999Z","shell.execute_reply":"2022-01-03T11:31:09.290042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.294022Z","iopub.execute_input":"2022-01-03T11:31:09.294457Z","iopub.status.idle":"2022-01-03T11:31:09.299495Z","shell.execute_reply.started":"2022-01-03T11:31:09.294424Z","shell.execute_reply":"2022-01-03T11:31:09.298503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Класс Dataset (особенность pytorch - нужно переопределять dataset под свои нужды)","metadata":{}},{"cell_type":"code","source":"class CommonLitReadabiltyDataset(Dataset):\n    def __init__(self, X, Y):\n        self.X = X\n        self.y = Y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx] #, self.X[idx][1]","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.301311Z","iopub.execute_input":"2022-01-03T11:31:09.301644Z","iopub.status.idle":"2022-01-03T11:31:09.310438Z","shell.execute_reply.started":"2022-01-03T11:31:09.301573Z","shell.execute_reply":"2022-01-03T11:31:09.309439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.312244Z","iopub.execute_input":"2022-01-03T11:31:09.312793Z","iopub.status.idle":"2022-01-03T11:31:09.320056Z","shell.execute_reply.started":"2022-01-03T11:31:09.312763Z","shell.execute_reply":"2022-01-03T11:31:09.319056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = list(train['encoded'])\ny = list(train['target'])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n\ntrain_dataset = CommonLitReadabiltyDataset(X_train, y_train)\nval_dataset = CommonLitReadabiltyDataset(X_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.322232Z","iopub.execute_input":"2022-01-03T11:31:09.322727Z","iopub.status.idle":"2022-01-03T11:31:09.335255Z","shell.execute_reply.started":"2022-01-03T11:31:09.322673Z","shell.execute_reply":"2022-01-03T11:31:09.334363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Класс модели","metadata":{}},{"cell_type":"code","source":"class LSTM_model(nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, linear1) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear1 = nn.Linear(hidden_dim, linear1)\n        self.act1 = nn.Tanh()\n        self.linear2 = nn.Linear(linear1, 1)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        x = self.act1( self.linear1(ht[-1]) )\n        return self.linear2(x)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.336688Z","iopub.execute_input":"2022-01-03T11:31:09.337509Z","iopub.status.idle":"2022-01-03T11:31:09.349462Z","shell.execute_reply.started":"2022-01-03T11:31:09.337467Z","shell.execute_reply":"2022-01-03T11:31:09.348607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(words)\nembedding_dim = 500\nhidden_dim = 80\nlinear1 = 50\n\nmodel_ft =  LSTM_model(vocab_size, embedding_dim, hidden_dim, linear1).to('cuda')\nmodel_ft","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.351084Z","iopub.execute_input":"2022-01-03T11:31:09.351508Z","iopub.status.idle":"2022-01-03T11:31:09.508836Z","shell.execute_reply.started":"2022-01-03T11:31:09.351465Z","shell.execute_reply":"2022-01-03T11:31:09.508044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model,criterion,optimizer,dataset,epoch):\n    \n    train_dataset=dataset\n    data_loader=DataLoader(dataset,batch_size=4,shuffle=True,num_workers=4)\n    dataset_size=len(dataset)\n    \n    print(f\"Epoch#{epoch}. Train\")\n    model.train()\n    \n    running_loss=0.0   #накопление лосса    \n    epoch_loss=0.0\n    \n    for inputs,labels in tqdm( data_loader):\n        inputs=inputs.to('cuda').type(torch.long)\n        labels=labels.to('cuda').type(torch.float) #передаем батч на GPU(cuda)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss=criterion(outputs,labels)\n        loss.backward() # обратное распостранение градиента\n        optimizer.step() # шаг оптимизатора\n        running_loss+=loss.item()*inputs.size(0)\n    epoch_loss = running_loss / dataset_size\n    print(f'Loss RMSE: { np.sqrt(epoch_loss) }')\n    print(f\"Epoch#{epoch} (Train) completed.\")\n    return model, epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.510071Z","iopub.execute_input":"2022-01-03T11:31:09.510487Z","iopub.status.idle":"2022-01-03T11:31:09.527778Z","shell.execute_reply.started":"2022-01-03T11:31:09.510451Z","shell.execute_reply":"2022-01-03T11:31:09.523459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_epoch(model,criterion,optimizer,dataset,epoch):\n    val_dataset=dataset\n    data_loader=DataLoader(dataset,batch_size=4,shuffle=True,num_workers=4)\n    dataset_size=len(val_dataset)\n    print(f\"Epoch#{epoch}. Validation\")\n    model.eval()\n    running_loss=0.0 # накопление лосc\n    epoch_loss=0.0\n    with torch.no_grad():\n        for inputs,labels in tqdm( data_loader):\n            inputs=inputs.to('cuda').type(torch.long)\n            labels=labels.to('cuda').type(torch.float) #передаем батч на GPU(cuda)\n            outputs = model(inputs)\n            loss=criterion(outputs,labels)\n            running_loss+=loss.item()*inputs.size(0)\n    epoch_loss = running_loss / dataset_size\n    print(f'Loss RMSE: { np.sqrt(epoch_loss) } ')\n    print(f\"Epoch#{epoch} (Validation) completed. \")\n    return model, epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.532063Z","iopub.execute_input":"2022-01-03T11:31:09.532806Z","iopub.status.idle":"2022-01-03T11:31:09.548119Z","shell.execute_reply.started":"2022-01-03T11:31:09.532766Z","shell.execute_reply":"2022-01-03T11:31:09.544747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.552073Z","iopub.execute_input":"2022-01-03T11:31:09.553595Z","iopub.status.idle":"2022-01-03T11:31:09.561356Z","shell.execute_reply.started":"2022-01-03T11:31:09.553538Z","shell.execute_reply":"2022-01-03T11:31:09.560241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.MSELoss()\noptimizer= optim.Adam(params=model_ft.parameters(),lr=3e-5)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.563389Z","iopub.execute_input":"2022-01-03T11:31:09.563999Z","iopub.status.idle":"2022-01-03T11:31:09.571385Z","shell.execute_reply.started":"2022-01-03T11:31:09.563957Z","shell.execute_reply":"2022-01-03T11:31:09.570502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Тренировка модели","metadata":{}},{"cell_type":"code","source":"best_model = model_ft\nbest_epoch = 1\nbest_loss = 1000000\nnum_epochs = 10\n\ntrain_loss_history = []\nval_loss_history = []\n\nfor epoch in range(1,num_epochs+1):\n    #тренировка\n    model_ft, train_loss = train_epoch(model_ft,criterion,optimizer,train_dataset,epoch)\n    train_loss_history.append(train_loss)\n    \n    #валидация\n    model_ft, val_loss = valid_epoch(model_ft,criterion,optimizer,val_dataset,epoch)\n    val_loss_history.append(val_loss)\n    \n    if(val_loss<best_loss):\n        best_model = model_ft\n        best_epoch = epoch","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:31:09.573133Z","iopub.execute_input":"2022-01-03T11:31:09.573741Z","iopub.status.idle":"2022-01-03T11:32:27.806722Z","shell.execute_reply.started":"2022-01-03T11:31:09.573699Z","shell.execute_reply":"2022-01-03T11:32:27.805885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference - для соревнования на Kaggle (вывод на тестовой выборке)","metadata":{}},{"cell_type":"code","source":"model = best_model\nmodel.eval()\n\ntest['encoded'] = test['excerpt'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\nexcerpts_test = test['encoded']\n\nX_test = [excerpts_test[i][0] for i in range(len(test))]\nX_test = torch.LongTensor(X_test).to('cuda')\n\ny_hat = model(X_test)\ny_hat","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:32:27.808237Z","iopub.execute_input":"2022-01-03T11:32:27.808466Z","iopub.status.idle":"2022-01-03T11:32:27.840366Z","shell.execute_reply.started":"2022-01-03T11:32:27.808435Z","shell.execute_reply":"2022-01-03T11:32:27.839495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[\"target\"] = y_hat.cpu().detach().numpy()\nsub","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:33:00.713121Z","iopub.execute_input":"2022-01-03T11:33:00.713859Z","iopub.status.idle":"2022-01-03T11:33:00.724843Z","shell.execute_reply.started":"2022-01-03T11:33:00.71382Z","shell.execute_reply":"2022-01-03T11:33:00.723986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission_lstm.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T11:33:22.980561Z","iopub.execute_input":"2022-01-03T11:33:22.981139Z","iopub.status.idle":"2022-01-03T11:33:22.989763Z","shell.execute_reply.started":"2022-01-03T11:33:22.981087Z","shell.execute_reply":"2022-01-03T11:33:22.988869Z"},"trusted":true},"execution_count":null,"outputs":[]}]}