{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**【Points】**\n\n・I used RobertaForSequenceClassification model for prediction.\n\n・Some parameters are freezed during training.\n\n・I focused on a better understanding of RoBERTa than Leaderboard.\n\n---\n\n**Comments**: Thanks to previous great Notebooks.\n\n1. [Pytorch BERT beginner's room][1]\n\n2. [CLRP: Pytorch Roberta Finetune][2]\n\n[1]: https://www.kaggle.com/chumajin/pytorch-bert-beginner-s-room\n\n[2]: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune","metadata":{}},{"cell_type":"code","source":"# Import Dependencies\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport os\nimport random\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport transformers\n\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:00:47.478444Z","iopub.execute_input":"2021-12-10T03:00:47.478866Z","iopub.status.idle":"2021-12-10T03:00:49.87515Z","shell.execute_reply.started":"2021-12-10T03:00:47.478789Z","shell.execute_reply":"2021-12-10T03:00:49.874048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Settings\nconfig = {\n    'train_batch_size': 16,\n    'valid_batch_size': 32,\n    'max_len': 314,\n    'nfolds': 5,\n    'seed': 42,\n}\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'{device} is used')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.detarministic = True \n    torch.backends.cudnn.benchmark = True \n\nseed_everything(seed=config['seed'])\n\ndef rmse_score(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:00:51.479558Z","iopub.execute_input":"2021-12-10T03:00:51.480277Z","iopub.status.idle":"2021-12-10T03:00:51.578262Z","shell.execute_reply.started":"2021-12-10T03:00:51.480218Z","shell.execute_reply":"2021-12-10T03:00:51.573734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Datasets and DataLoaders","metadata":{}},{"cell_type":"code","source":"# Load the data\ntrain_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:00:53.926661Z","iopub.execute_input":"2021-12-10T03:00:53.927063Z","iopub.status.idle":"2021-12-10T03:00:54.051393Z","shell.execute_reply.started":"2021-12-10T03:00:53.927033Z","shell.execute_reply":"2021-12-10T03:00:54.050438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# k-fold\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:, 'bins'] = pd.cut(train_data['target'], bins=num_bins, labels=False)\n\ntrain_data['kfold'] = -1\nkfold = StratifiedKFold(n_splits=config['nfolds'],\n                        shuffle=True,\n                        random_state=config['seed'])\nfor k, (train_idx, valid_idx) in enumerate(kfold.split(X=train_data, y=train_data.bins)):\n    train_data.loc[valid_idx, 'kfold'] = k\n","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:00:55.098105Z","iopub.execute_input":"2021-12-10T03:00:55.098461Z","iopub.status.idle":"2021-12-10T03:00:55.12743Z","shell.execute_reply.started":"2021-12-10T03:00:55.098429Z","shell.execute_reply":"2021-12-10T03:00:55.126148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset, DataLoader\nclass CLRPDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.target = df['target'].to_numpy()\n        self.tokenizer = tokenizer \n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, idx):\n        sentence = self.excerpt[idx]\n        sentence = sentence.replace('\\n', ' ')\n        bert_sens = tokenizer.encode_plus(sentence,\n                                          add_special_tokens=True,\n                                          max_length=config['max_len'],\n                                          pad_to_max_length=True,\n                                          truncation=True,\n                                          return_attention_mask=True)\n        ids = torch.tensor(bert_sens['input_ids'])\n        mask = torch.tensor(bert_sens['attention_mask'])\n        targets = torch.tensor(self.target[idx], dtype=torch.float)\n        return {'ids': ids, 'mask': mask, 'targets': targets}\n        ","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:00:56.302661Z","iopub.execute_input":"2021-12-10T03:00:56.303062Z","iopub.status.idle":"2021-12-10T03:00:56.312053Z","shell.execute_reply.started":"2021-12-10T03:00:56.30303Z","shell.execute_reply":"2021-12-10T03:00:56.310526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '../input/roberta-base'\n\ntokenizer = transformers.RobertaTokenizer.from_pretrained(model_path)\n\np_fold = 0\np_train = train_data.query(f'kfold != {p_fold}').reset_index(drop=True)\np_valid = train_data.query(f'kfold == {p_fold}').reset_index(drop=True)\n\ntrain_dataset = CLRPDataset(p_train, tokenizer)\nvalid_dataset = CLRPDataset(p_valid, tokenizer)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=config['train_batch_size'],\n                              shuffle=True, num_workers=4, pin_memory=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=config['valid_batch_size'],\n                              shuffle=False, num_workers=4, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:00:57.764145Z","iopub.execute_input":"2021-12-10T03:00:57.764507Z","iopub.status.idle":"2021-12-10T03:00:58.129842Z","shell.execute_reply.started":"2021-12-10T03:00:57.764477Z","shell.execute_reply":"2021-12-10T03:00:58.12881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"model = transformers.RobertaForSequenceClassification.from_pretrained(model_path, num_labels=1)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:01:01.299443Z","iopub.execute_input":"2021-12-10T03:01:01.299837Z","iopub.status.idle":"2021-12-10T03:01:16.70262Z","shell.execute_reply.started":"2021-12-10T03:01:01.299809Z","shell.execute_reply":"2021-12-10T03:01:16.70156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freezing parameters\nfor param in model.roberta.embeddings.parameters():\n    param.requires_grad = False\n\nfor param in model.roberta.encoder.parameters():\n    param.requires_grad = False \n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:01:16.704544Z","iopub.execute_input":"2021-12-10T03:01:16.705164Z","iopub.status.idle":"2021-12-10T03:01:16.713997Z","shell.execute_reply.started":"2021-12-10T03:01:16.705121Z","shell.execute_reply":"2021-12-10T03:01:16.712977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam([\n    {'params': model.classifier.parameters(), 'lr': 1e-3},\n], betas=(0.9, 0.98))\n\ncriterion = nn.MSELoss()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:01:31.887984Z","iopub.execute_input":"2021-12-10T03:01:31.888348Z","iopub.status.idle":"2021-12-10T03:01:31.893864Z","shell.execute_reply.started":"2021-12-10T03:01:31.888319Z","shell.execute_reply":"2021-12-10T03:01:31.892614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"scaler = torch.cuda.amp.GradScaler()\n\ndef training(train_dataloader, model, optimizer):\n\n    model.train()\n    \n    all_preds = []\n    all_targets = []\n    losses = []\n\n    for a in train_dataloader:\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n\n            ids = a['ids'].to(device, non_blocking=True)\n            mask = a['mask'].to(device, non_blocking=True)\n\n            output = model(ids, mask)\n            output = output['logits'].squeeze(-1)\n\n            target = a['targets'].to(device, non_blocking=True)\n\n            loss = criterion(output, target)\n\n            losses.append(loss.item())\n            all_preds.append(output.detach().cpu().numpy())\n            all_targets.append(target.detach().squeeze(-1).cpu().numpy())\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        del loss\n        torch.cuda.empty_cache()\n\n    all_preds = np.concatenate(all_preds)\n    all_targets = np.concatenate(all_targets)\n    losses = np.mean(losses)\n    train_score = rmse_score(all_targets, all_preds)\n\n    return losses, train_score","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:01:35.794025Z","iopub.execute_input":"2021-12-10T03:01:35.794487Z","iopub.status.idle":"2021-12-10T03:01:35.813203Z","shell.execute_reply.started":"2021-12-10T03:01:35.794446Z","shell.execute_reply":"2021-12-10T03:01:35.811867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validating(valid_dataloader, model):\n\n    model.eval()\n\n    all_preds = []\n    all_targets = []\n    losses = []\n\n    for b in valid_dataloader:\n\n        with torch.no_grad():\n\n            ids = b['ids'].to(device, non_blocking=True)\n            mask = b['mask'].to(device, non_blocking=True)\n\n            output = model(ids, mask)\n            output = output['logits'].squeeze(-1)\n\n            target = b['targets'].to(device, non_blocking=True)\n\n            loss = criterion(output, target)\n            losses.append(loss.item())\n            all_preds.append(output.detach().cpu().numpy())\n            all_targets.append(target.detach().squeeze(-1).cpu().numpy())\n            \n            del loss\n            torch.cuda.empty_cache()\n\n    all_preds = np.concatenate(all_preds)\n    all_targets = np.concatenate(all_targets)\n\n    losses = np.mean(losses)\n    valid_score = rmse_score(all_targets, all_preds)\n    \n    return all_preds, losses, valid_score\n","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:01:36.772165Z","iopub.execute_input":"2021-12-10T03:01:36.772541Z","iopub.status.idle":"2021-12-10T03:01:36.781856Z","shell.execute_reply.started":"2021-12-10T03:01:36.77251Z","shell.execute_reply":"2021-12-10T03:01:36.780684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_valid(epochs=5):\n    \n    train_losses = []\n    valid_losses = []\n    best_score = None \n    \n    train_scores = []\n    valid_scores = []\n    \n    for epoch in tqdm(range(epochs)):\n        print(\"---------------\" + str(epoch) + \"start-------------\\n\")\n        train_loss, train_score = training(train_dataloader, model, optimizer)\n        train_losses.append(train_loss)\n        train_scores.append(train_score)\n        print(f'train_score is {train_score}\\n')\n        \n        preds, valid_loss, valid_score = validating(valid_dataloader, model)\n        valid_losses.append(valid_loss)\n        valid_scores.append(valid_score)\n        print(f'valid_score is {valid_score}\\n')\n        \n        if (epoch+1 >= 3) and (best_score is None):\n            best_score = valid_score\n            torch.save(model.state_dict(), 'model0.pth')\n            print('Save the first model')\n            \n        elif (epoch+1 >= 3) and (best_score > valid_score):\n            best_score = valid_score\n            torch.save(model.state_dict(), 'model0.pth')        \n            print('found better point')\n            \n        else:\n            pass\n        \n    return {'train_losses': train_losses,\n            'valid_losses': valid_losses,\n            'train_scores': train_scores,\n            'valid_scores': valid_scores,\n            'best_score': best_score,\n            'preds': preds}\n        ","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:01:37.978468Z","iopub.execute_input":"2021-12-10T03:01:37.978896Z","iopub.status.idle":"2021-12-10T03:01:37.988362Z","shell.execute_reply.started":"2021-12-10T03:01:37.978866Z","shell.execute_reply":"2021-12-10T03:01:37.987014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nresult = train_and_valid(epochs=epochs)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:01:49.958733Z","iopub.execute_input":"2021-12-10T03:01:49.959112Z","iopub.status.idle":"2021-12-10T03:05:08.848337Z","shell.execute_reply.started":"2021-12-10T03:01:49.959084Z","shell.execute_reply":"2021-12-10T03:05:08.847082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses = result['train_losses']\nvalid_losses = result['valid_losses']\ntrain_scores = result['train_scores']\nvalid_scores = result['valid_scores']\nbest_score = result['best_score']\npreds = result['preds']\n\n#visualization of the results_1\nplt.scatter(p_valid['target'], preds)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:05:14.02826Z","iopub.execute_input":"2021-12-10T03:05:14.028648Z","iopub.status.idle":"2021-12-10T03:05:14.292883Z","shell.execute_reply.started":"2021-12-10T03:05:14.028613Z","shell.execute_reply":"2021-12-10T03:05:14.291623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualization of the results_2\nx = np.arange(epochs)\nplt.plot(x, train_losses)\nplt.plot(x, valid_losses)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:05:19.315589Z","iopub.execute_input":"2021-12-10T03:05:19.316024Z","iopub.status.idle":"2021-12-10T03:05:19.504957Z","shell.execute_reply.started":"2021-12-10T03:05:19.315992Z","shell.execute_reply":"2021-12-10T03:05:19.503688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# un-freezing parameters\nfor param in model.roberta.embeddings.parameters():\n    param.requires_grad = True\n\nfor param in model.roberta.encoder.parameters():\n    param.requires_grad = True\n    \noptimizer = optim.Adam([\n    {'params': model.classifier.parameters(), 'lr': 1e-4},\n    {'params': model.roberta.parameters(), 'lr': 5e-5},\n], betas=(0.9, 0.98))\n\nepochs = 5\nresult = train_and_valid(epochs=epochs)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:05:31.141978Z","iopub.execute_input":"2021-12-10T03:05:31.142378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remaining k-fold\nbest_scores = []\nbest_scores.append(result['best_score'])\n\nfor p_fold in range(1, 5):\n    # initializing the data, model and optimizer\n    p_train = train_data.query(f'kfold != {p_fold}').reset_index(drop=True)\n    p_valid = train_data.query(f'kfold == {p_fold}').reset_index(drop=True)\n\n    train_dataset = CLRPDataset(p_train, tokenizer)\n    valid_dataset = CLRPDataset(p_valid, tokenizer)\n    train_dataloader = DataLoader(train_dataset, batch_size=config['train_batch_size'],\n                                  shuffle=True, num_workers=4, pin_memory=True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=config['valid_batch_size'],\n                                  shuffle=False, num_workers=4, pin_memory=True)\n    \n    model = transformers.RobertaForSequenceClassification.from_pretrained(model_path, num_labels=1)\n    model.to(device)\n    \n    for param in model.roberta.embeddings.parameters():\n        param.requires_grad = False   \n    for param in model.roberta.encoder.parameters():\n        param.requires_grad = False \n\n    optimizer = optim.Adam([\n        {'params': model.classifier.parameters(), 'lr': 1e-3},\n    ], betas=(0.9, 0.98))\n    criterion = nn.MSELoss()\n    scaler = torch.cuda.amp.GradScaler()\n    \n    epochs = 5\n    result = train_and_valid(epochs=epochs)\n    \n    for param in model.roberta.embeddings.parameters():\n        param.requires_grad = True   \n    for param in model.roberta.encoder.parameters():\n        param.requires_grad = True \n\n    optimizer = optim.Adam([\n        {'params': model.classifier.parameters(), 'lr': 1e-4},\n        {'params': model.roberta.parameters(), 'lr': 5e-5},\n    ], betas=(0.9, 0.98))\n    \n    epochs = 5\n    result = train_and_valid(epochs=epochs)\n    best_scores.append(result['best_score'])\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"tokenizer = transformers.RobertaTokenizer.from_pretrained('../input/roberta-base')\n\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset and DataLoader for inference\nclass CLRPInferenceDataset(Dataset):\n\n    def __init__(self, df, tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, idx):\n        sentence = self.excerpt[idx]\n        sentence = sentence.replace('\\n', ' ')\n        roberta_sens = tokenizer.encode_plus(sentence,\n                                            add_special_tokens=True,\n                                            max_length=config['max_len'],\n                                            pad_to_max_length=True,\n                                            truncation=True,\n                                            return_attention_mask=True)\n        ids = torch.tensor(roberta_sens['input_ids'])\n        mask = torch.tensor(roberta_sens['attention_mask'])\n        return {'ids': ids, 'mask': mask}\n\ntest_dataset = CLRPInferenceDataset(test_data, tokenizer)\ntest_dataloader = DataLoader(test_dataset, batch_size=config['valid_batch_size'],\n                             shuffle=False, num_workers=4, pin_memory=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = transformers.RobertaForSequenceClassification.from_pretrained('../input/roberta-base', num_labels=1)\n\nfinetune_result_path = './'\nmodel_names = [s for s in os.listdir(finetune_result_path) if '.pth' in s]\npthes = [os.path.join(finetune_result_path, s) for s in os.listdir(finetune_result_path) if '.pth' in s]\n\ndef clrp_inference(test_dataloader, model, model_names, pthes):\n    all_preds = []\n    all_models = []\n    for model_name, state in zip(model_names, pthes):\n        model.load_state_dict(torch.load(state))\n        model.to(device)\n        model.eval()\n\n        preds = []\n        all_valid_loss = 0\n\n        with torch.no_grad():\n            for a in test_dataloader:\n                ids = a['ids'].to(device)\n                mask = a['mask'].to(device)\n\n                output = model(ids, mask)\n                output = output['logits'].squeeze(-1)\n\n                preds.append(output.cpu().numpy())\n\n            preds = np.concatenate(preds)\n            all_preds.append(preds)\n            all_models.append(model_name)\n\n    print('\\npredicted!')\n    return all_preds, all_models\n\nall_preds, all_models = clrp_inference(test_dataloader, model, model_names, pthes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df = pd.DataFrame(all_preds).T\npreds_df.columns = all_models\n\npreds_df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fin_preds = preds_df.mean(axis=1)\nsample['target'] = fin_preds\nsample\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.to_csv('submission.csv', index=False)\n","metadata":{},"execution_count":null,"outputs":[]}]}