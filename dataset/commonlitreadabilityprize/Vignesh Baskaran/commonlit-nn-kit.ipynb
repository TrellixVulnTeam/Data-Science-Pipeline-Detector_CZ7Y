{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Objective\nThe objective of this notebook is to build a pipeline combining all the components that we have developed until now. In the end it should be possible to convert this notebook into a script so that we can train a Ranker or Regressor on External or internal data.","metadata":{"_uuid":"3b982c40-fbab-459a-8745-56e21b6bc740","_cell_guid":"295c02f3-11c9-495e-ba46-edb75604398f","trusted":true}},{"cell_type":"code","source":"import gc\nimport os\nimport json\nimport wandb\nimport torch\nimport shutil\nimport random\nimport operator\nimport numpy as np\nimport pandas as pd\n\nfrom torch import nn\nfrom scipy import stats\nfrom pathlib import Path\nfrom itertools import chain\nfrom sklearn.pipeline import make_pipeline\nfrom torch.optim.lr_scheduler import CyclicLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom transformers import AdamW, AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup\n\npd.options.display.max_colwidth = None","metadata":{"_uuid":"b2ed4284-3ec2-4953-a240-ece31f7c0b67","_cell_guid":"09b8d72b-72de-40c5-a1b2-58e817f7c631","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:02:59.497069Z","iopub.execute_input":"2021-08-01T22:02:59.49757Z","iopub.status.idle":"2021-08-01T22:03:07.96004Z","shell.execute_reply.started":"2021-08-01T22:02:59.497489Z","shell.execute_reply":"2021-08-01T22:03:07.959176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"_uuid":"266af960-050b-43db-9f3e-02f09f3534be","_cell_guid":"6b6427f8-7963-4326-9bb7-dcac60007eb6","trusted":true}},{"cell_type":"code","source":"class UnoTextDataset(Dataset):\n    def __init__(self, text_excerpts, targets):\n        self.text_excerpts = text_excerpts\n        self.targets = targets\n    \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        sample = {'text_excerpt': self.text_excerpts[idx],\n                  'target': self.targets[idx]}\n        return sample","metadata":{"_uuid":"697cede6-1962-4cec-a792-772f89391758","_cell_guid":"d08dc607-926e-4fd5-95c9-05b85b99359e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:07.961277Z","iopub.execute_input":"2021-08-01T22:03:07.96154Z","iopub.status.idle":"2021-08-01T22:03:07.967462Z","shell.execute_reply.started":"2021-08-01T22:03:07.961511Z","shell.execute_reply":"2021-08-01T22:03:07.966429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DuoTextDataset(Dataset):\n    \"\"\"\n    If the first input is easier to read then the target is 1\n    If the second input is easier to read then the target is -1\n    \"\"\"\n    def __init__(self, text_excerpts_left, text_excerpts_right, targets):\n        self.text_excerpts_left = text_excerpts_left\n        self.text_excerpts_right = text_excerpts_right\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        sample = {'text_excerpt_left': self.text_excerpts_left[idx],\n                  'text_excerpt_right': self.text_excerpts_right[idx],\n                  'target': self.targets[idx]}\n        return sample","metadata":{"_uuid":"25f797f3-459d-4d96-9bb2-7c49ba26200d","_cell_guid":"e44d11a0-6e58-4293-8b8b-806597dd49b7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:07.969681Z","iopub.execute_input":"2021-08-01T22:03:07.970028Z","iopub.status.idle":"2021-08-01T22:03:07.978322Z","shell.execute_reply.started":"2021-08-01T22:03:07.970001Z","shell.execute_reply":"2021-08-01T22:03:07.977505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sampler","metadata":{"_uuid":"53ba59e8-be47-4b2f-8fd7-0812bf53e954","_cell_guid":"392772f2-d7e5-404d-85b5-8c4330c42d11","trusted":true}},{"cell_type":"code","source":"def get_category_weighted_sampler(categories):\n    category_frequency = Counter(categories)\n    weights = [1/category_frequency.get(category) for category in categories]\n    category_weighted_sampler = WeightedRandomSampler(weights=weights, num_samples=len(categories), replacement=True)\n    return category_weighted_sampler","metadata":{"_uuid":"09769970-bf3d-4652-98af-8e6fb7e8242d","_cell_guid":"0b200f69-42e4-4d27-8a50-41ee2b6598fd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:07.980046Z","iopub.execute_input":"2021-08-01T22:03:07.98055Z","iopub.status.idle":"2021-08-01T22:03:07.990005Z","shell.execute_reply.started":"2021-08-01T22:03:07.980464Z","shell.execute_reply":"2021-08-01T22:03:07.9892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{"_uuid":"08defb7a-7bed-4934-9d92-a002665b68dc","_cell_guid":"d1d5a255-7532-47b0-a0ee-ada8c079ec12","trusted":true}},{"cell_type":"code","source":"def create_uno_text_dataloader(data, batch_size, shuffle, sampler, apply_preprocessing=True, num_workers=4, pin_memory=True, drop_last=False):\n    # Preprocessing\n    if apply_preprocessing:\n        data['excerpt'] = data['excerpt'].apply(lambda x: x.replace('\\n', ' '))\n        data['excerpt'] = data['excerpt'].apply(lambda x: ' '.join(x.split()))\n    \n    text_excerpts = data['excerpt'].tolist()\n    targets = data['target'].to_numpy().astype(np.float32).reshape(-1, 1)\n    dataset = UnoTextDataset(text_excerpts=text_excerpts, targets=targets)\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler,\n                            num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n    return dataloader","metadata":{"_uuid":"e982fc02-6181-4423-a133-d492ec38b336","_cell_guid":"1a81efdd-79e6-412b-8cd8-3ec667a2977a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:07.991115Z","iopub.execute_input":"2021-08-01T22:03:07.991377Z","iopub.status.idle":"2021-08-01T22:03:08.004356Z","shell.execute_reply.started":"2021-08-01T22:03:07.991351Z","shell.execute_reply":"2021-08-01T22:03:08.003395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_duo_text_dataloader(data, batch_size, shuffle, sampler, apply_preprocessing=True, num_workers=4, pin_memory=True, drop_last=False):\n    if apply_preprocessing:\n        data['easy_text'] = data['easy_text'].apply(lambda x: x.replace('\\n', ' '))\n        data['easy_text'] = data['easy_text'].apply(lambda x: ' '.join(x.split()))\n\n        data['difficult_text'] = data['difficult_text'].apply(lambda x: x.replace('\\n', ' '))\n        data['difficult_text'] = data['difficult_text'].apply(lambda x: ' '.join(x.split()))\n    \n    text_excerpts_left = data['easy_text'].tolist()\n    text_excerpts_right = data['difficult_text'].tolist()\n    targets = len(data) * [1]\n    targets = np.asarray(targets).astype(np.float32).reshape(-1, 1)\n\n    dataset = DuoTextDataset(text_excerpts_left=text_excerpts_left,\n                             text_excerpts_right=text_excerpts_right,\n                             targets=targets)\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler,\n                            num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n    return dataloader","metadata":{"_uuid":"43038653-de00-4e29-95b8-2299369a181e","_cell_guid":"a31349b7-c1cc-4f04-9821-ec4f4bcf4b6a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.005321Z","iopub.execute_input":"2021-08-01T22:03:08.00559Z","iopub.status.idle":"2021-08-01T22:03:08.016376Z","shell.execute_reply.started":"2021-08-01T22:03:08.005565Z","shell.execute_reply":"2021-08-01T22:03:08.015284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"_uuid":"98780797-a6bb-49f8-aebd-cd1f12aac592","_cell_guid":"13a4fcbe-a1ae-4a59-ba9a-8db5e1ffc073","trusted":true}},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, input_dim, head_hidden_dim):\n        super(AttentionHead, self).__init__()\n        head_hidden_dim = input_dim if head_hidden_dim is None else head_hidden_dim\n        self.W = nn.Linear(input_dim, head_hidden_dim)\n        self.V = nn.Linear(head_hidden_dim, 1)\n        \n    def forward(self, x):\n        attention_scores = self.V(torch.tanh(self.W(x)))\n        attention_scores = torch.softmax(attention_scores, dim=1)\n        attentive_x = attention_scores * x\n        attentive_x = attentive_x.sum(axis=1)\n        return attentive_x","metadata":{"_uuid":"26297be4-dcff-4f3b-9084-d7267d1932aa","_cell_guid":"a83d3fb6-6d75-45de-b665-bdd450600118","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.01747Z","iopub.execute_input":"2021-08-01T22:03:08.017802Z","iopub.status.idle":"2021-08-01T22:03:08.029537Z","shell.execute_reply.started":"2021-08-01T22:03:08.017773Z","shell.execute_reply":"2021-08-01T22:03:08.028874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskFilledAttentionHead(nn.Module):\n    def __init__(self, input_dim, head_hidden_dim):\n        super(MaskFilledAttentionHead, self).__init__()\n        head_hidden_dim = input_dim if head_hidden_dim is None else head_hidden_dim\n        self.W = nn.Linear(input_dim, head_hidden_dim)\n        self.V = nn.Linear(head_hidden_dim, 1)\n        \n    def forward(self, x, attention_mask):\n        attention_scores = self.V(torch.tanh(self.W(x)))\n        attention_scores[attention_mask==0] = -10\n        attention_scores = torch.softmax(attention_scores, dim=1)\n        attentive_x = attention_scores * x\n        attentive_x = attentive_x.sum(axis=1)\n        return attentive_x","metadata":{"_uuid":"e9e5d735-ef03-4b6b-87b8-c9d8581a4e56","_cell_guid":"1ac0a617-b8a1-4c4c-a2c2-1f1ecd732f40","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.031733Z","iopub.execute_input":"2021-08-01T22:03:08.032161Z","iopub.status.idle":"2021-08-01T22:03:08.039263Z","shell.execute_reply.started":"2021-08-01T22:03:08.032124Z","shell.execute_reply":"2021-08-01T22:03:08.038616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskAddedAttentionHead(nn.Module):\n    def __init__(self, input_dim, head_hidden_dim):\n        super(MaskAddedAttentionHead, self).__init__()\n        head_hidden_dim = input_dim if head_hidden_dim is None else head_hidden_dim\n        self.W = nn.Linear(input_dim, head_hidden_dim)\n        self.V = nn.Linear(head_hidden_dim, 1)\n        \n    def forward(self, x, attention_mask):\n        attention_scores = self.V(torch.tanh(self.W(x)))\n        attention_scores = attention_scores + attention_mask\n        attention_scores = torch.softmax(attention_scores, dim=1)\n        attentive_x = attention_scores * x\n        attentive_x = attentive_x.sum(axis=1)\n        return attentive_x","metadata":{"_uuid":"df503631-c868-4823-8e4a-95de90975712","_cell_guid":"5b1a3c62-f75e-486c-858e-75f75187ce86","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.040627Z","iopub.execute_input":"2021-08-01T22:03:08.04101Z","iopub.status.idle":"2021-08-01T22:03:08.052664Z","shell.execute_reply.started":"2021-08-01T22:03:08.040977Z","shell.execute_reply":"2021-08-01T22:03:08.051763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaPoolerOutputRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1,  roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaPoolerOutputRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        pooler_output = roberta_outputs['pooler_output']\n        pooler_output = self.dropout(pooler_output)\n        logits = self.regressor(pooler_output)\n        return logits","metadata":{"_uuid":"28736d0b-ed7f-429e-9f50-c3cfe17c6880","_cell_guid":"5a640412-fd7d-44bb-891b-d4161b81ec29","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.05439Z","iopub.execute_input":"2021-08-01T22:03:08.054886Z","iopub.status.idle":"2021-08-01T22:03:08.064287Z","shell.execute_reply.started":"2021-08-01T22:03:08.054846Z","shell.execute_reply":"2021-08-01T22:03:08.063238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaLastHiddenStateRegressor(nn.Module):\n    def __init__(self, model_path, head_hidden_dim=None, dropout_prob=0.1, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaLastHiddenStateRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = AttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state)\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","metadata":{"_uuid":"15a58f42-bc79-4cd1-86c5-b67d1b8a0978","_cell_guid":"d785eae2-0b97-4324-9044-3a231dd5170c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.065874Z","iopub.execute_input":"2021-08-01T22:03:08.066277Z","iopub.status.idle":"2021-08-01T22:03:08.077994Z","shell.execute_reply.started":"2021-08-01T22:03:08.066238Z","shell.execute_reply":"2021-08-01T22:03:08.077019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaMaskedLastHiddenStateRegressor(nn.Module):\n    def __init__(self, model_path, head_hidden_dim=None, dropout_prob=0.1, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaMaskedLastHiddenStateRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = AttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        masked_last_hidden_state = last_hidden_state * torch.unsqueeze(inputs['attention_mask'], dim=2)\n        attentive_vector = self.head(masked_last_hidden_state)\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","metadata":{"_uuid":"f67dfc91-5444-4251-94d7-20d20870af84","_cell_guid":"3e8c0214-51d4-49b0-b1d0-e6f43ef10f50","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.079685Z","iopub.execute_input":"2021-08-01T22:03:08.08019Z","iopub.status.idle":"2021-08-01T22:03:08.092344Z","shell.execute_reply.started":"2021-08-01T22:03:08.080151Z","shell.execute_reply":"2021-08-01T22:03:08.09146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaMaskFilledAttentionHeadRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaMaskFilledAttentionHeadRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = MaskFilledAttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state, torch.unsqueeze(inputs['attention_mask'], dim=2))\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","metadata":{"_uuid":"84c1fc77-2ad4-47ab-a07f-d8b1d6bad5b2","_cell_guid":"a0998b81-5bdf-46fe-8535-3584005f777e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.093771Z","iopub.execute_input":"2021-08-01T22:03:08.094274Z","iopub.status.idle":"2021-08-01T22:03:08.105613Z","shell.execute_reply.started":"2021-08-01T22:03:08.094244Z","shell.execute_reply":"2021-08-01T22:03:08.104586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaMaskAddedAttentionHeadRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaMaskAddedAttentionHeadRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = MaskAddedAttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state, torch.unsqueeze(inputs['attention_mask'], dim=2))\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","metadata":{"_uuid":"963381dc-58dd-4f8c-bda0-251c40940321","_cell_guid":"df7bd60a-a70d-4119-935c-dfbbd0c42a6b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.107114Z","iopub.execute_input":"2021-08-01T22:03:08.107569Z","iopub.status.idle":"2021-08-01T22:03:08.119147Z","shell.execute_reply.started":"2021-08-01T22:03:08.10753Z","shell.execute_reply":"2021-08-01T22:03:08.11819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaBigLinearMaskAddedAttentionHeadRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaBigLinearMaskAddedAttentionHeadRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = MaskAddedAttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Sequential(\n            nn.Linear(self.roberta.config.hidden_size, self.roberta.config.hidden_size//2),\n            nn.ReLU(),\n            nn.Linear(self.roberta.config.hidden_size//2, 1), \n        )\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state, torch.unsqueeze(inputs['attention_mask'], dim=2))\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","metadata":{"_uuid":"4d789a25-f033-40eb-98dc-acb40c829eb5","_cell_guid":"81a2290c-b540-46fa-8c0c-6f7ec7690f1d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.120643Z","iopub.execute_input":"2021-08-01T22:03:08.121133Z","iopub.status.idle":"2021-08-01T22:03:08.131122Z","shell.execute_reply.started":"2021-08-01T22:03:08.121095Z","shell.execute_reply":"2021-08-01T22:03:08.130331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaNHiddenStateRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaNHiddenStateRegressor, self).__init__()\n        self.num_last_hidden_states = kwargs.pop('num_last_hidden_states')\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = MaskAddedAttentionHead(input_dim=self.roberta.config.hidden_size * self.num_last_hidden_states, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size *  self.num_last_hidden_states, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs, output_hidden_states=True)\n        last_hidden_states = torch.cat((roberta_outputs['hidden_states'][- self.num_last_hidden_states:]), axis=2)\n        attentive_vector = self.head(last_hidden_states, torch.unsqueeze(inputs['attention_mask'], dim=2))\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","metadata":{"_uuid":"b9cd911c-0ed4-4059-b0e3-6de22a1524bd","_cell_guid":"fc817b00-85a1-46d3-98de-bc139aeb7f25","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.131916Z","iopub.execute_input":"2021-08-01T22:03:08.132202Z","iopub.status.idle":"2021-08-01T22:03:08.144646Z","shell.execute_reply.started":"2021-08-01T22:03:08.132176Z","shell.execute_reply":"2021-08-01T22:03:08.143829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaLastHiddenStateMeanPooler(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None,  roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaLastHiddenStateMeanPooler, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        masked_last_hidden_state = last_hidden_state * torch.unsqueeze(inputs['attention_mask'], dim=2)\n        num_tokens = torch.unsqueeze(inputs['attention_mask'], dim=2)\n        num_tokens = torch.clamp(num_tokens, min=1e-9)\n        mean_embeddings = masked_last_hidden_state.sum(axis=1) / num_tokens.sum(axis=1)\n        mean_embeddings = self.dropout(mean_embeddings)\n        logits = self.regressor(mean_embeddings)\n        return logits","metadata":{"_uuid":"90e64947-ecef-4f6b-8493-75fe56961888","_cell_guid":"ccbbc415-e8c0-41fc-bfd8-f86e6d872e08","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.145643Z","iopub.execute_input":"2021-08-01T22:03:08.1459Z","iopub.status.idle":"2021-08-01T22:03:08.156761Z","shell.execute_reply.started":"2021-08-01T22:03:08.145876Z","shell.execute_reply":"2021-08-01T22:03:08.155875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SquiveldMeanPooling(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None,  roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super().__init__()\n        config = AutoConfig.from_pretrained(model_path)\n        self.roberta = AutoModel.from_pretrained(model_path, config=config)\n        self.regressor = nn.Linear(1024, 1)\n        \n    def forward(self, inputs):\n        outputs = self.roberta(**inputs)\n        last_hidden_state = outputs[0]\n        attention_mask = inputs['attention_mask']\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        logits = self.regressor(mean_embeddings)\n        return logits","metadata":{"_uuid":"1f76beca-55d5-4d05-a00f-c06d70955a3f","_cell_guid":"1e72c4dd-b18a-4988-ac09-cf461ad654f7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.160324Z","iopub.execute_input":"2021-08-01T22:03:08.160806Z","iopub.status.idle":"2021-08-01T22:03:08.170381Z","shell.execute_reply.started":"2021-08-01T22:03:08.160774Z","shell.execute_reply":"2021-08-01T22:03:08.169413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertLastHiddenStateRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, **kwargs):\n        super(BertLastHiddenStateRegressor, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_path, **kwargs)\n        self.head = AttentionHead(self.bert.config.hidden_size)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        bert_outputs = self.bert(**inputs)\n        last_hidden_state = bert_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state)\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","metadata":{"_uuid":"9e65e7e6-3c0a-4941-82d1-e5d57d31f38a","_cell_guid":"23b3643e-ca72-4e60-8d0d-59d3e0529030","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.171674Z","iopub.execute_input":"2021-08-01T22:03:08.172082Z","iopub.status.idle":"2021-08-01T22:03:08.183357Z","shell.execute_reply.started":"2021-08-01T22:03:08.172017Z","shell.execute_reply":"2021-08-01T22:03:08.182499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiModel(nn.Module):\n    def __init__(self, model_1_path, model_1_class, model_2_path, model_2_class):\n        super(MultiModel, self).__init__()\n        self.examiner_1 = model_1_class(model_path=model_1_path)\n        self.examiner_2 = model_2_class(model_path=model_2_path)\n        self.bradley_terry = nn.Linear(2, 1)\n    \n    def forward(self, inputs_1, inputs_2):\n        logits_1 = self.examiner_1(inputs_1)\n        logits_2 = self.examiner_2(inputs_2)\n        concatenated_outputs = torch.hstack((logits_1, logits_2))\n        bradley_terry_score = self.bradley_terry(concatenated_outputs)\n        return bradley_terry_score","metadata":{"_uuid":"1d483d81-cc33-4c11-903e-9e360f6ab118","_cell_guid":"b8c078da-a372-49c5-b7d2-0f49a452ab74","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.18491Z","iopub.execute_input":"2021-08-01T22:03:08.185341Z","iopub.status.idle":"2021-08-01T22:03:08.193872Z","shell.execute_reply.started":"2021-08-01T22:03:08.185303Z","shell.execute_reply":"2021-08-01T22:03:08.193114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute predictions","metadata":{"_uuid":"3dec2280-9269-4e30-a4d9-c1e0bed76f53","_cell_guid":"01290c43-38bc-4512-9cd7-67e8cbe749d2","trusted":true}},{"cell_type":"code","source":"def compute_predictions(text_excerpts, tokenizer, model, max_length, device, **kwargs):\n    if max_length is None:\n        # Sequence bucketing\n        inputs = tokenizer(text=text_excerpts, padding=True, truncation=True, return_tensors='pt')\n    else:\n        inputs = tokenizer(text=text_excerpts, padding='max_length', truncation=True, max_length=max_length,  return_tensors='pt')\n    inputs = {key:value.to(device) for key, value in inputs.items()}\n    predictions = model(inputs)\n    return predictions","metadata":{"_uuid":"03f62e29-0e11-43c1-8e79-31c13898821f","_cell_guid":"b5d1af76-edc9-4d27-8cd6-35b05c168280","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.195221Z","iopub.execute_input":"2021-08-01T22:03:08.195624Z","iopub.status.idle":"2021-08-01T22:03:08.205553Z","shell.execute_reply.started":"2021-08-01T22:03:08.195593Z","shell.execute_reply":"2021-08-01T22:03:08.204864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss functions","metadata":{"_uuid":"ffa33971-ca89-4d19-a31f-086f5e952064","_cell_guid":"b67605c0-0a8a-4fec-a829-d1989735b857","trusted":true}},{"cell_type":"code","source":"mse_loss_fn = nn.MSELoss()\n\ndef compute_mse_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    loss = mse_loss_fn(predictions, targets)\n    return loss\n\ndef compute_rmse_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    loss = torch.sqrt(mse_loss_fn(predictions, targets))\n    return loss\n\ndef compute_weighted_rmse_loss(outputs, targets, device, eps=1e-6, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    if 'weights' in kwargs.keys():\n        weights = kwargs['weights']\n    else:\n        weights = torch.ones_like(targets)\n    loss = torch.sqrt(torch.mean(weights * (predictions - targets) ** 2) + eps)\n    return loss","metadata":{"_uuid":"0971d58f-61b6-4cf8-8008-aa08879b489c","_cell_guid":"29645702-6a13-4e11-b603-30c4b35d7cde","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.206442Z","iopub.execute_input":"2021-08-01T22:03:08.20673Z","iopub.status.idle":"2021-08-01T22:03:08.214841Z","shell.execute_reply.started":"2021-08-01T22:03:08.206706Z","shell.execute_reply":"2021-08-01T22:03:08.214184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranking_loss_fn = nn.MarginRankingLoss()\n\ndef compute_ranking_loss(outputs, targets, device, **kwargs):\n    predictions_left = outputs['predictions_left']\n    predictions_right = outputs['predictions_right']\n    targets = targets.to(device)\n    \n    predictions_left = predictions_left.reshape(len(predictions_left))\n    predictions_right = predictions_right.reshape(len(predictions_right))\n    targets = targets.reshape(len(targets))\n    \n    loss = ranking_loss_fn(predictions_left, predictions_right, targets)\n    return loss\n\ndef compute_hard_ranking_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    \n    predictions = predictions.reshape(len(predictions))\n    targets = targets.reshape(len(targets))\n    \n    # Sort targets based on difficulty\n    sorted_targets, indices = torch.sort(targets)\n    shifted_sorted_targets = torch.roll(sorted_targets, -1, 0)\n    \n    # Sort the corresponding predictions\n    sorted_predictions = predictions[indices]\n    shifted_sorted_predictions = torch.roll(sorted_predictions, -1, 0)\n    \n    targets = torch.sign(sorted_targets - shifted_sorted_targets)\n    outputs = {'predictions_left': sorted_predictions, 'predictions_right': shifted_sorted_predictions}\n    loss = compute_ranking_loss(outputs, targets, device)\n    return loss\n\ndef compute_random_pair_ranking_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    \n    predictions = predictions.reshape(len(predictions))\n    targets = targets.reshape(len(targets))\n    \n    # Sort targets and predictions randomly\n    indices = torch.randperm(len(targets))\n    sorted_targets = targets[indices]\n    sorted_predictions = predictions[indices]\n    \n    ranking_targets = torch.sign(targets - sorted_targets)\n    outputs = {'predictions_left': predictions, 'predictions_right': sorted_predictions}   \n    loss = compute_ranking_loss(outputs, ranking_targets, device)\n    return loss\n\n\ndef compute_pairwise_ranking_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    \n    predictions = predictions.reshape(len(predictions))\n    targets = targets.reshape(len(targets))\n    \n    predictions_combinations = torch.combinations(predictions, r=2)\n    targets_combinations = torch.combinations(targets, r=2) \n       \n    predictions_left  = predictions_combinations[:, 0]\n    predictions_right = predictions_combinations[:, 1]\n    \n    targets_left  =  targets_combinations[:, 0]\n    targets_right = targets_combinations[:, 1]\n\n    targets = torch.sign(targets_left - targets_right)\n    outputs = {'predictions_left': predictions_left, 'predictions_right': predictions_right}   \n    loss = compute_ranking_loss(outputs, targets, device)\n    return loss","metadata":{"_uuid":"e05b7cd0-61fe-44cc-b0a4-ac65d2bdbd7b","_cell_guid":"7204e4c8-adaf-4765-aeec-0b77486784ab","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.218591Z","iopub.execute_input":"2021-08-01T22:03:08.218833Z","iopub.status.idle":"2021-08-01T22:03:08.23528Z","shell.execute_reply.started":"2021-08-01T22:03:08.21881Z","shell.execute_reply":"2021-08-01T22:03:08.234355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# New loss functions","metadata":{"_uuid":"8ead17de-f824-48f4-86a9-13bdf8a54aca","_cell_guid":"12b9631e-e938-4a64-94b3-8b711fc5e261","trusted":true}},{"cell_type":"code","source":"from torch import nn\n\nclass LogCoshLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_t, y_prime_t):\n        ey_t = y_t - y_prime_t\n        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n\n\nclass XTanhLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_t, y_prime_t):\n        ey_t = y_t - y_prime_t\n        return torch.mean(ey_t * torch.tanh(ey_t))\n\n\nclass XSigmoidLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_t, y_prime_t):\n        ey_t = y_t - y_prime_t\n        return torch.mean(2 * ey_t / (1 + torch.exp(-ey_t)) - ey_t)","metadata":{"_uuid":"aaa50cd1-f097-44ab-a834-705d2178677e","_cell_guid":"7a2a50d9-bdf3-4f9f-b4fc-61f7f6431e45","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.236998Z","iopub.execute_input":"2021-08-01T22:03:08.237311Z","iopub.status.idle":"2021-08-01T22:03:08.248904Z","shell.execute_reply.started":"2021-08-01T22:03:08.237285Z","shell.execute_reply":"2021-08-01T22:03:08.248167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_cosh_loss = LogCoshLoss()\n\ndef compute_log_cosh_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    loss = log_cosh_loss(predictions, targets)\n    return loss","metadata":{"_uuid":"99788b63-bfc0-4945-bf5b-4101f09ad212","_cell_guid":"afc559b1-44b2-4b3f-a96f-c4edabe268c0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.250175Z","iopub.execute_input":"2021-08-01T22:03:08.250532Z","iopub.status.idle":"2021-08-01T22:03:08.257895Z","shell.execute_reply.started":"2021-08-01T22:03:08.250504Z","shell.execute_reply":"2021-08-01T22:03:08.257072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtanh_loss = XTanhLoss()\n\ndef compute_xtanh_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    loss = xtanh_loss(predictions, targets)\n    return loss","metadata":{"_uuid":"1a3e9651-fca7-40bb-b410-7f6e3e88dac6","_cell_guid":"538a1808-39b7-46d1-a504-822b02eed73e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.258978Z","iopub.execute_input":"2021-08-01T22:03:08.259536Z","iopub.status.idle":"2021-08-01T22:03:08.266334Z","shell.execute_reply.started":"2021-08-01T22:03:08.259506Z","shell.execute_reply":"2021-08-01T22:03:08.265709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xsigmoid_loss = XSigmoidLoss()\n\ndef compute_xsigmoid_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    loss = xsigmoid_loss(predictions, targets)\n    return loss","metadata":{"_uuid":"f2b7c193-6e92-4a18-9ea8-578f49fd2f37","_cell_guid":"6bc536c9-0515-482c-87fa-18ce51869f06","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.267431Z","iopub.execute_input":"2021-08-01T22:03:08.267856Z","iopub.status.idle":"2021-08-01T22:03:08.275808Z","shell.execute_reply.started":"2021-08-01T22:03:08.267827Z","shell.execute_reply":"2021-08-01T22:03:08.275157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stackers","metadata":{"_uuid":"098ab241-ecec-40e7-980f-3536f77651c7","_cell_guid":"409cfac7-85a7-40b3-a49e-9e7855517c36","trusted":true}},{"cell_type":"code","source":"class UnoStacker:\n    def __init__(self):\n        self.predictions = []\n        self.targets = []\n        \n    def update(self, batch_outputs, batch_targets):\n        self.predictions.append(batch_outputs['predictions'])\n        self.targets.append(batch_targets)\n    \n    def get_stack(self):\n        predictions = torch.vstack(self.predictions)\n        targets = torch.vstack(self.targets)\n        predictions = {'predictions': predictions}\n        return predictions, targets","metadata":{"_uuid":"5d436494-1ab5-43f4-8d3c-90363a9afe69","_cell_guid":"3e770725-2bf9-4580-9cfe-6d3c4be263d1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.276888Z","iopub.execute_input":"2021-08-01T22:03:08.277332Z","iopub.status.idle":"2021-08-01T22:03:08.287114Z","shell.execute_reply.started":"2021-08-01T22:03:08.277302Z","shell.execute_reply":"2021-08-01T22:03:08.286496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DuoStacker:\n    def __init__(self):\n        self.predictions_left = []\n        self.predictions_right = []\n        self.targets = []\n        \n    def update(self, batch_outputs, batch_targets):\n        self.predictions_left.append(batch_outputs['predictions_left'])\n        self.predictions_right.append(batch_outputs['predictions_right'])\n        self.targets.append(batch_targets)\n    \n    def get_stack(self):\n        predictions_left = torch.vstack(self.predictions_left)\n        predictions_right = torch.vstack(self.predictions_right)\n        targets = torch.vstack(self.targets)\n        predictions = {'predictions_left': predictions_left,\n                       'predictions_right': predictions_right}\n        return predictions, targets","metadata":{"_uuid":"b09437e2-a364-4950-b8f9-af13fb7e8a08","_cell_guid":"848c3de4-00fd-447e-93da-ee411b844581","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.288143Z","iopub.execute_input":"2021-08-01T22:03:08.288723Z","iopub.status.idle":"2021-08-01T22:03:08.296518Z","shell.execute_reply.started":"2021-08-01T22:03:08.288678Z","shell.execute_reply":"2021-08-01T22:03:08.295801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{"_uuid":"ba79e6ca-baac-4a1f-93d9-8c84336a8158","_cell_guid":"3fbc39b5-3855-42d2-8a6b-7da845da860f","trusted":true}},{"cell_type":"code","source":"def compute_rmse_score(outputs, targets, **kwargs):\n    predictions = outputs['predictions']\n    predictions = predictions.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n    rmse_score = mean_squared_error(targets, predictions, squared=False)\n    return rmse_score","metadata":{"_uuid":"efa825ec-0c8f-4a8a-b03b-57fdca8f40c1","_cell_guid":"b7ef5003-c471-4fee-ac73-4e971908636b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.297511Z","iopub.execute_input":"2021-08-01T22:03:08.297864Z","iopub.status.idle":"2021-08-01T22:03:08.306607Z","shell.execute_reply.started":"2021-08-01T22:03:08.297818Z","shell.execute_reply":"2021-08-01T22:03:08.305701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_rmse_score_from_ranker_predictions(outputs, targets, **kwargs):\n    predictions = outputs['predictions']\n    predictions = predictions.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n        \n    regressor = Ridge(fit_intercept=True, normalize=False)    \n    predictions = predictions.reshape(len(predictions), 1)\n    targets = targets.reshape(len(targets))\n        \n    scores = cross_val_score(regressor, predictions, targets, cv=kwargs['cv'], scoring='neg_root_mean_squared_error')\n    rmse_score = np.abs(np.mean(scores))\n    return rmse_score","metadata":{"_uuid":"ada30fa3-f54e-48cb-afd8-8d0ac01904ed","_cell_guid":"156d849e-49f6-410b-b6aa-ab92d2bc6cee","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.307746Z","iopub.execute_input":"2021-08-01T22:03:08.308036Z","iopub.status.idle":"2021-08-01T22:03:08.316843Z","shell.execute_reply.started":"2021-08-01T22:03:08.308013Z","shell.execute_reply":"2021-08-01T22:03:08.316088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_correlation_coefficient(outputs, targets, **kwargs):\n    predictions = outputs['predictions']\n    predictions = predictions.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n    predictions = predictions.reshape(len(predictions), )\n    targets = targets.reshape(len(targets), )\n    correlation_coefficient, _ = stats.pearsonr(predictions, targets)\n    return correlation_coefficient","metadata":{"_uuid":"3c0126e5-d37d-4c90-8f72-24f0ef304649","_cell_guid":"6d51d77b-debc-436a-8105-dc9c29fe4545","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.317953Z","iopub.execute_input":"2021-08-01T22:03:08.318327Z","iopub.status.idle":"2021-08-01T22:03:08.329042Z","shell.execute_reply.started":"2021-08-01T22:03:08.3183Z","shell.execute_reply":"2021-08-01T22:03:08.328245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_rmse_score(outputs, targets, **kwargs):\n    predictions = outputs['predictions']\n    predictions = predictions.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n    rmse_score = mean_squared_error(targets, predictions, squared=False)\n    return rmse_score","metadata":{"_uuid":"361db79a-6b3b-4a47-87bb-88d6128ca7ca","_cell_guid":"95d7954e-abc3-4577-854c-7a35c7380a0b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.330303Z","iopub.execute_input":"2021-08-01T22:03:08.33068Z","iopub.status.idle":"2021-08-01T22:03:08.337713Z","shell.execute_reply.started":"2021-08-01T22:03:08.330652Z","shell.execute_reply":"2021-08-01T22:03:08.336905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scale_and_fit_regressor(outputs, targets, **kwargs):\n    predictions = outputs['predictions']\n    predictions = predictions.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n    \n    targets = targets.reshape(len(targets), 1)\n    predictions = predictions.reshape(len(predictions), 1)\n    \n    scaler = kwargs['scaler']\n    \n    targets = scaler.inverse_transform(targets)\n    predictions = scaler.inverse_transform(predictions)\n    targets = targets.reshape(len(targets), )\n    \n    regressor = make_pipeline(StandardScaler(), Ridge(fit_intercept=True, normalize=False))    \n    scores = cross_val_score(regressor, predictions, targets, cv=kwargs['cv'], scoring='neg_root_mean_squared_error')\n    rmse_score = np.abs(np.mean(scores))\n    return rmse_score","metadata":{"_uuid":"2ff5d36e-00a7-4832-a534-0eec935bb1a1","_cell_guid":"4a508bbd-fd8e-470f-be5a-505a159c6032","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.338811Z","iopub.execute_input":"2021-08-01T22:03:08.339173Z","iopub.status.idle":"2021-08-01T22:03:08.350352Z","shell.execute_reply.started":"2021-08-01T22:03:08.339147Z","shell.execute_reply":"2021-08-01T22:03:08.349551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_ranking_accuracy(outputs, targets, **kwargs):\n    predictions_left = outputs['predictions_left']\n    predictions_right = outputs['predictions_right']\n    \n    predictions_left = predictions_left.reshape(len(predictions_left))\n    predictions_right = predictions_right.reshape(len(predictions_right))\n    targets = targets.reshape(len(targets))\n\n    predictions_left = predictions_left.detach().cpu().numpy()\n    predictions_right = predictions_right.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n\n    predictions = np.sign(predictions_left - predictions_right)\n    ranking_accuracy = accuracy_score(targets, predictions)\n    return ranking_accuracy","metadata":{"_uuid":"2b7c8cdf-5fae-4a5b-b0e3-9ef921e41435","_cell_guid":"07e935f6-bd25-4c99-9c2c-3639081dc11c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.351291Z","iopub.execute_input":"2021-08-01T22:03:08.351672Z","iopub.status.idle":"2021-08-01T22:03:08.362586Z","shell.execute_reply.started":"2021-08-01T22:03:08.351645Z","shell.execute_reply":"2021-08-01T22:03:08.361667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Forward pass functions","metadata":{"_uuid":"10506339-69fe-4453-a9d9-3792538174ee","_cell_guid":"34429f90-1f98-4ed2-bfd8-5d4ae712ac4f","trusted":true}},{"cell_type":"code","source":"def forward_pass_uno_text_batch(batch, tokenizer, model, compute_loss_fn, max_length, device, **kwargs):\n    predictions = compute_predictions(text_excerpts=batch['text_excerpt'], tokenizer=tokenizer, model=model, max_length=max_length, device=device, **kwargs)\n    outputs = {'predictions': predictions}\n    loss = compute_loss_fn(outputs=outputs, targets=batch['target'], device=device, **kwargs) if compute_loss_fn is not None else None\n    outputs['loss'] = loss\n    return outputs\n\n\ndef forward_pass_duo_text_batch(batch, tokenizer, model, compute_loss_fn, max_length, device, **kwargs):\n    predictions_left = compute_predictions(text_excerpts=batch['text_excerpt_left'], tokenizer=tokenizer, model=model, max_length=max_length, device=device, **kwargs)\n    predictions_right = compute_predictions(text_excerpts=batch['text_excerpt_right'], tokenizer=tokenizer, model=model, max_length=max_length, device=device, **kwargs)\n    outputs = {'predictions_left': predictions_left,  'predictions_right': predictions_right}\n    loss = compute_loss_fn(outputs=outputs, targets=batch['target'], device=device, **kwargs) if compute_loss_fn is not None else None\n    outputs['loss'] = loss\n    return outputs","metadata":{"_uuid":"b60147bb-59f9-4c5e-8d62-112f43c37f1b","_cell_guid":"562f4cab-b6d7-46dd-960b-b23c944178f6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.363478Z","iopub.execute_input":"2021-08-01T22:03:08.363842Z","iopub.status.idle":"2021-08-01T22:03:08.373092Z","shell.execute_reply.started":"2021-08-01T22:03:08.363809Z","shell.execute_reply":"2021-08-01T22:03:08.372181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scheduler\n\n``` python\ndef lr_scheduler(lrs, optimizer, lr, train_dataloader):\n    if lrs == 'cyclic':\n        scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=4.5e-5, step_size_up=len(train_dataloader)/2, cycle_momentum=False)\n    \n    elif lrs == 'cosine_schedule_with_warmup':\n        scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                                    num_training_steps=config['epochs'] * len(train_dataloader),\n                                                    num_warmup_steps=0.2*len(train_dataloader))\n    else:\n        scheduler = None\n    return scheduler\n```","metadata":{"_uuid":"a0704c99-edb1-4c30-96bf-db373d27f460","_cell_guid":"27d743a9-a092-4feb-8659-40cbd14fc23e","trusted":true}},{"cell_type":"code","source":"def make_cyclic_scheduler(optimizer, **kwargs):\n    base_lr = kwargs['base_lr']\n    step_size_up = kwargs['step_size_up']\n    max_lr = kwargs['max_lr']\n    cycle_momentum = kwargs['cycle_momentum']\n    scheduler = CyclicLR(optimizer=optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=step_size_up, cycle_momentum=cycle_momentum)\n    return scheduler\n\ndef make_cosine_schedule_with_warmup(optimizer, **kwargs):\n    num_warmup_steps = kwargs['num_warmup_steps']\n    num_training_steps = kwargs['num_training_steps']\n    scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,\n                                                num_warmup_steps=num_warmup_steps,\n                                                num_training_steps=num_training_steps)\n    return scheduler\n\ndef make_linear_schedule_with_warmup(optimizer, **kwargs):\n    num_warmup_steps = kwargs['num_warmup_steps']\n    num_training_steps = kwargs['num_training_steps']\n    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,\n                                                num_warmup_steps=num_warmup_steps,\n                                                num_training_steps=num_training_steps)\n    return scheduler\n    \ndef make_cosine_with_hard_restarts_schedule_with_warmup(optimizer, **kwargs):\n    num_warmup_steps = kwargs['num_warmup_steps']\n    num_training_steps = kwargs['num_training_steps']\n    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer=optimizer, \n                                                                   num_warmup_steps=num_warmup_steps,\n                                                                   num_training_steps=num_training_steps,\n                                                                   num_cycles=3)\n    return scheduler\n\ndef make_polynomial_decay_schedule_with_warmup(optimizer, **kwargs):\n    num_warmup_steps = kwargs['num_warmup_steps']\n    num_training_steps = kwargs['num_training_steps']\n    scheduler = get_polynomial_decay_schedule_with_warmup(optimizer=optimizer, \n                                                          num_warmup_steps=num_warmup_steps,\n                                                          num_training_steps=num_training_steps,\n                                                          power=3.0)\n    return scheduler\n    \ndef get_scheduler(scheduler_type, optimizer, **kwargs):\n    if scheduler_type == 'cyclic':\n        scheduler = make_cyclic_scheduler(optimizer, **kwargs)\n    elif scheduler_type == 'cosine_schedule_with_warmup':\n        scheduler = make_cosine_schedule_with_warmup(optimizer, **kwargs)\n    elif scheduler_type == 'get_linear_schedule_with_warmup':\n        scheduler = make_linear_schedule_with_warmup(optimizer, **kwargs)\n    elif scheduler_type == 'cosine_with_hard_restarts_schedule_with_warmup':\n        scheduler = make_cosine_with_hard_restarts_schedule_with_warmup(optimizer, **kwargs)\n    elif scheduler_type == 'polynomial_decay_schedule_with_warmup':\n        scheduler = make_polynomial_decay_schedule_with_warmup(optimizer, **kwargs)\n    else:\n        scheduler = None\n    return scheduler","metadata":{"_uuid":"23df39ef-b5cc-48c9-9060-ac874466da4b","_cell_guid":"933bbe64-c2cb-4dce-8d46-8bd6ee6fb1fa","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.37408Z","iopub.execute_input":"2021-08-01T22:03:08.374401Z","iopub.status.idle":"2021-08-01T22:03:08.386691Z","shell.execute_reply.started":"2021-08-01T22:03:08.374372Z","shell.execute_reply":"2021-08-01T22:03:08.385975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer","metadata":{"_uuid":"083a9cfc-400a-4851-b3dc-226b7b609fd5","_cell_guid":"0638aa85-c251-4f6e-9f37-6b71d41c0588","trusted":true}},{"cell_type":"code","source":"def split_into_wd_groups(param_group, weight_decay):\n    # Applies weight decay\n    weight_parameters = {'params': [param_group['params'][index] for index, name in enumerate(param_group['param_names']) if 'weight' in name and 'LayerNorm' not in name],\n                         'param_names': [param_group['param_names'][index] for index, name in enumerate(param_group['param_names']) if 'weight' in name and 'LayerNorm' not in name],\n                         'lr': param_group['lr'],\n                         'weight_decay': weight_decay,\n                         'name': param_group['name']+'_weight'}\n    # Does not apply weight decay\n    bias_ln_parameters = {'params': [param_group['params'][index] for index, name in enumerate(param_group['param_names']) if 'bias' in name or 'LayerNorm' in name],\n                          'param_names': [param_group['param_names'][index] for index, name in enumerate(param_group['param_names']) if 'bias' in name or 'LayerNorm' in name],\n                          'lr': param_group['lr'],\n                          'weight_decay': 0.0,\n                          'name': param_group['name']+'_bias_ln'}\n    parameters = [weight_parameters, bias_ln_parameters]\n    return parameters","metadata":{"_uuid":"c9647dc0-9180-4988-bc17-c6c9973415d7","_cell_guid":"dd5bfaa4-784b-4295-a636-f9661794db5d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.38772Z","iopub.execute_input":"2021-08-01T22:03:08.388176Z","iopub.status.idle":"2021-08-01T22:03:08.399707Z","shell.execute_reply.started":"2021-08-01T22:03:08.388138Z","shell.execute_reply":"2021-08-01T22:03:08.399078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_optimizer_parameters(group_mode, lr, model, **kwargs):\n    \n    param_optimizer = list(model.named_parameters())\n    non_bert = [(n,p) for n,p in model.named_parameters() if 'roberta' not in n]\n    no_decay = ['bias', 'gamma', 'beta']\n\n    if group_mode == 'i':\n        optimizer_parameters = [{'params': [p for n,p in model.named_parameters() if ('bias' not in n) and ('pooler' not in n)],\n                                 'lr': lr, 'weight_decay':0.01, 'name': 'weights'}, \n                                {'params': [p for n,p in model.named_parameters() if ('bias' in n) and ('pooler' not in n)],\n                                 'lr': lr, 'weight_decay':0.00, 'name': 'bias'}]\n        \n    elif group_mode == 'j':\n        optimizer_parameters = [{'params': [p for n,p in model.named_parameters() if ('bias' not in n)],\n                                 'lr': lr, 'weight_decay':0.01, 'name': 'weights'}, \n                                {'params': [p for n,p in model.named_parameters() if ('bias' in n)],\n                                 'lr': lr, 'weight_decay':0.00, 'name': 'bias'}]\n\n    elif group_mode == 'k':\n        # Finetuning task specific layers\n        optimizer_parameters = [{'params': [p for n,p in non_bert if 'bias' not in n],\n                                 'param_names': [n for n,p in non_bert if 'bias' not in n],\n                                 'lr': lr, 'weight_decay':0.01, 'name': 'non_roberta_weights'}, \n                                {'params': [p for n,p in non_bert if 'bias' in n],\n                                 'param_names': [n for n,p in non_bert if 'bias' in n],\n                                 'lr': lr, 'weight_decay':0.00, 'name': 'non_roberta_bias'}]\n        \n    elif group_mode  == 's':\n        multiplicative_factor = kwargs['multiplicative_factor']\n        optimizer_parameters = [{'params': [p for n,p in model.roberta.named_parameters() if (not any(nd in n for nd in no_decay)) and ('pooler' not in n)],\n                                 'lr': lr, 'weight_decay' : 0.01, 'name': 'roberta_weights'},\n                                {'params': [p for n,p in model.roberta.named_parameters() if (any(nd in n for nd in no_decay))  and ('pooler' not in n)],\n                                 'lr': lr,'weight_decay': 0.0, 'name': 'roberta_bias'},\n                                {'params': [p for n,p in model.named_parameters() if all(nd not in n for nd in ['roberta','bias'])],\n                                 'lr': lr * multiplicative_factor, 'weight_decay':0.01, 'name': 'non_roberta_weights'}, \n                                {'params': [p for n,p in non_bert if 'bias' in n],\n                                 'lr': lr * multiplicative_factor, 'weight_decay':0.00, 'name': 'non_roberta_bias'}]\n    elif group_mode == 'b':\n        multiplicative_factor = kwargs['multiplicative_factor']\n        model_parameters = {name: param for name, param in model.named_parameters()}\n        model_parameters_names = [name for name, param in model.named_parameters()]\n        \n        # LR for task specific layers\n        if kwargs['train_pooler']:\n            task_specific_layer_names = [name for name, param in model.named_parameters() if 'regressor' in name or 'head' in name or 'pooler' in name or 'layer_norm' in name]\n        else:\n            task_specific_layer_names = [name for name, param in model.named_parameters() if 'regressor' in name or 'head' in name or 'layer_norm' in name]\n        task_specific_layer_params = [model_parameters.get(name) for name in task_specific_layer_names]\n        task_specific_optimizer_parameters = [{'params': task_specific_layer_params,\n                                               'param_names': task_specific_layer_names,\n                                               'lr': lr,\n                                               'name': 'task_specific_layers'}]\n        # LR for roberta layers\n        # Freeze embeddings\n        roberta_layer_names = [name for name, param in model.named_parameters() if 'roberta' in name]\n        max_num_layers = model.roberta.config.num_hidden_layers\n        roberta_layers_groups = {layer_num: {'params': [],\n                                             'param_names': [],\n                                             'lr': lr * multiplicative_factor ** (max_num_layers - layer_num),\n                                             'name': f'layer_{layer_num}'} for layer_num in range(max_num_layers)}\n        for layer_num in range(max_num_layers):\n            for layer_name in roberta_layer_names:\n                if f'layer.{layer_num}.' in layer_name:\n                    roberta_layers_groups[layer_num]['param_names'].append(layer_name)\n                    roberta_layers_groups[layer_num]['params'].append(model_parameters.get(layer_name))\n        roberta_layers_optimizer_parameters = list(roberta_layers_groups.values())\n        # Combine task specific layers and roberta layers\n        optimizer_parameters = roberta_layers_optimizer_parameters + task_specific_optimizer_parameters\n\n    elif group_mode == 'b_wd':\n        multiplicative_factor = kwargs['multiplicative_factor']\n        model_parameters = {name: param for name, param in model.named_parameters()}\n        model_parameters_names = [name for name, param in model.named_parameters()]\n        \n        # LR for task specific layers\n        if kwargs['train_pooler']:\n            task_specific_layer_names = [name for name, param in model.named_parameters() if 'regressor' in name or 'head' in name or 'pooler' in name or 'layer_norm' in name]\n        else:\n            task_specific_layer_names = [name for name, param in model.named_parameters() if 'regressor' in name or 'head' in name or 'layer_norm' in name]\n        task_specific_layer_params = [model_parameters.get(name) for name in task_specific_layer_names]\n        task_specific_optimizer_parameters = [{'params': task_specific_layer_params,\n                                               'param_names': task_specific_layer_names,\n                                               'lr': lr,\n                                               'name': 'task_specific_layers'}]\n        # LR for roberta layers\n        # Freeze embeddings\n        roberta_layer_names = [name for name, param in model.named_parameters() if 'roberta' in name]\n        max_num_layers = model.roberta.config.num_hidden_layers\n        roberta_layers_groups = {layer_num: {'params': [],\n                                             'param_names': [],\n                                             'lr': lr * multiplicative_factor ** (max_num_layers - layer_num),\n                                             'name': f'layer_{layer_num}'} for layer_num in range(max_num_layers)}\n        for layer_num in range(max_num_layers):\n            for layer_name in roberta_layer_names:\n                if f'layer.{layer_num}.' in layer_name:\n                    roberta_layers_groups[layer_num]['param_names'].append(layer_name)\n                    roberta_layers_groups[layer_num]['params'].append(model_parameters.get(layer_name))\n        roberta_layers_optimizer_parameters = list(roberta_layers_groups.values())\n        # Combine task specific layers and roberta layers\n        optimizer_parameters_without_wd = roberta_layers_optimizer_parameters + task_specific_optimizer_parameters\n\n        # Assign weight decay\n        weight_decay = kwargs['weight_decay']\n        optimizer_parameters = []\n        for layer_parameters in optimizer_parameters_without_wd:\n            weight_parameters = {'params': [], 'param_names': [], 'lr': layer_parameters['lr'], 'name': layer_parameters['name']+'_weights', 'weight_decay': weight_decay}\n            bias_parameters = {'params': [], 'param_names': [], 'lr': layer_parameters['lr'], 'name': layer_parameters['name']+'_bias', 'weight_decay': 0.0}\n            layer_norm_parameters = {'params': [], 'param_names': [], 'lr': layer_parameters['lr'], 'name': layer_parameters['name']+'_layer_norm', 'weight_decay': 0.0}\n            for param, param_name in zip(layer_parameters['params'], layer_parameters['param_names']):\n                if 'LayerNorm' in param_name:\n                    layer_norm_parameters['params'].append(param)\n                    layer_norm_parameters['param_names'].append(param_name)\n                elif 'bias' in param_name:\n                    bias_parameters['params'].append(param)\n                    bias_parameters['param_names'].append(param_name)\n                else:\n                    weight_parameters['params'].append(param)\n                    weight_parameters['param_names'].append(param_name)\n            optimizer_parameters.append(weight_parameters)\n            optimizer_parameters.append(bias_parameters)\n            optimizer_parameters.append(layer_norm_parameters)\n            \n    elif group_mode == 'be_wd':\n        multiplicative_factor = kwargs['multiplicative_factor']\n        weight_decay = kwargs['weight_decay']\n        max_num_layers = model.roberta.config.num_hidden_layers\n\n        # Task Specific Layer group\n        tsl_param_group = [{'params': [param for name, param in model.named_parameters() if 'roberta' not in name],\n                            'param_names': [name for name, param in model.named_parameters() if 'roberta' not in name],\n                            'lr': lr,\n                            'name': 'tsl'}]\n\n        # Roberta Layer group\n        roberta_layers_param_groups = []\n        for layer_num in reversed(range(max_num_layers)):\n            roberta_layer_param_groups = {'params': [param for name, param in model.named_parameters() if f'roberta.encoder.layer.{layer_num}.' in name],\n                                          'param_names': [name for name, param in model.named_parameters() if f'roberta.encoder.layer.{layer_num}.' in name],\n                                          'lr': lr * (multiplicative_factor ** (max_num_layers - layer_num)),\n                                          'name': f'layer_{layer_num}'}\n            roberta_layers_param_groups.append(roberta_layer_param_groups)\n\n        # Embeddding group\n        embedding_lr = lr * (multiplicative_factor ** (max_num_layers + 1))\n        embedding_param_group = [{'params': [param for name, param in model.named_parameters() if 'embedding' in name],\n                                  'param_names': [name for name, param in model.named_parameters() if 'embedding' in name],\n                                  'lr': embedding_lr,\n                                  'name': 'embedding'}]\n\n        param_groups = tsl_param_group + roberta_layers_param_groups + embedding_param_group\n        optimizer_parameters = list(chain(*[split_into_wd_groups(param_group, weight_decay=weight_decay) for param_group in param_groups]))\n\n    elif group_mode == 'a':\n        group1 = ['layer.0.','layer.1.','layer.2.','layer.3.']\n        group2 = ['layer.4.','layer.5.','layer.6.','layer.7.']    \n        group3 = ['layer.8.','layer.9.','layer.10.','layer.11.']\n        group_all = group1 + group2 + group3\n        optimizer_parameters = [\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.01, 'lr': lr/2.6, 'name': 'roberta_group_1_weights'},\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.01, 'lr': lr, 'name': 'roberta_group_2_weights'},\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.01, 'lr': lr*2.6, 'name': 'roberta_group_3_weights'},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.0, 'lr': lr/2.6, 'name': 'roberta_group_1_bias'},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.0, 'lr': lr, 'name': 'roberta_group_2_bias'},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.0, 'lr': lr*2.6, 'name': 'roberta_group_3_bias'},\n                \n            {'params': [p for n, p in non_bert if  'bias' not in n and 'head' in n], 'lr':lr*10, \"momentum\" : 0.99,'weight_decay_rate':0.01, 'name': 'head_weights'},\n            {'params': [p for n, p in non_bert if  'bias' in n and 'head' in n], 'lr':lr*10, \"momentum\" : 0.99,'weight_decay_rate':0.0, 'name': 'head_bias'},\n\n            {'params': [p for n, p in non_bert if  'bias' not in n and 'regressor' in n], 'lr':lr*10, \"momentum\" : 0.99,'weight_decay_rate':0.01, 'name': 'regressor_weights'},\n            {'params': [p for n, p in non_bert if \"bias\"  in n and 'regressor' in n], 'lr':lr*10, \"momentum\" : 0.99,'weight_decay_rate':0.00, 'name': 'regressor_bias'},\n        ]\n    return optimizer_parameters","metadata":{"_uuid":"21ce9701-8910-48a7-bfb2-0f1f767bfaf6","_cell_guid":"10d2d915-5948-4aa3-87f9-7f3e853016a6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.400859Z","iopub.execute_input":"2021-08-01T22:03:08.401333Z","iopub.status.idle":"2021-08-01T22:03:08.454575Z","shell.execute_reply.started":"2021-08-01T22:03:08.401293Z","shell.execute_reply":"2021-08-01T22:03:08.453666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saver","metadata":{"_uuid":"51b261f1-50fd-47d6-ac66-fdcc7eb7371c","_cell_guid":"f69aaf55-2500-4460-81b3-b5b217c3d5fc","trusted":true}},{"cell_type":"code","source":"class Saver:\n    def __init__(self, metric_name, is_lower_better, config, save_name, should_save=True):\n        self.metric_name = metric_name\n        self.save_path = Path(f'{save_name}/{metric_name}')\n        self.operator_function = operator.le if is_lower_better else operator.ge\n        self.best_score = np.inf if is_lower_better else 0\n        self.best_iteration_num = 0\n        self.config = config\n        self.save_config()\n        self.should_save = should_save\n    \n    def update(self, current_iteration_num, current_score, model, tokenizer):\n        if self.operator_function(current_score, self.best_score):\n            self.best_score = current_score\n            self.best_iteration_num = current_iteration_num\n            if self.should_save:\n                self.save(model, tokenizer)\n            print(f'{self.metric_name} attained best score: {current_score:.3f}. Saving the model')\n            \n    def save_config(self):\n        shutil.rmtree(self.save_path, ignore_errors=True)\n        os.makedirs(self.save_path)\n        with open(self.save_path / 'config.json', 'w') as fp:\n            json.dump(self.config, fp, sort_keys=True, indent=4)\n    \n    def save(self, model, tokenizer):\n        shutil.rmtree(self.save_path, ignore_errors=True)\n        os.makedirs(self.save_path)\n        torch.save(model.state_dict(), self.save_path / 'model.pth')\n        tokenizer.save_pretrained(self.save_path)\n        \n    def get_best_score(self):\n        return {'best_score': self.best_score, 'best_iteration_num': self.best_iteration_num}","metadata":{"_uuid":"6d3cfab5-ca13-4bf2-a3d3-c76041aa45b8","_cell_guid":"fa63d0ef-0e4d-4606-868c-89a5e9fa21fc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.455772Z","iopub.execute_input":"2021-08-01T22:03:08.456336Z","iopub.status.idle":"2021-08-01T22:03:08.468185Z","shell.execute_reply.started":"2021-08-01T22:03:08.456297Z","shell.execute_reply":"2021-08-01T22:03:08.467264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Evaluate functions","metadata":{"_uuid":"b27bfd17-f29f-4852-92b2-fb9c4c1a5655","_cell_guid":"9bdf0a16-46e2-4797-9777-9d5ecb838fc4","trusted":true}},{"cell_type":"code","source":"def train_one_batch(iteration_num, batch, tokenizer, model, optimizer, scheduler, forward_pass_fn, compute_loss_fn, max_length, accumulation_steps, device, **kwargs):\n    model.train()\n    if iteration_num == 0:\n        optimizer.zero_grad()\n    batch_loss = forward_pass_fn(batch=batch, tokenizer=tokenizer, model=model, \n                                 compute_loss_fn=compute_loss_fn, max_length=max_length, \n                                 device=device, **kwargs)['loss']                                            # Forward pass\n    batch_loss = batch_loss / accumulation_steps                                                   # Normalize our loss (if averaged)\n    batch_loss.backward()                                                                          # Backward pass\n    if (iteration_num + 1) % accumulation_steps == 0:                                              # Wait for several backward steps\n        optimizer.step()                                                                           # Now we can do an optimizer step\n        optimizer.zero_grad()\n        if scheduler is not None:\n            scheduler.step()\n    return model, batch_loss * accumulation_steps","metadata":{"_uuid":"56710b63-482c-4527-98d0-a4a0960f9b86","_cell_guid":"c0d4d278-2534-4411-a10d-fb7a5885e697","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.46959Z","iopub.execute_input":"2021-08-01T22:03:08.469978Z","iopub.status.idle":"2021-08-01T22:03:08.481306Z","shell.execute_reply.started":"2021-08-01T22:03:08.469938Z","shell.execute_reply":"2021-08-01T22:03:08.480472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(dataloader, tokenizer, model, forward_pass_fn, compute_loss_fn, compute_metric_fn, stacker_class, max_length, device, **kwargs):\n    epoch_loss = 0\n    model.eval()\n    stacker = stacker_class()\n    with torch.no_grad():\n        for batch_num, batch in enumerate(dataloader):\n            batch_outputs = forward_pass_fn(batch=batch, tokenizer=tokenizer, model=model, \n                                            compute_loss_fn=compute_loss_fn, max_length=max_length, \n                                            device=device, **kwargs)\n            batch_loss = batch_outputs['loss']\n            epoch_loss += batch_loss.item()\n            stacker.update(batch_outputs=batch_outputs, batch_targets=batch['target'])\n    average_epoch_loss = epoch_loss/(batch_num+1)\n    outputs, targets = stacker.get_stack()\n    metric_score = compute_metric_fn(outputs=outputs, targets=targets, **kwargs)\n    return average_epoch_loss, metric_score","metadata":{"_uuid":"270d20b3-983a-4a1a-9fa5-9d786f903fdd","_cell_guid":"32a9964b-0d16-4f18-a76e-4943f0db0e7b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.482363Z","iopub.execute_input":"2021-08-01T22:03:08.482746Z","iopub.status.idle":"2021-08-01T22:03:08.494303Z","shell.execute_reply.started":"2021-08-01T22:03:08.482699Z","shell.execute_reply":"2021-08-01T22:03:08.493329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(dataloader, tokenizer, model, forward_pass_fn, stacker_class, max_length, device, **kwargs):\n    model.eval()\n    stacker = stacker_class()\n    with torch.no_grad():\n        for batch_num, batch in enumerate(dataloader):\n            batch_outputs = forward_pass_fn(batch=batch, tokenizer=tokenizer, model=model, \n                                            compute_loss_fn=None, max_length=max_length, \n                                            device=device, **kwargs)\n            stacker.update(batch_outputs=batch_outputs, batch_targets=batch['target'])\n    outputs, _ = stacker.get_stack()\n    return outputs","metadata":{"_uuid":"fdc52ba3-391b-48a5-a8f1-a5c5887611d3","_cell_guid":"f750c154-70ff-4cf6-b6f7-d31cf27e5868","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.495573Z","iopub.execute_input":"2021-08-01T22:03:08.49586Z","iopub.status.idle":"2021-08-01T22:03:08.507716Z","shell.execute_reply.started":"2021-08-01T22:03:08.495832Z","shell.execute_reply":"2021-08-01T22:03:08.506678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_evaluate(num_epochs, train_dataloader, valid_dataloader, tokenizer, model, optimizer, scheduler,\n                       forward_pass_fn_train, forward_pass_fn_valid, compute_loss_fn_train, compute_loss_fn_valid,\n                       compute_metric_fn, stacker_class, max_length, accumulation_steps, validate_every_n_iteraion, \n                       valid_loss_saver, valid_score_saver, device, **kwargs):\n    iteration_num = 0\n    for epoch_num in range(num_epochs):\n        for batch in train_dataloader:\n            for param_group in optimizer.param_groups:\n                wandb.log({param_group['name']: {\"lr\": param_group['lr']}})\n            model, iteration_train_loss = train_one_batch(iteration_num=iteration_num, batch=batch, tokenizer=tokenizer, model=model,\n                                                          optimizer=optimizer, scheduler=scheduler, forward_pass_fn=forward_pass_fn_train,\n                                                          compute_loss_fn=compute_loss_fn_train, max_length=max_length,\n                                                          accumulation_steps=accumulation_steps, device=device, **kwargs)\n            wandb.log({'Epoch_num': epoch_num, 'iteration_num': iteration_num, 'iteration_train_loss': iteration_train_loss})\n            \n            if 'validate_after_n_iteration' in kwargs:\n                validate_after_n_iteration = kwargs['validate_after_n_iteration']\n            else:\n                validate_after_n_iteration = -1\n\n            if ((iteration_num + 1) % validate_every_n_iteraion == 0) and (iteration_num > validate_after_n_iteration):\n                valid_loss, valid_score = evaluate(dataloader=valid_dataloader, tokenizer=tokenizer, model=model,\n                                                   forward_pass_fn=forward_pass_fn_valid, compute_loss_fn=compute_loss_fn_valid,\n                                                   compute_metric_fn=compute_metric_fn, stacker_class=stacker_class,\n                                                   max_length=max_length, device=device, **kwargs)\n                valid_loss_saver.update(current_iteration_num=iteration_num,current_score=valid_loss, model=model, tokenizer=tokenizer)\n                valid_score_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n                wandb.log({\"iteration_num\": iteration_num, \"valid_loss\": valid_loss, \"valid_score\": valid_score})\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, iteration_train_loss: {iteration_train_loss}')\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, valid_loss: {valid_loss}, valid_score: {valid_score}')\n                wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n                wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n            iteration_num += 1\n            \n    if 'final_model_saver' in kwargs:\n        final_model_saver = kwargs['final_model_saver']\n        valid_score = 1\n        final_model_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n    \n    wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n    wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n\n    output = {'model': model, 'best_score': valid_score_saver.get_best_score()['best_score'], 'best_loss': valid_loss_saver.get_best_score()['best_score']}\n    return output","metadata":{"_uuid":"9278f3f3-3e91-4494-acb0-a6c3b05f3031","_cell_guid":"c71b8870-5c14-4478-8a4a-8e3fcbd7b819","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.509323Z","iopub.execute_input":"2021-08-01T22:03:08.509746Z","iopub.status.idle":"2021-08-01T22:03:08.525543Z","shell.execute_reply.started":"2021-08-01T22:03:08.509703Z","shell.execute_reply":"2021-08-01T22:03:08.5248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Crazy evaluation","metadata":{"_uuid":"c25a553f-e210-487f-b065-fe324f1626bd","_cell_guid":"28b22113-f4f2-490b-9064-6e18e0cb06a6","trusted":true}},{"cell_type":"code","source":"def train_and_evaluate_crazily(num_epochs, train_dataloader, valid_dataloader, tokenizer, model, optimizer, scheduler,\n                               forward_pass_fn_train, forward_pass_fn_valid, compute_loss_fn_train, compute_loss_fn_valid,\n                               compute_metric_fn, stacker_class, max_length, accumulation_steps, validate_every_n_iteraion, \n                               valid_loss_saver, valid_score_saver, device, **kwargs):\n    iteration_num = 0\n    valid_score = np.inf\n    for epoch_num in range(num_epochs):\n        for batch in train_dataloader:\n            for param_group in optimizer.param_groups:\n                wandb.log({param_group['name']: {\"lr\": param_group['lr']}})\n            model, iteration_train_loss = train_one_batch(iteration_num=iteration_num, batch=batch, tokenizer=tokenizer, model=model,\n                                                          optimizer=optimizer, scheduler=scheduler, forward_pass_fn=forward_pass_fn_train,\n                                                          compute_loss_fn=compute_loss_fn_train, max_length=max_length,\n                                                          accumulation_steps=accumulation_steps, device=device, **kwargs)\n            wandb.log({'Epoch_num': epoch_num, 'iteration_num': iteration_num, 'iteration_train_loss': iteration_train_loss})\n            \n            if 'validate_after_n_iteration' in kwargs:\n                validate_after_n_iteration = kwargs['validate_after_n_iteration']\n            else:\n                validate_after_n_iteration = -1\n                \n            if valid_score < 0.51:\n                validate_every_n_iteraion = 2\n            else:\n                validate_every_n_iteraion = 10                \n\n            if ((iteration_num + 1) % validate_every_n_iteraion == 0) and (iteration_num > validate_after_n_iteration):\n                valid_loss, valid_score = evaluate(dataloader=valid_dataloader, tokenizer=tokenizer, model=model,\n                                                   forward_pass_fn=forward_pass_fn_valid, compute_loss_fn=compute_loss_fn_valid,\n                                                   compute_metric_fn=compute_metric_fn, stacker_class=stacker_class,\n                                                   max_length=max_length, device=device, **kwargs)\n                valid_loss_saver.update(current_iteration_num=iteration_num,current_score=valid_loss, model=model, tokenizer=tokenizer)\n                valid_score_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n                wandb.log({\"iteration_num\": iteration_num, \"valid_loss\": valid_loss, \"valid_score\": valid_score})\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, iteration_train_loss: {iteration_train_loss}')\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, valid_loss: {valid_loss}, valid_score: {valid_score}')\n                wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n                wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n            iteration_num += 1\n            \n    if 'final_model_saver' in kwargs:\n        final_model_saver = kwargs['final_model_saver']\n        valid_score = 1\n        final_model_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n    \n    wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n    wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n\n    output = {'model': model, 'best_score': valid_score_saver.get_best_score()['best_score'], 'best_loss': valid_loss_saver.get_best_score()['best_score']}\n    return output","metadata":{"_uuid":"58478bb5-97a3-4f83-9e68-142bf3adedbe","_cell_guid":"a5df90a8-f212-484b-83e9-39c490f7107f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.526513Z","iopub.execute_input":"2021-08-01T22:03:08.526938Z","iopub.status.idle":"2021-08-01T22:03:08.542918Z","shell.execute_reply.started":"2021-08-01T22:03:08.526912Z","shell.execute_reply":"2021-08-01T22:03:08.5422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SWA","metadata":{"_uuid":"e6054d85-035e-4dba-924c-215ad983c395","_cell_guid":"0fdf8d67-96f6-40af-b205-15cd660dc5cd","trusted":true}},{"cell_type":"code","source":"def train_and_evaluate_swa(num_epochs, train_dataloader, valid_dataloader, tokenizer, model, optimizer, scheduler,\n                           forward_pass_fn_train, forward_pass_fn_valid, compute_loss_fn_train, compute_loss_fn_valid,\n                           compute_metric_fn, stacker_class, max_length, accumulation_steps, validate_every_n_iteraion, \n                           valid_loss_saver, valid_score_saver, device, swa_freq, **kwargs):\n    iteration_num = 0\n    for epoch_num in range(num_epochs):\n        data_in_epoch = len(train_dataloader.dataset)\n        epoch_step = 0\n        n_data = 0\n        for batch in train_dataloader:\n            for param_group in optimizer.param_groups:\n                wandb.log({param_group['name']: {\"lr\": param_group['lr']}})\n            model, iteration_train_loss = train_one_batch(iteration_num=iteration_num, batch=batch, tokenizer=tokenizer, model=model,\n                                                          optimizer=optimizer, scheduler=scheduler, forward_pass_fn=forward_pass_fn_train,\n                                                          compute_loss_fn=compute_loss_fn_train, max_length=max_length,\n                                                          accumulation_steps=accumulation_steps, device=device, **kwargs)\n            wandb.log({'Epoch_num': epoch_num, 'iteration_num': iteration_num, 'iteration_train_loss': iteration_train_loss})\n            epoch_step += 1\n            n_data += len(batch['target'])\n            eps = int(n_data/data_in_epoch)\n            if (epoch_step>=int(0.75*len(train_dataloader))) and (epoch_step%swa_freq==0):\n                    print(f\"taking swa snapshot @ {epoch_step}\")\n                    optimizer.update_swa()\n            if ((iteration_num + 1) % validate_every_n_iteraion == 0) or (eps!=0):\n                if eps!=0:\n                    print(f\"swap swa weights @ {epoch_step}\")\n                    optimizer.swap_swa_sgd()\n                valid_loss, valid_score = evaluate(dataloader=valid_dataloader, tokenizer=tokenizer, model=model,\n                                                   forward_pass_fn=forward_pass_fn_valid, compute_loss_fn=compute_loss_fn_valid,\n                                                   compute_metric_fn=compute_metric_fn, stacker_class=stacker_class,\n                                                   max_length=max_length, device=device, **kwargs)\n                valid_loss_saver.update(current_iteration_num=iteration_num,current_score=valid_loss, model=model, tokenizer=tokenizer)\n                valid_score_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n                wandb.log({\"iteration_num\": iteration_num, \"valid_loss\": valid_loss, \"valid_score\": valid_score})\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, iteration_train_loss: {iteration_train_loss}')\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, valid_loss: {valid_loss}, valid_score: {valid_score}')\n                wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n                wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n                \n                if eps!=0:\n                    print(\"swap weights back\")\n                    optimizer.swap_swa_sgd()\n            iteration_num += 1\n    if 'final_model_saver' in kwargs:\n        final_model_saver = kwargs['final_model_saver']\n        final_model_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n    wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n    wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n    output = {'model': model, 'best_score': valid_score_saver.get_best_score()['best_score'], 'best_loss': valid_loss_saver.get_best_score()['best_score']}\n    return output","metadata":{"_uuid":"9eab9b58-cdba-4bee-a49c-890b83c3c570","_cell_guid":"0d2bb467-f1a5-4089-9ff0-ea2f8bbd714d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.543881Z","iopub.execute_input":"2021-08-01T22:03:08.544268Z","iopub.status.idle":"2021-08-01T22:03:08.560905Z","shell.execute_reply.started":"2021-08-01T22:03:08.544239Z","shell.execute_reply":"2021-08-01T22:03:08.560384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions","metadata":{"_uuid":"5a9251f8-a1a0-4651-bff4-6e628bf76654","_cell_guid":"0fabb1c2-8ec6-4f38-a382-ebd067ae0145","trusted":true}},{"cell_type":"code","source":"def clear_cuda():\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_uuid":"e11b9220-287f-441e-abcc-5c36ef9a6b5e","_cell_guid":"23af4432-3cf1-49aa-8ae6-bc8347df21db","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.562306Z","iopub.execute_input":"2021-08-01T22:03:08.56285Z","iopub.status.idle":"2021-08-01T22:03:08.574244Z","shell.execute_reply.started":"2021-08-01T22:03:08.562808Z","shell.execute_reply":"2021-08-01T22:03:08.573327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"_uuid":"97523656-7e6c-422b-bafb-a8cb21d2412b","_cell_guid":"5ef88463-4973-40ec-8ead-8d762cd8391b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-01T22:03:08.575314Z","iopub.execute_input":"2021-08-01T22:03:08.57576Z","iopub.status.idle":"2021-08-01T22:03:08.583395Z","shell.execute_reply.started":"2021-08-01T22:03:08.575733Z","shell.execute_reply":"2021-08-01T22:03:08.58263Z"},"trusted":true},"execution_count":null,"outputs":[]}]}