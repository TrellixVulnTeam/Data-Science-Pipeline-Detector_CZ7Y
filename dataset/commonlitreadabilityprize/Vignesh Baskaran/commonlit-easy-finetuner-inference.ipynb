{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Commonlit Finetuned Roberta inference\nA Pretrained Roberta-base transformer is finetuned with the Competition dataset. This inference notebook is based on the training notebook here: https://www.kaggle.com/vigneshbaskaran/commonlit-easy-transformer-finetuner","metadata":{}},{"cell_type":"code","source":"import gc\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom torch import nn\n\nfrom pathlib import Path\nfrom transformers.file_utils import ModelOutput\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AdamW, AutoTokenizer, RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:02:31.081563Z","iopub.execute_input":"2021-06-10T18:02:31.082121Z","iopub.status.idle":"2021-06-10T18:02:37.81978Z","shell.execute_reply.started":"2021-06-10T18:02:31.082018Z","shell.execute_reply":"2021-06-10T18:02:37.81893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COMPETITION_DATA_PATH = Path('../input/commonlitreadabilityprize')\nTEST_DATA_PATH = COMPETITION_DATA_PATH / 'test.csv'","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:02:37.821099Z","iopub.execute_input":"2021-06-10T18:02:37.821435Z","iopub.status.idle":"2021-06-10T18:02:37.828018Z","shell.execute_reply.started":"2021-06-10T18:02:37.821398Z","shell.execute_reply":"2021-06-10T18:02:37.827299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Dataset and Dataloader","metadata":{}},{"cell_type":"code","source":"class PredictionDataset(Dataset):\n    def __init__(self, text_excerpts):\n        self.text_excerpts = text_excerpts\n        \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        sample = {'text_excerpt': self.text_excerpts[idx]}\n        return sample\n    \ndef create_prediction_dataloader(data, batch_size):\n    text_excerpts = data['excerpt'].tolist()\n    dataset = PredictionDataset(text_excerpts=text_excerpts)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:02:37.829894Z","iopub.execute_input":"2021-06-10T18:02:37.830253Z","iopub.status.idle":"2021-06-10T18:02:37.836979Z","shell.execute_reply.started":"2021-06-10T18:02:37.830216Z","shell.execute_reply":"2021-06-10T18:02:37.83591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Model","metadata":{}},{"cell_type":"code","source":"class RegressorOutput(ModelOutput):\n    loss = None\n    logits = None\n    hidden_states = None\n    attentions = None","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:02:37.838693Z","iopub.execute_input":"2021-06-10T18:02:37.839139Z","iopub.status.idle":"2021-06-10T18:02:37.846739Z","shell.execute_reply.started":"2021-06-10T18:02:37.839104Z","shell.execute_reply":"2021-06-10T18:02:37.845849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaPoolerRegressor(RobertaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        \n        self.roberta = RobertaModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self.loss_fct = nn.MSELoss()\n        self.init_weights()\n        \n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        bert_outputs = self.roberta(input_ids=input_ids,\n                                    attention_mask=attention_mask)\n        pooler_output = bert_outputs['pooler_output']\n        pooler_output = self.dropout(pooler_output)\n        logits = self.regressor(pooler_output)\n        loss = self.loss_fct(labels, logits) if labels is not None else None\n        return RegressorOutput(loss=loss, logits=logits)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:02:37.847897Z","iopub.execute_input":"2021-06-10T18:02:37.848479Z","iopub.status.idle":"2021-06-10T18:02:37.85711Z","shell.execute_reply.started":"2021-06-10T18:02:37.848408Z","shell.execute_reply":"2021-06-10T18:02:37.856304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define prediction loop","metadata":{}},{"cell_type":"code","source":"def predict(dataloader, model, tokenizer, device):\n    model.eval()\n    predictions = []\n    for batch_num, batch in enumerate(dataloader):\n        inputs = tokenizer(batch['text_excerpt'], padding=True, truncation=True, return_tensors=\"pt\")\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        batch_predictions = outputs.logits.detach().cpu().numpy()\n        predictions.append(batch_predictions)\n    predictions = np.vstack(predictions)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:02:37.858226Z","iopub.execute_input":"2021-06-10T18:02:37.858804Z","iopub.status.idle":"2021-06-10T18:02:37.866777Z","shell.execute_reply.started":"2021-06-10T18:02:37.858763Z","shell.execute_reply":"2021-06-10T18:02:37.866021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\n\ntest_data = pd.read_csv(TEST_DATA_PATH)\ntest_dataloader = create_prediction_dataloader(test_data, batch_size=4)\nTOKENIZER_PATH = '../input/commonlit-data-download/roberta-base'\npredictions = []\nfor fold_num in range(5):\n    model_path = '../input/commonlit-easy-transformer-finetuner/additional-pretrained-roberta-base-pooler-regressor/' + str(fold_num)\n    model = RobertaPoolerRegressor.from_pretrained(model_path)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    model.to(device)\n    \n    fold_predictions = predict(test_dataloader, model, tokenizer, device)\n    predictions.append(fold_predictions)\n\npredictions = np.hstack(predictions)\nmean_predictions = np.mean(predictions, axis=1)\ntest_data['target'] = mean_predictions\ntest_data[['id','target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:02:37.868014Z","iopub.execute_input":"2021-06-10T18:02:37.868585Z","iopub.status.idle":"2021-06-10T18:03:31.119609Z","shell.execute_reply.started":"2021-06-10T18:02:37.868548Z","shell.execute_reply":"2021-06-10T18:03:31.118734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}