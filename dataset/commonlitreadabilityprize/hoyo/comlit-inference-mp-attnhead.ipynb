{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How This Notebook Works\nThis notebook takes my own pretrained and finetuned roberta-large and roberta-base models and uses them to give predictions on the competition data.  \nThe parameters as well as the attention head class were taken from **Maunish Dave's** fantastic notebook [here](https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference)  \n\n### Roberta Large\n[My pretraining notebook](https://www.kaggle.com/bumjunkoo/comlit-pretrain-rob-lrg)  \n[My finetuning notebook](https://www.kaggle.com/bumjunkoo/comlit-finetune-rob-lrg)\n\n### Roberta Base\n[My pretraining notebook](https://www.kaggle.com/bumjunkoo/comlit-pretrainer)  \n[My finetuning notebook](https://www.kaggle.com/bumjunkoo/comlit-finetune)  \n\nThe output is a weighted sum of the results of roBERTa base and roBERTa large with more weight given to roBERTa large's results.","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# transformers\nfrom transformers import AutoModel, AutoModelForMaskedLM, AutoTokenizer, AutoConfig, DataCollatorWithPadding, get_scheduler, AdamW\nfrom transformers import logging\nlogging.set_verbosity(50) # prevents the model warning from popping up\n\n# sklearn\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# torch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\nimport random, time, sys, os, gc, math\n\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-07-27T02:05:08.400677Z","iopub.execute_input":"2021-07-27T02:05:08.401054Z","iopub.status.idle":"2021-07-27T02:05:15.981142Z","shell.execute_reply.started":"2021-07-27T02:05:08.400977Z","shell.execute_reply":"2021-07-27T02:05:15.980104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T02:05:15.983022Z","iopub.execute_input":"2021-07-27T02:05:15.983417Z","iopub.status.idle":"2021-07-27T02:05:16.003114Z","shell.execute_reply.started":"2021-07-27T02:05:15.983376Z","shell.execute_reply":"2021-07-27T02:05:16.002248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T02:05:16.009348Z","iopub.execute_input":"2021-07-27T02:05:16.010004Z","iopub.status.idle":"2021-07-27T02:05:16.019818Z","shell.execute_reply.started":"2021-07-27T02:05:16.009964Z","shell.execute_reply":"2021-07-27T02:05:16.018959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n\ndef data_loader(text, tokenizer, labels=None, batch=32, shuffle=False):\n    tokens = tokenizer(text, max_length=256, padding='max_length', truncation=True) \n    dataset = CommonLitDataset(tokens, labels)\n    dataloader = DataLoader(dataset, shuffle=shuffle, batch_size=batch, pin_memory=True) # set pin_memory = True when training on GPU\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2021-07-27T02:05:16.021342Z","iopub.execute_input":"2021-07-27T02:05:16.022072Z","iopub.status.idle":"2021-07-27T02:05:16.031945Z","shell.execute_reply.started":"2021-07-27T02:05:16.022034Z","shell.execute_reply":"2021-07-27T02:05:16.031083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLModel(nn.Module):\n    def __init__(self, model_or_path):\n        super(CLModel, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_or_path)\n        self.hidden_size = self.roberta.config.hidden_size\n        self.head = AttentionHead(self.hidden_size, self.hidden_size, 1) # hidden size is 768\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        attn_head = self.head(outputs['last_hidden_state'])\n        dropout = self.dropout(attn_head)\n        linear = self.linear(dropout)\n        return linear","metadata":{"execution":{"iopub.status.busy":"2021-07-27T02:05:16.033576Z","iopub.execute_input":"2021-07-27T02:05:16.034355Z","iopub.status.idle":"2021-07-27T02:05:16.046825Z","shell.execute_reply.started":"2021-07-27T02:05:16.034246Z","shell.execute_reply":"2021-07-27T02:05:16.045904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        self.model = AutoModel.from_pretrained(model_name)\n        hidden_size = self.model.config.hidden_size\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        self.linear = nn.Linear(hidden_size, 1)\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs['last_hidden_state']\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        norm_mean_embeddings = self.layer_norm(mean_embeddings)\n        logits = self.linear(norm_mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        return preds","metadata":{"execution":{"iopub.status.busy":"2021-07-27T02:05:16.048068Z","iopub.execute_input":"2021-07-27T02:05:16.048471Z","iopub.status.idle":"2021-07-27T02:05:16.061234Z","shell.execute_reply.started":"2021-07-27T02:05:16.048416Z","shell.execute_reply":"2021-07-27T02:05:16.059792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def get_prediction(df, model, path, model_path,device='cuda'):        \n\n    model.load_state_dict(torch.load(path,map_location=device))\n    model.to(device)\n    model.eval()\n    \n    print(\"Tokenizing data...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    dataloader = data_loader(df.excerpt.tolist(), tokenizer, batch = 16)\n    \n    predictions = list()\n    print(\"Predicting...\")\n    for step, batch in enumerate(dataloader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n            \n        outputs = outputs.cpu().detach().numpy().ravel().tolist()\n        predictions.extend(outputs)\n    print(\"Prediction Finished\")\n    torch.cuda.empty_cache()\n    return np.array(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T02:05:16.063138Z","iopub.execute_input":"2021-07-27T02:05:16.063569Z","iopub.status.idle":"2021-07-27T02:05:16.075712Z","shell.execute_reply.started":"2021-07-27T02:05:16.063527Z","shell.execute_reply":"2021-07-27T02:05:16.074602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# roberta large models\nlarge_predictions=[]\nmodel_path = '../input/robertalarge'\ncl = CLModel(model_path)\nprint('-'*30)\nfor i in range(5):\n    i=i+1\n    print(f\"Model {i}\")\n    print('-'*30)\n    path = f'../input/comlit-finetune-rob-lrg/model_{i}/model_{i}.bin'\n    large_predictions.append( get_prediction(test[['excerpt']], cl, path, model_path ) )\n    print('-'*30)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T02:07:40.718039Z","iopub.execute_input":"2021-07-27T02:07:40.718411Z","iopub.status.idle":"2021-07-27T02:08:51.076526Z","shell.execute_reply.started":"2021-07-27T02:07:40.718378Z","shell.execute_reply":"2021-07-27T02:08:51.075538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# roberta base models\nbase_predictions=[]\nmodel_path = '../input/roberta-base'\nmp = MeanPoolingModel(model_path)\nprint('-'*30)\nfor i in range(5):\n    i=i+1\n    print(f\"Model {i}\")\n    print('-'*30)\n    path = f'../input/cl-base-finetune-mean-pool/model_{i}/model_{i}.bin'\n    base_predictions.append( get_prediction(test[['excerpt']], mp, path, model_path ) )\n    print('-'*30)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T02:09:50.03192Z","iopub.execute_input":"2021-07-27T02:09:50.032244Z","iopub.status.idle":"2021-07-27T02:10:26.658803Z","shell.execute_reply.started":"2021-07-27T02:09:50.032214Z","shell.execute_reply":"2021-07-27T02:10:26.657985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum_base_preds = sum(base_predictions)/5\nsum_large_preds = sum(large_predictions)/5\n# combined = (sum_large_preds+sum_base_preds)/2\nweighted = (sum_large_preds*0.7) + (sum_base_preds*0.3)\nsample['target'] = weighted\nsample.to_csv('submission.csv',index=False)\nsample","metadata":{"execution":{"iopub.status.busy":"2021-07-27T02:10:26.662084Z","iopub.execute_input":"2021-07-27T02:10:26.662383Z","iopub.status.idle":"2021-07-27T02:10:27.073455Z","shell.execute_reply.started":"2021-07-27T02:10:26.662354Z","shell.execute_reply":"2021-07-27T02:10:27.072539Z"},"trusted":true},"execution_count":null,"outputs":[]}]}