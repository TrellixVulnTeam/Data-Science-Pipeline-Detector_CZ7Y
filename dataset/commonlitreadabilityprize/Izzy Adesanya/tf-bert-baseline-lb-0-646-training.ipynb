{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"border:2px solid Purple;text-align:center\">Introduction ðŸŽ–</h1>","metadata":{}},{"cell_type":"markdown","source":"This notebook is aimed at folks trying to get started with the competition in Tensorflow & BERT. We will be going through the entire training pipeline, so sit tight.\n\nIt is very simple and intuitive to understand. I hope it is useful for everyone !!","metadata":{}},{"cell_type":"markdown","source":"**Please remember to upvote the notebook, if you liked the content !!** ðŸ˜ƒðŸ˜ƒ ","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"border:2px solid Purple;text-align:center\">Topics Covered ðŸ“Œ</h1>\n\n**1. Importing Libraries ðŸ“š**\n\n**2. Defining Configs ðŸ’¬**\n\n**3. Data Pipeline ðŸ“‚**\n\n**4. Training Pipeline ðŸŽ¯**\n\n**5. Evaluation ðŸ–Š**","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"border:2px solid Purple;text-align:center\">1. Importing Libraries ðŸ“š</h1>","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nfrom copy import copy\nimport gc\nimport joblib\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport sys\nfrom warnings import simplefilter\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input, backend as K\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Dropout\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import TFBertModel, BertConfig, BertTokenizerFast","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"border:2px solid Purple;text-align:center\">2. Defining Configs ðŸ’¬</h1>","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    model_name = 'best_model'\n\n    data_dir = Path('../input/commonlitreadabilityprize')\n    train_file = data_dir / 'train.csv'\n    test_file = data_dir / 'test.csv'\n    sample_file = data_dir / 'sample_submission.csv'\n\n    build_dir = Path('./build/')\n    output_dir = build_dir / model_name\n    trn_encoded_file = output_dir / 'trn.enc.joblib'\n    val_predict_file = output_dir / f'{model_name}.val.txt'\n    submission_file = 'submission.csv'\n\n    pretrained_dir = '../input/tfbert-large-uncased'\n\n    id_col = 'id'\n    target_col = 'target'\n    text_col = 'excerpt'\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    max_len = 205\n    n_fold = 5\n    n_est = 10\n    n_stop = 2\n    batch_size = 8\n    seed = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"border:2px solid Purple;text-align:center\">3. Data Pipeline ðŸ“‚</h1>","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(CFG.train_file, index_col=CFG.id_col)\ntest_df = pd.read_csv(CFG.test_file, index_col=CFG.id_col)\ny = train_df[CFG.target_col].values\nprint(test_df.shape, y.shape, train_df.shape)\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Tokenize:\n    \n    def load_tokenizer():\n        \n        if not os.path.exists(CFG.pretrained_dir + '/vocab.txt'):\n            Path(CFG.pretrained_dir).mkdir(parents=True, exist_ok=True)\n            tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased\")\n            tokenizer.save_pretrained(CFG.pretrained_dir)\n        else:\n            print('loading the saved pretrained tokenizer')\n            tokenizer = BertTokenizerFast.from_pretrained(CFG.pretrained_dir)\n\n        model_config = BertConfig.from_pretrained(CFG.pretrained_dir)\n        model_config.output_hidden_states = True\n        \n        return tokenizer, model_config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT:\n    \n    def load_bert(config):\n        \n        if not os.path.exists(CFG.pretrained_dir + '/tf_model.h5'):\n            Path(CFG.pretrained_dir).mkdir(parents=True, exist_ok=True)\n            bert_model = TFBertModel.from_pretrained(\"bert-large-uncased\", config=config)\n            bert_model.save_pretrained(CFG.pretrained_dir)\n        else:\n            print('loading the saved pretrained model')\n            bert_model = TFBertModel.from_pretrained(CFG.pretrained_dir, config=config)\n        return bert_model\n\n    def bert_encode(texts, tokenizer, max_len=CFG.max_len):\n        \n        input_ids = []\n        token_type_ids = []\n        attention_mask = []\n\n        for text in texts:\n            token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n                             add_special_tokens=True)\n            input_ids.append(token['input_ids'])\n            token_type_ids.append(token['token_type_ids'])\n            attention_mask.append(token['attention_mask'])\n\n        return np.array(input_ids), np.array(token_type_ids), np.array(attention_mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"border:2px solid Purple;text-align:center\">4. Training Pipeline ðŸŽ¯</h1>","metadata":{}},{"cell_type":"code","source":"tokenizer, bert_config = Tokenize.load_tokenizer()\n\nX = BERT.bert_encode(train_df[CFG.text_col].values, tokenizer, max_len=CFG.max_len)\nX_tst = BERT.bert_encode(test_df[CFG.text_col].values, tokenizer, max_len=CFG.max_len)\ny = train_df[CFG.target_col].values\nprint(X[0].shape, X_tst[0].shape, y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(X, CFG.trn_encoded_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class model:\n\n    def build_model(bert_model, max_len=CFG.max_len):    \n        \n        input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n        token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n        attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n        sequence_output = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n        clf_output = sequence_output[:, 0, :]\n        clf_output = Dropout(.1)(clf_output)\n        out = Dense(1, activation='linear')(clf_output)\n\n        model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n        model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n\n        return model\n    \n    def scheduler(epoch, lr, warmup=5, decay_start=10):\n        \n        if epoch <= warmup:\n            return lr / (warmup - epoch + 1)\n        elif warmup < epoch <= decay_start:\n            return lr\n        else:\n            return lr * tf.math.exp(-.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Train:\n    \n    def train():\n\n        ls = LearningRateScheduler(model.scheduler)\n        es = EarlyStopping(patience=CFG.n_stop, restore_best_weights=True)\n\n        cv = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n\n        p = np.zeros_like(y, dtype=float)\n        p_tst = np.zeros((X_tst[0].shape[0], ), dtype=float)\n        for i, (i_trn, i_val) in enumerate(cv.split(X[0]), 1):\n            print(f'Training CV #{i}:')\n            tf.random.set_seed(CFG.seed + i)\n\n            bert_model = BERT.load_bert(bert_config)\n            clf = model.build_model(bert_model, max_len=CFG.max_len)\n            if i == 1:\n                print(clf.summary())\n            history = clf.fit([x[i_trn] for x in X], y[i_trn],\n                              validation_data=([x[i_val] for x in X], y[i_val]),\n                              epochs=CFG.n_est,\n                              batch_size=CFG.batch_size,\n                              callbacks=[ls])\n            clf.save_weights(f'{CFG.model_name}_cv{i}.h5')\n\n            p[i_val] = clf.predict([x[i_val] for x in X]).flatten()\n            p_tst += clf.predict(X_tst).flatten() / CFG.n_fold\n\n            K.clear_session()\n            del clf, bert_model\n            gc.collect()\n        \n        return p, p_tst","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p, p_test = Train.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"border:2px solid Purple;text-align:center\">5. Evaluation ðŸ–Š</h1>","metadata":{}},{"cell_type":"code","source":"print(f'CV RMSE: {mean_squared_error(y, p, squared=False):.6f}')\nnp.savetxt(CFG.val_predict_file, p, fmt='%.6f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hope you liked the notebook !!","metadata":{}}]}