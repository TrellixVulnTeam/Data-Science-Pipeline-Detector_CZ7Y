{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"border:2px solid Purple;text-align:center\">Introduction ðŸŽ–</h1>","metadata":{}},{"cell_type":"markdown","source":"This notebook is in continuation of my previous notebook on **training** Tf BERT based baseline model here -> https://www.kaggle.com/prvnkmr/tf-bert-baseline-lb-0-646-training. \n\nPlease go through that if not already done so.","metadata":{}},{"cell_type":"markdown","source":"This notebook focuses on inferencing on test data, by the model trained in our training notebook.\nRemember that we trained Multiple CV models, so we will go over all those models and take the mean output (sort of like what we do in bagging).","metadata":{}},{"cell_type":"markdown","source":"**As always if you like the content, please do remember to upvote !! ðŸ˜ƒðŸ˜ƒ**","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nimport os\nimport numpy as np\nfrom sklearn.model_selection import KFold\nimport gc\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input, backend as K\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Dropout\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import TFBertModel, BertConfig, BertTokenizerFast","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    model_name = 'best_model'\n\n    data_dir = Path('../input/commonlitreadabilityprize')\n    train_file = data_dir / 'train.csv'\n    test_file = data_dir / 'test.csv'\n    sample_file = data_dir / 'sample_submission.csv'\n\n    build_dir = Path('./build/')\n    output_dir = build_dir / model_name\n    trn_encoded_file = output_dir / 'trn.enc.joblib'\n    val_predict_file = output_dir / f'{model_name}.val.txt'\n    submission_file = 'submission.csv'\n\n    pretrained_dir = '../input/tfbert-large-uncased'\n\n    id_col = 'id'\n    target_col = 'target'\n    text_col = 'excerpt'\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    max_len = 205\n    n_fold = 5\n    n_est = 10\n    n_stop = 2\n    batch_size = 8\n    seed = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Model","metadata":{}},{"cell_type":"code","source":"class Tokenize:\n    \n    def load_tokenizer():\n        \n        if not os.path.exists(CFG.pretrained_dir + '/vocab.txt'):\n            Path(CFG.pretrained_dir).mkdir(parents=True, exist_ok=True)\n            tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased\")\n            tokenizer.save_pretrained(CFG.pretrained_dir)\n        else:\n            print('loading the saved pretrained tokenizer')\n            tokenizer = BertTokenizerFast.from_pretrained(CFG.pretrained_dir)\n\n        model_config = BertConfig.from_pretrained(CFG.pretrained_dir)\n        model_config.output_hidden_states = True\n        \n        return tokenizer, model_config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT:\n    \n    def load_bert(config):\n        \n        if not os.path.exists(CFG.pretrained_dir + '/tf_model.h5'):\n            Path(CFG.pretrained_dir).mkdir(parents=True, exist_ok=True)\n            bert_model = TFBertModel.from_pretrained(\"bert-large-uncased\", config=config)\n            bert_model.save_pretrained(CFG.pretrained_dir)\n        else:\n            print('loading the saved pretrained model')\n            bert_model = TFBertModel.from_pretrained(CFG.pretrained_dir, config=config)\n        return bert_model\n\n    def bert_encode(texts, tokenizer, max_len=CFG.max_len):\n        \n        input_ids = []\n        token_type_ids = []\n        attention_mask = []\n\n        for text in texts:\n            token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n                             add_special_tokens=True)\n            input_ids.append(token['input_ids'])\n            token_type_ids.append(token['token_type_ids'])\n            attention_mask.append(token['attention_mask'])\n\n        return np.array(input_ids), np.array(token_type_ids), np.array(attention_mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class model:\n\n    def build_model(bert_model, max_len=CFG.max_len):    \n        \n        input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n        token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n        attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n        sequence_output = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n        clf_output = sequence_output[:, 0, :]\n        clf_output = Dropout(.1)(clf_output)\n        out = Dense(1, activation='linear')(clf_output)\n\n        model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n        model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n\n        return model\n    \n    def scheduler(epoch, lr, warmup=5, decay_start=10):\n        \n        if epoch <= warmup:\n            return lr / (warmup - epoch + 1)\n        elif warmup < epoch <= decay_start:\n            return lr\n        else:\n            return lr * tf.math.exp(-.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer, bert_config = Tokenize.load_tokenizer()\n\ntest_df = pd.read_csv(CFG.test_file, index_col=CFG.id_col)\nX_test = BERT.bert_encode(test_df[CFG.text_col].values, tokenizer, max_len=CFG.max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"class Infer:\n    \n    def infer():\n        infer_result = np.zeros((X_test[0].shape[0], ), dtype=float)\n        cv = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n\n        for i, (i_trn, i_val) in enumerate(cv.split(X_test[0]), 1):\n\n            bert_model = BERT.load_bert(bert_config)\n            clf = model.build_model(bert_model, max_len=CFG.max_len)\n\n            clf.load_weights(Path(f'../input/tf-bert-baseline-lb-0-646-training/bert_v13_cv{i}.h5'))\n\n            infer_result += clf.predict(X_test).flatten() / CFG.n_fold\n            \n        return infer_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_result = Infer.infer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(CFG.sample_file, index_col=CFG.id_col)\nsub[CFG.target_col] = infer_result\nsub.to_csv(CFG.submission_file)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}