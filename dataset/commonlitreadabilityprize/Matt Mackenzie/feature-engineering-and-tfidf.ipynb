{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyphen\n!pip install tidybear","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:57:29.035916Z","iopub.execute_input":"2021-07-28T01:57:29.036767Z","iopub.status.idle":"2021-07-28T01:57:46.419795Z","shell.execute_reply.started":"2021-07-28T01:57:29.036606Z","shell.execute_reply":"2021-07-28T01:57:46.418581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tidybear as tb\n\nfrom tqdm import tqdm\n\nimport nltk\nfrom pyphen import Pyphen\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndef summarise_cv_scores(scores):\n    return (len(scores), np.mean(scores), np.std(scores))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-28T01:57:46.42193Z","iopub.execute_input":"2021-07-28T01:57:46.422255Z","iopub.status.idle":"2021-07-28T01:57:48.323727Z","shell.execute_reply.started":"2021-07-28T01:57:46.422221Z","shell.execute_reply":"2021-07-28T01:57:48.322559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set Up\n\nTo start, I read in the training data and select the columns I care about.\n\nThen I assign an approx grade by dividing the target into 10 regioins (deciles, grades 3-12).","metadata":{}},{"cell_type":"code","source":"train_ = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ntrain_ = train_.loc[:, [\"id\", \"excerpt\", \"target\", \"standard_error\"]]\n\ngrade_lbls = [i for i in range(12, 2, -1)]\ntrain_[\"grade\"] = pd.qcut(train_.target, q=len(grade_lbls), labels=grade_lbls)\ntrain_[\"grade\"] = train_.grade.astype(int)\n\ndef get_school_level(grade):\n    if grade <= 5:\n        return \"elementary\"\n    elif grade <= 8:\n        return \"middle\"\n    else:\n        return \"high\"\n    \ntrain_[\"school\"] = train_.grade.apply(get_school_level)\n\nprint(train_.shape)\ntrain_.tail()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:57:48.32587Z","iopub.execute_input":"2021-07-28T01:57:48.326226Z","iopub.status.idle":"2021-07-28T01:57:48.505785Z","shell.execute_reply.started":"2021-07-28T01:57:48.326192Z","shell.execute_reply":"2021-07-28T01:57:48.504485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we look at an example of the easiest and hardest (by target) excerpts to read. As per the discussion and by example, higher scores are eaiser (lower grade level), lower scores are harder (higher grade level).","metadata":{}},{"cell_type":"code","source":"print(\"Max Target - Easiest to read - lowest grade level\\n\")\nprint(train_[train_.target == train_.target.max()].excerpt.values[0])\n\nprint(\"\\n-------------------------\\n\")\n\nprint(\"Min Target - Hardest to read - highest grade level\\n\")\nprint(train_[train_.target == train_.target.min()].excerpt.values[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:57:48.507779Z","iopub.execute_input":"2021-07-28T01:57:48.508136Z","iopub.status.idle":"2021-07-28T01:57:48.522874Z","shell.execute_reply.started":"2021-07-28T01:57:48.508101Z","shell.execute_reply":"2021-07-28T01:57:48.52122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_.target.plot.hist();","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:57:48.524528Z","iopub.execute_input":"2021-07-28T01:57:48.52486Z","iopub.status.idle":"2021-07-28T01:57:48.927125Z","shell.execute_reply.started":"2021-07-28T01:57:48.524827Z","shell.execute_reply":"2021-07-28T01:57:48.926164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the distribution above, the target is pretty normal. This tells me there is lots of overlap between grade level readability... or as the discussion puts it, the categories are squisy....\n\nHowever below, using the approx grade level, the words being used on average for higher grade levels are longer.","metadata":{}},{"cell_type":"markdown","source":"## Non-Text Features","metadata":{}},{"cell_type":"code","source":"train = train_.copy()\n\npyphen = Pyphen(lang=\"en\")\n\ndef syllables(word):\n    return len(pyphen.positions(word)) + 1\n\ndef engineer_features(df):\n    word_tok = df.excerpt.apply(nltk.tokenize.word_tokenize)\n    sent_tok = df.excerpt.apply(nltk.tokenize.sent_tokenize)\n    syls_tok = word_tok.apply(lambda x: [syllables(w) for w in x])\n\n    total_charachters = word_tok.apply(lambda x: np.sum([len(w) for w in x]))\n    total_words = word_tok.apply(lambda x: len(x))\n    total_syllables = syls_tok.apply(lambda x: np.sum(x))\n    total_sentences = sent_tok.apply(lambda x: len(x))\n\n    df[\"unique_words\"] = word_tok.apply(lambda x: np.unique(x).shape[0]) / total_words\n    df[\"words_geq_len8\"] = word_tok.apply(lambda x: np.sum([len(w) >= 8 for w in x])) / total_words\n    df[\"hard_words\"] = syls_tok.apply(lambda x: np.sum([s >= 3 for s in x])) / total_words\n\n    df[\"characters_per_word\"] = total_charachters / total_words\n    df[\"syllables_per_word\"] = total_syllables / total_words\n    df[\"words_per_sentence\"] = total_words / total_sentences\n\nengineer_features(train)\ntrain.drop(columns=[\"standard_error\", \"grade\"]).corr()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:57:48.928488Z","iopub.execute_input":"2021-07-28T01:57:48.928791Z","iopub.status.idle":"2021-07-28T01:58:00.566789Z","shell.execute_reply.started":"2021-07-28T01:57:48.928763Z","shell.execute_reply":"2021-07-28T01:58:00.56586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_text_features = [\n    \"unique_words\",\n    \"words_geq_len8\",\n    \"words_per_sentence\"\n]\n\nwith tb.GroupBy(train, \"grade\") as g:\n    g.mean(non_text_features, decimals=2)\n    grade_summary = g.summarise()\n    \ngrade_summary = grade_summary \\\n    .stack() \\\n    .rename(\"value\") \\\n    .reset_index() \\\n    .rename(columns={\"level_1\": \"feature\"})\n\ng = sns.FacetGrid(grade_summary, col=\"feature\", col_wrap=3, sharey=False, height=4)\ng.map_dataframe(sns.barplot, x=\"grade\", y=\"value\");","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:58:00.568078Z","iopub.execute_input":"2021-07-28T01:58:00.568368Z","iopub.status.idle":"2021-07-28T01:58:01.377084Z","shell.execute_reply.started":"2021-07-28T01:58:00.56834Z","shell.execute_reply":"2021-07-28T01:58:01.375942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, GridSearchCV\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.feature_selection import RFECV","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:58:01.380844Z","iopub.execute_input":"2021-07-28T01:58:01.381243Z","iopub.status.idle":"2021-07-28T01:58:01.530755Z","shell.execute_reply.started":"2021-07-28T01:58:01.381208Z","shell.execute_reply":"2021-07-28T01:58:01.529803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_params = {\n    \"cv\": 10, \n    \"scoring\": \"neg_root_mean_squared_error\", \n    \"n_jobs\": -1, \n    \"verbose\": 2\n}\n\nmodel_grid = {\"reg\": [\n    DummyRegressor(),\n    LinearRegression(),\n    RidgeCV(),\n    RandomForestRegressor(max_depth=3, random_state=123)\n]}\n\npipe = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"scale\", StandardScaler()),\n    (\"reg\", RidgeCV())\n])\n\ngrid = GridSearchCV(pipe, model_grid, **gs_params)\n\nX = train[non_text_features]\ny = train.target\n\ngrid.fit(X, y)\n\npd.DataFrame(grid.cv_results_)[[\"param_reg\", \"mean_test_score\", \"std_test_score\"]]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:58:01.532632Z","iopub.execute_input":"2021-07-28T01:58:01.533059Z","iopub.status.idle":"2021-07-28T01:58:06.218432Z","shell.execute_reply.started":"2021-07-28T01:58:01.532995Z","shell.execute_reply":"2021-07-28T01:58:06.216857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_text_pipe = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"scale\", StandardScaler()),\n    (\"poly\", PolynomialFeatures()),\n    (\"feat\", RFECV(LinearRegression(), cv=10)),\n    (\"reg\", RidgeCV())\n])\n\ngrid = GridSearchCV(non_text_pipe, model_grid, **gs_params)\ngrid.fit(X, y)\n\npd.DataFrame(grid.cv_results_)[[\"param_reg\", \"mean_test_score\", \"std_test_score\"]]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:58:06.219822Z","iopub.execute_input":"2021-07-28T01:58:06.220159Z","iopub.status.idle":"2021-07-28T01:58:11.149137Z","shell.execute_reply.started":"2021-07-28T01:58:06.220127Z","shell.execute_reply":"2021-07-28T01:58:11.148069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_text_lm = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"scale\", StandardScaler()),\n    (\"reg\", LinearRegression())\n])\n\nscores = cross_val_score(non_text_lm, X, y, **gs_params)\nprint(\"RMSE (cv={}): {:.3f} ({:.3f})\".format(*summarise_cv_scores(-scores)))\n\nnon_text_lm.fit(X, y)\ncoefs = pd.DataFrame({\"coef\": non_text_lm.named_steps[\"reg\"].coef_}, index=non_text_features)\ncoefs.sort_values(\"coef\").plot.barh()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:58:11.150603Z","iopub.execute_input":"2021-07-28T01:58:11.151202Z","iopub.status.idle":"2021-07-28T01:58:11.407246Z","shell.execute_reply.started":"2021-07-28T01:58:11.151158Z","shell.execute_reply":"2021-07-28T01:58:11.406198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With no NLP, just summary stats about the vocab of the text, we get an average RMSE across 10 folds of .84 for the Ridge regression. We'll need to halve that to win...","metadata":{}},{"cell_type":"markdown","source":"## Simple NLP","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\ntm_grid = {\"reg\": [\n    LinearRegression(),\n    Ridge(),\n    Lasso()\n]}\n\ntext_pipe = Pipeline([\n    (\"count\", CountVectorizer()),\n    (\"scale\", TfidfTransformer()),\n    (\"reg\", Ridge())\n])\n\ngrid = GridSearchCV(text_pipe, tm_grid, **gs_params)\ngrid.fit(train.excerpt, train.target)\n\npd.DataFrame(grid.cv_results_)[[\"param_reg\", \"mean_test_score\", \"std_test_score\"]]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:58:11.408501Z","iopub.execute_input":"2021-07-28T01:58:11.4088Z","iopub.status.idle":"2021-07-28T01:58:22.122216Z","shell.execute_reply.started":"2021-07-28T01:58:11.408773Z","shell.execute_reply":"2021-07-28T01:58:22.12117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combine Non-Text and Simple NLP","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\nnon_text_trans = make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    StandardScaler(),\n    PolynomialFeatures(),\n    RFECV(LinearRegression(), cv=10)\n)\n\ntext_trans = make_pipeline(\n    CountVectorizer(),\n    TfidfTransformer(),\n)\n\ncombined_pipe = Pipeline([\n    (\"transform\", make_column_transformer(\n        (non_text_trans, non_text_features),\n        (text_trans, \"excerpt\"),\n        remainder=\"drop\"\n    )),\n    (\"predict\", Ridge())\n])\n\nscores = cross_val_score(combined_pipe, train, train.target, **gs_params)\nprint(\"RMSE (cv={}): {:.3f} ({:.3f})\".format(*summarise_cv_scores(-scores)))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:58:22.123869Z","iopub.execute_input":"2021-07-28T01:58:22.124583Z","iopub.status.idle":"2021-07-28T01:58:26.181017Z","shell.execute_reply.started":"2021-07-28T01:58:22.124536Z","shell.execute_reply":"2021-07-28T01:58:26.180293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nengineer_features(test)\n\ntrain_cols = [\"excerpt\"] + non_text_features\ncombined_pipe.fit(train[train_cols], train.target.values)\ntest_pred = combined_pipe.predict(test[train_cols])\n\nsubmission = pd.DataFrame({\n    \"id\": test.id,\n    \"target\": test_pred\n})\n\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:58:26.181928Z","iopub.execute_input":"2021-07-28T01:58:26.182204Z","iopub.status.idle":"2021-07-28T01:58:27.205954Z","shell.execute_reply.started":"2021-07-28T01:58:26.182178Z","shell.execute_reply":"2021-07-28T01:58:27.204694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:58:27.210923Z","iopub.execute_input":"2021-07-28T01:58:27.211412Z","iopub.status.idle":"2021-07-28T01:58:27.221812Z","shell.execute_reply.started":"2021-07-28T01:58:27.211366Z","shell.execute_reply":"2021-07-28T01:58:27.22052Z"},"trusted":true},"execution_count":null,"outputs":[]}]}