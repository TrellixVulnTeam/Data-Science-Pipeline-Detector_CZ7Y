{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction 📝\n🎯 **Goal:** To build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. \n\n📖 **Data:** \n> **train.csv / test.csv** - the training and testing set\n> - ```id``` - unique ID for excerpt\n> - ```url_legal``` - URL of source \n> - ```license``` - license of source material \n> - ```excerpt``` - text to predict reading ease of\n> - ```target``` - reading ease\n> - ```standard_error``` - measure of spread of scores among multiple raters for each excerpt\n\n📌 **Note:** ```url_legal```, ```license``` and ```standard error``` are blank in the test set.","metadata":{}},{"cell_type":"markdown","source":"### Copied and adapted from https://www.kaggle.com/ruchi798/commonlit-readability-prize-eda-baseline","metadata":{}},{"cell_type":"markdown","source":"# Import libraries 📚","metadata":{}},{"cell_type":"code","source":"\n#!pip install textstat\n#!pip install nlpaug\n#!pip install torch>=1.6.0 transformers>=4.0.0\n#!pip install torch>=1.6.0 fairseq>=0.9.0 sacremoses>=0.0.43 fastBPE>=0.1.0\n#!pip install nltk>=3.4.5\n\nimport sys\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport re\nimport nltk\nsys.path.append('..input/textstat')\n#import textstat\nimport time\nimport wandb\nimport xgboost as xgb\n\n\nfrom pandas import DataFrame\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as mse\n\nfrom nltk import tokenize\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom scipy.stats import gaussian_kde\n\n#nltk.download('stopwords')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-05-23T09:23:10.567369Z","iopub.execute_input":"2021-05-23T09:23:10.567791Z","iopub.status.idle":"2021-05-23T09:23:10.577175Z","shell.execute_reply.started":"2021-05-23T09:23:10.567751Z","shell.execute_reply":"2021-05-23T09:23:10.575947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:23:13.268201Z","iopub.execute_input":"2021-05-23T09:23:13.268651Z","iopub.status.idle":"2021-05-23T09:23:13.363728Z","shell.execute_reply.started":"2021-05-23T09:23:13.268603Z","shell.execute_reply":"2021-05-23T09:23:13.3626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Statistical feature extraction","metadata":{}},{"cell_type":"markdown","source":"We want to extract some feature that could influence the readability of a text, like the average sentence length, average word length ...","metadata":{}},{"cell_type":"code","source":"stopwords_en = set(stopwords.words('english'))\nlemma = nltk.WordNetLemmatizer()\n\ndef tokenize(text):\n    text = re.sub(r'[^a-z]', ' ', text.lower())\n    words = nltk.word_tokenize(text)\n    words = [lemma.lemmatize(w) for w in words if not w in stopwords_en]\n    return words\ndef normalize(text):\n    return ' '.join(tokenize(text))\ndef word_count(text):\n    return len(text.split(' '))\ndef long_words(text, length):\n    return len([w for w in text.split(' ') if len(w) >= length])\ndef max_word_len(text):\n    words = (re.sub(r\"[,.;@#?!&$]+\\ *-\", \" \", text)).split()\n    return np.max([len(w) for w in text.split(' ')])\ndef avg_sen_len(text):\n    text.replace('...','.')\n    sen_lens = []\n    for sentence in text.split('.'):\n        sen_lens.append(len(sentence.split())) \n    return sum(sen_lens)/len(sen_lens)\ndef avg_word_len(text):\n    words = (re.sub(r\"[,.;@#?!&$]+\\ *-\", \" \", text)).split()\n    return sum([len(word) for word in words])/len(words)\ndef punct_count(text):\n    return sum(text.count(x) for x in \"[,.;:@#?!&$]+\\ *-\")\ndef min_sen_len(text):\n    text.replace('...','.')\n    text.replace('..','.')\n    sen_lens = []\n    for sentence in text.split('.'):\n        sen_lens.append(len(sentence.split())) \n    return np.min(sen_lens)\ndef max_sen_len(text):\n    text.replace('...','.')\n    text.replace('..','.')\n    sen_lens = []\n    for sentence in text.split('.'):\n        sen_lens.append(len(sentence.split())) \n    return np.max(sen_lens)\n    \n\ndef extract_statistical_feature(dataframe):\n    dataframe['tokens'] = [np.array(tokenize(str(dataframe['excerpt'][i]))) for i in range(0, dataframe['excerpt'].size)]\n    dataframe['normalized'] = [normalize(str(dataframe['excerpt'][i])) for i in range(0, dataframe['excerpt'].size)]\n    dataframe['count'] = dataframe['excerpt'].apply(word_count)\n    dataframe['len'] = dataframe['excerpt'].apply(len)\n    dataframe['word7'] = dataframe['excerpt'].apply(lambda t: long_words(t, 7))\n    dataframe['word10'] = dataframe['excerpt'].apply(lambda t: long_words(t, 10))\n    dataframe['word13'] = dataframe['excerpt'].apply(lambda t: long_words(t, 13))\n    dataframe['max_word_len'] = [max_word_len(dataframe['excerpt'][i]) for i in range(0,dataframe['excerpt'].size)]\n    dataframe['punct_count'] = [punct_count(dataframe['excerpt'][i]) for i in range(0, dataframe['excerpt'].size)]\n    #dataframe['sentence_count'] = [textstat.sentence_count(dataframe['excerpt'][i]) for i in range(0,dataframe['excerpt'].size)]\n    dataframe['avg_sen_len'] = [avg_sen_len(dataframe['excerpt'][i]) for i in range(0,dataframe['excerpt'].size)]\n    dataframe['min_sen_len'] = [min_sen_len(dataframe['excerpt'][i]) for i in range(0,dataframe['excerpt'].size)]\n    dataframe['max_sen_len'] = [max_sen_len(dataframe['excerpt'][i]) for i in range(0,dataframe['excerpt'].size)]\n    dataframe['avg_word_len'] = [avg_word_len(dataframe['excerpt'][i]) for i in range(0,dataframe['excerpt'].size)]\n    #dataframe['syllabe_num'] = [textstat.syllable_count(dataframe['excerpt'][i]) for i in range(0,dataframe['excerpt'].size)]\n    #dataframe['lexicon_count'] = [textstat.lexicon_count(dataframe['excerpt'][i]) for i in range(0,dataframe['excerpt'].size)]\n    #dataframe['difficult_words'] = [textstat.difficult_words(dataframe['excerpt'][i]) for i in range(0,dataframe['excerpt'].size)]\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:23:14.882018Z","iopub.execute_input":"2021-05-23T09:23:14.882477Z","iopub.status.idle":"2021-05-23T09:23:14.923137Z","shell.execute_reply.started":"2021-05-23T09:23:14.882437Z","shell.execute_reply":"2021-05-23T09:23:14.922304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_statistical_feature(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:23:15.714079Z","iopub.execute_input":"2021-05-23T09:23:15.714603Z","iopub.status.idle":"2021-05-23T09:23:30.858535Z","shell.execute_reply.started":"2021-05-23T09:23:15.714569Z","shell.execute_reply":"2021-05-23T09:23:30.85735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def readability_label(target):#these parameters allow an almost even distribution among the classes\n    if target >= -0.5:\n        return 3\n    elif target >= -1.5:\n        return 2\n    else:\n        return 1\n\ntrain_df['readability_label'] = [readability_label(train_df['target'][i]) for i in range(0,train_df['excerpt'].size)]  #useful for plotting","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:23:30.860066Z","iopub.execute_input":"2021-05-23T09:23:30.860368Z","iopub.status.idle":"2021-05-23T09:23:30.893821Z","shell.execute_reply.started":"2021-05-23T09:23:30.860339Z","shell.execute_reply":"2021-05-23T09:23:30.892974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['readability_label'].hist()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:23:55.799249Z","iopub.execute_input":"2021-05-23T09:23:55.799757Z","iopub.status.idle":"2021-05-23T09:23:56.007005Z","shell.execute_reply.started":"2021-05-23T09:23:55.799722Z","shell.execute_reply":"2021-05-23T09:23:56.005855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_vars = ['count','len','word7','word10','word13',\n                 'avg_sen_len','avg_word_len','punct_count','max_sen_len','min_sen_len','max_word_len']","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:23:57.404294Z","iopub.execute_input":"2021-05-23T09:23:57.404678Z","iopub.status.idle":"2021-05-23T09:23:57.409134Z","shell.execute_reply.started":"2021-05-23T09:23:57.404648Z","shell.execute_reply":"2021-05-23T09:23:57.408275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:23:58.735445Z","iopub.execute_input":"2021-05-23T09:23:58.735799Z","iopub.status.idle":"2021-05-23T09:23:58.804393Z","shell.execute_reply.started":"2021-05-23T09:23:58.735763Z","shell.execute_reply":"2021-05-23T09:23:58.80333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:23:59.666844Z","iopub.execute_input":"2021-05-23T09:23:59.667193Z","iopub.status.idle":"2021-05-23T09:23:59.698467Z","shell.execute_reply.started":"2021-05-23T09:23:59.667165Z","shell.execute_reply":"2021-05-23T09:23:59.697285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['excerpt'][0])\nprint(train_df['target'][0])\nprint(train_df['standard_error'][0])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:24:00.671103Z","iopub.execute_input":"2021-05-23T09:24:00.671701Z","iopub.status.idle":"2021-05-23T09:24:00.679996Z","shell.execute_reply.started":"2021-05-23T09:24:00.671662Z","shell.execute_reply":"2021-05-23T09:24:00.679014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data visualization","metadata":{}},{"cell_type":"code","source":"#plt.scatter(train_df['target'],train_df['standard_error'])\n\nx = train_df['target']\ny = train_df['standard_error']\n\n# Calculate the point density\nxy = np.vstack([x,y])\nz = gaussian_kde(xy)(xy)\n\n# Sort the points by density, so that the densest points are plotted last\nidx = z.argsort()\nx, y, z = x[idx], y[idx], z[idx]\n\nfig, ax = plt.subplots()\nax.scatter(x, y, c=z, s=0.5)\nplt.xlabel('target')\nplt.ylabel('standard error')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:24:02.763321Z","iopub.execute_input":"2021-05-23T09:24:02.763712Z","iopub.status.idle":"2021-05-23T09:24:03.135271Z","shell.execute_reply.started":"2021-05-23T09:24:02.763672Z","shell.execute_reply":"2021-05-23T09:24:03.134092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We wan to eliminate the point with target and standard error equal to 0 in order to better see the plot","metadata":{}},{"cell_type":"code","source":"train_df.drop(columns = ['url_legal','license','id'], inplace = True)\ntrain_df = train_df[train_df['standard_error'] != 0.].reset_index()\ntrain_df.drop(columns = ['index'], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:24:05.379477Z","iopub.execute_input":"2021-05-23T09:24:05.379992Z","iopub.status.idle":"2021-05-23T09:24:05.412621Z","shell.execute_reply.started":"2021-05-23T09:24:05.379956Z","shell.execute_reply":"2021-05-23T09:24:05.41145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.scatter(train_df['target'],train_df['standard_error'])\n\nx = train_df['target']\ny = train_df['standard_error']\n\n# Calculate the point density\nxy = np.vstack([x,y])\nz = gaussian_kde(xy)(xy)\n\n# Sort the points by density, so that the densest points are plotted last\nidx = z.argsort()\nx, y, z = x[idx], y[idx], z[idx]\n\nfig, ax = plt.subplots()\nax.scatter(x, y, c=z, s=0.5)\nplt.xlabel('target')\nplt.ylabel('standard error')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:24:06.356816Z","iopub.execute_input":"2021-05-23T09:24:06.35717Z","iopub.status.idle":"2021-05-23T09:24:06.706306Z","shell.execute_reply.started":"2021-05-23T09:24:06.35714Z","shell.execute_reply":"2021-05-23T09:24:06.705446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = 1\n\ndef show_word_cloud(corpus):\n    global fig\n    wc = WordCloud(stopwords=STOPWORDS, width=1000, height=600, max_words=150)\n    wc.generate(' '.join(corpus['normalized']))\n    plt.figure(fig)\n    fig += 1\n    plt.imshow(wc, interpolation='bilinear')\n\nshow_word_cloud(train_df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:24:08.514835Z","iopub.execute_input":"2021-05-23T09:24:08.515325Z","iopub.status.idle":"2021-05-23T09:24:13.169702Z","shell.execute_reply.started":"2021-05-23T09:24:08.515293Z","shell.execute_reply":"2021-05-23T09:24:13.168794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot the scatter plot between each pair of variables. This allow us to see if there is some evident correlation between the variables we extracted.\n\nP.S. Plotting this could be a bit slow","metadata":{}},{"cell_type":"code","source":"datasScatter = sns.pairplot(train_df, hue = 'readability_label', plot_kws={'alpha': 0.2}, \n                            corner = True, diag_kind=\"kde\", palette = 'cividis')\n#datasScatter.map_lower(sns.kdeplot, levels=4, color=\".2\")\n#datasScatter.savefig(\"Datas_scatter.png\", facecolor = 'white')","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:24:16.639382Z","iopub.execute_input":"2021-05-23T09:24:16.63996Z","iopub.status.idle":"2021-05-23T09:24:57.049083Z","shell.execute_reply.started":"2021-05-23T09:24:16.639922Z","shell.execute_reply":"2021-05-23T09:24:57.04827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBOOST regressor model","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(train_df[training_vars], train_df['target'], test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:25:12.336633Z","iopub.execute_input":"2021-05-23T09:25:12.336983Z","iopub.status.idle":"2021-05-23T09:25:12.346454Z","shell.execute_reply.started":"2021-05-23T09:25:12.336952Z","shell.execute_reply":"2021-05-23T09:25:12.345275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.5, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 100, verbosity = 1, random_state = 42)\n\nxg_reg.fit(x_train,y_train)\n\npreds = xg_reg.predict(x_test)\n\nrmse = np.sqrt(mse(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))         #about 0.82","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:25:13.679753Z","iopub.execute_input":"2021-05-23T09:25:13.680112Z","iopub.status.idle":"2021-05-23T09:25:13.901604Z","shell.execute_reply.started":"2021-05-23T09:25:13.680082Z","shell.execute_reply":"2021-05-23T09:25:13.900698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xg_reg.fit(train_df[training_vars], train_df['target'])\n\nextract_statistical_feature(test_df)\ntest_pred = xg_reg.predict(test_df[training_vars])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:25:17.099961Z","iopub.execute_input":"2021-05-23T09:25:17.100644Z","iopub.status.idle":"2021-05-23T09:25:17.344619Z","shell.execute_reply.started":"2021-05-23T09:25:17.100606Z","shell.execute_reply":"2021-05-23T09:25:17.343695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission file 📝","metadata":{}},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:25:26.24084Z","iopub.execute_input":"2021-05-23T09:25:26.241259Z","iopub.status.idle":"2021-05-23T09:25:26.260368Z","shell.execute_reply.started":"2021-05-23T09:25:26.241202Z","shell.execute_reply":"2021-05-23T09:25:26.259347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}