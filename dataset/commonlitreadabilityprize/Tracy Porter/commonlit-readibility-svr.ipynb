{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this competition, youâ€™ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:27:59.5292Z","iopub.execute_input":"2021-06-23T15:27:59.529554Z","iopub.status.idle":"2021-06-23T15:27:59.533329Z","shell.execute_reply.started":"2021-06-23T15:27:59.529525Z","shell.execute_reply":"2021-06-23T15:27:59.532444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:27:59.535192Z","iopub.execute_input":"2021-06-23T15:27:59.535654Z","iopub.status.idle":"2021-06-23T15:27:59.552825Z","shell.execute_reply.started":"2021-06-23T15:27:59.535585Z","shell.execute_reply":"2021-06-23T15:27:59.551882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read files","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:27:59.554528Z","iopub.execute_input":"2021-06-23T15:27:59.554832Z","iopub.status.idle":"2021-06-23T15:27:59.60951Z","shell.execute_reply.started":"2021-06-23T15:27:59.554803Z","shell.execute_reply":"2021-06-23T15:27:59.608459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\ntest","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:27:59.610919Z","iopub.execute_input":"2021-06-23T15:27:59.611248Z","iopub.status.idle":"2021-06-23T15:27:59.629452Z","shell.execute_reply.started":"2021-06-23T15:27:59.611218Z","shell.execute_reply":"2021-06-23T15:27:59.628282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:27:59.630806Z","iopub.execute_input":"2021-06-23T15:27:59.631119Z","iopub.status.idle":"2021-06-23T15:27:59.645008Z","shell.execute_reply.started":"2021-06-23T15:27:59.631089Z","shell.execute_reply":"2021-06-23T15:27:59.644065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:27:59.64744Z","iopub.execute_input":"2021-06-23T15:27:59.647754Z","iopub.status.idle":"2021-06-23T15:27:59.653345Z","shell.execute_reply.started":"2021-06-23T15:27:59.647722Z","shell.execute_reply":"2021-06-23T15:27:59.652575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyse target","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nsns.displot(train['target']);","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-23T15:27:59.654544Z","iopub.execute_input":"2021-06-23T15:27:59.654811Z","iopub.status.idle":"2021-06-23T15:27:59.997531Z","shell.execute_reply.started":"2021-06-23T15:27:59.654786Z","shell.execute_reply":"2021-06-23T15:27:59.996543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyse standard error","metadata":{}},{"cell_type":"code","source":"sns.displot(train['standard_error']);","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:27:59.998696Z","iopub.execute_input":"2021-06-23T15:27:59.99898Z","iopub.status.idle":"2021-06-23T15:28:00.562318Z","shell.execute_reply.started":"2021-06-23T15:27:59.998942Z","shell.execute_reply":"2021-06-23T15:28:00.561322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create new column of processed text","metadata":{}},{"cell_type":"code","source":"train['processed_text'] = train['excerpt']\ntest['processed_text'] = test['excerpt']","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:00.565963Z","iopub.execute_input":"2021-06-23T15:28:00.566378Z","iopub.status.idle":"2021-06-23T15:28:00.572954Z","shell.execute_reply.started":"2021-06-23T15:28:00.566331Z","shell.execute_reply":"2021-06-23T15:28:00.571965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apostrophe dictionary","metadata":{}},{"cell_type":"code","source":"# Apostrophe Dictionary\napostrophe_dict = {\n\"ain't\": \"am not / are not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is\",\n\"i'd\": \"I had / I would\",\n\"i'd've\": \"I would have\",\n\"i'll\": \"I shall / I will\",\n\"i'll've\": \"I shall have / I will have\",\n\"i'm\": \"I am\",\n\"i've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\napostrophe_dict\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:00.574667Z","iopub.execute_input":"2021-06-23T15:28:00.575259Z","iopub.status.idle":"2021-06-23T15:28:00.603586Z","shell.execute_reply.started":"2021-06-23T15:28:00.575217Z","shell.execute_reply":"2021-06-23T15:28:00.602286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define lookup dict","metadata":{}},{"cell_type":"code","source":"def lookup_dict(text, dictionary):\n    for word in text.split():\n        if word.lower() in dictionary:\n            if word.lower() in text.split():\n                text = text.replace(word, dictionary[word.lower()])\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:00.605135Z","iopub.execute_input":"2021-06-23T15:28:00.605527Z","iopub.status.idle":"2021-06-23T15:28:00.620393Z","shell.execute_reply.started":"2021-06-23T15:28:00.605488Z","shell.execute_reply":"2021-06-23T15:28:00.619479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply apostrophe to processed_text","metadata":{}},{"cell_type":"code","source":"train['processed_text'] = train['processed_text'].apply(lambda x: lookup_dict(x,apostrophe_dict))\ntest['processed_text'] = test['processed_text'].apply(lambda x: lookup_dict(x,apostrophe_dict))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:00.621589Z","iopub.execute_input":"2021-06-23T15:28:00.62211Z","iopub.status.idle":"2021-06-23T15:28:00.755268Z","shell.execute_reply.started":"2021-06-23T15:28:00.622076Z","shell.execute_reply":"2021-06-23T15:28:00.754454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stem the words in the processed text","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nwords = stopwords.words(\"english\")\n\ntrain['processed_text'] = train['excerpt'].apply(lambda x: \" \".join([stemmer.stem(i) \nfor i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n\ntest['processed_text'] = test['excerpt'].apply(lambda x: \" \".join([stemmer.stem(i) \nfor i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:00.756407Z","iopub.execute_input":"2021-06-23T15:28:00.756864Z","iopub.status.idle":"2021-06-23T15:28:09.85155Z","shell.execute_reply.started":"2021-06-23T15:28:00.756834Z","shell.execute_reply":"2021-06-23T15:28:09.850417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove special characters","metadata":{}},{"cell_type":"code","source":"import string\n\n#make all words lower case\ntrain['processed_text'] = train['processed_text'].str.lower()\ntest['processed_text'] = test['processed_text'].str.lower()\n\n#Remove punctuation\ntable = str.maketrans('', '', string.punctuation)\ntrain['processed_text'] = [train['processed_text'][row].translate(table) for row in range(len(train['processed_text']))]\ntest['processed_text'] = [test['processed_text'][row].translate(table) for row in range(len(test['processed_text']))]\n\n# remove hash tags\ntrain['processed_text'] = train['processed_text'].str.replace(\"#\", \" \")\ntest['processed_text'] = test['processed_text'].str.replace(\"#\", \" \")\n\n#remove words less than 1 character\ntrain['processed_text'] = train['processed_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\ntest['processed_text'] = test['processed_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:09.854264Z","iopub.execute_input":"2021-06-23T15:28:09.854614Z","iopub.status.idle":"2021-06-23T15:28:09.968894Z","shell.execute_reply.started":"2021-06-23T15:28:09.85458Z","shell.execute_reply":"2021-06-23T15:28:09.967934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove rare words","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom itertools import chain\n\n# split words into lists\nv = train['processed_text'].str.split().tolist() \n# compute global word frequency\nc = Counter(chain.from_iterable(v))\n# filter, join, and re-assign\ntrain['processed_text'] = [' '.join([j for j in i if c[j] > 1]) for i in v]\n\n# split words into lists\nv = test['processed_text'].str.split().tolist() \n# compute global word frequency\nc = Counter(chain.from_iterable(v))\n# filter, join, and re-assign\ntest['processed_text'] = [' '.join([j for j in i if c[j] > 1]) for i in v]\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:09.97066Z","iopub.execute_input":"2021-06-23T15:28:09.971072Z","iopub.status.idle":"2021-06-23T15:28:10.132203Z","shell.execute_reply.started":"2021-06-23T15:28:09.971028Z","shell.execute_reply":"2021-06-23T15:28:10.131113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define X, y and X_test","metadata":{}},{"cell_type":"code","source":"y=train[['target','standard_error']]\nX=train['processed_text']\nX_test=test['processed_text']","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:10.133777Z","iopub.execute_input":"2021-06-23T15:28:10.134088Z","iopub.status.idle":"2021-06-23T15:28:10.141432Z","shell.execute_reply.started":"2021-06-23T15:28:10.134059Z","shell.execute_reply":"2021-06-23T15:28:10.140083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split X up for training and testing","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=42, shuffle=True)\nX_train.shape, X_val.shape, y_train.shape,y_val.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:10.143236Z","iopub.execute_input":"2021-06-23T15:28:10.143749Z","iopub.status.idle":"2021-06-23T15:28:10.169347Z","shell.execute_reply.started":"2021-06-23T15:28:10.143701Z","shell.execute_reply":"2021-06-23T15:28:10.168452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert text to word vectoriser","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\ntrain_tfIdf = vectorizer_tfidf.fit_transform(X_train.values.astype('U'))\nval_tfIdf = vectorizer_tfidf.transform(X_val.values.astype('U'))\nX_test_tfIdf = vectorizer_tfidf.transform(X_test.values.astype('U'))\nprint(vectorizer_tfidf.get_feature_names()[:5])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:10.170526Z","iopub.execute_input":"2021-06-23T15:28:10.170954Z","iopub.status.idle":"2021-06-23T15:28:10.553183Z","shell.execute_reply.started":"2021-06-23T15:28:10.170915Z","shell.execute_reply":"2021-06-23T15:28:10.552023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tfIdf.shape,  y_train.shape, val_tfIdf.shape, y_val.shape,  X_test_tfIdf.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:10.554297Z","iopub.execute_input":"2021-06-23T15:28:10.554628Z","iopub.status.idle":"2021-06-23T15:28:10.561256Z","shell.execute_reply.started":"2021-06-23T15:28:10.554597Z","shell.execute_reply":"2021-06-23T15:28:10.560066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define model","metadata":{}},{"cell_type":"code","source":"from sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.svm import LinearSVR\n\n\nmodel = MultiOutputRegressor(LinearSVR(random_state=1)).fit(train_tfIdf.todense(), y_train)\nprint(model.score(train_tfIdf.todense(), y_train))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:10.56247Z","iopub.execute_input":"2021-06-23T15:28:10.562842Z","iopub.status.idle":"2021-06-23T15:28:11.525841Z","shell.execute_reply.started":"2021-06-23T15:28:10.562811Z","shell.execute_reply":"2021-06-23T15:28:11.524544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict on validation set","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(val_tfIdf.todense())\nmodel.score(val_tfIdf.todense(), y_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:11.527824Z","iopub.execute_input":"2021-06-23T15:28:11.528407Z","iopub.status.idle":"2021-06-23T15:28:11.569195Z","shell.execute_reply.started":"2021-06-23T15:28:11.528355Z","shell.execute_reply":"2021-06-23T15:28:11.568004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict on test set","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(X_test_tfIdf)\npredictions.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:11.570988Z","iopub.execute_input":"2021-06-23T15:28:11.571839Z","iopub.status.idle":"2021-06-23T15:28:11.580963Z","shell.execute_reply.started":"2021-06-23T15:28:11.571782Z","shell.execute_reply":"2021-06-23T15:28:11.579836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare submission","metadata":{}},{"cell_type":"code","source":"submission.target = predictions[:, 0]","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:11.582938Z","iopub.execute_input":"2021-06-23T15:28:11.583822Z","iopub.status.idle":"2021-06-23T15:28:11.593441Z","shell.execute_reply.started":"2021-06-23T15:28:11.583765Z","shell.execute_reply":"2021-06-23T15:28:11.592133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubmission = pd.read_csv(\"submission.csv\")\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:28:11.59538Z","iopub.execute_input":"2021-06-23T15:28:11.596289Z","iopub.status.idle":"2021-06-23T15:28:11.630522Z","shell.execute_reply.started":"2021-06-23T15:28:11.596235Z","shell.execute_reply":"2021-06-23T15:28:11.62924Z"},"trusted":true},"execution_count":null,"outputs":[]}]}