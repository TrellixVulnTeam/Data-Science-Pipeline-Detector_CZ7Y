{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport regex as re\nfrom functools import lru_cache\nimport numpy as np\n# The GPT-2 implementation uses TF 1\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:14:26.214248Z","iopub.execute_input":"2021-06-22T03:14:26.214823Z","iopub.status.idle":"2021-06-22T03:14:32.271829Z","shell.execute_reply.started":"2021-06-22T03:14:26.214725Z","shell.execute_reply":"2021-06-22T03:14:32.270624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To see available data\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:14:32.273619Z","iopub.execute_input":"2021-06-22T03:14:32.274022Z","iopub.status.idle":"2021-06-22T03:14:32.330237Z","shell.execute_reply.started":"2021-06-22T03:14:32.27398Z","shell.execute_reply":"2021-06-22T03:14:32.328216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From https://github.com/modelblocks/modelblocks-release/blob/master/resource-gpt2/scripts/encoder.py","metadata":{}},{"cell_type":"code","source":"\"\"\"Byte pair encoding utilities\"\"\"\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors='replace'):\n        self.encoder = encoder\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = ''.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n        return text\n\ndef get_encoder(model_name, models_dir):\n    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n        encoder = json.load(f)\n    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n    return Encoder(\n        encoder=encoder,\n        bpe_merges=bpe_merges,\n    )","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:14:32.332388Z","iopub.execute_input":"2021-06-22T03:14:32.33279Z","iopub.status.idle":"2021-06-22T03:14:32.358284Z","shell.execute_reply.started":"2021-06-22T03:14:32.332747Z","shell.execute_reply":"2021-06-22T03:14:32.357068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From https://github.com/modelblocks/modelblocks-release/blob/master/resource-gpt2/scripts/model.py","metadata":{}},{"cell_type":"code","source":"def default_hparams_dict():\n    return {\n        \"n_vocab\": 0,\n        \"n_ctx\": 1024,\n        \"n_embd\": 768,\n        \"n_head\": 12,\n        \"n_layer\": 12,        \n    }\n#    return HParams(\n#        n_vocab=0,\n#        n_ctx=1024,\n#        n_embd=768,\n#        n_head=12,\n#        n_layer=12,\n#    )\n\ndef shape_list(x):\n    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n    static = x.shape.as_list()\n    dynamic = tf.shape(x)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n\ndef softmax(x, axis=-1):\n    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n    ex = tf.exp(x)\n    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n\ndef gelu(x):\n    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n\ndef norm(x, scope, *, axis=-1, epsilon=1e-5):\n    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n    with tf.variable_scope(scope):\n        n_state = x.shape[-1].value\n        g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))\n        b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))\n        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n        x = (x - u) * tf.rsqrt(s + epsilon)\n        x = x*g + b\n        return x\n\ndef split_states(x, n):\n    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n    *start, m = shape_list(x)\n    return tf.reshape(x, start + [n, m//n])\n\ndef merge_states(x):\n    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n    *start, a, b = shape_list(x)\n    return tf.reshape(x, start + [a*b])\n\ndef conv1d(x, scope, nf, *, w_init_stdev=0.02):\n    with tf.variable_scope(scope):\n        *start, nx = shape_list(x)\n        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n        return c\n\ndef attention_mask(nd, ns, *, dtype):\n    \"\"\"1's in the lower triangle, counting from the lower right corner.\n\n    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n    \"\"\"\n    i = tf.range(nd)[:,None]\n    j = tf.range(ns)\n    m = i >= j - ns + nd\n    return tf.cast(m, dtype)\n\n\ndef attn(x, scope, n_state, *, past, hparams_dict):\n    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n    assert n_state % hparams_dict[\"n_head\"] == 0\n    if past is not None:\n        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n\n    def split_heads(x):\n        # From [batch, sequence, features] to [batch, heads, sequence, features]\n        return tf.transpose(split_states(x, hparams_dict[\"n_head\"]), [0, 2, 1, 3])\n\n    def merge_heads(x):\n        # Reverse of split_heads\n        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n\n    def mask_attn_weights(w):\n        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n        _, _, nd, ns = shape_list(w)\n        b = attention_mask(nd, ns, dtype=w.dtype)\n        b = tf.reshape(b, [1, 1, nd, ns])\n        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n        return w\n\n    def multihead_attn(q, k, v):\n        # q, k, v have shape [batch, heads, sequence, features]\n        w = tf.matmul(q, k, transpose_b=True)\n        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n\n        w = mask_attn_weights(w)\n        w = softmax(w)\n        a = tf.matmul(w, v)\n        return a\n\n    with tf.variable_scope(scope):\n        c = conv1d(x, 'c_attn', n_state*3)\n        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n        present = tf.stack([k, v], axis=1)\n        if past is not None:\n            pk, pv = tf.unstack(past, axis=1)\n            k = tf.concat([pk, k], axis=-2)\n            v = tf.concat([pv, v], axis=-2)\n        a = multihead_attn(q, k, v)\n        a = merge_heads(a)\n        a = conv1d(a, 'c_proj', n_state)\n        return a, present\n\n\ndef mlp(x, scope, n_state, *, hparams_dict):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        h = gelu(conv1d(x, 'c_fc', n_state))\n        h2 = conv1d(h, 'c_proj', nx)\n        return h2\n\n\ndef block(x, scope, *, past, hparams_dict):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams_dict=hparams_dict)\n        x = x + a\n        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams_dict=hparams_dict)\n        x = x + m\n        return x, present\n\ndef past_shape(*, hparams_dict, batch_size=None, sequence=None):\n    return [batch_size, hparams_dict[\"n_layer\"], 2, hparams_dict[\"n_head\"], sequence, hparams_dict[\"n_embd\"] // hparams_dict[\"n_head\"]]\n\ndef expand_tile(value, size):\n    \"\"\"Add a new axis of given size.\"\"\"\n    value = tf.convert_to_tensor(value, name='value')\n    ndims = value.shape.ndims\n    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n\ndef positions_for(tokens, past_length):\n    batch_size = tf.shape(tokens)[0]\n    nsteps = tf.shape(tokens)[1]\n    return expand_tile(past_length + tf.range(nsteps), batch_size)\n\n\ndef model(hparams_dict, X, past=None, scope='model', reuse=False):\n    with tf.variable_scope(scope, reuse=reuse):\n        results = {}\n        batch, sequence = shape_list(X)\n\n        wpe = tf.get_variable('wpe', [hparams_dict[\"n_ctx\"], hparams_dict[\"n_embd\"]],\n                             initializer=tf.random_normal_initializer(stddev=0.01))\n        wte = tf.get_variable('wte', [hparams_dict[\"n_vocab\"], hparams_dict[\"n_embd\"]],\n                             initializer=tf.random_normal_initializer(stddev=0.02))\n        past_length = 0 if past is None else tf.shape(past)[-2]\n        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n\n        # Transformer\n        presents = []\n        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams_dict[\"n_layer\"]\n        assert len(pasts) == hparams_dict[\"n_layer\"]\n        for layer, past in enumerate(pasts):\n            h, present = block(h, 'h%d' % layer, past=past, hparams_dict=hparams_dict)\n            presents.append(present)\n        results['present'] = tf.stack(presents, axis=1)\n        h = norm(h, 'ln_f')\n\n        # Language model loss.  Do tokens <n predict token n?\n        h_flat = tf.reshape(h, [batch*sequence, hparams_dict[\"n_embd\"]])\n        logits = tf.matmul(h_flat, wte, transpose_b=True)\n        logits = tf.reshape(logits, [batch, sequence, hparams_dict[\"n_vocab\"]])\n        results['logits'] = logits\n        return results","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:14:32.360263Z","iopub.execute_input":"2021-06-22T03:14:32.360557Z","iopub.status.idle":"2021-06-22T03:14:32.405285Z","shell.execute_reply.started":"2021-06-22T03:14:32.360529Z","shell.execute_reply":"2021-06-22T03:14:32.404067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From https://raw.githubusercontent.com/modelblocks/modelblocks-release/master/resource-gpt2/scripts/surprisal.py","metadata":{}},{"cell_type":"code","source":"# maybe needs to be None?\nBATCH_SIZE = 1\n\ndef get_per_subword_surprisal(*, corpus, hparams_dict, encoder):\n    start_token = encoder.encoder['<|endoftext|>']\n    context = tf.fill([BATCH_SIZE, 1], start_token)\n\n    def step(hparams_dict, tokens, past=None):\n        lm_output = model(hparams_dict=hparams_dict, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n        logits = lm_output['logits'][:, :, :hparams_dict[\"n_vocab\"]]\n        presents = lm_output['present']\n        presents.set_shape(past_shape(hparams_dict=hparams_dict, batch_size=BATCH_SIZE))\n        return {\n            'logits': logits,\n            'presents': presents,\n        }\n\n    with tf.name_scope('get_per_word_surprisal'):\n        # word is a list of integers (encoded chunks)\n        def body(corpus, i, past, prev, surprisals):\n            # chunk should be a scalar here\n            chunk = corpus[i]\n            next_outputs = step(hparams_dict, prev, past=past)\n            # dimension is batch_size x vocab_size\n            logits = next_outputs['logits'][:, -1, :]\n            softmax = tf.nn.softmax(logits)\n            surp = tf.math.scalar_mul(-1, tf.math.log(softmax[0, chunk]))\n            # TODO assuming here that batch size is 1.\n            # find a better solultion\n            return [\n                corpus,\n                tf.add(i, 1),\n                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n                tf.reshape(chunk, [1, 1]),\n                tf.concat([surprisals, tf.reshape(surp, [1, 1])], axis=1),\n            ]\n\n        corpus = tf.constant(corpus)\n        i = tf.constant(0)\n        corpus, i, past, prev, surprisals = body(corpus, i, None, context, tf.constant([[]]))\n\n        corpus_length = corpus.shape[0].value\n\n        def cond(corpus, i, past, prev, surprisals):\n            return tf.less(i, corpus_length)\n\n        _, _, _, _, surprisals = tf.while_loop(\n            cond=cond, body=body,\n            loop_vars=[corpus, i, past, prev, surprisals],\n            shape_invariants=[\n                corpus.get_shape(),\n                i.get_shape(),\n                tf.TensorShape(past_shape(hparams_dict=hparams_dict, batch_size=BATCH_SIZE)),\n                tf.TensorShape([BATCH_SIZE, None]),\n                tf.TensorShape([BATCH_SIZE, None]),\n            ],\n            back_prop=False,\n        )\n\n        return surprisals","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:14:32.407132Z","iopub.execute_input":"2021-06-22T03:14:32.407753Z","iopub.status.idle":"2021-06-22T03:14:32.425966Z","shell.execute_reply.started":"2021-06-22T03:14:32.407708Z","shell.execute_reply":"2021-06-22T03:14:32.424745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From https://github.com/modelblocks/modelblocks-release/blob/master/resource-gpt2/scripts/per_word_surprisal.py","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 1\nDELIMITER = '!ARTICLE'\n\n\ndef get_delimited_texts(lineitems_fn):\n    '''Returns the texts in the delim.linetimes file. Each text\n    is preceded by a line with a special delimiter token. Output is a\n    list of strings, where each string is one complete text'''\n    texts = list()\n\n    f = open(lineitems_fn)\n    firstline = f.readline()\n    assert firstline.strip() == DELIMITER\n\n    curr_text = list()\n    for l in f:\n        sentence = l.strip()\n        if sentence == DELIMITER:\n            texts.append(' '.join(curr_text))\n            curr_text = list()\n        else:\n            curr_text.append(sentence)\n    # don't forget the last text\n    texts.append(' '.join(curr_text))\n        \n    return texts\n\n\ndef get_subword_windows(encoded_text, context_size):\n    '''Splits encoded_text, a list of subword IDs, into a list of\n    overlapping windows of subwords, each of length <= context_size. These\n    windows are what get fed into GPT-2.'''\n\n    # Need an even context_size for splitting the subwords into windows\n    if context_size % 2 == 1:\n        raise ValueError(\"context_size must be an even number\")\n    windows = list()\n    while len(encoded_text) > context_size:\n        windows.append(encoded_text[:context_size])\n        # size of overlap between windows is context_size/2\n        encoded_text = encoded_text[int(context_size/2):]\n    windows.append(encoded_text)\n    return windows\n\n\ndef combine_window_surprisals(window_surprisals, context_size):\n    '''Given the surprisal measurements for a list of overlapping windows,\n    returns the recombined list of subword surprisals.'''\n    # use all subword surprisals from the first window. For subsequent\n    # windows, the first context_size/2 terms will overlap with the\n    # previous window, so throw them out\n    subword_surprisals = window_surprisals[0]\n    for w in window_surprisals[1:]:\n        subword_surprisals.extend(w[int(context_size/2):])\n    return subword_surprisals\n\n\ndef get_per_char_surprisal(subwords, subword_surps):\n    # subword_surps[i] is the surprisal for subwords[i]\n    assert len(subwords) == len(subword_surps)\n    chars = list()\n    char_surps = list()\n    for sub, surp in zip(subwords, subword_surps):\n        per_char_surp = surp/len(sub)\n        for char in sub:\n            chars.append(char)\n            char_surps.append(per_char_surp)\n    assert len(chars) == len(char_surps)\n    return chars, char_surps\n\n\ndef roll_subword_surprisal(subwords, subword_surps, words, line_idx):\n    chars, char_surps = get_per_char_surprisal(subwords, subword_surps)\n    word_surps = list()\n    char_index = 0\n    curr_char = chars[char_index]\n    curr_surp = char_surps[char_index]\n    for w in words:\n        word_surp = 0\n        # roll surprisals from spaces before the word into the word\n        while curr_char.isspace():\n            word_surp += curr_surp\n            char_index += 1\n            curr_char = chars[char_index]\n            curr_surp = char_surps[char_index]\n        for c in w:\n            assert c == curr_char\n#             if c != curr_char:\n#                 print('Failed text line_idx', line_idx)\n#                 continue\n            word_surp += curr_surp\n            char_index += 1\n            # at the very end of the string we can't advance\n            if char_index < len(chars):\n                curr_char = chars[char_index]\n                curr_surp = char_surps[char_index]\n        word_surps.append(word_surp)\n    return word_surps\n\n\ndef get_surprisal(\n    texts, \n    model_name='124M',\n    models_dir='models',\n    context_size=1024\n):\n    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n    hparams_dict = default_hparams_dict()\n    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n        override_params = json.load(f)\n    hparams_dict.update(override_params)\n\n    all_words = list()\n    all_word_surprisals = list()\n    for line_idx, text in enumerate(texts):\n        print('line_idx', line_idx)\n        \n        # get sequence of subword encodings\n        enc = get_encoder(model_name, models_dir)\n        enc_text = enc.encode(text)\n        subwords = [enc.decode([x]) for x in enc_text]\n        words = text.split()\n    \n        # split subword sequence into windows of size <= context_size\n        windows = get_subword_windows(enc_text, context_size)\n    \n        # feed each window into GPT-2, get per-subword surprisals\n        window_surprisals = list()\n        for window in windows:\n            with tf.Session(graph=tf.Graph()) as sess:\n        \n                output = get_per_subword_surprisal(\n                    corpus=window, hparams_dict=hparams_dict,\n                    encoder=enc\n                )\n        \n                saver = tf.train.Saver()\n                ckpt = tf.train.latest_checkpoint(\n                           os.path.join(models_dir, model_name))\n                saver.restore(sess, ckpt)\n        \n                # out has dimension batch_size x num_subwords\n                # containing per-subword surprisals\n                assert BATCH_SIZE == 1\n                out = sess.run(output)\n                surps = list(out[0])\n            window_surprisals.append(surps)\n        subword_surprisals = combine_window_surprisals(\n                                 window_surprisals, context_size)\n    \n        # \"roll\" surprisals together to get per-word surprisal\n        word_surprisals = roll_subword_surprisal(\n                              subwords, subword_surprisals, words, line_idx)\n\n        all_words.append(words)\n        all_word_surprisals.append(word_surprisals)\n    \n    #print(\"word gpt2surp\")\n    #for i in range(len(all_words)):\n    #    print('{} {}'.format(all_words[i], all_word_surprisals[i]))\n    return all_word_surprisals, all_words\n\n\ndef per_word_surprisal(\n    lineitems, \n    model_name='124M',\n    models_dir=\"/kaggle/input/gpt2simple-all-models-not-finetuned/models\",\n    context_size=1024\n):\n    \"\"\"\n    Use GPT-2 to calculate per-word surprisal for a provided lineitems file\n\n    Parameters\n    ----------\n    lineitems :\n        path to delimited lineitems file (*.delim.lineitems)\n\n    model_name :\n        which model to use\n\n    models_dir :\n        path to parent folder containing model subfolders\n        (i.e. contains the <model_name> folder)\n\n    context_size :\n        the maximum context size allowed by the model (n_ctx). \n        If the length of the input text exceeds context_size, the text is \n        split into overlapping windows to calculate surprisal\n    \"\"\"\n\n    # prepare each text as a single string to feed into GPT-2\n    texts = get_delimited_texts(lineitems)\n\n    return get_surprisal(\n        texts=texts, \n        model_name='124M',\n        models_dir=\"/kaggle/input/gpt2simple-all-models-not-finetuned/models\",\n        context_size=1024)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:14:32.427378Z","iopub.execute_input":"2021-06-22T03:14:32.427764Z","iopub.status.idle":"2021-06-22T03:14:32.454504Z","shell.execute_reply.started":"2021-06-22T03:14:32.427733Z","shell.execute_reply":"2021-06-22T03:14:32.453348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full natural stories dataset (~10K words) -- takes 30-60 minutes\n#lineitems = \"/kaggle/input/d/ceclark/gpt2surp/naturalstories.delim.lineitems\"\n# Small example (1 sentence) -- takes less than a minute\nmodels_dir = \"/kaggle/input/gpt2simple-all-models-not-finetuned/models\"\n# available sizes \nmodel_name = \"124M\"","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:14:32.456052Z","iopub.execute_input":"2021-06-22T03:14:32.456517Z","iopub.status.idle":"2021-06-22T03:14:32.478561Z","shell.execute_reply.started":"2021-06-22T03:14:32.456474Z","shell.execute_reply":"2021-06-22T03:14:32.47773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lineitems = \"/kaggle/input/d/ceclark/gpt2surp/naturalstories.mini.delim.lineitems\"\n\n# surps, sents = per_word_surprisal(lineitems, model_name, models_dir)\n\n# for i, sent in enumerate(sents):\n#     for j, word in enumerate(sent):\n#         print(word, round(surps[i][j], 4), sep='\\t')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:14:32.481011Z","iopub.execute_input":"2021-06-22T03:14:32.481362Z","iopub.status.idle":"2021-06-22T03:14:32.490938Z","shell.execute_reply.started":"2021-06-22T03:14:32.481331Z","shell.execute_reply":"2021-06-22T03:14:32.489806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# texts = [\n#     'Earth is the planet we live on. It is the third planet from the Sun. It is the only planet known to have life on it. The Earth formed around 4.5 billion years ago. It is one of four rocky planets on the inside of the Solar System. The other three are Mercury, Venus and Mars. The large mass of the Sun makes the Earth move around it, just as the mass of the Earth makes the Moon move around it. The Earth also turns round in space, so different parts face the Sun at different times. The Earth goes around the Sun once (one \"year\") for every 365 times it turns all the way around (one \"day\"). The Moon goes around the Earth about every 27 days. As the Earth goes round the Sun at the same time, the changing light of the Moon takes about 29 days to go from dark to bright to dark again. That is where the idea of \"month\" came from. However, now most months have 30 or 31 days so they fit into one year.'\n# ]\n# surps, text_lines = get_surprisal(texts, \n#     model_name, models_dir)\n\n# print(surps)\n# print(text_lines)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:14:32.494734Z","iopub.execute_input":"2021-06-22T03:14:32.495128Z","iopub.status.idle":"2021-06-22T03:14:32.502798Z","shell.execute_reply.started":"2021-06-22T03:14:32.495088Z","shell.execute_reply":"2021-06-22T03:14:32.501711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\n\nimport unicodedata\n\n# data = [{'name': 'saylı'}, {'name': 'öhdəliyi'}, {'name': 'said—\\n‘I say—about that charm—Jane—come out. We ought to talk about it, a'}]\n# df = pd.DataFrame.from_dict(data, orient='columns')\n# df['name'].apply(lambda val: unicodedata.normalize('NFKD', val).encode('ascii', 'ignore').decode())\n\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv', nrows=1000)\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:14:32.506473Z","iopub.execute_input":"2021-06-22T03:14:32.506762Z","iopub.status.idle":"2021-06-22T03:14:32.548666Z","shell.execute_reply.started":"2021-06-22T03:14:32.506736Z","shell.execute_reply":"2021-06-22T03:14:32.547709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['excerpt'] = train['excerpt'].str.replace('\\n', ' ')\n# train['excerpt'] = train['excerpt'].str.replace('‘', \"'\")\n\ntrain['text'] = train['excerpt'].apply(lambda val: unicodedata.normalize('NFKD', val).encode('ascii', 'ignore').decode())\n\n\n# print(train.iloc[338]['excerpt'])\n\ntrain_texts = train['text'].values.tolist()\n\nsurps, text_lines = get_surprisal(train_texts, model_name, models_dir)\n\ntrain_surps = [np.mean(x) for x in surps]\n\ntrain['surprisal'] = train_surps\n\ntrain.to_csv('train_surp.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.iloc[18]['excerpt']","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:17:12.642988Z","iopub.execute_input":"2021-06-22T03:17:12.643289Z","iopub.status.idle":"2021-06-22T03:17:12.649546Z","shell.execute_reply.started":"2021-06-22T03:17:12.643263Z","shell.execute_reply":"2021-06-22T03:17:12.648532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nsns.jointplot(data=train, x='target', y='surprisal', kind='reg', height=10)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-22T03:17:12.651038Z","iopub.execute_input":"2021-06-22T03:17:12.651529Z","iopub.status.idle":"2021-06-22T03:17:14.227232Z","shell.execute_reply.started":"2021-06-22T03:17:12.651474Z","shell.execute_reply":"2021-06-22T03:17:14.226207Z"},"trusted":true},"execution_count":null,"outputs":[]}]}