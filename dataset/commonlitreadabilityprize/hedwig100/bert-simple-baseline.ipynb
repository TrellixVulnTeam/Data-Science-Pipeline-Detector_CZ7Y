{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT Simple Baseline \n\nthis code is almost same as [this notebook](https://www.kaggle.com/chumajin/pytorch-bert-beginner-s-room). <br>\n\nI haven't tried bert-large-uncased, so I'll try next.\n\n----------------------------------------------------\n\nI tried bert-base-uncased, bert-large-uncased, roberta-base ever. <br>\nRoberta-base performed well. (CV~0.506) <br> \nThere should be more space to archive better score.","metadata":{}},{"cell_type":"code","source":"# Library \n# utils \nimport os,gc,pickle,random\nfrom tqdm import tqdm \nimport numpy as np  \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings \nwarnings.simplefilter(\"ignore\",FutureWarning)\n\n# imgs \nimport cv2 \nimport albumentations as A\n\n# torch \nimport torch \nimport torch.optim as optim \nfrom torch.utils.data import Dataset,DataLoader \nimport torch.nn as nn \nimport torch.nn.functional as F \n\n# other  \nimport transformers \nfrom transformers import get_linear_schedule_with_warmup\n\n\n# Config \nWHERE = \"kaggle\" #  or colab \nif WHERE == \"kaggle\":\n    INPUT_DIR = \"../input/\"\n    OUTPUT_DIR = \"./\"\nelif WHERE == \"colab\":\n    INPUT_DIR = \"../input/\"\n    OUTPUT_DIR = \"./\"\n\nDEBUG = False \nNB = 'public1'\nVERSION = 3   \n\nclass CFG:\n    # utils\n    seed = 42\n    print_freq = 50\n    num_workers = 4 \n    use_amp = False \n\n    # training param\n    folds = [0]\n    n_epoch = 10\n    batch_size = 8   \n    learning_rate = 2e-5 \n\n    optimizer = \"adamw\"\n    weight_decay = 1e-1 \n    betas = (0.9,0.999) \n\n    scheduler = \"linear_warmup\"\n\n    # bert param\n    model_name = \"roberta-large\" # \"bert-large-uncased\" \"bert-base-uncased\"\n    max_sentence = 256  \n\nif DEBUG:\n    CFG.n_epoch = 1\n\n\n# Utils \ndef random_seed(SEED):\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass Loader():\n    def __init__(self):\n        pass \n    \n    def load(self,path):\n        \"\"\"\n        Args:\n            path : from which data shouled be loaded\n        Returns:\n            data : loaded data \n        \"\"\"\n        obj = pickle.load(open(path,\"rb\"))\n        return obj \n    \n    def dump(self,obj,path):\n        \"\"\"\n        Args:\n            obj (object) : object which should be dumped\n            path (str) : to which object should be dumped \n        \"\"\"\n        f = open(path,\"wb\")\n        pickle.dump(obj,f)\n        f.close \n\nclass History():\n    def __init__(self,metric=None,others=None):\n        \"\"\"\n        Default:\n            columns has default value below\n            - epoch \n            - train_loss\n            - valid_loss\n            - train_{metric}\n            _ valid_{metric}\n            \n        Args:\n            metric (int) : metric \n            others (list) : other parameters which is logged \n        \"\"\"\n\n        columns = [\"epoch\",\"train_loss\",\"valid_loss\"]\n        if metric is not None:\n            columns.extend([f\"train_{metric}\",f\"valid_{metric}\"]) \n        if others is not None:\n            columns.extend(others)\n        \n        self.df = pd.DataFrame(columns=columns)\n        self.epoch = 1\n    \n    def log(self,dict):\n        \"\"\"\n        Args:   \n            dict (dict) : dict which should have these keys : \"train_loss\",\"valid_loss\",\"train_{metric}\",\"valid_{metric}\" \n        \"\"\"\n        info = \" \".join(f\"{k}: {v:.4f}\" for k,v in dict.items())\n        print(f\"\\n[EPOCH{self.epoch}] {info}\\n\")\n        dict[\"epoch\"] = self.epoch\n        self.df = self.df.append(dict,ignore_index=True)\n        self.epoch += 1 \n    \n    def dump(self,path=\"./training_history.csv\"):\n        self.df.to_csv(path,index=False)\n\ndef metric(pred,target):\n    # rmse\n    return np.mean((pred - target)**2)**0.5\n\n# Dataset \nclass COMMONLITDatasetBert(Dataset):\n    def __init__(self,df,model_name,mode,max_sentence=315):\n        super(COMMONLITDatasetBert,self).__init__()\n        self.texts = df[\"excerpt\"].values \n        self.model_name = model_name\n        if model_name == \"bert-base-uncased\":\n            self.tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/bert-base-uncased\")\n        elif model_name == \"bert-large-uncased\":\n            self.tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/huggingface-bert/bert-large-uncased\")\n        elif model_name == \"roberta-base\":\n            self.tokenizer = transformers.RobertaTokenizer.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\")\n        elif model_name == \"roberta-large\":\n            self.tokenizer = transformers.RobertaTokenizer.from_pretrained(\"../input/huggingface-roberta-variants/roberta-large/roberta-large\")\n        else:\n            raise ValueError(\"This name bert is not here.\")\n    \n        self.mode = mode\n        self.max_sentence = max_sentence\n        \n        if mode != \"test\":\n            self.target = df[\"target\"].values \n    \n    def __len__(self):\n        return len(self.texts) \n    \n    def __getitem__(self,idx):\n        sentence = self.texts[idx]\n        tokenized = self.tokenizer(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_sentence,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            truncation=True\n        )\n\n        ids = torch.tensor(tokenized[\"input_ids\"],dtype=torch.long) \n        mask = torch.tensor(tokenized[\"attention_mask\"],dtype=torch.long)\n        if \"roberta\" not in self.model_name:\n            token_type = torch.tensor(tokenized[\"token_type_ids\"],dtype=torch.long)\n        else:\n            token_type = 0  \n\n        if self.mode == \"test\":\n            return (\n                ids,\n                mask,\n                token_type\n            )\n        \n        else: # train and valid\n            target = torch.tensor(self.target[idx],dtype=torch.float) \n            return (\n                ids,\n                mask,\n                token_type,\n                target\n            )\n\n# Model \nclass COMMONLITModelBert(nn.Module):\n    def __init__(self,model_name):\n        super(COMMONLITModelBert,self).__init__()\n        self.model_name = model_name\n        if model_name == \"bert-base-uncased\":\n            self.model = transformers.BertForSequenceClassification.from_pretrained(\"../input/bert-base-uncased\",num_labels=1)\n        elif model_name == \"bert-large-uncased\":\n            self.model = transformers.BertForSequenceClassification.from_pretrained(\"../input/huggingface-bert/bert-large-uncased\",num_labels=1)\n        elif model_name == \"roberta-base\":\n            self.model = transformers.RobertaForSequenceClassification.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\",num_labels=1)\n        elif model_name == \"roberta-large\":\n            self.model = transformers.RobertaForSequenceClassification.from_pretrained(\"../input/huggingface-roberta-variants/roberta-large/roberta-large\",num_labels=1)\n        else:\n            raise ValueError(\"This name bert is not here.\")\n    \n    def forward(self,ids,mask):\n        output = self.model.forward(ids,mask)\n        return output[\"logits\"]\n        \n\n# Train,Valid \ndef train_fn(train_loader,model,optimizer,criterion,device,epoch,scheduler):\n    model.train()\n    preds = [] \n    targets = []\n    losses = AverageMeter()\n    if CFG.use_amp:\n        scaler = torch.cuda.amp.GradScaler()\n    train_len = len(train_loader)\n    \n    for step,(ids,mask,token_type,target) in enumerate(train_loader):\n        step += 1\n        batch_size = ids.shape[0] \n        ids = ids.to(device,non_blocking=True)\n        mask = mask.to(device,non_blocking=True) \n        target = target.to(device,non_blocking=True) \n\n        if CFG.use_amp:\n            with torch.cuda.amp.autocast():\n                y_pred = model(ids,mask).squeeze(-1)\n                loss = criterion(y_pred,target)\n            \n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update() \n            scheduler.step() \n\n        else:\n            y_pred = model(ids,mask).squeeze(-1) \n            loss = criterion(y_pred,target)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step() \n            scheduler.step()\n        \n        losses.update(loss.item(),batch_size) \n        preds.append(y_pred.detach().to(\"cpu\").numpy())\n        targets.append(target.detach().squeeze(-1).to(\"cpu\").numpy())\n\n        if step%CFG.print_freq == 0 or step == train_len:\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  #'LR: {lr:.6f}  '\n                  .format(\n                   epoch+1, step, train_len,\n                   loss=losses,\n                   #lr=scheduler.get_lr()[0],\n                   ))\n    \n    predictions = np.concatenate(preds)\n    targets = np.concatenate(targets)\n    return losses.avg,predictions,targets\n\ndef valid_fn(valid_loader,model,criterion,device,epoch): \n    model.eval()\n    preds = [] \n    targets = []\n    losses = AverageMeter()\n    valid_len = len(valid_loader)\n    \n    for step,(ids,mask,token_type,target) in enumerate(valid_loader):\n        step += 1\n        batch_size = ids.shape[0] \n        ids = ids.to(device,non_blocking=True)\n        mask = mask.to(device,non_blocking=True) \n        target = target.to(device,non_blocking=True)\n        \n        with torch.no_grad(): \n            y_pred = model(ids,mask).squeeze(-1)  \n            loss = criterion(y_pred,target) \n            \n        losses.update(loss.item(),batch_size)\n        preds.append(y_pred.detach().to(\"cpu\").numpy())\n        targets.append(target.detach().squeeze(-1).to(\"cpu\").numpy())\n        \n        if step%CFG.print_freq == 0 or step == valid_len:\n            print('Valid : [{0}][{1}/{2}] '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  #'LR: {lr:.6f}  '\n                  .format(\n                   epoch+1, step, valid_len,\n                   loss=losses,\n                   #lr=scheduler.get_lr()[0],\n                   ))\n\n    predictions = np.concatenate(preds)\n    targets = np.concatenate(targets)\n    return losses.avg,predictions,targets\n\n# Training \ndef train_loop(df,fold):\n    print(\"-\"*20,f\"Fold {fold}\",\"-\"*20)\n    random_seed(CFG.seed)\n    loader = Loader() \n\n    train = df[df[\"fold\"] != fold].reset_index(drop=True)\n    valid = df[df[\"fold\"] == fold].reset_index(drop=True)\n    if DEBUG:\n        train = train.sample(n = CFG.batch_size).reset_index()\n        valid = valid.sample(n = CFG.batch_size*2).reset_index()\n    \n    train_dset = COMMONLITDatasetBert(train,CFG.model_name,\"train\",max_sentence=CFG.max_sentence)\n    valid_dset = COMMONLITDatasetBert(valid,CFG.model_name,\"valid\",max_sentence=CFG.max_sentence) \n    \n    train_loader = DataLoader(train_dset,batch_size=CFG.batch_size,shuffle=True,num_workers=CFG.num_workers,pin_memory=True,drop_last=True)\n    valid_loader = DataLoader(valid_dset,batch_size=CFG.batch_size*2,shuffle=False,num_workers=CFG.num_workers,pin_memory=True,drop_last=False) \n    \n    model = COMMONLITModelBert(model_name=CFG.model_name)\n\n    if CFG.optimizer == \"adamw\":\n        optimizer = optim.Adam(model.parameters(),lr = CFG.learning_rate,weight_decay = CFG.weight_decay) \n    \n    if CFG.scheduler == \"linear_warmup\":\n        train_steps = int(len(train)/CFG.batch_size*CFG.n_epoch)\n        num_steps = int(train_steps*0.1)\n        scheduler = get_linear_schedule_with_warmup(optimizer,num_steps,train_steps)\n    \n    criterion = nn.MSELoss() \n    device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n    model.to(device) \n    n_epochs = CFG.n_epoch \n    \n    best_loss = 1e20\n    history = History(metric=\"rmse\")\n    model_path = f\"{OUTPUT_DIR}{CFG.model_name}_nb{NB}ver{VERSION}fold{fold}\"\n\n    for epoch in range(n_epochs):\n        tr_loss,tr_pred,tr_target = train_fn(train_loader,model,optimizer,criterion,device,epoch,scheduler)\n        val_loss,val_pred,val_target = valid_fn(valid_loader,model,criterion,device,epoch) \n\n        tr_rmse = metric(tr_pred,tr_target)\n        val_rmse = metric(val_pred,val_target)\n\n        history.log({\n            \"train_loss\":tr_loss,\n            \"valid_loss\":val_loss,\n            \"train_rmse\":tr_rmse,\n            \"valid_rmse\":val_rmse\n        })\n\n        if best_loss > val_loss:\n            print(f\"score update! {best_loss} -> {val_loss}\\n\")\n            best_loss = val_loss\n            torch.save(model.state_dict(),f\"{model_path}epoch{epoch}.pth\")\n            loader.dump(tr_pred,f\"{model_path}fold{fold}_tr.pkl\")\n            loader.dump(val_pred,f\"{model_path}fold{fold}_val.pkl\")\n    \n    #del train_loader,valid_loader,model,criterion,optimizer\n    gc.collect() \n\n    history.dump(f\"{model_path}.csv\")\n    return history\n\ndef main():\n    train = pd.read_csv(f\"{INPUT_DIR}commonlitutils/train_folds.csv\")\n\n    for fold in CFG.folds:\n        history = train_loop(train,fold=fold)\n\n        fig,ax = plt.subplots(1,1,figsize=(10,7))\n        ax.plot(history.df[\"train_loss\"],label = \"train loss\")\n        ax.plot(history.df[\"valid_loss\"],label = \"valid loss\")\n        ax.set_title(f\"FOLD {fold}\")\n        plt.legend()\n        plt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-03T06:37:02.493267Z","iopub.execute_input":"2021-06-03T06:37:02.493717Z","iopub.status.idle":"2021-06-03T06:37:11.945547Z","shell.execute_reply.started":"2021-06-03T06:37:02.493623Z","shell.execute_reply":"2021-06-03T06:37:11.944463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T06:37:22.914742Z","iopub.execute_input":"2021-06-03T06:37:22.915143Z","iopub.status.idle":"2021-06-03T06:37:47.478868Z","shell.execute_reply.started":"2021-06-03T06:37:22.915111Z","shell.execute_reply":"2021-06-03T06:37:47.477692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}