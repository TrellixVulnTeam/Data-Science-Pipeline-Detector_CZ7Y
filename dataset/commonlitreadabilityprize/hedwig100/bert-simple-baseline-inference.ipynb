{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT Simple Baseline \n\nThis is inference code. [Training code is here.](https://www.kaggle.com/hedwig100/bert-simple-baseline?scriptVersionId=64726110) \n\nModel is roberta-base,5fold. LB = 0.504. \nYou can train models in [this notebook.](https://www.kaggle.com/hedwig100/bert-simple-baseline?scriptVersionId=64726110)(same above). ","metadata":{}},{"cell_type":"code","source":"# Library \n# utils \nimport os,gc,pickle,random\nfrom tqdm import tqdm \nimport numpy as np  \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings \nwarnings.simplefilter(\"ignore\",FutureWarning)\n\n# imgs \nimport cv2 \nimport albumentations as A\n\n# torch \nimport torch \nimport torch.optim as optim \nfrom torch.utils.data import Dataset,DataLoader \nimport torch.nn as nn \nimport torch.nn.functional as F \n\n# other  \nimport transformers \nfrom transformers import get_linear_schedule_with_warmup\n\n\n# Config \nINPUT_DIR = \"../input/\"\nOUTPUT_DIR = \"./\"\n\nDEBUG = False \n\nclass CFG:\n    # utils\n    num_workers = 4\n    batch_size = 32\n\n    # bert param\n    model_name = \"roberta-base\"\n    max_sentence = 315\n    model_path = [\n        \"../input/commonlitmodels/roberta-base_nb3ver3epoch2.pth\",\n        \"../input/commonlitmodels/roberta-base_nb3ver3epoch6.pth\",\n        \"../input/commonlitmodels/roberta-base_nb3ver3epoch3.pth\",\n        \"../input/commonlitmodels/roberta-base_nb3ver8epoch4.pth\",\n        \"../input/commonlitmodels/roberta-base_nb3ver9epoch9.pth\",\n    ]\n\n# Utils \ndef random_seed(SEED):\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass Loader():\n    def __init__(self):\n        pass \n    \n    def load(self,path):\n        \"\"\"\n        Args:\n            path : from which data shouled be loaded\n        Returns:\n            data : loaded data \n        \"\"\"\n        obj = pickle.load(open(path,\"rb\"))\n        return obj \n    \n    def dump(self,obj,path):\n        \"\"\"\n        Args:\n            obj (object) : object which should be dumped\n            path (str) : to which object should be dumped \n        \"\"\"\n        f = open(path,\"wb\")\n        pickle.dump(obj,f)\n        f.close \n\nclass History():\n    def __init__(self,metric=None,others=None):\n        \"\"\"\n        Defauls:\n            columns has default value\n            - epoch \n            - train_loss\n            - valid_loss\n            - train_{metric}\n            _ valid_{metric}\n            \n        Args:\n            metric (int) : metric \n            others (list) : other parameters which is logged \n        \"\"\"\n\n        columns = [\"epoch\",\"train_loss\",\"valid_loss\"]\n        if metric is not None:\n            columns.extend([f\"train_{metric}\",f\"valid_{metric}\"]) \n        if others is not None:\n            columns.extend(others)\n        \n        self.df = pd.DataFrame(columns=columns)\n        self.epoch = 1\n    \n    def log(self,dict):\n        \"\"\"\n        Args:   \n            dict (dict) : dict which should have these keys : \"train_loss\",\"valid_loss\",\"train_{metric}\",\"valid_{metric}\" \n        \"\"\"\n        dict[\"epoch\"] = self.epoch\n        self.df.append(dict,ignore_index=True)\n        self.epoch += 1\n    \n    def dump(self,path=\"./training_history.csv\"):\n        self.df.to_csv(path,index=False)\n\ndef metric(pred,target):\n    # rmse\n    return np.mean((pred - target)**2)**0.5\n\n\n# Dataset \nclass COMMONLITDatasetBert(Dataset):\n    def __init__(self,df,model_name,mode,max_sentence=315):\n        super(COMMONLITDatasetBert,self).__init__()\n        self.texts = df[\"excerpt\"].values \n        self.model_name = model_name\n        if model_name == \"bert-base-uncased\":\n            self.tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/bert-base-uncased\")\n        elif model_name == \"bert-large-uncased\":\n            self.tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/huggingface-bert/bert-large-uncased\")\n        elif model_name == \"roberta-base\":\n            self.tokenizer = transformers.RobertaTokenizer.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\")\n        else:\n            raise ValueError(\"This name bert is not here.\")\n    \n        self.mode = mode\n        self.max_sentence = max_sentence\n        \n        if mode != \"test\":\n            self.target = df[\"target\"].values \n    \n    def __len__(self):\n        return len(self.texts) \n    \n    def __getitem__(self,idx):\n        sentence = self.texts[idx]\n        tokenized = self.tokenizer(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_sentence,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            truncation=True\n        )\n\n        ids = torch.tensor(tokenized[\"input_ids\"],dtype=torch.long) \n        mask = torch.tensor(tokenized[\"attention_mask\"],dtype=torch.long)\n        if \"roberta\" not in self.model_name:\n            token_type = torch.tensor(tokenized[\"token_type_ids\"],dtype=torch.long)\n        else:\n            token_type = 0  \n\n        if self.mode == \"test\":\n            return (\n                ids,\n                mask,\n                token_type\n            )\n        \n        else: # train and valid\n            target = torch.tensor(self.target[idx],dtype=torch.float) \n            return (\n                ids,\n                mask,\n                token_type,\n                target\n            )\n\n# Model \nclass COMMONLITModelBert(nn.Module):\n    def __init__(self,model_name):\n        super(COMMONLITModelBert,self).__init__()\n        self.model_name = model_name\n        if model_name == \"bert-base-uncased\":\n            self.model = transformers.BertForSequenceClassification.from_pretrained(\"../input/bert-base-uncased\",num_labels=1)\n        elif model_name == \"bert-large-uncased\":\n            self.model = transformers.BertForSequenceClassification.from_pretrained(\"../input/huggingface-bert/bert-large-uncased\",num_labels=1)\n        elif model_name == \"roberta-base\":\n            self.model = transformers.RobertaForSequenceClassification.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\",num_labels=1)\n        else:\n            raise ValueError(\"This name bert is not here.\")\n    \n    def forward(self,ids,mask):\n        output = self.model.forward(ids,mask)\n        return output[\"logits\"]      \n      \n\n# Test\ndef test_fn(test_loader,model,device):\n    model.eval() \n    preds = [] \n\n    for ids,mask,token_type in test_loader:\n        ids = ids.to(device,non_blocking=True)\n        mask = mask.to(device,non_blocking=True) \n\n        with torch.no_grad():\n            y_pred = model(ids,mask).squeeze(-1)\n        \n        preds.append(y_pred.detach().to(\"cpu\").numpy())\n    \n    predictions = np.concatenate(preds)\n    return predictions\n\n\n# Training \ndef test_pred(test,CFG):\n    test_dset = COMMONLITDatasetBert(test,CFG.model_name,max_sentence=CFG.max_sentence,mode=\"test\")\n    test_loader = DataLoader(test_dset,batch_size=CFG.batch_size,shuffle=False,num_workers=CFG.num_workers,pin_memory=True)\n\n    model = COMMONLITModelBert(model_name=CFG.model_name)\n    device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n    model.to(device) \n\n    preds = np.zeros(len(test)) \n    for path in CFG.model_path:\n        print(f\"start model {path}\")\n        if device == torch.device(\"cuda\"):\n            model.load_state_dict(torch.load(path)) \n        else:\n            model.load_state_dict(torch.load(path,map_location=\"cpu\"))\n        preds += test_fn(test_loader,model,device) \n    \n    preds /= len(CFG.model_path) \n    return preds \n\ndef main():\n    test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    submit = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n\n    pred = test_pred(test,CFG) \n    \n    submit[\"target\"] = pred \n    submit.to_csv(\"submission.csv\",index=False)\n    print(\"Done !\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-03T06:43:55.442338Z","iopub.execute_input":"2021-06-03T06:43:55.442727Z","iopub.status.idle":"2021-06-03T06:44:05.15702Z","shell.execute_reply.started":"2021-06-03T06:43:55.442647Z","shell.execute_reply":"2021-06-03T06:44:05.155954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T06:44:15.309012Z","iopub.execute_input":"2021-06-03T06:44:15.30939Z","iopub.status.idle":"2021-06-03T06:44:57.033609Z","shell.execute_reply.started":"2021-06-03T06:44:15.309361Z","shell.execute_reply":"2021-06-03T06:44:57.031767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}