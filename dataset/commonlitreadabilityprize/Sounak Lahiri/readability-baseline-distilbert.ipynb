{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Dataloader ready\n\nTo Do :\n\n\nAdd training and validation code\nadd model","metadata":{"_uuid":"aa0af4cd-5cdc-4b23-ba23-236ab7c2b063","_cell_guid":"ea43ef59-b2cd-4b86-813e-b0cd315b31fb","trusted":true}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"f65e4420-be53-4b9b-8331-22a5fbe77ebd","_cell_guid":"b4973ba5-e38d-4a30-a369-0f3d001f4a25","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-15T13:36:04.277778Z","iopub.execute_input":"2021-06-15T13:36:04.278149Z","iopub.status.idle":"2021-06-15T13:36:04.956516Z","shell.execute_reply.started":"2021-06-15T13:36:04.27807Z","shell.execute_reply":"2021-06-15T13:36:04.955515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import DistilBertTokenizerFast, DistilBertModel, DistilBertConfig\n\nimport re\nimport string\n\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn \nfrom transformers import AdamW\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"0ac37843-4b3d-4aca-b9d6-3ce8b3a0f620","_cell_guid":"2597bd20-06b5-4086-9fca-7dfa963b70b2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-15T13:36:04.959843Z","iopub.execute_input":"2021-06-15T13:36:04.960112Z","iopub.status.idle":"2021-06-15T13:36:11.753374Z","shell.execute_reply.started":"2021-06-15T13:36:04.960084Z","shell.execute_reply":"2021-06-15T13:36:11.752501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataloader(train_file_path, test_file_path):\n    \n    print(train_file_path)\n    \n    train_df = pd.read_csv(train_file_path)\n    test_df = pd.read_csv(test_file_path)\n    \n    #print(train_df.head())\n    #print(test_df.head())\n    \n    X = train_df[\"excerpt\"]\n    Y = train_df[\"target\"]\n    \n    X_test = test_df[\"excerpt\"] \n    \n    length = []\n    \n    for index, row in train_df.iterrows():\n        length.append(len(row['excerpt']))\n    \n    length.sort()\n    eda = pd.DataFrame(list(zip(length,Y)), columns=['length', 'target'])   \n    \n    print('Length of X :', len(X))\n    \n    token_length = np.percentile(length, 90)\n    \n    length = []\n    \n    for index, row in test_df.iterrows():\n        length.append(len(row['excerpt']))\n    \n    length.sort()\n    eda_test = pd.DataFrame(length, columns=['length'])   \n    print('Length of X Test :', len(X_test))\n    \n    xtrain, xvalidate, ytrain, yvalidate = train_test_split(X,Y,  test_size=0.2, random_state=123)\n    \n    return(xtrain, ytrain, xvalidate, yvalidate, X_test) #, eda, eda_test)","metadata":{"_uuid":"14c9234d-4293-4e14-9136-cf06e116da60","_cell_guid":"6ed4004c-5b43-4d95-a6e1-852fab85e608","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-15T13:36:11.7552Z","iopub.execute_input":"2021-06-15T13:36:11.755562Z","iopub.status.idle":"2021-06-15T13:36:11.764779Z","shell.execute_reply.started":"2021-06-15T13:36:11.755524Z","shell.execute_reply":"2021-06-15T13:36:11.763276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsub_file = os.path.join(dirname, filenames[0])\ntrain_file_path = os.path.join(dirname, filenames[1])\ntest_file_path = os.path.join(dirname, filenames[2])\n    \nxtrain, ytrain, xvalidate, yvalidate, token_length, eda, eda_test = dataloader(train_file_path, test_file_path)\n    \nprint(\"Token Length 90th percentile : \", token_length)","metadata":{"_uuid":"ed4e5b04-3da6-4c8d-a218-aaf39f29abf4","_cell_guid":"3c72a9d2-7c0d-4e9e-8ba2-e7ca972c476b","execution":{"iopub.status.busy":"2021-05-27T03:48:07.085733Z","iopub.execute_input":"2021-05-27T03:48:07.08627Z","iopub.status.idle":"2021-05-27T03:48:07.399648Z","shell.execute_reply.started":"2021-05-27T03:48:07.086235Z","shell.execute_reply":"2021-05-27T03:48:07.398596Z"},"trusted":true}},{"cell_type":"markdown","source":"eda['length'].hist(bins=50)","metadata":{"_uuid":"7587f682-0a9e-4e16-8ad7-6b17cfadb4bf","_cell_guid":"86482489-b2da-4a09-a975-fd81e4e7d9bb","execution":{"iopub.status.busy":"2021-05-27T03:54:54.965888Z","iopub.execute_input":"2021-05-27T03:54:54.966285Z","iopub.status.idle":"2021-05-27T03:54:55.210348Z","shell.execute_reply.started":"2021-05-27T03:54:54.96625Z","shell.execute_reply":"2021-05-27T03:54:55.209412Z"},"trusted":true}},{"cell_type":"markdown","source":"eda_test['length'].hist(bins=50)","metadata":{"_uuid":"ea604971-3e37-4fcb-b37c-66cb63bf418b","_cell_guid":"f7691313-19df-406d-a1b3-81500775933b","execution":{"iopub.status.busy":"2021-05-27T03:55:09.641935Z","iopub.execute_input":"2021-05-27T03:55:09.642284Z","iopub.status.idle":"2021-05-27T03:55:09.876258Z","shell.execute_reply.started":"2021-05-27T03:55:09.642253Z","shell.execute_reply":"2021-05-27T03:55:09.875292Z"},"trusted":true}},{"cell_type":"markdown","source":"eda['target'].hist(bins=50)","metadata":{"_uuid":"798d99cb-973f-4e75-8f2a-4aa544b1ac29","_cell_guid":"676cc055-a9d3-461a-a763-9eabe375f0e3","execution":{"iopub.status.busy":"2021-05-27T03:55:18.192301Z","iopub.execute_input":"2021-05-27T03:55:18.192656Z","iopub.status.idle":"2021-05-27T03:55:18.418092Z","shell.execute_reply.started":"2021-05-27T03:55:18.192624Z","shell.execute_reply":"2021-05-27T03:55:18.417237Z"},"trusted":true}},{"cell_type":"code","source":"class text_dataset(Dataset):\n    \n    def __init__(self, x,y, max_length_tokens, split):\n        self.x = x\n        self.y = y\n        self.max_length_tokens = max_length_tokens\n        self.split = split\n        \n    def __len__(self):\n        return len(self.x)\n        \n    def __getitem__(self, index):\n        \n        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n        \n        text = str(self.x.iloc[index])\n        text = \" \".join(text.split())\n        \n        if(self.split != 'test'):\n            target = self.y.iloc[index]\n        \n        #print(text, target)\n        \n        text = ' '.join(tokenizer.tokenize(text))\n        \n        encoded = tokenizer.encode_plus(\n            text,                    # Sentence to encode.\n            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n            max_length = self.max_length_tokens,           # Pad & truncate all sentences.\n            pad_to_max_length = True,\n            return_attention_mask = True,   # Construct attn. masks.\n            return_tensors = 'pt',  \n            truncation= True,\n            )\n        \n       \n        \n        input_ids = torch.squeeze(encoded['input_ids'])\n        attention_mask = torch.squeeze(encoded['attention_mask'])\n        #token_type_ids= torch.squeeze(encoded['token_type_ids'])\n        \n        if(self.split != 'test'):\n            target = torch.as_tensor(target)\n\n            sample = {'input_ids' : input_ids, 'attention_mask' : attention_mask, \n                       'target' : target} #, 'token_type_ids': token_type_ids}\n        \n        else:\n            sample = {'input_ids' : input_ids, 'attention_mask' : attention_mask}\n            \n        \n        return sample\n        \n        \n    def clean_text(self, text):\n  \n        text = text.lower()\n        text = re.sub('\\[.*?\\]', '', text)\n        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n        text = re.sub('<.*?>+', '', text)\n        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n        text = re.sub('\\n', '', text)\n        text = re.sub('\\w*\\d\\w*', '', text)\n        return text\n\n\n    def text_preprocessing(self, text, tokenizer):\n   \n        nopunc = self.clean_text(text)\n        tokenized_text = tokenizer.tokenize(nopunc)\n        #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n        combined_text = ' '.join(tokenized_text)\n        return combined_text","metadata":{"_uuid":"fbfe3943-f127-427f-a146-cc4570fa81c0","_cell_guid":"220940c9-e432-4e2b-8e69-c5a4f7bf5b95","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-15T13:36:11.766534Z","iopub.execute_input":"2021-06-15T13:36:11.767075Z","iopub.status.idle":"2021-06-15T13:36:11.782064Z","shell.execute_reply.started":"2021-06-15T13:36:11.767037Z","shell.execute_reply":"2021-06-15T13:36:11.78127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class readability_model(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        \n        self.config = DistilBertConfig()\n       \n        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n               \n        self.regressor = nn.Sequential(nn.LayerNorm(768, eps=1e-12),\n                                       nn.Linear(768, 128),\n                                       nn.ReLU(inplace=True),\n                                       nn.BatchNorm1d(num_features=128),\n                                       nn.Dropout(0.5),\n                                       nn.Linear(128, 16),\n                                       nn.ReLU(inplace=True),\n                                       nn.BatchNorm1d(num_features=16),\n                                       nn.Dropout(0.5),\n                                       nn.Linear(16, 1))\n        \n\n        \n    def forward(self, input_ids=None, attention_mask=None, head_mask=None, \n                position_ids = None, inputs_embeds=None,labels=None, output_attentions=None, \n                output_hidden_states=None, return_dict=None): #token_type_ids=None,\n        \n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        \"\"\"outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            #token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        pooled_output = outputs[1]\"\"\"\n        \n        distilbert_output = self.distilbert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n        pooled_output = hidden_state[:, 0]\n        \n        out = self.regressor(pooled_output)\n        \n        return (out)","metadata":{"_uuid":"3a5e6ab3-060d-4741-b8dd-1bfa8b49bc72","_cell_guid":"b5b54f1f-7453-4868-84c1-b674ca17f056","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-15T13:36:11.783587Z","iopub.execute_input":"2021-06-15T13:36:11.784164Z","iopub.status.idle":"2021-06-15T13:36:11.796317Z","shell.execute_reply.started":"2021-06-15T13:36:11.784125Z","shell.execute_reply":"2021-06-15T13:36:11.795563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion, device):\n    \n    model.cuda() #to(device)\n    model.train()\n    \n    epoch_loss= 0.0\n    \n    tbar = tqdm(dataloader, unit=\"batch\")\n    \n    for i, data in enumerate(tbar):\n                \n        input_ids = data['input_ids'].to(device)\n        attention_mask = data['attention_mask'].to(device)\n        #token_type_ids = data['token_type_ids'].to(device)\n        target = data['target'].to(device)\n\n        #print(target.size())\n            \n        optimizer.zero_grad()\n        output = model(input_ids=input_ids, attention_mask=attention_mask) #, \n                            #token_type_ids=token_type_ids)\n        \n        output = torch.squeeze(output)\n        \n        loss = criterion(output, target.float())\n            \n        loss.backward()\n            \n        optimizer.step()\n            \n        epoch_loss += loss.item()\n            \n        tbar.set_description('Train loss: %.3f' % (epoch_loss / (i+1) ))\n            \n    return(model, (epoch_loss/(i+1)))","metadata":{"_uuid":"13c50fa2-11bd-486e-9460-f90d4e58a14d","_cell_guid":"2c896157-fdbf-43a4-b4d9-f28da648281f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-15T13:36:11.797591Z","iopub.execute_input":"2021-06-15T13:36:11.798161Z","iopub.status.idle":"2021-06-15T13:36:11.8088Z","shell.execute_reply.started":"2021-06-15T13:36:11.798121Z","shell.execute_reply":"2021-06-15T13:36:11.807995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, dataloader, criterion, device):\n    \n    model.cuda() #to(device)\n    model.eval()\n    \n    epoch_loss= 0.0\n    \n    tbar = tqdm(dataloader, unit=\"batch\")\n    \n    for i, data in enumerate(tbar):\n                    \n        input_ids = data['input_ids'].cuda() #to(device)\n        attention_mask = data['attention_mask'].cuda() #to(device)\n        #token_type_ids = data['token_type_ids'].to(device)\n        target = data['target'].cuda() #to(device)\n\n            #print(input_ids.size(), attention_mask.shape, token_type_ids.shape)\n            \n        with torch.no_grad():\n            output = model(input_ids=input_ids, attention_mask=attention_mask )#, \n                            #token_type_ids=token_type_ids)\n        \n        output = torch.squeeze(output)\n        loss = criterion(output, target.float())\n            \n        epoch_loss += loss.item()\n            \n        tbar.set_description('Validation loss: %.3f' % (epoch_loss / (i+1) ))\n            \n    return((epoch_loss/(i+1)))","metadata":{"_uuid":"db65f29c-adaf-47bf-94f2-760f243f3023","_cell_guid":"cb1e983c-bdd7-4cce-b057-91d4e2b07428","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-15T13:36:11.810174Z","iopub.execute_input":"2021-06-15T13:36:11.811362Z","iopub.status.idle":"2021-06-15T13:36:11.820703Z","shell.execute_reply.started":"2021-06-15T13:36:11.811322Z","shell.execute_reply":"2021-06-15T13:36:11.81975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataloader, device):\n    \n    model.cuda() #to(device)\n    model.eval()\n    \n    out = []\n    \n    tbar = tqdm(dataloader, unit=\"batch\")\n    \n    for i, data in enumerate(tbar):\n                    \n        input_ids = data['input_ids'].cuda() #to(device)\n        attention_mask = data['attention_mask'].cuda() #to(device)\n        #token_type_ids = data['token_type_ids'].to(device)\n        \n\n            #print(input_ids.size(), attention_mask.shape, token_type_ids.shape)\n            \n        with torch.no_grad():\n            output = model(input_ids=input_ids, attention_mask=attention_mask )#, \n                            #token_type_ids=token_type_ids)\n        \n        output = torch.squeeze(output)\n        output = output.cpu().tolist()\n                    \n        out.append(output)   \n        \n        tbar.set_description('Files: %.3f' % (i+1))\n            \n    return(out)","metadata":{"_uuid":"a61bcabd-b41b-49d8-8e03-7948c8093069","_cell_guid":"b881e580-d8be-4ae3-a3f3-880119bf1b43","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-15T13:36:11.823801Z","iopub.execute_input":"2021-06-15T13:36:11.824247Z","iopub.status.idle":"2021-06-15T13:36:11.833172Z","shell.execute_reply.started":"2021-06-15T13:36:11.824175Z","shell.execute_reply":"2021-06-15T13:36:11.832275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot(y,x, title, ylabel, xlabel, savepath):\n\n    plt.figure()\n\n    plt.plot(x,y)\n    plt.xlabel(xlabel)\n    plt.title(title)\n    plt.ylabel(ylabel)\n\n    plt.savefig(savepath)\n\n    plt.clf()","metadata":{"_uuid":"ae2fbc71-124a-4d68-9170-7f882d95ac02","_cell_guid":"5d6dc8b7-4838-4500-bab1-e4465a2c47f7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-15T13:36:11.834742Z","iopub.execute_input":"2021-06-15T13:36:11.83539Z","iopub.status.idle":"2021-06-15T13:36:11.843287Z","shell.execute_reply.started":"2021-06-15T13:36:11.835351Z","shell.execute_reply":"2021-06-15T13:36:11.842467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    \n    max_length = 512\n    batch_size = 32\n    epochs = 5\n    \n    training_loss = []\n    validation_loss = []\n    \n    model_save_path = \"readability_baseline_BERT.pth\"\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(device)\n\n    \n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n    sub_file = os.path.join(dirname, filenames[0])\n    train_file_path = os.path.join(dirname, filenames[1])\n    test_file_path = os.path.join(dirname, filenames[2])\n    \n    xtrain, ytrain, xvalidate, yvalidate, test = dataloader(train_file_path, test_file_path)\n   \n       \n    model = readability_model()\n    #model = model.to(device)\n    \n    \n    traindata = text_dataset(x=xtrain, y=ytrain, max_length_tokens = max_length, \n                             split='train')\n    validdata = text_dataset(x=xvalidate, y=yvalidate, max_length_tokens = max_length, \n                             split='validation')\n    testdata = text_dataset(x=test, y=None, max_length_tokens = max_length, split='test')\n    \n    \n   # print(traindata.__getitem__(1010))\n    \n    train_dataloader = DataLoader(traindata, batch_size = batch_size, shuffle= True)\n    valid_dataloader = DataLoader(validdata, batch_size = batch_size, shuffle= True)\n    \n    test_dataloader = DataLoader(testdata, batch_size = 1, shuffle = False)\n    \n    optimizer = AdamW(model.parameters(), lr=0.001)\n    criterion = nn.MSELoss()\n    \n    \n    \n    for epoch in range(epochs) :\n        \n        print(\"Epoch : \", epoch)\n        \n        model, trainloss = train(model, train_dataloader, optimizer, criterion, device)\n        print(\"Average Training Loss : \", trainloss)\n        training_loss.append(trainloss)\n        \n        valid_loss = validate(model, valid_dataloader, criterion, device)\n        print(\"Average Validation Loss : \", valid_loss)\n        validation_loss.append(valid_loss)\n        \n        if(epoch == 0):\n            torch.save(model.state_dict(), model_save_path)\n            print(\"Initial Model Saved\")\n        elif(valid_loss <= min(validation_loss)):\n            torch.save(model.state_dict(), model_save_path)\n            print(\"Best Model Saved till now\")\n            \n            \n    \n    \n    model.load_state_dict(torch.load(model_save_path))\n            \n    out = evaluate(model, test_dataloader, device)\n    test_ids = test_df['id']\n    \n    submission = pd.DataFrame(list(zip(test_ids,out)), columns=['id', 'target'])\n    print(submission.head())\n    \n    submission.to_csv(\"submission.csv\", index=False)\n    \n        \n\n    \n\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"3a0bec0a-fafb-4d49-84d0-0c2f27c901a1","_cell_guid":"5a30b285-195b-405c-b8cb-81ce3ecc8496","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-15T13:36:11.844679Z","iopub.execute_input":"2021-06-15T13:36:11.845062Z","iopub.status.idle":"2021-06-15T13:38:00.102885Z","shell.execute_reply.started":"2021-06-15T13:36:11.845024Z","shell.execute_reply":"2021-06-15T13:38:00.099357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-06-15T13:38:00.104091Z","iopub.status.idle":"2021-06-15T13:38:00.104778Z"},"trusted":true},"execution_count":null,"outputs":[]}]}