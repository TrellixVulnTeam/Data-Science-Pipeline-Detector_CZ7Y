{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## For Everyone and Anyone !! ** PLease Upvote if helped !! **","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\n\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n## for text processing\nfrom dateutil import parser\nimport string\nimport re\nimport nltk\n# nltk.download('punkt')\n# nltk.download('wordnet')\nfrom nltk.stem.porter import PorterStemmer\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom gensim.models import KeyedVectors\n\n## for language detection\n## warnings !!\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-09T14:45:36.916848Z","iopub.execute_input":"2021-06-09T14:45:36.917278Z","iopub.status.idle":"2021-06-09T14:45:36.932034Z","shell.execute_reply.started":"2021-06-09T14:45:36.917223Z","shell.execute_reply":"2021-06-09T14:45:36.93064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import spacy.cli/\n# spacy.cli.download(\"en_core_web_lg\")\nimport en_core_web_lg\nnlp = en_core_web_lg.load()\n\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:45:37.403477Z","iopub.execute_input":"2021-06-09T14:45:37.403901Z","iopub.status.idle":"2021-06-09T14:45:44.549528Z","shell.execute_reply.started":"2021-06-09T14:45:37.403863Z","shell.execute_reply":"2021-06-09T14:45:44.548357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! pip install langdetect\n# ! pip install translate\n# ! pip install gensim\n# ! pip install googletrans","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:45:44.551027Z","iopub.execute_input":"2021-06-09T14:45:44.551394Z","iopub.status.idle":"2021-06-09T14:45:44.554582Z","shell.execute_reply.started":"2021-06-09T14:45:44.551359Z","shell.execute_reply":"2021-06-09T14:45:44.553814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\nsubmission = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\nprint('Training shape : {}'.format(train.shape))\nprint('Testing shape : {}'.format(test.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:45:44.556062Z","iopub.execute_input":"2021-06-09T14:45:44.556414Z","iopub.status.idle":"2021-06-09T14:45:44.61714Z","shell.execute_reply.started":"2021-06-09T14:45:44.556382Z","shell.execute_reply":"2021-06-09T14:45:44.615995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:45:44.619231Z","iopub.execute_input":"2021-06-09T14:45:44.619725Z","iopub.status.idle":"2021-06-09T14:45:44.634494Z","shell.execute_reply.started":"2021-06-09T14:45:44.619678Z","shell.execute_reply":"2021-06-09T14:45:44.63336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Feature Engineering :-------------->\n\n## Feature Engg Prior to Cleaning :  to a generate some relevant features and then clenaing the irrelevnt data","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:45:44.636081Z","iopub.execute_input":"2021-06-09T14:45:44.636728Z","iopub.status.idle":"2021-06-09T14:45:44.647978Z","shell.execute_reply.started":"2021-06-09T14:45:44.636679Z","shell.execute_reply":"2021-06-09T14:45:44.647197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Extraction from the job discription :\n\nprint('* ------------------------- for train data-------------------------------- *')\n\n# the relevance of these feature will be tested on the later stages !!\n\ntrain['word_count'] = train[\"excerpt\"].apply(lambda x: len(str(x).split(\" \")))\ntrain['char_count'] = train[\"excerpt\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\ntrain[\"num_unique_words\"] = train[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\ntrain[\"num_stopwords\"] = train[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntrain[\"num_punctuations\"] =train['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntrain[\"num_words_upper\"] = train[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntrain[\"num_words_title\"] = train[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntrain[\"mean_word_len\"] = train[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\nprint('* ------------------------- for test data-------------------------------- *')\n\ntest['word_count'] = test[\"excerpt\"].apply(lambda x: len(str(x).split(\" \")))\ntest['char_count'] = test[\"excerpt\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\ntest[\"num_unique_words\"] = test[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_stopwords\"] = test[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"num_punctuations\"] =test['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest[\"num_words_upper\"] = test[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_title\"] = test[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"mean_word_len\"] = test[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:45:44.649366Z","iopub.execute_input":"2021-06-09T14:45:44.649978Z","iopub.status.idle":"2021-06-09T14:45:45.485524Z","shell.execute_reply.started":"2021-06-09T14:45:44.649932Z","shell.execute_reply":"2021-06-09T14:45:45.484226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:45:45.487289Z","iopub.execute_input":"2021-06-09T14:45:45.487845Z","iopub.status.idle":"2021-06-09T14:45:45.506673Z","shell.execute_reply.started":"2021-06-09T14:45:45.487791Z","shell.execute_reply":"2021-06-09T14:45:45.505399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Data Cleaning :------------->","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:45:45.509011Z","iopub.execute_input":"2021-06-09T14:45:45.509357Z","iopub.status.idle":"2021-06-09T14:45:45.51918Z","shell.execute_reply.started":"2021-06-09T14:45:45.509324Z","shell.execute_reply":"2021-06-09T14:45:45.518241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_valid_date(date_str): # sub function\n    try:\n        parser.parse(date_str)\n        return True\n    except:\n        return False\n    \ndef date_removal(data):\n    new_list = [' '.join([w for w in line.split() if not is_valid_date(w)]) for line in data]\n    return (new_list[0])\n\ndef stemmer_and_stopWord(doc):\n    \n    doc= nlp(doc)\n    token_list = []\n    for token in doc:\n        lemma = token.lemma_\n        if lemma == '-PRON-' or lemma == 'be':\n            lemma = token.text\n        token_list.append(lemma)\n\n    stemmed = token_list\n    \n    # Create list of word tokens after removing stopwords\n    \n    filtered_sentence =[] \n    for word in stemmed[:100]:\n        lexeme = nlp.vocab[word]\n        if lexeme.is_stop == False:\n            filtered_sentence.append(word)\n    return (' '.join(filtered_sentence))\n\ndef normaliz(filtered_sentence):\n        \n    words = [str(word).lower() for word in filtered_sentence.split()]\n    return  ' '.join(words[:100])\n\n# For number removal ~\n\ndef numbers_removal(data):\n    s = [data]\n    result = ''.join([i for i in s if not i.isdigit()])\n    return (result)\n# For punchuation & double white spaces ~\n\ndef punch_removal(words):\n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in [words]]\n    return re.sub(' +', ' ', stripped[:100][0])\n\n\ndef cleaner(data):\n    string = [data]\n    string = date_removal(string)\n    # string = numbers_removal(string)\n    string = punch_removal(string)\n    string = stemmer_and_stopWord(string)\n    string = normaliz(string)\n    return string","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:45:45.600421Z","iopub.execute_input":"2021-06-09T14:45:45.600782Z","iopub.status.idle":"2021-06-09T14:45:45.615513Z","shell.execute_reply.started":"2021-06-09T14:45:45.600751Z","shell.execute_reply":"2021-06-09T14:45:45.61422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleaner(data):\n    string = [data]\n    string = date_removal(string)\n    string = numbers_removal(string)\n    string = punch_removal(string)\n    string = stemmer_and_stopWord(string)\n    string = normaliz(string)\n    return string\n\nprint('* --------------- for train data --------------- *')\n\nlist=[]\nfor i in train['excerpt']: # cleansing !!\n    list.append(cleaner(i))\n\ntrain['excerpt']=pd.Series(list) # updating the attributee !!\n\nprint('* --------------- for test data --------------- *')\n\nlist=[]\nfor i in test['excerpt']: # cleansing !!\n    list.append(cleaner(i))\n\ntest['excerpt']=pd.Series(list) # updating the attributee !!","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:46:04.135367Z","iopub.execute_input":"2021-06-09T14:46:04.135946Z","iopub.status.idle":"2021-06-09T14:47:53.473892Z","shell.execute_reply.started":"2021-06-09T14:46:04.135896Z","shell.execute_reply":"2021-06-09T14:47:53.472808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import langdetect\n\n# print('* ------------------------- for train data-------------------------------- *')\n\n# train['lang'] = train[\"excerpt\"].apply(lambda x: langdetect.detect(x) if x.strip() != \"\" else \"\")\n\n# fig, ax = plt.subplots()\n# fig.suptitle('lang', fontsize=12)\n# train['lang'].reset_index().groupby('lang').count().sort_values(by= \"index\").plot(kind=\"barh\", legend=False, ax=ax).grid(axis='x')\n# plt.show()\n\n# print('* -------------------------- for test data ------------------------------- *')\n\n# test['lang'] = test[\"excerpt\"].apply(lambda x: langdetect.detect(x) if x.strip() != \"\" else \"\")\n\n# fig, ax = plt.subplots()\n# fig.suptitle('lang', fontsize=12)\n# test['lang'].reset_index().groupby('lang').count().sort_values(by= \"index\").plot(kind=\"barh\", legend=False, ax=ax).grid(axis='x')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:47:53.475779Z","iopub.execute_input":"2021-06-09T14:47:53.476493Z","iopub.status.idle":"2021-06-09T14:47:53.48195Z","shell.execute_reply.started":"2021-06-09T14:47:53.476423Z","shell.execute_reply":"2021-06-09T14:47:53.480517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:47:53.484438Z","iopub.execute_input":"2021-06-09T14:47:53.484942Z","iopub.status.idle":"2021-06-09T14:47:53.503718Z","shell.execute_reply.started":"2021-06-09T14:47:53.484891Z","shell.execute_reply":"2021-06-09T14:47:53.502947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extra_features = ['word_count', 'char_count', 'num_unique_words', 'num_stopwords',\n       'num_punctuations', 'num_words_upper', 'num_words_title',\n       'mean_word_len']","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:47:53.505229Z","iopub.execute_input":"2021-06-09T14:47:53.50607Z","iopub.status.idle":"2021-06-09T14:47:53.519685Z","shell.execute_reply.started":"2021-06-09T14:47:53.506008Z","shell.execute_reply":"2021-06-09T14:47:53.518788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training :----------->","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:40.120472Z","iopub.execute_input":"2021-06-09T14:49:40.121208Z","iopub.status.idle":"2021-06-09T14:49:40.125148Z","shell.execute_reply.started":"2021-06-09T14:49:40.121167Z","shell.execute_reply":"2021-06-09T14:49:40.123897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:40.427074Z","iopub.execute_input":"2021-06-09T14:49:40.427469Z","iopub.status.idle":"2021-06-09T14:49:40.442939Z","shell.execute_reply.started":"2021-06-09T14:49:40.427437Z","shell.execute_reply":"2021-06-09T14:49:40.44179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## if It helped !! do care to Upvote !! :)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:40.660274Z","iopub.execute_input":"2021-06-09T14:49:40.660764Z","iopub.status.idle":"2021-06-09T14:49:40.66462Z","shell.execute_reply.started":"2021-06-09T14:49:40.660732Z","shell.execute_reply":"2021-06-09T14:49:40.663495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences \nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\n\n\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dropout, GRU, Activation, Embedding, Bidirectional,SpatialDropout1D, BatchNormalization, Conv1D, MaxPool1D","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:40.881494Z","iopub.execute_input":"2021-06-09T14:49:40.88199Z","iopub.status.idle":"2021-06-09T14:49:40.887971Z","shell.execute_reply.started":"2021-06-09T14:49:40.881958Z","shell.execute_reply":"2021-06-09T14:49:40.887281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_NB_WORDS = 28523 + 1\n# Max number of words in each description.\nMAX_SEQUENCE_LENGTH = 100\n# This is fixed.\n\nEMBEDDING_DIM = 100\n\nJob_type = {\n    'Permanent':0,\n    'Contract/Interim':1,\n    'Contract/Temp':2,\n    'Temporary/Seasonal':3,\n    'Any':4,\n    'Part-Time':6\n    }\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(train['excerpt'].values)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:41.103563Z","iopub.execute_input":"2021-06-09T14:49:41.104218Z","iopub.status.idle":"2021-06-09T14:49:41.269475Z","shell.execute_reply.started":"2021-06-09T14:49:41.10418Z","shell.execute_reply":"2021-06-09T14:49:41.26797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS, \n                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n                      lower=True, oov_token='OOV')\n\ntokenizer.fit_on_texts(train['excerpt'].values)\nX = tokenizer.texts_to_sequences(train['excerpt'].values)\n\nMAX_NB_WORDS = len(tokenizer.word_index) + 1 \n# Max number of words in each description.\nMAX_SEQUENCE_LENGTH = 100\n# This is fixed.\nEMBEDDING_DIM = 100\n\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH) # Training labels","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:41.356892Z","iopub.execute_input":"2021-06-09T14:49:41.357522Z","iopub.status.idle":"2021-06-09T14:49:41.662285Z","shell.execute_reply.started":"2021-06-09T14:49:41.357468Z","shell.execute_reply":"2021-06-09T14:49:41.661478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tokenizer.texts_to_sequences(train['excerpt'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\n\nY = train['target']\nprint('Shape of label tensor:', Y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:41.663827Z","iopub.execute_input":"2021-06-09T14:49:41.664416Z","iopub.status.idle":"2021-06-09T14:49:41.819327Z","shell.execute_reply.started":"2021-06-09T14:49:41.664369Z","shell.execute_reply":"2021-06-09T14:49:41.818285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extra_features_train = train[extra_features].to_numpy(dtype='int32')\nX = np.concatenate((X,extra_features_train), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:41.821164Z","iopub.execute_input":"2021-06-09T14:49:41.821799Z","iopub.status.idle":"2021-06-09T14:49:41.830749Z","shell.execute_reply.started":"2021-06-09T14:49:41.821749Z","shell.execute_reply":"2021-06-09T14:49:41.829567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## modeling :","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:41.997776Z","iopub.execute_input":"2021-06-09T14:49:41.998154Z","iopub.status.idle":"2021-06-09T14:49:42.001625Z","shell.execute_reply.started":"2021-06-09T14:49:41.998122Z","shell.execute_reply":"2021-06-09T14:49:42.000534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Random Forest : !!!\n\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn import metrics\n\n# X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.10, random_state=0 )\n\n# '''parameters = {\n#     \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n#     \"max_depth\":[3,5,8],\n#     \"max_features\":[\"log2\",\"sqrt\"],\n#     \"criterion\": [\"friedman_mse\"],\n#     \"subsample\":[0.5, 0.8, 0.9, 1.0],\n#     \"n_estimators\":[100,500,200]\n#     }'''\n\n\n# reg = RandomForestRegressor(max_depth=8,n_estimators=200,n_jobs=2)\n# reg.fit(X_train, y_train)\n\n# y_pred=reg.predict(X_test)\n# print('MAE:-->',metrics.mean_absolute_error(y_test, y_pred))\n# print('MSE:-->',metrics.mean_squared_error(y_test, y_pred))\n# print('RMSE:-->',np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n# print('R2:-->',metrics.r2_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:42.212063Z","iopub.execute_input":"2021-06-09T14:49:42.212498Z","iopub.status.idle":"2021-06-09T14:49:42.216693Z","shell.execute_reply.started":"2021-06-09T14:49:42.212464Z","shell.execute_reply":"2021-06-09T14:49:42.215623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n\n# X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n# scaler= StandardScaler() # before reshaping\n\n# # print(X_test[0])\n\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.fit_transform(X_test)\n\n# # print(X_test[0])\n\n# print(X_train.shape)\n# print(X_test.shape)\n\n# X_train=X_train.reshape(X_train.shape[0], X_train.shape[1],1) # it needs to be a nummpy array\n# X_test=X_test.reshape(X_test.shape[0], X_test.shape[1],1)\n\n# print(X_train.shape)\n# print(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:42.428293Z","iopub.execute_input":"2021-06-09T14:49:42.42882Z","iopub.status.idle":"2021-06-09T14:49:42.433182Z","shell.execute_reply.started":"2021-06-09T14:49:42.428786Z","shell.execute_reply":"2021-06-09T14:49:42.432125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## convolution !!\n\n# epochs=10\n# cat_model=Sequential()\n\n# cat_model.add(Conv1D(filters=32, activation='relu', kernel_size=2, input_shape=(103,1)))\n# cat_model.add(Conv1D(filters=64, activation='relu', kernel_size=2))\n# cat_model.add(Dropout(0.2))\n# # cat_model.add(Conv1D(filters=128, activation='relu', kernel_size=2))\n# # cat_model.add(BatchNormalization())\n# # cat_model.add(Dropout(0.2))\n# cat_model.add(Flatten())\n# cat_model.add(Dense(128, activation='relu'))\n# cat_model.add(Dropout(0.2))\n\n# cat_model.add(Dense(1,activation='linear'))\n\n# # cat_model.summary()\n# cat_model.compile( optimizer=Adam(lr=0.05), loss='mae', metrics=['mae'])\n\n# history=cat_model.fit(X_train,y_train, epochs=epochs, validation_data=(X_test,y_test), verbose=2)\n\n# accr = cat_model.evaluate(X_test,y_test)\n# print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:42.654318Z","iopub.execute_input":"2021-06-09T14:49:42.654841Z","iopub.status.idle":"2021-06-09T14:49:42.65959Z","shell.execute_reply.started":"2021-06-09T14:49:42.654809Z","shell.execute_reply":"2021-06-09T14:49:42.658412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:49:42.86383Z","iopub.execute_input":"2021-06-09T14:49:42.86421Z","iopub.status.idle":"2021-06-09T14:49:42.873078Z","shell.execute_reply.started":"2021-06-09T14:49:42.864176Z","shell.execute_reply":"2021-06-09T14:49:42.871919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_model = Sequential()\ncat_model.add(Embedding(MAX_NB_WORDS, 300, input_length=X.shape[1]))\ncat_model.add(Bidirectional(LSTM(32, return_sequences=True)))\ncat_model.add(Bidirectional(LSTM(64)))\n\n# cat_model.add(LSTM(64))\ncat_model.add(Dropout(0.2))\ncat_model.add(Dense(64, activation='relu'))\ncat_model.add(Dropout(0.2))\n\ncat_model.add(Dense(1, activation='linear'))\ncat_model.compile(loss='mse', optimizer=Adam(lr=0.05), metrics=['mae'])","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:01:56.03229Z","iopub.execute_input":"2021-06-09T15:01:56.032754Z","iopub.status.idle":"2021-06-09T15:01:57.039363Z","shell.execute_reply.started":"2021-06-09T15:01:56.032717Z","shell.execute_reply":"2021-06-09T15:01:57.038283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\nhistory = cat_model.fit(X_train, Y_train, epochs=epochs,validation_split=0.1,\n                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:01:58.367813Z","iopub.execute_input":"2021-06-09T15:01:58.368199Z","iopub.status.idle":"2021-06-09T15:04:04.586394Z","shell.execute_reply.started":"2021-06-09T15:01:58.368167Z","shell.execute_reply":"2021-06-09T15:04:04.585202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accr = cat_model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:14:35.443517Z","iopub.execute_input":"2021-06-09T15:14:35.44411Z","iopub.status.idle":"2021-06-09T15:14:35.927455Z","shell.execute_reply.started":"2021-06-09T15:14:35.444068Z","shell.execute_reply":"2021-06-09T15:14:35.926314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Pedictions","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:14:40.001414Z","iopub.execute_input":"2021-06-09T15:14:40.001922Z","iopub.status.idle":"2021-06-09T15:14:40.006901Z","shell.execute_reply.started":"2021-06-09T15:14:40.001882Z","shell.execute_reply":"2021-06-09T15:14:40.005548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tok = tokenizer.texts_to_sequences(test['excerpt'].values)\ntest_tok = pad_sequences(test_tok, maxlen=MAX_SEQUENCE_LENGTH)\n\nextra_features_test = test[extra_features].to_numpy(dtype='int32')\ntest_tok = np.concatenate((test_tok, extra_features_test), axis=1)\n\n# test_tok = scaler.fit_transform(test_tok)\n# test_tok = test_tok.reshape(7, 103,1)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:14:40.269888Z","iopub.execute_input":"2021-06-09T15:14:40.27028Z","iopub.status.idle":"2021-06-09T15:14:40.281866Z","shell.execute_reply.started":"2021-06-09T15:14:40.27023Z","shell.execute_reply":"2021-06-09T15:14:40.280524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target_pred = []\n# extra_features_test = test[['word_count', 'char_count', 'avg_word_length']].to_numpy(dtype='int32')\n\n# for j,i in enumerate(test['excerpt']):\n#     seq = tokenizer.texts_to_sequences([i]) # Using the same fitted tokenizer \n#     padded = pad_sequences(seq, maxlen = MAX_SEQUENCE_LENGTH) # which was trained on the train data\n#     padded = np.concatenate((padded,[extra_features_test[j]]), axis=1)\n# #     target_pred.append(reg.predict(padded)[0])\n# #     padded = scaler.fit_transform(padded)\n#     padded=padded.reshape(1, 103,1)\n#     target_pred.append(cat_model.predict(padded)[0][0])\n    \ntarget_pred = cat_model.predict(test_tok)\ntarget_pred = [item for sublist in target_pred for item in sublist]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:14:41.478666Z","iopub.execute_input":"2021-06-09T15:14:41.479041Z","iopub.status.idle":"2021-06-09T15:14:42.829651Z","shell.execute_reply.started":"2021-06-09T15:14:41.47901Z","shell.execute_reply":"2021-06-09T15:14:42.82875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_tok","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:14:42.830827Z","iopub.execute_input":"2021-06-09T15:14:42.83112Z","iopub.status.idle":"2021-06-09T15:14:42.836918Z","shell.execute_reply.started":"2021-06-09T15:14:42.831093Z","shell.execute_reply":"2021-06-09T15:14:42.836239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['target'] = pd.Series(target_pred)\n\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:14:42.838443Z","iopub.execute_input":"2021-06-09T15:14:42.83892Z","iopub.status.idle":"2021-06-09T15:14:42.862608Z","shell.execute_reply.started":"2021-06-09T15:14:42.838887Z","shell.execute_reply":"2021-06-09T15:14:42.861156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:14:48.421021Z","iopub.execute_input":"2021-06-09T15:14:48.421424Z","iopub.status.idle":"2021-06-09T15:14:48.429864Z","shell.execute_reply.started":"2021-06-09T15:14:48.421391Z","shell.execute_reply":"2021-06-09T15:14:48.428734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}