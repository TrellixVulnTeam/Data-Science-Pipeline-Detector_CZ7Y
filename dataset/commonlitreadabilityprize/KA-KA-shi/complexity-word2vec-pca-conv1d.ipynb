{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## For Everyone and Anyone !! ** Do up-vote if you liked it **","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-09T16:57:55.876935Z","iopub.execute_input":"2021-06-09T16:57:55.877254Z","iopub.status.idle":"2021-06-09T16:57:55.881329Z","shell.execute_reply.started":"2021-06-09T16:57:55.877182Z","shell.execute_reply":"2021-06-09T16:57:55.880345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom dateutil import parser\nimport string\nimport re\nimport nltk\n# nltk.download('punkt')\n# nltk.download('wordnet')\nfrom nltk.stem.porter import PorterStemmer\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:57:57.071939Z","iopub.execute_input":"2021-06-09T16:57:57.072322Z","iopub.status.idle":"2021-06-09T16:57:57.786356Z","shell.execute_reply.started":"2021-06-09T16:57:57.072298Z","shell.execute_reply":"2021-06-09T16:57:57.785462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.manifold import TSNE\nimport scipy.stats as stats\nimport pylab \nimport matplotlib.pyplot as plt\n\nimport en_core_web_lg\nnlp = en_core_web_lg.load()\n\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:57:59.007111Z","iopub.execute_input":"2021-06-09T16:57:59.00742Z","iopub.status.idle":"2021-06-09T16:58:03.834056Z","shell.execute_reply.started":"2021-06-09T16:57:59.007397Z","shell.execute_reply":"2021-06-09T16:58:03.833211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\nprint('Training shape : {}'.format(train.shape))\nprint('Testing shape : {}'.format(test.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:58:03.83521Z","iopub.execute_input":"2021-06-09T16:58:03.83542Z","iopub.status.idle":"2021-06-09T16:58:03.877203Z","shell.execute_reply.started":"2021-06-09T16:58:03.835399Z","shell.execute_reply":"2021-06-09T16:58:03.875795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:58:03.878533Z","iopub.execute_input":"2021-06-09T16:58:03.878832Z","iopub.status.idle":"2021-06-09T16:58:03.894117Z","shell.execute_reply.started":"2021-06-09T16:58:03.878809Z","shell.execute_reply":"2021-06-09T16:58:03.893447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Feature Engg Prior to Cleaning :  to a generate some relevant features and then clenaing the irrelevnt data","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:58:03.894992Z","iopub.execute_input":"2021-06-09T16:58:03.895278Z","iopub.status.idle":"2021-06-09T16:58:03.905435Z","shell.execute_reply.started":"2021-06-09T16:58:03.895254Z","shell.execute_reply":"2021-06-09T16:58:03.904788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Feature Engg !!\n\nprint('* ------------------------- for train data-------------------------------- *')\n\n# the relevance of these feature will be tested on the later stages !!\n\ntrain['word_count'] = train[\"excerpt\"].apply(lambda x: len(str(x).split(\" \")))\ntrain['char_count'] = train[\"excerpt\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\ntrain[\"num_unique_words\"] = train[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\ntrain[\"num_stopwords\"] = train[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntrain[\"num_punctuations\"] =train['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntrain[\"num_words_upper\"] = train[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntrain[\"num_words_title\"] = train[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntrain[\"mean_word_len\"] = train[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\nprint('* ------------------------- for test data-------------------------------- *')\n\ntest['word_count'] = test[\"excerpt\"].apply(lambda x: len(str(x).split(\" \")))\ntest['char_count'] = test[\"excerpt\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\ntest[\"num_unique_words\"] = test[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_stopwords\"] = test[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"num_punctuations\"] =test['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest[\"num_words_upper\"] = test[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_title\"] = test[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"mean_word_len\"] = test[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:58:05.179744Z","iopub.execute_input":"2021-06-09T16:58:05.18015Z","iopub.status.idle":"2021-06-09T16:58:05.883037Z","shell.execute_reply.started":"2021-06-09T16:58:05.180126Z","shell.execute_reply":"2021-06-09T16:58:05.88251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:58:05.884098Z","iopub.execute_input":"2021-06-09T16:58:05.884399Z","iopub.status.idle":"2021-06-09T16:58:05.897409Z","shell.execute_reply.started":"2021-06-09T16:58:05.884362Z","shell.execute_reply":"2021-06-09T16:58:05.896591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Cleaning !!\n\ndef is_valid_date(date_str): # sub function\n    try:\n        parser.parse(date_str)\n        return True\n    except:\n#         print(date_str)\n        return False\n    \ndef date_removal(data):\n    new_list = [' '.join([w for w in line.split() if not is_valid_date(w)]) for line in data]\n    return (new_list[0])\n\ndef stemmer_and_stopWord(doc):\n    \n    doc= nlp(doc)\n    token_list = []\n    for token in doc:\n        lemma = token.lemma_\n        if lemma == '-PRON-' or lemma == 'be':\n            lemma = token.text\n        token_list.append(lemma)\n\n    stemmed = token_list\n    \n    # Create list of word tokens after removing stopwords\n    \n    filtered_sentence =[] \n    for word in stemmed[:100]:\n        lexeme = nlp.vocab[word]\n        if lexeme.is_stop == False:\n            filtered_sentence.append(word)\n    return (' '.join(filtered_sentence))\n\ndef normaliz(filtered_sentence):\n        \n    words = [str(word).lower() for word in filtered_sentence.split()]\n    return  ' '.join(words[:100])\n\n# For number removal ~\n\ndef numbers_removal(data):\n    s = [data]\n    result = ''.join([i for i in s if not i.isdigit()])\n    return (result)\n# For punchuation & double white spaces ~\n\ndef punch_removal(words):\n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in [words]]\n    return re.sub(' +', ' ', stripped[:100][0])\n\n\ndef cleaner(data):\n    string = [data]\n    string = date_removal(string)\n    string = numbers_removal(string)\n    string = punch_removal(string)\n    string = stemmer_and_stopWord(string)\n    string = normaliz(string)\n    return string\n\nprint('* --------------- for train data --------------- *')\n\nlist=[]\nfor i in train['excerpt']: # cleansing !!\n    list.append(cleaner(i))\n\ntrain['excerpt']=pd.Series(list) # updating the attributee !!\n\nprint('* --------------- for test data --------------- *')\n\nlist=[]\nfor i in test['excerpt']: # cleansing !!\n    list.append(cleaner(i))\n\ntest['excerpt']=pd.Series(list) # updating the attributee !!\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:58:06.087433Z","iopub.execute_input":"2021-06-09T16:58:06.087841Z","iopub.status.idle":"2021-06-09T16:59:27.307864Z","shell.execute_reply.started":"2021-06-09T16:58:06.087816Z","shell.execute_reply":"2021-06-09T16:59:27.307352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:59:27.308802Z","iopub.execute_input":"2021-06-09T16:59:27.309093Z","iopub.status.idle":"2021-06-09T16:59:27.323528Z","shell.execute_reply.started":"2021-06-09T16:59:27.309065Z","shell.execute_reply":"2021-06-09T16:59:27.322521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Processing on Text data (Excerpt) : Starts !!","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:59:27.32502Z","iopub.execute_input":"2021-06-09T16:59:27.325263Z","iopub.status.idle":"2021-06-09T16:59:27.353161Z","shell.execute_reply.started":"2021-06-09T16:59:27.325238Z","shell.execute_reply":"2021-06-09T16:59:27.351861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list ofString in Title\ntitles_list = [title for title in train['excerpt']]\n# Collapse the list of strings into a single long string for processing\nbig_title_string = ' '.join(titles_list)\n# converting string into words\ntokens = word_tokenize(big_title_string)\n# Remove non-alphabetic tokens, such as punctuation\nwords = [word.lower() for word in tokens if word.isalpha()]\n# Filter out stopwords, exampe At, for, in , is for getting relevant words\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nwords = [word for word in words if not word in stop_words]\n# Print first 10 words\nwords[:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:59:27.354801Z","iopub.execute_input":"2021-06-09T16:59:27.355074Z","iopub.status.idle":"2021-06-09T16:59:27.934343Z","shell.execute_reply.started":"2021-06-09T16:59:27.355051Z","shell.execute_reply":"2021-06-09T16:59:27.933325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Word2Vec model (trained on an enormous Google corpus)\nmodel = gensim.models.KeyedVectors.load_word2vec_format('../input/googlenewsvectors/GoogleNews-vectors-negative300.bin', binary = True) \n# Check dimension of word vectors\nmodel.vector_size","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:59:27.935282Z","iopub.execute_input":"2021-06-09T16:59:27.93553Z","iopub.status.idle":"2021-06-09T16:59:56.430153Z","shell.execute_reply.started":"2021-06-09T16:59:27.935506Z","shell.execute_reply":"2021-06-09T16:59:56.429411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter the list of vectors to include only those that Word2Vec has a vector for\nvector_list = [model[word] for word in words if word in model.index_to_key]\n# Create a list of the words corresponding to these vectors\nwords_filtered = [word for word in words if word in model.index_to_key]\n# Zip the words together with their vector representations\nword_vec_zip = zip(words_filtered, vector_list)\n# Cast to a dict so we can turn it into a dataframe\nword_vec_dict = dict(word_vec_zip)\ndf = pd.DataFrame.from_dict(word_vec_dict, orient='index')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:59:56.431164Z","iopub.execute_input":"2021-06-09T16:59:56.431415Z","iopub.status.idle":"2021-06-09T17:10:41.473622Z","shell.execute_reply.started":"2021-06-09T16:59:56.431387Z","shell.execute_reply":"2021-06-09T17:10:41.472471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def document_vector(word2vec_model, doc):\n    # remove out-of-vocabulary words\n    doc = [word for word in doc if word in model.index_to_key]\n    return np.mean(model[doc], axis=0)\n\n# Our earlier preprocessing was done when we were dealing only with word vectors\n# Here, we need each document to remain a document \ndef preprocess(text):\n    text = text.lower()\n    doc = word_tokenize(text)\n    doc = [word for word in doc if word not in stop_words]\n    doc = [word for word in doc if word.isalpha()] \n    return doc\n\n# Function that will help us drop documents that have no word vectors in word2vec\ndef has_vector_representation(word2vec_model, doc):\n    \"\"\"check if at least one word of the document is in the\n    word2vec dictionary\"\"\"\n    return not all(word not in word2vec_model.index_to_key for word in doc)\n\n# Filter out documents\ndef filter_docs(corpus, texts, condition_on_doc):\n    \"\"\"\n    Filter corpus and texts given the function condition_on_doc which takes\n    a doc. The document doc is kept if condition_on_doc(doc) is true.\n    \"\"\"\n    number_of_docs = len(corpus)\n\n    if texts is not None:\n        texts = [text for (text, doc) in zip(texts, corpus)\n                 if condition_on_doc(doc)]\n\n    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n\n    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n\n    return (corpus, texts)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:10:41.47497Z","iopub.execute_input":"2021-06-09T17:10:41.475222Z","iopub.status.idle":"2021-06-09T17:10:41.48246Z","shell.execute_reply.started":"2021-06-09T17:10:41.475201Z","shell.execute_reply":"2021-06-09T17:10:41.481703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess the corpus\ncorpus = [preprocess(title) for title in titles_list]\n# Remove docs that don't include any words in W2V's vocab\ncorpus, titles_list = filter_docs(corpus, titles_list, lambda doc: has_vector_representation(model, doc))\n# Filter out any empty docs\ncorpus, titles_list = filter_docs(corpus, titles_list, lambda doc: (len(doc) != 0))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:10:41.484431Z","iopub.execute_input":"2021-06-09T17:10:41.484831Z","iopub.status.idle":"2021-06-09T17:11:07.401664Z","shell.execute_reply.started":"2021-06-09T17:10:41.484801Z","shell.execute_reply":"2021-06-09T17:11:07.399842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize an array for the size of the corpus\nx = []\nfor doc in corpus: # append the vector for each document\n    x.append(document_vector(model, doc))\n    \nX = np.array(x) # list to array","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:11:07.403523Z","iopub.execute_input":"2021-06-09T17:11:07.40388Z","iopub.status.idle":"2021-06-09T17:17:00.683007Z","shell.execute_reply.started":"2021-06-09T17:11:07.403844Z","shell.execute_reply":"2021-06-09T17:17:00.682157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if test.shape[0] > 30:\n    component = 30\nelse:\n    component = test.shape[0]\n\nprint(test.shape[0], component)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:17:00.684075Z","iopub.execute_input":"2021-06-09T17:17:00.684388Z","iopub.status.idle":"2021-06-09T17:17:00.690079Z","shell.execute_reply.started":"2021-06-09T17:17:00.684337Z","shell.execute_reply":"2021-06-09T17:17:00.688816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=component, random_state=10) \n# as a reminder, x is the array with our 300-dimensional vectors\nreduced_vecs = pca.fit_transform(x) \ndf_w_vectors = pd.DataFrame(reduced_vecs) \ndf_w_vectors ['excerpt'] = titles_list \n# Use pd.concat to match original titles with their vectors\nmain_w_vectors = pd.concat ((df_w_vectors, train), axis = 1)\n# Get rid of vectors that could not be matched with the main_df\nmain_w_vectors .dropna(axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:17:00.691226Z","iopub.execute_input":"2021-06-09T17:17:00.691594Z","iopub.status.idle":"2021-06-09T17:17:00.893187Z","shell.execute_reply.started":"2021-06-09T17:17:00.691561Z","shell.execute_reply":"2021-06-09T17:17:00.892381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop all non-numeric, non-dummy columns, for feeding into the models\ncols_to_drop = ['url_legal', 'license', 'excerpt', 'id']\ndata = main_w_vectors.drop(columns = cols_to_drop) ","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:17:00.894504Z","iopub.execute_input":"2021-06-09T17:17:00.894973Z","iopub.status.idle":"2021-06-09T17:17:00.901624Z","shell.execute_reply.started":"2021-06-09T17:17:00.894939Z","shell.execute_reply":"2021-06-09T17:17:00.90035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.columns)\ndata.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:17:00.902905Z","iopub.execute_input":"2021-06-09T17:17:00.903235Z","iopub.status.idle":"2021-06-09T17:17:00.938243Z","shell.execute_reply.started":"2021-06-09T17:17:00.903199Z","shell.execute_reply":"2021-06-09T17:17:00.937633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## ----------- Modeling ------------ ##","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingRegressor,AdaBoostRegressor,RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import ElasticNet,SGDRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nX=data.drop(['target', 'standard_error'], axis=1)\nY=data['target']\n\n\nfre = RandomForestRegressor(max_depth=8,n_estimators=10,n_jobs=2)\nfre.fit(X,Y.values.ravel())\nprint(fre.feature_importances_) \nfeat_importances = pd.Series(fre.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:00.421022Z","iopub.execute_input":"2021-06-09T17:24:00.42136Z","iopub.status.idle":"2021-06-09T17:24:00.960516Z","shell.execute_reply.started":"2021-06-09T17:24:00.421331Z","shell.execute_reply":"2021-06-09T17:24:00.958905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences \nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\n\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dropout, GRU, Activation, Embedding, Bidirectional,SpatialDropout1D, BatchNormalization, Conv1D, MaxPool1D","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:02.134892Z","iopub.execute_input":"2021-06-09T17:24:02.135185Z","iopub.status.idle":"2021-06-09T17:24:06.659861Z","shell.execute_reply.started":"2021-06-09T17:24:02.135161Z","shell.execute_reply":"2021-06-09T17:24:06.658777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape,Y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:06.661057Z","iopub.execute_input":"2021-06-09T17:24:06.661334Z","iopub.status.idle":"2021-06-09T17:24:06.665952Z","shell.execute_reply.started":"2021-06-09T17:24:06.661301Z","shell.execute_reply":"2021-06-09T17:24:06.664929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\n\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\nX_train=X_train.reshape(X_train.shape[0], X_train.shape[1],1) # it needs to be a nummpy array\nX_test=X_test.reshape(X_test.shape[0], X_test.shape[1],1)\n\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:06.667499Z","iopub.execute_input":"2021-06-09T17:24:06.667767Z","iopub.status.idle":"2021-06-09T17:24:06.699888Z","shell.execute_reply.started":"2021-06-09T17:24:06.667739Z","shell.execute_reply":"2021-06-09T17:24:06.698562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convolution !!\n\nepochs=10\ncat_model=Sequential()\n\ncat_model.add(Conv1D(filters=32, activation='relu', kernel_size=2, input_shape=(X_test.shape[1],1)))\ncat_model.add(Conv1D(filters=64, activation='relu', kernel_size=2))\ncat_model.add(Dropout(0.2))\ncat_model.add(Flatten())\ncat_model.add(Dense(128, activation='relu'))\ncat_model.add(Dropout(0.2))\ncat_model.add(Dense(1,activation='linear'))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:39.861574Z","iopub.execute_input":"2021-06-09T17:24:39.861841Z","iopub.status.idle":"2021-06-09T17:24:39.909303Z","shell.execute_reply.started":"2021-06-09T17:24:39.861819Z","shell.execute_reply":"2021-06-09T17:24:39.908171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_model.summary()\n\ncat_model.compile( optimizer=Adam(lr=0.05), loss='mae', metrics=['mae'])\nhistory=cat_model.fit(X_train,y_train, epochs=epochs, validation_data=(X_test,y_test), verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:43.546979Z","iopub.execute_input":"2021-06-09T17:24:43.547287Z","iopub.status.idle":"2021-06-09T17:24:46.096622Z","shell.execute_reply.started":"2021-06-09T17:24:43.54726Z","shell.execute_reply":"2021-06-09T17:24:46.095594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accr = cat_model.evaluate(X_test,y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:48.433971Z","iopub.execute_input":"2021-06-09T17:24:48.434265Z","iopub.status.idle":"2021-06-09T17:24:48.496218Z","shell.execute_reply.started":"2021-06-09T17:24:48.43424Z","shell.execute_reply":"2021-06-09T17:24:48.495231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--------------------------------------------------------------#","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:49.696317Z","iopub.execute_input":"2021-06-09T17:24:49.696637Z","iopub.status.idle":"2021-06-09T17:24:49.700172Z","shell.execute_reply.started":"2021-06-09T17:24:49.696614Z","shell.execute_reply":"2021-06-09T17:24:49.699399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## test data processing !!","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:50.50379Z","iopub.execute_input":"2021-06-09T17:24:50.50428Z","iopub.status.idle":"2021-06-09T17:24:50.507754Z","shell.execute_reply.started":"2021-06-09T17:24:50.50425Z","shell.execute_reply":"2021-06-09T17:24:50.506781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list ofString in Title\ntitles_list = [title for title in test['excerpt']]\n# Collapse the list of strings into a single long string for processing\nbig_title_string = ' '.join(titles_list)\nfrom nltk.tokenize import word_tokenize\n# converting string into words\ntokens = word_tokenize(big_title_string)\n# Remove non-alphabetic tokens, such as punctuation\nwords = [word.lower() for word in tokens if word.isalpha()]\n# Filter out stopwords, exampe At, for, in , is for getting relevant words\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nwords = [word for word in words if not word in stop_words]\n\n# Filter the list of vectors to include only those that Word2Vec has a vector for\nvector_list = [model[word] for word in words if word in model.index_to_key]\n# Create a list of the words corresponding to these vectors\nwords_filtered = [word for word in words if word in model.index_to_key]\n# Zip the words together with their vector representations\nword_vec_zip = zip(words_filtered, vector_list)\n# Cast to a dict so we can turn it into a dataframe\nword_vec_dict = dict(word_vec_zip)\ndf = pd.DataFrame.from_dict(word_vec_dict, orient='index')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:51.022897Z","iopub.execute_input":"2021-06-09T17:24:51.023252Z","iopub.status.idle":"2021-06-09T17:24:53.014002Z","shell.execute_reply.started":"2021-06-09T17:24:51.023223Z","shell.execute_reply":"2021-06-09T17:24:53.01294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter the list of vectors to include only those that Word2Vec has a vector for\nvector_list = [model[word] for word in words if word in model.index_to_key]\n# Create a list of the words corresponding to these vectors\nwords_filtered = [word for word in words if word in model.index_to_key]\n# Zip the words together with their vector representations\nword_vec_zip = zip(words_filtered, vector_list)\n# Cast to a dict so we can turn it into a dataframe\nword_vec_dict = dict(word_vec_zip)\ndf = pd.DataFrame.from_dict(word_vec_dict, orient='index')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:53.01553Z","iopub.execute_input":"2021-06-09T17:24:53.015756Z","iopub.status.idle":"2021-06-09T17:24:54.958797Z","shell.execute_reply.started":"2021-06-09T17:24:53.015733Z","shell.execute_reply":"2021-06-09T17:24:54.958281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess the corpus\ncorpus = [preprocess(title) for title in titles_list]\n# Remove docs that don't include any words in W2V's vocab\ncorpus, titles_list = filter_docs(corpus, titles_list, lambda doc: has_vector_representation(model, doc))\n# Filter out any empty docs\ncorpus, titles_list = filter_docs(corpus, titles_list, lambda doc: (len(doc) != 0))\n\n# Initialize an array for the size of the corpus\nx = []\nfor doc in corpus: # append the vector for each document\n    x.append(document_vector(model, doc))\nX = np.array(x) # list to array","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:54.959852Z","iopub.execute_input":"2021-06-09T17:24:54.960131Z","iopub.status.idle":"2021-06-09T17:24:56.044093Z","shell.execute_reply.started":"2021-06-09T17:24:54.960109Z","shell.execute_reply":"2021-06-09T17:24:56.042822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=component, random_state=10) \nreduced_vecs = pca.fit_transform(x) \ndf_w_vectors = pd.DataFrame(reduced_vecs) \ndf_w_vectors [ 'excerpt'] = titles_list \nmain_w_vectors = pd.concat ((df_w_vectors, test), axis = 1)\n\n# Drop all non-numeric, non-dummy columns, for feeding into the models\ncols_to_drop = ['url_legal', 'license', 'excerpt']\n\ndata_only_df=pd.DataFrame()\ndata_only_df = main_w_vectors.drop(columns = cols_to_drop)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:56.045652Z","iopub.execute_input":"2021-06-09T17:24:56.045976Z","iopub.status.idle":"2021-06-09T17:24:56.05775Z","shell.execute_reply.started":"2021-06-09T17:24:56.045944Z","shell.execute_reply":"2021-06-09T17:24:56.056855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_only_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:56.060401Z","iopub.execute_input":"2021-06-09T17:24:56.060791Z","iopub.status.idle":"2021-06-09T17:24:56.080418Z","shell.execute_reply.started":"2021-06-09T17:24:56.060757Z","shell.execute_reply":"2021-06-09T17:24:56.079447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title=data_only_df.drop(['id'], axis=1)\ntitle = scaler.fit_transform(title)\n\ntitle=title.reshape(title.shape[0], title.shape[1],1) # it needs to be a nummpy array\n# headline=data_only_df['target']\nsubmission=pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:24:57.462264Z","iopub.execute_input":"2021-06-09T17:24:57.462606Z","iopub.status.idle":"2021-06-09T17:24:57.474934Z","shell.execute_reply.started":"2021-06-09T17:24:57.46258Z","shell.execute_reply":"2021-06-09T17:24:57.473761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predtitle=cat_model.predict(title)\ny_predtitle","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:25:11.395022Z","iopub.execute_input":"2021-06-09T17:25:11.39534Z","iopub.status.idle":"2021-06-09T17:25:11.434833Z","shell.execute_reply.started":"2021-06-09T17:25:11.395317Z","shell.execute_reply":"2021-06-09T17:25:11.433633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\nsubmission['target'] = y_predtitle\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:25:13.152727Z","iopub.execute_input":"2021-06-09T17:25:13.153069Z","iopub.status.idle":"2021-06-09T17:25:13.171801Z","shell.execute_reply.started":"2021-06-09T17:25:13.15304Z","shell.execute_reply":"2021-06-09T17:25:13.171089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:25:15.041486Z","iopub.execute_input":"2021-06-09T17:25:15.041991Z","iopub.status.idle":"2021-06-09T17:25:15.050933Z","shell.execute_reply.started":"2021-06-09T17:25:15.041958Z","shell.execute_reply":"2021-06-09T17:25:15.04931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}