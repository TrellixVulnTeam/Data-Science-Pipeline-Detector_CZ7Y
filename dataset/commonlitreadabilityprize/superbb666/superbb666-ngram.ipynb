{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntest_df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\ntrain_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ngram model\nimport math\nfrom collections import Counter\nfrom nltk.util import ngrams  # This is the ngram magic.\n\n# calculate cos\ndef cosine_similarity_ngrams(a, b):\n    if not a or not b:\n        return 0.0\n    vec1 = Counter(a)\n    vec2 = Counter(b)\n\n    intersection = set(vec1.keys()) & set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n\n    sum1 = sum([vec1[x] ** 2 for x in vec1.keys()])\n    sum2 = sum([vec2[x] ** 2 for x in vec2.keys()])\n    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n\n    if not denominator:\n        return 0.0\n    return float(numerator) / denominator\n\n# get ngram\ndef get_character_level_tuples_nosentences(text, ngram=6):\n    if not text:\n        return []\n    text = text.lower()\n\n    ng = ngrams(list(text), ngram)\n    return list(ng)\n\n# calculate score\ndef test_similarity(text_a, text_b, ngram=6):\n    gram_a = get_character_level_tuples_nosentences(text_a, ngram)\n    gram_b = get_character_level_tuples_nosentences(text_b, ngram)\n    return cosine_similarity_ngrams(gram_a, gram_b)\n\n# based on ngramï¼Œcalculate score n times\ndef test_accum_similarity(text_a, text_b, low_ngram=4, high_ngram=10):\n    score = 0\n    for n in range(high_ngram, low_ngram-1, -1):  # from 10 to 4\n        s = test_similarity(text_a, text_b, n)\n        score = score * 10 + s\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the best score's index\ndef get_score_index(raw_text, eval_text):\n    score_list = [test_accum_similarity(text, eval_text) for text in raw_text]\n    return sorted(list(enumerate(score_list)), key=lambda x:-x[1])[0][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntest_score = [0] * test_df.shape[0]\nfor i, eval_text in tqdm(enumerate(test_df['excerpt'])):\n    ind = get_score_index(train_df['excerpt'], eval_text)\n    test_score[i] = train_df['target'][ind]\ntest_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['target'] = test_score\ntest_df[['id','target']].to_csv('submission.csv', index=False)\ntest_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}