{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/25914/logos/header.png)\n\nThis notebook mainly covers basic explorations of the data and a baseline Linear Model. Since the size of the test data is very less in this case, nothing was performed on it.\n\nAt the end of the notebook please feel free to share your thoughts and ideas along with feedback for the overall kernel\n\nFollowing points are covered in this notebook:\n\ni) The EDA part\n\nii) Two classes easy and difficult have been created based on the readability score and EDA has been done to make it more simple. Also baseline classification models have been developed for comparison\n\niii) Last and final a regression model has been created to predict the actual readability scores (to be added)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:46:03.715969Z","iopub.execute_input":"2021-05-21T15:46:03.716661Z","iopub.status.idle":"2021-05-21T15:46:04.449477Z","shell.execute_reply.started":"2021-05-21T15:46:03.716599Z","shell.execute_reply":"2021-05-21T15:46:04.448093Z"}}},{"cell_type":"markdown","source":"#### Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\ncolor = sns.color_palette()\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nimport plotly.tools as tls\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nos.listdir(\"../input/commonlitreadabilityprize\")\nfrom nltk.corpus import stopwords\nimport string\neng_stopwords = set(stopwords.words(\"english\"))\npd.options.mode.chained_assignment = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-01T18:45:44.748115Z","iopub.execute_input":"2021-06-01T18:45:44.748561Z","iopub.status.idle":"2021-06-01T18:45:46.685587Z","shell.execute_reply.started":"2021-06-01T18:45:44.748476Z","shell.execute_reply":"2021-06-01T18:45:46.681775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ndf.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T18:45:46.6884Z","iopub.execute_input":"2021-06-01T18:45:46.689246Z","iopub.status.idle":"2021-06-01T18:45:46.797877Z","shell.execute_reply.started":"2021-06-01T18:45:46.689197Z","shell.execute_reply":"2021-06-01T18:45:46.796835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Simple understanding of the Data","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:45:46.807926Z","iopub.execute_input":"2021-06-01T18:45:46.808397Z","iopub.status.idle":"2021-06-01T18:45:46.816196Z","shell.execute_reply.started":"2021-06-01T18:45:46.808344Z","shell.execute_reply":"2021-06-01T18:45:46.814813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:45:46.818437Z","iopub.execute_input":"2021-06-01T18:45:46.819031Z","iopub.status.idle":"2021-06-01T18:45:46.831852Z","shell.execute_reply.started":"2021-06-01T18:45:46.818987Z","shell.execute_reply":"2021-06-01T18:45:46.830219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round((df.isnull().sum()/df.shape[0])*100, 2) # % of null values in each column","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:45:46.833983Z","iopub.execute_input":"2021-06-01T18:45:46.834458Z","iopub.status.idle":"2021-06-01T18:45:46.863526Z","shell.execute_reply.started":"2021-06-01T18:45:46.834413Z","shell.execute_reply":"2021-06-01T18:45:46.862437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing unnecessary columns\ndf.drop(['url_legal', 'license'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:45:46.86515Z","iopub.execute_input":"2021-06-01T18:45:46.865576Z","iopub.status.idle":"2021-06-01T18:45:46.872744Z","shell.execute_reply.started":"2021-06-01T18:45:46.865534Z","shell.execute_reply":"2021-06-01T18:45:46.871048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### EDA","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(df['excerpt'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T18:45:46.877511Z","iopub.execute_input":"2021-06-01T18:45:46.878013Z","iopub.status.idle":"2021-06-01T18:45:47.387779Z","shell.execute_reply.started":"2021-06-01T18:45:46.877967Z","shell.execute_reply":"2021-06-01T18:45:47.386738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Checking distribution of target variable","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfig = px.histogram(df, x=\"target\")\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T18:45:47.390102Z","iopub.execute_input":"2021-06-01T18:45:47.390521Z","iopub.status.idle":"2021-06-01T18:45:51.443745Z","shell.execute_reply.started":"2021-06-01T18:45:47.390483Z","shell.execute_reply":"2021-06-01T18:45:51.442689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['excerpt_length']=df['excerpt'].apply(len)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T18:45:51.445099Z","iopub.execute_input":"2021-06-01T18:45:51.445406Z","iopub.status.idle":"2021-06-01T18:45:51.453141Z","shell.execute_reply.started":"2021-06-01T18:45:51.445376Z","shell.execute_reply":"2021-06-01T18:45:51.451954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Check length of passage with level of ease of reading passages","metadata":{}},{"cell_type":"code","source":"# create easy readability flag; 0 (difficult) - when target <= 0, 1 (easy) - when target > 0\ndf['is_easy_excerpt'] = np.where(df['target'] <= 0, 0, 1)\n\nsns.set(font_scale=1.0)\n\ng = sns.FacetGrid(df,col='is_easy_excerpt',size=5)\ng.map(plt.hist,'excerpt_length')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:45:51.454894Z","iopub.execute_input":"2021-06-01T18:45:51.455704Z","iopub.status.idle":"2021-06-01T18:45:52.240433Z","shell.execute_reply.started":"2021-06-01T18:45:51.455658Z","shell.execute_reply":"2021-06-01T18:45:52.239391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt_srs = df['is_easy_excerpt'].astype(str).value_counts().head()\ntrace = go.Bar(\n    x=cnt_srs.index[::-1],\n    y=cnt_srs.values[::-1],\n    orientation = 'v',\n    marker=dict(\n        color=cnt_srs.values[::-1],\n        colorscale = 'agsunset',\n        reversescale = True\n    ),\n)\n\nlayout = dict(\n    title='Easy v Difficult Passages distribution',\n    )\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"Readability\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T18:45:52.241943Z","iopub.execute_input":"2021-06-01T18:45:52.242338Z","iopub.status.idle":"2021-06-01T18:45:52.299861Z","shell.execute_reply.started":"2021-06-01T18:45:52.242305Z","shell.execute_reply":"2021-06-01T18:45:52.298733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0 corresponds to easy excerpts and 1 to difficult ","metadata":{}},{"cell_type":"markdown","source":"#### Creating Meta Features","metadata":{}},{"cell_type":"code","source":"df[\"num_words\"] = df[\"excerpt\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ndf[\"num_unique_words\"] = df[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ndf[\"num_chars\"] = df[\"excerpt\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ndf[\"num_stopwords\"] = df[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\ndf[\"num_punctuations\"] =df['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ndf[\"num_words_upper\"] = df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ndf[\"num_words_title\"] = df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ndf[\"mean_word_len\"] = df[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T18:45:52.301492Z","iopub.execute_input":"2021-06-01T18:45:52.302156Z","iopub.status.idle":"2021-06-01T18:45:53.071749Z","shell.execute_reply.started":"2021-06-01T18:45:52.302112Z","shell.execute_reply":"2021-06-01T18:45:53.070651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### No. of words by easy of reading","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.violinplot(x='is_easy_excerpt', y='num_words', data=df)\nplt.xlabel('Passage Easy to Read', fontsize=12)\nplt.ylabel('Number of words in text', fontsize=12)\nplt.title(\"Number of words by ease in reading\", fontsize=15)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T18:45:53.073253Z","iopub.execute_input":"2021-06-01T18:45:53.073853Z","iopub.status.idle":"2021-06-01T18:45:53.315568Z","shell.execute_reply.started":"2021-06-01T18:45:53.073771Z","shell.execute_reply":"2021-06-01T18:45:53.314207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### No. of punctuations by easy of reading","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.violinplot(x='is_easy_excerpt', y='num_punctuations', data=df)\nplt.xlabel('Passage Easy to Read', fontsize=12)\nplt.ylabel('Number of puntuations in text', fontsize=12)\nplt.title(\"Number of punctuations by ease in reading\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:45:53.317467Z","iopub.execute_input":"2021-06-01T18:45:53.317929Z","iopub.status.idle":"2021-06-01T18:45:53.548058Z","shell.execute_reply.started":"2021-06-01T18:45:53.317882Z","shell.execute_reply":"2021-06-01T18:45:53.546703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Exploration w.r.t. Standard Error","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=df[df['is_easy_excerpt'] == 1]['standard_error'], name='Easy Exceprt'))\nfig.add_trace(go.Histogram(x=df[df['is_easy_excerpt'] == 0]['standard_error'], name='Tough Excerpt'))\n\n# Overlay both histograms\nfig.update_layout(barmode='stack', title_text='Standard Error Comparison between Easy and Difficult Excerpt', \n    xaxis_title_text='Standard Error Distribution', \n    yaxis_title_text='Count')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T18:45:53.549984Z","iopub.execute_input":"2021-06-01T18:45:53.550428Z","iopub.status.idle":"2021-06-01T18:45:53.600643Z","shell.execute_reply.started":"2021-06-01T18:45:53.550373Z","shell.execute_reply":"2021-06-01T18:45:53.599458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Modeling Part for Binary Classification","metadata":{}},{"cell_type":"code","source":"df.head(2)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-01T18:45:53.602224Z","iopub.execute_input":"2021-06-01T18:45:53.602905Z","iopub.status.idle":"2021-06-01T18:45:53.626261Z","shell.execute_reply.started":"2021-06-01T18:45:53.602858Z","shell.execute_reply":"2021-06-01T18:45:53.62475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating Tf-Idf vectors and EDA on the excerpt text","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndf['excerpt_id'] = df['is_easy_excerpt'].factorize()[0]\ncategory_id_df = df[['is_easy_excerpt', 'excerpt_id']].drop_duplicates().sort_values('excerpt_id')\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['excerpt_id', 'is_easy_excerpt']].values)\n\ntfidf = TfidfVectorizer(sublinear_tf=True, \n                        norm='l2', \n                        encoding='latin-1', \n                        ngram_range=(1, 2), \n                        stop_words='english')\n\nfeatures = tfidf.fit_transform(df.excerpt)\nlabels = df.is_easy_excerpt\nfeatures.shape\n\nfrom sklearn.feature_selection import chi2\nimport numpy as np\n\nN = 2\nfor Product, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"# '{}':\".format(Product))\n  print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:])))\n  print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:])))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:45:53.628392Z","iopub.execute_input":"2021-06-01T18:45:53.629008Z","iopub.status.idle":"2021-06-01T18:45:57.057712Z","shell.execute_reply.started":"2021-06-01T18:45:53.628912Z","shell.execute_reply":"2021-06-01T18:45:57.056568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import cross_val_score\n\n\nmodels = [\n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:45:57.061141Z","iopub.execute_input":"2021-06-01T18:45:57.061435Z","iopub.status.idle":"2021-06-01T18:46:12.307874Z","shell.execute_reply.started":"2021-06-01T18:45:57.061405Z","shell.execute_reply":"2021-06-01T18:46:12.306825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfVectorizer(sublinear_tf=True, \n                        norm='l2', \n                        encoding='latin-1', \n                        ngram_range=(3, 6), \n                        stop_words='english')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T18:46:12.309506Z","iopub.execute_input":"2021-06-01T18:46:12.310081Z","iopub.status.idle":"2021-06-01T18:46:12.327774Z","shell.execute_reply.started":"2021-06-01T18:46:12.31002Z","shell.execute_reply":"2021-06-01T18:46:12.326331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntest.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-01T18:46:12.330106Z","iopub.execute_input":"2021-06-01T18:46:12.330693Z","iopub.status.idle":"2021-06-01T18:46:12.36396Z","shell.execute_reply.started":"2021-06-01T18:46:12.330644Z","shell.execute_reply":"2021-06-01T18:46:12.36295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Linear Regression Model","metadata":{}},{"cell_type":"code","source":"features_train = tfidf.fit_transform(df.excerpt)\nfeatures_test = tfidf.transform(test.excerpt)\n\nlabels = df['target']\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\n\nmodel = SVR(C=10, kernel='rbf', gamma='auto')\n\nX_train, X_test, y_train, y_test = train_test_split(features_train, labels, test_size=0.25, random_state=0)\nmodel.fit(X_train, y_train)\ny_repeat = model.predict(X_train)\ny_val = model.predict(X_test)\ny_preds = model.predict(features_test)\n\nfrom sklearn.metrics import mean_squared_error\n\nprint('The RMSE for validation data is:', np.sqrt(mean_squared_error(y_repeat, y_train)))\nprint('The RMSE for validation data is:', np.sqrt(mean_squared_error(y_val, y_test)))\n\n# The RMSE values prove that the model is not overfitting, however it needs a lot of improvement\n\n# test['target'] = y_preds\n# test[['id', 'target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:46:12.365473Z","iopub.execute_input":"2021-06-01T18:46:12.365962Z","iopub.status.idle":"2021-06-01T18:46:29.695685Z","shell.execute_reply.started":"2021-06-01T18:46:12.365907Z","shell.execute_reply":"2021-06-01T18:46:29.694439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The development part is inspired from this notebook: https://www.kaggle.com/duttadebadri/clrp-roberta-linear-svc/data","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\nimport random\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom tqdm import tqdm\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n                                      ReduceLROnPlateau)\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification,get_constant_schedule_with_warmup)\n\ntrain_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\ntarget = train_data['target'].to_numpy()\n\n#for kfold  \nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\nbins = train_data.bins.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\nconfig = {\n    'batch_size':32,\n    'max_len':512,\n    'seed':23,\n}\n\ndef seed_everything(seed=23):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\nclass CLRPDataset(nn.Module):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)  \n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    MODEL_PATH = path\n    model = AutoModel.from_pretrained(MODEL_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    model.to(device)\n    model.eval()\n\n    ds = CLRPDataset(df,tokenizer,config['max_len'])\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs[0][:,0].detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)\n\ntrain_embeddings1 =  get_embeddings(train_data,'../input/modelf1')\ntest_embeddings1 = get_embeddings(test_data,'../input/modelf1')\n\ntrain_embeddings2 =  get_embeddings(train_data,'../input/modelf2')\ntest_embeddings2 = get_embeddings(test_data,'../input/modelf2')\n\ntrain_embeddings3 =  get_embeddings(train_data,'../input/modelf3')\ntest_embeddings3 = get_embeddings(test_data,'../input/modelf3')\n\ntrain_embeddings4 =  get_embeddings(train_data,'../input/modelf4')\ntest_embeddings4 = get_embeddings(test_data,'../input/modelf4')\n\ntrain_embeddings5 =  get_embeddings(train_data,'../input/modelf5')\ntest_embeddings5 = get_embeddings(test_data,'../input/modelf5')\n\ndef get_preds_svm(X,y,X_test,bins=bins,nfolds=5,C=10,kernel='rbf'):\n    kfold = StratifiedKFold(n_splits=nfolds)\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        model = SVR()\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)/nfolds\n\nsvm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\nsvm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\nsvm_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3)\nsvm_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\nsvm_preds5 = get_preds_svm(train_embeddings5,target,test_embeddings5)\n\nsvm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5)/5\n\nsample.target = svm_preds\nsample.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:46:29.699271Z","iopub.execute_input":"2021-06-01T18:46:29.699613Z","iopub.status.idle":"2021-06-01T18:52:49.334617Z","shell.execute_reply.started":"2021-06-01T18:46:29.699582Z","shell.execute_reply":"2021-06-01T18:52:49.333217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}