{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n\n* Pytorch DistilBert training code.\n* Inference notebook is [here](https://www.kaggle.com/snnclsr/commonlit-pytorch-distilbert-inference/).\n\n\nIf this notebook is helpful, feel free to upvote :)\n\n**Some of the parts of this notebook taken from [Y.Nakama](https://www.kaggle.com/yasufuminakama)'s notebooks. Please also check his notebooks as well from [here](https://www.kaggle.com/yasufuminakama/code)**","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import DistilBertTokenizer, DistilBertModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"OUTPUT_DIR = \"./\"\nBASE_DATA_PATH = Path(\"../input/commonlitreadabilityprize/\")\n\n!ls {BASE_DATA_PATH}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(BASE_DATA_PATH / \"train.csv\")\ndf_test = pd.read_csv(BASE_DATA_PATH / \"test.csv\")\ndf_sub = pd.read_csv(BASE_DATA_PATH / \"sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.target.max(), df_train.target.min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\n\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n        \nLOGGER = init_logger()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CommonLitDataset(Dataset):\n    \n    def __init__(self, df, tokenizer, max_length):\n    \n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        tokenized_input = self.tokenizer(row.excerpt, return_tensors=\"pt\", \n                                         max_length=self.max_length, \n                                         padding=\"max_length\", truncation=True)\n        return {\n            \"ids\": tokenized_input[\"input_ids\"][0],\n            \"masks\": tokenized_input[\"attention_mask\"][0],\n            \"targets\": torch.tensor(row.target).float()\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class TextRegressionModel(nn.Module):\n    \n    def __init__(self, model_name, dropout_p=0.1):\n        super(TextRegressionModel, self).__init__()\n        \n        self.model = AutoModel.from_pretrained(model_name)\n        self.features = nn.Linear(768, 768)\n        self.dropout = nn.Dropout(dropout_p)\n        self.out = nn.Linear(768, 1)\n        \n    def forward(self, input_ids, attention_mask):\n        \n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        output = F.relu(self.features(output.last_hidden_state[:, 0]))\n        output = self.dropout(output)\n        output = self.out(output)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"def train_step(model, criterion, optimizer, data_loader, epoch, device=device):\n    \n    batch_time = AverageMeter()\n    data_time  = AverageMeter()\n    train_loss = AverageMeter()\n    model.train()\n    \n    start = end = time.time()\n    \n    for step, batch in enumerate(data_loader):\n        data_time.update(time.time() - end)\n        \n        input_ids = batch[\"ids\"].to(device)\n        attention_masks = batch[\"masks\"].to(device)\n        targets = batch[\"targets\"].to(device)\n        bs = input_ids.size(0)\n        \n        output = model(input_ids, attention_masks)\n        loss = criterion(output.squeeze(1), targets)\n        train_loss.update(loss.item(), bs)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        if step % CFG.print_freq == 0 or step == (len(data_loader) - 1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(\n                      epoch+1, step, len(data_loader), batch_time=batch_time,\n                      data_time=data_time, loss=train_loss,\n                      remain=timeSince(start, float(step+1)/len(data_loader)))\n                 )\n\n    return train_loss.avg\n        \n    \ndef eval_step(model, criterion, data_loader, epoch, device=device):\n    \n    batch_time = AverageMeter()\n    data_time  = AverageMeter()\n    eval_loss = AverageMeter()\n    model.eval()\n    \n    start = end = time.time()\n    \n    for step, batch in enumerate(data_loader):\n        data_time.update(time.time() - end)\n        \n        input_ids = batch[\"ids\"].to(device)\n        attention_masks = batch[\"masks\"].to(device)\n        targets = batch[\"targets\"].to(device)\n        bs = input_ids.size(0)\n        \n        output = model(input_ids, attention_masks)\n        loss = criterion(output.squeeze(1), targets)\n        eval_loss.update(loss.item(), bs)\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        if step % CFG.print_freq == 0 or step == (len(data_loader) - 1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(\n                      epoch+1, step, len(data_loader), batch_time=batch_time,\n                      data_time=data_time, loss=eval_loss,\n                      remain=timeSince(start, float(step+1)/len(data_loader)))\n                 )\n\n    return eval_loss.avg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    model_name = \"distilbert-base-cased\"\n    \n    max_length = 256\n    dropout_p = 0.5\n    batch_size = 32\n    n_epochs = 5\n    weight_decay = 1e-6\n    lr = 3e-4\n    min_lr = 1e-6\n    scheduler = \"CosineAnnealingLR\"\n    T_max = 10\n    seed = 42\n    n_folds = 5    \n    print_freq = 50\n    num_workers = 4\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"def train_loop(folds, fold):\n    \n    train_index = folds[folds[\"fold\"] != fold].index\n    valid_index = folds[folds[\"fold\"] == fold].index\n    \n    train_folds = folds.loc[train_index].reset_index(drop=True)\n    valid_folds = folds.loc[valid_index].reset_index(drop=True)\n    \n    tokenizer = DistilBertTokenizer.from_pretrained(CFG.model_name)\n    tokenizer.save_pretrained(f\"{CFG.model_name}_tokenizer\")\n    \n    train_dataset = CommonLitDataset(df=train_folds, tokenizer=tokenizer, max_length=CFG.max_length)\n    valid_dataset = CommonLitDataset(df=valid_folds, tokenizer=tokenizer, max_length=CFG.max_length)\n    \n    train_data_loader = DataLoader(train_dataset, \n                                   batch_size=CFG.batch_size, \n                                   shuffle=True, \n                                   num_workers=CFG.num_workers, \n                                   pin_memory=True)\n    valid_data_loader = DataLoader(valid_dataset, \n                                   batch_size=CFG.batch_size, \n                                   shuffle=False, \n                                   num_workers=CFG.num_workers, \n                                   pin_memory=True)\n    \n    def get_scheduler(optimizer):\n        if CFG.scheduler=='CosineAnnealingLR':\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        return scheduler\n    \n    model = TextRegressionModel(model_name=CFG.model_name, dropout_p=CFG.dropout_p)\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n    scheduler = get_scheduler(optimizer)\n    \n    criterion = nn.MSELoss().to(device)\n    best_loss = np.inf\n    \n    for epoch in range(CFG.n_epochs):\n        \n        start_time = time.time()\n        train_loss = train_step(model, criterion, optimizer, train_data_loader, epoch)\n        eval_loss = eval_step(model, criterion, valid_data_loader, epoch)\n        scheduler.step()\n        \n        elapsed = time.time() - start_time\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {train_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - avg_eval_loss: {eval_loss:.4f}')\n        \n        if eval_loss < best_loss:\n            best_loss = eval_loss\n            \n            torch.save({\n                \"model\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"scheduler\": scheduler.state_dict()\n            }, f\"{CFG.model_name}_fold_{fold}_best.pth\")\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nfolds = df_train.copy()\nFold = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\nfor n, (train_idx, valid_idx) in enumerate(Fold.split(folds)):\n    folds.loc[valid_idx, \"fold\"] = int(n)\n    \nfolds[\"fold\"] = folds[\"fold\"].astype(int)\nprint(folds.groupby([\"fold\"]).size())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"def main():\n    \n    # oof_df = pd.DataFrame()\n    for fold in range(CFG.n_folds):\n        train_loop(folds, fold)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_torch(seed=CFG.seed)\nmain()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}