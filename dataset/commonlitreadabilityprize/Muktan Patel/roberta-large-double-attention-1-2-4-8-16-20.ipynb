{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\nMy Idea: 2D attention\n\n\nProvide attention to hidden states for multiple layers and create a context vector based on attention weights.\nthen provide attention to context vectors of multiple layers\n\nand \n\n\nAcknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish) and [Andrey Tuganov](https://www.kaggle.com/andretugan).","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom sklearn.model_selection import KFold\n\nimport gc\ngc.enable()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-28T07:35:03.661496Z","iopub.execute_input":"2021-07-28T07:35:03.661915Z","iopub.status.idle":"2021-07-28T07:35:11.323515Z","shell.execute_reply.started":"2021-07-28T07:35:03.661827Z","shell.execute_reply":"2021-07-28T07:35:11.322445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 10\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"roberta-large\"\nTOKENIZER_PATH = \"roberta-large\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:11.32802Z","iopub.execute_input":"2021-07-28T07:35:11.328407Z","iopub.status.idle":"2021-07-28T07:35:11.387995Z","shell.execute_reply.started":"2021-07-28T07:35:11.32837Z","shell.execute_reply":"2021-07-28T07:35:11.386979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:11.391017Z","iopub.execute_input":"2021-07-28T07:35:11.392502Z","iopub.status.idle":"2021-07-28T07:35:11.403465Z","shell.execute_reply.started":"2021-07-28T07:35:11.392459Z","shell.execute_reply":"2021-07-28T07:35:11.40252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n\n# Remove incomplete entries if any.\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:11.405205Z","iopub.execute_input":"2021-07-28T07:35:11.407525Z","iopub.status.idle":"2021-07-28T07:35:11.552003Z","shell.execute_reply.started":"2021-07-28T07:35:11.407486Z","shell.execute_reply":"2021-07-28T07:35:11.551142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:11.556538Z","iopub.execute_input":"2021-07-28T07:35:11.558724Z","iopub.status.idle":"2021-07-28T07:35:16.262722Z","shell.execute_reply.started":"2021-07-28T07:35:11.558684Z","shell.execute_reply":"2021-07-28T07:35:16.261746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:16.264923Z","iopub.execute_input":"2021-07-28T07:35:16.265614Z","iopub.status.idle":"2021-07-28T07:35:16.276057Z","shell.execute_reply.started":"2021-07-28T07:35:16.26557Z","shell.execute_reply":"2021-07-28T07:35:16.275191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\nThe model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm).","metadata":{}},{"cell_type":"code","source":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention1 = nn.Sequential(            \n            nn.Linear(1024, 256),            \n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        self.attention2 = nn.Sequential(            \n            nn.Linear(1024, 256),            \n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        self.attention4 = nn.Sequential(            \n            nn.Linear(1024, 256),            \n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        self.attention8 = nn.Sequential(            \n            nn.Linear(1024, 256),            \n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        \n        self.attention16 = nn.Sequential(            \n            nn.Linear(1024,256),\n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        self.attention20 = nn.Sequential(            \n            nn.Linear(1024,256),\n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        \n        self.attention_sm = nn.Sequential(            \n            nn.Linear(1024, 6),\n            nn.Tanh(),                       \n            nn.Linear(6, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n        \n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights_1 = self.attention1(last_layer_hidden_states)\n        context_vector_1 = torch.sum(weights_1 * last_layer_hidden_states, dim=1)\n        \n        last2_layer_hidden_states = roberta_output.hidden_states[-2]\n        weights_2 = self.attention2(last2_layer_hidden_states)\n        context_vector_2 = torch.sum(weights_2 * last2_layer_hidden_states, dim=1)\n        \n        last4_layer_hidden_states = roberta_output.hidden_states[-3]\n        weights_4 = self.attention4(last4_layer_hidden_states)\n        context_vector_4 = torch.sum(weights_4 * last4_layer_hidden_states, dim=1)\n        \n        last8_layer_hidden_states = roberta_output.hidden_states[-4]\n        weights_8 = self.attention8(last8_layer_hidden_states)\n        context_vector_8 = torch.sum(weights_8 * last8_layer_hidden_states, dim=1)\n        \n        last16_layer_hidden_states = roberta_output.hidden_states[-16]\n        weights_16 = self.attention16(last16_layer_hidden_states)\n        context_vector_16 = torch.sum(weights_16 * last16_layer_hidden_states, dim=1)\n        \n        last20_layer_hidden_states = roberta_output.hidden_states[-20]\n        weights_20 = self.attention20(last20_layer_hidden_states)\n        context_vector_20 = torch.sum(weights_20 * last20_layer_hidden_states, dim=1)\n        \n#         print(context_vector_1.shape)\n        con_context_vectors = torch.stack([context_vector_1, context_vector_2, context_vector_4, context_vector_8, context_vector_16, context_vector_20], dim=1)\n#         print(con_context_vectors.shape)\n        layer_weights = self.attention_sm(con_context_vectors)\n#         print(layer_weights.shape)\n        final_context_vector = torch.sum(layer_weights * con_context_vectors, dim=1)\n        \n#         print(final_context_vector.shape)\n        ans = self.regressor(final_context_vector)\n#         print(ans.shape)\n        \n        # Now we reduce the context vector to the prediction score.\n        return ans","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:16.277568Z","iopub.execute_input":"2021-07-28T07:35:16.27798Z","iopub.status.idle":"2021-07-28T07:35:16.300372Z","shell.execute_reply.started":"2021-07-28T07:35:16.277918Z","shell.execute_reply":"2021-07-28T07:35:16.29941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_mse(model, data_loader):\n    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n    model.eval()            \n    mse_sum = 0\n\n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)                        \n            target = target.to(DEVICE)           \n            \n            pred = model(input_ids, attention_mask)                       \n\n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n                \n\n    return mse_sum / len(data_loader.dataset)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:16.303064Z","iopub.execute_input":"2021-07-28T07:35:16.303558Z","iopub.status.idle":"2021-07-28T07:35:16.313626Z","shell.execute_reply.started":"2021-07-28T07:35:16.303472Z","shell.execute_reply":"2021-07-28T07:35:16.312666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n            pred = model(input_ids, attention_mask)                        \n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:16.315273Z","iopub.execute_input":"2021-07-28T07:35:16.315796Z","iopub.status.idle":"2021-07-28T07:35:16.327159Z","shell.execute_reply.started":"2021-07-28T07:35:16.315756Z","shell.execute_reply":"2021-07-28T07:35:16.326143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, model_path, train_loader, val_loader,\n          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n    start = time.time()\n    for epoch in range(num_epochs):                           \n        val_rmse = None         \n        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)            \n            target = target.to(DEVICE)                        \n            optimizer.zero_grad()\n            model.train()\n            pred = model(input_ids, attention_mask)\n            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n            mse.backward()\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            if step >= last_eval_step + eval_period:\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                last_eval_step = step\n                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n                for rmse, period in EVAL_SCHEDULE:\n                    if val_rmse >= rmse:\n                        eval_period = period\n                        break                               \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n                    torch.save(model.state_dict(), model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                start = time.time()                       \n            step += 1\n    return best_val_rmse","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:16.328502Z","iopub.execute_input":"2021-07-28T07:35:16.328939Z","iopub.status.idle":"2021-07-28T07:35:16.341608Z","shell.execute_reply.started":"2021-07-28T07:35:16.32888Z","shell.execute_reply":"2021-07-28T07:35:16.340725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:389]    \n    attention_parameters = named_parameters[391:395]\n    regressor_parameters = named_parameters[395:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = 1.2e-5\n\n        if layer_num >= 133:        \n            lr = 3e-5\n\n        if layer_num >= 261:\n            lr = 7.5e-5\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return AdamW(parameters)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:16.343231Z","iopub.execute_input":"2021-07-28T07:35:16.343633Z","iopub.status.idle":"2021-07-28T07:35:16.35488Z","shell.execute_reply.started":"2021-07-28T07:35:16.343585Z","shell.execute_reply":"2021-07-28T07:35:16.354033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:16.35639Z","iopub.execute_input":"2021-07-28T07:35:16.356794Z","iopub.status.idle":"2021-07-28T07:35:16.536641Z","shell.execute_reply.started":"2021-07-28T07:35:16.356759Z","shell.execute_reply":"2021-07-28T07:35:16.535506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n\nSEED = 1000\nlist_val_rmse = []\n\nkfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n\nfor fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n    model_path = f\"model_{fold + 1}.pth\"\n        \n    set_random_seed(SEED + fold)\n    \n    train_dataset = LitDataset(train_df.loc[train_indices])    \n    val_dataset = LitDataset(train_df.loc[val_indices])    \n        \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              drop_last=True, shuffle=True, num_workers=2)    \n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                            drop_last=False, shuffle=False, num_workers=2)    \n        \n    set_random_seed(SEED + fold)    \n    \n    model = LitModel().to(DEVICE)\n    \n    optimizer = create_optimizer(model)                        \n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_training_steps=NUM_EPOCHS * len(train_loader),\n        num_warmup_steps=50)    \n    \n    list_val_rmse.append(train(model, model_path, train_loader,\n                               val_loader, optimizer, scheduler=scheduler))\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    \nprint(\"\\nPerformance estimates:\")\nprint(list_val_rmse)\nprint(\"Mean:\", np.array(list_val_rmse).mean())","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:35:16.538156Z","iopub.execute_input":"2021-07-28T07:35:16.538628Z","iopub.status.idle":"2021-07-28T08:08:51.188442Z","shell.execute_reply.started":"2021-07-28T07:35:16.53851Z","shell.execute_reply":"2021-07-28T08:08:51.186128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0.474","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:08:51.189882Z","iopub.status.idle":"2021-07-28T08:08:51.190613Z"},"trusted":true},"execution_count":null,"outputs":[]}]}