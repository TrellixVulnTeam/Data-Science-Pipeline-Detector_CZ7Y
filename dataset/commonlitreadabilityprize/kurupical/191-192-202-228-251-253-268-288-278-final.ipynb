{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/timm-pytorch-image-models/pytorch-image-models-master","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U transformers --no-index --find-links=file:///kaggle/input/huggingfaces/transformers==4.8.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\ntransformers.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sys\nsys.path.append(\"/kaggle/input/d/kurupical/commonlit-kurupical-src/exp\")\nfrom transformers import AutoTokenizer\nimport torch\nimport tqdm\nimport gc\nimport pickle\nfrom exp007 import Config, CommonLitModule, CommonLitDataset\nfrom exp054 import Config, CommonLitModule, CommonLitDataset, TemporalConvNet\nimport glob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"debug = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineering(df):\n    def total_words(x):\n        return len(x.split(\" \"))\n\n    def total_unique_words(x):\n        return len(np.unique(x.split(\" \")))\n\n    def total_charactors(x):\n        x = x.replace(\" \", \"\")\n        return len(x)\n\n    def total_sentence(x):\n        x = x.replace(\"!\", \"[end]\").replace(\"?\", \"[end]\").replace(\".\", \"[end]\")\n        return len(x.split(\"[end]\"))\n\n    df_ret = df[[\"id\", \"excerpt\", \"target\", \"standard_error\"]]\n    excerpt = df[\"excerpt\"].values\n    df_ret[\"total_words\"] = [total_words(x) for x in excerpt]\n    df_ret[\"total_unique_words\"] = [total_unique_words(x) for x in excerpt]\n    df_ret[\"total_characters\"] = [total_charactors(x) for x in excerpt]\n    df_ret[\"total_sentence\"] = [total_sentence(x) for x in excerpt]\n\n    df_ret[\"div_sentence_characters\"] = df_ret[\"total_sentence\"] / df_ret[\"total_characters\"]\n    df_ret[\"div_sentence_words\"] = df_ret[\"total_sentence\"] / df_ret[\"total_words\"]\n    df_ret[\"div_characters_words\"] = df_ret[\"total_characters\"] / df_ret[\"total_words\"]\n    df_ret[\"div_words_unique_words\"] = df_ret[\"total_words\"] / df_ret[\"total_unique_words\"]\n\n    for i, word in enumerate([\"!\", \"?\", \"(\", \")\", \"'\", '\"', \";\", \".\", \",\"]):\n        df_ret[f\"count_word_special_{i}\"] = [x.count(word) for x in excerpt]\n\n    return df_ret.fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if debug:\n    # fold: 0\n    df_test = pd.read_csv(\"/kaggle/input/commonlit-train-fold/train_folds.csv\")\n    df_test = df_test[df_test[\"kfold\"] == 0]\n    df_test = feature_engineering(df_test)\nelse:\n    df_test = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")   \n    df_test[\"target\"] = 0\n    df_test[\"standard_error\"] = 1    \n    df_test = feature_engineering(df_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(exp_name, fold):\n    model_path = f\"/kaggle/input/d/kurupical/kurupical-commonlit/{exp_name}/best_fold{fold}.ckpt\"\n    if exp_name == \"exp030_roberta_base_bilstm\":\n        from exp030 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", rnn_module_shrink_ratio=0.25)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    if exp_name == \"exp039_roberta_base\":\n        from exp039 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\")\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    if exp_name == \"exp040_bert_base_cased\":\n        from exp040 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-cased\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\")\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    if exp_name == \"exp040_roberta_base_without_finetune\":\n        from exp040 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\")\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    if exp_name == \"exp046_roberta_base_mlm\":\n        from exp046 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\")\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    if exp_name == \"exp045_bert_base_uncased\":\n        from exp045 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-uncased\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\")\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp045_deberta_base\":\n        from exp045 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/deberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\")\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp045_deberta_base\":\n        from exp045 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/deberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\")\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp054_roberta_large_mlm\":\n        model_path = f\"/kaggle/input/exp054-roberta-large-mlm/best_fold{fold}.ckpt\"\n        from exp054 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", tcn_module_num=2, tcn_module_kernel_size=3, tcn_module_enable=True)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp053_roberta_base_tcn_residual\":\n        model_path = f\"/kaggle/input/d/kurupical/kurupical-commonlit2/{exp_name}/best_fold{fold}.ckpt\"\n        from exp053 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", tcn_module_enable=True, tcn_module_num=3, tcn_module_kernel_size=4)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp060_roberta_base_attn\":\n        model_path = f\"/kaggle/input/d/kurupical/kurupical-commonlit2/{exp_name}/best_fold{fold}.ckpt\"\n        from exp060 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", tcn_module_enable=True)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp062_roberta_large_mlm\":\n        model_path = f\"/kaggle/input/exp062-roberta-large-mlm/best_fold{fold}.ckpt\"\n        from exp062 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", tcn_module_num=3, tcn_module_kernel_size=4, tcn_module_enable=True)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp056_roberta_base_dense\":\n        model_path = f\"/kaggle/input/d/kurupical/kurupical-commonlit2/{exp_name}/best_fold{fold}.ckpt\"\n        from exp056 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", linear_vocab_dim=8)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp066_roberta_base_without_finetune_bilstm\":\n        model_path = f\"/kaggle/input/d/kurupical/kurupical-commonlit2/{exp_name}/best_fold{fold}.ckpt\"\n        from exp066 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", rnn_module_num=1, \n                     prep_enable=False, hidden_stack_enable=True)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    \n    if exp_name == \"exp066_roberta_large_without_finetune\":\n        model_path = f\"/kaggle/input/d/kurupical/kurupical-commonlit2/{exp_name}/best_fold{fold}.ckpt\"\n        from exp066 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", prep_enable=False, \n                     hidden_stack_enable=True, tcn_module_enable=True, linear_vocab_dim=2, linear_vocab_enable=True)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp082_roberta_large_bilstm\":\n        model_path = f\"/kaggle/input/d/kurupical/kurupical-commonlit2/{exp_name}/best_fold{fold}.ckpt\"\n        from exp082 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", rnn_module_num=1, \n                     prep_enable=True, hidden_stack_enable=True)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp079_bert_base_uncased\":\n        model_path = f\"/kaggle/input/d/kurupical/kurupical-commonlit2/{exp_name}/best_fold{fold}.ckpt\"\n        from exp079 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-uncased\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", rnn_module_num=1, \n                     prep_enable=True, hidden_stack_enable=True)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp077_roberta_base_bilstm\":\n        model_path = f\"/kaggle/input/d/kurupical/kurupical-commonlit2/{exp_name}/best_fold{fold}.ckpt\"\n        from exp077 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", rnn_module_num=1, \n                     prep_enable=True, hidden_stack_enable=True)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    \n    if exp_name == \"exp043\":\n        model_path = f\"/kaggle/input/kurupical-commonlit3/{exp_name}/best_fold{fold}.ckpt\"\n        from exp043 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-cased\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\")\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer)\n        \n        model.load_state_dict(torch.load(model_path))\n        return model, dataset\n    \n    \n    if exp_name == \"exp047_roberta_base\":\n        model_path = f\"/kaggle/input/exp047-roberta-base/best_fold{fold}.ckpt\"\n        from exp047 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\")\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path))\n        return model, dataset\n    \n    \n    if exp_name == \"exp060\":\n        model_path = f\"/kaggle/input/kurupical-commonlit3/{exp_name}/best_fold{fold}.ckpt\"\n        from exp060 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\")\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    \n    \n    if exp_name == \"exp062_roberta_large\":\n        model_path = f\"/kaggle/input/exp062-roberta-large/best_fold{fold}.ckpt\"\n        from exp062 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", linear_vocab_dim=16, linear_vocab_enable=True)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path))\n        return model, dataset\n    \n    if exp_name == \"exp063\":\n        model_path = f\"/kaggle/input/kurupical-commonlit3/{exp_name}/best_fold{fold}.ckpt\"\n        from exp063 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", tcn_module_enable=True)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp084_bert_base_cased\":\n        model_path = f\"/kaggle/input/kurupical-commonlit3/{exp_name}/best_fold{fold}.ckpt\"\n        from exp084 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-cased\"\n        cfg = Config(nlp_model_name=nlp_model_path, experiment_name=\"\", prep_enable=True, hidden_stack_enable=True, rnn_module_num=1)\n        cfg.fine_tuned_path = None\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp091_roberta_large\":\n        from exp091 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp091-roberta-large/{exp_name}\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp110_roberta_large\":\n        from exp110 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp110-roberta-large\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp116_roberta_large\":\n        from exp116 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp116-roberta-large\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    \n    if exp_name == \"exp119_deberta_large\":\n        from exp119 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp119-deberta-large\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/deberta/large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n        \n    if exp_name == \"exp126_luke_base\":\n        from exp126 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp126-luke-base\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    \n    if exp_name == \"exp125_deberta_large\":\n        from exp125 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp125-deberta-large\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/deberta/large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp134_roberta1\" or exp_name == \"exp134_roberta2\" or exp_name == \"exp134_roberta3\":\n        from exp134 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/seed-variance-test/{exp_name}\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp129_luke_large\":\n        from exp129 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp129-luke-large\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp124_bert_base_cased\":\n        from exp129 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp124-bert-base-cased\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp138_roberta_large_another_fold\":\n        from exp133 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp138-roberta-large-another-fold\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp139_roberta_base\":\n        from exp139 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp139-roberta-base\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    if exp_name == \"exp131_luke_base\":\n        from exp131 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp131-luke-base\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp139_roberta_base2\":\n        from exp139 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp139-roberta-base2\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp141_roberta_base\":\n        from exp141 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp141-roberta-base\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp143_roberta_base_lstm\":\n        from exp141 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp143-roberta-base-lstm\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, \n    \n\n    if exp_name == \"exp149_roberta_base\":\n        from exp141 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp149-roberta-base\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp144_roberta_base_vocab\":\n        from exp144 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp144-roberta-base-vocab\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp144_2_roberta_base_vocab\":\n        from exp144 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp144-2-roberta-base-vocab\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp142_2_roberta_base_stack\":\n        from exp142 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp142-2-roberta-base-stack\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp155_roberta_base_feature\":\n        from exp155 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp155-roberta-base-feature\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp156_bert_base_cased\":\n        from exp129 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp156-bert-base-cased\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp156_luke_base\":\n        from exp156 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp156-luke-base\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp162_roberta_large\":\n        from exp162 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp162-roberta-large\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n        \n    if exp_name == \"exp162_roberta_large2\":\n        from exp162 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp162-roberta-large2\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp163_bert_base_uncased\":\n        from exp129 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp163-bert-base-uncased\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-uncased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp165_xlnet_base_cased\":\n        from exp165 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp165-xlnet-base-cased\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/xlnetbasecased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp164_bert_large_cased\":\n        from exp129 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp164-bert-large-cased\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-large-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp165_2_xlnet_base_cased\":\n        from exp165 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp165-2-xlnet-base-cased\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/xlnetbasecased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp164_luke_large\":\n        from exp164 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp164-luke-large\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp167_bert_large_uncased\":\n        from exp167 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp167-bert-large-uncased\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-large-uncased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp167_xlnet_large_cased_2\":\n        from exp167 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp167-xlnet-large-cased-2\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/xlnet-large-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp171_roberta_base\":\n        from exp171 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp171-roberta-base\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp170_luke_large\":\n        from exp170 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp170-luke-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    if exp_name == \"exp171_luke_base\":\n        from exp171 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp171-luke-base\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp170_bert_large_cased\":\n        from exp170 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp170-bert-large-cased-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-large-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp176_roberta_base\":\n        from exp176 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp176-roberta-base\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n        \n    if exp_name == \"exp177_roberta_large\":\n        from exp177 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp177-roberta-large\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n        \n    if exp_name == \"exp501_roberta_large\":\n        from exp501 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp501-roberta-large\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp501_luke_large\":\n        from exp501 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp501-luke-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp502_bert_base_cased\":\n        from exp502 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp502-bert-base-cased\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp179_xlnet_base_cased\":\n        from exp179 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp179-xlnet-base-cased\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/xlnetbasecased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp180_roberta_large\":\n        from exp180 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp180-roberta-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp180_luke_large\":\n        from exp180 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp180-luke-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp181_bert_base_cased\":\n        from exp181 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp181-bert-base-cased\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    \n    if exp_name == \"exp181_bert_base_cased2\":\n        from exp181 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp181-bert-base-cased2\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp181_deberta_base\":\n        from exp181 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp181-deberta-base\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/deberta-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp180_deberta_large\":\n        from exp180 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp180-deberta-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/deberta/large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp182_deberta_large\":\n        from exp182 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp182-deberta-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/deberta/large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp182_roberta_large\":\n        from exp182 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp182-roberta-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp183_roberta_large\":\n        from exp183 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp183-roberta-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp180_bert_large_cased\":\n        from exp180 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp180-bert-large-cased-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-large-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp185_funnel_large\":\n        from exp185 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp185-funnel-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/funnel-large-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp185_funnel_large2\":\n        from exp185 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp185-funnel-large2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/funnel-large-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp192_electra_large\":\n        from exp192 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp192-electra-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/electra-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp192_electra_large2\":\n        from exp192 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp192-electra-large2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/electra-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp197_funnel_medium\":\n        from exp197 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp197-funnel-medium-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/funnel-medium-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp191_funnel_large\":\n        from exp191 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp191-funnel-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/funnel-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp207_electra_large\":\n        from exp207 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp207-electra-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/electra-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp174_luke_base\":\n        from exp174 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp174-luke-base-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp202_deberta_large\":\n        from exp202 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp202-deberta-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/deberta/large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp211_bert_large_cased2\":\n        from exp211 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp211-bert-large-cased2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-large-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp214_luke_base\":\n        from exp214 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp214-luke-base-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp217_gpt2_medium\":\n        from exp217 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp217-gpt2-medium-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-medium\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp216_gpt2_medium\":\n        from exp216 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp216-gpt2-medium-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-medium\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp217_gpt2_medium2\":\n        from exp217 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp217-gpt2-medium2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-medium\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp227_electra_large\":\n        from exp227 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp227-electra-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/electra-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp223_bert_base_uncased\":\n        from exp223 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp223-bert-base-uncased-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/bert-base-uncased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp228_gpt2_medium\":\n        from exp228 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp228-gpt2-medium-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-medium\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp228_gpt2_medium2\":\n        from exp228 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp228-gpt2-medium2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-medium\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp235_xlnet_large_cased\":\n        from exp235 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp235-xlnet-large-cased-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/xlnet-large-cased\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp238_funnel_large\":\n        from exp238 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp238-funnel-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/funnel-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp238_funnel_large2\":\n        from exp238_2 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp238-funnel-large2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/funnel-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp240_funnel_large_base\":\n        from exp240 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp240-funnel-large-base-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/funnel-large-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp242_electra_large\":\n        from exp242 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp242-electra-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/electra-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp242_electra_large2\":\n        from exp242 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp242-electra-large2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/electra-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp238_funnel_large3\":\n        from exp238_2 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp238-funnel-large3-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/funnel-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp201_electra_base\":\n        from exp201 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp201-electra-base-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/electra-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp250_gpt2_medium\":\n        from exp250 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp250-gpt2-medium-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-medium\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp250_gpt2_medium2\":\n        from exp250 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp250-gpt2-medium2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-medium\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp252_gpt2_medium\":\n        from exp252 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp252-gpt2-medium-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-medium\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp252_gpt2_medium2\":\n        from exp252 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp252-gpt2-medium2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-medium\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    \n    if exp_name == \"exp251_luke_large\":\n        from exp251 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp251-luke-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp251_luke_large2\":\n        from exp251 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp251-luke-large2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp251_luke_large3\":\n        from exp251 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp251-luke-large3-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp251_luke_large4\":\n        from exp251 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp251-luke-large4-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/luke-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp256_ernie_large\":\n        from exp251 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp256-ernie-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/ernie-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    \n    if exp_name == \"exp253_deberta_large\":\n        from exp253 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp253-deberta-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/deberta/large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    \n    if exp_name == \"exp257_funnel_xlarge_base\":\n        from exp257 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp257-funnel-xlarge-base-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/funnel-xlarge-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp268_mpnet_base\":\n        from exp268 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp268-mpnet-base-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/mpnet-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    if exp_name == \"exp264_roberta_base\":\n        from exp264 import Config, CommonLitModule, CommonLitDataset\n        nlp_model_path = \"/kaggle/input/roberta-transformers-pytorch/roberta-base\"\n        model_dir = f\"/kaggle/input/exp264-roberta-base-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    \n    if exp_name == \"exp273_funnel_xlarge_base\":\n        from exp273 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp273-funnel-xlarge-base-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/funnel-xlarge-base\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n    \n    if exp_name == \"exp275_gpt2_large\":\n        from exp275 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp275-gpt2-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp278_gpt2_large\":\n        from exp278 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp278-gpt2-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    \n    if exp_name == \"exp278_gpt2_large2\":\n        from exp278 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp278-gpt2-large2-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    \n    if exp_name == \"exp288_ernie_large\":\n        from exp288 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp288-ernie-large-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/ernie-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n\n    if exp_name == \"exp278_gpt2_large3\":\n        from exp278 import Config, CommonLitModule, CommonLitDataset\n        model_dir = f\"/kaggle/input/exp278-gpt2-large3-nb\"\n        model_path = f\"{model_dir}/best_fold{fold}.ckpt\"\n        cfg_path = f\"{model_dir}/cfg.pickle\"\n        nlp_model_path = \"/kaggle/input/huggingface-pretrained-files/gpt2-large\"\n        with open(cfg_path, \"rb\") as f:\n            cfg = pickle.load(f)\n        cfg.nlp_model_name = nlp_model_path\n        model = CommonLitModule(cfg=cfg, output_dir=\"\").to(\"cuda\")\n        dataset = CommonLitDataset(df=df_test, tokenizer=model.tokenizer, cfg=cfg)\n        \n        model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n        return model, dataset\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction(exp_name, n_multidropout=1, folds=[0, 1, 2, 3, 4]):\n    df = df_test[[\"id\"]]\n    \n    if debug:\n        folds = [0]\n    \n    print(f\"--------- {exp_name} ---------\")\n    for fold in folds:\n        with torch.no_grad():\n            print(f\"--------- fold: {fold} ---------\")\n            model, dataset = get_model(exp_name, fold=fold)\n            if \"deberta_large\" in exp_name or \"gpt2_medium\" in exp_name:\n                dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\n            elif \"gpt2_large\" in exp_name:\n                dataloader = torch.utils.data.DataLoader(dataset, batch_size=2)\n            else:\n                dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n\n            if n_multidropout == 1:\n                model.eval()\n            for i in range(n_multidropout):\n                pred = []\n                for batch in dataloader:\n                    if \"mlm\" in exp_name or \"exp060\" in exp_name or \"exp053\" in exp_name or \"exp056\" in exp_name or \"exp066\" in exp_name or \"exp077\" in exp_name or \"exp079\" in exp_name or \"exp082\" in exp_name or \"exp062\" in exp_name or \"exp063\" in exp_name or \"exp047\" in exp_name or \"exp084\" in exp_name or \"exp091\" in exp_name:\n                        input_ids_masked = batch[0].to(\"cuda\")\n                        attention_mask = batch[1].to(\"cuda\")\n                        token_type_ids = batch[2].to(\"cuda\")\n                        input_ids = batch[3].to(\"cuda\")\n                        output = model(input_ids_masked, attention_mask, token_type_ids, input_ids)\n                    elif \"exp045\" in exp_name:\n                        input_ids = batch[0].to(\"cuda\")\n                        attention_mask = batch[1].to(\"cuda\")\n                        output = model(input_ids, attention_mask)\n                    elif  \"exp110\" in exp_name or \"exp116\" in exp_name or \"exp119\" in exp_name or \"exp125\" in exp_name or \"exp126\" in exp_name or \"exp134\" in exp_name or \"exp129\" in exp_name or \"exp124\" in exp_name or \"exp138\" in exp_name or \"exp139\" in exp_name or \"exp131\" in exp_name or \"exp141\" in exp_name:\n                        input_ids_masked = batch[0].to(\"cuda\")\n                        attention_mask = batch[1].to(\"cuda\")\n                        token_type_ids = batch[2].to(\"cuda\")\n                        input_ids = batch[3].to(\"cuda\")\n                        output, _ = model(input_ids_masked, attention_mask, token_type_ids, input_ids)                    \n                    elif \"exp288\" in exp_name or \"exp278\" in exp_name or \"exp275\" in exp_name or \"exp273\" in exp_name or \"exp264\" in exp_name or \"exp268\" in exp_name or \"exp257\" in exp_name or \"exp253\" in exp_name or \"exp256\" in exp_name or \"exp251\" in exp_name or \"exp252\" in exp_name or \"exp250\" in exp_name or \"exp201\" in exp_name or \"exp242\" in exp_name or \"exp240\" in exp_name or \"exp238\" in exp_name or \"exp235\" in exp_name or \"exp228\" in exp_name or \"exp223\" in exp_name or \"exp227\" in exp_name or \"exp216\" in exp_name or \"exp217\" in exp_name or \"exp214\" in exp_name or \"exp211\" in exp_name or \"exp202\" in exp_name or \"exp174\" in exp_name or \"exp207\" in exp_name or \"exp191\" in exp_name or \"exp197\" in exp_name or \"exp192\" in exp_name or \"exp185\" in exp_name or \"exp183\" in exp_name or \"exp182\" in exp_name or \"exp181\" in exp_name or \"exp179\" in exp_name or \"exp180\" in exp_name or \"exp155\" in exp_name or \"exp156_luke_base\" in exp_name or \"exp162\" in exp_name or \"exp165\" in exp_name or \"exp164_luke_large\" in exp_name or \"exp167\" in exp_name or \"exp171\" in exp_name or \"exp170\" in exp_name or \"exp174\" in exp_name or \"exp176\" in exp_name or \"exp177\" in exp_name or \"exp501\" in exp_name or \"exp502\" in exp_name:\n                        input_ids_masked = batch[0].to(\"cuda\")\n                        attention_mask = batch[1].to(\"cuda\")\n                        token_type_ids = batch[2].to(\"cuda\")\n                        input_ids = batch[3].to(\"cuda\")\n                        features = batch[4].to(\"cuda\")\n                        output, _ = model(input_ids_masked, attention_mask, token_type_ids, input_ids, features)                                        \n                    elif \"exp040\" in exp_name:                    \n                        input_ids = batch[0].to(\"cuda\")\n                        attention_mask = batch[1].to(\"cuda\")\n                        token_type_ids = batch[2].to(\"cuda\")\n                        output = model(input_ids, attention_mask, token_type_ids)\n                    else:\n                        input_ids_masked = batch[0].to(\"cuda\")\n                        attention_mask = batch[1].to(\"cuda\")\n                        token_type_ids = batch[2].to(\"cuda\")\n                        input_ids = batch[3].to(\"cuda\")\n                        output, _ = model(input_ids_masked, attention_mask, token_type_ids, input_ids)                                        \n                    pred.extend(output.detach().cpu().numpy().flatten().tolist())\n                    del output\n                    gc.collect()\n                    torch.cuda.empty_cache()\n                df[f\"pred_fold{fold}_i{i}\"] = pred\n            del model, dataset, dataloader\n            gc.collect()\n            torch.cuda.empty_cache()\n    print(df)\n    df[exp_name] = df.drop(\"id\", axis=1).mean(axis=1)\n    \n    return df[exp_name].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predict(model_dict):\n    pred = np.zeros(len(df_test))\n    \n    for model, weight in model_dict.items():\n        if model == \"coef\":\n            pred += weight\n        else:\n            pred += get_prediction(model) * weight\n    return pred","metadata":{"execution":{"iopub.status.busy":"2021-07-20T12:58:23.642691Z","iopub.execute_input":"2021-07-20T12:58:23.643081Z","iopub.status.idle":"2021-07-20T12:58:23.654396Z","shell.execute_reply.started":"2021-07-20T12:58:23.643001Z","shell.execute_reply":"2021-07-20T12:58:23.653036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1 = make_predict(model_dict={\n 'exp192_electra_large2': 0.08295192073911264,\n 'exp191_funnel_large': 0.12150247993915793,\n 'exp202_deberta_large': 0.15167895227258077,\n 'exp228_gpt2_medium': 0.12193089040813881,\n 'exp251_luke_large': 0.09370238709826298,\n 'exp253_deberta_large': 0.11799288574092036,\n 'exp268_mpnet_base': 0.09995350620369359,\n 'exp288_ernie_large': 0.07922850022202096,\n 'exp278_gpt2_large3': 0.16487686471775878,\n 'coef': 0.009468716183650404\n})\npred = pred1\n# pred = pred1*0.9 + pred2*0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-16T09:03:55.482935Z","iopub.execute_input":"2021-07-16T09:03:55.483306Z","iopub.status.idle":"2021-07-16T09:03:55.548698Z","shell.execute_reply.started":"2021-07-16T09:03:55.483223Z","shell.execute_reply":"2021-07-16T09:03:55.547361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_test[[\"id\"]]\ndf[\"target\"] = pred","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:53:41.412747Z","iopub.execute_input":"2021-07-06T09:53:41.413009Z","iopub.status.idle":"2021-07-06T09:53:41.421704Z","shell.execute_reply.started":"2021-07-06T09:53:41.412983Z","shell.execute_reply":"2021-07-06T09:53:41.418721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if debug:\n    target = df_test[\"target\"].values\n    rmse = np.sqrt(1 / len(pred) * ((target - pred)**2).sum())\n    print(rmse)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:53:41.423191Z","iopub.execute_input":"2021-07-06T09:53:41.423568Z","iopub.status.idle":"2021-07-06T09:53:41.44127Z","shell.execute_reply.started":"2021-07-06T09:53:41.423533Z","shell.execute_reply":"2021-07-06T09:53:41.440151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:53:41.44265Z","iopub.execute_input":"2021-07-06T09:53:41.443079Z","iopub.status.idle":"2021-07-06T09:53:41.605953Z","shell.execute_reply.started":"2021-07-06T09:53:41.443045Z","shell.execute_reply":"2021-07-06T09:53:41.605124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:53:41.610095Z","iopub.execute_input":"2021-07-06T09:53:41.610336Z","iopub.status.idle":"2021-07-06T09:53:41.629959Z","shell.execute_reply.started":"2021-07-06T09:53:41.610312Z","shell.execute_reply":"2021-07-06T09:53:41.628736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}