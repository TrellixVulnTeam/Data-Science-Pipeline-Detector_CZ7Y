{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# In this Notebook we use DeepSpeed to pretrain RoBERTa","metadata":{}},{"cell_type":"markdown","source":"## What is DeepSpeed?\n### DeepSpeed is a deep learning optimization library. It's used for efficient, effective and easy distributed training.","metadata":{}},{"cell_type":"markdown","source":"#### The main reason to use this library in this competetion is the memory efficiency that it provides.\n#### Even with single GPU we'll be able to train bigger and better SOTA models\n#### Use larger models and increased batch sizes without Out Of Memory errors","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport gc\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-10T08:15:45.121702Z","iopub.execute_input":"2021-07-10T08:15:45.122109Z","iopub.status.idle":"2021-07-10T08:15:46.239649Z","shell.execute_reply.started":"2021-07-10T08:15:45.122029Z","shell.execute_reply":"2021-07-10T08:15:46.238871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers/","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:15:46.240965Z","iopub.execute_input":"2021-07-10T08:15:46.241272Z","iopub.status.idle":"2021-07-10T08:16:16.141318Z","shell.execute_reply.started":"2021-07-10T08:15:46.241237Z","shell.execute_reply":"2021-07-10T08:16:16.140298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (AutoModel,AutoModelForMaskedLM,AutoModelForSequenceClassification, AutoTokenizer, LineByLineTextDataset, DataCollatorForLanguageModeling,Trainer, TrainingArguments)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:16:16.143475Z","iopub.execute_input":"2021-07-10T08:16:16.143857Z","iopub.status.idle":"2021-07-10T08:16:22.121607Z","shell.execute_reply.started":"2021-07-10T08:16:16.143805Z","shell.execute_reply":"2021-07-10T08:16:22.120787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install deepspeed","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:16:22.123202Z","iopub.execute_input":"2021-07-10T08:16:22.123533Z","iopub.status.idle":"2021-07-10T08:16:37.93749Z","shell.execute_reply.started":"2021-07-10T08:16:22.123499Z","shell.execute_reply":"2021-07-10T08:16:37.936493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import deepspeed","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:16:37.939083Z","iopub.execute_input":"2021-07-10T08:16:37.939417Z","iopub.status.idle":"2021-07-10T08:16:37.991973Z","shell.execute_reply.started":"2021-07-10T08:16:37.93938Z","shell.execute_reply":"2021-07-10T08:16:37.991122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\ntext  = '.'.join(train_data.excerpt.tolist() + test_data.excerpt.tolist())\n\nwith open('excerpt.txt','w') as f:\n    f.write(text)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:16:37.993229Z","iopub.execute_input":"2021-07-10T08:16:37.993575Z","iopub.status.idle":"2021-07-10T08:16:38.103866Z","shell.execute_reply.started":"2021-07-10T08:16:37.993538Z","shell.execute_reply":"2021-07-10T08:16:38.102911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'roberta-large'\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained('./clrp_roberta_large_ds')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:16:38.105109Z","iopub.execute_input":"2021-07-10T08:16:38.10546Z","iopub.status.idle":"2021-07-10T08:17:58.686453Z","shell.execute_reply.started":"2021-07-10T08:16:38.105421Z","shell.execute_reply":"2021-07-10T08:17:58.685679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '9998'\nos.environ['RANK'] = \"0\"\nos.environ['LOCAL_RANK'] = \"0\"\nos.environ['WORLD_SIZE'] = \"1\"\n# os.environ['MAX_JOBS'] = \"4\"","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:17:58.688945Z","iopub.execute_input":"2021-07-10T08:17:58.689287Z","iopub.status.idle":"2021-07-10T08:17:58.695063Z","shell.execute_reply.started":"2021-07-10T08:17:58.689252Z","shell.execute_reply":"2021-07-10T08:17:58.694115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deepspeed_config = {\n    \"fp16\": {\n        \"enabled\": False,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 0,\n        \"offload_optimizer\": {\n         \"device\": \"cpu\",\n         \"pin_memory\": True\n     },\n     \"offload_param\": {\n        \"device\": \"cpu\",\n        \"pin_memory\": True\n        },\n        \"allgather_partitions\": True,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": True,\n        \"reduce_scatter\": True,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": True,\n        \"cpu_offload\": True\n    },\n\n    \"zero_allow_untested_optimizer\": True,\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\" : \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\"\n        }\n    },\n    \"sparse_attention\": {\n        \"mode\": \"fixed\",\n        \"block\": 16,\n        \"different_layout_per_head\": True,\n        \"num_local_blocks\": 4,\n        \"num_global_blocks\": 1,\n        \"attention\": \"bidirectional\",\n        \"horizontal_global_attention\": False,\n        \"num_different_global_patterns\": 4,\n        \"num_random_blocks\": 0,\n        \"local_window_blocks\": [4],\n        \"global_block_indices\": [0],\n        \"global_block_end_indices\": None,\n        \"num_sliding_window_blocks\": 3\n  },\n\n    \"steps_per_print\": 2000,\n    \"wall_clock_breakdown\": False,\n    \"train_micro_batch_size_per_gpu\" : 'auto',\n    \"gradient_clipping\": \"auto\",\n    \"prescale_gradients\" : False\n}\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:17:58.696944Z","iopub.execute_input":"2021-07-10T08:17:58.697351Z","iopub.status.idle":"2021-07-10T08:17:58.707956Z","shell.execute_reply.started":"2021-07-10T08:17:58.697314Z","shell.execute_reply":"2021-07-10T08:17:58.706917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = LineByLineTextDataset(\n    tokenizer = tokenizer,\n    file_path = \"excerpt.txt\",\n    block_size = 256,\n)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer = tokenizer,\n    file_path = \"excerpt.txt\",\n    block_size = 256)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:17:58.709495Z","iopub.execute_input":"2021-07-10T08:17:58.70996Z","iopub.status.idle":"2021-07-10T08:18:02.465344Z","shell.execute_reply.started":"2021-07-10T08:17:58.709871Z","shell.execute_reply":"2021-07-10T08:18:02.464494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer = tokenizer,mlm = True, mlm_probability = 0.15\n)\n\ntraining_args = TrainingArguments(\n    num_train_epochs = 5,\n    per_device_train_batch_size = 8,\n    per_device_eval_batch_size = 8,\n    evaluation_strategy = 'epoch',\n    save_total_limit = 1,\n    #     eval_steps = 66,\n    save_steps = 268,\n    metric_for_best_model = 'eval_loss',\n    greater_is_better = False,\n#     gradient_accumulation_steps = 1,\n    load_best_model_at_end = True,\n    prediction_loss_only = True,\n    report_to = \"none\",\n    output_dir = \"./clrp_roberta_base_trainer\",\n    overwrite_output_dir = True,\n    ## DeepSpeed Args\n    max_grad_norm = 1.0,\n    local_rank = 0,\n    ## optimizer\n    adam_beta1 = 0.9,\n    adam_beta2 = 0.999,\n    adam_epsilon = 1e-8,\n    weight_decay = 1e-7,\n    ## learning rate scheduler\n#     warmup_steps = 0,\n#     warmup_max_lr = 2e-5,\n    fp16 = False,\n    learning_rate = 3e-5,)\n#     deepspeed = deepspeed_config)\n\ntraining_args._setup_devices","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:18:02.46668Z","iopub.execute_input":"2021-07-10T08:18:02.467018Z","iopub.status.idle":"2021-07-10T08:18:08.644403Z","shell.execute_reply.started":"2021-07-10T08:18:02.466984Z","shell.execute_reply":"2021-07-10T08:18:08.643596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model = model,\n    args=training_args,\n    data_collator = data_collator,\n    train_dataset = dataset,\n    eval_dataset = valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:18:08.645588Z","iopub.execute_input":"2021-07-10T08:18:08.645923Z","iopub.status.idle":"2021-07-10T08:18:09.059662Z","shell.execute_reply.started":"2021-07-10T08:18:08.645889Z","shell.execute_reply":"2021-07-10T08:18:09.058607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T08:18:09.061112Z","iopub.execute_input":"2021-07-10T08:18:09.061517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model('./clrp_roberta_large_ds')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finetuning : In Progress\n## Inference : In Progress","metadata":{}}]}