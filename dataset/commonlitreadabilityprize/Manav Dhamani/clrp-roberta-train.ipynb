{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import random_split\n\nimport pandas as pd\nimport seaborn as sns\nimport gc\nimport time\nfrom tqdm import tqdm\nimport datatable as dt\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold,KFold\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nimport os\nimport random\n\nfrom colorama import Fore, Back, Style\nred = Fore.RED\ngrn = Fore.GREEN\nblu = Fore.BLUE\nylw = Fore.YELLOW\nwht = Fore.WHITE\nbred = Back.RED\nbgrn = Back.GREEN\nbblu = Back.BLUE\nbylw = Back.YELLOW\nbwht = Back.WHITE\nrst = Style.RESET\n\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, Pool, CatBoost","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:51:39.942849Z","iopub.execute_input":"2021-05-24T05:51:39.943173Z","iopub.status.idle":"2021-05-24T05:51:45.625081Z","shell.execute_reply.started":"2021-05-24T05:51:39.943099Z","shell.execute_reply":"2021-05-24T05:51:45.62409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:51:45.626768Z","iopub.execute_input":"2021-05-24T05:51:45.627132Z","iopub.status.idle":"2021-05-24T05:51:52.516274Z","shell.execute_reply.started":"2021-05-24T05:51:45.627094Z","shell.execute_reply":"2021-05-24T05:51:52.51536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/commonlitreadabilityprize/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsample = pd.read_csv(path + 'sample_submission.csv')\n\nnbins = 12\ntrain.loc[:,'bins'] = pd.cut(train['target'],nbins,labels=False)\nbins = train.bins.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:51:52.518347Z","iopub.execute_input":"2021-05-24T05:51:52.518715Z","iopub.status.idle":"2021-05-24T05:51:52.618696Z","shell.execute_reply.started":"2021-05-24T05:51:52.518675Z","shell.execute_reply":"2021-05-24T05:51:52.617961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:51:52.620273Z","iopub.execute_input":"2021-05-24T05:51:52.62061Z","iopub.status.idle":"2021-05-24T05:51:52.638749Z","shell.execute_reply.started":"2021-05-24T05:51:52.620575Z","shell.execute_reply":"2021-05-24T05:51:52.637652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForSequenceClassification,BertModel\nfrom transformers import InputExample, InputFeatures\nconfig = {\n    'MAX_LEN' : 256,\n    'TRAIN_BATCH_SIZE' : 8,\n    'VALID_BATCH_SIZE' : 4,\n    'EPOCHS' : 10,\n    'SEED': 43,\n    'FOLDS': 6,\n    'BERT_PATH' : \"roberta-base\",\n    'CSV_PATH' : 'lgbmtrain.csv',\n    'AUGMENTED_CSV' : 'lgbmtrainAUG.csv',\n    'MODEL_PATH' : './CLRPmodel',\n    'TOKENIZER' : AutoTokenizer.from_pretrained('roberta-base'),\n}","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:51:52.640441Z","iopub.execute_input":"2021-05-24T05:51:52.640844Z","iopub.status.idle":"2021-05-24T05:52:03.989817Z","shell.execute_reply.started":"2021-05-24T05:51:52.640805Z","shell.execute_reply":"2021-05-24T05:52:03.98894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train['target'].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:52:03.991024Z","iopub.execute_input":"2021-05-24T05:52:03.991355Z","iopub.status.idle":"2021-05-24T05:52:03.99574Z","shell.execute_reply.started":"2021-05-24T05:52:03.991321Z","shell.execute_reply":"2021-05-24T05:52:03.99469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitDataset(nn.Module):\n    def __init__(self, data, tokenizer, max_len = config['MAX_LEN']):\n        self.excerpt = data['excerpt'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.targets = data['target']\n        \n    def __len__(self):\n        return len(self.excerpt)\n    \n    def __getitem__(self,item):\n        inputs = self.tokenizer(self.excerpt[item],\n                            max_length=self.max_len,\n                            padding='max_length',\n                            truncation=True,\n                            return_tensors='pt')\n        target = torch.tensor(self.targets[item], dtype=torch.float)   \n        \n        return inputs,target\n        ","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:52:03.997252Z","iopub.execute_input":"2021-05-24T05:52:03.997886Z","iopub.status.idle":"2021-05-24T05:52:04.007451Z","shell.execute_reply.started":"2021-05-24T05:52:03.99785Z","shell.execute_reply":"2021-05-24T05:52:04.006439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn(output,target):\n    return torch.sqrt(nn.MSELoss()(output,target))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:52:04.009863Z","iopub.execute_input":"2021-05-24T05:52:04.010245Z","iopub.status.idle":"2021-05-24T05:52:04.023763Z","shell.execute_reply.started":"2021-05-24T05:52:04.010206Z","shell.execute_reply":"2021-05-24T05:52:04.022823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=43):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    config['SEED'] = seed\nseed_everything(43)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:52:04.025393Z","iopub.execute_input":"2021-05-24T05:52:04.025823Z","iopub.status.idle":"2021-05-24T05:52:04.040021Z","shell.execute_reply.started":"2021-05-24T05:52:04.025785Z","shell.execute_reply":"2021-05-24T05:52:04.039143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, scheduler, device):\n    model.train()\n    losses = []\n\n    for idx, (data,targets) in tqdm(enumerate(data_loader), total = len(data_loader)):\n        data = {key:val.reshape(val.shape[0],-1).to(device) for key,val in data.items()}\n        \n#         targets = data['targets']\n        targets = targets.to(device)\n        outputs = model(**data)\n\n        optimizer.zero_grad()\n        outputs = model(**data)\n        outputs = outputs[\"logits\"].squeeze(-1)\n        \n        loss = loss_fn(outputs, targets)\n\n#         loss = loss_fn(outputs, targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    print(f\"Train Loss:- {np.mean(losses)}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:52:04.041236Z","iopub.execute_input":"2021-05-24T05:52:04.042009Z","iopub.status.idle":"2021-05-24T05:52:04.050223Z","shell.execute_reply.started":"2021-05-24T05:52:04.041969Z","shell.execute_reply":"2021-05-24T05:52:04.049329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval(data_loader, model, device):\n    model.eval()\n    with torch.no_grad():\n        fin_targets = []\n        fin_outputs = []\n        for idx, (data,targets) in tqdm(enumerate(data_loader), total = len(data_loader)):\n            data = {key:val.reshape(val.shape[0],-1).to(device) for key,val in data.items()}\n            \n            targets = targets.to(device)\n            outputs = model(**data)\n            \n            outputs = outputs[\"logits\"].squeeze(-1)\n\n#             targets = data['targets']\n\n#             outputs = model(data['input_ids'], data['attention_mask'])\n            \n            fin_targets.extend(targets.detach().cpu().detach().numpy().tolist())\n            fin_outputs.extend(outputs.detach().cpu().numpy().tolist())\n        fin_targets = torch.tensor(fin_targets)\n        fin_outputs = torch.tensor(fin_outputs)\n        loss = loss_fn(fin_outputs,fin_targets)\n    return loss,fin_outputs","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:52:04.051334Z","iopub.execute_input":"2021-05-24T05:52:04.051752Z","iopub.status.idle":"2021-05-24T05:52:04.061384Z","shell.execute_reply.started":"2021-05-24T05:52:04.051714Z","shell.execute_reply":"2021-05-24T05:52:04.060506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection,metrics\nimport numpy as np\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\ndef run():\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    \n    tokenizer = config['TOKENIZER']\n    kfold = StratifiedKFold(n_splits=config['FOLDS'],shuffle=True,random_state=config['SEED'])\n    for fold , (train_idx,valid_idx) in enumerate(kfold.split(X=train,y=bins)):\n        start_time = time.time()\n        train_x,valid_x = train.loc[train_idx],train.loc[valid_idx]\n        \n        train_x = train_x.reset_index(drop=True)\n        valid_x = valid_x.reset_index(drop=True)\n\n        train_ds = CommonLitDataset(train_x, tokenizer)\n\n        train_loader = torch.utils.data.DataLoader(\n            train_ds,\n            pin_memory = True,\n            batch_size = config['TRAIN_BATCH_SIZE'],\n            num_workers = 3\n        )\n\n        valid_ds = CommonLitDataset(valid_x, tokenizer)\n\n        valid_loader = torch.utils.data.DataLoader(\n            valid_ds,\n            pin_memory = True,\n            batch_size = config['VALID_BATCH_SIZE'],\n            num_workers = 1\n        )\n\n        \n        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        print(f\"========== USING {device} ==========\")\n        print(f'========== Fold: {fold} ==========')\n        model = AutoModelForSequenceClassification.from_pretrained(config['BERT_PATH'],num_labels=1)\n        model.to(device)\n        \n        tokenizer = config['TOKENIZER']\n\n        param_optimizer = list(model.named_parameters())\n        no_decay = ['bias','LayerNorm.bias','LayerNorm.weight']\n        optimizer_parameters = [\n            {'params' : [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay' : 0.001},\n            {'params' : [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay' : 0.0},\n        ]\n\n        num_train_steps = int(len(train_ds) / config['TRAIN_BATCH_SIZE'] * config['EPOCHS'])\n\n#         optimizer = AdamW(optimizer_parameters, lr = 3e-5, betas=(0.9, 0.999))\n        optimizer = AdamW(model.parameters(), lr = 3e-5, betas=(0.9, 0.999), weight_decay=1e-5)\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps = 0,\n            num_training_steps = num_train_steps\n        )\n#         scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, steps_per_epoch=len(train_ds), max_lr=1e-4, epochs=config['EPOCHS'])\n\n        best_loss = 99999\n        \n        losses_valid = list()\n        best_preds = list()\n        \n        for epoch in range(config['EPOCHS']):\n            start = time.time()\n            \n            print(f'========== epoch : {epoch+1} / {config[\"EPOCHS\"]} ==========')\n            train_fn(train_loader, model, optimizer,scheduler,device)\n            loss,outputs = eval(valid_loader,model,device)\n            print(f'Loss : {loss}')\n            losses_valid.append(loss)\n            \n            end = time.time()\n            elapsed_time = end - start\n            start = end\n            \n            print(f'===== epoch time {elapsed_time} =====')\n\n            if loss < best_loss:\n                print(f'{blu} Loss decreased from {best_loss} -> {loss}')\n                model.save_pretrained(f'{config[\"MODEL_PATH\"]}_{fold}_{epoch}')\n                tokenizer.save_pretrained(f'{config[\"MODEL_PATH\"]}_{fold}_{epoch}')\n                best_preds = outputs\n    #             torch.save(model.state_dict(), config['MODEL_PATH'])\n                best_loss = loss\n        end_time = time.time()\n        elp_fold = end_time - start_time\n        print(f'===== Fold Time: {elp_fold} =====')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:52:04.062938Z","iopub.execute_input":"2021-05-24T05:52:04.063548Z","iopub.status.idle":"2021-05-24T05:52:04.08743Z","shell.execute_reply.started":"2021-05-24T05:52:04.063497Z","shell.execute_reply":"2021-05-24T05:52:04.086602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T05:52:04.088668Z","iopub.execute_input":"2021-05-24T05:52:04.089022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}