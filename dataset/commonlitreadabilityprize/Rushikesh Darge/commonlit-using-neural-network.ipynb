{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# importing libraries","metadata":{"id":"xeG4cEvor50q"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"id":"OlvRq5mFhiL0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Data","metadata":{"id":"-SzxSKtar51O"}},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ndf.head()","metadata":{"id":"CpfYeA69hm75","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Preprocessing","metadata":{"id":"cd7nTvpQr51W"}},{"cell_type":"code","source":"import re\nimport string\nimport nltk\n# nltk.download('stopwords')\n# nltk.download('punkt')\n# nltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n\ndef preprocessing_text(text):\n    text = text.lower()\n    text = re.sub(r'd+','', text)\n    # remove punctuation\n    text = text.translate(str.maketrans('','',string.punctuation))\n    # removing spaces\n    text = text.strip()\n    # remove stopwords\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word not in stop_words]\n    text = ' '.join(str(elem) for elem in filtered_text)\n    # steaming\n    stemmer = PorterStemmer()\n    token_text = word_tokenize(text)\n    for word in token_text:\n        # print(stemmer.stem(word))\n        text = text +' '+stemmer.stem(word)\n    # lemmatization\n    lemmatizer = WordNetLemmatizer()\n    input_text = word_tokenize(text)\n    for word in input_text:\n        text = text + '' + lemmatizer.lemmatize(word)\n    return text","metadata":{"id":"eESb9qWSFhFB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply preprocessing on dataset","metadata":{"id":"9LVDNigHr51Z"}},{"cell_type":"code","source":"df['process_text'] = ''\nfor i in range(0,len(df)):\n    df['process_text'][i] = preprocessing_text(df.excerpt[i])","metadata":{"id":"a-LAlCeHFiBX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"_ryphVlvr51l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## seperate dependent and independent variables","metadata":{"id":"60czHsB6r51m"}},{"cell_type":"code","source":"X = df.process_text\ny = df.target\n\n# splitting dataset for training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, shuffle=False)","metadata":{"id":"hmK9aRa3Fh7m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply TF-IDF","metadata":{"id":"eoMF6TX9r51p"}},{"cell_type":"code","source":"# Initialize the `tfidf_vectorizer` \ntfidf_vectorizer = TfidfVectorizer() \n\n# Fit and transform the training data \ntfidf_train = tfidf_vectorizer.fit_transform(X_train) \n\n# Transform the test set \ntfidf_test = tfidf_vectorizer.transform(X_test)","metadata":{"id":"LwCXFB7fFh16","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_train.shape, X_train.shape, tfidf_test.shape, X_test.shape","metadata":{"id":"ptZrPWB6Fhyr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(tfidf_train)","metadata":{"id":"Un7Muu86io0q","outputId":"fb941035-bb09-40d2-cefc-344748791fd0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting sparse matrix to pandas dataframe\ntfidf_train_df = pd.DataFrame(tfidf_train.toarray())\ntfidf_test_df = pd.DataFrame(tfidf_test.toarray())","metadata":{"id":"zvxrkZUfioyf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network","metadata":{"id":"In_afG_-znUs"}},{"cell_type":"code","source":"# keras tuner help us to choose number of layer and neurons in that layer\n# using randomsearch\n!pip install -U keras-tuner","metadata":{"id":"oeLjj3V9zklD","outputId":"23fbe7ca-32d3-4aff-dbe8-8a59989cfe32","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kerastuner.tuners import RandomSearch","metadata":{"id":"9Rj-8oJ6zQPy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define a model-building function","metadata":{"id":"ZOQULUMr7YLh"}},{"cell_type":"code","source":"def build_model(hp): # hp as a Hyperparameter\n    model = keras.Sequential()\n    for i in range(hp.Int('num_layers',2,30)): # minimum hidden layers 2 and maximum 30\n        model.add(layers.Dense(units=hp.Int('units_' + str(i), # he can choose any between them\n                                            min_value=20, # minimum neuron 20\n                                            max_value=1000, # maximum neuron 1000\n                                            step=32),\n                               activation='relu'))\n        model.add(layers.Dense(1,activation='linear')) # output layer only only contain 1 neuron\n        model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',[1e-2,1e-3,1e-4])), # hp.Choice we restrict his choice between fiven learning rates\n                      loss=keras.losses.MeanSquaredError(),metrics=['mse'])\n    return model","metadata":{"id":"j_GGY6lB0amS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## You can increase executions_per_trial & max_trial to reduce RMSE","metadata":{}},{"cell_type":"code","source":"tuner = RandomSearch(\n    build_model,\n    objective='val_mse',\n    max_trials=3,  # total number of trials\n    executions_per_trial=3, # number of models that should be built and fit for each trial \n    directory='weights',\n    project_name='commonLit'\n)","metadata":{"id":"bZFWnQxJ49tw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.search_space_summary()","metadata":{"id":"LZ9l_2Uzzron","outputId":"6e0f7134-992d-4ac5-d603-fb333e8a435b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### same as model.fit()","metadata":{"id":"25AbOOB-7m9x"}},{"cell_type":"code","source":"tuner.search(tfidf_train_df, y_train,\n             epochs=15,\n             validation_data=(tfidf_test_df, y_test))","metadata":{"id":"uJ7AuJEWzrmN","outputId":"f5799812-d45f-4f3a-e953-4be75b19fdf9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.results_summary()","metadata":{"id":"mrTKlqj_zrkb","outputId":"b19b8456-49e4-48c8-c142-171597813ad7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# give us best top 2 models\nmodels = tuner.get_best_models(num_models=2)","metadata":{"id":"NDdhXerK91rx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we are predicting using best model\ny_pred = models[0].predict(tfidf_test_df)","metadata":{"id":"JsTpyr-X93p0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nprint('Root mean Square is :',rmse)","metadata":{"id":"hhBPDGMM9_xb","outputId":"19d672dc-30d0-432d-86b1-8267ac304eb2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Best Model","metadata":{"id":"8Be0cimD8f3K"}},{"cell_type":"code","source":"# structure of model\nmodels[0].summary()","metadata":{"id":"xAmcmZ7pzrh7","outputId":"7990a2cc-1db2-4fb6-86c5-cb69302eae20","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\n\nmodels[0].save('CommonLit NN.h5')  # creates a HDF5 file 'my_model.h5'\n\n# returns a compiled model\n# identical to the previous one\n# model = load_model('my_model.h5')","metadata":{"id":"ehqYfyYSlhNQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"jVifTAhN4_Sf"},"execution_count":null,"outputs":[]}]}