{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#jsonモジュールのインポート\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nfrom gensim.models.doc2vec import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim.parsing.preprocessing import remove_stopwords\nimport nltk\nfrom nltk.stem import SnowballStemmer\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"snowball = SnowballStemmer(language='english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntrain_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    text = ''.join([k if k not in string.punctuation else ' ' for k in text])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    \n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['excerpt'] = train_df['excerpt'].apply(text_cleaning)\ntest_df['excerpt'] = test_df['excerpt'].apply(text_cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_taggedDocument_from_text(row):\n    text = row['excerpt']\n    #text = remove_stopwords(text)\n    \n    textWordlist = nltk.word_tokenize(text)\n\n    wordlist = [word for word in textWordlist]\n    #wordlist = [snowball.stem(word) for word in textWordlist]\n    return TaggedDocument(words=wordlist, tags=[row['id']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['taggedDocument'] = train_df.apply(create_taggedDocument_from_text, axis=1)\ntest_df['taggedDocument'] = test_df.apply(create_taggedDocument_from_text, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_docs = train_df['taggedDocument'].values.tolist() + test_df['taggedDocument'].values.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 学習実行（パラメータを調整可能）\n# documents:学習データ（TaggedDocumentのリスト）\n# min_count=1:最低1回出現した単語を学習に使用する\n# dm=0:学習モデル=DBOW（デフォルトはdm=1:学習モデル=DM）\ndvmodel = Doc2Vec(documents=training_docs, \n                epochs=50, \n                alpha=0.0025, \n                min_alpha=0.000001, \n                sample=0.001, \n                min_count=5, \n                window=15, \n                negative=5,\n                ns_exponent=0.75, \n                dbow_words=0, \n                dm=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Doc2Vecからベクトルを特徴量として抽出\ntrain_docvecs_df = pd.DataFrame()\ntest_docvecs_df = pd.DataFrame()\n\n\nfor Id in train_df[\"id\"]:\n    train_docvecs_df[Id] = dvmodel.dv[Id]\nfor Id in test_df[\"id\"]:\n    test_docvecs_df[Id] = dvmodel.dv[Id]\n\ntrain_docvecs_df = train_docvecs_df.T\ntrain_docvecs_df = train_docvecs_df.rename_axis('id').reset_index()\n\ntest_docvecs_df = test_docvecs_df.T\ntest_docvecs_df = test_docvecs_df.rename_axis('id').reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(train_docvecs_df.drop('id', axis=1), train_df['target'], test_size = 0.3, random_state=71)\nlgb_train = lgb.Dataset(train_X.values, train_y.values)\nlgb_eval = lgb.Dataset(val_X.values, val_y.values, reference=lgb_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    # 回帰問題\n    'objective': 'regression',\n    # RMSEで評価\n    'metric': 'rmse',\n}\nlgbModel = lgb.train(params, lgb_train, valid_sets=lgb_eval,\n                     verbose_eval=100,  # 50イテレーション毎に学習結果出力\n                     num_boost_round=1000,  # 最大イテレーション回数指定\n                     early_stopping_rounds=500,\n                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = lgbModel.predict(val_X.values, num_iteration=lgbModel.best_iteration)\nrmse = np.sqrt(metrics.mean_squared_error(val_y.values, y_pred))\nrmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = lgbModel.predict(test_docvecs_df.drop('id', axis=1).values, num_iteration=lgbModel.best_iteration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nmy_submission['target'] = predicted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}