{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is created, to perform EDA tasks on the provided datasets to enhance the complexity algorithm**\n\nThis solution is divided into following parts :\n\n*I.* Data Pre-Processing - consists of basic data structuring for visualization purposes\n\n*II.* Data Visualization - consists of graphs, and various other visualization techniques to understand the data\n\n*III.* Model Morphing \n","metadata":{}},{"cell_type":"markdown","source":"**Data Pre-processing**","metadata":{}},{"cell_type":"code","source":"# Importing All the required libraries #\n# A few of them which have local use, have been imported in that block#\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport bokeh as b\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n######## DATA Visualization libraries\n\nfrom pandas_profiling import ProfileReport\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.offline as py\nimport plotly.express as px\ncolor = sns.color_palette()\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nimport plotly.tools as tls\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nos.listdir(\"../input\")\n\n######### For Cleaning data\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n################ For Product Embedding \n\nimport re\nimport nltk \nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nen_stop = set(nltk.corpus.stopwords.words('english'))\nfrom keras.preprocessing.text import Tokenizer\nfrom gensim.models.fasttext import FastText\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom string import punctuation\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize\nfrom nltk import WordPunctTokenizer\nfrom gensim.models.keyedvectors import FastTextKeyedVectors\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generating a basic Pandas Report to check, various aspects of the data available**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\nprof = ProfileReport(df)\nprof.to_file(output_file='output.html')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Since, we 6 columns, for rating the complexity purposes, there is no relation with license and their url, thus we will be dropping them for making our DataFrame smaller and more workable. Later, URL will be added to see, which websites have considerably complex reading materials.*","metadata":{}},{"cell_type":"code","source":"df.shape\ndf.dtypes\ndf.isnull().sum()\ndf.drop(['url_legal', 'license'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}