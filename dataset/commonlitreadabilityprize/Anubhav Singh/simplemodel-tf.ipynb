{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.text import *\nfrom tensorflow.keras.preprocessing.sequence import *\nfrom tensorflow.keras.models import *\nimport tensorflow.keras.backend as k\nfrom tensorflow.keras.optimizers import *\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom tensorflow.keras.callbacks import *\nfrom nltk.corpus import *\nfrom nltk.stem import *\nimport string\nfrom sklearn.preprocessing import *\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_df=pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample_sub=pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load Word Embeddings\nembedding_path='../input/glove840b300dtxt/glove.840B.300d.txt'\nembedding_dict={}\nembd_file=open(embedding_path,'r',errors = 'ignore',encoding='utf8')\nfor line in tqdm(embd_file):\n    values=line.split(' ')\n    word=values[0]\n    coef=np.asarray(values[1:],dtype='float32')\n    embedding_dict[word]=coef\nembd_file.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Clean Text\nsp=stopwords.words('english')\nlm=WordNetLemmatizer()\n\ndef clean_text(df):\n    #Remove punctuation\n    print('Cleaning Punctuations')\n    cleaned_text=[txt.translate(str.maketrans('','',string.punctuation)) for txt in df['excerpt']]\n\n    print('Cleaning numbers')\n    cleaned_text=[' '.join([i for i in txt.lower().split() if i.isalpha()]) for txt in cleaned_text]\n\n    print('Cleaning Stopwords')\n    cleaned_text=[' '.join(i for i in txt.split() if i not in sp) for txt in cleaned_text]\n    \n    #Normalize Word\n    print('Word Normalizing')\n    cleaned_text=[' '.join(lm.lemmatize(i) for i in txt.split()) for txt in cleaned_text]\n    \n    return cleaned_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cleaned=clean_text(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen_=500\nmax_words=20000\nprint('Word Tokenization and Transforming')\ntokenizer=Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(train_cleaned)\nsequences=tokenizer.texts_to_sequences(train_cleaned)\ntrain_data_preped=pad_sequences(sequences,maxlen=maxlen_)\nword_index=tokenizer.word_index\nprint('Tokenization Done!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix=np.zeros((max_words,300))\nprint('Loading Embedding Matrix..\\n')\nfor word,ix in tqdm(word_index.items()):\n    if ix<max_words:\n        embed_vec=embedding_dict.get(word)\n        if embed_vec is not None:\n            embedding_matrix[ix]=embed_vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split Dataset\nX_train,X_val,y_train,y_val=train_test_split(train_data_preped,train_df['target'],test_size=0.15)\nprint('Size of Train: ',X_train.shape)\nprint('Size of Validation: ',X_val.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp=Input(maxlen_)\nx=Embedding(max_words,300)(inp)\nx=Bidirectional(LSTM(256,return_sequences=True))(x)\n\nx=Conv1D(16,5,strides=2,padding='same')(x)\nx=Activation('relu')(x)\n\nx=Conv1D(32,3,strides=2,padding='same')(x)\nx=Activation('relu')(x)\n\nx=Conv1D(64,3,strides=4,padding='same')(x)\nx=Activation('relu')(x)\n\nx=Conv1D(128,3,strides=4,padding='same')(x)\nx=Activation('relu')(x)\n\nx=GRU(256)(x)\nx=Dense(128,activation='relu')(x)\nout=Dense(1)(x)\nmodel=Model(inp,out)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.layers[1].set_weights([embedding_matrix])\nmodel.layers[1].trainable=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n        return k.sqrt(k.mean(k.square(y_pred - y_true))) \n    \nmodel.compile(loss=rmse,optimizer=RMSprop(0.001))\n\n#Callbacks\nrop=ReduceLROnPlateau(min_lr=0.00000001,patience=10)\nmc=ModelCheckpoint('model.h5',save_freq='epoch')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=128,epochs=300,validation_data=(X_val,y_val),\n                  callbacks=[mc])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(figsize=(10,5))\nplt.plot(epochs, loss, 'b', color='red', label='Training loss')\nplt.plot(epochs, val_loss, 'b',color='blue', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test Data**","metadata":{}},{"cell_type":"code","source":"claned_test=clean_text(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Word Tokenization and Transforming of Test data')\ntokenizer=Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(claned_test)\nsequences=tokenizer.texts_to_sequences(claned_test)\ntest_data_preped=pad_sequences(sequences,maxlen=maxlen_)\nword_index=tokenizer.word_index\nprint('Tokenization Done!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=load_model('model.h5',custom_objects={'rmse': rmse})\npreds=model.predict(test_data_preped)\nsample_sub['target']=preds\nsample_sub.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}