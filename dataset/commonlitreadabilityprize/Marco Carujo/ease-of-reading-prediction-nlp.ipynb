{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## The Kernel\n\nThis kernel is an approach to solve the competition problem [CommonLit Readability Prize](https://www.kaggle.com/c/commonlitreadabilityprize).\n\nThe problem could be represented by the following paragraph: \n\n\"Can machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.\"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport plotly.express as px\nfrom plotly import graph_objects as go\n\ndataset = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ndataset.head()\n\n\ndef plot_training(history):\n    # Creating the plotly figure object\n    fig = go.Figure()\n    # Adding accuracy line\n    fig.add_trace(\n        go.Scatter(\n            x=np.arange(len(history.history[\"loss\"])),\n            y=history.history[\"loss\"],\n            text=np.arange(len(history.history[\"loss\"])),\n            mode=\"lines\",\n            name=\"loss\",\n        )\n    )\n    # Adding val_accuracy line\n    fig.add_trace(\n        go.Scatter(\n            x=np.arange(len(history.history[\"val_loss\"])),\n            text=np.arange(len(history.history[\"val_loss\"])),\n            y=history.history[\"val_loss\"],\n            mode=\"lines\",\n            name=\"val_loss\",\n        )\n    )\n    # Formating the graph\n    fig.update_layout(title=\"NN training curve plot.\", xaxis_title=\"Epochs\", yaxis_title=\"Score\", title_font_size=24)\n    # Showing the image\n    fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:31.085003Z","iopub.execute_input":"2021-07-31T22:04:31.085329Z","iopub.status.idle":"2021-07-31T22:04:31.125088Z","shell.execute_reply.started":"2021-07-31T22:04:31.085299Z","shell.execute_reply":"2021-07-31T22:04:31.124126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n\n## First part\n\nStarting the first exploratory data analyses using the columns present in the dataset. \n\n### Files\n- train.csv - the training set\n- test.csv - the test set\n- sample_submission.csv - a sample submission file in the correct format\n### Columns\n- id - unique ID for excerpt\n- url_legal - URL of source - this is blank in the test set.\n- license - license of source material - this is blank in the test set.\n- excerpt - text to predict reading ease of\n- target - reading ease\n- standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n\n[Source](https://www.kaggle.com/c/commonlitreadabilityprize/data)","metadata":{}},{"cell_type":"code","source":"print(\"Dataset shape\", dataset.shape)\ndataset.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:31.126549Z","iopub.execute_input":"2021-07-31T22:04:31.126905Z","iopub.status.idle":"2021-07-31T22:04:31.138883Z","shell.execute_reply.started":"2021-07-31T22:04:31.126868Z","shell.execute_reply":"2021-07-31T22:04:31.137786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the Data descriptions says the licensing information is provided for the public test set (because the associated excerpts are available for display / use), the hidden private test set includes only blank license / legal information.\n","metadata":{}},{"cell_type":"markdown","source":"## id\nJust an ID with a hash with a lenght of 8 chars.","metadata":{}},{"cell_type":"code","source":"print(\"How many unique values?\", len(dataset.id.unique()))\ndataset.id.value_counts()[:5]","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:31.143571Z","iopub.execute_input":"2021-07-31T22:04:31.143873Z","iopub.status.idle":"2021-07-31T22:04:31.160536Z","shell.execute_reply.started":"2021-07-31T22:04:31.143846Z","shell.execute_reply":"2021-07-31T22:04:31.15974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only one ID per line.","metadata":{}},{"cell_type":"markdown","source":"### url_legal\nLooks like the link that you can find the text...","metadata":{}},{"cell_type":"code","source":"print(\"How many missing values?\", dataset.url_legal.isna().sum())\ndataset.url_legal.sample(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:31.164595Z","iopub.execute_input":"2021-07-31T22:04:31.164908Z","iopub.status.idle":"2021-07-31T22:04:31.176889Z","shell.execute_reply.started":"2021-07-31T22:04:31.164881Z","shell.execute_reply":"2021-07-31T22:04:31.175879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### license","metadata":{}},{"cell_type":"code","source":"print(\"How many missing values?\", dataset.license.isna().sum())\ndataset.license.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:31.18104Z","iopub.execute_input":"2021-07-31T22:04:31.181283Z","iopub.status.idle":"2021-07-31T22:04:31.189858Z","shell.execute_reply.started":"2021-07-31T22:04:31.18126Z","shell.execute_reply":"2021-07-31T22:04:31.188864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### excerpt","metadata":{}},{"cell_type":"code","source":"print(\"How many missing values?\", dataset.excerpt.isna().sum())\nprint(\"How many unique values?\", len(dataset.excerpt.unique()))\ndataset[\"excerpt_len\"] = dataset.excerpt.apply(len)\nprint(\"How big or how small is the texts?\")\nfig = px.box(dataset, y=\"excerpt_len\", width=300, title=\"Boxpot excerpt_len column.\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:31.201278Z","iopub.execute_input":"2021-07-31T22:04:31.201567Z","iopub.status.idle":"2021-07-31T22:04:31.283907Z","shell.execute_reply.started":"2021-07-31T22:04:31.201528Z","shell.execute_reply":"2021-07-31T22:04:31.282931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The smaller text has 669 chars, and the bigger has 1341 chars.\n\nJust an example...","metadata":{}},{"cell_type":"code","source":"dataset.excerpt.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:31.285506Z","iopub.execute_input":"2021-07-31T22:04:31.285881Z","iopub.status.idle":"2021-07-31T22:04:31.292546Z","shell.execute_reply.started":"2021-07-31T22:04:31.28584Z","shell.execute_reply":"2021-07-31T22:04:31.291546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### wordcloud (without process)\nJust a word cloud plot to identify the most common words in the dataset.","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\ndef wordcloud(df, column_name, title):\n    all_words = \" \".join([text for text in df[column_name]])\n    wordcloud = WordCloud(\n        width=800, height=500, max_font_size=80, collocations=False\n    ).generate(all_words)\n    plt.figure(figsize=(24, 12))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\n\n\nwordcloud(dataset, \"excerpt\", \"Wordcloud for excerpt column.\")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:31.295009Z","iopub.execute_input":"2021-07-31T22:04:31.295552Z","iopub.status.idle":"2021-07-31T22:04:33.032485Z","shell.execute_reply.started":"2021-07-31T22:04:31.295511Z","shell.execute_reply":"2021-07-31T22:04:33.031481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### target\nReading ease information.","metadata":{}},{"cell_type":"code","source":"dataset.target.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:33.034341Z","iopub.execute_input":"2021-07-31T22:04:33.03466Z","iopub.status.idle":"2021-07-31T22:04:33.049309Z","shell.execute_reply.started":"2021-07-31T22:04:33.034627Z","shell.execute_reply":"2021-07-31T22:04:33.048233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(dataset, x=\"target\", width=600, title=\"Boxpot target column.\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:33.050797Z","iopub.execute_input":"2021-07-31T22:04:33.051175Z","iopub.status.idle":"2021-07-31T22:04:33.123339Z","shell.execute_reply.started":"2021-07-31T22:04:33.051132Z","shell.execute_reply":"2021-07-31T22:04:33.122577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Continuous values that could be negative, zero or positive. A perfect gaussian distribution.","metadata":{}},{"cell_type":"markdown","source":"### standard_error","metadata":{}},{"cell_type":"code","source":"dataset.standard_error.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:33.124532Z","iopub.execute_input":"2021-07-31T22:04:33.12501Z","iopub.status.idle":"2021-07-31T22:04:33.135931Z","shell.execute_reply.started":"2021-07-31T22:04:33.124972Z","shell.execute_reply":"2021-07-31T22:04:33.134952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.box(\n    dataset, y=\"standard_error\", width=300, title=\"Boxpot standard_error column.\"\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:33.1375Z","iopub.execute_input":"2021-07-31T22:04:33.138033Z","iopub.status.idle":"2021-07-31T22:04:33.212352Z","shell.execute_reply.started":"2021-07-31T22:04:33.137995Z","shell.execute_reply":"2021-07-31T22:04:33.211477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There is relation between the text size and dificulty to read? ","metadata":{}},{"cell_type":"code","source":"with sns.axes_style(\"white\"):\n    table = dataset.corr()\n    mask = np.zeros_like(table)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(5, 5))\n    sns.heatmap(\n        table,\n        cmap=\"Blues\",\n        mask=mask,\n        vmax=0.3,\n        linewidths=0.5,\n        annot=True,\n        annot_kws={\"size\": 15},\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:33.214851Z","iopub.execute_input":"2021-07-31T22:04:33.215409Z","iopub.status.idle":"2021-07-31T22:04:33.476464Z","shell.execute_reply.started":"2021-07-31T22:04:33.215366Z","shell.execute_reply":"2021-07-31T22:04:33.475563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a considerable inverse correlation between target and excerpt lenght!","metadata":{}},{"cell_type":"markdown","source":"# NLP\n\n- 1 - Remove stopwords and ponctuation.\n- 2 - TFDIF (word frequency).\n- 3 - Regression.\n- 4 - XAI - SHAP.","metadata":{}},{"cell_type":"markdown","source":"### Remove stowords and ponctuation","metadata":{}},{"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.stem import SnowballStemmer\n\n\nstemmer = nltk.SnowballStemmer(\"english\")\nstopwords = list(stopwords.words(\"english\"))\npunctuation = [word for word in string.punctuation]\npunctuation += [\"...\", \"  \", \"\\n\"]\n\n\ndef remove_punctuation(serie, stopwords):\n    aux = list()\n    for el in serie:\n        for word in stopwords:\n            el = el.replace(word, \" \")\n        aux.append(el)\n    return aux\n\n\ndef remove_stopwords(serie, stopwords):\n    tokenizer = nltk.WordPunctTokenizer()\n\n    result_serie = list()\n    for row in serie:\n        aux = list()\n        text_row = tokenizer.tokenize(row.lower())\n        for word in text_row:\n            if word not in stopwords:  # stopwords\n                aux.append(stemmer.stem(word))\n        result_serie.append(\" \".join(aux))\n    return result_serie\n\n\ndataset[\"excerpt_cleaned\"] = dataset.excerpt.str.lower()\ndataset[\"excerpt_cleaned\"] = remove_stopwords(dataset.excerpt_cleaned, punctuation)\ndataset[\"excerpt_cleaned\"] = remove_stopwords(dataset.excerpt_cleaned, stopwords)\ndataset[\"excerpt_cleaned\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:33.478346Z","iopub.execute_input":"2021-07-31T22:04:33.478752Z","iopub.status.idle":"2021-07-31T22:04:46.02245Z","shell.execute_reply.started":"2021-07-31T22:04:33.478693Z","shell.execute_reply":"2021-07-31T22:04:46.021682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TFDIF (word frequency) and feature transformation","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nvectorize = TfidfVectorizer(min_df=50,  max_features=None, \n            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n            stop_words = 'english')\n\nohe = OneHotEncoder()\nmms_X = StandardScaler()\nmms_y = MinMaxScaler()\n\ndataset = dataset[['license','excerpt_len','excerpt_cleaned', 'target']]\ndataset['license'] = dataset.license.fillna(\"Not Known\")\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset.drop('target',axis=1), dataset.target , test_size=0.10, random_state=42\n)\n\ndef feature_transformation(X_raw, y_raw,  vectorize, ohe, mms_X, mms_y, fit=True):\n    if fit:\n        ohe.fit(X_raw.license.values.reshape(-1, 1))\n        mms_X.fit(X_raw.excerpt_len.values.reshape(-1, 1))\n        vectorize.fit(X_raw[\"excerpt_cleaned\"])\n        mms_y.fit(y_raw.values.reshape(-1,1))\n            \n    X = np.concatenate(\n        (\n            ohe.transform(X_raw.license.values.reshape(-1, 1)).todense(),\n            mms_X.transform(X_raw.excerpt_len.values.reshape(-1, 1)),\n            vectorize.transform(X_raw[\"excerpt_cleaned\"]).todense(),\n        ),\n        axis=1,\n    )\n#     y = mms_y.transform(y_raw.values.reshape(-1,1))\n    y = y_raw.values.reshape(-1,1)\n    return X,y\n\nX_train, y_train = feature_transformation(X_train, y_train, vectorize, ohe, mms_X, mms_y,  fit=True)\nX_test, y_test = feature_transformation(X_test, y_test,vectorize, ohe, mms_X, mms_y,  fit=False)\n\nprint(\"How many features (bag of words): \", len(vectorize.get_feature_names()))\nprint(\"Dataset shape: \", X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:46.023623Z","iopub.execute_input":"2021-07-31T22:04:46.023956Z","iopub.status.idle":"2021-07-31T22:04:47.82232Z","shell.execute_reply.started":"2021-07-31T22:04:46.023929Z","shell.execute_reply":"2021-07-31T22:04:47.821283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Y target transformed","metadata":{}},{"cell_type":"code","source":"\naux = pd.DataFrame(y_train)\naux.columns = ['Y_value']\n\nfig = px.histogram(aux, x=\"Y_value\", nbins=50)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:47.82376Z","iopub.execute_input":"2021-07-31T22:04:47.824338Z","iopub.status.idle":"2021-07-31T22:04:47.890124Z","shell.execute_reply.started":"2021-07-31T22:04:47.824294Z","shell.execute_reply":"2021-07-31T22:04:47.889171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Regression\n\n\nRegression using neural network and target scalded with values between 0~1.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (Dense, Dropout,Conv1D,GlobalMaxPooling1D, Embedding, BatchNormalization)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import RMSprop,Adam\n\nINPUT_SIZE = X_train.shape[1]\ncallback = EarlyStopping(monitor=\"loss\", patience=5)\n\ndef plot_history(history):\n    acc = history.history['mae']\n    val_acc = history.history['val_mae']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training mae')\n    plt.plot(x, val_acc, 'r', label='Validation mae')\n    plt.title('Training and validation mae')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    \ndef baseline_model(neurons=1000, hidden_lenght=10, learning_rate=0.0001, dropout=0.2):\n\n    model = Sequential()\n    model.add(Dense(neurons, input_dim=INPUT_SIZE, activation='relu'))\n    model.add(Dropout(dropout))\n    \n    for _ in range(1,hidden_lenght + 1):\n        model.add(Dense(neurons, activation='relu'))\n        model.add(Dropout(dropout))\n        \n    model.add(Dense(1))\n    \n    optimizer = Adam(lr=learning_rate)\n    model.compile(loss='mean_squared_error',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:47.891491Z","iopub.execute_input":"2021-07-31T22:04:47.891832Z","iopub.status.idle":"2021-07-31T22:04:47.90628Z","shell.execute_reply.started":"2021-07-31T22:04:47.891797Z","shell.execute_reply":"2021-07-31T22:04:47.905242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Traning the model","metadata":{}},{"cell_type":"code","source":"model = baseline_model(\n    neurons=1000,\n    hidden_lenght=5,\n    learning_rate=0.0001,\n    dropout=0.4\n)\nhistory = model.fit(\n    X_train, y_train, batch_size=5,\n    validation_data=(X_test, y_test),\n    epochs=100,\n    verbose=1,\n    callbacks=[callback],\n)\nprint('R2 score:',r2_score(y_test, model.predict(X_test)))\nplot_training(history)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:47.907814Z","iopub.execute_input":"2021-07-31T22:04:47.908356Z","iopub.status.idle":"2021-07-31T22:04:50.872095Z","shell.execute_reply.started":"2021-07-31T22:04:47.908313Z","shell.execute_reply":"2021-07-31T22:04:50.870008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nLoading the submision dataset.","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nsubmission[\"excerpt_cleaned\"] = submission.excerpt.str.lower()\nsubmission[\"excerpt_cleaned\"] = remove_stopwords(\n    submission.excerpt_cleaned, punctuation\n)\nsubmission[\"excerpt_cleaned\"] = remove_stopwords(submission.excerpt_cleaned, stopwords)\nsubmission[\"excerpt_len\"] = submission.excerpt.apply(len)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:50.873332Z","iopub.status.idle":"2021-07-31T22:04:50.874029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Formating the submission dataset.","metadata":{}},{"cell_type":"code","source":"X_sub = np.concatenate(\n    (\n        ohe.transform(\n            submission.license.fillna(\"Not Known\").values.reshape(-1, 1)\n        ).todense(),\n        mms_X.transform(submission.excerpt_len.values.reshape(-1, 1)),\n        vectorize.transform(submission[\"excerpt_cleaned\"]).todense(),\n\n    ),\n    axis=1,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:50.875222Z","iopub.status.idle":"2021-07-31T22:04:50.87592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction the new dataset.","metadata":{}},{"cell_type":"code","source":"# submission[\"target\"] = mms_y.inverse_transform(model.predict(X_sub))\nsubmission[\"target\"] = model.predict(X_sub)\nsubmission[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:04:50.876974Z","iopub.status.idle":"2021-07-31T22:04:50.877663Z"},"trusted":true},"execution_count":null,"outputs":[]}]}