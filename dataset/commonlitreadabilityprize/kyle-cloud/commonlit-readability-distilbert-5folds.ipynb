{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# overview\n\nFor Competition [CommonLit Readability Prize](https://www.kaggle.com/c/commonlitreadabilityprize)\n\n    release the train section for train\n    release the submit section for test","metadata":{}},{"cell_type":"markdown","source":"    well, this notebook has a really low rank -_-! but I hope its structure could help you a little bit more.\n    The only thing you should change is the get_model function\n    Then you could train your own model with preprocess, K-FOLD and inferences.\n    Thank you!","metadata":{}},{"cell_type":"markdown","source":"# 1. load data","metadata":{"trusted":true}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-29T08:03:20.745525Z","iopub.execute_input":"2021-07-29T08:03:20.745956Z","iopub.status.idle":"2021-07-29T08:03:22.115444Z","shell.execute_reply.started":"2021-07-29T08:03:20.74587Z","shell.execute_reply":"2021-07-29T08:03:22.114633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ndf_test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:22.116903Z","iopub.execute_input":"2021-07-29T08:03:22.117268Z","iopub.status.idle":"2021-07-29T08:03:22.218058Z","shell.execute_reply.started":"2021-07-29T08:03:22.11723Z","shell.execute_reply":"2021-07-29T08:03:22.217262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:22.219654Z","iopub.execute_input":"2021-07-29T08:03:22.219964Z","iopub.status.idle":"2021-07-29T08:03:22.246522Z","shell.execute_reply.started":"2021-07-29T08:03:22.219938Z","shell.execute_reply":"2021-07-29T08:03:22.245477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:22.24821Z","iopub.execute_input":"2021-07-29T08:03:22.248561Z","iopub.status.idle":"2021-07-29T08:03:22.259386Z","shell.execute_reply.started":"2021-07-29T08:03:22.248517Z","shell.execute_reply":"2021-07-29T08:03:22.258206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:22.261114Z","iopub.execute_input":"2021-07-29T08:03:22.261519Z","iopub.status.idle":"2021-07-29T08:03:22.283585Z","shell.execute_reply.started":"2021-07-29T08:03:22.261479Z","shell.execute_reply":"2021-07-29T08:03:22.282578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:22.285036Z","iopub.execute_input":"2021-07-29T08:03:22.285393Z","iopub.status.idle":"2021-07-29T08:03:22.297316Z","shell.execute_reply.started":"2021-07-29T08:03:22.285357Z","shell.execute_reply":"2021-07-29T08:03:22.296156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of count of words\nwords = df_train['excerpt'].str.split().apply(len)\nplt.figure(figsize=(10,5))\nplt.hist(words, alpha=0.8, bins=15)\nplt.legend(loc='best')\nplt.xlabel('Count of words')\nplt.ylabel('Count')\nplt.title('Count of words in excerpt')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:22.298841Z","iopub.execute_input":"2021-07-29T08:03:22.299574Z","iopub.status.idle":"2021-07-29T08:03:22.548463Z","shell.execute_reply.started":"2021-07-29T08:03:22.299509Z","shell.execute_reply":"2021-07-29T08:03:22.547509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. preprocess data","metadata":{}},{"cell_type":"code","source":"import re","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:22.551025Z","iopub.execute_input":"2021-07-29T08:03:22.551375Z","iopub.status.idle":"2021-07-29T08:03:22.555325Z","shell.execute_reply.started":"2021-07-29T08:03:22.551339Z","shell.execute_reply":"2021-07-29T08:03:22.554209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(txt):\n    txt = re.sub(\"[^a-zA-Z]\", \" \", txt)\n    txt = txt.lower()\n\n    txt = nltk.word_tokenize(txt)\n    txt = [word for word in txt if not word in set(stopwords.words(\"english\"))]\n\n    lemma = nltk.WordNetLemmatizer()\n    txt = [lemma.lemmatize(word) for word in txt]\n    txt = \" \".join(txt)\n    return txt\n\n# df_train['excerpt'] = df_train['excerpt'].apply(lambda x: clean_text(x))\n# df_test['excerpt'] = df_test['excerpt'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:22.557113Z","iopub.execute_input":"2021-07-29T08:03:22.557532Z","iopub.status.idle":"2021-07-29T08:03:22.565319Z","shell.execute_reply.started":"2021-07-29T08:03:22.557492Z","shell.execute_reply":"2021-07-29T08:03:22.564479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:22.566696Z","iopub.execute_input":"2021-07-29T08:03:22.56705Z","iopub.status.idle":"2021-07-29T08:03:22.582181Z","shell.execute_reply.started":"2021-07-29T08:03:22.567014Z","shell.execute_reply":"2021-07-29T08:03:22.581155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Tokenization","metadata":{}},{"cell_type":"markdown","source":"# 4. Train model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, RobertaTokenizer, TFRobertaForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:22.583954Z","iopub.execute_input":"2021-07-29T08:03:22.584477Z","iopub.status.idle":"2021-07-29T08:03:28.716887Z","shell.execute_reply.started":"2021-07-29T08:03:22.584434Z","shell.execute_reply":"2021-07-29T08:03:28.715723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAXLEN = 512\nBATCH_SIZE = 3\nEPOCHS = 20\nLR = 1e-5\nN_SPLITS = 5\n\ndef get_model(bert_model):\n    input_ids = tf.keras.layers.Input(shape=(MAXLEN, ), dtype='int32', name='input_ids')\n    attention_mask = tf.keras.layers.Input(shape=(MAXLEN, ), dtype='int32', name='attention_mask')\n#     token_type_ids = tf.keras.layers.Input(shape=(MAXLEN, ), dtype='int32', name='token_type_ids')\n\n    X = bert_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n    outputs = tf.keras.layers.Dense(1, use_bias=True, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(X)\n    \n    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(lr=LR), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:28.721621Z","iopub.execute_input":"2021-07-29T08:03:28.721967Z","iopub.status.idle":"2021-07-29T08:03:28.744184Z","shell.execute_reply.started":"2021-07-29T08:03:28.721932Z","shell.execute_reply":"2021-07-29T08:03:28.742242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold = 0\ntrain = df_train['excerpt']\ntargets = df_train['target']\n\nfor train_idx, val_idx in KFold(N_SPLITS, shuffle=True, random_state=2021).split(train):\n    # get data\n    if fold != 0:\n        fold += 1\n        continue\n    \n    X_train = train[train_idx]\n    X_val = train[val_idx]\n    y_train = targets[train_idx]\n    y_val = targets[val_idx]\n    \n    y_train = tf.constant(y_train, dtype=tf.float32)\n    y_val = tf.constant(y_val, dtype=tf.float32)\n    \n    # process data\n    X_train = [clean_text(x) for x in X_train]\n    X_val = [clean_text(x) for x in X_val]\n    \n    # get model\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    bert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n\n    model = get_model(bert_model)\n    # model.load_weights('../input/commonlit-readability-model/checkpoint/variables/variables')\n    model.summary()\n    \n    # tokenize data\n    X_train = tokenizer(X_train, padding=\"max_length\", max_length=MAXLEN, return_tensors='tf', truncation=True)\n    X_val = tokenizer(X_val, padding=\"max_length\", max_length=MAXLEN, return_tensors='tf', truncation=True)\n    X_train = {\"input_ids\": X_train['input_ids'], \"attention_mask\": X_train['attention_mask']}\n    X_val = {\"input_ids\": X_val['input_ids'], \"attention_mask\": X_val['attention_mask']}\n    \n    # train model\n    checkpoint = [tf.keras.callbacks.ModelCheckpoint(f'Fold{fold}/checkpoint', save_weights_only=False, save_best_only=True)]\n    model.fit(X_train, y_train, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), epochs=EPOCHS, callbacks=[checkpoint])\n    \n    # save model\n    tokenizer.save_pretrained(f'Fold{fold}/tokenizer/')\n    bert_model.save_pretrained(f'Fold{fold}/distil_bert/')\n    \n    fold += 1","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:28.7489Z","iopub.execute_input":"2021-07-29T08:03:28.749404Z","iopub.status.idle":"2021-07-29T08:03:28.761314Z","shell.execute_reply.started":"2021-07-29T08:03:28.749368Z","shell.execute_reply":"2021-07-29T08:03:28.760355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Save model","metadata":{}},{"cell_type":"markdown","source":"# 6. Submission","metadata":{}},{"cell_type":"code","source":"# test = df_test['excerpt']\n# X_test = [clean_text(x) for x in test]\n# X_test[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:28.767611Z","iopub.execute_input":"2021-07-29T08:03:28.770985Z","iopub.status.idle":"2021-07-29T08:03:31.071371Z","shell.execute_reply.started":"2021-07-29T08:03:28.770945Z","shell.execute_reply":"2021-07-29T08:03:31.070638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result = 0\n# for i in range(N_SPLITS):\n#     # get model\n#     tokenizer = RobertaTokenizer.from_pretrained(f'../input/commonlit-readability-model/robert-large/Fold{i}/tokenizer/')\n#     bert_model = TFRobertaForSequenceClassification.from_pretrained(f'../input/commonlit-readability-model/robert-large/Fold{i}/distil_bert')\n\n#     model = get_model(bert_model)\n#     if i != 1: model.load_weights(f'../input/commonlit-readability-model/robert-large/Fold{i}/checkpoint/variables/variables')\n#     else: model.load_weights(f'../input/commonlit-readability-model/robert-large/Fold{i}/checkpoint/variables')\n#     # tokenize data\n#     X_test_token = tokenizer(X_test, padding=\"max_length\", max_length=MAXLEN, return_tensors='tf', truncation=True)\n#     X_test_token = {\"input_ids\": X_test_token['input_ids'], \"attention_mask\": X_test_token['attention_mask']}\n    \n#     # predict\n#     result += model.predict(X_test_token)\n\n# # result /= N_SPLITS","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:06:30.992968Z","iopub.execute_input":"2021-07-29T08:06:30.99336Z","iopub.status.idle":"2021-07-29T08:06:55.370515Z","shell.execute_reply.started":"2021-07-29T08:06:30.993315Z","shell.execute_reply":"2021-07-29T08:06:55.368139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # result = 0\n# for i in range(N_SPLITS):\n#     # get model\n#     tokenizer = DistilBertTokenizer.from_pretrained(f'../input/commonlit-readability-model/distil/Fold{i}/tokenizer/')\n#     bert_model = TFDistilBertForSequenceClassification.from_pretrained(f'../input/commonlit-readability-model/distil/Fold{i}/distil_bert')\n\n#     model = get_model(bert_model)\n#     model.load_weights(f'../input/commonlit-readability-model/distil/Fold{i}/checkpoint/variables/variables')\n    \n#     # tokenize data\n#     X_test_token = tokenizer(X_test, padding=\"max_length\", max_length=MAXLEN, return_tensors='tf', truncation=True)\n#     X_test_token = {\"input_ids\": X_test_token['input_ids'], \"attention_mask\": X_test_token['attention_mask']}\n    \n#     # predict\n#     result += model.predict(X_test_token)\n\n# result /= (N_SPLITS*2)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T08:03:31.074248Z","iopub.execute_input":"2021-07-29T08:03:31.074504Z","iopub.status.idle":"2021-07-29T08:03:31.077859Z","shell.execute_reply.started":"2021-07-29T08:03:31.074477Z","shell.execute_reply":"2021-07-29T08:03:31.077038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df = pd.DataFrame({'id': df_test.id, 'target': 0})\n# submission_df.target = result\n\n# submission_file = 'submission.csv'\n# submission_df.to_csv(submission_file, index=False)\n\n# submission_df","metadata":{"execution":{"iopub.status.busy":"2021-07-29T07:00:31.764962Z","iopub.status.idle":"2021-07-29T07:00:31.765342Z"},"trusted":true},"execution_count":null,"outputs":[]}]}