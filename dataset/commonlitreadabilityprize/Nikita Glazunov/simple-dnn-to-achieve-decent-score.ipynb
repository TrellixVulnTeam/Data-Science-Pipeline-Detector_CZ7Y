{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Here is my notebook to solve this task using simple dnn (dense neural network)!**","metadata":{}},{"cell_type":"markdown","source":"# Import libs","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T18:34:55.762664Z","iopub.execute_input":"2021-07-27T18:34:55.763041Z","iopub.status.idle":"2021-07-27T18:35:01.703841Z","shell.execute_reply.started":"2021-07-27T18:34:55.762958Z","shell.execute_reply":"2021-07-27T18:35:01.702182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n\ntrain.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:01.705675Z","iopub.execute_input":"2021-07-27T18:35:01.70606Z","iopub.status.idle":"2021-07-27T18:35:01.900562Z","shell.execute_reply.started":"2021-07-27T18:35:01.706024Z","shell.execute_reply":"2021-07-27T18:35:01.899757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"During all experiments I found out the best way to preprocess the text. Somehow non alphabet characters doens't need to be deleted.","metadata":{}},{"cell_type":"code","source":"def text_preprocessing(text):\n    tokenized = word_tokenize(text)\n    text = [i for i in tokenized if i not in stopwords.words('english')]\n    text = ' '.join(text)\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:01.902391Z","iopub.execute_input":"2021-07-27T18:35:01.902724Z","iopub.status.idle":"2021-07-27T18:35:01.913158Z","shell.execute_reply.started":"2021-07-27T18:35:01.902689Z","shell.execute_reply":"2021-07-27T18:35:01.91236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = train.excerpt.apply(text_preprocessing)\ny = train.target","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:01.914847Z","iopub.execute_input":"2021-07-27T18:35:01.919305Z","iopub.status.idle":"2021-07-27T18:36:13.720843Z","shell.execute_reply.started":"2021-07-27T18:35:01.91926Z","shell.execute_reply":"2021-07-27T18:36:13.720013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\n\n# fit only train set\ntokenizer.fit_on_texts(x)\n\n# in case you have validation/test dataset don't forget to transform val/test data on already fitted train data\nx = tokenizer.texts_to_sequences(x)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:36:13.722262Z","iopub.execute_input":"2021-07-27T18:36:13.722597Z","iopub.status.idle":"2021-07-27T18:36:14.229527Z","shell.execute_reply.started":"2021-07-27T18:36:13.722564Z","shell.execute_reply":"2021-07-27T18:36:14.228604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find out the longest sequence length for padding\nlen_seq_list = [len(s) for s in x]\nmax_seq_len = np.max(len_seq_list)\n\n# pad sequence for first embedding layer\nx = tf.keras.preprocessing.sequence.pad_sequences(\n    x,\n    padding='post',\n    truncating='post',\n    maxlen=max_seq_len\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:36:14.230788Z","iopub.execute_input":"2021-07-27T18:36:14.231305Z","iopub.status.idle":"2021-07-27T18:36:14.311613Z","shell.execute_reply.started":"2021-07-27T18:36:14.231268Z","shell.execute_reply":"2021-07-27T18:36:14.310773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyperparameters\nvoc_size = len(tokenizer.index_word) + 1\nepochs = 100\nbatch_size = 1024\nembedding_dim = 512\ndropout_rate = .5","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:36:14.312903Z","iopub.execute_input":"2021-07-27T18:36:14.313313Z","iopub.status.idle":"2021-07-27T18:36:14.320096Z","shell.execute_reply.started":"2021-07-27T18:36:14.313278Z","shell.execute_reply":"2021-07-27T18:36:14.319256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(voc_size, embedding_dim, input_length=max_seq_len),\n#     gap1d layer shows better accuracy than flatten layer\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(dropout_rate),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(dropout_rate),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(dropout_rate),\n    tf.keras.layers.Dense(1)\n])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:36:14.322651Z","iopub.execute_input":"2021-07-27T18:36:14.323066Z","iopub.status.idle":"2021-07-27T18:36:16.447118Z","shell.execute_reply.started":"2021-07-27T18:36:14.32301Z","shell.execute_reply":"2021-07-27T18:36:16.446236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:36:16.448682Z","iopub.execute_input":"2021-07-27T18:36:16.449032Z","iopub.status.idle":"2021-07-27T18:36:16.462519Z","shell.execute_reply.started":"2021-07-27T18:36:16.448995Z","shell.execute_reply":"2021-07-27T18:36:16.461684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decent callbacks to control train/val loss to not let the model to overfit.","metadata":{}},{"cell_type":"code","source":"early_stopping = EarlyStopping(patience=8, restore_best_weights=True, verbose=1)\nlr_reduce = ReduceLROnPlateau(patience=3, verbose=1)\n\ncallbacks = [early_stopping, lr_reduce]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:36:16.464182Z","iopub.execute_input":"2021-07-27T18:36:16.464587Z","iopub.status.idle":"2021-07-27T18:36:16.47176Z","shell.execute_reply.started":"2021-07-27T18:36:16.464548Z","shell.execute_reply":"2021-07-27T18:36:16.470757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss='mean_squared_error'\n)\n\nhistory = model.fit(\n    x,\n    y,\n    epochs=epochs,\n    batch_size=batch_size,\n    validation_split=.2,\n    callbacks=callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:36:16.473468Z","iopub.execute_input":"2021-07-27T18:36:16.473955Z","iopub.status.idle":"2021-07-27T18:36:50.19203Z","shell.execute_reply.started":"2021-07-27T18:36:16.473843Z","shell.execute_reply":"2021-07-27T18:36:50.191099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make a submission file","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:36:50.193535Z","iopub.execute_input":"2021-07-27T18:36:50.193926Z","iopub.status.idle":"2021-07-27T18:36:50.210358Z","shell.execute_reply.started":"2021-07-27T18:36:50.193885Z","shell.execute_reply":"2021-07-27T18:36:50.20961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = test.excerpt.apply(text_preprocessing)\ntest_data = tokenizer.texts_to_sequences(test_data)\ntest_data = tf.keras.preprocessing.sequence.pad_sequences(\n    test_data,\n    padding='post',\n    truncating='post',\n    maxlen=max_seq_len\n)\n\ntest_pred = model.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:36:50.211567Z","iopub.execute_input":"2021-07-27T18:36:50.211928Z","iopub.status.idle":"2021-07-27T18:36:50.512133Z","shell.execute_reply.started":"2021-07-27T18:36:50.211895Z","shell.execute_reply":"2021-07-27T18:36:50.511177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'] = test_pred\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:36:50.513592Z","iopub.execute_input":"2021-07-27T18:36:50.513997Z","iopub.status.idle":"2021-07-27T18:36:50.524299Z","shell.execute_reply.started":"2021-07-27T18:36:50.513958Z","shell.execute_reply":"2021-07-27T18:36:50.523387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks for watching!\n\nIf you have any *questions* or *suggestions* feel free to write 'em down below!","metadata":{}}]}