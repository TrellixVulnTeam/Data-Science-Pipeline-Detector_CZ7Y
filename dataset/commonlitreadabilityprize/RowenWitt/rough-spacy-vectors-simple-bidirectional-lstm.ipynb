{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('output/kaggle/working//model1'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:57:15.382732Z","iopub.execute_input":"2021-07-15T07:57:15.383129Z","iopub.status.idle":"2021-07-15T07:57:15.400641Z","shell.execute_reply.started":"2021-07-15T07:57:15.38302Z","shell.execute_reply":"2021-07-15T07:57:15.399554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Use spacy for one go\nimport spacy\nnlp = spacy.load('en_core_web_lg')\n\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:57:15.402721Z","iopub.execute_input":"2021-07-15T07:57:15.403157Z","iopub.status.idle":"2021-07-15T07:57:33.1505Z","shell.execute_reply.started":"2021-07-15T07:57:15.403115Z","shell.execute_reply":"2021-07-15T07:57:33.149288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\nsubmission = pd.read_csv('/kaggle/input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:57:33.152693Z","iopub.execute_input":"2021-07-15T07:57:33.153016Z","iopub.status.idle":"2021-07-15T07:57:33.273715Z","shell.execute_reply.started":"2021-07-15T07:57:33.152986Z","shell.execute_reply":"2021-07-15T07:57:33.27266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to up url_legal category\ndef clean_url(data):\n    if type(data) == float:\n        return \"None\"\n    else:\n        data = re.sub(r\"https://|/wiki/|/|.org|pdf|wp-cont|-|%20|%|www.|frym.\", \" \", data)\n        data = re.sub(\"[^a-zA-Z -]\", \" \", data.lower())\n        data = re.sub(\"\\s+\", \" \", data)\n    return data\n\n# Function to clean up text category\ndef clean_text(data):\n    if type(data) == float:\n        pass\n    else:\n        data = re.sub(r'\\n|\\\\', \" \", data.lower())\n        data = re.sub(\"[^a-zA-Z]\", \" \", data.lower())\n        data = re.sub(\"\\s+\", \" \", data)\n    return data\n\n# Function to get sentences from input text\ndef get_sentences(data):\n    row_doc = nlp(data)\n    sents = list(row_doc.sents)\n    return sents\n# This will probably nor be helpful in this case, more useful for sentiment than complexity\n\n# Function to generate tokens from input text\ndef gen_tokens(data):\n    row_doc = nlp(data)\n    return row_doc\n\n# Removes stop words from tokenized text\ndef remove_stopwords(data):\n    n_list = []\n    for i in data:\n        n_list.append([j.vector for j in i if not j.is_stop])\n    return n_list\n\ndef remove_stopwords_url(data):\n    n_list = []\n    for i in data:\n        n_list.append([j.text for j in i if not j.is_stop])\n    return n_list","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:57:33.291732Z","iopub.execute_input":"2021-07-15T07:57:33.292181Z","iopub.status.idle":"2021-07-15T07:57:33.307565Z","shell.execute_reply.started":"2021-07-15T07:57:33.29213Z","shell.execute_reply":"2021-07-15T07:57:33.306233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# work = 'Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?” So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her. There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, “Oh dear! Oh dear! I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge. In another moment down went Alice after it, never once considering how in the world she was to get out again. The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well. Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled “ORANGE MARMALADE”, but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody underneath, so managed to put it into one of the cupboards as she fell past it. “Well!” thought Alice to herself, “after such a fall as this, I shall think nothing of tumbling down stairs! How brave they’ll all think me at home! Why, I wouldn’t say anything about it, even if I fell off the top of the house!” (Which was very likely true.) Down, down, down. Would the fall never come to an end? “I wonder how many miles I’ve fallen by this time?” she said aloud. “I must be getting somewhere near the centre of the earth. Let me see: that would be four thousand miles down, I think—” (for, you see, Alice had learnt several things of this sort in her lessons in the schoolroom, and though this was not a very good opportunity for showing off her knowledge, as there was no one to listen to her, still it was good practice to say it over) “—yes, that’s about the right distance—but then I wonder what Latitude or Longitude I’ve got to?” (Alice had no idea what Latitude was, or Longitude either, but thought they were nice grand words to say.)'","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:57:33.309621Z","iopub.execute_input":"2021-07-15T07:57:33.310603Z","iopub.status.idle":"2021-07-15T07:57:33.322232Z","shell.execute_reply.started":"2021-07-15T07:57:33.310554Z","shell.execute_reply":"2021-07-15T07:57:33.320961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split train into X and Y\nX = train.drop(columns='target')\nY = train['target']\n# Apply clean_url function to train & test 'url_legal'\n# clean_urls_train = [clean_url(i) for i in X['url_legal']]\n# clean_urls_test = [clean_url(i) for i in test['url_legal']]\n# Apply clean_text to train & test 'excerpt'\nclean_text_train = [clean_text(i) for i in X['excerpt']]\nclean_text_test = [clean_text(i) for i in test['excerpt']]\n\n#clean_work = clean_text(work)\n\n# Add cleaned columns back\n# X['clean_url'] = clean_urls_train\nX['clean_text'] = clean_text_train\n# test['clean_url'] = clean_urls_test\ntest['clean_text'] = clean_text_test\n# Generate tokens from clean_text\n# train_url_tokens = remove_stopwords_url([gen_tokens(i) for i in X['clean_url']])\ntrain_excerpt_tokens = remove_stopwords([gen_tokens(i) for i in X['clean_text']])\n# test_url_tokens = remove_stopwords_url([gen_tokens(i) for i in test['clean_url']])\ntest_excerpt_tokens = remove_stopwords([gen_tokens(i) for i in test['clean_text']])\n\n#clean_work_tokens = remove_stopwords(gen_tokens(clean_work))\n\n# Add tokenized columns to train & test\n# X['url_tokens'] = train_url_tokens\nX['excerpt_tokens'] = train_excerpt_tokens\n# test['url_tokens'] = test_url_tokens\ntest['excerpt_tokens'] = test_excerpt_tokens\n# Drop uneeded\n# X.drop(columns=['url_legal', 'excerpt', 'clean_text', 'id', 'standard_error'], inplace=True)\n# test.drop(columns=['url_legal', 'excerpt', 'clean_text', 'id'], inplace=True)\n# Split train into train & validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=42)\nprint(\"Train:\",(X_train.shape, y_train.shape), \"Val:\",(X_val.shape, y_val.shape))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:57:33.326675Z","iopub.execute_input":"2021-07-15T07:57:33.32763Z","iopub.status.idle":"2021-07-15T07:59:07.620379Z","shell.execute_reply.started":"2021-07-15T07:57:33.327585Z","shell.execute_reply":"2021-07-15T07:59:07.617484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_vect = [np.array(i) for i in X_train['excerpt_tokens']]\nX_val_vect = [np.array(i) for i in X_val['excerpt_tokens']]\ntest_vect = [np.array(i) for i in test['excerpt_tokens']]\n\n#clean_vect = np.array(clean_work_tokens)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:59:07.622669Z","iopub.execute_input":"2021-07-15T07:59:07.623099Z","iopub.status.idle":"2021-07-15T07:59:07.804114Z","shell.execute_reply.started":"2021-07-15T07:59:07.623052Z","shell.execute_reply":"2021-07-15T07:59:07.803019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_array(data):\n    data_out = []\n    for i in range(len(data)):\n        an_array = data[i]\n        shape = np.shape(an_array)\n        padded_array = np.zeros((200,300))\n        padded_array[:shape[0],:shape[1]] = an_array\n        data_out.append(padded_array)\n    return data_out","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:59:07.805867Z","iopub.execute_input":"2021-07-15T07:59:07.806332Z","iopub.status.idle":"2021-07-15T07:59:07.814228Z","shell.execute_reply.started":"2021-07-15T07:59:07.806268Z","shell.execute_reply":"2021-07-15T07:59:07.812715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cool_array_X = pad_array(X_train_vect)\ncool_array_V = pad_array(X_val_vect)\ncool_array_test = pad_array(test_vect)\nt = np.array(cool_array_X).reshape(len(X_train), 200, 300)\nv = np.array(cool_array_V).reshape(len(y_val), 200, 300)\nvt = np.array(cool_array_test).reshape(len(test), 200, 300)\n\n#cw_t = pad_array(clean_vect)\n#cw = np.array(cw_t).reshape(len())\n\ny = np.array(y_train)\ny = y.reshape(1, -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:59:07.81579Z","iopub.execute_input":"2021-07-15T07:59:07.816324Z","iopub.status.idle":"2021-07-15T07:59:09.041196Z","shell.execute_reply.started":"2021-07-15T07:59:07.816247Z","shell.execute_reply":"2021-07-15T07:59:09.040066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = Sequential()\n\nmodel1.add(Bidirectional(LSTM(512, input_shape=(None, 300), return_sequences=True),\n                            backward_layer=LSTM(512, go_backwards=True, return_sequences=True)))\n\nmodel1.add(Dense(128, activation='relu'))\n\nmodel1.add(Dense(1))\n\nmodel1.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:59:09.042822Z","iopub.execute_input":"2021-07-15T07:59:09.043256Z","iopub.status.idle":"2021-07-15T07:59:11.689588Z","shell.execute_reply.started":"2021-07-15T07:59:09.043198Z","shell.execute_reply":"2021-07-15T07:59:11.688401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.fit(t, y_train,\n           validation_data=(v, y_val),\n           batch_size = 20,\n           epochs=100,\n           verbose=1,\n           \n          )","metadata":{"execution":{"iopub.status.busy":"2021-07-15T07:59:11.692382Z","iopub.execute_input":"2021-07-15T07:59:11.693192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.save('model1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = model1.predict(vt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = [np.mean(i) for i in l]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['target'] = f\n\nsubmission.to_csv(\"./submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}