{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport transformers\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport torch\nfrom sklearn import model_selection\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import RobertaConfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"MAX_LEN=256\nTEST_BATCH_SIZE=4\nTOKENIZER=transformers.RobertaTokenizerFast.from_pretrained(\"../input/roberta-new\")\nRoberta_PATH = \"../input/roberta-full-training/model.bin\"\nTRAINING_FILE=\"../input/commonlitreadabilityprize/train.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create the Dataset","metadata":{}},{"cell_type":"code","source":"class RobertaDataset:\n    def __init__(self,excerpt,target):\n        self.excerpt=excerpt\n        self.target=target\n\n        \n    def __len__(self):\n        return len(self.excerpt)\n    \n    def __getitem__(self,item):\n        excerpt=str(self.excerpt[item])\n        excerpt=\" \".join(excerpt.split())\n        \n        inputs =TOKENIZER(excerpt,add_special_tokens=True,max_length=MAX_LEN,padding=True,truncation=True)\n        \n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_len=MAX_LEN-len(ids)\n        ids=ids+([0]*padding_len)\n        mask=mask+([0]*padding_len)\n \n        return {\"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"targets\": torch.tensor(self.target[item], dtype=torch.float)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class RobertaModel(nn.Module):\n    \n    def __init__(self, conf):\n        super(RobertaModel,self).__init__()\n        self.roberta=transformers.RobertaModel.from_pretrained(Roberta_PATH,config=conf)\n        self.dropout=nn.Dropout(0.3)\n        self.linear=nn.Linear(768,1)\n        \n    def freeze(self):\n        for child in self.roberta.children():\n            for param in child.parameters():\n                param.requires_grad = False\n\n    def unfreeze(self):\n        for child in self.roberta.children():\n            for param in child.parameters():\n                param.requires_grad = True\n        \n    def forward(self,ids,mask):\n        \n        output1=self.roberta(ids,attention_mask=mask)\n        output1 = output1.hidden_states\n        output1 = output1[-1]\n        xlnet_output=self.dropout(output1)\n        \n        out = torch.mean(xlnet_output, 1, False)\n        final_output=self.linear(out)\n        final_outputs = final_output.squeeze(-1).squeeze(-1)\n        \n        return final_outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def inference(data_loader, model, device):\n    model.eval()\n    final_loss=0\n    i=0\n    \n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            predictions = model(ids=ids, mask=mask)\n            predictions = predictions.cpu().detach().numpy()\n            \n            if i==0:\n                preds_test = predictions\n            else:\n                preds_test = np.concatenate((preds_test,predictions), axis=None)\n                \n            i+=1\n\n            \n    return preds_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_config = RobertaConfig.from_pretrained(\"../input/roberta-new\")\n\ndevice = torch.device(\"cuda\")\nmodel_config.output_hidden_states = True\nmodel = RobertaModel(model_config)\n\nmodel.load_state_dict(torch.load(Roberta_PATH))\n\nmodel.to(device)\n\ntest=pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntest[\"target\"]=0\ntest_dataset = RobertaDataset(excerpt=test.excerpt.values, target=test.target.values)\n\ntest_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, num_workers=1)\n\npreds_test=inference(test_data_loader, model, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.target=preds_test\ntest=test[[\"id\",\"target\"]]\ntest.to_csv(\"submission.csv\",index=False)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}