{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-21T16:12:51.433037Z","iopub.execute_input":"2021-07-21T16:12:51.433431Z","iopub.status.idle":"2021-07-21T16:12:51.51735Z","shell.execute_reply.started":"2021-07-21T16:12:51.43335Z","shell.execute_reply":"2021-07-21T16:12:51.516566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install accelerate\nimport os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom accelerate import Accelerator\nfrom transformers import (AutoModel,AutoConfig,AdamW,\n                          AutoTokenizer,get_cosine_schedule_with_warmup)\n\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\ntrain_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\ntrain_data['excerpt'] = train_data['excerpt'].apply(lambda x: x.replace('\\n',''))\n\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\nbins = train_data.bins.to_numpy()\ntarget = train_data.target.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T16:12:51.520117Z","iopub.execute_input":"2021-07-21T16:12:51.520381Z","iopub.status.idle":"2021-07-21T16:13:07.017379Z","shell.execute_reply.started":"2021-07-21T16:12:51.520355Z","shell.execute_reply":"2021-07-21T16:13:07.016142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'lr': 2e-5,\n    'wd':0.01,\n    'batch_size':8,\n    'valid_step':10,\n    'max_len':256,\n    'epochs':4,\n    'nfolds':5,\n    'seed':1000,\n    'model_path':'../input/notebookb33113bcde/epoch2roberta_large',\n}\n\nfor i in range(config['nfolds']):\n    os.makedirs(f'model{i}',exist_ok=True)\n\ndef seed_everything(seed=1000):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\ntrain_data['Fold'] = -1\nkfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n    train_data.loc[valid_idx,'Fold'] = k\nclass CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len=248):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        target = torch.tensor(self.targets[idx],dtype=torch.float) \n        return encode, target\n    \n    def __len__(self):\n        return len(self.excerpt)\nclass Model(nn.Module): \n    def __init__(self, model_name):\n        super().__init__() \n\n\n        config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        \n        self.drop_out1 = nn.Dropout(0)\n        self.drop_out2 = nn.Dropout(0.1)\n\n        self.layer_norm1 = nn.LayerNorm(1024)\n        self.l1 = nn.Linear(1024, 512)\n        self.l2 = nn.Linear(512, 1)\n\n        self._init_weights(self.layer_norm1)\n        self._init_weights(self.l1)\n        self._init_weights(self.l2)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    def forward(self, input_ids, attention_mask):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]     \n        out = torch.mean(last_hidden_state, 1)\n        out = self.layer_norm1(out)\n        out = self.drop_out1(out)\n        out = self.l1(out)\n       \n        out = self.drop_out2(out)\n        out = self.l2(out)\n#         print(\"out:\",out.shape)\n        \n        preds = out.squeeze(-1)\n#         raise\n\n        return preds    \ndef run(fold):\n    \n    def loss_fn(outputs,targets):\n        return torch.sqrt(nn.MSELoss()(outputs.view(-1),targets.view(-1)))\n    \n    def evaluate(model,valid_loader):\n        model.eval()\n        valid_loss = 0\n        all_targets, all_outputs = [], []\n        with torch.no_grad():\n            for i, (inputs,targets) in enumerate(valid_loader):\n                inputs = {key:val.reshape(val.shape[0],-1) for key,val in inputs.items()}\n                outputs = model(**inputs)\n                valid_loss += loss_fn(outputs,targets).item()\n                all_outputs.extend(outputs.cpu().detach().numpy().tolist())\n                all_targets.extend(targets.cpu().detach().numpy().tolist())\n\n        valid_loss /= len(valid_loader)\n        valid_rmse = rmse_score(all_outputs,all_targets)\n        return valid_loss,valid_rmse\n        \n    def train_and_evaluate_loop(train_loader,valid_loader,model,loss_fn,optimizer,\n                                epoch,fold,best_score,valid_step,lr_scheduler=None):\n        train_loss = 0\n        for i, (inputs,targets) in enumerate(train_loader):\n            optimizer.zero_grad()\n            model.train()\n            inputs = {key:val.reshape(val.shape[0],-1) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            loss = loss_fn(outputs,targets)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n                        \n            if (i% valid_step ==0) or ((i+1) == len(train_loader)):\n                valid_loss,valid_rmse = evaluate(model,valid_loader) \n                        \n                if valid_loss <= best_score:\n                    print(f\"Epoch:{epoch}|Batch: {i}|Step:{valid_step}|Train Loss:{train_loss/(i+1)}|Valid Loss:{valid_loss}|Rmse Score: {valid_rmse}\")\n                    print(f\"{g_}Validation loss Decreased from {best_score} to {valid_loss}{sr_}\")\n\n                    best_score = valid_loss\n                    torch.save(model.state_dict(),f'./model{fold}/model{fold}.bin')\n                    tokenizer.save_pretrained(f'./model{fold}')\n                    \n        return best_score\n        \n    accelerator = Accelerator()\n    print(f\"{accelerator.device} is used\")\n    \n    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n    \n    model = Model(config['model_path'])\n    tokenizer = AutoTokenizer.from_pretrained('../input/huggingface-roberta/roberta-large')\n    \n    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n    train_dl = DataLoader(train_ds,\n                        batch_size = 8,\n                        num_workers = 4,\n                        shuffle=True,\n                        pin_memory=True,\n                        drop_last=True)\n\n    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n    valid_dl = DataLoader(valid_ds,\n                        batch_size = 10,\n                        num_workers = 4,\n                        shuffle=False,\n                        pin_memory=True,\n                        drop_last=False)\n        \n    def create_optimizer(model):\n        parameters = []\n        lr = 3e-5\n        for layer in range(23,-1,-1):\n            \n            layer_params = {\n            'params': [p for n,p in model.named_parameters() if f'encoder.layer.{layer}.' in n],\n                \"weight_decay\": 0.01,\n            'lr': lr\n                \n        }\n            parameters.append(layer_params)\n            lr *= 0.975\n        classifier_params = {\n        'params': [p for n,p in model.named_parameters() if 'layer_norm' in n or 'linear' in n \n                   or 'pooling' in n],\n            \"weight_decay\": 0.01,\n        'lr': 2e-5\n    }\n        parameters.append(classifier_params)\n        return AdamW(parameters)\n    optimizer = create_optimizer(model)\n    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 4* len(train_dl))\n\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    print(f\"Fold: {fold}\")\n    best_score = 9999\n    start_time = time.time()\n    for epoch in range(config[\"epochs\"]):\n        print(f\"Epoch Started:{epoch}\")\n        best_score = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,optimizer,epoch,fold,\n                                             best_score,config['valid_step'],lr_scheduler)\n        \n        end_time = time.time()\n        print(f\"{m_}Time taken by epoch {epoch} is {end_time-start_time:.2f}s{sr_}\")\n        start_time = end_time\n        \n    return best_score    \nbest_score_per_fold = [run(f) for f in range(config['nfolds'])]","metadata":{"execution":{"iopub.status.busy":"2021-07-21T16:13:07.022568Z","iopub.execute_input":"2021-07-21T16:13:07.024728Z"},"trusted":true},"execution_count":null,"outputs":[]}]}