{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, random, sys, time, re\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchtext\nfrom torchtext import vocab, data\n\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"597eea9d6295671bd36a809b579d12777738a392","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"../input/commonlitreadabilityprize/\"\nEMB_PATH = \"../input/embeddings-glove-crawl-torch-cached\"\nEMB_FILENAME = 'crawl-300d-2M.vec'#'glove.840B.300d.txt'\nN_FOLDS = 5\nEPOCHES = 25\nBATCH_SIZE = 256\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"_uuid":"b7c04a817b5ba68498dc9b30638605da891ba6c4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nrandom.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"_uuid":"6ddd98901408dbe7c7fb9efdb0ec17cecd511864","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\ndef tokenizer(text):\n    return tknzr.tokenize(text)","metadata":{"_uuid":"1fd588f56c18a0993c7db729b7524f81dc016126","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'), index_col='id')\n# test_csv = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'), index_col='id')\n\nsubm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'), index_col='id')\n\ny = (train_csv.target.values > 0).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the columns that we want to process and how to process\ntxt_field = data.Field(sequential=True, tokenize=tokenizer, include_lengths=False,  use_vocab=True)\nnum_field = data.Field(sequential=False, dtype=torch.float,  use_vocab=False)\nraw_field = data.RawField()\n\ntrain_fields = [\n    ('id', raw_field), \n    ('url_legal', raw_field),\n    ('license', raw_field),\n    ('excerpt', txt_field), \n    ('target', num_field),\n    ('standard_error', num_field),\n]\n\ntest_fields = [\n    ('id', raw_field), \n    ('url_legal', raw_field),\n    ('license', raw_field),\n    ('excerpt', txt_field), \n]","metadata":{"_uuid":"9640b7fef754155e0162b2cd1e70cadf94c6ec6c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading csv file\ntrain_ds = data.TabularDataset(path=os.path.join(DATA_PATH, 'train.csv'), \n                           format='csv',\n                           fields=train_fields, \n                           skip_header=True)\n\ntest_ds = data.TabularDataset(path=os.path.join(DATA_PATH, 'test.csv'), \n                           format='csv',\n                           fields=test_fields, \n                           skip_header=True)","metadata":{"_uuid":"92ea577a18faedfdccf5198d626c5c27861133ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# specify the path to the localy saved vectors\nvec = vocab.Vectors(os.path.join(EMB_PATH, EMB_FILENAME), cache=EMB_PATH)\n# build the vocabulary using train and validation dataset and assign the vectors\ntxt_field.build_vocab(train_ds, test_ds, max_size=300000, vectors=vec)\n\nembs_vocab = train_ds.fields['excerpt'].vocab.vectors\nprint('Embedding vocab size: ', embs_vocab.size()[0])\nvocab_size = embs_vocab.size()[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Wrapper for loaders, which structured fields\nclass BatchWrapper:\n      def __init__(self, dataloader, mode='train'):\n            self.dataloader, self.mode = dataloader, mode\n     \n      def __iter__(self):\n            if self.mode =='test':\n                for batch in self.dataloader:\n                    yield (batch.id, batch.excerpt)\n            else:\n                for batch in self.dataloader:\n                    yield (batch.excerpt,  batch.target)\n  \n      def __len__(self):\n            return len(self.dl)\n\ndef wrapper(ds, mode='train', **kwargs):\n    dataloader = data.BucketIterator(ds, device=DEVICE, **kwargs)\n    return BatchWrapper(dataloader, mode)\n\ncv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\ndef splits_cv(dataset, cv=cv, y=y, batch_size=BATCH_SIZE):\n    \"\"\"\n        Split dataset to train and validation used cross-validator and wrap loader\n    \"\"\"\n    for indices in cv.split(range(len(dataset)), y):\n        (train_data, valid_data) = tuple([dataset.examples[i] for i in index] for index in indices)\n        yield tuple(wrapper(\n            data.Dataset(d, dataset.fields), batch_size=batch_size) for d in (train_data, valid_data) if d)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = wrapper(test_ds, batch_size=BATCH_SIZE, shuffle=False, repeat=False, mode='test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionAwareAttention(nn.Module):\n    \n    def __init__(self, input_size, attn_size):\n        super().__init__()\n        self.input_size  = input_size\n        self.wx = nn.Conv1d(input_size, attn_size, 1, bias=True)  # from input to attention matrix\n        self.wh = nn.Conv1d(input_size, attn_size, 1, bias=False) # from hidden to attention matrix\n        self.wt = nn.Conv1d(attn_size, 1, 1, bias=True)           # from attention matrix to score\n        \n    def forward(self, x, h):\n        x = x.permute(1,2,0) # features last\n        wx = self.wx(x)\n        wh = self.wh(h.permute(1,0,2).contiguous().view(-1,self.input_size,1))\n        score = self.wt(torch.tanh(wx + wh))\n        score = F.softmax(score, dim=2)\n        out = torch.bmm(score, x.permute(0,2,1)).squeeze()\n        \n        return out\n    \nclass RecNN(nn.Module):\n    def __init__(self, embs_vocab, hidden_size, layers=1, atten_features = 24, \n                 dropout=0., bidirectional=False):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n        self.num_layers = layers\n        self.emb_dim = embs_vocab.size(1)\n        self.emb = nn.Embedding(embs_vocab.size(0), self.emb_dim)\n        self.emb.weight.data.copy_(embs_vocab) # load pretrained vectors\n        self.emb.weight.requires_grad = False # make embedding non trainable\n        \n        self.lstm = nn.LSTM(self.emb_dim, self.hidden_size,\n                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n        \n        self.gru = nn.GRU(self.emb_dim, self.hidden_size,\n                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n        self.pregru = nn.Conv1d(self.emb_dim, self.emb_dim, 1, bias=True)\n        self.atten = PositionAwareAttention(hidden_size*(bidirectional+1), atten_features)\n        \n        self.out = nn.Linear(2* hidden_size*(bidirectional+1), 32)\n        self.last = nn.Linear(32, 1)\n                \n    def forward(self, x):\n        \n        embs = self.emb(x)\n        \n        lstm, (h1, c) = self.lstm(embs)\n        gru = F.relu(self.pregru(embs.permute(1,2,0)), inplace=True).permute(2,0,1)\n        \n        gru, h2 = self.gru(gru, h1)\n        lstm = lstm + gru\n        \n        x_max, _ = lstm.max(dim=0, keepdim=False) \n        x_atten = self.atten(lstm, h1+h2)\n        out = self.out(torch.cat([x_max, x_atten],dim = 1))\n        out = self.last(F.relu(out)).squeeze()\n        return out","metadata":{"_uuid":"e64a4a40f2e23749d937004f00db7210cdd3d946","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Table for results\nheader = r'''\n             Train        Validation\nEpoch |  MSE  |  RMSE |  MSE  |  RMSE | Time, m\n'''\n#          Epoch         metrics            time\nraw_line = '{:6d}' + '\\u2502{:7.3f}'*4 + '\\u2502{:6.2f}'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef validation_fn(model, loader, loss_fn):\n    y_pred, y_true, tloss = [], [], []\n    model.eval();\n    for texts, target in loader:\n        outputs = model(texts)\n        loss = loss_fn(outputs, target)\n        tloss.append(loss.item())\n    tloss = np.array(tloss).mean()\n    return tloss\n\ndef oof_preds(train_ds, test_loader, embs_vocab,\n              hidden_size=128, bidirectional=True, epochs = EPOCHES):\n\n    for loader, vloader in splits_cv(train_ds, cv):\n        \n        model = model = RecNN(embs_vocab, hidden_size,\n                              dropout=0.1, bidirectional=bidirectional).to(DEVICE)\n        \n        optimizer = optim.AdamW(model.parameters(), 1e-3, betas=(0.75, 0.999), weight_decay=1e-1)\n        loss_fn = torch.nn.MSELoss()\n        \n        print(header)\n        for epoch in range(1, epochs+1):      \n            start_time = time.time()\n            tloss = []          \n            model.train()\n            \n            for texts, target in loader:\n                optimizer.zero_grad()\n                outputs = model(texts)\n                loss = loss_fn(outputs, target)\n                tloss.append(loss.item())\n                loss.backward()\n                optimizer.step()\n\n            tloss = np.array(tloss).mean()\n            vloss = validation_fn(model, vloader, loss_fn)\n            tmetric = tloss**.5\n            vmetric = vloss**.5\n            if epoch % 1 == 0:\n                print(raw_line.format(epoch,tloss,tmetric,vloss,vmetric,(time.time()-start_time)/60**1))\n\n       \n        # Get prediction for test set\n        ids, preds = [], [] \n        with torch.no_grad():\n            for batch_ids, texts in test_loader:\n                outputs = model(texts)\n                ids += batch_ids\n                preds.append(outputs.detach().cpu().numpy())\n            \n        # Save prediction of test set\n        preds = np.concatenate(preds)\n        subm.loc[ids, 'target']  =  subm.loc[ids, 'target'].values + preds / N_FOLDS","metadata":{"_uuid":"e566ecdb5f44cf32af12127f073021566d6e66dd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_preds(train_ds, test_loader, embs_vocab, epochs = EPOCHES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm.to_csv('submission.csv')","metadata":{"_uuid":"7c6a41bd21bf783455fd11b63745ed454e0413f0","trusted":true},"execution_count":null,"outputs":[]}]}