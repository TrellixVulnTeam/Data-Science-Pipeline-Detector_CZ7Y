{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The inspiration to write this notebook is from [here](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240871). This notebook goes through the implementation of **NON Transformers** models, before going and trying with transformers models.\nThis notebook is currently in progress, ","metadata":{}},{"cell_type":"markdown","source":"This notebook gives submission for below models,\n\n1. TFIDF + Linear Regression\n2. TFIDF + Random Forest Regressor\n3. TFIDF + SVD + Random Forest regression\n4. TFIDF + SVD + Ridge regression\n5. Spacy vectors + Ridge Regression\n6. Universal Sentence Encoder + Ridge Regression\n7. Spacy vectors + Pytorch Regressor [To Do]\n8. Spacy vectors + Pytorch Ranker [To Do]\n9. Pytorch LSTM [To Do]","metadata":{}},{"cell_type":"markdown","source":"### Load Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression,Ridge \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import TruncatedSVD\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nSTOPWORDS = set(stopwords.words('english'))\nLEMMATIZER = WordNetLemmatizer()\nimport string\nimport re\n\n##parallel processing\nimport dask\n\nimport spacy\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-04T04:26:06.449367Z","iopub.execute_input":"2021-06-04T04:26:06.449953Z","iopub.status.idle":"2021-06-04T04:26:09.331069Z","shell.execute_reply.started":"2021-06-04T04:26:06.449866Z","shell.execute_reply":"2021-06-04T04:26:09.330071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Data","metadata":{}},{"cell_type":"code","source":"datapath = \"/kaggle/input/commonlitreadabilityprize/\"\nsub_df = pd.read_csv(f\"{datapath}/sample_submission.csv\")\ntrain_df = pd.read_csv(f\"{datapath}/train.csv\")\ntest_df = pd.read_csv(f\"{datapath}/test.csv\")\ntrain_df.shape, test_df.shape, sub_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:26:09.33279Z","iopub.execute_input":"2021-06-04T04:26:09.333192Z","iopub.status.idle":"2021-06-04T04:26:09.445709Z","shell.execute_reply.started":"2021-06-04T04:26:09.333149Z","shell.execute_reply":"2021-06-04T04:26:09.4447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Please note that I am getting better public LB score without using any of preprocessing steps compared to using processing steps, for default parameters**","metadata":{}},{"cell_type":"markdown","source":"### Text preprocessing (Cleaning)","metadata":{}},{"cell_type":"markdown","source":"1. Remove stop words\n2. stemming and lemmatization\n3. Remove punctuation/change punctuation\n4. Remove URLs(links)\n5. Remove Numbers ","metadata":{}},{"cell_type":"code","source":"# %%time\n# def preprocess_stemming(text):\n#     tokens = word_tokenize(text)\n#     stems = []\n#     for w in tokens:\n#         stems.append(PorterStemmer().stem(w))\n#     return stems\n\n# def process_lemmatization(text):\n#     tokens = word_tokenize(text)\n#     lemmas = []\n#     for w in tokens:\n#         word1 = LEMMATIZER.lemmatize(w, pos = \"n\")\n#         word2 = LEMMATIZER.lemmatize(word1, pos = \"v\")\n#         word3 = LEMMATIZER.lemmatize(word2, pos = (\"a\"))\n#         lemmas.append(word3)\n#     return \" \".join(lemmas)\n\n# ### to remove stopwords\n# train_df['excerpt'] = train_df['excerpt'].apply(lambda text: \" \".join([val for val in word_tokenize(text) if val not in STOPWORDS]))\n\n# # To perform lemmatization\n# ##using dask to speedup\n# #tasks = train_df['excerpt'].apply(lambda text: (dask.delayed(process_lemmatization)(text)))\n# #train_df['excerpt'] = dask.compute(*tasks)\n# train_df['excerpt'] = train_df['excerpt'].apply(lambda text: process_lemmatization(text))\n\n# #string punctuation removal\n# train_df['excerpt'] = train_df['excerpt'].apply(lambda text: text.translate(str.maketrans('', '', string.punctuation)))\n# ## Removes links\n# train_df['excerpt'] = train_df['excerpt'].apply(lambda text: re.sub('https?://\\S+|www\\.\\S+', '', text))\n# ## Removes numbers\n# train_df['excerpt'] = train_df['excerpt'].apply(lambda text: re.sub(r'[^\\D\\s]','',text))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:12:24.673771Z","iopub.execute_input":"2021-05-30T17:12:24.674122Z","iopub.status.idle":"2021-05-30T17:12:38.8275Z","shell.execute_reply.started":"2021-05-30T17:12:24.674096Z","shell.execute_reply":"2021-05-30T17:12:38.826546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:26:14.511471Z","iopub.execute_input":"2021-06-04T04:26:14.511869Z","iopub.status.idle":"2021-06-04T04:26:14.538594Z","shell.execute_reply.started":"2021-06-04T04:26:14.511836Z","shell.execute_reply":"2021-06-04T04:26:14.537679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TFIDF","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(train_df['excerpt'])\nX_test = vectorizer.transform(test_df['excerpt'])\ny = train_df['target']","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:26:17.834067Z","iopub.execute_input":"2021-06-04T04:26:17.834415Z","iopub.status.idle":"2021-06-04T04:26:18.472855Z","shell.execute_reply.started":"2021-06-04T04:26:17.834384Z","shell.execute_reply":"2021-06-04T04:26:18.471867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"markdown","source":"#### 1. Linear Regression","metadata":{}},{"cell_type":"code","source":"LR_tfidf = LinearRegression().fit(X_train, y)\nLR_tfidf.score(X_train, y)\ny_train_lr_pred = LR_tfidf.predict(X_train)\ntest_lr_pred = LR_tfidf.predict(X_test)\nsub_df['target'] = test_lr_pred\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_train_lr_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_train_lr_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y,y_train_lr_pred)))\n\nprint(sub_df.head())\nsub_df.to_csv('LR_submission.csv', index=False)\n\n## 1.01 - with preprocessing \n## 0.72 - without preprocessing","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:26:24.676617Z","iopub.execute_input":"2021-05-30T17:26:24.676986Z","iopub.status.idle":"2021-05-30T17:26:24.891178Z","shell.execute_reply.started":"2021-05-30T17:26:24.676939Z","shell.execute_reply":"2021-05-30T17:26:24.890168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. RandomForestRegressor","metadata":{}},{"cell_type":"code","source":"%%time\nrf_tfidf = RandomForestRegressor().fit(X_train, y)\ny_train_rf_pred = rf_tfidf.predict(X_train)\ntest_rf_pred = rf_tfidf.predict(X_test)\nsub_df['target'] = test_rf_pred\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_train_rf_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_train_rf_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y,y_train_rf_pred)))\n\nprint(sub_df.head())\nsub_df.to_csv('submission.csv', index=False) \n## 0.92 - with preprocessing \n## 0.81 - without preprocessing","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:51:32.968927Z","iopub.execute_input":"2021-05-30T17:51:32.969303Z","iopub.status.idle":"2021-05-30T17:54:14.16036Z","shell.execute_reply.started":"2021-05-30T17:51:32.969268Z","shell.execute_reply":"2021-05-30T17:54:14.159576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. TFIDF+SVD+ RandomForestRegressor","metadata":{}},{"cell_type":"markdown","source":"##### SVD","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:12:41.825822Z","iopub.execute_input":"2021-06-04T04:12:41.826338Z","iopub.status.idle":"2021-06-04T04:12:41.834586Z","shell.execute_reply.started":"2021-06-04T04:12:41.826258Z","shell.execute_reply":"2021-06-04T04:12:41.833319Z"}}},{"cell_type":"code","source":"%%time\nsvdT = TruncatedSVD(n_components=400)\nsvd_X_train = svdT.fit_transform(X_train)\nsvd_X_test = svdT.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:26:26.757653Z","iopub.execute_input":"2021-06-04T04:26:26.758027Z","iopub.status.idle":"2021-06-04T04:26:34.512827Z","shell.execute_reply.started":"2021-06-04T04:26:26.757993Z","shell.execute_reply":"2021-06-04T04:26:34.51187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsvd_rf_tfidf = RandomForestRegressor().fit(svd_X_train, y)\ny_train_svdrf_pred = svd_rf_tfidf.predict(svd_X_train)\ntest_svdrf_pred = svd_rf_tfidf.predict(svd_X_test)\nsub_df['target'] = test_svdrf_pred\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_train_svdrf_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_train_svdrf_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y,y_train_svdrf_pred)))\n\nprint(sub_df.head())\nsub_df.to_csv('submission.csv', index=False) \n## 0.773 - without preprocessing","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:26:34.51453Z","iopub.execute_input":"2021-06-04T04:26:34.514922Z","iopub.status.idle":"2021-06-04T04:27:37.75079Z","shell.execute_reply.started":"2021-06-04T04:26:34.514882Z","shell.execute_reply":"2021-06-04T04:27:37.749728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. TFIDF+SVD+Ridge Regressor","metadata":{}},{"cell_type":"code","source":"%%time\nregressor = Ridge(fit_intercept=True, normalize=False)\nscores = cross_val_score(regressor, svd_X_train, y, cv=5, \n                         scoring='neg_root_mean_squared_error')\nprint(f'Average Root mean squared error: {np.abs(np.mean(scores))}')\n\nregressor = regressor.fit(svd_X_train, y)\ntest_df['target'] = regressor.predict(svd_X_test)\ntest_df[['id','target']].to_csv('submission.csv', index=False)\n## 0.722 - without preprocessing","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:33:39.744896Z","iopub.execute_input":"2021-06-04T04:33:39.745256Z","iopub.status.idle":"2021-06-04T04:33:40.124041Z","shell.execute_reply.started":"2021-06-04T04:33:39.745224Z","shell.execute_reply":"2021-06-04T04:33:40.121693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.Spacy vectors + Ridge Regression","metadata":{}},{"cell_type":"markdown","source":"All Credits to Sumit Kumar @anaverageengineer https://www.kaggle.com/anaverageengineer/comlrp-baseline-for-complete-beginners","metadata":{}},{"cell_type":"code","source":"%%time\nRANDOM_STATE = 147\nnlp = spacy.load('en_core_web_lg')\n\nwith nlp.disable_pipes():\n    X_train = np.vstack([nlp(text).vector for text in tqdm(train_df['excerpt'])])\n    y = train_df['target']\n    print(f'Shape of Train vectors: {X_train.shape}')\n\n    X_test = np.vstack([nlp(text).vector for text in tqdm(test_df['excerpt'])])\n    print(f'Shape of Test vectors: {X_test.shape}')\n    \nregressor = Ridge(fit_intercept=True, normalize=False)\nscores = cross_val_score(regressor, X_train, y, cv=5, \n                         scoring='neg_root_mean_squared_error')\nprint(f'Average Root mean squared error: {np.abs(np.mean(scores))}')\n\nregressor = regressor.fit(X_train, y)\ntest_df['target'] = regressor.predict(X_test)\ntest_df[['id','target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:40:28.544132Z","iopub.execute_input":"2021-05-30T18:40:28.544444Z","iopub.status.idle":"2021-05-30T18:40:34.526828Z","shell.execute_reply.started":"2021-05-30T18:40:28.544419Z","shell.execute_reply":"2021-05-30T18:40:34.52511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.Universal Sentence Encoder + Ridge Regression","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:09:30.409945Z","iopub.execute_input":"2021-06-04T05:09:30.410324Z","iopub.status.idle":"2021-06-04T05:09:30.414835Z","shell.execute_reply.started":"2021-06-04T05:09:30.410292Z","shell.execute_reply":"2021-06-04T05:09:30.413711Z"}}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\n\n#embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\nembed = hub.load(\"../input/universalsentenceencodermodels/universal-sentence-encoder-models/use-large\")\n# embeddings = embed([\n#     \"The quick brown fox jumps over the lazy dog.\",\n#     \"I am a sentence for which I would like to get its embedding\"])\n# print(embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:12:16.784613Z","iopub.execute_input":"2021-06-04T05:12:16.784973Z","iopub.status.idle":"2021-06-04T05:12:26.514745Z","shell.execute_reply.started":"2021-06-04T05:12:16.784942Z","shell.execute_reply":"2021-06-04T05:12:26.513639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nRANDOM_STATE = 147\n\nX_train = np.vstack([embed([text]) for text in tqdm(train_df['excerpt'])])\ny = train_df['target']\nprint(f'Shape of Train vectors: {X_train.shape}')\n\nX_test = np.vstack([embed([text]) for text in tqdm(test_df['excerpt'])])\nprint(f'Shape of Test vectors: {X_test.shape}')\n    \nregressor = Ridge(fit_intercept=True, normalize=False)\nscores = cross_val_score(regressor, X_train, y, cv=5, \n                         scoring='neg_root_mean_squared_error')\nprint(f'Average Root mean squared error: {np.abs(np.mean(scores))}')\n\nregressor = regressor.fit(X_train, y)\ntest_df['target'] = regressor.predict(X_test)\ntest_df[['id','target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:28:44.497977Z","iopub.execute_input":"2021-06-04T05:28:44.498376Z","iopub.status.idle":"2021-06-04T05:34:07.805638Z","shell.execute_reply.started":"2021-06-04T05:28:44.498339Z","shell.execute_reply":"2021-06-04T05:34:07.804003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:34:38.821216Z","iopub.execute_input":"2021-06-04T05:34:38.821686Z","iopub.status.idle":"2021-06-04T05:34:38.838058Z","shell.execute_reply.started":"2021-06-04T05:34:38.821635Z","shell.execute_reply":"2021-06-04T05:34:38.837082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}