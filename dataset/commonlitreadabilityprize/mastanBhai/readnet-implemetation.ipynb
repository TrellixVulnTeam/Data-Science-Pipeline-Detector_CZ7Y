{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-18T13:26:48.979015Z","iopub.execute_input":"2021-07-18T13:26:48.979387Z","iopub.status.idle":"2021-07-18T13:26:49.014413Z","shell.execute_reply.started":"2021-07-18T13:26:48.979309Z","shell.execute_reply":"2021-07-18T13:26:49.013385Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ReadNet: A Hierarchical Transformer Framework for Web Article Readability Analysis","metadata":{}},{"cell_type":"markdown","source":"The   [ReadNet](https://arxiv.org/abs/2103.04083) was published in 2021 march, it  proposed a new and comprehensive framework which uses a hierarchical self-attention model to analyze document readability.It has been evaluated over three widely-used benchmark datasets against several strong baseline approaches. Experimental results showed that the  proposed method achieves the state-of-the-art performance on estimating the readability for various web articles and literature.","metadata":{}},{"cell_type":"markdown","source":"In this notebook we'll try to use ReadNet for calculating the readability value of given samples. Since readnet has achived a far better acuuracy than BERT on the Text Classification on WeeBit (Readability Assessment) , we'll see whether the same improved performance is reflected in CommonLit Readability Competition.\n","metadata":{}},{"cell_type":"markdown","source":"### Importing Required libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch import Tensor, nn, tensor\nimport math\nimport pandas as pd\nimport csv\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom pathlib import Path\nfrom fastai.vision.all import *\nfrom fastai.text.all import *","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:26:49.015812Z","iopub.execute_input":"2021-07-18T13:26:49.01614Z","iopub.status.idle":"2021-07-18T13:27:13.727106Z","shell.execute_reply.started":"2021-07-18T13:26:49.016106Z","shell.execute_reply":"2021-07-18T13:27:13.726259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Readnet ](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-45439-5_3/MediaObjects/492459_1_En_3_Fig1_HTML.png)","metadata":{}},{"cell_type":"markdown","source":"## Defining the layers \nThe layers from the representation of readnet is implemented below.","metadata":{}},{"cell_type":"markdown","source":"### Multi-Head Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, masked):\n        super().__init__()\n        assert d_model % num_heads == 0, \"num_heads must evenly chunk d_model\"\n        self.num_heads = num_heads\n        self.wq = nn.Linear(d_model, d_model, bias=False)  # QQ what if bias=True?\n        self.wk = nn.Linear(d_model, d_model, bias=False)\n        self.wv = nn.Linear(d_model, d_model, bias=False)\n        self.masked = masked\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, q, k, v):\n        qs = self.wq(q).chunk(self.num_heads, dim=2)\n        ks = self.wk(k).chunk(self.num_heads, dim=2)\n        vs = self.wv(v).chunk(self.num_heads, dim=2)\n        outs = []\n        # TODO Use einsum instead of for loop\n        for qi, ki, vi in zip(qs, ks, vs):\n            attns = qi.bmm(ki.transpose(1, 2)) / (ki.shape[2] ** 0.5)\n            if self.masked:\n                attns = attns.tril()  # Zero out upper triangle so it can't look ahead\n            attns = self.softmax(attns)\n            outs.append(attns.bmm(vi))\n        return torch.cat(outs, dim=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add & Norm","metadata":{}},{"cell_type":"code","source":"class AddNorm(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.ln = nn.LayerNorm(d_model)\n\n    def forward(self, x1, x2):\n        return self.ln(x1+x2)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feed Forward","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.l1 = nn.Linear(d_model, d_model)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(d_model, d_model)\n    def forward(self, x):\n        return self.l2(self.relu(self.l1(x)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Atention ","metadata":{}},{"cell_type":"code","source":"class AttentionAggregation(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.query = nn.Linear(d_model, 1, bias=False)\n\n    def forward(self, x):  # (b, s, m)\n        attns = self.query(x).softmax(dim=1)  # (b, s, 1)\n        enc = torch.bmm(attns.transpose(1, 2), x)  # (b, 1, m)\n        return enc.squeeze(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Readnet ","metadata":{}},{"cell_type":"code","source":"class ReadNetBlock(nn.Module):\n    def __init__(self, d_model, n_heads, n_blocks, n_feats, n_out):\n        super().__init__()\n        self.blocks = nn.Sequential(*[EncoderBlock(d_model=d_model, num_heads=n_heads) for _ in range(n_blocks)])\n        self.lin_tanh = LinTanh(d_model=d_model)\n        self.attn_agg = AttentionAggregation(d_model=d_model)\n        self.lin_feat_concat = LinFeatConcat(d_model=d_model, n_feats=n_feats, n_out=n_out)\n\n    def forward(self, x, feats):  # (b, s, m), (b, f)\n        x = pos_encode(x)\n        x = self.blocks(x)\n        x = self.lin_tanh(x)\n        x = self.attn_agg(x)\n        return self.lin_feat_concat(x, feats)\n    \n    \nclass ReadNet(nn.Module):\n    def __init__(self, embed, d_model, n_heads, n_blocks, n_feats_sent, n_feats_doc):\n        super().__init__()\n        self.embed = embed\n        self.sent_block = ReadNetBlock(\n            d_model=d_model, n_heads=n_heads, n_blocks=n_blocks, n_feats=n_feats_sent, n_out=d_model\n        )\n        self.doc_block = ReadNetBlock(\n            d_model=d_model, n_heads=n_heads, n_blocks=n_blocks, n_feats=n_feats_doc, n_out=d_model + n_feats_doc\n        )\n        self.head = nn.Sequential(\n            nn.Linear(d_model + n_feats_doc, 1),\n        )\n\n    def forward(self, x, feats_sent=None, feats_doc=None):  # (b, d, s) tokens, (b, d, n_f_s), (b, n_f_d)\n        if feats_sent is None: feats_sent = Tensor([])\n        if feats_doc is None: feats_doc = Tensor([])\n        if x.is_cuda:\n            feats_sent = feats_sent.cuda()\n            feats_doc = feats_doc.cuda()\n        #print(len(x))\n        #print(x.shape)\n        #print(x)\n        \n        x = self.embed(x)\n        b, d, s, m = x.shape\n        x = x.reshape(b * d, s, m)\n        sents_enc = self.sent_block(x, feats_sent.reshape(b * d, -1))  # (b*d, m)\n        docs = sents_enc.reshape(b, d, m)\n        docs_enc = self.doc_block(docs, feats_doc)\n        out = self.head(docs_enc)\n        return out.squeeze(1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Positional Encoding and Tokenizing Layers","metadata":{}},{"cell_type":"code","source":"\n\n\n# Make sure to have your glove embeddings stored here\nroot_dir = '../input/glove-embeddings'\n\n\n## MODEL CODE ##\n\n\n\n\n\n\n\n\n\n\ndef pos_encode(x):\n    pos, dim = torch.meshgrid(torch.arange(x.shape[1]), torch.arange(x.shape[2]))\n    dim = 2 * (dim // 2)\n    enc_base = pos/(10_000**(dim / x.shape[2]))\n    addition = torch.zeros_like(x)\n    for d in range(x.shape[2]):\n        enc_func = torch.sin if d % 2 == 0 else torch.cos\n        addition[:,:,d] = enc_func(enc_base[:,d])\n    if x.is_cuda:\n        addition = addition.cuda()\n    #print(x+addition)\n    return x + addition\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads, masked=False)\n        self.an1 = AddNorm(d_model)\n        self.ff = FeedForward(d_model)\n        self.an2 = AddNorm(d_model)\n\n    def forward(self, x):\n        x = self.an1(x, self.mha(q=x, k=x, v=x))\n        return self.an2(x, self.ff(x))\n\n\n\n\n\nclass LinTanh(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.lin = nn.Linear(d_model, d_model)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        return self.tanh(self.lin(x))\n\n\nclass LinFeatConcat(nn.Module):\n    def __init__(self, d_model, n_feats, n_out):\n        super().__init__()\n        self.lin = nn.Linear(d_model + n_feats, n_out, bias=False)  # TODO what if True?\n\n    def forward(self, x, feats):\n        return self.lin(torch.cat([x, feats], dim=1))\n\n\n\n\n\nclass GloveEmbedding(nn.Module):\n    def __init__(self, num):\n        super().__init__()\n        # Make embedding\n        self.embed = nn.Embedding(400_000 + 1, num)\n        emb_w = pd.read_csv(\n            root_dir +'/'+ f'glove.6B.{num}d.txt', header=None, sep=\" \", quoting=csv.QUOTE_NONE\n        ).values[:, 1:].astype('float64')\n        emb_w = Tensor(emb_w)\n        emb_w = torch.cat([emb_w, torch.zeros(1, num)], dim=0)\n        self.embed.weight = nn.Parameter(emb_w)\n\n    def forward(self, x):\n        return self.embed(x.to(torch.long))\n\n\n\n\n## DATA PREPARATION ##\n\nclass GloveTokenizer:\n    def __init__(self, num):\n        words = pd.read_csv(\n            root_dir + '/'+ f'glove.6B.{num}d.txt', header=None, sep=\" \", quoting=csv.QUOTE_NONE, usecols=[0]\n        ).values\n        words = [word[0] for word in words]\n        self.word2idx = {w: i for i, w in enumerate(words)}\n\n    def __call__(self, sent):\n        toks = [self.word2idx.get(w.lower()) for w in word_tokenize(sent)]\n        return [self.unk_token if t is None else t for t in toks]\n\n    @property\n    def unk_token(self):\n        return 400_000  # We appended this to the end of the embedding to return all zeros\n\n    @property\n    def pad_token(self):\n        return self.unk_token  # Seems that this is the best option for GLOVE\n\n\ndef prepare_txts(txts, tokenizer):\n    # Input: (bs,) str, Output: (bs, max_doc_len, max_sent_len)\n    # We choose to elongate all docs and sentences to the max rather than truncate some of them\n    # TODO: Do this better later:\n    # (1) Truncate smartly (if there is one very long outlier sentence or doc)\n    # (2) Group together docs of similar lengths (in terms of num_sents)\n    docs = [[tokenizer(sent) for sent in sent_tokenize(txt)] for txt in txts]\n    # pkl_save(root_dir/\"doc_lens\", pd.Series([len(doc) for doc in docs]))\n    max_doc_len = max([len(doc) for doc in docs])\n    docs = [doc + [[]] * (max_doc_len - len(doc)) for doc in docs]\n    # pkl_save(root_dir/\"sent_lens\", pd.Series([len(sent) for doc in docs for sent in doc]))\n    max_sent_len = max([len(sent) for doc in docs for sent in doc])\n    docs = [[s + [tokenizer.pad_token] * (max_sent_len - len(s)) for s in doc] for doc in docs]\n    return Tensor(docs)\n\n\ndef prepare_txts_cut(txts, tokenizer, max_doc_len=18, max_sent_len=49):\n    docs = [[tokenizer(sent)[:max_sent_len] for sent in sent_tokenize(txt)[:max_doc_len]] for txt in txts]\n    docs = [doc + [[]] * (max_doc_len - len(doc)) for doc in docs]\n    docs = [[s + [tokenizer.pad_token] * (max_sent_len - len(s)) for s in doc] for doc in docs]\n    return Tensor(docs)\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:27:13.728853Z","iopub.execute_input":"2021-07-18T13:27:13.72916Z","iopub.status.idle":"2021-07-18T13:27:13.777321Z","shell.execute_reply.started":"2021-07-18T13:27:13.729134Z","shell.execute_reply":"2021-07-18T13:27:13.776308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization and embedding\n\nThe golve tokenizer is used to tokenize and then glove embeddings of corresponding words are givenas input to the model , we'll be using glove 200 d vector since it gave the best results for us.","metadata":{}},{"cell_type":"code","source":"## TRAIN ## (using fastai)\n\ntokenizer = GloveTokenizer(200)\nembed = GloveEmbedding(200)\n\ndef get_splits(data):\n  num = len(data)\n  idx = list(range(num))\n  random.seed(42)\n  random.shuffle(idx)\n  split = int(num*0.75)\n  return idx[:split], idx[split:]\n\n\ndef get_dls(bs,path):\n  data = pd.read_csv(path)\n  txts = data.excerpt.tolist()\n  x = prepare_txts_cut(txts, tokenizer)\n  y = data.target.tolist()\n\n  ds = TfmdLists(\n      zip(x, y),\n      tfms=[],\n      splits=get_splits(data),\n  )\n  #print(ds)\n  dls = ds.dataloaders(batch_size=bs)\n  print(dls)\n  return dls\n","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:27:13.780976Z","iopub.execute_input":"2021-07-18T13:27:13.781295Z","iopub.status.idle":"2021-07-18T13:27:56.051377Z","shell.execute_reply.started":"2021-07-18T13:27:13.781248Z","shell.execute_reply":"2021-07-18T13:27:56.050186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\nThe model is loaded into cuda using the get model function and we define the parameters here.\nNote: d_model must be a multiple of n_heads.","metadata":{}},{"cell_type":"code","source":"def get_model():\n    readnet = ReadNet(\n        embed=embed,\n        d_model=200,\n        n_heads=4,\n        n_blocks=6,\n        n_feats_sent=0,\n        n_feats_doc=0,\n    )\n    readnet = readnet.cuda()\n\n    # Automatically freeze the embedding. We should not be learning this\n    for p in readnet.embed.parameters():\n        p.requires_grad = False\n\n    return readnet","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:27:56.052808Z","iopub.execute_input":"2021-07-18T13:27:56.05315Z","iopub.status.idle":"2021-07-18T13:27:56.05814Z","shell.execute_reply.started":"2021-07-18T13:27:56.053112Z","shell.execute_reply":"2021-07-18T13:27:56.057344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\nTrain the model with the given ","metadata":{}},{"cell_type":"code","source":"\nlearn = Learner(dls=get_dls(32,path=\"../input/commonlitreadabilityprize/train.csv\"), model=get_model(), loss_func=MSELossFlat())\nlearn.summary()\nlearn.lr_find()\nlearn.fit_one_cycle(20, 3e-5,cbs=SaveModelCallback())","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:27:56.059579Z","iopub.execute_input":"2021-07-18T13:27:56.060176Z","iopub.status.idle":"2021-07-18T13:35:07.293476Z","shell.execute_reply.started":"2021-07-18T13:27:56.060114Z","shell.execute_reply":"2021-07-18T13:35:07.289452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.294452Z","iopub.status.idle":"2021-07-18T13:35:07.294842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#del learn\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.295821Z","iopub.status.idle":"2021-07-18T13:35:07.296368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df= pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.297643Z","iopub.status.idle":"2021-07-18T13:35:07.298233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.299623Z","iopub.status.idle":"2021-07-18T13:35:07.300198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_txts_pred(txts, tokenizer, max_doc_len=18, max_sent_len=49):\n    docs = [[tokenizer(sent)[:max_sent_len] for sent in sent_tokenize(txt)[:max_doc_len]] for txt in txts]\n    docs = [doc + [[]] * (max_doc_len - len(doc)) for doc in docs]\n    docs = [[s + [tokenizer.pad_token] * (max_sent_len - len(s)) for s in doc] for doc in docs]\n    return Tensor(docs)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.301682Z","iopub.status.idle":"2021-07-18T13:35:07.302253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txts = test_df.excerpt.tolist()\nx = prepare_txts_pred(txts, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.303418Z","iopub.status.idle":"2021-07-18T13:35:07.304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.305081Z","iopub.status.idle":"2021-07-18T13:35:07.305702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del learn\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.306858Z","iopub.status.idle":"2021-07-18T13:35:07.30743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Model\n\nLoad the trained model for prediction on test dataset","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('./models/model.pth'))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.30858Z","iopub.status.idle":"2021-07-18T13:35:07.309198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda:0\"\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.310368Z","iopub.status.idle":"2021-07-18T13:35:07.310971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(x)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.312094Z","iopub.status.idle":"2021-07-18T13:35:07.312702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_batch_List=[]","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.313916Z","iopub.status.idle":"2021-07-18T13:35:07.314529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last = 0","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.315642Z","iopub.status.idle":"2021-07-18T13:35:07.316251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Batching\nBatch the inputs during prediction as well since the test set during submission run is a much larger dataset.","metadata":{}},{"cell_type":"code","source":"for i in range(0,len(x)-4,4):\n    pred_batch_List.append((x[i:i+4]))\n    last = i + 4\n    torch.cuda.empty_cache()\npred_batch_List.append(x[last:len(x)])","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.317259Z","iopub.status.idle":"2021-07-18T13:35:07.317949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.318992Z","iopub.status.idle":"2021-07-18T13:35:07.319581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting","metadata":{}},{"cell_type":"code","source":"for x in pred_batch_List:\n    with torch.no_grad():\n        a= x.to(device)\n        y_temp = model(a)\n        y_temp = y_temp.cpu()\n        predictions.append(y_temp)\n    del y_temp\n    del a\n    gc.collect()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.320702Z","iopub.status.idle":"2021-07-18T13:35:07.321301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import operator as op","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.322345Z","iopub.status.idle":"2021-07-18T13:35:07.322936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" torch.cuda.memory_summary(device=None, abbreviated=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.324084Z","iopub.status.idle":"2021-07-18T13:35:07.324695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pred_batch_List)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.325822Z","iopub.status.idle":"2021-07-18T13:35:07.326413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"y=torch.cat(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.327514Z","iopub.status.idle":"2021-07-18T13:35:07.328102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.329183Z","iopub.status.idle":"2021-07-18T13:35:07.329798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = y.cpu()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.330866Z","iopub.status.idle":"2021-07-18T13:35:07.331579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = y.detach().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.332861Z","iopub.status.idle":"2021-07-18T13:35:07.333503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.334589Z","iopub.status.idle":"2021-07-18T13:35:07.33522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(preds)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.336321Z","iopub.status.idle":"2021-07-18T13:35:07.336908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.337961Z","iopub.status.idle":"2021-07-18T13:35:07.338547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = test_df","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.339585Z","iopub.status.idle":"2021-07-18T13:35:07.340195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp= pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.341293Z","iopub.status.idle":"2021-07-18T13:35:07.341931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.343062Z","iopub.status.idle":"2021-07-18T13:35:07.343652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[\"target\"] = preds","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.344767Z","iopub.status.idle":"2021-07-18T13:35:07.345369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.drop([\"url_legal\",\"license\",\"excerpt\"],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.346381Z","iopub.status.idle":"2021-07-18T13:35:07.346989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.348072Z","iopub.status.idle":"2021-07-18T13:35:07.348685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## submission\nsave the predictions to sub and save it to submission.csv for prediction","metadata":{}},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T13:35:07.34982Z","iopub.status.idle":"2021-07-18T13:35:07.350617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Credits","metadata":{}},{"cell_type":"markdown","source":"* This notebook uses the readnet implementation from https://github.com/vdefont/readnet.\n* Readnet Paper : https://arxiv.org/abs/2103.04083\n","metadata":{}},{"cell_type":"markdown","source":"## Results \nAlthough the validation score was 0.47 , The submission score was not 0.674 .The model has not generalized well, I have observed this with other models too, do share your thoughts on this in the comments.","metadata":{}}]}