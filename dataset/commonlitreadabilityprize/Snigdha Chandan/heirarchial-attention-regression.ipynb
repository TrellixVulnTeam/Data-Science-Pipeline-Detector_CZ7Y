{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import datetime, pickle, os, codecs, re, string\nimport json\nimport random\nimport numpy as np\nimport keras\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import *\nfrom keras import regularizers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import backend as K\nfrom keras.utils import CustomObjectScope\nfrom keras.engine.topology import Layer\nfrom keras.engine import InputSpec\n\nfrom keras import initializers as initializers, regularizers, constraints\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoModel, AutoTokenizer\nimport string\nfrom spacy.lang.en import English\nimport gensim, logging\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport en_core_web_sm\n\nfrom IPython.display import HTML, display\n\nimport tensorflow as tf\n\nfrom numpy.random import seed\nfrom nltk.tokenize import sent_tokenize,word_tokenize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-20T08:16:33.041373Z","iopub.execute_input":"2021-06-20T08:16:33.041784Z","iopub.status.idle":"2021-06-20T08:16:33.054279Z","shell.execute_reply.started":"2021-06-20T08:16:33.04175Z","shell.execute_reply":"2021-06-20T08:16:33.053022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_embedding_type = \"from_scratch\" \nword_vector_model = \"fasttext\" \nrnn_type = \"GRU\" \nlearning_rate = 0.001\nepochs = 8\nbatch_size = 64","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:16:33.056358Z","iopub.execute_input":"2021-06-20T08:16:33.056837Z","iopub.status.idle":"2021-06-20T08:16:33.077157Z","shell.execute_reply.started":"2021-06-20T08:16:33.05679Z","shell.execute_reply":"2021-06-20T08:16:33.075843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Text Preprocessing**","metadata":{}},{"cell_type":"code","source":"def clean_str(string):\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    cleanr = re.compile('<.*?>')\n    string = re.sub(cleanr, '', string)\n    string = string.replace('_', '')\n    return string.strip().lower()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:16:33.079635Z","iopub.execute_input":"2021-06-20T08:16:33.080076Z","iopub.status.idle":"2021-06-20T08:16:33.091671Z","shell.execute_reply.started":"2021-06-20T08:16:33.080033Z","shell.execute_reply":"2021-06-20T08:16:33.090686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Use pre-trained word embeddings**","metadata":{}},{"cell_type":"code","source":"def load_subword_embedding_300d(word_index):\n    print('load_subword_embedding...')\n    embeddings_index = {}\n    f = codecs.open(\"../input/fasttext-english-embeddings/wiki-news-300d-1M-subword.vec\", encoding='utf-8')\n    for line in tqdm(f):\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n    print('found %s word vectors' % len(embeddings_index))\n    \n    #embedding matrix\n    print('preparing embedding matrix...')\n    words_not_found = []\n    \n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    \n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if (embedding_vector is not None) and len(embedding_vector) > 0:\n            \n            embedding_matrix[i] = embedding_vector\n        else:\n            words_not_found.append(word)\n    \n    return embedding_matrix","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:16:33.093353Z","iopub.execute_input":"2021-06-20T08:16:33.093974Z","iopub.status.idle":"2021-06-20T08:16:33.106779Z","shell.execute_reply.started":"2021-06-20T08:16:33.093927Z","shell.execute_reply":"2021-06-20T08:16:33.105676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalize texts**","metadata":{}},{"cell_type":"code","source":"def normalize(text):\n    text = text.lower().strip()\n    doc = sent_tokenize(text)\n    filtered_sentences = []\n    for sentence in doc:                    \n        sentence = clean_str(sentence)            \n        #sentence = remove_stopwords(sentence)                \n        filtered_sentences.append(sentence)\n    return filtered_sentences","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:16:33.108301Z","iopub.execute_input":"2021-06-20T08:16:33.108759Z","iopub.status.idle":"2021-06-20T08:16:33.123857Z","shell.execute_reply.started":"2021-06-20T08:16:33.108715Z","shell.execute_reply":"2021-06-20T08:16:33.122797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training word embeddings**","metadata":{}},{"cell_type":"code","source":"def create_fasttext(embed_dim, data):\n    \n    filename = './fasttext_model.txt'\n    \n    if not os.path.isfile(filename):    \n        print('create_fasttext...')\n        sent_lst = []\n\n        for doc in data['excerpt']:\n            doc = clean_str(doc)\n            sentences = sent_tokenize(doc)\n            for sent in sentences:\n                word_lst = [w for w in word_tokenize(sent) if w.isalnum()]\n                sent_lst.append(word_lst)\n\n\n        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n        fasttext_model = gensim.models.FastText(\n            word_ngrams=1,\n            sentences=sent_lst, \n            vector_size = embed_dim, \n            workers=os.cpu_count(), \n            window = 1)\n        fasttext_model.save(\"./fasttext_model.txt\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:16:33.125418Z","iopub.execute_input":"2021-06-20T08:16:33.126028Z","iopub.status.idle":"2021-06-20T08:16:33.14198Z","shell.execute_reply.started":"2021-06-20T08:16:33.125981Z","shell.execute_reply":"2021-06-20T08:16:33.140051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data_commonlit(data_dir):\n    vector_dim = 200\n    dim = 5\n    df_train = pd.read_csv(data_dir + 'train.csv')\n    df_test = pd.read_csv(data_dir + 'test.csv')\n    train_tokens = []\n    test_tokens = []\n    for row in tqdm(df_train['excerpt']):    \n        train_tokens.append(normalize(row))  \n    for row in tqdm(df_test['excerpt']):    \n        test_tokens.append(normalize(row))\n    df_train['train_tokens'] = train_tokens \n    df_test['test_tokens'] = test_tokens \n    del train_tokens\n    del test_tokens\n    vector_dim = 200     \n    if word_embedding_type is 'from_scratch':\n        create_fasttext(vector_dim, df_train)\n    \n    X = df_train['train_tokens'].values\n    Y = df_train['target'].values\n    yy=[]\n    for val in Y:\n        yy.append([val])\n    Y = np.array(yy)\n    Z = df_test['test_tokens'].values\n    return (X, Y , df_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:16:33.143563Z","iopub.execute_input":"2021-06-20T08:16:33.144063Z","iopub.status.idle":"2021-06-20T08:16:33.160355Z","shell.execute_reply.started":"2021-06-20T08:16:33.143941Z","shell.execute_reply":"2021-06-20T08:16:33.159385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(X, Y ,test) = load_data_commonlit(\"../input/commonlitreadabilityprize/\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:16:33.162992Z","iopub.execute_input":"2021-06-20T08:16:33.163462Z","iopub.status.idle":"2021-06-20T08:16:35.569706Z","shell.execute_reply.started":"2021-06-20T08:16:33.163397Z","shell.execute_reply":"2021-06-20T08:16:35.568572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Attention Layer**","metadata":{}},{"cell_type":"code","source":"\ndef dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatibl|e with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n\nclass AttentionWithContext(Layer):\n    \"\"\"\n    Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Note: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification/regression) or whatever...\n    \"\"\"\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight(shape=(input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight(shape=(input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:16:35.571417Z","iopub.execute_input":"2021-06-20T08:16:35.571735Z","iopub.status.idle":"2021-06-20T08:16:35.59107Z","shell.execute_reply.started":"2021-06-20T08:16:35.571705Z","shell.execute_reply":"2021-06-20T08:16:35.589879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model architecture**","metadata":{}},{"cell_type":"code","source":"class HAHNetwork():\n    def __init__(self):\n        self.model = None\n        self.MAX_SENTENCE_LENGTH = 0\n        self.MAX_SENTENCE_COUNT = 0\n        self.VOCABULARY_SIZE = 0\n        self.word_embedding = None\n        self.model = None\n        self.word_attention_model = None\n        self.tokenizer = None\n        self.class_count = 1\n\n    def build_model(self, n_classes=1, embedding_dim=200, embeddings_path=False):\n        \n        l2_reg = regularizers.l2(0.001)\n        \n        embedding_weights = np.random.normal(0, 1, (len(self.tokenizer.word_index) + 1, embedding_dim))\n        \n        if embeddings_path is not None:\n\n            if word_embedding_type is 'from_scratch':\n                # FastText\n                filename = './fasttext_model.txt'                \n                model =  gensim.models.FastText.load(filename)\n\n                embeddings_index = model.wv                    \n                embedding_matrix = np.zeros( ( len(self.tokenizer.word_index) + 1, embedding_dim) )\n                #print(self.tokenizer.word_index.items())\n                for word, i in self.tokenizer.word_index.items():\n                    try:\n                        embedding_vector = embeddings_index[word]\n                        if embedding_vector is not None:\n                            embedding_matrix[i] = embedding_vector\n                    except Exception as e:\n                        #print(str(e))\n                        continue\n\n\n            else:                \n                embedding_dim = 300\n                embedding_matrix = load_subword_embedding_300d(self.tokenizer.word_index)\n            embedding_weights = embedding_matrix\n        sentence_in = Input(shape=(self.MAX_SENTENCE_LENGTH,), dtype='float32', name=\"input_1\")\n        embedding_trainable = True\n        \n        \n        \n        if word_embedding_type is 'pre_trained':\n            embedding_trainable = False\n        \n        embedded_word_seq = Embedding(\n            self.VOCABULARY_SIZE,\n            embedding_dim,\n            weights=[embedding_weights],\n            input_length=self.MAX_SENTENCE_LENGTH,\n            trainable=embedding_trainable,\n            #mask_zero=True,\n            mask_zero=False,\n            name='word_embeddings',)(sentence_in) \n        \n                    \n        dropout = Dropout(0.2)(embedded_word_seq)\n        filter_sizes = [3,4,5]\n        convs = []\n        for filter_size in filter_sizes:\n            conv = Conv1D(filters=64, kernel_size=filter_size, padding='same', activation='relu')(dropout)\n            pool = MaxPool1D(filter_size)(conv)\n            convs.append(pool)\n        \n        concatenate = Concatenate(axis=1)(convs)\n        \n        if rnn_type is 'GRU':\n            #word_encoder = Bidirectional(CuDNNGRU(50, return_sequences=True, dropout=0.2))(concatenate)                \n            dropout = Dropout(0.1)(concatenate)\n            word_encoder = Bidirectional(GRU(100, return_sequences=True))(dropout)                \n        else:\n            word_encoder = Bidirectional(LSTM(100, return_sequences=True, dropout=0.2))(embedded_word_seq)\n        dense_transform_word = Dense(\n            100, \n            activation='relu', \n            name='dense_transform_word', \n            kernel_regularizer=l2_reg)(word_encoder)\n        # word attention\n        attention_weighted_sentence = Model(sentence_in, AttentionWithContext(name=\"word_attention\")(dense_transform_word))\n        #attention_weighted_sentence = Model(sentence_in, dense_transform_word)\n        \n        self.word_attention_model = attention_weighted_sentence\n        \n        attention_weighted_sentence.summary()\n\n        # sentence-attention-weighted document scores\n        \n        texts_in = Input(shape=(self.MAX_SENTENCE_COUNT, self.MAX_SENTENCE_LENGTH), dtype='float32', name=\"input_2\")\n        \n        attention_weighted_sentences = TimeDistributed(attention_weighted_sentence)(texts_in)\n        \n        \n        if rnn_type is 'GRU':\n            #sentence_encoder = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))(attention_weighted_sentences)\n            dropout = Dropout(0.1)(attention_weighted_sentences)\n            sentence_encoder = Bidirectional(GRU(100, return_sequences=True))(dropout)\n        else:\n            sentence_encoder = Bidirectional(LSTM(100, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))(attention_weighted_sentences)\n        \n        \n        dense_transform_sentence = Dense(\n            100, \n            activation='relu', \n            name='dense_transform_sentence',\n            kernel_regularizer=l2_reg)(sentence_encoder)\n        \n        # sentence attention\n        attention_weighted_text = AttentionWithContext(name=\"sentence_attention\")(dense_transform_sentence)\n        #attention_weighted_text = dense_transform_sentence\n        \n        \n        prediction = Dense(n_classes, activation='linear')(attention_weighted_text)\n        \n        model = Model(texts_in, prediction)\n        model.summary()\n        optimizer=Adam(lr=learning_rate, decay=0.0001)\n\n        model.compile(\n                      optimizer=optimizer,\n                      loss='mse',\n                      metrics=['accuracy'])\n\n        return model\n\n\n    def get_tokenizer_filename(self, saved_model_filename):\n        return saved_model_filename + '.tokenizer'\n\n    def fit_on_texts(self, texts):\n        self.tokenizer = Tokenizer(filters='\"()*,-/;[\\]^_`{|}~', oov_token='UNK');\n        all_sentences = []\n        max_sentence_count = 0\n        max_sentence_length = 0\n        for text in texts:\n            sentence_count = len(text)\n            if sentence_count > max_sentence_count:\n                max_sentence_count = sentence_count\n            for sentence in text:\n                sentence_length = len(sentence)\n                if sentence_length > max_sentence_length:\n                    max_sentence_length = sentence_length\n                all_sentences.append(sentence)\n\n\n        self.MAX_SENTENCE_COUNT = min(max_sentence_count,30)\n        self.MAX_SENTENCE_LENGTH = min(max_sentence_length,75)\n        \n        self.tokenizer.fit_on_texts(all_sentences)\n        self.VOCABULARY_SIZE = len(self.tokenizer.word_index) + 1\n        self.create_reverse_word_index()\n\n    def create_reverse_word_index(self):\n        self.reverse_word_index = {value:key for key,value in self.tokenizer.word_index.items()}\n\n    def encode_texts(self, texts):\n        encoded_texts = np.zeros((len(texts), self.MAX_SENTENCE_COUNT, self.MAX_SENTENCE_LENGTH))\n        for i, text in enumerate(texts):\n            encoded_text = np.array(pad_sequences(\n                self.tokenizer.texts_to_sequences(text), \n                maxlen=self.MAX_SENTENCE_LENGTH))[:self.MAX_SENTENCE_COUNT]\n            encoded_texts[i][-len(encoded_text):] = encoded_text\n        return encoded_texts\n    def save_tokenizer_on_epoch_end(self, path, epoch):\n        if epoch == 0:\n            tokenizer_state = {\n                'tokenizer': self.tokenizer,\n                'maxSentenceCount': self.MAX_SENTENCE_COUNT,\n                'maxSentenceLength': self.MAX_SENTENCE_LENGTH,\n                'vocabularySize': self.VOCABULARY_SIZE\n            }\n            pickle.dump(tokenizer_state, open(path, \"wb\" ) )\n    def train(self, train_x, train_y,\n              batch_size=16, \n              epochs=1, \n              embedding_dim=200, \n              embeddings_path=False, \n              saved_model_dir='saved_models', \n              saved_model_filename=None,):\n        \n        self.fit_on_texts(train_x)\n        self.model = self.build_model(\n            n_classes=1, \n            embedding_dim=200,\n            embeddings_path=embeddings_path)\n        encoded_train_x = self.encode_texts(train_x)\n        callbacks = [\n            ReduceLROnPlateau(),\n            LambdaCallback(\n                on_epoch_end=lambda epoch, logs: self.save_tokenizer_on_epoch_end(\n                    os.path.join(saved_model_dir, \n                        self.get_tokenizer_filename(saved_model_filename)), epoch))\n        ]\n\n        if saved_model_filename:\n            callbacks.append(\n                ModelCheckpoint(\n                    filepath=os.path.join(saved_model_dir, saved_model_filename),\n                    monitor='val_acc',\n                    save_best_only=True,\n                    save_weights_only=False,\n                )\n            )\n        history = self.model.fit(\n                       x=encoded_train_x, \n                       y=train_y, \n                       batch_size=batch_size, \n                       epochs=epochs, \n                       verbose=1, \n                       validation_split=0.1,  \n                       shuffle=True)\n    def predict(self, x):\n        encoded_x = self.encode_texts(x)\n        return self.model.predict(encoded_x)\n    def encode_input(self, x, log=False):\n        x = np.array(x)\n        if not x.shape:\n            x = np.expand_dims(x, 0)\n        texts = np.array([normalize(text) for text in x])\n        return self.encode_texts(texts)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:16:35.592724Z","iopub.execute_input":"2021-06-20T08:16:35.593115Z","iopub.status.idle":"2021-06-20T08:16:35.634569Z","shell.execute_reply.started":"2021-06-20T08:16:35.593078Z","shell.execute_reply":"2021-06-20T08:16:35.633398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.clear_session()\nmodel = HAHNetwork()\nmodel.train(X, Y, batch_size=128, epochs=20, embeddings_path=True, saved_model_dir=None, saved_model_filename=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:16:35.635911Z","iopub.execute_input":"2021-06-20T08:16:35.636427Z","iopub.status.idle":"2021-06-20T09:12:46.31137Z","shell.execute_reply.started":"2021-06-20T08:16:35.636388Z","shell.execute_reply":"2021-06-20T09:12:46.310313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(test['test_tokens'].values)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:12:46.31305Z","iopub.execute_input":"2021-06-20T09:12:46.313394Z","iopub.status.idle":"2021-06-20T09:12:48.527153Z","shell.execute_reply.started":"2021-06-20T09:12:46.313351Z","shell.execute_reply":"2021-06-20T09:12:48.525791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['prediction'] = preds\nsubmission = pd.DataFrame()\nsubmission['id'] = test['id'].copy()\nsubmission['target'] = test['prediction'].copy()\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:12:48.52843Z","iopub.execute_input":"2021-06-20T09:12:48.528778Z","iopub.status.idle":"2021-06-20T09:12:48.568457Z","shell.execute_reply.started":"2021-06-20T09:12:48.528746Z","shell.execute_reply":"2021-06-20T09:12:48.567411Z"},"trusted":true},"execution_count":null,"outputs":[]}]}