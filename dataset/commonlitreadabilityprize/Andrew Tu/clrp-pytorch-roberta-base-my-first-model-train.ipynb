{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 参考： https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n\nimport sys\nimport os\nimport time\nimport random\nimport re\nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import BertTokenizer, BertModel, BertConfig, AdamW\nfrom transformers import (AutoModel, AutoTokenizer, \n     get_linear_schedule_with_warmup, get_constant_schedule_with_warmup)\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nclass CFG:\n    #model_name_or_path = 'roberta-base'\n    #model_name_or_path = 'bert-base-cased'\n    #model_name_or_path = '../input/huggingface-bert-variants/bert-large-cased/bert-large-cased'\n    #model_name_or_path = '../input/huggingface-bert-variants/bert-base-cased/bert-base-cased'\n    model_name_or_path = '../input/roberta-transformers-pytorch/roberta-base'\n    #model_name_or_path = '../input/clrp-pytorch-roberta-pretrain/clrp_roberta_base'\n#     model_name_or_path = '../input/roberta-transformers-pytorch/roberta-large'\n    batch_size = 8\n    max_seq_length = 512\n    seq_length = 100\n    learning_rate = 2.0e-5  #0.7e-5\n    weight_decay = 1e-1\n    use_lr_scheduler = True\n    mid_eval = True\n    mid_eval_step_num = 50\n    random_seed = 2021\n    model_output_dir = './'\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \ncfg = CFG()\nseed_everything(cfg.random_seed)\n\nQUICK_CHECK = False\n\nglobal_start_t = time.time()\nprint('ok')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-24T18:42:49.686524Z","iopub.execute_input":"2021-06-24T18:42:49.687104Z","iopub.status.idle":"2021-06-24T18:42:58.513035Z","shell.execute_reply.started":"2021-06-24T18:42:49.687002Z","shell.execute_reply":"2021-06-24T18:42:58.512255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntrain_data = train_data[['excerpt', 'target']]\nprint('000, train_data.shape: ', train_data.shape, 'test_data.shape: ', test_data.shape)\n\ntrain_data = train_data.sample(len(train_data))\nif QUICK_CHECK:\n    train_data = train_data[:800]\n\nprint('111, train_data.shape: ', train_data.shape)\n\nTRAIN_RATIO = 0.8\ntrain_num = int(TRAIN_RATIO * len(train_data))\ntrain_data, valid_data = train_data[:train_num], train_data[train_num:]\ntrain_data = train_data.reset_index(drop=True)\nvalid_data = valid_data.reset_index(drop=True)\n\nprint('222, train_data.shape: ', train_data.shape, 'valid_data.shape: ', valid_data.shape)\n\nprint(f'train_data.shape: {train_data.shape}, test_data.shape: {test_data.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-06-24T18:42:58.514385Z","iopub.execute_input":"2021-06-24T18:42:58.514877Z","iopub.status.idle":"2021-06-24T18:42:58.652847Z","shell.execute_reply.started":"2021-06-24T18:42:58.514845Z","shell.execute_reply":"2021-06-24T18:42:58.651928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse1 = mean_squared_error(train_data['target'], [train_data['target'].median()]*len(train_data), squared=False)\nrmse2 = mean_squared_error(train_data['target'], [train_data['target'].mean()]*len(train_data), squared=False)\nRMSE1 = sqrt((np.square(np.subtract(train_data['target'], train_data['target'].median()))).mean())\nRMSE2 = sqrt((np.square(np.subtract(train_data['target'].mean(), train_data['target']))).mean())\n\nrmse1, rmse2, RMSE1, RMSE2","metadata":{"execution":{"iopub.status.busy":"2021-06-24T18:42:58.654547Z","iopub.execute_input":"2021-06-24T18:42:58.654821Z","iopub.status.idle":"2021-06-24T18:42:58.689252Z","shell.execute_reply.started":"2021-06-24T18:42:58.654795Z","shell.execute_reply":"2021-06-24T18:42:58.688275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRP_Dataset(Dataset):\n    def __init__(self, data, tokenizer, max_seq_length):\n        self.df = data\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index):\n        excerpt = self.df.iloc[index]['excerpt']\n        target = self.df.iloc[index]['target']\n        d_encode = self.tokenizer(excerpt,\n                                  #return_tensors='pt',\n                                  padding=\"max_length\",\n                                  max_length=self.max_seq_length,\n                                  truncation=True)\n        #print('d_encode.keys()', d_encode.keys())\n        if 'token_type_ids' in d_encode:\n            return {\"input_ids\": d_encode['input_ids'],\n                    \"token_type_ids\": d_encode['token_type_ids'],\n                    \"attention_mask\": d_encode['attention_mask'],\n                    \"length\" : sum(d_encode['attention_mask']),\n                    \"target\": target}\n        else:\n            return {\"input_ids\": d_encode['input_ids'],\n                    \"attention_mask\": d_encode['attention_mask'],\n                    \"length\" : sum(d_encode['attention_mask']),\n                    \"target\": target}\n    \ndef collate_fn(batch):\n    max_len = max([x['length'] for x in batch])\n    input_ids = torch.tensor([x['input_ids'][:max_len] for x in batch])\n    attention_mask = torch.tensor([x['attention_mask'][:max_len] for x in batch])\n    targets = torch.tensor([x[\"target\"] for x in batch]).float()\n    \n    if 'token_type_ids' in batch[0]:\n        token_type_ids = torch.tensor([x['token_type_ids'][:max_len] for x in batch])\n        return {\"all_input_ids\": input_ids,\n                \"all_attention_mask\": attention_mask,\n                \"all_token_type_ids\": token_type_ids,\n                \"all_targets\": targets}\n    else:\n        return {\"all_input_ids\": input_ids,\n                \"all_attention_mask\": attention_mask,\n                \"all_targets\": targets}\n    \ntokenizer = AutoTokenizer.from_pretrained(cfg.model_name_or_path)\ncfg.tokenizer = tokenizer\n\nds_train = CLRP_Dataset(train_data, tokenizer, cfg.max_seq_length)\ndl_train = DataLoader(ds_train, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\nprint('len of ds_train: ', len(ds_train), 'len of dl_train: ', len(dl_train))\n\nds_valid = CLRP_Dataset(valid_data, tokenizer, cfg.max_seq_length)\ndl_valid = DataLoader(ds_valid, batch_size=2*cfg.batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0)\nprint('len of ds_valid: ', len(ds_valid), 'len of dl_valid: ', len(dl_valid))\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-24T18:42:58.690812Z","iopub.execute_input":"2021-06-24T18:42:58.691096Z","iopub.status.idle":"2021-06-24T18:42:58.991935Z","shell.execute_reply.started":"2021-06-24T18:42:58.691068Z","shell.execute_reply":"2021-06-24T18:42:58.99124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRP_model_V1(nn.Module):\n    def __init__(self, pretrained_model_path, embedding_dim=256):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(pretrained_model_path)\n        use_large_model = True if 'large' in pretrained_model_path else False\n        self.drop_out = nn.Dropout(0.1)\n        if use_large_model:\n            self.fc1 = nn.Linear(1024*2, embedding_dim)\n        else:\n            self.fc1 = nn.Linear(768*2, embedding_dim)\n        self.activation1 = nn.ReLU()\n        self.fc2 = nn.Linear(embedding_dim, 1)\n\n    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None):\n        output = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, return_dict=True)\n        assert len(output)==2\n        \n        last_hidden_state, pooled_output = output['last_hidden_state'], output['pooler_output']\n        last_hidden_state = self.drop_out(last_hidden_state)\n        seq_avg = torch.mean(last_hidden_state, dim=1)\n        seq_max = torch.max(last_hidden_state, dim=1)[0]\n        concat_out = torch.cat((seq_avg, seq_max), dim=1)\n        preds = self.fc2(self.activation1(self.fc1(concat_out)))        \n        preds = preds.squeeze(-1).squeeze(-1)  ### 这一行非常非常关键！！！\n        \n        return preds\n    \nclass CLRP_model_V2(nn.Module):\n    def __init__(self, pretrained_model_path, embedding_dim=256):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(pretrained_model_path)\n        use_large_model = True if 'large' in pretrained_model_path else False\n        self.drop_out = nn.Dropout(0.1)\n        if use_large_model:\n            self.fc1 = nn.Linear(1024*1, 1)\n        else:\n            self.fc1 = nn.Linear(768*1, 1)\n        \n\n    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None):\n        output = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, return_dict=True)\n        assert len(output)==2\n        \n        pooled_output = output['pooler_output']\n        pooled_output = self.drop_out(pooled_output)\n        preds = self.fc1(pooled_output)\n        preds = preds.squeeze(-1).squeeze(-1)  ### 这一行非常非常关键！！！\n        \n        return preds\n    \nCLRP_model = CLRP_model_V2\nmodel = CLRP_model(cfg.model_name_or_path)\nmodel.to(cfg.device)\n\nprint('model is ', model)\n\nmodel_param_num = sum(p.numel() for p in model.parameters())\nmodel_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('model_param_num: ', model_param_num, 'model_trainable_param_num: ', \n      model_trainable_param_num)\n            \nprint('ok')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-24T18:42:58.993089Z","iopub.execute_input":"2021-06-24T18:42:58.993504Z","iopub.status.idle":"2021-06-24T18:43:07.7054Z","shell.execute_reply.started":"2021-06-24T18:42:58.993474Z","shell.execute_reply":"2021-06-24T18:43:07.704357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T18:43:07.706902Z","iopub.execute_input":"2021-06-24T18:43:07.707318Z","iopub.status.idle":"2021-06-24T18:43:07.725777Z","shell.execute_reply.started":"2021-06-24T18:43:07.707273Z","shell.execute_reply":"2021-06-24T18:43:07.725077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(eval_iter, model, criterion, train_part=False):\n    global cfg\n    model.eval()\n\n    predictions_lst, targets_lst = [], []\n    for step, batch in enumerate(eval_iter):\n        for key in batch.keys():\n            batch[key] = batch[key].to(cfg.device)\n        with torch.no_grad():\n            if 'all_token_type_ids' in batch:\n                predictions = model(\n                    input_ids=batch['all_input_ids'],\n                    attention_mask=batch['all_attention_mask'],\n                    token_type_ids=batch['all_token_type_ids'])\n            else:\n                predictions = model(\n                    input_ids=batch['all_input_ids'],\n                    attention_mask=batch['all_attention_mask'])\n        predictions_lst += list(predictions.cpu().numpy().ravel())\n        targets_lst += list(batch['all_targets'].cpu().numpy().ravel())\n\n    #model.train()  # 将模型重新置为训练状态\n    assert len(targets_lst)==len(predictions_lst), 'length should be equal'\n\n    RMSE_val = mean_squared_error(targets_lst, predictions_lst, squared=False)\n    return RMSE_val\n\ndef train(train_iter, test_iter, model, optimizer, criterion, lr_scheduler=None):\n    global cfg, global_step_num, global_best_valid_loss, valid_loss_history\n    model.train()\n    #model.eval()\n    \n    predictions_lst, targets_lst = [], []\n    for step, batch in enumerate(train_iter):\n        global_step_num += 1\n        for key in batch.keys():\n            batch[key] = batch[key].to(cfg.device)\n        if 'all_token_type_ids' in batch:\n            predictions = model(\n                input_ids=batch['all_input_ids'],\n                attention_mask=batch['all_attention_mask'],\n                token_type_ids=batch['all_token_type_ids'])\n        else:\n            predictions = model(\n                input_ids=batch['all_input_ids'],\n                attention_mask=batch['all_attention_mask'])\n        #print('in train() predictions.shape: ', predictions.shape, 'batch[all_targets]', batch['all_targets'].shape)\n        loss = criterion(predictions, batch['all_targets'])\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n            \n        predictions_lst += list(predictions.detach().cpu().numpy().ravel())\n        targets_lst += list(batch['all_targets'].cpu().numpy().ravel())\n        \n        if cfg.mid_eval and global_step_num%cfg.mid_eval_step_num==0:\n            valid_loss = evaluate(test_iter, model, criterion)\n            valid_loss_history.append(valid_loss)\n            print(f'mid eval, global_step_num: {global_step_num}, valid_loss: {valid_loss:.7f}')\n            if valid_loss < global_best_valid_loss:\n                global_best_valid_loss = valid_loss\n                print(f'get new valid_loss: {valid_loss: .7f}, saving model now!')\n                torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, \"best_model.pth\"))\n                \n    assert len(targets_lst)==len(predictions_lst), 'length should be equal'\n    RMSE_val = mean_squared_error(targets_lst, predictions_lst, squared=False)\n    \n    return RMSE_val\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-24T18:43:07.727082Z","iopub.execute_input":"2021-06-24T18:43:07.727667Z","iopub.status.idle":"2021-06-24T18:43:07.74663Z","shell.execute_reply.started":"2021-06-24T18:43:07.727612Z","shell.execute_reply":"2021-06-24T18:43:07.745413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCH_NUM = 3\nglobal_step_num = 0\n#optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=1e-4)\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\nlr_scheduler = None\nif cfg.use_lr_scheduler:\n#     lr_scheduler = get_constant_schedule_with_warmup(optimizer, 100)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer, \n                       num_warmup_steps = int(0.1*EPOCH_NUM*len(dl_train)),\n                       num_training_steps = EPOCH_NUM*len(dl_train))\ntrain_loss_history = []\nvalid_loss_history = []\nglobal_best_train_loss = 1e9\nglobal_best_valid_loss = 1e9\ncriterion = nn.MSELoss().to(cfg.device)\n\nfor epoch_n in range(EPOCH_NUM):\n    print('epoch_n: ', epoch_n)\n    if time.time() - global_start_t > 60*60*7.5:\n        break\n    train_loss = train(dl_train, dl_valid, model, optimizer, criterion, lr_scheduler=lr_scheduler)\n    valid_loss = evaluate(dl_valid, model, criterion)\n    \n    train_loss_history.append(train_loss)\n    valid_loss_history.append(valid_loss)\n    \n    if train_loss < global_best_train_loss:\n        global_best_train_loss = train_loss\n    if valid_loss < global_best_valid_loss:\n        global_best_valid_loss = valid_loss\n        print(f'after epoch {epoch_n}, global_step_num: {global_step_num} get new best_valid_loss: {valid_loss:.5f}, save the model now!')\n        torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_model.pth'))\n        \nprint(f'ok, global_best_train_loss: {global_best_train_loss:.7f} global_best_valid_loss: {global_best_valid_loss:.7f}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-24T18:43:07.749385Z","iopub.execute_input":"2021-06-24T18:43:07.749698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epoch_n:  0\n# mid eval, global_step_num: 50, valid_loss: 0.9781250\n# get new valid_loss:  0.9781250, saving model now!\n# mid eval, global_step_num: 100, valid_loss: 0.6129924\n# get new valid_loss:  0.6129924, saving model now!\n# mid eval, global_step_num: 150, valid_loss: 0.6577457\n# mid eval, global_step_num: 200, valid_loss: 0.5382791\n# get new valid_loss:  0.5382791, saving model now!\n# mid eval, global_step_num: 250, valid_loss: 0.5990030\n# after epoch 0, global_step_num: 284 get new best_valid_loss: 0.50697, save the model now!\n# epoch_n:  1\n# mid eval, global_step_num: 300, valid_loss: 0.5512493\n# mid eval, global_step_num: 350, valid_loss: 0.4743678\n# get new valid_loss:  0.4743678, saving model now!\n# mid eval, global_step_num: 400, valid_loss: 0.5294796\n# mid eval, global_step_num: 450, valid_loss: 0.4888377\n# mid eval, global_step_num: 500, valid_loss: 0.5500329\n# mid eval, global_step_num: 550, valid_loss: 0.4636581\n# get new valid_loss:  0.4636581, saving model now!\n# after epoch 1, global_step_num: 568 get new best_valid_loss: 0.46200, save the model now!\n# epoch_n:  2\n# mid eval, global_step_num: 600, valid_loss: 0.5433109\n# mid eval, global_step_num: 650, valid_loss: 0.4648798\n# mid eval, global_step_num: 700, valid_loss: 0.4613321\n# get new valid_loss:  0.4613321, saving model now!\n# mid eval, global_step_num: 750, valid_loss: 0.4566262\n# get new valid_loss:  0.4566262, saving model now!\n# mid eval, global_step_num: 800, valid_loss: 0.4564575\n# get new valid_loss:  0.4564575, saving model now!\n# mid eval, global_step_num: 850, valid_loss: 0.4538009\n# get new valid_loss:  0.4538009, saving model now!\n# after epoch 2, global_step_num: 852 get new best_valid_loss: 0.45379, save the model now!\n# ok, global_best_train_loss: 0.3171585 global_best_valid_loss: 0.4537924","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lr=1.5e-5  weight_decay=1e-4\n# epoch_n:  0\n# mid eval, global_step_num: 50, valid_loss: 0.9391119\n# get new valid_loss:  0.9391119, saving model now!\n# mid eval, global_step_num: 100, valid_loss: 0.7008228\n# get new valid_loss:  0.7008228, saving model now!\n# mid eval, global_step_num: 150, valid_loss: 0.5801484\n# get new valid_loss:  0.5801484, saving model now!\n# mid eval, global_step_num: 200, valid_loss: 0.5481895\n# get new valid_loss:  0.5481895, saving model now!\n# mid eval, global_step_num: 250, valid_loss: 0.5780302\n# after epoch 0, global_step_num: 284 get new best_valid_loss: 0.52035, save the model now!\n# epoch_n:  1\n# mid eval, global_step_num: 300, valid_loss: 0.4985234\n# get new valid_loss:  0.4985234, saving model now!\n# mid eval, global_step_num: 350, valid_loss: 0.4815534\n# get new valid_loss:  0.4815534, saving model now!\n# mid eval, global_step_num: 400, valid_loss: 0.5474174\n# mid eval, global_step_num: 450, valid_loss: 0.4902881\n# mid eval, global_step_num: 500, valid_loss: 0.5117764\n# mid eval, global_step_num: 550, valid_loss: 0.4805826\n# get new valid_loss:  0.4805826, saving model now!\n# after epoch 1, global_step_num: 568 get new best_valid_loss: 0.47472, save the model now!\n# epoch_n:  2\n# mid eval, global_step_num: 600, valid_loss: 0.4689728\n# get new valid_loss:  0.4689728, saving model now!\n# mid eval, global_step_num: 650, valid_loss: 0.4680705\n# get new valid_loss:  0.4680705, saving model now!\n# mid eval, global_step_num: 700, valid_loss: 0.4656835\n# get new valid_loss:  0.4656835, saving model now!\n# mid eval, global_step_num: 750, valid_loss: 0.4662796\n# mid eval, global_step_num: 800, valid_loss: 0.4656795\n# get new valid_loss:  0.4656795, saving model now!\n# mid eval, global_step_num: 850, valid_loss: 0.4609354\n# get new valid_loss:  0.4609354, saving model now!\n# after epoch 2, global_step_num: 852 get new best_valid_loss: 0.46093, save the model now!\n# ok, global_best_train_loss: 0.3325094 global_best_valid_loss: 0.4609301\n\n\n# lr=2.0e-5  weight_decay=1e-1\n# epoch_n:  0\n# mid eval, global_step_num: 50, valid_loss: 0.9731567\n# get new valid_loss:  0.9731567, saving model now!\n# mid eval, global_step_num: 100, valid_loss: 0.8206061\n# get new valid_loss:  0.8206061, saving model now!\n# mid eval, global_step_num: 150, valid_loss: 0.6018713\n# get new valid_loss:  0.6018713, saving model now!\n# mid eval, global_step_num: 200, valid_loss: 0.5351610\n# get new valid_loss:  0.5351610, saving model now!\n# mid eval, global_step_num: 250, valid_loss: 0.5814825\n# after epoch 0, global_step_num: 284 get new best_valid_loss: 0.51046, save the model now!\n# epoch_n:  1\n# mid eval, global_step_num: 300, valid_loss: 0.4970206\n# get new valid_loss:  0.4970206, saving model now!\n# mid eval, global_step_num: 350, valid_loss: 0.4771110\n# get new valid_loss:  0.4771110, saving model now!\n# mid eval, global_step_num: 400, valid_loss: 0.5300065\n# mid eval, global_step_num: 450, valid_loss: 0.5006436\n# mid eval, global_step_num: 500, valid_loss: 0.5250693\n# mid eval, global_step_num: 550, valid_loss: 0.4652649\n# get new valid_loss:  0.4652649, saving model now!\n# epoch_n:  2\n# mid eval, global_step_num: 600, valid_loss: 0.4700000\n# mid eval, global_step_num: 650, valid_loss: 0.4708829\n# mid eval, global_step_num: 700, valid_loss: 0.4619811\n# get new valid_loss:  0.4619811, saving model now!\n# mid eval, global_step_num: 750, valid_loss: 0.4615610\n# get new valid_loss:  0.4615610, saving model now!\n# mid eval, global_step_num: 800, valid_loss: 0.4591095\n# get new valid_loss:  0.4591095, saving model now!\n# mid eval, global_step_num: 850, valid_loss: 0.4576899\n# get new valid_loss:  0.4576899, saving model now!\n# after epoch 2, global_step_num: 852 get new best_valid_loss: 0.45769, save the model now!\n# ok, global_best_train_loss: 0.3082862 global_best_valid_loss: 0.4576878","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CLRP_model(cfg.model_name_or_path)\nmodel.to(cfg.device)\n\nmodel.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, 'best_model.pth')))\nvalid_loss = evaluate(dl_valid, model, criterion)\n\nprint(f'final best model valid_loss: {valid_loss:.7f}, global_best_valid_loss: {global_best_valid_loss:.7f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure, axes = plt.subplots(figsize=(15, 6))\n\naxes.plot(list(range(len(valid_loss_history))), valid_loss_history, color='red', label='Valid_RMSE_Loss')\nplt.title('valid_loss History')\nplt.xlabel('step_num')\nplt.ylabel('valid_loss')\nplt.legend()\nplt.savefig('loss_history_valid.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure, axes = plt.subplots(figsize=(15, 6))\n\naxes.plot(list(range(len(train_loss_history))), train_loss_history, color='blue', label='Train_RMSE_Loss')\nplt.title('train_loss History')\nplt.xlabel('step_num')\nplt.ylabel('train_loss')\nplt.legend()\nplt.savefig('loss_history_train.png')\nplt.show()\n\nprint('finished, total cost time: ', time.time()-global_start_t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}