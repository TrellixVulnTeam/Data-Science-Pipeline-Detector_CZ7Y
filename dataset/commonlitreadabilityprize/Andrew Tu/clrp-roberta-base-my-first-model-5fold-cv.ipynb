{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 参考： https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n\nimport sys\nimport os\nimport time\nimport random\nimport re\nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import BertTokenizer, BertModel, BertConfig, AdamW\nfrom transformers import (AutoModel, AutoTokenizer, \n     get_linear_schedule_with_warmup, AdamW)\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nclass CFG:\n    #model_name_or_path = 'roberta-base'\n    #model_name_or_path = 'bert-base-cased'\n    #model_name_or_path = '../input/huggingface-bert-variants/bert-large-cased/bert-large-cased'\n    #model_name_or_path = '../input/huggingface-bert-variants/bert-base-cased/bert-base-cased'\n    model_name_or_path = '../input/roberta-transformers-pytorch/roberta-base'\n    #model_name_or_path = '../input/roberta-transformers-pytorch/roberta-large'\n    batch_size = 8\n    max_seq_length = 512\n    learning_rate = 2.5e-5\n    weight_decay = 1e-1\n    use_lr_scheduler = True\n    mid_eval = True\n    mid_eval_step_num = 50\n    CV_fold_num = 5\n    random_seed = 2021\n    model_output_dir = './'\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \ncfg = CFG()\nseed_everything(cfg.random_seed)\n\nQUICK_CHECK = False\n\nglobal_start_t = time.time()\nprint('ok')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-25T08:13:56.800749Z","iopub.execute_input":"2021-06-25T08:13:56.801114Z","iopub.status.idle":"2021-06-25T08:14:04.44199Z","shell.execute_reply.started":"2021-06-25T08:13:56.801037Z","shell.execute_reply":"2021-06-25T08:14:04.441181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntrain_data = train_data[['excerpt', 'target']]\nprint('000, train_data.shape: ', train_data.shape, 'test_data.shape: ', test_data.shape)\n\ndata = train_data.sample(len(train_data)).reset_index(drop=True)\nif QUICK_CHECK:\n    data = data[:800]\n    \nnum_bins = int(np.floor(1 + np.log2(len(data))))\ndata.loc[:,'bins'] = pd.cut(data['target'], bins=num_bins, labels=False)\nbins = data['bins'].to_numpy()\n\nprint('111, data.shape: ', data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:14:04.443531Z","iopub.execute_input":"2021-06-25T08:14:04.443889Z","iopub.status.idle":"2021-06-25T08:14:04.531409Z","shell.execute_reply.started":"2021-06-25T08:14:04.44385Z","shell.execute_reply":"2021-06-25T08:14:04.530553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRP_Dataset(Dataset):\n    def __init__(self, data, tokenizer, max_seq_length):\n        self.df = data\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index):\n        excerpt = self.df.iloc[index]['excerpt']\n        target = self.df.iloc[index]['target']\n        d_encode = self.tokenizer(excerpt,\n                                  #return_tensors='pt',\n                                  padding=\"max_length\",\n                                  max_length=self.max_seq_length,\n                                  truncation=True)\n        #print('d_encode.keys()', d_encode.keys())\n        if 'token_type_ids' in d_encode:\n            return {\"input_ids\": d_encode['input_ids'],\n                    \"token_type_ids\": d_encode['token_type_ids'],\n                    \"attention_mask\": d_encode['attention_mask'],\n                    \"length\" : sum(d_encode['attention_mask']),\n                    \"target\": target}\n        else:\n            return {\"input_ids\": d_encode['input_ids'],\n                    \"attention_mask\": d_encode['attention_mask'],\n                    \"length\" : sum(d_encode['attention_mask']),\n                    \"target\": target}\n    \ndef collate_fn(batch):\n    max_len = max([x['length'] for x in batch])\n    input_ids = torch.tensor([x['input_ids'][:max_len] for x in batch])\n    attention_mask = torch.tensor([x['attention_mask'][:max_len] for x in batch])\n    targets = torch.tensor([x[\"target\"] for x in batch]).float()\n    \n    if 'token_type_ids' in batch[0]:\n        token_type_ids = torch.tensor([x['token_type_ids'][:max_len] for x in batch])\n        return {\"all_input_ids\": input_ids,\n                \"all_attention_mask\": attention_mask,\n                \"all_token_type_ids\": token_type_ids,\n                \"all_targets\": targets}\n    else:\n        return {\"all_input_ids\": input_ids,\n                \"all_attention_mask\": attention_mask,\n                \"all_targets\": targets}\n    \ntokenizer = AutoTokenizer.from_pretrained(cfg.model_name_or_path)\ncfg.tokenizer = tokenizer\n\ndef get_dataloader(train_data, valid_data):\n    global cfg\n    ds_train = CLRP_Dataset(train_data, cfg.tokenizer, cfg.max_seq_length)\n    dl_train = DataLoader(ds_train, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n    print('len of ds_train: ', len(ds_train), 'len of dl_train: ', len(dl_train))\n\n    ds_valid = CLRP_Dataset(valid_data, cfg.tokenizer, cfg.max_seq_length)\n    dl_valid = DataLoader(ds_valid, batch_size=2*cfg.batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0)\n    print('len of ds_valid: ', len(ds_valid), 'len of dl_valid: ', len(dl_valid))\n    return dl_train, dl_valid\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:14:04.533461Z","iopub.execute_input":"2021-06-25T08:14:04.533776Z","iopub.status.idle":"2021-06-25T08:14:04.763695Z","shell.execute_reply.started":"2021-06-25T08:14:04.533739Z","shell.execute_reply":"2021-06-25T08:14:04.762796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRP_model_V1(nn.Module):\n    def __init__(self, pretrained_model_path, embedding_dim=256):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(pretrained_model_path)\n        use_large_model = True if 'large' in pretrained_model_path else False\n        self.drop_out = nn.Dropout(0.1)\n        if use_large_model:\n            self.fc1 = nn.Linear(1024*2, embedding_dim)\n        else:\n            self.fc1 = nn.Linear(768*2, embedding_dim)\n        self.activation1 = nn.ReLU()\n        self.fc2 = nn.Linear(embedding_dim, 1)\n\n    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None):\n        output = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, return_dict=True)\n        assert len(output)==2\n        \n        last_hidden_state, pooled_output = output['last_hidden_state'], output['pooler_output']\n        last_hidden_state = self.drop_out(last_hidden_state)\n        seq_avg = torch.mean(last_hidden_state, dim=1)\n        seq_max = torch.max(last_hidden_state, dim=1)[0]\n        concat_out = torch.cat((seq_avg, seq_max), dim=1)\n        preds = self.fc2(self.activation1(self.fc1(concat_out)))        \n        preds = preds.squeeze(-1).squeeze(-1)  ### 这一行非常非常关键！！！\n        \n        return preds\n    \nclass CLRP_model_V2(nn.Module):\n    def __init__(self, pretrained_model_path, embedding_dim=256):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(pretrained_model_path)\n        use_large_model = True if 'large' in pretrained_model_path else False\n        self.drop_out = nn.Dropout(0.1)\n        if use_large_model:\n            self.fc1 = nn.Linear(1024*1, 1)\n        else:\n            self.fc1 = nn.Linear(768*1, 1)\n        \n\n    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None):\n        output = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, return_dict=True)\n        assert len(output)==2\n        \n        pooled_output = output['pooler_output']\n        pooled_output = self.drop_out(pooled_output)\n        preds = self.fc1(pooled_output)\n        preds = preds.squeeze(-1).squeeze(-1)  ### 这一行非常非常关键！！！\n        \n        return preds\n    \nCLRP_model = CLRP_model_V2\n    \nmodel = CLRP_model(cfg.model_name_or_path)\nmodel.to(cfg.device)\n\nmodel_param_num = sum(p.numel() for p in model.parameters())\nmodel_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('model_param_num: ', model_param_num, 'model_trainable_param_num: ', \n      model_trainable_param_num)\ndel model\n            \nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:14:04.765265Z","iopub.execute_input":"2021-06-25T08:14:04.76576Z","iopub.status.idle":"2021-06-25T08:14:18.914536Z","shell.execute_reply.started":"2021-06-25T08:14:04.765719Z","shell.execute_reply":"2021-06-25T08:14:18.913663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(eval_iter, model, criterion, train_part=False):\n    global cfg\n    model.eval()\n\n    predictions_lst, targets_lst = [], []\n    for step, batch in enumerate(eval_iter):\n        for key in batch.keys():\n            batch[key] = batch[key].to(cfg.device)\n        with torch.no_grad():\n            if 'all_token_type_ids' in batch:\n                predictions = model(\n                    input_ids=batch['all_input_ids'],\n                    attention_mask=batch['all_attention_mask'],\n                    token_type_ids=batch['all_token_type_ids'])\n            else:\n                predictions = model(\n                    input_ids=batch['all_input_ids'],\n                    attention_mask=batch['all_attention_mask'])\n        predictions_lst += list(predictions.cpu().numpy().ravel())\n        targets_lst += list(batch['all_targets'].cpu().numpy().ravel())\n\n    #model.train()  # 将模型重新置为训练状态\n    assert len(targets_lst)==len(predictions_lst), 'length should be equal'\n\n    RMSE_val = mean_squared_error(targets_lst, predictions_lst, squared=False)\n    return RMSE_val\n\ndef train(fold_num, train_iter, test_iter, model, optimizer, criterion, lr_scheduler=None):\n    global cfg, global_step_num, global_best_valid_loss\n    model.train()\n    \n    predictions_lst, targets_lst = [], []\n    for step, batch in enumerate(train_iter):\n        global_step_num += 1\n        for key in batch.keys():\n            batch[key] = batch[key].to(cfg.device)\n        if 'all_token_type_ids' in batch:\n            predictions = model(\n                input_ids=batch['all_input_ids'],\n                attention_mask=batch['all_attention_mask'],\n                token_type_ids=batch['all_token_type_ids'])\n        else:\n            predictions = model(\n                input_ids=batch['all_input_ids'],\n                attention_mask=batch['all_attention_mask'])\n        loss = criterion(predictions, batch['all_targets'])\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        model.zero_grad()\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n            \n        predictions_lst += list(predictions.detach().cpu().numpy().ravel())\n        targets_lst += list(batch['all_targets'].cpu().numpy().ravel())\n        \n        if cfg.mid_eval and global_step_num%cfg.mid_eval_step_num==0:\n            valid_loss = evaluate(test_iter, model, criterion)\n            print(f'mid eval, global_step_num: {global_step_num}, valid_loss: {valid_loss:.7f}')\n            if valid_loss < global_best_valid_loss:\n                global_best_valid_loss = valid_loss\n                print(f'get new valid_loss: {valid_loss: .7f}, saving model now!')\n                torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, f'best_model_fold{fold_num}.pth'))\n                \n    assert len(targets_lst)==len(predictions_lst), 'length should be equal'\n    RMSE_val = mean_squared_error(targets_lst, predictions_lst, squared=False)\n    \n    return RMSE_val\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:14:18.91741Z","iopub.execute_input":"2021-06-25T08:14:18.917679Z","iopub.status.idle":"2021-06-25T08:14:18.93537Z","shell.execute_reply.started":"2021-06-25T08:14:18.917652Z","shell.execute_reply":"2021-06-25T08:14:18.934608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_whole_train(fold_num, dl_train, dl_valid):\n    global cfg, EPOCH_NUM, global_step_num\n    global global_best_train_loss, global_best_valid_loss\n    \n    model = CLRP_model(cfg.model_name_or_path)\n    model.to(cfg.device)\n    criterion = nn.MSELoss()\n    \n    global_step_num = 0\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n    #optimizer = AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n    lr_scheduler = None\n    if cfg.use_lr_scheduler:\n        lr_scheduler = get_linear_schedule_with_warmup(optimizer, \n                           num_warmup_steps = int(0.1*EPOCH_NUM*len(dl_train)),\n                           num_training_steps = EPOCH_NUM*len(dl_train))\n    global_best_train_loss = 1e9\n    global_best_valid_loss = 1e9\n\n    for epoch_n in range(EPOCH_NUM):\n        print('epoch_n: ', epoch_n)\n        if time.time() - global_start_t > 60*60*7.5:\n            break\n        train_loss = train(fold_num, dl_train, dl_valid, model, optimizer, criterion, lr_scheduler=lr_scheduler)\n        valid_loss = evaluate(dl_valid, model, criterion)\n\n        if train_loss < global_best_train_loss:\n            global_best_train_loss = train_loss\n        if valid_loss < global_best_valid_loss:\n            global_best_valid_loss = valid_loss\n            print(f'after epoch {epoch_n}, global_step_num: {global_step_num} get new best_valid_loss: {valid_loss:.5f}, save the model now!')\n            torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, f'best_model_fold{fold_num}.pth'))\n            \n    return global_best_valid_loss\n        \nprint('ok')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-25T08:14:18.93662Z","iopub.execute_input":"2021-06-25T08:14:18.936986Z","iopub.status.idle":"2021-06-25T08:14:18.960349Z","shell.execute_reply.started":"2021-06-25T08:14:18.936948Z","shell.execute_reply":"2021-06-25T08:14:18.959486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCH_NUM = 3\nglobal_step_num = 0\nglobal_best_train_loss = 1e9\nglobal_best_valid_loss = 1e9\n\nvalid_rmse_lst = []\nkfold = StratifiedKFold(cfg.CV_fold_num, shuffle=True, random_state=cfg.random_seed)\nfor fold_num, (train_index, valid_index) in enumerate(kfold.split(data, bins)):\n# kfold = KFold(cfg.CV_fold_num, shuffle=True, random_state=cfg.random_seed)\n# for fold_num, (train_index, valid_index) in enumerate(kfold.split(data)):\n    train_data, valid_data = data.iloc[train_index], data.iloc[valid_index]\n    print('****'*20)\n    print(f'run fold_num: {fold_num} now! train_data.shape: {train_data.shape} valid_data.shape: {valid_data.shape}')\n    print('****'*20)\n    dl_train, dl_valid = get_dataloader(train_data, valid_data)\n    valid_rmse = run_whole_train(fold_num, dl_train, dl_valid)\n    print(f'run fold_num: {fold_num} finished! get valid_rmse: {valid_rmse:7f}')\n    valid_rmse_lst.append(valid_rmse)\n    #break\n    \nprint('ok, final valid_rmse_lst:', valid_rmse_lst, 'avg rmse: ', np.mean(valid_rmse_lst))\n# ok, final valid_rmse_lst: [0.5544281, 0.6582969, 0.5821244, 0.6942738, 0.70970976] avg rmse:  0.6397666\n# ok, final valid_rmse_lst: [0.64655215, 0.6353341, 0.6530859, 0.64252084, 0.6323978] avg rmse:  0.64197814\n# ok, final valid_rmse_lst: [0.6535841, 0.6094322, 0.77323335, 0.6097351, 0.5807594] avg rmse:  0.64534885","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-25T08:14:18.961552Z","iopub.execute_input":"2021-06-25T08:14:18.962008Z","iopub.status.idle":"2021-06-25T08:19:48.158994Z","shell.execute_reply.started":"2021-06-25T08:14:18.961974Z","shell.execute_reply":"2021-06-25T08:19:48.157559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# learning_rate = 2.5e-5      weight_decay = 1e-1  torch.optim.AdamW\n# ********************************************************************************\n# run fold_num: 0 now! train_data.shape: (2267, 3) valid_data.shape: (567, 3)\n# ********************************************************************************\n# len of ds_train:  2267 len of dl_train:  284\n# len of ds_valid:  567 len of dl_valid:  36\n# epoch_n:  0\n# mid eval, global_step_num: 50, valid_loss: 0.9053377\n# get new valid_loss:  0.9053377, saving model now!\n# mid eval, global_step_num: 100, valid_loss: 0.6626867\n# get new valid_loss:  0.6626867, saving model now!\n# mid eval, global_step_num: 150, valid_loss: 0.6542238\n# get new valid_loss:  0.6542238, saving model now!\n# mid eval, global_step_num: 200, valid_loss: 0.5725907\n# get new valid_loss:  0.5725907, saving model now!\n# mid eval, global_step_num: 250, valid_loss: 0.5422387\n# get new valid_loss:  0.5422387, saving model now!\n# epoch_n:  1\n# mid eval, global_step_num: 300, valid_loss: 0.5664148\n# mid eval, global_step_num: 350, valid_loss: 0.5028599\n# get new valid_loss:  0.5028599, saving model now!\n# mid eval, global_step_num: 400, valid_loss: 0.5335220\n# mid eval, global_step_num: 450, valid_loss: 0.5373504\n# mid eval, global_step_num: 500, valid_loss: 0.4964960\n# get new valid_loss:  0.4964960, saving model now!\n# mid eval, global_step_num: 550, valid_loss: 0.4986220\n# after epoch 1, global_step_num: 568 get new best_valid_loss: 0.49409, save the model now!\n# epoch_n:  2\n# mid eval, global_step_num: 600, valid_loss: 0.5517622\n# mid eval, global_step_num: 650, valid_loss: 0.4894424\n# get new valid_loss:  0.4894424, saving model now!\n# mid eval, global_step_num: 700, valid_loss: 0.4783250\n# get new valid_loss:  0.4783250, saving model now!\n# mid eval, global_step_num: 750, valid_loss: 0.4814345\n# mid eval, global_step_num: 800, valid_loss: 0.4791949\n# mid eval, global_step_num: 850, valid_loss: 0.4801228\n# run fold_num: 0 finished! get valid_rmse: 0.478325\n# ok, final valid_rmse_lst: [0.47832498] avg rmse:  0.47832498","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:19:48.168274Z","iopub.execute_input":"2021-06-25T08:19:48.168659Z","iopub.status.idle":"2021-06-25T08:19:48.180231Z","shell.execute_reply.started":"2021-06-25T08:19:48.168623Z","shell.execute_reply":"2021-06-25T08:19:48.179445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ********************************************************************************\n# run fold_num: 0 now! train_data.shape: (2267, 3) valid_data.shape: (567, 3)\n# ********************************************************************************\n# len of ds_train:  2267 len of dl_train:  284\n# len of ds_valid:  567 len of dl_valid:  36\n# epoch_n:  0\n# mid eval, global_step_num: 50, valid_loss: 0.9642584\n# get new valid_loss:  0.9642584, saving model now!\n# mid eval, global_step_num: 100, valid_loss: 0.6496056\n# get new valid_loss:  0.6496056, saving model now!\n# mid eval, global_step_num: 150, valid_loss: 0.6068502\n# get new valid_loss:  0.6068502, saving model now!\n# mid eval, global_step_num: 200, valid_loss: 0.6726950\n# mid eval, global_step_num: 250, valid_loss: 0.5651695\n# get new valid_loss:  0.5651695, saving model now!\n# epoch_n:  1\n# mid eval, global_step_num: 300, valid_loss: 0.5648718\n# get new valid_loss:  0.5648718, saving model now!\n# mid eval, global_step_num: 350, valid_loss: 0.5158271\n# get new valid_loss:  0.5158271, saving model now!\n# mid eval, global_step_num: 400, valid_loss: 0.5241506\n# mid eval, global_step_num: 450, valid_loss: 0.5661162\n# mid eval, global_step_num: 500, valid_loss: 0.5110499\n# get new valid_loss:  0.5110499, saving model now!\n# mid eval, global_step_num: 550, valid_loss: 0.5000905\n# get new valid_loss:  0.5000905, saving model now!\n# epoch_n:  2\n# mid eval, global_step_num: 600, valid_loss: 0.6079005\n# mid eval, global_step_num: 650, valid_loss: 0.4984870\n# get new valid_loss:  0.4984870, saving model now!\n# mid eval, global_step_num: 700, valid_loss: 0.4881012\n# get new valid_loss:  0.4881012, saving model now!\n# mid eval, global_step_num: 750, valid_loss: 0.4873407\n# get new valid_loss:  0.4873407, saving model now!\n# mid eval, global_step_num: 800, valid_loss: 0.4882061\n# mid eval, global_step_num: 850, valid_loss: 0.4905744\n# run fold_num: 0 finished! get valid_rmse: 0.487341\n# ok, final valid_rmse_lst: [0.48734075] avg rmse:  0.48734075","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:19:48.181464Z","iopub.execute_input":"2021-06-25T08:19:48.181802Z","iopub.status.idle":"2021-06-25T08:19:48.192299Z","shell.execute_reply.started":"2021-06-25T08:19:48.181759Z","shell.execute_reply":"2021-06-25T08:19:48.19136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ********************************************************************************\n# run fold_num: 0 now! train_data.shape: (2267, 3) valid_data.shape: (567, 3)\n# ********************************************************************************\n# len of ds_train:  2267 len of dl_train:  284\n# len of ds_valid:  567 len of dl_valid:  36\n# epoch_n:  0\n# mid eval, global_step_num: 50, valid_loss: 0.9642584\n# get new valid_loss:  0.9642584, saving model now!\n# mid eval, global_step_num: 100, valid_loss: 0.7282062\n# get new valid_loss:  0.7282062, saving model now!\n# mid eval, global_step_num: 150, valid_loss: 0.6355429\n# get new valid_loss:  0.6355429, saving model now!\n# mid eval, global_step_num: 200, valid_loss: 0.6707624\n# mid eval, global_step_num: 250, valid_loss: 0.5698632\n# get new valid_loss:  0.5698632, saving model now!\n# epoch_n:  1\n# mid eval, global_step_num: 300, valid_loss: 0.5696163\n# get new valid_loss:  0.5696163, saving model now!\n# mid eval, global_step_num: 350, valid_loss: 0.5831050\n# mid eval, global_step_num: 400, valid_loss: 0.5705826\n# mid eval, global_step_num: 450, valid_loss: 0.5149580\n# get new valid_loss:  0.5149580, saving model now!\n# mid eval, global_step_num: 500, valid_loss: 0.6398769\n# mid eval, global_step_num: 550, valid_loss: 0.6121566\n# after epoch 1, global_step_num: 568 get new best_valid_loss: 0.50032, save the model now!\n# epoch_n:  2\n# mid eval, global_step_num: 600, valid_loss: 0.5818124\n# mid eval, global_step_num: 650, valid_loss: 0.5793452\n# mid eval, global_step_num: 700, valid_loss: 0.5918077\n# mid eval, global_step_num: 750, valid_loss: 0.5861278\n# mid eval, global_step_num: 800, valid_loss: 0.5341970\n# mid eval, global_step_num: 850, valid_loss: 0.5541372\n# run fold_num: 0 finished! get valid_rmse: 0.500319\n# ok, final valid_rmse_lst: [0.5003194] avg rmse:  0.5003194","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:19:48.1936Z","iopub.execute_input":"2021-06-25T08:19:48.193933Z","iopub.status.idle":"2021-06-25T08:19:48.205422Z","shell.execute_reply.started":"2021-06-25T08:19:48.193899Z","shell.execute_reply":"2021-06-25T08:19:48.204542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CLRP_model(cfg.model_name_or_path)\nmodel.to(cfg.device)\ncriterion = nn.MSELoss()\n\nmodel.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, 'best_model_fold0.pth')))\nvalid_loss = evaluate(dl_valid, model, criterion)\n\nprint(f'final best model valid_loss: {valid_loss:.7f}, global_best_valid_loss: {global_best_valid_loss:.7f}')\nprint('finished, total cost time: ', time.time()-global_start_t)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:19:48.20661Z","iopub.execute_input":"2021-06-25T08:19:48.207096Z","iopub.status.idle":"2021-06-25T08:19:56.53317Z","shell.execute_reply.started":"2021-06-25T08:19:48.207055Z","shell.execute_reply":"2021-06-25T08:19:56.531153Z"},"trusted":true},"execution_count":null,"outputs":[]}]}