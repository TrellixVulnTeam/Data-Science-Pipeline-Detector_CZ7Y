{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 参考： https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n\nimport sys\nimport os\nimport time\nimport random\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import BertTokenizer, BertModel, BertConfig, AdamW\nfrom transformers import (AutoModel, AutoModelForMaskedLM, AutoTokenizer, \n    LineByLineTextDataset, DataCollatorForLanguageModeling,\n    Trainer, TrainingArguments, get_linear_schedule_with_warmup)\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nclass CFG:\n    model_name_or_path = '../input/roberta-transformers-pytorch/roberta-base'\n    #model_name_or_path = '../input/huggingface-bert-variants/bert-base-cased/bert-base-cased/'\n    clip_by_train_range = False\n    batch_size = 8\n    max_seq_length = 512\n    seq_length = 100\n    learning_rate = 2e-5\n    use_lr_scheduler = True\n    mid_eval = True\n    mid_eval_step_num = 50\n    random_seed = 2021\n    model_output_dir = './'\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \ncfg = CFG()\nseed_everything(cfg.random_seed)\n\nglobal_start_t = time.time()\nprint('ok')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-25T08:58:51.575266Z","iopub.execute_input":"2021-06-25T08:58:51.575635Z","iopub.status.idle":"2021-06-25T08:58:59.027827Z","shell.execute_reply.started":"2021-06-25T08:58:51.575556Z","shell.execute_reply":"2021-06-25T08:58:59.026254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntrain_target_min, train_target_max = train_data['target'].min(), train_data['target'].max()\n\nprint(f'train_data.shape: {train_data.shape}, test_data.shape: {test_data.shape}')\ntrain_target_min, train_target_max\n# (-3.676267773, 1.7113898269999999)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:58:59.029891Z","iopub.execute_input":"2021-06-25T08:58:59.030252Z","iopub.status.idle":"2021-06-25T08:58:59.135076Z","shell.execute_reply.started":"2021-06-25T08:58:59.030217Z","shell.execute_reply":"2021-06-25T08:58:59.134309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:58:59.136863Z","iopub.execute_input":"2021-06-25T08:58:59.137224Z","iopub.status.idle":"2021-06-25T08:58:59.154275Z","shell.execute_reply.started":"2021-06-25T08:58:59.137187Z","shell.execute_reply":"2021-06-25T08:58:59.153225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRP_Dataset(Dataset):\n    def __init__(self, data, tokenizer, max_seq_length, train=True):\n        self.df = data\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n        self.train = train\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index):\n        excerpt = self.df.iloc[index]['excerpt']\n        d_encode = self.tokenizer(excerpt,\n                                  padding=\"max_length\",\n                                  max_length=self.max_seq_length,\n                                  truncation=True)\n        return {\"input_ids\": d_encode['input_ids'],\n                \"attention_mask\": d_encode['attention_mask'],\n                \"length\" : sum(d_encode['attention_mask'])}\n\n    \ndef collate_fn_test(batch):\n    max_len = max([x['length'] for x in batch])\n    input_ids = torch.tensor([x['input_ids'][:max_len] for x in batch])\n    attention_mask = torch.tensor([x['attention_mask'][:max_len] for x in batch])\n    #token_type_ids = torch.tensor([x['token_type_ids'][:max_len] for x in batch])\n    \n    if 'token_type_ids' in batch[0]:\n        token_type_ids = torch.tensor([x['token_type_ids'][:max_len] for x in batch])\n        return {\"all_input_ids\": input_ids,\n                \"all_attention_mask\": attention_mask,\n                \"all_token_type_ids\": token_type_ids}\n    else:\n        return {\"all_input_ids\": input_ids,\n                \"all_attention_mask\": attention_mask}\n    \ntokenizer = AutoTokenizer.from_pretrained(cfg.model_name_or_path)\ncfg.tokenizer = tokenizer\n\nds_test = CLRP_Dataset(test_data, tokenizer, cfg.max_seq_length, train=False)\ndl_test = DataLoader(ds_test, batch_size=2*cfg.batch_size, shuffle=False, collate_fn=collate_fn_test, num_workers=0)\nprint('len of ds_test: ', len(ds_test), 'len of dl_test: ', len(dl_test))\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:58:59.15614Z","iopub.execute_input":"2021-06-25T08:58:59.156504Z","iopub.status.idle":"2021-06-25T08:58:59.368869Z","shell.execute_reply.started":"2021-06-25T08:58:59.156471Z","shell.execute_reply":"2021-06-25T08:58:59.367841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRP_model_V1(nn.Module):\n    def __init__(self, pretrained_model_path, embedding_dim=256):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(pretrained_model_path)\n        use_large_model = True if 'large' in pretrained_model_path else False\n        self.drop_out = nn.Dropout(0.1)\n        if use_large_model:\n            self.fc1 = nn.Linear(1024*2, embedding_dim)\n        else:\n            self.fc1 = nn.Linear(768*2, embedding_dim)\n        self.activation1 = nn.ReLU()\n        self.fc2 = nn.Linear(embedding_dim, 1)\n\n    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None):\n        output = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, return_dict=True)\n        assert len(output)==2\n        \n        last_hidden_state, pooled_output = output['last_hidden_state'], output['pooler_output']\n        last_hidden_state = self.drop_out(last_hidden_state)\n        seq_avg = torch.mean(last_hidden_state, dim=1)\n        seq_max = torch.max(last_hidden_state, dim=1)[0]\n        concat_out = torch.cat((seq_avg, seq_max), dim=1)\n        preds = self.fc2(self.activation1(self.fc1(concat_out)))        \n        preds = preds.squeeze(-1).squeeze(-1)  ### 这一行非常非常关键！！！\n        \n        return preds\n    \nclass CLRP_model_V2(nn.Module):\n    def __init__(self, pretrained_model_path, embedding_dim=256):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(pretrained_model_path)\n        use_large_model = True if 'large' in pretrained_model_path else False\n        self.drop_out = nn.Dropout(0.1)\n        if use_large_model:\n            self.fc1 = nn.Linear(1024*1, 1)\n        else:\n            self.fc1 = nn.Linear(768*1, 1)\n        \n\n    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None):\n        output = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, return_dict=True)\n        assert len(output)==2\n        \n        pooled_output = output['pooler_output']\n        pooled_output = self.drop_out(pooled_output)\n        preds = self.fc1(pooled_output)\n        preds = preds.squeeze(-1).squeeze(-1)  ### 这一行非常非常关键！！！\n        \n        return preds\n    \nCLRP_model = CLRP_model_V2\n    \nmodel = CLRP_model(cfg.model_name_or_path)\nmodel.to(cfg.device)\n\nmodel_path = '../input/clrp-pytorch-roberta-base-my-first-model-train/best_model.pth'\nmodel.load_state_dict(torch.load(model_path))\n\nmodel_param_num = sum(p.numel() for p in model.parameters())\nmodel_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('model_param_num: ', model_param_num, 'model_trainable_param_num: ', \n      model_trainable_param_num)\n            \nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:58:59.370238Z","iopub.execute_input":"2021-06-25T08:58:59.370566Z","iopub.status.idle":"2021-06-25T08:59:18.806764Z","shell.execute_reply.started":"2021-06-25T08:58:59.370529Z","shell.execute_reply":"2021-06-25T08:59:18.805958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(eval_iter, model):\n    global cfg\n    predictions_lst = []\n    \n    model.eval()\n    for step, batch in enumerate(eval_iter):\n        for key in batch.keys():\n            batch[key] = batch[key].to(cfg.device)\n        with torch.no_grad():\n            predictions = model(\n                input_ids=batch['all_input_ids'],\n                attention_mask=batch['all_attention_mask'])\n        predictions_lst += list(predictions.cpu().numpy().ravel())\n\n    print('len of predictions_lst: ', len(predictions_lst))\n    return predictions_lst\n\n# predictions = evaluate(dl_test, model)\n# predictions = np.array(predictions)\n# print('before predictions.shape is ', predictions.shape, 'target min: ', \n#       predictions.min(), 'target max: ', predictions.max())\n\n# if cfg.clip_by_train_range:\n#     predictions = np.clip(predictions, train_target_min, train_target_max)\n# print('after predictions.shape is ', predictions.shape, 'target min: ', \n#       predictions.min(), 'target max: ', predictions.max())\n\n# submission_df = test_data[['id']]\n# submission_df['target'] = predictions\n# submission_df.to_csv('./submission.csv', index=False)\n\n# print('total cost time: ', time.time() - global_start_t)\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:59:18.808067Z","iopub.execute_input":"2021-06-25T08:59:18.80842Z","iopub.status.idle":"2021-06-25T08:59:18.816903Z","shell.execute_reply.started":"2021-06-25T08:59:18.808384Z","shell.execute_reply":"2021-06-25T08:59:18.816033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '../input/clrp-roberta-base-my-first-model-5fold-cv/'\npredictions_lst = []\nCV_FOLD_NUM = 5\nfor fold_num in range(CV_FOLD_NUM):\n    model = CLRP_model(cfg.model_name_or_path)\n    model.to(cfg.device)\n    model.load_state_dict(torch.load(model_path + f'best_model_fold{fold_num}.pth'))\n    predictions = evaluate(dl_test, model)\n    predictions = np.array(predictions)\n    print('fold_num: ', fold_num, 'predictions.shape: ', predictions.shape)\n    predictions_lst.append(predictions)\n    del model\n    \npredictions_array = np.array(predictions_lst)\npredictions_avg = np.mean(predictions_array, axis=0)\nprint(f'predictions_array.shape: {predictions_array.shape}, predictions_avg.shape: {predictions_avg.shape}')\n\nsubmission_df = test_data[['id']]\nsubmission_df['target'] = predictions_avg\nsubmission_df.to_csv('./submission.csv', index=False)\n\nprint('total cost time: ', time.time() - global_start_t)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:59:18.818458Z","iopub.execute_input":"2021-06-25T08:59:18.819115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}