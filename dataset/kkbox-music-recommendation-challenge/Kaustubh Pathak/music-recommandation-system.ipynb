{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport datetime\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(pd.read_csv('../input/train.csv'))\ntest = reduce_mem_usage(pd.read_csv('../input/test.csv'))\nsei = pd.read_csv('../input/song_extra_info.csv')\nmembers = pd.read_csv('../input/members.csv',parse_dates=['registration_init_time','expiration_date'])\nsongs = pd.read_csv('../input/songs.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of train is ->',train.shape)\nprint('Shape of test is ->',test.shape)\nprint('Shape of Song Extra Info is ->',sei.shape)\nprint('Shape of Members is ->',members.shape)\nprint('Shape of Songs is ->',songs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_codes(isrc):\n#     if isrc!= isrc:\n#         return np.nan \n#     else:\n#         return [str(isrc)[0:2] , str(isrc)[2:5]  , str(isrc)[5:7] , str(isrc)[7:]]\n# sei_null = sei[sei['isrc'].isnull()]\n# sei = sei.dropna(subset=['isrc'])\n# sei['first_code'] = sei['code'].apply(lambda x: x[0])\n# sei['forth_code'] = sei['code'].apply(lambda x: x[3])\n# sei['second_code'] = sei['code'].apply(lambda x: x[1])\n# sei['third_code'] = sei['code'].apply(lambda x: x[2])\n# sei_null['first_code'] = np.nan \n# sei_null['second_code'] = np.nan \n# sei_null['third_code'] = np.nan \n# sei_null['forth_code'] = np.nan \n# sei = pd.concat([sei_null  ,sei] , axis = 0)\n# sei.drop(columns = ['code' , 'isrc','name'] , inplace = True)\n# sei = sei.sample(frac = 1  , random_state = 98)\n# del sei_null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_codes(isrc):\n    if pd.isnull(isrc):\n        return np.nan\n    else:\n        if int(str(isrc)[5:7]) > 17:\n            temp =  1900+int(str(isrc)[5:7])\n        else:\n            temp = 2000+int(isrc[5:7])\n        return temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sei['year'] = sei['isrc'].apply(lambda x: get_codes(x))\nsei.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"members['membership_days'] = members['expiration_date'].subtract(members['registration_init_time']).dt.days.astype(int)\nmembers['registration_year'] = members['registration_init_time'].dt.year\nmembers['expiration_year'] = members['expiration_date'].dt.year\nmembers.drop(columns = ['registration_init_time' , 'expiration_date'] , inplace = True)\nmembers.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extending columns\n# merging the database\ntrain = train.merge(songs , on='song_id' , how='left')\ntrain = train.merge(members , on = 'msno' , how='left')\ntrain = train.merge(sei , on = 'song_id' , how='left')\ntest  = test.merge(songs , on='song_id' , how='left')\ntest = test.merge(members , on = 'msno' , how = 'left')\ntest =  test.merge(sei , on = 'song_id' , how = 'left')\ndel sei ,members , songs\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['song_length'].isnull().value_counts()/train.shape[0])\ntrain['song_length'].fillna(train['song_length'].mean() , inplace = True)\ntrain['song_length'] = train['song_length'].astype(np.uint32)\nprint(train['language'].isnull().value_counts()/train.shape[0])\ntrain['language'].fillna(train['language'].mode().values[0] , inplace= True)\ntrain['language'] = train['language'].astype(np.int8)\ntest['song_length'].fillna(test['song_length'].mean() , inplace = True)\ntest['song_length'] = test['song_length'].astype(np.uint32)\ntest['language'].fillna(test['language'].mode().values[0] , inplace= True)\ntest['language'] = test['language'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def genre_count(genre):\n    if genre == 'no_genre_id':\n        return 0\n    else :\n        return genre.count('|') + 1\nprint(train['genre_ids'].isnull().value_counts()/train.shape[0])\ntrain['genre_ids'].fillna('no_genre_id' , inplace= True)\ntrain['genre_ids_count'] = train['genre_ids'].apply(lambda x: genre_count(x)).astype(np.int8)\ntest['genre_ids'].fillna('no_genre_id' , inplace= True)\ntest['genre_ids_count'] = test['genre_ids'].apply(lambda x: genre_count(x)).astype(np.int8)\n                                                       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef artist_count(art):\n    if art=='no_artist_name':\n        return 0\n    else:\n        return art.count('|')+art.count('/') + art.count('//') + art.count(';') + 1\ntrain['artist_name'].isnull().value_counts()\ntrain['artist_name'].fillna('no_artist_name' , inplace = True)\ntrain['artist_count'] = train['artist_name'].apply(lambda x : artist_count(x)).astype(np.int8)\ntest['artist_name'].fillna('no_artist_name' , inplace = True)\ntest['artist_count'] = test['artist_name'].apply(lambda x : artist_count(x)).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def  count_composer(comp):\n    if comp=='no_composer':\n        return 0\n    else:\n        return comp.count('|')+comp.count('/') + comp.count('//') + comp.count(';') + 1\ndef  count_lyricist(lyr):\n    if lyr=='no_lyricist':\n        return 0\n    else:\n        return lyr.count('|')+lyr.count('/') + lyr.count('//') + lyr.count(';') + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['composer'].fillna('no_composer',inplace=True)\ntrain['composer_count'] = train['composer'].apply(lambda x: count_composer(x)).astype(np.int8)\ntrain['lyricist'].fillna('no_lyricist',inplace=True)\ntrain['lyricist_count'] = train['lyricist'].apply(lambda x: count_lyricist(x)).astype(np.int8)\ntest['composer'].fillna('no_composer',inplace=True)\ntest['composer_count'] = test['composer'].apply(lambda x: count_composer(x)).astype(np.int8)\ntest['lyricist'].fillna('no_lyricist',inplace=True)\ntest['lyricist_count'] = test['lyricist'].apply(lambda x: count_lyricist(x)).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_count_song_played_train = {k: v for k, v in train['song_id'].value_counts().iteritems()}\ndict_count_song_played_test = {k: v for k, v in test['song_id'].value_counts().iteritems()}\ndef return_number_played(x):\n    try:\n        return dict_count_song_played_train[x]\n    except KeyError:\n        try:\n            return dict_count_song_played_test[x]\n        except KeyError:\n            return 0\ntrain['number_of_time_played'] = train['song_id'].apply(lambda x: return_number_played(x))\ntest['number_of_time_played'] = test['song_id'].apply(lambda x: return_number_played(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_user_activity = {k:v for k,v in pd.concat([train['msno'] , test['msno']] , axis = 0).value_counts().iteritems()}\ndef return_user_activity(x):\n    try:\n        return dict_user_activity[x]\n    except KeyError:\n        return 0\ntrain['user_activity_msno'] = train['msno'].apply(lambda x: return_user_activity(x))\ntest['user_activity_msno'] = test['msno'].apply(lambda x: return_user_activity(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# f,ax = plt.subplots(figsize=(15, 15))\n# sns.countplot(x='artist_count' ,hue= 'target'  , data = train)\n# plt.xticks(rotation=90)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_col = list(train.columns)\ntest_col = list(test.columns)\nfor f in test_col :\n    if f not in train_col:\n        print('ERROR !!!  Column from Test not found in train is ->' , f)\nlabel_encoding = ['source_system_tab', 'source_screen_name',\n       'source_type','gender']\ndrop = ['msno', 'song_id' , 'isrc','artist_name',\n       'composer', 'lyricist','name','genre_ids']\nmin_max_scaling = ['number_of_time_played', 'user_activity_msno','membership_days', 'song_length']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in label_encoding:\n    lb = LabelEncoder()\n    lb.fit(list(train[f].values) + list(test[f].values))\n    train[f] = lb.transform(list(train[f].values))\n    test[f] = lb.transform(list(test[f].values))\nfor f in min_max_scaling:\n    ms = MinMaxScaler()\n    train[f] = ms.fit_transform(train[[f]])\n    test[f] = ms.transform(test[[f]])\n# train.drop(columns = drop , inplace = True)\n# test.drop(columns=drop , inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    if train[col].dtype == object:\n        train[col] = train[col].astype('category')\n        test[col] = test[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(columns = ['target'] , axis = 1)\nY_train = train['target'].values\nX_test = test.drop(columns = ['id'] , axis = 1)\nids = test['id'].values\ndel train , test\ngc.collect()\ntrain_set = lgb.Dataset(X_train , Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting': 'gbdt',\n        'learning_rate': 0.3 ,\n        'verbose': 0,\n        'num_leaves': 108,\n        'bagging_fraction': 0.95,\n        'bagging_freq': 1,\n        'bagging_seed': 1,\n        'feature_fraction': 0.9,\n        'feature_fraction_seed': 1,\n        'max_bin': 256,\n        'max_depth': 10,\n        'num_rounds': 200,\n        'metric' : 'auc'\n    }\n\n%time model_f1 = lgb.train(params, train_set=train_set,  valid_sets=train_set, verbose_eval=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test = model_f1.predict(X_test)\nprint('Saving Predictions')\nsub = pd.DataFrame()\nsub['id'] = ids\nsub['target'] = pred_test\nsub.to_csv('1st_submission.csv' , index = False , float_format ='%.5f' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}