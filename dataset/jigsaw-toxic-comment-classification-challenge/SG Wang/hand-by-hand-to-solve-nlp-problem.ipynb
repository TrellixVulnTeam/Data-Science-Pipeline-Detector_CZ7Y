{"cells":[{"metadata":{"_cell_guid":"921dedd6-c1a8-44eb-a627-18a34bc1ad8c","_uuid":"61de36363d61dc80b13faa85e6cf1711673ee1c0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b999d1de-89a5-46c0-96e9-3d3c6b260bd0","_uuid":"b63e912e5d07887e9eb959414f3c7702d1d60cda","collapsed":true,"trusted":true},"cell_type":"code","source":"import re, os, time\nimport nltk\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b7145a7d-eccb-46e0-8b98-ea21885abea8","_uuid":"bb3f80e554983bfb7c00296b2c1b015d8e022168"},"cell_type":"markdown","source":"## 1.清理数据\n\n机器学习工作中广为流传的一句话：“数据决定机器学习的上限，算法让我们不断逼近这个上限”。\n\n一个干净的数据集是我们在运用机器学习算法取得成功的关键，因此，对文本进行合适的处理是非常关键的一步。","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2d4d957c-5d5d-4668-9924-11e0518b2969","_uuid":"2ca145b1731c3ee915cb1c9f66909e9389272424","collapsed":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsample_submission = pd.read_csv('../input/sample_submission.csv')\n\nlabels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cdba71e9-2509-4c14-8d91-e09aa263249a","_uuid":"7ac47579ee48a3dffe6d81b5b88d00dddbc33fb3"},"cell_type":"markdown","source":"以下是我在清洗文本过程中主要完成的工作：\n\n1. 把你的文章分成一个个单独的单词。 \n2. 将所有字符转换为小写。\n3. 删除所有不相关的字符，例如任何非字母、数字字符。\n4. 恢复所有简写形式的单词\n5. 考虑将“@$&”等字符转换为“at，dollar，and”。\n6. 最后，有很多单词是拼写错误的，这个部分还需要想办法来处理。\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a8a5304c-9534-4502-9248-40c55d08a01e","_uuid":"497bd25a1279401cea15673a5cc8ec500a289391","collapsed":true,"trusted":true},"cell_type":"code","source":"def clean_text(comment_text):\n    comment_list = []\n    for text in comment_text:\n        # 将单词转换为小写\n        text = text.lower()\n        # 删除非字母、数字字符\n        text = re.sub(r\"[^A-Za-z0-9(),!?@&$\\'\\`\\\"\\_\\n]\", \" \", text)\n        text = re.sub(r\"\\n\", \" \", text)\n        \n        # 恢复常见的简写\n        text = re.sub(r\"what's\", \"what is \", text)\n        text = re.sub(r\"\\'s\", \" \", text)\n        text = re.sub(r\"\\'ve\", \" have \", text)\n        text = re.sub(r\"can't\", \"can not \", text)\n        text = re.sub(r\"cannot\", \"can not \", text)\n        text = re.sub(r\"n't\", \" not \", text)\n        text = re.sub(r\"i'm\", \"i am \", text)\n        text = re.sub(r\"\\'re\", \" are \", text)\n        text = re.sub(r\"\\'d\", \" would \", text)\n        text = re.sub(r\"\\'ll\", \" will \", text)\n        \n        # 恢复特殊符号的英文单词\n        text = text.replace('&', ' and')\n        text = text.replace('@', ' at')\n        text = text.replace('$', ' dollar')\n        \n        comment_list.append(text)\n    return comment_list\n\ntrain[\"clean_comment_text\"] = clean_text(train['comment_text'])\ntest['clean_comment_text'] = clean_text(test['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"22ec757d-e745-44ec-a3e8-011457095b26","_uuid":"4db5cefe716d347a6695511ec97c91d58abac267","trusted":true},"cell_type":"code","source":"train[['comment_text','clean_comment_text']].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"51379078-ccce-4db2-bc2a-db258c5749a2","_uuid":"d0280d92ed143f9df5e76faafcec84a0f5d387b7"},"cell_type":"markdown","source":"## 2.提取TF-IDF特征\n\n自然语言处理的一个难点问题就是如何表示文本，机器学习模型都是以数值为输入，所以我们需要找到一种很好的表达方式让我们的算法能够理解文本数据。\n\n### TF-IDF\n为了帮助我们的模型更多地关注有意义的单词，我们可以使用TF-IDF进行特征提取。","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5b970e75-786e-4690-b170-b34d67ae4f2c","_uuid":"2551ea809ba557d32c070aacc63d63a2096b6166","trusted":true},"cell_type":"code","source":"all_comment_list = list(train['clean_comment_text']) + list(test['clean_comment_text'])\ntext_vector = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode',token_pattern=r'\\w{1,}',\n                         max_features=5000, ngram_range=(1, 1), analyzer='word')\ntext_vector.fit(all_comment_list)\ntrain_vec = text_vector.transform(train['clean_comment_text'])\ntest_vec = text_vector.transform(test['clean_comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"068c4042-7ab3-4857-95e6-9f6250742bf5","_uuid":"c347783cbca9496c9765194a2a15f23c90366936","trusted":true},"cell_type":"code","source":"train_vec","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bea202e0-0ed3-4406-be2b-d427cfa149f4","_uuid":"a25292f465ef97d227e415a805c50854da0e1222"},"cell_type":"markdown","source":"## 3.训练模型\n\n我们使用一个非常简单的Logistic回归模型来进行分类。","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a3b1bec4-7a62-4547-a462-b10e256d5178","_uuid":"b859dae5f651fa5d0049b42df38fa2c036f62159","collapsed":true,"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(train_vec, train[labels], test_size=0.1, random_state=2018)\nx_test = test_vec","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f9095fd0-febe-4136-8889-ec8e7d902588","_uuid":"b0fb88da928b3e5f1381a1c13bcc930f93cbd2f6","trusted":true},"cell_type":"code","source":"accuracy = []\nfor label in labels:\n    clf = LogisticRegression(C=6)\n    clf.fit(x_train, y_train[label])\n    y_pre = clf.predict(x_valid)\n    train_scores = clf.score(x_train, y_train[label])\n    valid_scores = accuracy_score(y_pre, y_valid[label])\n    print(\"{} train score is {}, valid score is {}\".format(label, train_scores, valid_scores))\n    accuracy.append(valid_scores)\n    pred_proba = clf.predict_proba(x_test)[:, 1]\n    sample_submission[label] = pred_proba\nprint(\"Total cv accuracy is {}\".format(np.mean(accuracy)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"30528c87-feb4-4ddc-89e8-e8d4f962b256","_uuid":"368f939a7d06eb7365ff0fd495ddbbc367b0f1a7","trusted":true},"cell_type":"code","source":"from datetime import datetime\n\ndef submission(submission):\n    file_name = '{}.csv'.format(datetime.now().strftime(\"%Y-%m-%d_%H-%M\"))\n    submission.to_csv(file_name, index=False)\n    print(\"write to {}\".format(file_name))\nsubmission(sample_submission)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"43bb47db-4e04-4c4a-b68c-fefe38618a4f","_uuid":"e67c03844b35a83cca24f66068fde72a849a6d9a","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}