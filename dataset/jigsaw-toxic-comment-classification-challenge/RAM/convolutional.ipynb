{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Preparing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# My solution based on the great tutorial:\n# https://github.com/bentrevett/pytorch-sentiment-analysis\n\n%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport torch\nimport torchtext\nfrom torchtext import data\nimport spacy\nimport os\nimport re\n\n\nos.environ['OMP_NUM_THREADS'] = '4'\n\n\nSEED = 1234\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nTEXT = data.Field(lower=True,include_lengths=True ,tokenize='spacy')\n\nLABEL = data.Field(sequential=False, \n                         use_vocab=False, \n                         pad_token=None, \n                            unk_token=None, dtype = torch.float)\n\n\n\n\ndataFields = {\"comment_text\": (\"comment_text\", TEXT), \n              'toxic': (\"toxic\", LABEL), \n              'severe_toxic': (\"severe_toxic\", LABEL),\n              'threat': (\"threat\", LABEL), \n              'obscene': (\"obscene\", LABEL),\n              'insult': (\"insult\", LABEL), \n              'identity_hate': (\"identity_hate\", LABEL)}\n\ndataset= data.TabularDataset(path='../input/toxicjson/train.json', \n                                            format='json',\n                                            fields=dataFields, \n                                            skip_header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nSEED = 3\n#train, unimportant = dataset.split(split_ratio=0.5,random_state = random.seed(SEED)) \n\ntrain_data, val_data = dataset.split(split_ratio=0.5,random_state = random.seed(SEED))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_VOCAB_SIZE = 20_000\n\nTEXT.build_vocab(train_data, \n                 max_size = MAX_VOCAB_SIZE, \n                 vectors = \"glove.6B.100d\", \n                 unk_init = torch.Tensor.normal_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 512\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator = data.BucketIterator.splits(\n    (train_data, val_data), \n    batch_size = BATCH_SIZE,\n    sort_key=lambda x: len(x.comment_text),\n    sort_within_batch = True,\n    device = device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yFields = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\niaux=0\nfor batch in valid_iterator:\n    iaux+=1\n    aux = batch\n    aux2= torch.stack([getattr(batch, y) for y in yFields])\n    if iaux==20: break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n        \ntorch.transpose( torch.stack([getattr(aux, y) for y in yFields]),0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux.comment_text[0].size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux.toxic.size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nfrom torch.functional import F\nclass CNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n                 dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        self.convs = nn.ModuleList([\n                                    nn.Conv2d(in_channels = 1, \n                                              out_channels = n_filters, \n                                              kernel_size = (fs, embedding_dim)) \n                                    for fs in filter_sizes\n                                    ])\n        \n        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text):\n        \n        #text = [sent len, batch size]\n        \n        text = text.permute(1, 0)\n                \n        #text = [batch size, sent len]\n        \n        embedded = self.embedding(text)\n                \n        #embedded = [batch size, sent len, emb dim]\n        \n        embedded = embedded.unsqueeze(1)\n        \n        #embedded = [batch size, 1, sent len, emb dim]\n        \n        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n            \n        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n        \n        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n        \n        #pooled_n = [batch size, n_filters]\n        \n        cat = self.dropout(torch.cat(pooled, dim = 1))\n\n        #cat = [batch size, n_filters * len(filter_sizes)]\n            \n        return self.fc(cat)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nINPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 100\nN_FILTERS = 100\nFILTER_SIZES = [3,3,4]\nOUTPUT_DIM = 6\nDROPOUT = 0.5\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n\nmodel = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_embeddings = TEXT.vocab.vectors\n\nprint(pretrained_embeddings.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.embedding.weight.data.copy_(pretrained_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n\nmodel.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\nmodel.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n\nprint(model.embedding.weight.data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\noptimizer = optim.Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nfrom sklearn.metrics import roc_auc_score\ndef roc_auc(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n    #round predictions to the closest integer\n    #rounded_preds = torch.sigmoid(preds)\n    \n    #assert preds.size()==y.size()\n    \n    #reds=rounded_preds.detach().numpy()\n\n    #y=y.numpy()\n    \n    global var_y\n    global var_preds\n    var_y = y\n    var_preds = preds\n    print('jeje', y.shape)\n    acc = roc_auc_score(y, preds)\n    print('jojo',preds.shape)\n    \n    return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    preds_list=[]\n    labels_list= []\n \n    \n    for i, batch in enumerate(iterator):\n        \n        optimizer.zero_grad()\n        \n        text, text_lengths = batch.comment_text\n        \n        predictions = model(text).squeeze(1)\n        \n        batch_labels=torch.stack([getattr(batch, y) for y in yFields]) #transpose?\n        batch_labels = torch.transpose(batch_labels,0,1)\n        \n        loss = criterion(predictions, batch_labels)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        preds_list+=[torch.sigmoid(predictions).detach().numpy()]\n        labels_list+=[batch_labels.numpy()]\n        \n        #if i%64==0:\n        #    epoch_acc += [roc_auc(np.vstack(preds_list), np.vstack(batch_labels))]\n        #    preds_list=[]\n        #    labels_list= []\n            \n        \n        epoch_loss += loss.item()\n        \n        \n        \n    return epoch_loss / len(iterator), roc_auc(np.vstack(preds_list), np.vstack(labels_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    preds_list=[]\n    labels_list= []\n    epoch_acc=[]\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n\n            text, text_lengths = batch.comment_text\n            \n            predictions = model(text).squeeze(1)\n            \n            batch_labels = torch.stack([getattr(batch, y) for y in yFields]) #transpose?\n            batch_labels = torch.transpose(batch_labels,0,1)\n            \n            loss = criterion(predictions, batch_labels)\n\n            epoch_loss += loss.item()\n            \n            preds_list+=[torch.sigmoid(predictions).detach().numpy()]\n            labels_list+=[batch_labels.numpy()]\n        \n            #if i%64==0:\n            #    epoch_acc += [roc_auc(np.vstack(preds_list), np.vstack(batch_labels))]\n            #    preds_list=[]\n            #    labels_list= []\n        \n    return epoch_loss / len(iterator), roc_auc(np.vstack(preds_list), np.vstack(labels_list))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchsummary import summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nN_EPOCHS = 8\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    print('jaja')\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    print('juju')\n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut2-model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataFields = {\"comment_text\": (\"comment_text\", TEXT)}\n\ntest_dataset= data.TabularDataset(path='./data/test.json', \n                                            format='json',\n                                            fields=test_dataFields, \n                                            skip_header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_iterator = data.BucketIterator.splits(test_dataset, device = device,\n    batch_size = BATCH_SIZE, sort=False, sort_key=lambda x: len(x.comment_text), \n    sort_within_batch=False, repeat=False, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myPreds=[]\nwith torch.no_grad():\n    model.eval()\n    for batch in test_iterator:\n\n        torch.cuda.empty_cache()\n    \n        text, text_lengths = batch.comment_text    \n        predictions = model(text).squeeze(1)         \n        myPreds+=[torch.sigmoid(predictions).detach().numpy()]\n    \n        torch.cuda.empty_cache()\nmyPreds = np.vstack(myPreds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset[0].comment_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for batch in test_iterator:\n    aux3= batch\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataFields = {\"comment_text\": (\"comment_text\", TEXT)}\n\ntestDataset= data.TabularDataset(path='../input/toxicjson/test.json', \n                                            format='json',\n                                            fields=dataFields, \n                                            skip_header=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(testDataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_iterator = torchtext.data.Iterator(testDataset, batch_size=32, device=device, \n                                     sort=False, sort_within_batch=False, \n                                     repeat=False,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myPreds=[]\nwith torch.no_grad():\n    model.eval()\n    for batch in test_iterator:\n\n        torch.cuda.empty_cache()\n    \n        text, text_lengths = batch.comment_text    \n        predictions = model(text).squeeze(1)         \n        myPreds+=[torch.sigmoid(predictions).detach().numpy()]\n    \n        torch.cuda.empty_cache()\nmyPreds = np.vstack(myPreds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDF= pd.read_csv(\"../input/test.csv\")\nfor i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n    testDF[col] = myPreds[:, i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myPreds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDF.drop(\"comment_text\", axis=1).to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}