{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install progressbar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy as sp\n\nfrom sklearn.linear_model import LogisticRegression\n\nimport string\nfrom string import digits \nimport unicodedata\nimport re\nimport nltk\nnltk.download('wordnet')\nnltk.download('punkt')\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction import text\nfrom progressbar import ProgressBar\nimport emoji\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train['comment_text']\ny_train = train[classes]\n\nX_test = test['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(_text):\n  new = []\n  pbar = ProgressBar()\n  for t in pbar(_text.values):\n\n    #demojify\n    t = emoji.demojize(t)\n\n    #remove number\n    t = ''.join([i for i in t if not i.isdigit()])\n\n    #remove stopword\n    stopwords = set(text.ENGLISH_STOP_WORDS)\n    tokens = tokenize(t)\n    text_modified = ''\n    for i in tokens:\n      if not i in stopwords:\n        lemmatizer = WordNetLemmatizer()\n        i = lemmatizer.lemmatize(i,'a')\n        text_modified += i + ' '\n\n    new.append(t)\n  return new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_f = preprocessing(X_train)\nX_test_f = preprocessing(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(ngram_range=(1,2),tokenizer=tokenize,token_pattern = None,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1)\n\nword_vectorizer.fit(X_train_f)\nX_train_word = word_vectorizer.transform(X_train_f)\nX_test_word = word_vectorizer.transform(X_test_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"char_vectorizer = TfidfVectorizer(ngram_range=(2,4),analyzer = 'char',\n               strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1, max_features=50000)\n\nchar_vectorizer.fit(X_train_f)\nX_train_char = char_vectorizer.transform(X_train_f)\nX_test_char = char_vectorizer.transform(X_test_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_char_word = sp.sparse.csr_matrix(sp.sparse.hstack([X_train_char, X_train_word]))\nX_test_char_word = sp.sparse.csr_matrix(sp.sparse.hstack([X_test_char,X_test_word]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = pd.DataFrame({'id':test['id']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = X_train_char_word\ntest_x = X_test_char_word\n\ndef pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _class in classes:\n    y_train_1 = y_train[_class]\n    \n    model = LogisticRegression(max_iter=200000,class_weight='balanced')\n    \n    r = np.log(pr(1,y_train_1.values) / pr(0,y_train_1.values))\n    x_nb = x.multiply(r)\n    model = LogisticRegression(max_iter=200000,class_weight='balanced')\n    print('fitting '+ _class + '...')\n    model.fit(x_nb, y_train_1)\n    pred = model.predict_proba(test_x.multiply(r))\n    subm[_class]=pred[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}