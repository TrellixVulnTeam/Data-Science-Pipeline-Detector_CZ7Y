{"cells":[{"metadata":{},"cell_type":"markdown","source":"*This is my first kernel on Pytorch-Lightning*"},{"metadata":{},"cell_type":"markdown","source":"Please UpVote if you like this kernel..."},{"metadata":{},"cell_type":"markdown","source":"*Import  the required Libraries*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as pe\nfrom wordcloud import WordCloud, STOPWORDS \n\nfrom sklearn import model_selection\n\nfrom transformers import (BertTokenizer,BertModel,AdamW,get_linear_schedule_with_warmup)\n\nimport torch\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader,Dataset\nfrom pytorch_lightning.metrics.functional.classification import auroc\nimport warnings\nfrom pylab import rcParams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*General Configuration parameters*"},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nRANDOM_SEED =42\nsns.set(style='whitegrid',palette='muted',font_scale=1.2)\nHAPPY_COLORS_PALETTE = ['#f0d407','#fbec7e','#04345b','#596e3e','#948304','#2f524f']\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize']=12,8\n\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\nBERT_MODEL_NAME = 'bert-base-cased'\nBATCH_SIZE = 32\nN_EPOCHS = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets check the files present for this competetion*"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PARENT_DIR = '../input/jigsaw-toxic-comment-classification-challenge'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Importing the datasets* "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(PARENT_DIR,'train.csv.zip'))\ntest_df = pd.read_csv(os.path.join(PARENT_DIR,'test.csv.zip'))\ntest_lab_df = pd.read_csv(os.path.join(PARENT_DIR,'test_labels.csv.zip'))\nsample_df = pd.read_csv(os.path.join(PARENT_DIR,'sample_submission.csv.zip'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets take a look into the data*"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_lab_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Below some sample of toxic comment can be seen*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_toxic_words = train_df.head(100)[train_df.head(5000).sum(axis=1)>=3]\nsample_toxic_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets build a wordcloud for a visual representation of toxic words*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordcloud(df):\n    comment_words = ''   \n    stopwords = set(STOPWORDS) \n    # iterate through the csv file \n    for val in df.comment_text: \n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n\n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        comment_words += \" \".join(tokens)+\" \"\n\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white',  \n                stopwords=stopwords,\n                min_font_size = 10).generate(comment_words) \n\n\n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud(sample_toxic_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets check of there is multiple comment present from same ID,s*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also there is exactly one comment from each ID,s\ntrain_df.id.value_counts()[train_df.id.value_counts(ascending=False)>1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Now lets split the data training dataset into training and validation*"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df,val_df = model_selection.train_test_split(train_df,test_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape,val_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets create a list of labels for modelling purpose*"},{"metadata":{"trusted":true},"cell_type":"code","source":"LABEL_COLUMNS = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets check the distribution of different type of toxic comments*"},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_c = pd.DataFrame(train_df[LABEL_COLUMNS].sum()).reset_index()\nlab_c.columns = ['Type','Count']\nfig = pe.bar(lab_c, x='Type', y='Count')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets Handle the imbalence in the dataset*"},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_toxic_clean_mix = pd.DataFrame(train_df[LABEL_COLUMNS].sum()).reset_index()\nlab_toxic_clean_mix.columns = ['Type','Count'] \nlab_toxic_clean_mix.loc[len(lab_toxic_clean_mix.index)] = ['Clean',len(train_df) -lab_toxic_clean_mix.Count.sum()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets plot toxic and clean data count*"},{"metadata":{},"cell_type":"markdown","source":"*In Below plot we can clearly notice that there is a huge count gap between the toxic and regular comments*"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = pe.bar(lab_toxic_clean_mix, x='Type', y='Count')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*I am creating a dataframe with equal number of samples from clean and toxic lables*"},{"metadata":{"trusted":true},"cell_type":"code","source":"toxic_df = train_df[train_df[LABEL_COLUMNS].sum(axis=1)>0]\nclean_df = train_df[train_df[LABEL_COLUMNS].sum(axis=1)==0]\n\ntrain_df = pd.concat([\n    toxic_df,\n    clean_df.sample(15_000)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets Experiment with a single comment*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_row = train_df[train_df.id=='325cd3656d865766']\nsample_row = sample_row.iloc[0]\nsample_comment = sample_row.comment_text\nsample_labels = sample_row[LABEL_COLUMNS]\n\nprint(sample_comment)\nprint()\nprint(sample_labels.to_dict())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Defining the tokenizer*"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding  = tokenizer.encode_plus(sample_comment,\n                     add_special_tokens=True,\n                     max_length=512,\n                     return_token_type_ids=False,\n                     padding = 'max_length',\n                     return_attention_mask=True,\n                     return_tensors='pt'\n                     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding consist of input_ids and attenssion mask\nencoding.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding['input_ids'].shape,encoding['attention_mask'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(encoding['input_ids'].squeeze()[:50])\nprint(encoding['attention_mask'].squeeze()[:50])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.convert_ids_to_tokens(encoding['input_ids'].squeeze()[:50]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Before Strting the dataset creation lets find out the max length from dataframe*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nlength = []\nfor sent in train_df.comment_text:\n    lent = len(word_tokenize(sent))\n    length.append(lent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Here we can notoce that the mean length is very less so i am going to use my max token length as 128*"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Now lets create our dataset for modeling , We can override few functions as per our need to create a custom dataset*"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToxicCommentsDataset(Dataset):\n    def __init__(self,data:pd.DataFrame,tokenizer:BertTokenizer,max_token_len:int=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_token_len = max_token_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self,idx:int):\n        data_row = self.data.iloc[idx]\n        comment_text = data_row.comment_text\n        labels = data_row[LABEL_COLUMNS]\n\n        encoding = self.tokenizer.encode_plus(\n                 comment_text,\n                 add_special_tokens=True,\n                 max_length=self.max_token_len,\n                 return_token_type_ids=False,\n                 padding = 'max_length',\n                 truncation = True,\n                 return_attention_mask=True,\n                 return_tensors='pt'\n                 )\n\n        return dict(\n            comment_text = comment_text,\n            input_ids = encoding['input_ids'].flatten(),\n            attention_mask = encoding['attention_mask'].flatten(),\n            labels = torch.FloatTensor(labels)\n\n            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = ToxicCommentsDataset(train_df,tokenizer)\nsample_row = train_dataset[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets see the size of our return values*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_row['input_ids'].shape,sample_row['attention_mask'].shape,sample_row['labels'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_text = sample_row['comment_text']\ninput_id = sample_row['input_ids']\nattention_m = sample_row['attention_mask']\ntext_lab = sample_row['labels']\n\nprint(comment_text)\nprint()\nprint(input_id)\nprint()\nprint(attention_m)\nprint()\nprint(text_lab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets Define our bert model for testing , Ignore below section not requied*"},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model = BertModel.from_pretrained(BERT_MODEL_NAME,return_dict = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_prediction = bert_model(sample_row['input_ids'].unsqueeze(dim=0),sample_row['attention_mask'].unsqueeze(dim=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_prediction.last_hidden_state.shape,sample_prediction.pooler_output.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Below is the code to create dataloader which will take Dataset class as input*"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToxicCommentDataModule(pl.LightningDataModule):\n    def __init__(self,train_df,test_df,tokenizer,batch_size=8,max_token_len=128):\n        super().__init__()\n        \n        self.train_df = train_df\n        self.test_df = test_df\n        self.tokenizer = tokenizer\n        self.batch_size = batch_size\n        self.max_token_len=max_token_len\n        \n    def setup(self):\n        self.train_dataset = ToxicCommentsDataset(\n            self.train_df,\n            self.tokenizer,\n            self.max_token_len\n        )\n        self.test_dataset = ToxicCommentsDataset(\n            self.test_df,\n            self.tokenizer,\n            self.max_token_len\n        )\n        \n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size = self.batch_size,\n            shuffle=True,\n            num_workers = 4\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(self.test_dataset,batch_size = 1,num_workers = 4)\n    \n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset,batch_size = 1,num_workers = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_module = ToxicCommentDataModule(train_df,val_df,tokenizer,batch_size=BATCH_SIZE)\ndata_module.setup()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets create class for actually model configuration , loss and metrics*"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToxicCommentClassifier(pl.LightningModule):\n    def __init__(self,n_classes:int,steps_per_epoch:None,n_epochs=None):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME,return_dict = True)\n        self.classifier = torch.nn.Linear(self.bert.config.hidden_size,n_classes)\n        \n        self.steps_per_epoch = steps_per_epoch\n        self.n_epochs = n_epochs\n        \n        self.criterion = torch.nn.BCELoss()\n        \n        \n    def forward(self,input_ids,attention_mask,labels=None):\n        output = self.bert(input_ids,attention_mask=attention_mask)\n        output = self.classifier(output.pooler_output)\n        output = torch.sigmoid(output)\n        \n        loss = 0\n        if labels is not None:\n            loss = self.criterion(output,labels)\n        return loss ,output\n    \n    def training_step(self,batch,batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n        \n        loss,outputs = self(input_ids,attention_mask,labels)\n        self.log('train_loss',loss,prog_bar=True,logger=True)\n        return {\"loss\":loss,\"predictions\":outputs,\"labels\":labels}\n    \n    def validation_step(self,batch,batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n        loss,outputs = self(input_ids,attention_mask,labels)\n        self.log('val_loss',loss,prog_bar=True,logger=True)\n        return loss\n    \n    \n    def test_step(self,batch,batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n        \n        loss,outputs = self(input_ids,attention_mask,labels)\n        self.log('test_loss',loss,prog_bar=True,logger=True)\n        return loss\n    \n    \n    def training_epoch_end(self,outputs):\n        labels = []\n        predictions=[]\n        \n        for output in outputs:\n            for out_labels in output['labels'].detach().cpu():\n                labels.append(out_labels)\n            for out_predictions in output['predictions'].detach().cpu():\n                predictions.append(out_predictions)\n                \n        labels = torch.stack(labels)\n        predictions = torch.stack(predictions)\n        \n        for i,name in enumerate(LABEL_COLUMNS):\n            roc_score = auroc(predictions[:,i],labels[:,i])\n            self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\",roc_score,self.current_epoch)\n            \n        \n    def configure_optimizers(self):\n        optimizer =  AdamW(self.parameters(), lr=2e-5)\n        \n        \n        warmup_steps = self.steps_per_epoch // 3\n        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n        \n        \n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            warmup_steps,\n            total_steps\n        )\n        \n        return [optimizer],[scheduler]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ToxicCommentClassifier(n_classes=6,\n                               steps_per_epoch=len(train_df)//BATCH_SIZE,\n                              n_epochs=N_EPOCHS\n                              )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets run our trainer enable fast_dev_run if you want to just do a quick run its helps in dubugging the code *"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = pl.Trainer(max_epochs=N_EPOCHS,\n                     gpus=1,\n                     progress_bar_refresh_rate=30\n#                      fast_dev_run=True\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(model,data_module)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the extension and start TensorBoard\n# %load_ext tensorboard\n# %tensorboard --logdir ./lightning_logs\n# %reload_ext tensorboard\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.test()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Save the best model*"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.save_checkpoint(\"final_checkpoint.ckpt\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Load the model*"},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model = ToxicCommentClassifier.load_from_checkpoint('./final_checkpoint.ckpt',\n                                                            n_classes=6,\n                                                            steps_per_epoch=len(train_df)//BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Work in progress below i have just checked for a single comment"},{"metadata":{},"cell_type":"markdown","source":"*Lets test on a small sample *"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_comment = \"Nonsense cocksucker Fuck? Kiss off,geek.What i said is true.I will have you account terminated.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding  = tokenizer.encode_plus(test_comment,\n                     add_special_tokens=True,\n                     max_length=128,\n                     return_token_type_ids=False,\n                     padding = 'max_length',\n                     return_attention_mask=True,\n                     return_tensors='pt'\n                     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_,prediction = trained_model(encoding['input_ids'],encoding['attention_mask'])\nprediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_prediction = prediction.detach().numpy()\ntest_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_labels = []\n\nfor i,label in enumerate(LABEL_COLUMNS):\n    label_prob = test_prediction[:,i]\n    \n    if label_prob>0.5:\n        prediction_labels.append(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}