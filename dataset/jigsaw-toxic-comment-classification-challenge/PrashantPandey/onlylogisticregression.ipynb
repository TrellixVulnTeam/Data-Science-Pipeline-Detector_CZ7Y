{"nbformat":4,"cells":[{"outputs":[],"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import log_loss\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB\nfrom sklearn import svm\nimport xgboost as xgb\nfrom sklearn.decomposition import TruncatedSVD\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"e5825421-3048-4e87-a859-a2248bb93a9a","_uuid":"fbc088d4976520309d25ad1cb12357202841373f"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nsampleSubmission = pd.read_csv(\"../input/sample_submission.csv\")","metadata":{"_cell_guid":"6b3055c3-8a40-42fa-9666-462528bb5486","collapsed":true,"_uuid":"bd012f975667e886ba1d61eb483fda2e4e691d9d"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrainTxt = train['comment_text']\ntestTxt = test['comment_text']\ntrainTxt = trainTxt.fillna(\"unknown\")\ntestTxt = testTxt.fillna(\"unknown\")\ncombinedTxt = pd.concat([trainTxt,testTxt],axis=0)","metadata":{"_cell_guid":"179cc113-bfc6-4311-ac49-0dd1f61f6a04","collapsed":true,"_uuid":"9c2ca37258371081c246c45267a5c2e8d260f17f"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"vect = TfidfVectorizer(decode_error='ignore',use_idf=True,smooth_idf=True,min_df=10,ngram_range=(1,3),lowercase=True,\n                      stop_words='english')","metadata":{"_cell_guid":"9ab12926-8938-47cb-8c70-710d86f04174","collapsed":true,"_uuid":"d075dccbc903590cb08439c4179849f2e2e595a0"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"combinedDtm = vect.fit_transform(combinedTxt) #fit on combine\ntrainDtm = combinedDtm[:train.shape[0]]\ntestDtm = vect.transform(testTxt) #transform only test","metadata":{"_cell_guid":"21434a80-de63-44b1-8304-5ea9ff3a658c","collapsed":true,"_uuid":"0864159d1607202dbd4d04ab8b56d10191530fdb"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"lrpreds = np.zeros((test.shape[0],len(col)))\nnbpreds = np.zeros((test.shape[0],len(col)))\nsvmpreds = np.zeros((test.shape[0],len(col)))\nxgbpreds = np.zeros((test.shape[0],len(col)))","metadata":{"_cell_guid":"be226ccd-289c-47c0-bbcb-a5f220504b25","collapsed":true,"_uuid":"c9f9491cbd5dedf6a3160e444417791883e6e783"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"svd = TruncatedSVD(n_components=100, n_iter=10, random_state=42)\ntrainDtmSvd = svd.fit_transform(trainDtm)\ntestDtmSvd = svd.transform(testDtm)","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#call fit on every single col value \n#normal lr\nloss = []\nfor i,j in enumerate(col):\n    lr = LogisticRegression(C=4)\n    lr.fit(trainDtm,train[j]) #train[j] is each type of comment\n    lrpreds[:,i] = lr.predict_proba(testDtm)[:,1]\n    train_preds = lr.predict_proba(trainDtm)[:,1]\n    loss.append(log_loss(train[j],train_preds))\nnp.mean(loss)","metadata":{"_cell_guid":"53f0a61a-94a3-4b93-a893-aef2aa3f1a6a","_uuid":"50ec5df1aaf7ec596cfa0301f7e561d23f1ce821"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#lr with Svd\nloss = []\nfor i,j in enumerate(col):\n    lr = LogisticRegression(C=4)\n    lr.fit(trainDtmSvd,train[j]) #train[j] is each type of comment\n    lrpreds[:,i] = lr.predict_proba(testDtmSvd)[:,1]\n    train_preds = lr.predict_proba(trainDtmSvd)[:,1]\n    loss.append(log_loss(train[j],train_preds))\nnp.mean(loss)","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#normal nb\nloss = []\nfor i,j in enumerate(col):\n    nb = MultinomialNB()\n    nb.fit(trainDtm,train[j]) #train[j] is each type of comment\n    nbpreds[:,i] = nb.predict_proba(testDtm)[:,1]\n    train_preds = nb.predict_proba(trainDtm)[:,1]\n    loss.append(log_loss(train[j],train_preds))\nnp.mean(loss)","metadata":{"_cell_guid":"7e0b86c7-79dd-43c3-8e7f-25eeaf2e208d","_uuid":"b1ea6603361e23e0761141bd571c6b3417faf896"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#GaussianNB  with svd\nloss = []\nfor i,j in enumerate(col):\n    nb = GaussianNB()\n    nb.fit(trainDtmSvd,train[j]) #train[j] is each type of comment\n    nbpreds[:,i] = nb.predict_proba(testDtmSvd)[:,1]\n    train_preds = nb.predict_proba(trainDtmSvd)[:,1]\n    loss.append(log_loss(train[j],train_preds))\nnp.mean(loss)","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#normal xgb\nloss = []\nfor i,j in enumerate(col):\n    xg = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n    xg.fit(trainDtm,train[j]) #train[j] is each type of comment\n    xgbpreds[:,i] = xg.predict_proba(testDtm)[:,1]\n    train_preds = xg.predict_proba(trainDtm)[:,1]\n    loss.append(log_loss(train[j],train_preds))\nnp.mean(loss)","metadata":{"_cell_guid":"991b3f0c-95fe-495b-a174-b6473df903db","_uuid":"fa682526efb6be6b587ac4d237e41d48adf18f3a"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#xgb with svd\nloss = []\nfor i,j in enumerate(col):\n    xg = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n    xg.fit(trainDtmSvd,train[j]) #train[j] is each type of comment\n    xgbpreds[:,i] = xg.predict_proba(testDtmSvd)[:,1]\n    train_preds = xg.predict_proba(trainDtmSvd)[:,1]\n    loss.append(log_loss(train[j],train_preds))\nnp.mean(loss)","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# predsMix = 0.6*lrpreds+0.3*xgbpreds+0.1*nbpreds\npredsMix = xgbpreds\npredsDf = pd.DataFrame(predsMix,columns = col)\nsubid = pd.DataFrame({'id':sampleSubmission['id']})\nfinalPreds = pd.concat([subid,predsDf],axis=1)\nfinalPreds.to_csv(\"mix.csv\",index=False)","metadata":{"_cell_guid":"7223d4a0-2bfa-4316-85d6-8f3a6162901e","_uuid":"221cc9df76de73fa95f797dae07a5604c89ad6c9"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"","metadata":{"_cell_guid":"a78c0695-480c-475e-8bc8-daef670ff9c5","collapsed":true,"_uuid":"3cb33203358ac9723a1520030e6a3405d2458f47"},"cell_type":"code"}],"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3"}}}