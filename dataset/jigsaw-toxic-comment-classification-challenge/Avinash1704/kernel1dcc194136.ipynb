{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input/jigsaw-toxic-comment-classification-challenge/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\ntest_data = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text cleaning ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Removing punctuation \n2. Removing Stop words \n3. Stemming the words ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport re\nimport nltk\nstopwords = nltk.corpus.stopwords.words(\"english\")\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenize ","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def remove_punc(text):\n    word = \"\".join([char.lower() for char in text if char not in string.punctuation])\n    return word\n\ntrain_data[\"removed_punch\"] = train_data['comment_text'].apply(lambda x : remove_punc(x))\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def token(text):\n    word = re.split(\"\\W+\",text)\n    return word\ntrain_data[\"token_word\"] = train_data['removed_punch'].apply(lambda x : token(x))\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Vectorization ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vector = TfidfVectorizer(tokenizer=token,analyzer='word',max_features=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_vectorization = word_vector.fit_transform(train_data['comment_text'])\ntest_vectorization = word_vector.fit_transform(test_data['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_vectorization.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_vectorization.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating DataFrame \ntrain_vectorization_df = pd.DataFrame(train_vectorization.toarray(), columns=word_vector.get_feature_names())\ntest_vectorization_df = pd.DataFrame(test_vectorization.toarray(), columns=word_vector.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_data[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\nX_train = train_vectorization_df\nX_test = test_vectorization_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning model ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_label = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = np.zeros((X_test.shape[0],y_train.shape[1]))\npredicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,label in enumerate(target_label):\n    lr = LogisticRegression(C=2,random_state = i,class_weight = 'balanced')\n    print('Building {} model for column:{''}'.format(i,label)) \n    lr.fit(X_train,y_train[label])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nlabel = 'insult'\ny_pred = lr.predict(X_train)\nprint(classification_report(y_train[label],y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in target_label:\n    print(\" Lable \",i,classification_report(y_train[i],y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predicted_labels = lr.predict_proba(X_train)[:,1]\ny_predicted_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ROC ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_train['insult'], y_predicted_labels)\nmetrics.auc(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predicted = pd.DataFrame(predicted,columns=y_train.columns)\nsubmission = pd.concat([test_data['id'],test_predicted],axis=1)\nsubmission.to_csv('submit.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}