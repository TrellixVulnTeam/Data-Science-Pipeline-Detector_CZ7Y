{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import MultinomialNB\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.metrics import accuracy_score\nfrom collections import Counter\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntrain.head()\ntest = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\nsample_submission = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\n\ntest.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def instance_per_category(data):\n    data_categories = data.drop(['id','comment_text'],axis=1)\n    count_toxic = Counter(data_categories['toxic'])\n    count_severe = Counter(data_categories['severe_toxic'])\n    count_obscene = Counter(data_categories['obscene'])\n    count_threat = Counter(data_categories['threat'])\n    count_insult = Counter(data_categories['insult'])\n    count_iden = Counter(data_categories['identity_hate'])\n    df = pd.DataFrame.from_dict([count_toxic,count_severe,count_obscene,count_threat,count_insult,count_iden])\n    df.index  = ['toxic','severe toxic','obscene','threat','insult','identity hate']\n    return df.T\n\nprint('total samples: {0}'.format(train.shape[0]))\ntrain_categories = instance_per_category(train)\ntrain_categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_category(data):\n    x = np.arange(len(data.columns))\n    labels = list(data.columns)\n    width = 0.25\n    fig,ax = plt.subplots()\n    fig.suptitle('Summary of training dataset')\n    ax1= ax.bar(x = x+ width/2,  height = data.iloc[0,:], width = width, label='negetive/ not labeled ')\n    ax2 = ax.bar(x = x- width/2,  height = data.iloc[1,:], width = width, label= 'positive')\n    ax.set_ylabel('number of comments')\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels)\n    ax.legend()\n    plt.show()\n\n\nplot_category(train_categories)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot below shows that this dataset contains comments which can be categorized into more than one categories. Moreover, this dataset contains large amount of unlabelled data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_label(data):\n    sum_ = data.iloc[:,2:].sum(axis=1)\n    x = sum_.value_counts()\n    fig = plt.figure(figsize = (17,8))\n    ax = plt.bar(x.index,x.values )\n    for bar in ax:\n        bar_h = bar.get_height()\n        plt.text(bar.get_x(), bar_h, bar_h)       \n    plt.xticks(x.index,['unlabeled','1 category','2 categories','3 categories','4 categories','5 categories'])\n    plt.title('number of categories for each comment')\n    plt.xlabel('number of categories')\n    plt.ylabel('comments')\n    plt.show()\n    \nmulti_label(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('total unlabelled data: {0}'.format( sum( (train.iloc[:,2] == 0) & (train.iloc[:,3] == 0) & (train.iloc[:,4] == 0) &(train.iloc[:,5] == 0) & (train.iloc[:,6] == 0) & (train.iloc[:,7] == 0))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('number of missing comments: {0}'.format(train.iloc[:,1].isnull().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_cleaning(text: str)-> str:\n    text = text.lower()\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"\\n\" , \" \", text)\n    text = re.sub(r\"\\'d'\", \" \", text)\n    text = re.sub(r\"\\'s'\" , \" \", text)\n    text = re.sub(r\"\\'ll\", \" \", text)\n    text = re.sub(r\"\\'m'\", \" \", text)\n    text = re.sub(r\"\\'re'\", \" \", text)\n    return text\n    \ntrain['comment_text'] = train['comment_text'].map( lambda text : text_cleaning(str(text)))\ntest['comment_text'] = test['comment_text'].map( lambda text : text_cleaning(str(text)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_cloud(text):\n    plt.figure(figsize=(20,9))\n    texts = text.values\n    cloud = WordCloud(stopwords=STOPWORDS, background_color = 'white', collocations = False, width=2000, height= 1000).generate(\" \".join(text))\n    plt.axis('off')\n    plt.title('word cloud')\n    plt.imshow(cloud)\nword_cloud(train['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def comment_length(data):\n    comments = data.iloc[:,1]\n    length_comments = [len (comment.split(' ')) for comment in comments]\n    max_len = max(length_comments)\n    min_len = min(length_comments)\n    count = Counter(length_comments)\n    print(\" frequency of the length of the comments\")\n    plt.hist(list(count))\n    return count\ncomment_lengths = comment_length(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"most of the comments have length between 0 to 1000 with few outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(text):\n    tokens = word_tokenize(text)\n    tokens_alpha = [token for token in tokens if token.isalpha()]\n    return tokens_alpha\n\ntrain['comment_text'] = train['comment_text'].map(lambda text: tokenize(text))\ntest['comment_text'] = test['comment_text'].map(lambda text: tokenize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set,test_set = train_test_split(train,random_state=42,test_size=0.3,shuffle=True)\nx_train = train_set['comment_text'].values\ny_train= train_set.iloc[:,2:]\n\nx_test = test_set['comment_text'].values\ny_test = test_set.iloc[:,2:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=1500)\nx_train = vectorizer.fit_transform(str(x) for x in x_train)\nx_test = vectorizer.transform(str(x) for x in x_test)\nsub_x = vectorizer.transform(str(x) for x in test['comment_text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = ClassifierChain(MultinomialNB(fit_prior=True))\nclassifier.fit(x_train,y_train)\ny_predict = classifier.predict(x_test)\nscore = accuracy_score(y_test,y_predict)\nprint('estimated accuracy score: {0}'.format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predict = classifier.predict_proba(sub_x)\ntest_predict= test_predict.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = pd.DataFrame(test['id'])\npredict = pd.DataFrame(test_predict,columns= ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\nsubmission = pd.concat([ids, predict] ,axis = 1)\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}