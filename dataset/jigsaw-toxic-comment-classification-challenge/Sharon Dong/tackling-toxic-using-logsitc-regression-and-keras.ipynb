{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h2> Introduction </h2>\nIn this kernel we will use Logistic Regression and Deep Learning methodologies to detect whether a text comment is a normal comment or a toxic (there are 6 toxic categories here). \n\n\n\n<h2> Goals: </h2>\n<ul>\n<li> Convert text comment to vectors using TFIDF, then build logistic regression to predict 6 toxic labels seperately</li>\n<li> Convert text comment to tokens using Tokenizer, then build deep learning model to predict 6 toxic labels all together (multi label classification) </li>\n<li> Convert text comment to tokens using Tokenizer, build deep learning model with GloVe weights, then predict 6 toxic labels all together (multi label classification) </li>\n<li> Ensemble all 3 models to get a better prediction</li>\n\n\n<h2> Outline: </h2>\n    \n    \nI. <b>Data Overview </b><br>\na) [Load Data](#load)<br>\nb) [Data Overview](#overview)<br><br>\n    \nII. <b>TFIDF + Logistic Regression </b><br>\na) [TFIDF](#TFIDF)<br>\nb) [MaxAbsScaler](#scaler)<br>\nc) [Logistic Regression](#lr)<br>\n\nIII. <b>Tokenizer + Deep learning </b><br>\na) [Tokenizer](#token)<br>\nb) [Padding](#padding)<br>\nc) [Build Deep learning model](#dlmodel)<br>\n\n\nIV. <b>Tokenizer + Deep learning + GloVe</b><br>\na) [Load Embedding from GloVe](#embedding)<br>\nb) [Build deep learning model](#dlmodel2)<br>\n\n    \nV. <b>Ensemble and model comparison</b><br>\na) [Ensemble 3 models](#ensemble)<br>\nb) [Model performance comparison](#performance)<br>\nc) [Error analysis](#error)<br>"},{"metadata":{},"cell_type":"markdown","source":"# Prepare the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom sklearn.pipeline import make_union\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"463f0f87-f96f-435c-8f40-d39dfef8dc36","_uuid":"09df08cb0050aa8fd4b5b7bd6606b4d79b7a9d08","trusted":true},"cell_type":"code","source":"import keras\nimport tensorflow as tf\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model,Sequential\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42634224-3ba5-41e3-8612-285cc8a8af16","_uuid":"5e88facdabb090f8eaf48d014838e08cde63c455"},"cell_type":"markdown","source":"## Loading the train and test files\n<a id=\"load\"></a>"},{"metadata":{"_cell_guid":"62c89f51-8315-4f50-97c9-de3539e884a9","_uuid":"447247729764c3579a4a2d6bf69287abff0b9af1","trusted":true},"cell_type":"code","source":"\ntrain = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_label = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\nprint(train.head())\nprint(test.head())\nprint(test_label.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-'*20+'Train data label distribution'+'-'*20 +'\\n')\nprint(train.drop(['id','comment_text'],axis=1).apply(pd.Series.value_counts,normalize=True))\nprint(\"\\n\")\n\nprint('-'*20+'Join test data and label then remove the -1s'+'-'*20 +'\\n')\ntest_wlabel = pd.merge(test, test_label, how='left',on='id')\n\ntest_wlabel = test_wlabel[test_wlabel['toxic']!= -1]\nprint(test_wlabel.drop(['id','comment_text'],axis=1).apply(pd.Series.value_counts,normalize=True))\nprint(\"\\n\")\nprint(\"test data count\")\nprint(test_wlabel.count())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d7ab4de0-419e-4b65-9191-d211bfb80cd5","_uuid":"a1cbcc835faa99b426d229f6b7c708e84c57278d"},"cell_type":"markdown","source":"## Data Overview\n<a id=\"Overview\"></a>"},{"metadata":{},"cell_type":"markdown","source":"check if there is any nulls in the data"},{"metadata":{"_cell_guid":"eda2a4d3-c4d4-4d09-a9f3-d3087b0ef96f","scrolled":true,"_uuid":"f5c25b08f4b5504ab984fc8a7b3748f8cef42440","trusted":true},"cell_type":"code","source":"train.isnull().any(),test.isnull().any() #No nulls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare the data for modeling"},{"metadata":{"_cell_guid":"4bf0415e-6dcc-40c1-8d04-96b1ac95eb27","_uuid":"548389f5096016fcb405fc73d5c21390d6c72d83","trusted":true},"cell_type":"code","source":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny_train = train[list_classes]\ny_test = test_wlabel[list_classes]\nlist_sentences_train = train[\"comment_text\"]\nlist_sentences_test = test_wlabel[\"comment_text\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TFIDF + Logistic"},{"metadata":{},"cell_type":"markdown","source":"## TFIDF at word level and char level\n<a id=\"TFIDF\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_text = pd.concat([list_sentences_train, list_sentences_test]) # Fit only on Training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(max_features=30000, \n                                  stop_words='english',\n                                  strip_accents = 'ascii', \n                                  token_pattern=r'[a-zA-Z]{1,}',\n                                  ngram_range=(1, 4))\n\nword_vectorizer.fit(list_sentences_train)\ntrain_word_features = word_vectorizer.transform(list_sentences_train)\ntest_word_features = word_vectorizer.transform(list_sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"char_vectorizer = TfidfVectorizer(analyzer='char', \n                                  max_features=30000, \n                                  strip_accents='ascii',\n                                  ngram_range=(1, 4))\n\nchar_vectorizer.fit(list_sentences_train)\ntrain_char_features = char_vectorizer.transform(list_sentences_train)\ntest_char_features = char_vectorizer.transform(list_sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = hstack([train_char_features,train_word_features])\ntest_features = hstack([test_char_features,test_word_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MaxAbsScaler \n<a id='scaler'> </a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MaxAbsScaler()\nscaler.fit(train_features)\ntrain_features_scaled = scaler.transform(train_features)\ntest_features_scaled = scaler.transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Logistic Regression</h2>\n<a id='lr'></a>\n<li>Use warm start to speed up convergence , and utilize the learning from the previous model.</li>\n<li>Use C = 0.01 to specify stronger regularization, and speed up convergence. </li>\n<li>Use solver = 'sag' to speed up convergence.</li>\n<li>Use class_weight to reduce the imbalance issue.</li>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nscore = []\ny_pred_lr = pd.DataFrame()\nfor class_name in class_names:\n    train_target = y_train[class_name]\n    test_target = y_test[class_name]\n    weight = sum(train_target==0)*0.2/sum(train_target==1)\n    lr = LogisticRegression(solver = 'sag', C=0.01, class_weight={0:0.2,1: weight},max_iter=1000, warm_start=True)\n    lr.fit(train_features_scaled, train_target)\n    y_pred = lr.predict(train_features_scaled)\n    auc_score = roc_auc_score(train_target,y_pred)\n    print('Train score for class {} is {}'.format(class_name, auc_score))\n    y_proba = lr.predict_proba(test_features_scaled)[:,1]\n    y_pred_lr[class_name] = y_proba\n    auc_score = roc_auc_score(test_target,y_proba>=0.5)\n    score.append(auc_score)\n    print('Test score for class {} is {}'.format(class_name, auc_score))\n\n    feature_importance = pd.DataFrame({'coef':lr.coef_[0],'feature_name':char_vectorizer.get_feature_names()+word_vectorizer.get_feature_names()})\n    # feature_importance\n    feature_importance.sort_values('coef',ascending=False, inplace=True)\n    print(\"top 10 variables are : {}\".format(feature_importance['feature_name'].head(10).tolist()))\nprint(\"\\nscore for total class is {}\".format(np.mean(score)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenizer + LSTM"},{"metadata":{},"cell_type":"markdown","source":"## Tokenizer\n<a id='token'> <a>"},{"metadata":{"_cell_guid":"0d373763-f5c6-4d28-a1d7-f5030d47320f","_uuid":"8d814a2dadca32a8810dca02c7d10dff38a84660","trusted":true},"cell_type":"code","source":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c09fa4e-9863-4d49-aa26-ec213a80e500","scrolled":false,"_uuid":"24c8936770226aafac6f868ace63f68718b3f18e","trusted":true},"cell_type":"code","source":"#commented it due to long output\n#for occurence of words\n#tokenizer.word_counts\n#for index of words\n#tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6cc19fbd-3034-42e3-a837-cedfde6a839f","_uuid":"8afd0371bef4f9954c586547f2400b71ced0bfb2"},"cell_type":"markdown","source":"<h2>Padding</h2>\n<a id='paddnig'> </a>\n    \nmake sure they all have same input length"},{"metadata":{"_cell_guid":"d5eca775-e326-421c-a90b-2fc69f14020e","_uuid":"77cb38be6af621207460fa657df85bc70535a21c","trusted":true},"cell_type":"code","source":"maxlen = 200\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c447319-2453-48ed-8e68-0c192c0873f6","_uuid":"69fde7d5c0ff270393321d28298610e2d3a56634","trusted":true},"cell_type":"code","source":"totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e522d821-136f-4b07-80f4-d1d9323f5ef4","_uuid":"4116cf21a3a94d2b05017c9ae8f3b773317a71c0","trusted":true},"cell_type":"code","source":"plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b360bb42-4779-410a-a171-e5bd9ef148ce","_uuid":"0b3389d55e4ba925a2a873acdd2277cc2f2818b4"},"cell_type":"markdown","source":"## Build the deep learning model\n<a id='dlmodel'> <a>"},{"metadata":{"_cell_guid":"f833649b-7a62-4480-9ee9-193128eefb9b","_uuid":"a8578e2b581989b196b3f8296995c3241f32f1d5","trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen, )) \nembed_size = 100\nx = Embedding(max_features, embed_size)(inp)\nx = LSTM(50, return_sequences=True,name='lstm_layer')(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0192471f-516d-4f3d-9ce0-784da0ca59ab","_uuid":"5da468e7b497b736510f0521863bc696809e134b","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ae6cadb-0543-4fee-9dba-1eb99987c441","_uuid":"1c26824601d297f618bea32e1e08c3af799fc5a4","trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 2\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=2) # Leave it here for future use, but in this case 2 epochs are good\nmodel.fit(X_t,y_train, batch_size=batch_size, epochs=2, validation_split=0.1,callbacks=[early_stopping_cb])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"View the output of embedding layer"},{"metadata":{"_cell_guid":"5bdea10d-3c9c-410d-a639-6ec810d36b79","_uuid":"875aea01dc1439076c37f56095851ed8bd5bc078","trusted":true},"cell_type":"code","source":"# from keras import backend as K\n\n# # with a Sequential model\n# get_3rd_layer_output = K.function([model.layers[0].input],\n#                                   [model.layers[1].output])\n# layer_output = get_3rd_layer_output([X_t[:1]])[0]\n# layer_output.shape\n# # layer_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_te)\nprint(\"AUCROC score of the model\", roc_auc_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the AUCROC curve to get the appropriate decision threshold (0.03 is chosen in this job)"},{"metadata":{},"cell_type":"markdown","source":"# Tokenizer + LSTM + Pretrained model GloVe (100d)"},{"metadata":{},"cell_type":"markdown","source":"## Load GloVe embedding file\n<a id='embedding'> <a>"},{"metadata":{"_cell_guid":"9f197004-0109-4955-895d-6be3b074d923","_uuid":"162c28bf0227dd13f8ebb2718981f21d4077ce0d","trusted":true},"cell_type":"code","source":"EMBEDDING_FILE='../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word,*arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\n# emb_mean,emb_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Use the same parameters as before\n# embed_size = 100 # how big is each word vector\n# max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n# maxlen = 200 # max number of words in a comment to use","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the deep learning model with embedding weights initialization\n<a id='dlmodel2'> <a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp2 = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp2)\nx = LSTM(50, return_sequences=True)(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel2 = Model(inputs=inp2, outputs=x)\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 2\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=2) # Leave it here for future use, but in this case 2 epochs are good\nmodel2.fit(X_t,y_train, batch_size=batch_size, epochs=2, validation_split=0.1,callbacks=[early_stopping_cb])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2 = model2.predict(X_te)\nprint(\"AUCROC score of the model\", roc_auc_score(y_test,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble and model comparison"},{"metadata":{},"cell_type":"markdown","source":"## Ensemble 3 models\n<a id='ensemble'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = pd.DataFrame(y_pred,columns=class_names)\ny_pred2 = pd.DataFrame(y_pred2,columns=class_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_score = pd.DataFrame()\n\nfor class_name in class_names:\n    ensemble_score[class_name] = (y_pred_lr[class_name] + y_pred[class_name] + y_pred2[class_name])/3\n# #     ensemble_score[class_name] = (y_pred[class_name] + y_pred2[class_name])/2\n#     fpr, tpr, thresholds = roc_curve(y_test[class_name],ensemble_score[class_name])\n#     idx = np.argmax(tpr - fpr ) \n#     print('Confusion matrix for class {}'.format(class_name))\n#     print(confusion_matrix(y_test[class_name],ensemble_score[class_name]>=thresholds[idx]))\n#     print('classification report for class {}'.format(class_name))\n#     print(classification_report(y_test[class_name],ensemble_score[class_name]>=thresholds[idx]))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model performance comparison\n<a id='performance'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AUCROC score of Logistic Regression\", roc_auc_score(y_test,y_pred_lr))\nprint(\"AUCROC score of LSTM\", roc_auc_score(y_test,y_pred))\nprint(\"AUCROC score of LSTM+GloVe\", roc_auc_score(y_test,y_pred2))\nprint(\"\\n\")\nprint(\"AUCROC score of Ensemble\", roc_auc_score(y_test,ensemble_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,4,figsize=(30,8))\nfor i in range(6):\n    ax[0].set_title('Logistic Regression')\n    fpr, tpr, thresholds = roc_curve(y_test.iloc[:,i],y_pred_lr.iloc[:,i])\n    ax[0].plot(fpr, tpr, label=class_names[i])\n    ax[0].legend(loc='best')\n    \n    ax[1].set_title('LSTM')\n    fpr, tpr, thresholds = roc_curve(y_test.iloc[:,i],y_pred.iloc[:,i])\n    ax[1].plot(fpr, tpr, label=class_names[i])\n    ax[1].legend(loc='best')\n    \n    ax[2].set_title('LSTM+GloVe')\n    fpr, tpr, thresholds = roc_curve(y_test.iloc[:,i],y_pred2.iloc[:,i])\n    ax[2].plot(fpr, tpr, label=class_names[i])\n    ax[2].legend(loc='best')\n    \n    ax[3].set_title('Ensemble')\n    fpr, tpr, thresholds = roc_curve(y_test.iloc[:,i],ensemble_score.iloc[:,i])\n    ax[3].plot(fpr, tpr, label=class_names[i])\n    ax[3].legend(loc='best')\n\nax[0].plot([0, 1], [0, 1],'r--')    \nax[1].plot([0, 1], [0, 1],'r--')    \nax[2].plot([0, 1], [0, 1],'r--')\nax[3].plot([0, 1], [0, 1],'r--')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_threshods_lr = []\noptimal_threshods_dl1 = []\noptimal_threshods_dl2 = []\noptimal_threshods_es = []\n\nprint(\"-\"*20+\"Logistic Regression decision threshold:\"+\"-\"*20)\nfor i in range(len(class_names)):\n    fpr, tpr, thresholds = roc_curve(y_test.iloc[:,i],y_pred_lr.iloc[:,i])\n    idx = np.argmax(tpr - fpr ) \n    print(\"optimal fpr, tpr: \",fpr[idx], \",\",tpr[idx])\n    print(\"optimal threshold: \",thresholds[idx])\n    optimal_threshods_lr.append(thresholds[idx])\n\n    \nprint(\"-\"*20+\"LSTM decision threshold:\"+\"-\"*20)\n\nfor i in range(len(class_names)):\n    fpr, tpr, thresholds = roc_curve(y_test.iloc[:,i],y_pred.iloc[:,i])\n    idx = np.argmax(tpr - fpr ) \n    print(\"optimal fpr, tpr: \",fpr[idx], \",\",tpr[idx])\n    print(\"optimal threshold: \",thresholds[idx])\n    optimal_threshods_dl1.append(thresholds[idx])\n\nprint(\"-\"*20+\"LSTM+GloVe decision threshold:\"+\"-\"*20)\nfor i in range(len(class_names)):\n    fpr, tpr, thresholds = roc_curve(y_test.iloc[:,i],y_pred2.iloc[:,i])\n    idx = np.argmax(tpr - fpr ) \n    print(\"optimal fpr, tpr: \",fpr[idx], \",\",tpr[idx])\n    print(\"optimal threshold: \",thresholds[idx])\n    optimal_threshods_dl2.append(thresholds[idx])\n    \nprint(\"-\"*20+\"Ensemble decision threshold:\"+\"-\"*20)\nfor i in range(len(class_names)):\n    fpr, tpr, thresholds = roc_curve(y_test.iloc[:,i],ensemble_score.iloc[:,i])\n    idx = np.argmax(tpr - fpr ) \n    print(\"optimal fpr, tpr: \",fpr[idx], \",\",tpr[idx])\n    print(\"optimal threshold: \",thresholds[idx])\n    optimal_threshods_es.append(thresholds[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ny_pred_label1 = pd.DataFrame()\ny_pred_label2 = pd.DataFrame()\ny_pred_label3 = pd.DataFrame()\ny_pred_label4 = pd.DataFrame()\n\n\nfor i in range(len(class_names)):\n    print('classification report for class {}'.format(class_names[i]))\n    \n    print(\"-\"*20+\"Logistic Regression:\"+\"-\"*20)\n#     print('Confusion matrix for class {}'.format(class_names[i]))\n#     print(confusion_matrix(y_test.iloc[:,i],y_pred[:,i]>=optimal_threshods_lr[i]))  \n    y_pred_label1[class_names[i]] = y_pred_lr.iloc[:,i]>=optimal_threshods_lr[i]\n    print(classification_report(y_test.iloc[:,i], y_pred_label1[class_names[i]]))\n    \n    \n    print(\"-\"*20+\"LSTM:\"+\"-\"*20)\n    y_pred_label2[class_names[i]] = y_pred.iloc[:,i]>=optimal_threshods_dl1[i]\n    print(classification_report(y_test.iloc[:,i], y_pred_label2[class_names[i]]))\n    \n    \n    print(\"-\"*20+\"LSTM+GloVe:\"+\"-\"*20)\n    y_pred_label3[class_names[i]] = y_pred2.iloc[:,i]>=optimal_threshods_dl2[i]\n    print(classification_report(y_test.iloc[:,i], y_pred_label3[class_names[i]]))\n    \n    print(\"-\"*20+\"ensemble_score:\"+\"-\"*20)\n    y_pred_label4[class_names[i]] = ensemble_score.iloc[:,i]>=optimal_threshods_es[i]\n    print(classification_report(y_test.iloc[:,i],y_pred_label4[class_names[i]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above comparison, the ensemble method performs the best!"},{"metadata":{},"cell_type":"markdown","source":"## Error Analysis\n<a id='error'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text_score = pd.concat([list_sentences_test.reset_index(drop=True),\\\n                             y_test.reset_index(drop=True),\\\n                             y_pred_lr.add_suffix('_pred1'),\\\n                             y_pred.add_suffix('_pred2'),\\\n                             y_pred2.add_suffix('_pred3'),\n                             ensemble_score.add_suffix('_pred4'),\n                             y_pred_label1.add_suffix('_label1').astype(int),\n                             y_pred_label2.add_suffix('_label2').astype(int),\n                             y_pred_label3.add_suffix('_label3').astype(int),\n                             y_pred_label4.add_suffix('_label4').astype(int)\n                            ] ,axis=1)\ntest_text_score.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at the labels that are wrongly predicted\nlabel = 'toxic'\n\ntest_text_score[(test_text_score[label]==0) & (test_text_score[label+'_label4'] !=0)][['comment_text', label+'_label1',label+'_label2',label+'_label3']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_text_score.loc[12,'comment_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}