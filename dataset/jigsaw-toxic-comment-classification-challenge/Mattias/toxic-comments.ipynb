{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip uninstall tensorflow -y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/'\ncomp = 'jigsaw-toxic-comment-classification-challenge/'\nEMBEDDING_FILE=f'{path}sdasdaa/glove.6B.50d.txt'\nTRAIN_DATA_FILE=f'{path}{comp}train.csv'\nTEST_DATA_FILE=f'{path}{comp}test.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_size = 50 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a comment to use","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터 호출\n\ntrain = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\ntest = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\")\n\ntrain[train[\"comment_text\"].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 문장 데이터의 결측치를 \"_na_\"로 채워넣은 문장 데이터를 list_sentences_train에 저장\n\nlist_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y값 설정 \nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# max_feature(20000) 개수의 유니크한 단어를 갖는 vocab을 만든다\ntokenizer = Tokenizer(num_words=max_features)\n\ntokenizer.fit_on_texts(list(list_sentences_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizer 인스턴스에 만들어진 vocab에 맞추어 text_to_sequences 메서드를 통해 각 문장의 단어를 vocab의 인덱스로 mapping\n\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 그 결과는 다음과 같음\nlist_tokenized_train[:1]\n\n# 테스트도 똑같이 해주고\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 문장마다 token의 갯수가 다르기때문에, 균일한 column의 matrix 형태로 만들어주기 위해 padding을 해줌\n# 이때, 가장 긴 길이를 가지는 sequence의 길이(maxlen)에 맞추어 줌.\nX_t = pad_sequences(list_tokenized_train, maxlen = maxlen)\n\nX_te = pad_sequences(list_tokenized_test, maxlen = maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 기학습한 GloVe 모델의 embedding 값을 가져오기\n# get_coefs 함수를 정의해 단어마다 저장된 계수 가져오기\n# dict로 \"단어\": 임베딩 벡터 형태로 저장\n\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.stack 함수를 통해 각 임베딩 인덱스의 값을 차곡차곡 쌓아올림\n# np.stack 함수의 axis = -1으로 놓으면 아웃풋의 첫번째 차원이 인풋의 마지막 차원을 따른다고 함\nall_embs = np.stack(embeddings_index.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_mean, emb_std = all_embs.mean(), all_embs.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> a = np.array([1, 2, 3,1,1])\n>>> b = np.array([2, 3, 4 ,5 ,6])\n>>> np.stack((a, b), axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_mean, emb_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 토크나이저의 vocab에 저장된 단어의 index를 word_index에 저장\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 단어 vocab의 길이와 내가 지정한 최대 단어수의 갯수 중 더 짧은 것을 nb_words로 취함\nnb_words = min(max_features, len(word_index))\n\n# emb_mean, emb_std를 따르고, nb_words X embed_size 사이즈의 정규분포 matrix를 embedding_matrix로 생성\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index.items()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for word, i in word_index.items():\n    # vocab(word_index.items()) 딕셔너리에서, max_features(20000)을 넘어가는 단어,\n    # 즉 빈도수 기준 20000위 이상의 단어의 경우 그냥 패스. \n    if i>= max_features: continue\n    \n    # 그렇지 않을 경우, .get(word)로 embedding_vector에 저장.\n    embedding_vector = embeddings_index.get(word)\n    \n    # 값이 있을 경우, embedding_matrix에 하나씩 차곡차곡 쌓아줌\n    # 어느정도 임의로 생성한 20000*50의 임베딩 매트릭스를, GloVe 임베딩 메트릭스의 값으로 갱신해주는 것\n    \n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape\n\n# inp의 shape는 2차원일 경우 (column,)\n# embedding layer에는 아까 만들어 놓은 GloVe기반 웨이트를 씌워줌, 앞의 두 arg로 임베딩 메트릭스의 차원을 지정해줌\n# BiLSTM - CNN으로 샇아주고, dropout 옵션\n# binary 분류로 레이어 마무리하고, fit\n# 끝","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size,trainable=True, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50,activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bayes_opt 적용"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\nimport time\nimport tensorflow.keras.initializers\nimport statistics\nimport tensorflow.keras\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom tensorflow.keras.layers import LeakyReLU,PReLU\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_model(dropout, neuronPct, neuronShrink):\n    # We start with some percent of 5000 starting neurons on the first hidden layer.\n    neuronCount = int(neuronPct * 5000)\n    \n    # Construct neural network\n    # kernel_initializer = tensorflow.keras.initializers.he_uniform(seed=None)\n    model = Sequential()\n\n    # So long as there would have been at least 25 neurons and fewer than 10\n    # layers, create a new layer.\n    layer = 0\n    while neuronCount>25 and layer<5:\n        # The first (0th) layer needs an input input_dim(neuronCount)\n        if layer==0:\n            model.add(Dense(neuronCount, \n                input_dim=X_t.shape[1], \n                activation=PReLU()))\n        else:\n            model.add(Dense(neuronCount, activation=PReLU())) \n        layer += 1\n\n        # Add dropout after each hidden layer\n        model.add(Dropout(dropout))\n\n        # Shrink neuron count for each layer\n        neuronCount = neuronCount * neuronShrink\n\n    model.add(Dense(1,activation='sigmoid')) # Output\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aa =[np.where(r.sum()>=1 , 1, 0) for r in y]\nindex_aa=np.where([np.where(a==1,1,0) for a in aa])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_modify_np = np.zeros(159571)\nindexes = index_aa[0]\nreplacements = np.ones(16225)\n\nfor (index, replacement) in zip(indexes, replacements):\n    to_modify_np[index] = replacement","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_modify=pd.Series(to_modify_np).astype(\"object\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\n\n# Split train and test\nx_train = X_t[:1000]\ny_train = to_modify[:1000]\nx_test = X_t[1000:]\ny_test = to_modify[1000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropout=0.2\nlr=1e-6\nneuronPct=0.2\nneuronShrink=0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n\nmodel = generate_model(dropout, neuronPct, neuronShrink)\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr))\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \npatience=100, verbose=0, mode='auto', restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nkeras.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(pd.DataFrame(x_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on the bootstrap sample\nmodel.fit(x_train,y_train,validation_data=(x_test,npy_test)),callbacks=[monitor],verbose=0,epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = monitor.stopped_epoch\nepochs_needed.append(epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(x_test)\n\ny_test1=np.array(pd.get_dummies(pd.Series(y_test).astype(\"object\")))\n\n#         pred=pd.Series(pred.flatten()).astype(\"int\")\n\ny_test=y_test.astype(\"int\")\n\nflatten=pd.Series(pred.flatten())\n\naa=pd.DataFrame(flatten)\n\naa['1']=1-flatten\n\n#         pred=list(pred)\n#         y_test=list(y_test)\n\n# Measure this bootstrap's log loss\n#         y_compare = np.argmax(y_test,axis=0) # For log loss calculation\nscore = metrics.log_loss(y_test1, np.array(aa))\nmean_benchmark.append(score)\nm1 = statistics.mean(mean_benchmark)\nm2 = statistics.mean(epochs_needed)\nmdev = statistics.pstdev(mean_benchmark)\n\n# Record this iteration\ntime_took = time.time() - start_time\n#print(f\"#{num}: score={score:.6f}, mean score={m1:.6f}, stdev={mdev:.6f}, epochs={epochs}, mean epochs={int(m2)}, time={hms_string(time_took)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_network(dropout,lr,neuronPct,neuronShrink):\n    SPLITS = 2\n\n    # Bootstrap\n    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.3)\n\n    # Track progress\n    mean_benchmark = []\n    epochs_needed = []\n    num = 0\n    \n\n    # Loop through samples\n    for train, test in boot.split(X_t,to_modify):\n        start_time = time.time()\n        num+=1\n\n        # Split train and test\n        x_train = X_t[train]\n        y_train = to_modify[train]\n        x_test = X_t[test]\n        y_test = to_modify[test]\n        \n        print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n        \n        model = generate_model(dropout, neuronPct, neuronShrink)\n        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr))\n        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n        patience=100, verbose=0, mode='auto', restore_best_weights=True)\n\n        # Train on the bootstrap sample\n        model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1)\n        epochs = monitor.stopped_epoch\n        epochs_needed.append(epochs)\n\n\n        pred = model.predict(x_test)\n\n        y_test1=np.array(pd.get_dummies(pd.Series(y_test).astype(\"object\")))\n\n#         pred=pd.Series(pred.flatten()).astype(\"int\")\n\n        y_test=y_test.astype(\"int\")\n\n        flatten=pd.Series(pred.flatten())\n\n        aa=pd.DataFrame(flatten)\n\n        aa['1']=1-flatten\n        \n#         pred=list(pred)\n#         y_test=list(y_test)\n        \n        # Measure this bootstrap's log loss\n#         y_compare = np.argmax(y_test,axis=0) # For log loss calculation\n        score = metrics.log_loss(y_test1, np.array(aa))\n        mean_benchmark.append(score)\n        m1 = statistics.mean(mean_benchmark)\n        m2 = statistics.mean(epochs_needed)\n        mdev = statistics.pstdev(mean_benchmark)\n\n        # Record this iteration\n        time_took = time.time() - start_time\n        #print(f\"#{num}: score={score:.6f}, mean score={m1:.6f}, stdev={mdev:.6f}, epochs={epochs}, mean epochs={int(m2)}, time={hms_string(time_took)}\")\n\n    tensorflow.keras.backend.clear_session()\n    return (-m1)\n\nprint(evaluate_network(\n    dropout=0.2,\n    lr=1e-6,\n    neuronPct=0.2,\n    neuronShrink=0.2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress NaN warnings, see: https://stackoverflow.com/questions/34955158/what-might-be-the-cause-of-invalid-value-encountered-in-less-equal-in-numpy\nimport warnings\nwarnings.filterwarnings(\"ignore\",category =RuntimeWarning)\n\n# Bounded region of parameter space\npbounds = {'dropout': (0.0, 0.499),\n           'lr': (0.0, 0.1),\n           'neuronPct': (0.01, 1),\n           'neuronShrink': (0.01, 1)\n          }\n\noptimizer = BayesianOptimization(\n    f=evaluate_network,\n    pbounds=pbounds,\n    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n    random_state=1,\n)\n\nstart_time = time.time()\noptimizer.maximize(init_points=10, n_iter=100,)\ntime_took = time.time() - start_time\n\nprint(f\"Total runtime: {hms_string(time_took)}\")\nprint(optimizer.max)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}