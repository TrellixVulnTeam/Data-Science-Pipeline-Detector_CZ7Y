{"cells":[{"metadata":{"_cell_guid":"7f169939-93d7-4217-9bda-1ddbbbd19ca6","_uuid":"04b4fb7875d109f67f8af07bb5c05d46c1f913e4"},"cell_type":"markdown","source":"**Keras CNN with FastText Embeddings**"},{"metadata":{"_cell_guid":"12f70aec-ceca-44e3-8a21-b9a2f28a6a4e","_uuid":"2bf9f497e1097faa70ec09686d20636fc71c05c3"},"cell_type":"markdown","source":"CNNs provide a faster alternative to LSTM models at a comparable performance. They are faster to train and use fewer parameters. CNN models are translation invariant and in application to text make sense when there is no strong dependence on recent past vs distant past of the input sequence. CNNs can learn patterns in word embeddings and given the nature of the dataset (e.g. multiple misspellings, out of vocabulary words), it makes sense to use sub-word information. In this notebook, a simple CNN architecture is used for multi-label classification with the help of FastText word embeddings. Thus, it can be a good addition (diverse and accurate) to your ensemble."},{"metadata":{"_cell_guid":"29a8d31b-451e-4427-afb3-fd40cd01bc60","_uuid":"887320b9e2d8efaaa27c3f7913debaf53c37ed52","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \nfrom keras.utils import plot_model\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\n\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer \nimport os, re, csv, math, codecs\n\nsns.set_style(\"whitegrid\")\nnp.random.seed(0)\n\nDATA_PATH = '../input/'\nEMBEDDING_DIR = '../input/fasttext-wikinews/'\n\nMAX_NB_WORDS = 100000\ntokenizer = RegexpTokenizer(r'\\w+')\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6d95a418-778d-4683-85f7-33351d82c18d","_uuid":"92b4cf0b1af7f4a423eacc83a1fb43e66400056f"},"cell_type":"markdown","source":"Let's load the data and the embeddings..."},{"metadata":{"_cell_guid":"3f6f79a5-ba5b-486d-b212-3332e6c2b6d4","_uuid":"b56846b453212ccc053461ba730f56dacb433c23","trusted":true},"cell_type":"code","source":"#load embeddings\nprint('loading word embeddings...')\nembeddings_index = {}\nf = codecs.open('../input/fasttext-wikinews/wiki-news-300d-1M.vec', encoding='utf-8')\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('found %s word vectors' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1ba47ff9-ac70-4a6e-9811-461f6819ca89","_uuid":"eee4b60da112577dbdd4b17b3760bc365e483222","trusted":true},"cell_type":"code","source":"# #load data\n# train_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge' + '/train.csv', sep=',', header=0)\n# test_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge' + '/test.csv', sep=',', header=0)\n# test_df = test_df.fillna('_NA_')\n\n# print(\"num train: \", train_df.shape[0])\n# print(\"num test: \", test_df.shape[0])\n\n# label_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n# y_train = train_df[label_names].values\n\n# #visualize word distribution\n# train_df['doc_len'] = train_df['comment_text'].apply(lambda words: len(words.split(\" \")))\n# max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n# sns.distplot(train_df['doc_len'], hist=True, kde=True, color='b', label='doc len')\n# plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\n# plt.title('comment length'); plt.legend()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\nnewsgroups_train = fetch_20newsgroups(subset='train')\nnewsgroups_test = fetch_20newsgroups(subset='test')\n\ntext=[]\nlabel=[]\ntext.extend(newsgroups_train['data'])\ntext.extend(newsgroups_test['data'])\nlabel.extend(newsgroups_train['target'])\nlabel.extend(newsgroups_test['target'])\n\ndf2=pd.DataFrame({'Text':text,'Label':label})\ndf2['Text']=df2['Text'].apply(lambda x: x.replace('\\n\\n',' ').replace('\\n',' '))\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(df2, test_size=0.1,stratify = df2['Label'], random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dc87384b-710c-4ec4-aec1-48bf8b2439f3","_uuid":"4df404e0692a1d585336e8994b776f5cd1fd0fab"},"cell_type":"markdown","source":"Let's pre-process the text, tokenize it and pad it to a maximum length (as in the figure above)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def selectN_example_eachClass(dataframe, targetColumn = 'Label', N=10, random_state=42):\n    df = pd.DataFrame()\n    for classes in np.unique(dataframe[targetColumn]):\n        proc_df = dataframe.loc[dataframe[targetColumn] == classes,].sample(N, random_state=random_state)\n        df = pd.concat([df,proc_df], axis=0)\n    \n    df = df.sample(frac=1, random_state=random_state)\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['Label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=selectN_example_eachClass(df2, targetColumn='Label', N=1, random_state=42)\ntest_data=df2.drop(train_data.index)\n\nX_train=train_data\nX_test=test_data\n# y_train=train_data['Label']\n# y_test=test_data['Label']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"19aeb800-f1c8-4206-8bf8-323256ff5c14","_uuid":"c600a3f2135982bf6f9b46cce3a97b1e2e5b912c","scrolled":true,"trusted":true},"cell_type":"code","source":"# raw_docs_train = train_df['comment_text'].tolist()\n# raw_docs_test = test_df['comment_text'].tolist() \n# num_classes = len(label_names)\n\nraw_docs_train = X_train['Text'].tolist()\nraw_docs_test = X_test['Text'].tolist() \nnum_classes = 20\n\n\n#visualize word distribution\nX_train['doc_len'] = X_train['Text'].apply(lambda words: len(words.split(\" \")))\nmax_seq_len = np.round(X_train['doc_len'].mean() + X_train['doc_len'].std()).astype(int)\n\nprint(\"pre-processing train data...\")\nprocessed_docs_train = []\nfor doc in tqdm(raw_docs_train):\n    tokens = tokenizer.tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_train.append(\" \".join(filtered))\n#end for\n\nprocessed_docs_test = []\nfor doc in tqdm(raw_docs_test):\n    tokens = tokenizer.tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_test.append(\" \".join(filtered))\n#end for\n\nprint(\"tokenizing input data...\")\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\ntokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\nword_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\nword_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\nword_index = tokenizer.word_index\nprint(\"dictionary size: \", len(word_index))\n\n#pad sequences\nword_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\nword_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3382a8db-f082-48c3-82de-898e7a716c5f","_uuid":"5d5b2e211c3b2a320aaa63c39999e913fe471f4e"},"cell_type":"markdown","source":"Let's define our training and model parameters:"},{"metadata":{"_cell_guid":"d524dcca-8299-46d0-adae-f12f7c1a3171","_uuid":"08e7abf92ce07c856f5e7d6f9ce371997495910b","trusted":true},"cell_type":"code","source":"#training params\nbatch_size = 256 \nnum_epochs = 8 \n\n#model parameters\nnum_filters = 64 \nembed_dim = 300 \nweight_decay = 1e-2 #1e-4","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3b2c9452-5780-4920-8896-0b9dfa7c562b","_uuid":"e8742cc989f4c71a8c401feef5e54c0e7718865a"},"cell_type":"markdown","source":"We can now prepare our embedding matrix limiting to a max number of words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_NB_WORDS","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"18708bcb-0ae5-467f-8725-e2dbb50f989e","_uuid":"630959b52ab4e120581fac11915bf432e40b8b5f","trusted":true},"cell_type":"code","source":"#embedding matrix\nprint('preparing embedding matrix...')\nwords_not_found = []\nnb_words = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_dim))\nfor word, i in word_index.items():\n    if i >= nb_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f1db355a-b0bf-4f58-8565-efb196c5b139","_uuid":"13eccb942d6c0027e6d4f317c9072579c7c747a2"},"cell_type":"markdown","source":"It's interesting to look at the words not found in the embeddings:"},{"metadata":{"_cell_guid":"e6777c80-d2e5-49fb-9d69-75de9f322b6f","_uuid":"fe157163ee51b46f011cd1b5395c36c633c11b18","trusted":true},"cell_type":"code","source":"print(\"sample words not found: \", np.random.choice(words_not_found, 10))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"86182902-7292-46dc-bedb-6aded3526a00","_uuid":"92fa5491dd28f186b9ff4772a4045b4e5d59df8d"},"cell_type":"markdown","source":"We can finally define the CNN architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import GlobalAveragePooling1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(nb_words, embed_dim,\n          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dense(num_classes, activation='softmax'))\nadam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a2f5f71-6777-4ab7-b419-3c867e1ab7d4","_uuid":"4f3484a6fc935b4da6dbe8bc21f6029c99f574bf","trusted":true},"cell_type":"code","source":"# #CNN architecture\n# print(\"training CNN ...\")\n# model = Sequential()\n# model.add(Embedding(nb_words, embed_dim,\n#           weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n# model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n# model.add(MaxPooling1D(2))\n# model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n# model.add(GlobalMaxPooling1D())\n# model.add(Dropout(0.5))\n# model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n# model.add(Dense(num_classes, activation='softmax'))  #multi-label (k-hot encoding)\n\n# adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n# model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n# model.summary()\n\n# # lr=0.001 -> TestAccuracy - 0.75 epochs = 20 TrainSize = 16961, TestSize = 1885 MaxSeqLen =1081\n# # lr=0.01 -> TestAccuracy - 0.82 epochs = 20  TrainSize = 16961, TestSize = 1885 MaxSeqLen =1081\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bcc9c1ac-db54-4356-bce7-0ef39dbaf909","_uuid":"169c30a270b00bf2088afc4268ada3042318adb3"},"cell_type":"markdown","source":"Because of the multi-label loss, we are using k-hot encoding of the output and sigmoid activations. As a result, the loss is binary cross-entropy."},{"metadata":{"_cell_guid":"e439dcfb-c97c-4ae5-b6d4-e48b5c44b919","_uuid":"c48a4db47861146bc4e90b3be6cd23106edb1255","trusted":true},"cell_type":"code","source":"#define callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=10, verbose=1)\ncallbacks_list = [early_stopping]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(word_seq_train.shape)\nprint(word_seq_test.shape)\nprint(max_seq_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#callbacks=callbacks_list,","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"38c67614-db6d-4d57-80d6-e46404bf4147","_uuid":"c1a947e9bc4acefed432c40ec79fa318921278a2","trusted":true},"cell_type":"code","source":"#model training\nhist = model.fit(word_seq_train, to_categorical(X_train['Label'].values), batch_size=batch_size, epochs=5000,  validation_split=0.0, shuffle=True, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #model training\n# hist = model.fit(word_seq_train, to_categorical(X_train['Label'].values), batch_size=batch_size, epochs=20,  validation_split=0.4, shuffle=True, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_len","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dfd0eaee-5f7b-4e6d-b809-dfe1316a30db","_uuid":"b48d3b7623e2ce8e814ed246595c213f4321af64"},"cell_type":"markdown","source":"Let's make predictions on the test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(X_train['Label'], np.argmax(model.predict(word_seq_train), axis=1), target_names=newsgroups_train.target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(X_test['Label'], np.argmax(model.predict(word_seq_test), axis=1), target_names=newsgroups_train.target_names))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"34f04d6b-f37f-4917-9128-1a4d2ddb318a","_uuid":"0739cf53451a26d1d8a7800b18c97ed706fb4b96"},"cell_type":"markdown","source":"Looking at training and validation loss / accuracy figures below, we can see there is no sign of over-fitting."},{"metadata":{"_cell_guid":"89fdc75d-43eb-445c-8588-af0eed779ca3","_uuid":"c70a282f66aa426654d9b02fec4a1f31f59286ab","trusted":true},"cell_type":"code","source":"#generate plots\nplt.figure()\nplt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('CNN News Group Categories')\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9f6fccbb-7888-41c1-99e5-eb1d95f19021","_uuid":"dcf22acf97cd6321d449db27fde638f5d71d68dd","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\nplt.title('CNN News Group Categories')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"febbb068-edfb-4157-8bd8-38d3ae698e92","_uuid":"1bf8b52609c0538f619ac4d6efea04b6491a4673"},"cell_type":"markdown","source":"**References:**\n\n[1] P. Bojanowski, E. Grave, A. Joulin, T. Mikolov, \"Enriching Word Vectors with Subword Information\", arXiv, 2016  \n[2] FastText Embeddings: https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md  \n[3] F. Chollet, \"Deep Learning with Python\", Manning Publications, 2017  "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}