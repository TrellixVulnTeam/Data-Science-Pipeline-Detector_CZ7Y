{"cells":[{"metadata":{"id":"_mjm0TWQ_qN7","outputId":"44d7f3cf-5fd1-432b-d94b-a47c1e085925","trusted":true},"cell_type":"code","source":"# https://deeplearningcourses.com/c/deep-learning-advanced-nlp\nfrom __future__ import print_function, division\nfrom builtins import range\n# Note: you may need to update your version of future\n# sudo pip install -U future\n\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, Input\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import roc_auc_score\n\nimport keras.backend as K\n# if len(K.tensorflow_backend._get_available_gpus()) > 0:\n#   from keras.layers import CuDNNLSTM as LSTM\n#   from keras.layers import CuDNNGRU as GRU\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"6EEe88z8_FJ_","trusted":true},"cell_type":"code","source":"# some configuration\nMAX_SEQUENCE_LENGTH = 100\nMAX_VOCAB_SIZE = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\nBATCH_SIZE = 128\nEPOCHS = 5\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ncqzMIvx_FKF","outputId":"a98619f1-8e67-4fcc-be22-11e5f603e414","trusted":true},"cell_type":"code","source":"# load in pre-trained word vectors\nprint('Loading word vectors...')\nword2vec = {}\nwith open(\"/kaggle/input/glove-vectors/glove.6B.100d.txt\") as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vec = np.asarray(values[1:], dtype='float32')\n    word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"sCP1WA73_FKO","outputId":"d9fa3542-a352-4b31-bc55-58a5282e79da","trusted":true},"cell_type":"code","source":"# prepare text samples and their labels\nprint('Loading in comments...')\n\ntrain = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\nsentences = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\npossible_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntargets = train[possible_labels].values\n","execution_count":null,"outputs":[]},{"metadata":{"id":"xroyB4r2CXa6","outputId":"78b0cfec-477e-44f6-c917-cc8fc4472241","trusted":true},"cell_type":"code","source":"sentences[1]","execution_count":null,"outputs":[]},{"metadata":{"id":"Uik5rNIBCXvw","outputId":"0a82e31d-375a-4dca-a27e-3f51ff56432c","trusted":true},"cell_type":"code","source":"targets[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"0H-pnY_1_FKT","trusted":true},"cell_type":"code","source":"# convert the sentences (strings) into integers\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)","execution_count":null,"outputs":[]},{"metadata":{"id":"LL5JhV92_FKX","outputId":"7b25ac97-21c8-4fef-a430-cfe7a3563e86","trusted":true},"cell_type":"code","source":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"HY5PvXca_FKn","outputId":"318ac36a-3443-47d1-bf46-b85162bdc697","trusted":true},"cell_type":"code","source":"# pad sequences so that we get a N x T matrix\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"gb1Fq9sQ_FLG","outputId":"d4b5835b-4f5b-429c-f2a1-6b615002fbe2","trusted":true},"cell_type":"code","source":"# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx.items():\n  if i < MAX_VOCAB_SIZE:\n    embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n      embedding_matrix[i] = embedding_vector\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"euxNF4YU_FLJ","trusted":true},"cell_type":"code","source":"# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  num_words,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False\n)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Hl9wgpbq_FLM","outputId":"2feba8c8-0b46-47ad-dbeb-428e3ffcb466","trusted":true},"cell_type":"code","source":"# create an LSTM network with a single LSTM\ninput_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\nx = embedding_layer(input_)\nprint(x.shape)\nx = LSTM(15, return_sequences=True)(x)\nprint(x.shape)\n# x = Bidirectional(LSTM(15, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nprint(x.shape)\noutput = Dense(len(possible_labels), activation=\"sigmoid\")(x)\nprint(output.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"WdMNMdvA_FLN","trusted":true},"cell_type":"code","source":"model = Model(input_, output)\nmodel.compile(\n  loss='binary_crossentropy',\n  optimizer=Adam(lr=0.01),\n  metrics=['accuracy'],\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"gBrJOb29_FLP","outputId":"a5eef9cd-4afd-4aa0-a5bf-92176ef898ed","trusted":false},"cell_type":"code","source":"print('Training model...')\nr = model.fit(\n  data,\n  targets,\n  batch_size=BATCH_SIZE,\n  epochs=EPOCHS,\n  validation_split=VALIDATION_SPLIT\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"VgBuMj1mFHir","trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5FUuQ7UsGJ6v","outputId":"2d62dd23-57c1-46b5-931c-3a6c52dca20f","trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"94mD3RytGLm0","outputId":"62744fe2-81f8-487e-f51c-9ed1988ec9cc","trusted":true},"cell_type":"code","source":"# prepare text samples and their labels\nprint('Loading test comments...')\n\ntrain = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\")\nsentences_test = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jBiyjLebGWTa","outputId":"bfa5f7a0-d8e9-457b-941e-cad50bc3df6d","trusted":false},"cell_type":"code","source":"# convert the sentences (strings) into integers\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences_test)\nsentences_test = tokenizer.texts_to_sequences(sentences_test)\nsentences_test","execution_count":null,"outputs":[]},{"metadata":{"id":"I0A9LZxRGmV3","outputId":"931c8702-a87c-4c1c-db2c-9d7a8575727d","trusted":false},"cell_type":"code","source":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"EdcQz3J0G7vS","outputId":"2e01858f-642f-48c8-cf92-3404cb5bb5a7","trusted":false},"cell_type":"code","source":"# pad sequences so that we get a N x T matrix\ndata = pad_sequences(sentences_test, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"mrYhQpC-HFAF","outputId":"9fec1bc3-2b9e-46e8-c4aa-e8588ddf5454","trusted":false},"cell_type":"code","source":"y_pred = model.predict(data)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"id":"H4zZgymGHTv9","outputId":"33776a9a-5071-4558-c680-6ef8ff12e00f","trusted":false},"cell_type":"code","source":"y_pred[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"_3S8we6aHjb0","outputId":"fd16866c-5eee-400b-d363-9d19a3affd0c","trusted":true},"cell_type":"code","source":"id = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\nid.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"LAMzDkPmIdnG","outputId":"c3d3c44f-ada5-4572-9d59-61042268c65d","trusted":false},"cell_type":"code","source":"output = pd.DataFrame({'id': id.id, 'toxic': y_pred[:,0], 'severe_toxic': y_pred[:,1], 'obscene': y_pred[:,2], 'threat': y_pred[:,3], 'insult': y_pred[:,4], 'identity_hate': y_pred[:,5]})\noutput.head()\n# output.to_csv('my_submission1.csv', index=False)\n# print(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ZNVC4ihYJoEb","outputId":"8ce310a0-1187-43a0-d352-b16c9ecbfe55","trusted":false},"cell_type":"code","source":"output.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"PsbhdSDiJrRA","outputId":"894f07c7-9ec4-4941-d667-c96da82a2d39","trusted":false},"cell_type":"code","source":"output.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"id":"e_w2zepIJ2Vq","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}