{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-23T13:01:22.761834Z","iopub.execute_input":"2022-01-23T13:01:22.76258Z","iopub.status.idle":"2022-01-23T13:01:22.775114Z","shell.execute_reply.started":"2022-01-23T13:01:22.762534Z","shell.execute_reply":"2022-01-23T13:01:22.774208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport zipfile\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Activation\nfrom keras.layers import Bidirectional, LSTM, Embedding, GlobalMaxPool1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras import backend as K\nfrom keras import callbacks\nfrom sklearn.model_selection import train_test_split\nimport transformers\nfrom transformers import TFBertModel, BertTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:24.060795Z","iopub.execute_input":"2022-01-23T13:01:24.061328Z","iopub.status.idle":"2022-01-23T13:01:31.414582Z","shell.execute_reply.started":"2022-01-23T13:01:24.06129Z","shell.execute_reply":"2022-01-23T13:01:31.413877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using BERT as strong baseline\n1. prepare data\n2. test [CLS], Avg(last_hidden_state), Glo(last_hidden_state)\n3. conclusion and future work","metadata":{}},{"cell_type":"markdown","source":"### Prepare data","metadata":{}},{"cell_type":"code","source":"# Using zipfile to extract the data\nimport zipfile\n\n\nsamplesub_zip = '../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip'\ntest_zip = '../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip'\ntest_labels_zip = '../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip'\ntrain_zip = '../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip'\n\n# read the zipfile and extract all of them\nfor file_dir in [samplesub_zip, test_zip, test_labels_zip, train_zip]:\n    zip_ref = zipfile.ZipFile(file_dir, 'r')\n    zip_ref.extractall('./jigsawtoxic/')\n    zip_ref.close()\n\nbase_dir = './jigsawtoxic/'\nos.listdir(base_dir)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:31.416051Z","iopub.execute_input":"2022-01-23T13:01:31.416273Z","iopub.status.idle":"2022-01-23T13:01:34.306697Z","shell.execute_reply.started":"2022-01-23T13:01:31.416241Z","shell.execute_reply":"2022-01-23T13:01:34.305881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get df datatype\n\n\ntrain = pd.read_csv(base_dir + 'train.csv')\ntest = pd.read_csv(base_dir + 'test.csv')\ntest_labels = pd.read_csv(base_dir + 'test_labels.csv')\nsample_submission = pd.read_csv(base_dir + 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:34.308148Z","iopub.execute_input":"2022-01-23T13:01:34.308414Z","iopub.status.idle":"2022-01-23T13:01:36.350583Z","shell.execute_reply.started":"2022-01-23T13:01:34.30838Z","shell.execute_reply":"2022-01-23T13:01:36.349829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the data\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:36.352672Z","iopub.execute_input":"2022-01-23T13:01:36.353221Z","iopub.status.idle":"2022-01-23T13:01:36.37641Z","shell.execute_reply.started":"2022-01-23T13:01:36.353183Z","shell.execute_reply":"2022-01-23T13:01:36.375724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:36.37775Z","iopub.execute_input":"2022-01-23T13:01:36.378157Z","iopub.status.idle":"2022-01-23T13:01:36.391316Z","shell.execute_reply.started":"2022-01-23T13:01:36.378121Z","shell.execute_reply":"2022-01-23T13:01:36.390564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape, test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:36.394901Z","iopub.execute_input":"2022-01-23T13:01:36.398005Z","iopub.status.idle":"2022-01-23T13:01:36.406226Z","shell.execute_reply.started":"2022-01-23T13:01:36.397949Z","shell.execute_reply":"2022-01-23T13:01:36.40546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:36.408324Z","iopub.execute_input":"2022-01-23T13:01:36.409923Z","iopub.status.idle":"2022-01-23T13:01:36.435667Z","shell.execute_reply.started":"2022-01-23T13:01:36.409881Z","shell.execute_reply":"2022-01-23T13:01:36.435012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see the classification distribution\n\nfor list_columns in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(train[list_columns].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:36.439214Z","iopub.execute_input":"2022-01-23T13:01:36.439537Z","iopub.status.idle":"2022-01-23T13:01:36.467766Z","shell.execute_reply.started":"2022-01-23T13:01:36.439501Z","shell.execute_reply":"2022-01-23T13:01:36.467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model buliding\n","metadata":{}},{"cell_type":"code","source":"# 模型\n\nclass MultiLabelBert(keras.Model):\n    def __init__(self, num_labels, token_used='CLS', add_tfidf=False):\n        super(MultiLabelBert, self).__init__()\n        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n        self.classifier = keras.layers.Dense(units=num_labels, activation='sigmoid')\n        self.add_tfidf = False\n        if add_tfidf:\n            self.add_tfidf = True\n            self.concate_layer = keras.layers.Concatenate(axis=-1)\n        self.token_used = token_used\n        assert(self.token_used in ['CLS', 'AVG', 'GLO'])\n        self.avg_pooling = keras.layers.GlobalAveragePooling1D()\n        self.glo_pooling = keras.layers.GlobalMaxPool1D()\n    \n    def call(self, x):\n        if self.token_used == 'CLS':\n            bert_embedding = self.bert(x)['pooler_output']\n        elif self.token_used == 'AVG':\n            bert_embedding = self.avg_pooling(self.bert(x)['last_hidden_state'])\n        else:\n            bert_embedding = self.glo_pooling(self.bert(x)['last_hidden_state'])\n        if self.add_tfidf:\n            tfidf_embedding = x['tfidf']\n            all_embedding = self.concate_layer([bert_embedding, tfidf_embedding])\n            return self.classifier(all_embedding)\n        # print(cls_token_embedding.shape)\n        return self.classifier(bert_embedding)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:36.469016Z","iopub.execute_input":"2022-01-23T13:01:36.469779Z","iopub.status.idle":"2022-01-23T13:01:36.480303Z","shell.execute_reply.started":"2022-01-23T13:01:36.469741Z","shell.execute_reply":"2022-01-23T13:01:36.47951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_example_to_feature(tokenizer, review):\n    \"\"\"\n        透過 Tokenizer 編碼成 BERT 輸入(dict)\n    \"\"\"\n    return tokenizer.encode_plus(\n        review,\n        add_special_tokens=True,\n        max_length=128,\n        pad_to_max_length=True,\n        return_attention_mask=True\n    )\n\n\ndef map_example_to_dict(input_ids, attention_masks, token_type_ids, label, tf_idf=None):\n    \"\"\"\n        將list of inputs 轉換成dict可以對應的BERT的輸入層\n    \"\"\"\n    if tf_idf:\n        return {\n          \"input_ids\": input_ids,\n          \"token_type_ids\": token_type_ids,\n          \"attention_mask\": attention_masks,\n          \"tf-idf\": tf_idf\n        }, label\n    \n    return {\n      \"input_ids\": input_ids,\n      \"token_type_ids\": token_type_ids,\n      \"attention_mask\": attention_masks,\n    }, label\n\n\ndef encode_examples(ds, limit=-1, add_tfidf=False):\n    \"\"\"\n        透過list 包裝，之後搭配 dict轉換。\n    \"\"\"\n    input_ids_list = []\n    token_type_ids_list = []\n    attention_mask_list = []\n    label_list = []\n    if add_tfidf:\n        tfidf_list = []\n        \n    if (limit > 0):\n        ds = ds.take(limit)    # DataFrame.take(indices, axis=0, is_copy=None, **kwargs): Return the elements in the given positional indices along an axis.\n    \n    for index, row in ds.iterrows():\n        review = row[\"comment_text\"]\n        label = row[\"y\"]\n        if add_tfidf:\n            tfidf_list.append(row[\"tfidf\"])\n\n        bert_input = convert_example_to_feature(tokenizer, review)\n  \n        input_ids_list.append(bert_input['input_ids'])\n        token_type_ids_list.append(bert_input['token_type_ids'])\n        attention_mask_list.append(bert_input['attention_mask'])\n        label_list.append(label)\n    if add_tfidf:\n        return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list, tfidf_list)).map(map_example_to_dict)\n    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:36.484101Z","iopub.execute_input":"2022-01-23T13:01:36.484645Z","iopub.status.idle":"2022-01-23T13:01:36.498331Z","shell.execute_reply.started":"2022-01-23T13:01:36.484611Z","shell.execute_reply":"2022-01-23T13:01:36.497526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:36.49971Z","iopub.execute_input":"2022-01-23T13:01:36.500064Z","iopub.status.idle":"2022-01-23T13:01:41.0088Z","shell.execute_reply.started":"2022-01-23T13:01:36.500024Z","shell.execute_reply":"2022-01-23T13:01:41.008155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_labels = 6\nglo_bert = MultiLabelBert(num_labels, 'GLO')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:01:41.010169Z","iopub.execute_input":"2022-01-23T13:01:41.010426Z","iopub.status.idle":"2022-01-23T13:02:00.964791Z","shell.execute_reply.started":"2022-01-23T13:01:41.010391Z","shell.execute_reply":"2022-01-23T13:02:00.964127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 因為是多標籤，將y包打好\ntrain['y'] = 0\ntrain['y'] = train['y'].apply(func=lambda x: [])\n\n\n    \nfor i in range(len(train)):\n    for j in range(2, len(train.columns)-1):\n        train.iloc[i, -1].append(train.iloc[i, j])\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:02:00.96622Z","iopub.execute_input":"2022-01-23T13:02:00.966466Z","iopub.status.idle":"2022-01-23T13:02:58.305215Z","shell.execute_reply.started":"2022-01-23T13:02:00.966432Z","shell.execute_reply":"2022-01-23T13:02:58.304492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nprint(train.shape)\ntrain = shuffle(train)\n\nnums =  int(len(train) * 0.9)\n\n# 90% train data\nval_set = train[nums:][['comment_text', 'y']]\ntrain_set = train[:nums][['comment_text', 'y']]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:02:58.306481Z","iopub.execute_input":"2022-01-23T13:02:58.306729Z","iopub.status.idle":"2022-01-23T13:02:58.376463Z","shell.execute_reply.started":"2022-01-23T13:02:58.306693Z","shell.execute_reply":"2022-01-23T13:02:58.375706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n\n# train dataset\nds_train_encoded = encode_examples(train_set).batch(batch_size)\n# val dataset\nds_val_encoded = encode_examples(val_set).batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:02:58.37763Z","iopub.execute_input":"2022-01-23T13:02:58.378434Z","iopub.status.idle":"2022-01-23T13:12:53.72345Z","shell.execute_reply.started":"2022-01-23T13:02:58.378395Z","shell.execute_reply":"2022-01-23T13:12:53.722691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_sentence_to_features(tokenizer, text):\n    return tokenizer.encode_plus(\n        text,\n        max_length=128,\n        add_special_tokens=True,\n        pad_to_max_length=True,\n        return_attention_mask=True\n    )\n\n\ndef map_datarow_to_dict(input_ids, token_type_ids, attention_mask, tfidf=None):\n    if tfidf:\n        return {\n            \"input_ids\": input_ids,\n            \"token_type_ids\": token_type_ids,\n            \"attention_mask\": attention_mask,\n            \"tfidf\": tfidf\n        }\n    return {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"attention_mask\": attention_mask,\n    }\n\n\ndef encode_examples_test(ds, limit=-1, add_tfidf=False):\n    input_ids_list, attention_mask_list, token_type_ids_list, label_list = [], [], [], []\n    if add_tfidf:\n        tfidf_list = []\n    if(limit > 0):\n        ds = ds.take(limit)\n    for idx, row in ds.iterrows():\n        text = row['comment_text']\n        if add_tfidf:\n            tfidf_list.append(row['tfidf'])\n        bert_input = convert_sentence_to_features(tokenizer, text)\n        input_ids_list.append(bert_input['input_ids'])\n        token_type_ids_list.append(bert_input['token_type_ids'])        \n        attention_mask_list.append(bert_input['attention_mask'])\n    if add_tfidf:\n        return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, tfidf_list)).map(map_datarow_to_dict)\n    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list)).map(map_datarow_to_dict)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T15:04:26.084556Z","iopub.execute_input":"2022-01-23T15:04:26.084856Z","iopub.status.idle":"2022-01-23T15:04:26.098398Z","shell.execute_reply.started":"2022-01-23T15:04:26.084822Z","shell.execute_reply":"2022-01-23T15:04:26.097614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\nds_test_encoded = encode_examples_test(test).batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T15:04:27.383174Z","iopub.execute_input":"2022-01-23T15:04:27.383449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in ds_val_encoded.take(1):\n    print(x)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:12:53.724634Z","iopub.execute_input":"2022-01-23T13:12:53.724886Z","iopub.status.idle":"2022-01-23T13:12:53.794239Z","shell.execute_reply.started":"2022-01-23T13:12:53.724853Z","shell.execute_reply":"2022-01-23T13:12:53.791786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_labels = 6\nlearning_rate = 2e-5\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08, clipnorm=1)\nglo_bert.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\nglo_bert.fit(ds_train_encoded, validation_data=ds_val_encoded, epochs=3)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T13:12:53.795593Z","iopub.execute_input":"2022-01-23T13:12:53.79583Z","iopub.status.idle":"2022-01-23T14:59:21.357594Z","shell.execute_reply.started":"2022-01-23T13:12:53.795797Z","shell.execute_reply":"2022-01-23T14:59:21.356901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_labels = 6\nlearning_rate = 2e-5\n\navg_bert = MultiLabelBert(num_labels, 'AVG')\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08, clipnorm=1)\navg_bert.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\navg_bert.fit(ds_train_encoded, validation_data=ds_val_encoded, epochs=3)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T14:59:21.359143Z","iopub.execute_input":"2022-01-23T14:59:21.359395Z","iopub.status.idle":"2022-01-23T14:59:30.284234Z","shell.execute_reply.started":"2022-01-23T14:59:21.359361Z","shell.execute_reply":"2022-01-23T14:59:30.281848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_labels = 6\nlearning_rate = 2e-5\n\ncls_bert = MultiLabelBert(num_labels, 'CLS')\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08, clipnorm=1)\ncls_bert.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\ncls_bert.fit(ds_train_encoded, validation_data=ds_val_encoded, epochs=3)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T14:59:30.285052Z","iopub.status.idle":"2022-01-23T14:59:30.285332Z","shell.execute_reply.started":"2022-01-23T14:59:30.285186Z","shell.execute_reply":"2022-01-23T14:59:30.285206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = cls_bert.predict(ds_test_encoded, batch_size=batch_size)\nsample_submission[data_labels] = y_test\nsample_submission.to_csv('cls_bert_submission.csv', index=False)\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2022-01-23T14:59:30.286585Z","iopub.status.idle":"2022-01-23T14:59:30.286865Z","shell.execute_reply.started":"2022-01-23T14:59:30.286721Z","shell.execute_reply":"2022-01-23T14:59:30.286739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = avg_bert.predict(ds_test_encoded, batch_size=batch_size)\nsample_submission[data_labels] = y_test\nsample_submission.to_csv('avg_bert_submission.csv', index=False)\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2022-01-23T14:59:30.289207Z","iopub.status.idle":"2022-01-23T14:59:30.28983Z","shell.execute_reply.started":"2022-01-23T14:59:30.28959Z","shell.execute_reply":"2022-01-23T14:59:30.289615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = glo_bert.predict(ds_test_encoded, batch_size=batch_size)\nsample_submission[data_labels] = y_test\nsample_submission.to_csv('glo_bert_submission.csv', index=False)\nsample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n- It's a interesting experiment to compare with [CLS]、Average [last_hidden_state]、Global [last_hidden_state].\n- The strong start method, but it needs lots of computation power to train a model(compared to simple CNN、RNN model).\n- Hope this work would help others to start a transformer-based model to start the competition.\n\n---\n\n## Future work\n- data part\n    - data preprocessing\n        - There are lots of noise in comment, like ==, @@.\n    - max_length:\n        - The tokenizer's max_length is a parameter needed to adjust because it is possible to drop necessary tokens.\n    - split method:\n        - The current split method is simple, and the data class labels are obviously unbalanced.\n    - data augmentation:\n        - We could use data augmentation to let model more robust. There are lots of method in NLP, like wordnet(probability not suitable in this case), [pre-trained NLP aug model](https://arxiv.org/abs/2003.02245)\n- model part\n    - BERT\n        - BERT is powerful, but nowadays there are lots of model based on BERT to go further in some features like speed( [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) ), hyperparameter ( [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)), or other language model ( [XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet) )\n        - optimizer: The optimizer, learning rate, and other hyperparameters are all adjustable, but the model is too big to run several experiments.\n        ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}