{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install gast==0.2.2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, math, operator, csv, random, pickle,re\nimport tensorflow as tf\nimport pandas as pd\nimport gc\n\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MaxPooling1D, BatchNormalization, Permute, Lambda, Activation, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Dense, Embedding, Dropout, Input, CuDNNGRU, CuDNNLSTM, Flatten, TimeDistributed, concatenate, SpatialDropout1D, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport gc\nfrom tensorflow.keras import backend as K\n\nfrom nltk.tokenize import TweetTokenizer\n\nfrom unidecode import unidecode\n\nfrom sklearn.model_selection import KFold, train_test_split\n\nimport numpy as np\n\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom tensorflow.keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU, Conv1D\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nprint(tf.__version__)\ntf.test.is_gpu_available(\n    cuda_only=False,\n    min_cuda_compute_capability=None\n)\n\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import get_custom_objects\n\nfrom tensorflow.keras.layers import Activation\n\nclass Mish(Activation):\n    '''\n    Mish Activation Function.\n    .. math::\n        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n    Shape:\n        - Input: Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n        - Output: Same shape as the input.\n    Examples:\n        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n    '''\n\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\n\n\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n\n\nget_custom_objects().update({'Mish': Mish(mish)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\nMAX_LEN = 220\n\n\nTEXT_COLUMN = 'comment_text'\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\nsubmission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n    \ndata_folder = \"../input/jigsaw-toxic-comment-classification-challenge/\"\npretrained_folder = \"../input/\"\ntrain_filepath = data_folder + \"train.csv\"\ntest_filepath = data_folder + \"test.csv\"\n\n#path to a submission\nsubmission_path =  data_folder + \"submission.csv\"\n\n#path to pretrained vectors\nfacebook_filepath = pretrained_folder + \"fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\ngoogle_filepath = pretrained_folder + \"glove840b300dtxt/glove.840B.300d.txt\"\n\n\nembedding_dim_facebook = 300\nembedding_dim_google = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#paths to pretrained dictionaries\nhyphens_filepath = \"../input/cleaning-dictionaries/hyphens_dictionary.bin\"\nmisspellings_filepath = \"../input/cleaning-dictionaries/misspellings_all_dictionary.bin\"\nmerged_filepath = \"../input/cleaning-dictionaries/merged_all_dictionary.bin\"\n\nhyphens_dict = misspellings_dict = merged_dict = {}\nwith open(hyphens_filepath, mode='rb') as file: hyphens_dict = pickle.load(file)\nwith open(misspellings_filepath, mode='rb') as file: misspellings_dict = pickle.load(file)\nwith open(merged_filepath, mode='rb') as file: merged_dict = pickle.load(file)\n    \nprint(len(hyphens_dict))\nprint(len(misspellings_dict))\nprint(len(merged_dict)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_samples_count = 149571\nvalidation_samples_count = 10000\n\nlength_threshold = 20000 #We are going to truncate a comment if its length > threshold\nword_count_threshold = 900 #We are going to truncate a comment if it has more words than our threshold\nwords_limit = 310000\n\n#We will filter all characters except alphabet characters and some punctuation\nvalid_characters = \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\" + \"abcdefghijklmnopqrstuvwxyz\".upper()\nvalid_characters_ext = valid_characters + \"abcdefghijklmnopqrstuvwxyz\".upper()\nvalid_set = set(x for x in valid_characters)\nvalid_set_ext = set(x for x in valid_characters_ext)\n\n#List of some words that often appear in toxic comments\n#Sorry about the level of toxicity in it!\ntoxic_words = [\"poop\", \"crap\", \"prick\", \"twat\", \"wikipedia\", \"wiki\", \"hahahahaha\", \"lol\", \"bastard\", \"sluts\", \"slut\", \"douchebag\", \"douche\", \"blowjob\", \"nigga\", \"dumb\", \"jerk\", \"wanker\", \"wank\", \"penis\", \"motherfucker\", \"fucker\", \"fuk\", \"fucking\", \"fucked\", \"fuck\", \"bullshit\", \"shit\", \"stupid\", \"bitches\", \"bitch\", \"suck\", \"cunt\", \"dick\", \"cocks\", \"cock\", \"die\", \"kill\", \"gay\", \"jewish\", \"jews\", \"jew\", \"niggers\", \"nigger\", \"faggot\", \"fag\", \"asshole\"]\nastericks_words = [('mother****ers', 'motherfuckers'), ('motherf*cking', 'motherfucking'), ('mother****er', 'motherfucker'), ('motherf*cker', 'motherfucker'), ('bullsh*t', 'bullshit'), ('f**cking', 'fucking'), ('f*ucking', 'fucking'), ('fu*cking', 'fucking'), ('****ing', 'fucking'), ('a**hole', 'asshole'), ('assh*le', 'asshole'), ('f******', 'fucking'), ('f*****g', 'fucking'), ('f***ing', 'fucking'), ('f**king', 'fucking'), ('f*cking', 'fucking'), ('fu**ing', 'fucking'), ('fu*king', 'fucking'), ('fuc*ers', 'fuckers'), ('f*****', 'fucking'), ('f***ed', 'fucked'), ('f**ker', 'fucker'), ('f*cked', 'fucked'), ('f*cker', 'fucker'), ('f*ckin', 'fucking'), ('fu*ker', 'fucker'), ('fuc**n', 'fucking'), ('ni**as', 'niggas'), ('b**ch', 'bitch'), ('b*tch', 'bitch'), ('c*unt', 'cunt'), ('f**ks', 'fucks'), ('f*ing', 'fucking'), ('ni**a', 'nigga'), ('c*ck', 'cock'), ('c*nt', 'cunt'), ('cr*p', 'crap'), ('d*ck', 'dick'), ('f***', 'fuck'), ('f**k', 'fuck'), ('f*ck', 'fuck'), ('fc*k', 'fuck'), ('fu**', 'fuck'), ('fu*k', 'fuck'), ('s***', 'shit'), ('s**t', 'shit'), ('sh**', 'shit'), ('sh*t', 'shit'), ('tw*t', 'twat')]\nfasttext_misspelings = {\"'n'balls\": 'balls', \"-nazi's\": 'nazis', 'adminabuse': 'admin abuse', \"admins's\": 'admins', 'arsewipe': 'arse wipe', 'assfack': 'asshole', 'assholifity': 'asshole', 'assholivity': 'asshole', 'asshoul': 'asshole', 'asssholeee': 'asshole', 'belizeans': 'mexicans', \"blowing's\": 'blowing', 'bolivians': 'mexicans', 'celtofascists': 'fascists', 'censorshipmeisters': 'censor', 'chileans': 'mexicans', 'clerofascist': 'fascist', 'cowcrap': 'crap', 'crapity': 'crap', \"d'idiots\": 'idiots', 'deminazi': 'nazi', 'dftt': \"don't feed the troll\", 'dildohs': 'dildo', 'dramawhores': 'drama whores', 'edophiles': 'pedophiles', 'eurocommunist': 'communist', 'faggotkike': 'faggot', 'fantard': 'retard', 'fascismnazism': 'fascism', 'fascistisized': 'fascist', 'favremother': 'mother', 'fuxxxin': 'fucking', \"g'damn\": 'goddamn', 'harassmentat': 'harassment', 'harrasingme': 'harassing me', 'herfuc': 'motherfucker', 'hilterism': 'fascism', 'hitlerians': 'nazis', 'hitlerites': 'nazis', 'hubrises': 'pricks', 'idiotizing': 'idiotic', 'inadvandals': 'vandals', \"jackass's\": 'jackass', 'jiggabo': 'nigga', 'jizzballs': 'jizz balls', 'jmbass': 'dumbass', 'lejittament': 'legitimate', \"m'igger\": 'nigger', \"m'iggers\": 'niggers', 'motherfacking': 'motherfucker', 'motherfuckenkiwi': 'motherfucker', 'muthafuggas': 'niggas', 'nazisms': 'nazis', 'netsnipenigger': 'nigger', 'niggercock': 'nigger', 'niggerspic': 'nigger', 'nignog': 'nigga', 'niqqass': 'niggas', \"non-nazi's\": 'not a nazi', 'panamanians': 'mexicans', 'pedidiots': 'idiots', 'picohitlers': 'hitler', 'pidiots': 'idiots', 'poopia': 'poop', 'poopsies': 'poop', 'presumingly': 'obviously', 'propagandaanddisinformation': 'propaganda and disinformation', 'propagandaministerium': 'propaganda', 'puertoricans': 'mexicans', 'puertorricans': 'mexicans', 'pussiest': 'pussies', 'pussyitis': 'pussy', 'rayaridiculous': 'ridiculous', 'redfascists': 'fascists', 'retardzzzuuufff': 'retard', \"revertin'im\": 'reverting', 'scumstreona': 'scums', 'southamericans': 'mexicans', 'strasserism': 'fascism', 'stuptarded': 'retarded', \"t'nonsense\": 'nonsense', \"threatt's\": 'threat', 'titoists': 'communists', 'twatbags': 'douchebags', 'youbollocks': 'you bollocks'}\nacronym_words = {} #{\"btw\":\"by the way\", \"yo\": \"you\", \"u\": \"you\", \"r\": \"are\", \"ur\": \"your\", \"ima\": \"i am going to\", \"imma\": \"i am going to\", \"i'ma\":\"i am going to\", \"cos\":\"because\", \"coz\":\"because\", \"stfu\": \"shut the fuck up\", \"wat\": \"what\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_patterns = [\n    (r'(W|w)on\\'t', r'will not'),\n    (r'(C|c)an\\'t', r'can not'),\n    (r'(I|i)\\'m', r'i am'),\n    (r'(A|a)in\\'t', r'is not'),\n    (r'(\\w+)\\'ll', r'\\g<1> will'),\n    (r'(\\w+)n\\'t', r'\\g<1> not'),\n    (r'(\\w+)\\'ve', r'\\g<1> have'),\n    (r'(\\w+)\\'s', r'\\g<1> is'),\n    (r'(\\w+)\\'re', r'\\g<1> are'),\n    (r'(\\w+)\\'d', r'\\g<1> would'),\n]\npatterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n\ndef split_word(word, toxic_words):\n    if word == \"\":\n        return \"\"\n    \n    lower = word.lower()\n    for toxic_word in toxic_words:\n        start = lower.find(toxic_word)\n        if start >= 0:\n            end = start + len(toxic_word)\n            result = \" \".join([word[0:start], word[start:end], split_word(word[end:], toxic_words)])\n            return result.replace(\"  \", \" \").strip()\n    return word\n\ntknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\ndef word_tokenize(sentence):\n    sentence = sentence.replace(\"$\", \"s\")\n    sentence = sentence.replace(\"@\", \"a\")    \n    sentence = sentence.replace(\"!\", \" ! \")\n    sentence = sentence.replace(\"?\", \" ? \")\n    \n    return tknzr.tokenize(sentence)\n\ndef replace_url(word):\n    if \"http://\" in word or \"www.\" in word or \"https://\" in word or \"wikipedia.org\" in word:\n        return \"\"\n    return word\n\ndef normalize_by_dictionary(normalized_word, dictionary):\n    result = []\n    for word in normalized_word.split():\n        if word == word.upper():\n            if word.lower() in dictionary:\n                result.append(dictionary[word.lower()].upper())\n            else:\n                result.append(word)\n        else:\n            if word.lower() in dictionary:\n                result.append(dictionary[word.lower()])\n            else:\n                result.append(word)\n    \n    return \" \".join(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom spacy.symbols import nsubj, VERB, dobj\nimport spacy\nnlp = spacy.load('en')\n\ndef normalize_comment(comment):\n    comment = unidecode(comment)\n    comment = comment[:length_threshold]\n    \n    normalized_words = []\n    \n    for w in astericks_words:\n        if w[0] in comment:\n            comment = comment.replace(w[0], w[1])\n        if w[0].upper() in comment:\n            comment = comment.replace(w[0].upper(), w[1].upper())\n    \n    for word in word_tokenize(comment):\n        #for (pattern, repl) in patterns:\n        #    word = re.sub(pattern, repl, word)\n\n        if word == \".\" or word == \",\":\n            normalized_words.append(word)\n            continue\n        \n        word = replace_url(word)\n        if word.count(\".\") == 1:\n            word = word.replace(\".\", \" \")\n        filtered_word = \"\".join([x for x in word if x in valid_set])\n                    \n        #Kind of hack: for every word check if it has a toxic word as a part of it\n        #If so, split this word by swear and non-swear part.\n        normalized_word = split_word(filtered_word, toxic_words)\n        normalized_word = normalize_by_dictionary(normalized_word, hyphens_dict)\n        normalized_word = normalize_by_dictionary(normalized_word, merged_dict)\n        normalized_word = normalize_by_dictionary(normalized_word, misspellings_dict)\n        normalized_word = normalize_by_dictionary(normalized_word, fasttext_misspelings)\n        normalized_word = normalize_by_dictionary(normalized_word, acronym_words)\n\n        normalized_words.append(normalized_word)\n        \n    normalized_comment = \" \".join(normalized_words)\n    \n    result = []\n    for word in normalized_comment.split():\n        if word.upper() == word:\n            result.append(word)\n        else:\n            result.append(word.lower())\n    \n    #apparently, people on wikipedia love to talk about sockpuppets :-)\n    result = \" \".join(result)\n    if \"sock puppet\" in result:\n        result = result.replace(\"sock puppet\", \"sockpuppet\")\n    \n    if \"SOCK PUPPET\" in result:\n        result = result.replace(\"SOCK PUPPET\", \"SOCKPUPPET\")\n    \n    return result\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data_files(train_filepath, test_filepath):\n    #read train data\n    train = pd.read_csv(train_filepath)\n\n\n    labels = train[categories].values\n    \n    #read test data\n    test = pd.read_csv(test_filepath)\n\n    test_comments = test[\"comment_text\"].fillna(\"_na_\").values\n\n    #normalize comments\n    np_normalize = np.vectorize(normalize_comment)\n    comments = train[\"comment_text\"].fillna(\"_na_\").values\n    normalized_comments = np_normalize(comments)\n    del comments\n    gc.collect()\n\n    \n    comments = test[\"comment_text\"].fillna(\"_na_\").values\n    normalized_test_comments = np_normalize(test_comments)\n    del comments\n    gc.collect()\n       \n\n    print('Shape of data tensor:', normalized_comments.shape)\n    print('Shape of label tensor:', labels.shape)\n    print('Shape of test data tensor:', normalized_test_comments.shape)\n    \n    return (labels, normalized_comments, normalized_test_comments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlabels, x_train, x_test = read_data_files(train_filepath, test_filepath) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\nfileObject = open('../input/cleaning-dictionaries/tokenizer','rb')  \n# load the object from the file into var b\ntokenizer = pickle.load(fileObject)  \n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembedding_matrix = np.load('../input/embedding-2/embedding_matrix_big.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(x_train, labels, test_size = 0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_matrix):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='Mish')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='Mish')(hidden)])\n    result = Dense(6, activation='sigmoid')(hidden)\n    \n    \n    model = Model(inputs=words, outputs=result)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 5\nSEEDS = 10\n\npred = 0\n\nfor ii in range(SEEDS):\n    model = build_model(embedding_matrix)\n    for global_epoch in range(EPOCHS):\n        print(global_epoch)\n        model.fit(\n                    x_train,\n                    y_train,\n                    validation_data = (x_valid, y_valid),\n                    batch_size=128,\n                    epochs=1,\n                    verbose=2,\n                    callbacks=[\n                        LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n                    ]\n                )\n        val_preds = model.predict(x_valid)\n        AUC = 0\n        for i in range(6):\n             AUC += roc_auc_score(y_valid[:,i], val_preds[:,i])/6.\n        print(AUC)\n\n    pred += model.predict(x_test, batch_size = 1024, verbose = 1)/SEEDS\n    np.save('pred', pred)\n    model.save_weights('model_weights_'+str(ii)+'.h5')\n    os.system('gzip '+'model_weights_'+str(ii)+'.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission[list_classes] = (pred)\nsubmission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}