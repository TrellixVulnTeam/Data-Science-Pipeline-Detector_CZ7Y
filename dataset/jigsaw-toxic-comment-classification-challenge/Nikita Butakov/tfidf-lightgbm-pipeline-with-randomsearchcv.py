# This search can take a while to run
# Increase n_iter_search to more exhaustivly sample the parameter space
# Increase n_folds to perform more accurate CV
# Increase max_vec_features to increase the size of the feature space
# Play around with the parameter distribution, there are other LightGBM variables not written down here

import numpy as np
import pandas as pd

# Define parameter distribution over which to perform CV

param_dist = {
    "vect__word_vect__ngram_range":  [(1, 1), (1, 2), (1, 3)],
    "vect__char_vect__ngram_range":  [(1, 1), (1, 2), (1, 3)],
    "clf__n_estimators":             np.arange(2, 10, 2),
    "clf__learning_rate":            [0.1, 0.5, 1],
    "clf__num_leaves":               [64, 128, 256, 512, 1024],
    "clf__max_bin":                  [4, 8, 16, 32, 64],
    "clf__subsample":                np.arange(0.5, 1.0, 0.1),
    "clf__subsample_freq":           [1, 2, 4, 8, 16, 32, 64, 128, 256],
    "clf__reg_lambda":               [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 0]
}


n_iter_search = 3 # Define number of search iterations

n_folds = 2 # Define number of CV folds

max_vec_features = 100 # Max number of features generated by TFIDF

#################################################################################################################
#################################################################################################################
#################################################################################################################

# Import Packages

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, RandomizedSearchCV
from scipy.sparse import hstack
from scipy.special import logit, expit
from sklearn.pipeline import Pipeline, FeatureUnion
from time import time
from lightgbm import LGBMClassifier;

# Import Data

class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

train = pd.read_csv('../input/train.csv').fillna(' ')
test = pd.read_csv('../input/test.csv').fillna(' ')

train_text = train['comment_text']
test_text = test['comment_text']

all_text = pd.concat([train_text, test_text])

# Define preprocessing and classification pipeline

word_vectorizer = TfidfVectorizer(sublinear_tf=True,
                                  strip_accents='unicode',
                                  analyzer='word',
                                  token_pattern=r'\w{1,}',
                                  max_features=max_vec_features)

char_vectorizer = TfidfVectorizer(sublinear_tf=True,
                                  strip_accents='unicode',
                                  analyzer='char',
                                  max_features=max_vec_features)

classifier = LGBMClassifier()

word_char_vectorizer = FeatureUnion([
    ('word_vect', word_vectorizer),
    ('char_vect', char_vectorizer),
     ])
     
pipeline = Pipeline([
    ('vect', word_char_vectorizer),
    ('clf', classifier),
     ])
     
random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=n_iter_search, cv=n_folds, 
                                   scoring='roc_auc', n_jobs=-1, verbose=2)

start = time()

random_search.fit(train['comment_text'].values, train[class_names[0]].values)

print("RandomizedSearchCV took %.2f seconds for %d candidates"
      " parameter settings." % ((time() - start), n_iter_search))
      
print("\nBest Score = " + str(random_search.best_score_))

print("\nBest Parameters = " + str(random_search.best_params_))

losses = []
test_predictions  = {'id': test['id']}

for class_name in class_names:
    start = time()
    train_target = train[class_name]
    classifier = pipeline.set_params(vect__word_vect__ngram_range = random_search.best_params_['vect__word_vect__ngram_range'], 
                                     vect__char_vect__ngram_range = random_search.best_params_['vect__char_vect__ngram_range'], 
                                     clf__n_estimators            = random_search.best_params_['clf__n_estimators'], 
                                     clf__learning_rate           = random_search.best_params_['clf__learning_rate'], 
                                     clf__num_leaves              = random_search.best_params_['clf__num_leaves'], 
                                     clf__max_bin                 = random_search.best_params_['clf__max_bin'], 
                                     clf__subsample               = random_search.best_params_['clf__subsample'], 
                                     clf__subsample_freq          = random_search.best_params_['clf__subsample_freq'], 
                                     clf__reg_lambda              = random_search.best_params_['clf__reg_lambda'] 
                                     )
                                     
    cv_loss = np.mean(cross_val_score(classifier, train['comment_text'].values, train_target, cv=n_folds, scoring='roc_auc'))
    losses.append(cv_loss)
    print('CV score for class {} is {}'.format(class_name, cv_loss))

    classifier.fit(train['comment_text'].values, train_target)
    test_predictions[class_name] = classifier.predict_proba(test['comment_text'].values)[:, 1]

    print("Fitting and predictions took %.2f seconds" % (time() - start) )

print('Total CV score is {}'.format(np.mean(losses)))

test_submission = pd.DataFrame.from_dict(test_predictions)
test_submission.to_csv('submission.csv', index=False)