# This search can take a while to run
# Increase n_iter_search to more exhaustivly sample the parameter space
# Increase n_folds to perform more accurate CV
# Increase max_vec_features to increase the size of the feature space
# Play around with the parameter distribution, there are other variables not written down here
# Kernel time limitation is annoying, better to use a local machine if possible (with more search iterations, folds, max_vec_features)

import numpy as np
import pandas as pd

# Define parameter distribution over which to perform CV

param_dist = {
    "vect__word_vect__ngram_range":  [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5)],
    "vect__char_vect__ngram_range":  [(1, 1), (1, 3), (1, 5), (2, 3), (2, 5), (3, 5)],
    "clf__C":                        [0.1, 0.5, 1, 5, 10]
}

n_iter_search = 2 # Define number of search iterations

n_folds = 2 # Define number of CV folds

max_vec_features = 100 # Max number of features generated by TFIDF

#################################################################################################################
#################################################################################################################
#################################################################################################################

# Import Packages

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, RandomizedSearchCV
from scipy.sparse import hstack
from scipy.special import logit, expit
from sklearn.pipeline import Pipeline, FeatureUnion
from time import time

# Import Data

class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

train = pd.read_csv('../input/train.csv').fillna(' ')
test = pd.read_csv('../input/test.csv').fillna(' ')

train_text = train['comment_text']
test_text = test['comment_text']

all_text = pd.concat([train_text, test_text])

# Define preprocessing and classification pipeline

word_vectorizer = TfidfVectorizer(sublinear_tf=True,
                                  strip_accents='unicode',
                                  analyzer='word',
                                  token_pattern=r'\w{1,}',
                                  max_features=max_vec_features)

char_vectorizer = TfidfVectorizer(sublinear_tf=True,
                                  strip_accents='unicode',
                                  analyzer='char',
                                  max_features=max_vec_features)

classifier = LogisticRegression(solver='sag')

word_char_vectorizer = FeatureUnion([
    ('word_vect', word_vectorizer),
    ('char_vect', char_vectorizer),
     ])
     
pipeline = Pipeline([
    ('vect', word_char_vectorizer),
    ('clf', classifier),
     ])
     
random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=n_iter_search, cv=n_folds, 
                                   scoring='roc_auc', n_jobs=-1, verbose=2)

start = time()

# perform a random search with the 'toxic' class, eventually will want to implement this to efficiently work over all classes simultaneously
random_search.fit(train['comment_text'].values, train[class_names[0]].values)

print("RandomizedSearchCV took %.2f seconds for %d candidates"
      " parameter settings." % ((time() - start), n_iter_search))
      
print("\nBest Score = " + str(random_search.best_score_))

print("\nBest Parameters = " + str(random_search.best_params_))

losses = []
test_predictions  = {'id': test['id']}

for class_name in class_names:
    start = time()
    train_target = train[class_name]
    pipe = pipeline.set_params(vect__word_vect__ngram_range = random_search.best_params_['vect__word_vect__ngram_range'], 
                                     vect__char_vect__ngram_range = random_search.best_params_['vect__char_vect__ngram_range'], 
                                     clf__C                       = random_search.best_params_['clf__C']
                                     )
    
    # Uncomment the following lines to get CV info on each class (this will increase computation time a bit)                                 
    #cv_loss = np.mean(cross_val_score(classifier, train['comment_text'].values, train_target, cv=n_folds, scoring='roc_auc'))
    #losses.append(cv_loss)
    #print('CV score for class {} is {}'.format(class_name, cv_loss))

    pipe.fit(train['comment_text'].values, train_target)
    test_predictions[class_name] = pipe.predict_proba(test['comment_text'].values)[:, 1]

    print("Fitting and predictions took %.2f seconds" % (time() - start) )

#print('Total CV score is {}'.format(np.mean(losses)))

test_submission = pd.DataFrame.from_dict(test_predictions)
test_submission.to_csv('submission.csv', index=False)