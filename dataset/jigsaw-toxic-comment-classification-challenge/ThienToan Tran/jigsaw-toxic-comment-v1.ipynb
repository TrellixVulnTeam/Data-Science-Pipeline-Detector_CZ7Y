{"cells":[{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"# !conda install keras\n# !conda install  nltk -y","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"# !conda list | grep keras","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from dateutil.parser import parse\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\n\nimport string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport re\n\nimport numpy as np\nimport pandas as pd\n\nimport keras\nfrom keras.metrics import Mean\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\n\nfrom keras import backend as K\n\n\n\nnltk.download('punkt')\n\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.setrecursionlimit(10000)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"# not_punctuation = r'[^\\w\\s]'","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ntrain = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip').fillna(' ')\ntest = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip').fillna(' ')","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"### 2. Data exploring\n\n\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 3. Data transforming"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"def clean_comment(comment):\n\n    comment = comment.lower()\n\n    # replace common short expression\n    comment = re.sub(r\"\\'ve\", ' have', comment)\n    comment = re.sub(r\"\\'s\", \" \", comment)\n    comment = re.sub(r\"can't\", \"cannot \", comment)\n    comment = re.sub(r\"n't\", \" not \", comment)\n    comment = re.sub(r\"i'm\", \"i am \", comment)\n    comment = re.sub(r\"\\'re\", \" are \", comment)\n    comment = re.sub(r\"\\'d\", \" would \", comment)\n    comment = re.sub(r\"\\'ll\", \" will \", comment)\n    comment = re.sub(r\"\\'scuse\", \" excuse \", comment)\n\n    # remove stand-alone numbers\n    comment = re.sub(r'(?<!\\S)\\d+(?!\\S)', '', comment)\n\n    # remove non-character\n    comment = re.sub(r'[^\\w\\s]', ' ', comment)\n    \n    # remove spaces\n    comment = re.sub(r'\\s+', ' ', comment)\n    \n    return comment","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"def build_stopwords():\n    result = stopwords.words('english')\n    for c in string.ascii_uppercase:\n        result.append( c )\n    for c in string.ascii_lowercase:\n        result.append(c)\n    for c in string.digits:\n        result.append(c)\n    return result\nall_stopwords = build_stopwords()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(comment):\n    return ' '.join([word for word in comment.split() if word not in all_stopwords])","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"def stemSentence(comment):\n    porter = PorterStemmer()\n    token_words=word_tokenize(comment)\n    stem_sentence=[]\n    for word in token_words:\n        stem_sentence.append(porter.stem(word))\n    return \" \".join(stem_sentence)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"def lemmaSentence(comment):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    token_words = nltk.word_tokenize(comment)\n    lemmas = []\n    for word in token_words:\n        lemmas.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n    return \" \".join(lemmas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].apply(lambda comment: clean_comment(comment))\ntest['comment_text'] = test['comment_text'].apply(lambda comment: clean_comment(comment))\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].apply(lambda comment: remove_stopwords(comment))\ntest['comment_text'] = test['comment_text'].apply(lambda comment: remove_stopwords(comment))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].apply(lambda comment: lemmaSentence(comment))\ntest['comment_text'] = test['comment_text'].apply(lambda comment: lemmaSentence(comment))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train['comment_text']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"\n# vect = TfidfVectorizer(max_features=10000,stop_words= all_stopwords)\n# vect","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"\nall_comments = train['comment_text'].append(test['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"\nmax_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\n\ntokenizer.fit_on_texts(list(all_comments))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_tokenized_comment = tokenizer.texts_to_sequences(train['comment_text'])\ntest_tokenized_comment = tokenizer.texts_to_sequences(test['comment_text'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def len_tokenied_comment(comments):\n    result = []\n    for comment in comments:\n        result.append(len(comment))\n    return result\n\ntrain_tokenized_comment_len = len_tokenied_comment(train_tokenized_comment)\ntest_tokenized_comment_len = len_tokenied_comment(test_tokenized_comment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.quantile(train_tokenized_comment_len, q=.9))\nplt.hist(train_tokenized_comment_len, bins = 200)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_tokenized_comment","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"test_tokenized_comment","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"max_token_len = 300\ntrain_tokenized_comment = pad_sequences(train_tokenized_comment, maxlen=max_token_len)\ntest_tokenized_comment = pad_sequences(test_tokenized_comment, maxlen=max_token_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokenized_comment_len = len_tokenied_comment(train_tokenized_comment)\ntest_tokenized_comment_len = len_tokenied_comment(test_tokenized_comment)\n\nprint(np.quantile(train_tokenized_comment_len, q=.9))\nplt.hist(train_tokenized_comment_len, bins = 200)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"inp = Input(shape=(max_token_len, ))\n\nembed_size = 128\nx = Embedding(max_features, embed_size)(inp)\nx = LSTM(60, return_sequences=True,name='lstm_layer')(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 2","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_classify = train[class_names].values\n\nmodel.fit(train_tokenized_comment, train_classify, batch_size=batch_size, epochs=epochs, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'model_20201027.sav'","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"pickle.dump(model, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"model = pickle.load(open(filename, 'rb'))\nresult = loaded_model.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"model.evaluate(train_tokenized_comment, train_classify)\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"# for i,index in enumerate(train.index):\n#     train[index,'tokenized_comment'] = train_tokenized_comment[i]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":true,"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"%%time\n\ntest_predict = model.predict(test_tokenized_comment , batch_size=batch_size, verbose=1)\n\n# predict_result = []\n# for test_data in test_tokenized_comment:\n#     test_data = np.array(test_data).T\n#     predict_result.append(\n#         loaded_model.predict(test_data)\n#     )\n    \n# predict_result = loaded_model.predict_classes(train_tokenized_comment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"test_predict_df = pd.DataFrame(test_predict)\ntest_predict_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[class_names] = test_predict_df\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[class_names] = test_predict_df\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}