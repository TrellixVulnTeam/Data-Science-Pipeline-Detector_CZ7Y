{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n__print__ = print\ndef print(string):\n    os.system(f'echo \\\"{string}\\\"')\n    __print__(string)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv')\nprint('Imported train and test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Checking for nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get x and y"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train['comment_text']\ny_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\nx_test = test['comment_text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenize our words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(x_train)\nprint('Fit tokenizer on texts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tokenized_train = tokenizer.texts_to_sequences(x_train)\nx_tokenized_test = tokenizer.texts_to_sequences(x_test)\nprint('Converted x_train and x_test to tokenized form')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pad our sequences"},{"metadata":{"trusted":true},"cell_type":"code","source":"lengths = [len(comment) for comment in x_tokenized_train]\nprint(f'The longest comment is {max(lengths)} words long.')\nsns.distplot(lengths, kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nmax_length = 200\nX_train = pad_sequences(x_tokenized_train, maxlen=max_length)\nX_test = pad_sequences(x_tokenized_test, maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, GlobalAveragePooling1D, Dropout, Dense, LeakyReLU, Activation, GlobalMaxPool1D\nfrom keras import metrics\n\nnum_features, embed_size = len(tokenizer.word_index), 128\nmetric = ['accuracy']\n\nmodels = []\n\nmodel1 = Sequential()\nmodel1.add(Embedding(num_features + 1, embed_size, input_length=max_length))\nmodel1.add(LSTM(64, return_sequences=True))\nmodel1.add(GlobalAveragePooling1D())\nmodel1.add(Dropout(0.1))\nmodel1.add(Dense(32))\nmodel1.add(Activation('relu'))\nmodel1.add(Dropout(0.1))\nmodel1.add(Dense(16))\nmodel1.add(Activation('relu'))\nmodel1.add(Dropout(0.1))\nmodel1.add(Dense(6, activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\nmodels.append(model1)\n\nmodel2 = Sequential()\nmodel2.add(Embedding(num_features + 1, embed_size, input_length=max_length))\nmodel2.add(LSTM(64, return_sequences=True))\nmodel2.add(GlobalMaxPool1D())\nmodel2.add(Dropout(0.1))\nmodel2.add(Dense(32))\nmodel2.add(Activation('relu'))\nmodel2.add(Dropout(0.1))\nmodel2.add(Dense(16))\nmodel2.add(Activation('relu'))\nmodel2.add(Dropout(0.1))\nmodel2.add(Dense(6, activation='sigmoid'))\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\nmodels.append(model2)\n\nmodel3 = Sequential()\nmodel3.add(Embedding(num_features + 1, embed_size, input_length=max_length))\nmodel3.add(LSTM(64, return_sequences=True))\nmodel3.add(GlobalMaxPool1D())\nmodel3.add(Dropout(0.05))\nmodel3.add(Dense(6, activation='sigmoid'))\nmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\nmodels.append(model3)\n\nmodel4 = Sequential()\nmodel4.add(Embedding(num_features + 1, embed_size, input_length=max_length))\nmodel4.add(LSTM(64, return_sequences=True))\nmodel4.add(GlobalMaxPool1D())\nmodel4.add(Dropout(0.1))\nmodel4.add(Dense(32, activation='relu'))\nmodel4.add(Dropout(0.05))\nmodel4.add(Dense(6, activation='sigmoid'))\nmodel4.compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\nmodels.append(model4)\n\ndel model1, model2, model3, model4\n\n# # 0.958\n# models[0].add(Embedding(num_features + 1, embed_size, input_length=max_length))\n# models[0].add(LSTM(64, return_sequences=True))\n# models[0].add(GlobalAveragePooling1D())\n# models[0].add(Dropout(0.1))\n# models[0].add(Dense(48))\n# models[0].add(LeakyReLU())\n# models[0].add(Dropout(0.1))\n# models[0].add(Dense(6, activation='sigmoid'))\n# models[0].compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\n# # 0.961\n# models[1].add(Embedding(num_features + 1, embed_size, input_length=max_length))\n# models[1].add(LSTM(64, return_sequences=True))\n# models[1].add(GlobalAveragePooling1D())\n# models[1].add(Dropout(0.1))\n# models[1].add(Dense(48))\n# models[1].add(Activation('relu'))\n# models[1].add(Dropout(0.1))\n# models[1].add(Dense(6, activation='sigmoid'))\n# models[1].compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\n# # 0.962\n# models[2].add(Embedding(num_features + 1, embed_size, input_length=max_length))\n# models[2].add(LSTM(64, return_sequences=True))\n# models[2].add(GlobalAveragePooling1D())\n# models[2].add(Dropout(0.1))\n# models[2].add(Dense(32))\n# models[2].add(Activation('relu'))\n# models[2].add(Dropout(0.1))\n# models[2].add(Dense(16))\n# models[2].add(Activation('relu'))\n# models[2].add(Dropout(0.1))\n# models[2].add(Dense(6, activation='sigmoid'))\n# models[2].compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\nprint('Created models')\nmodels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nbatch_size = 16\nvalidation_split = 0.1\nepochs = 3\n\nhistories = []\n\nfor i, model in enumerate(models):\n    print(f'Beginning to fit model {i}')\n    start_time = time.time()\n    history = model.fit(X_train, y_train,\n                        validation_split=validation_split,\n                        batch_size=batch_size,\n                        epochs=epochs)\n    histories.append(history)\n    end_time = time.time()\n    print(f'Fit model {i} in {end_time - start_time} seconds.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display\nfor i, history in enumerate(histories):\n    print(f'Model {i}')\n    h = pd.DataFrame(history.history)\n    h.index.name = 'epoch'\n    display(h)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get the submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = []\n\nfor i, model in enumerate(models):\n    print(f'Started predicting for model {i}')\n    y_pred = model.predict(X_test, batch_size=4096)\n    y_preds.append(y_pred)\n    print(f'Predicted stuff for model {i}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = []\nfor i, model in enumerate(models):\n    y_i = pd.DataFrame(data=y_preds[i],\n                        columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n    y_i = pd.concat([test['id'], y_i], axis=1)\n    y.append(y_i)\n# y = pd.DataFrame(data=y_pred, columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n# y = pd.concat([test['id'], y], axis=1)\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, y_i in enumerate(y):\n    filename = f'submision_{i}.csv'\n    y_i.to_csv(filename, index=False)\n    print(f'Created file {filename}')\n# y.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}