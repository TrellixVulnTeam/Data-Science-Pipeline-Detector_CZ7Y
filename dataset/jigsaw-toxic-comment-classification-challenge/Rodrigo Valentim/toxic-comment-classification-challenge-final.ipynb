{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introdução\n\n## 1.1 Sobre este notebook\n- Resolução da competição Kaggle **Toxic Comment Classification Challenge**\n- Trabalho final da disciplina IN1102 -- Aprendizagem de Máquina, CIn/UFPE\n- Semestre 2019.01\n- Prof. Cleber Zanchettin\n\n**Equipe**\n - Rodrigo Barros Bernardino\n - Rodrigo Emerson Valentim da Silva\n\n------\n## 1.2 Sobre a Competição\n- **Link:** <https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge>\n- **Dados:**\n    - **159.571** comentários de edições da Wikipedia\n    - Rotulados por avaliadores humanos \n    - Conteúdo \"tóxico\": `toxic` | `severe_toxic` | `obscene` | `threat` | `insult` | `identity_hate`\n    - **Múltiplas classes** simultaneamente (ex. `toxic` & `threat`)\n- **Objetivo:** classificar em uma ou mais classes\n\n----\n## 1.3 Abordagens Utilizadas\n- **Algoritmos clássicos**\n    - LinearSVC\n    - MultinomialNB\n    - LogisticRegression\n- **Redes Neurais**\n    - Rede neural LSTM sem Glove\n    - Rede neural LSTM com Glove\n- **Melhor Resultado**\n    - Rede neural LSTM com Glove\n\n-------------"},{"metadata":{},"cell_type":"markdown","source":"# 2. Inicialização"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Basic libraries\nimport warnings\nimport os\nimport re\nimport numpy as np\nimport random\nimport pandas as pd \nimport json\nfrom tqdm import tqdm\n\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Traditional Classifiers\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import LinearSVC\n\n# Classifiers validation and test utils\nfrom sklearn.model_selection import RepeatedKFold, train_test_split, cross_val_score\nfrom sklearn import metrics as mt\nfrom sklearn.multioutput import MultiOutputRegressor as mout\nfrom sklearn.multioutput import MultiOutputClassifier as cout\nfrom sklearn.metrics import roc_auc_score\n\n# NLP utils\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import text_to_word_sequence\n\n# Neural networks utils\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM, Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#############################################\n# INITIAL SETUP\n###############\nwarnings.simplefilter(\"ignore\", UserWarning)\nseed = 7\nnp.random.seed(seed)\n\n#############################################\n# FILE CONSTANTS\n###############\nTRAIN_CSV_PATH = \"../input/jigsaw-toxic-comment-classification-challenge/train.csv\"\nTEST_CSV_PATH = \"../input/jigsaw-toxic-comment-classification-challenge/test.csv\"\nSAMPLE_SUBM_CSV_PATH = \"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Funções Utilitárias"},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_full(x):\n    \"\"\"Evita truncamento ao imprimir tabelas com texto longo\"\"\"\n    pd.set_option('display.max_rows', len(x))\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.width', 2000)\n    pd.set_option('display.float_format', '{:20,.2f}'.format)\n    pd.set_option('display.max_colwidth', -1)\n    display(x)\n    pd.reset_option('display.max_rows')\n    pd.reset_option('display.max_columns')\n    pd.reset_option('display.width')\n    pd.reset_option('display.float_format')\n    pd.reset_option('display.max_colwidth')\n\ndef hasNonASCII(s):\n    \"\"\"Checa se string possui algum caractere não padrão do inglês\"\"\"\n    clean_str(s)\n    try:\n        s.encode(encoding='utf-8').decode('ascii')\n    except UnicodeDecodeError:\n        return True\n    else:\n        return False\n\ndef countNonASCII(s):\n    \"\"\"Conta quantos caracteres não padrão do inglês a string possui\"\"\"\n    if hasNonASCII(s):\n        space_split = s.split(' ')\n        non_ascii_count = 0\n        for item in space_split:\n            if(hasNonASCII(item)):\n                non_ascii_count += 1\n        return non_ascii_count\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Função para Limpeza de Texto"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_str(string):\n    # split contractions like \"he'll\"\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    \n    # remove punctuation\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string) # \"   \"     spaces\n    string = re.sub('<.*?>', '', string)    # <a src..> html tags\n    string = re.sub(r'\\d+', '', string)     # 1234      numbers\n    string = re.sub(\"'\", '', string)        # '         quotes\n    string = re.sub(r'\\W+', ' ', string)    # ABCD      abbrevs.\n    string = string.replace('_', '')        # _         underscore\n    \n    # fix words like \"finallllly\" and \"awwwwwesome\"\n    pttrn_repchar = re.compile(r\"(.)\\1{2,}\")\n    string = pttrn_repchar.sub(r\"\\1\\1\", string)\n\n    # Emojis pattern\n    emoji_pattern = re.compile(\"[\"\n                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                u\"\\U00002702-\\U000027B0\"u\"\\U000024C2-\\U0001F251\"\n                u\"\\U0001f926-\\U0001f937\"u'\\U00010000-\\U0010ffff'u\"\\u200d\"\n                u\"\\u2640-\\u2642\"u\"\\u2600-\\u2B55\"u\"\\u23cf\"u\"\\u23e9\"u\"\\u231a\"\n                u\"\\u3030\"u\"\\ufe0f\"\n    \"]+\", flags=re.UNICODE)\n    string = emoji_pattern.sub(u'', string)\n\n    # remove stop words (# words that don't add representativeness)\n    # ex. \"a\", \"at\", \"had\", \"has\" ...\n    stop_words = set(stopwords.words('english'))\n    word_list = text_to_word_sequence(string)\n    no_stop_words = [w for w in word_list if not w in stop_words]\n    no_stop_words = \" \".join(no_stop_words)\n    string = no_stop_words\n    \n    # convert all letters to lower\n    return string.strip().lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Leitura e Exploração dos Dados"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Train Data\ntrain_df_original = pd.read_csv(TRAIN_CSV_PATH)\ntrain_df = train_df_original.copy()\n\n# Labels of Train Data\nY = train_df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Data\ntest_df_original = pd.read_csv(TEST_CSV_PATH)\ntest_df = test_df_original.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exemplos de comentários"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(17)\ndisplay_full(train_df.filter([\"comment_text\"]).sample(3)) # display_full avoids truncating long text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exemplos de Comentários Tóxicos"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(17)\ndisplay_full(\n    train_df\n        .query('(toxic + severe_toxic + obscene + threat + insult + identity_hate)>0')\n        .filter([\"comment_text\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n        .sample(15)\n) # display_full avoids truncating long text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checar Comentários com Caracteres Especiais"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['comment_text'] = train_df['comment_text'].apply(clean_str)\ntrain_df['nASCII'] = train_df['comment_text'].apply(hasNonASCII)\nnon_ascii_rows = train_df[train_df['nASCII']]\nprint(\"Samples with non-ASCII characters:\", len(non_ascii_rows), \"samples\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Adiciona Coluna \"non-ASCII count\"\n- Conta quantidade de caracteres especiais em um comentário"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['nASCII_count'] = (train_df['comment_text']\n                                .apply(countNonASCII))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comentários Tóxicos com caracteres especiais"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDF_nonASCII = train_df[train_df['nASCII']] # recreate df to include new column\nnascii_tox_q = '(toxic + severe_toxic + obscene + threat + insult + identity_hate)>0'\ntox_nonascii = (trainDF_nonASCII\n                .query(nascii_tox_q)\n                .sort_values(by=['nASCII_count'], ascending=False))\nprint(\"Toxic with non-ascii chars:\", len(tox_nonascii))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tóxicos escondidos\n\n- ID 40c5011b970a3529\n```\nhello turd firśt aṃ ģoinģ ţo ţie ŷou uṗ keeṗ ŷou çonśçiouś durinģ ţhe folloŵinģ ṗroçeśś (...)\nṃuţilaţe ŷour ģeniţalś\n```\n \n ----\n- ID 631a0b5de02145f1\n```\nflay alive fking stalker ᵽlace ḟollowing ḻarģe text box shall enjoy screams agony blood slowly pools across floor basement\n```\n\n----\n- ID 64d74806192d2751\n```\nIt's not a fu©king joke, you motherfu©king idiot, (...)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_full(tox_nonascii\n               .filter(['id', 'comment_text']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test Set: Outra Língua"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_full(test_df_original.query(\"id == '51bb6805977dbbc2' | id == '564fa976421ed6a4'\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Limpeza dos Dados"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['comment_text'] = train_df['comment_text'].apply(lambda x: clean_str(x))\ntest_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_str(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Algoritmos Clássicos"},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Treinamento dos modelos"},{"metadata":{},"cell_type":"markdown","source":"### Definição dos Modelos"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n   mout(LinearSVC()),\n    cout(MultinomialNB()),\n    mout(LogisticRegression(solver='lbfgs'))\n]\nmodel_names = [\"LinearSVC\", \"MultinomialNB\", \"LogisticRegression\"]\nclasses_name = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Treinamento"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_norm(data, test_df):\n    X = data['comment_text'].values\n    count_vect = CountVectorizer()\n    X_counts = count_vect.fit_transform(X)\n    tfidf_transformer = TfidfTransformer()\n    \n    X_tfidf = tfidf_transformer.fit_transform(X_counts)\n    rkf = RepeatedKFold(n_splits=10, n_repeats=1, random_state=42)\n\n    model_name = []\n    model_accuracy = []\n    model_roc = []\n    i = 0\n    for model, name in zip (models, model_names):\n        sample_submission = pd.read_csv(SAMPLE_SUBM_CSV_PATH)\n        submission = sample_submission\n        print(\"Modelo: \",name)\n        for train, test in rkf.split(X):\n            model.fit(X_tfidf[train], Y[train])\n            y_pred = model.predict(X_tfidf[test])\n            model_name.append(name)\n            model_accuracy.append(mt.accuracy_score(Y[test], y_pred))\n            model_roc.append(mt.roc_auc_score(Y[test], y_pred))    \n        y_pred = model.predict(count_vect.transform(test_df.comment_text))     \n        submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred \n        submission.to_csv(\"submission_\"+name+\"_.csv\", index=False)\n        i = i+1\n    disc_result = {'Model':  model_name, 'Accuracy': model_accuracy, \"ROC\": model_roc}      \n    results = pd.DataFrame(disc_result, columns=[\"Model\", \"Accuracy\", \"ROC\"])\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = train_norm(train_df, test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Resultados\n- O melhor resultado foi o do **LinearSVC**\n    - private score: `0.89168`\n    - public score: `0.89795`"},{"metadata":{},"cell_type":"markdown","source":"### Amostra dos Resultados"},{"metadata":{"trusted":true},"cell_type":"code","source":"results.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gráficos com os resultados de acurácia e ROC"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot(data):\n    acc_linear = results.iloc[results[\"Model\"].values == \"LinearSVC\"].Accuracy.values\n    acc_nb = results.iloc[results[\"Model\"].values == \"MultinomialNB\"].Accuracy.values\n    acc_lg = results.iloc[results[\"Model\"].values == \"LogisticRegression\"].Accuracy.values\n    #loss = history.history['loss']\n    #val_loss = history.history['val_loss']\n    epochs = range(1, len(acc_nb) + 1)\n    plt.plot(epochs, acc_linear, 'b', label='LinearSVC acc')\n    plt.plot(epochs, acc_lg, 'b', label='LogisticRegression acc', color=\"g\")\n    plt.plot(epochs, acc_nb, 'b', label='MultinomialNB acc', color=\"r\")\n    plt.title('Grafíco com Acurácias')\n    plt.legend(loc='best')\n    \n    roc_linear = results.iloc[results[\"Model\"].values == \"LinearSVC\"].ROC.values\n    roc_nb = results.iloc[results[\"Model\"].values == \"MultinomialNB\"].ROC.values\n    roc_lg = results.iloc[results[\"Model\"].values == \"LogisticRegression\"].ROC.values\n\n    plt.figure()\n    plt.plot(epochs,  roc_linear, 'b', label='LinearSVC')\n    plt.plot(epochs,  roc_nb, 'b', label='MultinomialNB', color=\"g\" )\n    plt.plot(epochs,  roc_lg, 'b', label='LogisticRegression', color=\"r\")\n    plt.title('Gráfico com o valor da curva ROC')\n    plt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gp = results.groupby(by=\"Model\").Accuracy.mean()\ngp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\nplt.bar([\"LinearSVC\",\"LogisticRegression\",\"MultinomialNB\"], gp.values, color=[\"r\", \"g\", \"b\"])\nplt.xlabel(\"Labels\")\nplt.ylabel(\"Counts\")\nplt.title(\"Gráfico de barras para acurácia\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gp = results.groupby(by=\"Model\").ROC.mean()\ngp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\nplt.bar([\"LinearSVC\",\"LogisticRegression\",\"MultinomialNB\"], gp.values, color=[\"r\", \"g\", \"b\"])\nplt.xlabel(\"Modelos\")\nplt.ylabel(\"ROC\")\nplt.title(\"Gráfico de barras para valores da curva ROC\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Ensemble com os melhores modelos.\n* Não houve melhorias significativas com o Ensemble, ficou igual ao modelo Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df['comment_text'].values\ncount_vect = CountVectorizer()\nX_counts = count_vect.fit_transform(X)\ntfidf_transformer = TfidfTransformer()\nX_tfidf = tfidf_transformer.fit_transform(X_counts)\n\nclf1, clf2, clf3 = models[0], models[1], models[2]\neclf = mout((VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='hard')))\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Linear Regression', 'Logisctic', 'Ensemble']):\n    scores = cross_val_score(clf, X_tfidf, Y, cv=5, scoring='accuracy')    \n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--------------------\n# 6. Rede neural LSTM\n* A rede neural lstm foi treinada de duas formas, a primeira foi sem o Glove e a segunda foi utilizando o Glove.\n\n* O objetivo do treinamento do GloVe é aprender vetores de palavras de tal forma que seu produto escalar seja igual ao logaritmo da probabilidade de coocorrência das palavras."},{"metadata":{},"cell_type":"markdown","source":"## 6.1 Funções model() train()"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model():\n    if pre_trained_wv is True:\n        print(\"Usando glove..\")\n        num_words = min(max_fatures, len(word_index) + 1)\n        weights_embedding_matrix = load_pre_trained_wv(word_index, num_words, word_embedding_dim)\n        input_shape = (max_sequence_length,)\n        model_input = Input(shape=input_shape, name=\"input\", dtype='int32')    \n        embedding = Embedding(\n            num_words, \n            word_embedding_dim,\n            input_length=max_sequence_length, \n            name=\"embedding\", \n            weights=[weights_embedding_matrix], \n            trainable=False)(model_input)\n        if bilstm is True:\n            lstm = Bidirectional(LSTM(embed_dim, dropout=0.2, recurrent_dropout=0.2, name=\"lstm\"))(embedding)\n        else:\n            lstm = LSTM(embed_dim, dropout=0.2, recurrent_dropout=0.2, name=\"lstm\")(embedding)\n    else:\n        input_shape = (max_sequence_length,)\n        model_input = Input(shape=input_shape, name=\"input\", dtype='int32')    \n        embedding = Embedding(max_fatures, embed_dim, input_length=max_sequence_length, name=\"embedding\")(model_input)   \n        if bilstm is True:\n            lstm = Bidirectional(LSTM(embed_dim, dropout=0.2, recurrent_dropout=0.2, name=\"lstm\"))(embedding)\n        else:\n            lstm = LSTM(embed_dim, dropout=0.2, recurrent_dropout=0.2, name=\"lstm\")(embedding)\n    \n    model_output = Dense(6, activation='sigmoid', name=\"sigmoid\")(lstm)\n    model = Model(inputs=model_input,outputs=model_output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, X_train, Y_train, X_val, Y_val):\n    filepath=\"weights.best.hdf5\"\n    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    callbacks_list = [checkpoint]\n\n    history = model.fit(\n        X_train, \n        Y_train, \n        validation_data=(X_val, Y_val),\n        epochs=30,\n        batch_size=3000, \n        shuffle=True,\n        verbose=1, callbacks=callbacks_list)  \n\n    # Plot\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2 Prepara os Dados"},{"metadata":{},"cell_type":"markdown","source":"### Preparação para LSTM e definição de parâmetros"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_embedding_dim = 50     # pre-trained word embedding dimension\nbatch_size = 300            # number of samples to be used on each gradient update\nmax_fatures = 5000          # maximum amount of words to keep in the vocabulary\nembed_dim = 128             # output embedding layer dimension\nmax_sequence_length = 300   # maximum sentence length is limited to 300 words\nbilstm = False\npre_trained_wv = False\n\n\ndef prepare_data(data, label = None, test=False):    \n    text = []\n    for row in data['comment_text'].values:\n        text.append(row)\n\n    tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n    tokenizer.fit_on_texts(text)\n    X = tokenizer.texts_to_sequences(text)  \n\n    X = pad_sequences(X, maxlen=max_sequence_length)\n    #X = pad_sequences(X)\n\n    word_index = tokenizer.word_index\n    #Y = pd.get_dummies(data[label]).values\n\n    if test == True:\n        return X, word_index, tokenizer\n    else:\n        #Y = pd.get_dummies(data[labels]).values\n        X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n    \n        X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size = 0.10, random_state = 42)\n        return X_train, X_test, Y_train, Y_test, word_index, X_val, Y_val, tokenizer\n\n\ndef load_pre_trained_wv(word_index, num_words, word_embedding_dim):\n    embeddings_index = {}\n    f = open(os.path.join('../input/glove6b50dtxt', 'glove.6B.{}d.txt'.format(word_embedding_dim)), encoding='utf-8')\n    for line in tqdm(f):\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\n    print('%s word vectors.' % len(embeddings_index))\n\n    embedding_matrix = np.zeros((num_words, word_embedding_dim))\n    for word, i in word_index.items():\n        if i >= max_fatures:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(X_test, model):\n    X_test_data = tokenizer.texts_to_sequences(X_test['comment_text'])\n    X_test_data = pad_sequences(X_test_data, maxlen=max_sequence_length, dtype='int32', value=0)\n    y_pred = model.predict(X_test_data, batch_size=1000, verbose=1)\n\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test, word_index, X_val, Y_val, tokenizer = prepare_data(train_df)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.3 LSTM sem GloVe"},{"metadata":{},"cell_type":"markdown","source":"### Treino"},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_trained_wv = False\nmodel1 = model()\nmodel1.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\nprint(model1.summary())\nmodel1 = train(model1, X_train, Y_train, X_val, Y_val)   \ny_pred = model1.predict(X_test,batch_size=1024,verbose=6)\nprint(\"AUC: \", roc_auc_score(Y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Teste"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = test(test_df, model1)\nsubmission = sample_submission\nsubmission['id'] = test_df['id']\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\nsubmission.to_csv('submission_lstm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.4 LSTM com GloVe"},{"metadata":{},"cell_type":"markdown","source":"### Treino"},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_trained_wv = True\nmodel_glove = model()\nmodel_glove.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\nprint(model_glove.summary())\nmodel = train(model_glove, X_train, Y_train, X_val, Y_val)    \ny_pred = model_glove.predict(X_test,batch_size=1024,verbose=6)\nprint(\"AUC: \", roc_auc_score(Y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Teste"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(X_test, model):\n    X_test_data = tokenizer.texts_to_sequences(X_test['comment_text'])\n    X_test_data = pad_sequences(X_test_data, maxlen=max_sequence_length, dtype='int32', value=0)\n    y_pred = model.predict(X_test_data, batch_size=1000, verbose=1)\n\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = test(test_df, model_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(SAMPLE_SUBM_CSV_PATH)\nsubmission = sample_submission\nsubmission['id'] = test_df['id']\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\nsubmission.to_csv('submission_lstm_glove,.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}