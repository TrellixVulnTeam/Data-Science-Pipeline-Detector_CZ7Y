{"cells":[{"metadata":{"trusted":true,"_uuid":"bd2059e6933a5e65ac37ae865017128b9e971c07"},"cell_type":"markdown","source":"In Jeremy Howard's Kernel [NB-SVM strong linear baseline](https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline),  the log-count ratio `r` (in equation (2) of [Baselines and Bigrams: Simple, Good Sentiment and Topic Classiﬁcation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)) is implemented differently from the paper: the paper uses l1 norm, and the kernel used the vector length (`(y==y_i).sum()`). Many have questions about this and JH's explanation: \"Normally yes, but here that is rolled into the bias term in the logistic regression automatically.\"\n\nThe last part of this notebook \"Explain the 1norm\" looks into this difference and provides some explanation. The rest of the orginal \"NB-SVM strong linear baseline\" kernel is left unchanged."},{"metadata":{"_cell_guid":"d3b04218-0413-4e6c-8751-5d8a404d73a9","_uuid":"0bca9739b82d5d51e1229243e03ea1b6db35c17e"},"cell_type":"markdown","source":"## Introduction\n\nThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline for the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classiﬁcation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf). In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).\n\nIf you're not familiar with naive bayes and bag of words matrices, I've made a preview available of one of fast.ai's upcoming *Practical Machine Learning* course videos, which introduces this topic. Here is a link to the section of the video which discusses this: [Naive Bayes video](https://youtu.be/37sFIak42Sc?t=3745)."},{"metadata":{"_cell_guid":"ef06cd19-66b6-46bc-bf45-184e12d3f7d4","_uuid":"cca038ca9424a3f66e10262fc9129de807b5f855","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a494f561-0c2f-4a38-8973-6b60c22da357","_uuid":"f70ebe669fcf6b434c595cf6fb7a76120bf7809c","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubm = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3996a226-e1ca-4aa8-b39f-6524d4dadb07","_uuid":"2c18461316f17d1d323b1959c8eb4e5448e8a44e"},"cell_type":"markdown","source":"## Looking at the data\n\nThe training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict."},{"metadata":{"_cell_guid":"5ddb337b-c9b2-4fec-9652-cb26769dc3c6","_uuid":"5f5269c56ea6ded273881b0d4dcdb6af83a3e089","scrolled":true,"trusted":true},"cell_type":"raw","source":"train.head()"},{"metadata":{"_cell_guid":"b3b071fb-7a2c-4195-9817-b01983d11c0e","_uuid":"004d2e823056e98afc5adaac433b7afbfe93b82d"},"cell_type":"markdown","source":"Here's a couple of examples of comments, one toxic, and one with no labels."},{"metadata":{"_cell_guid":"d57f0b31-c09b-4305-a0b0-0b864e944fd1","_uuid":"1ba9522a65227881a3a55aefaee9de93c4cfd792","trusted":true},"cell_type":"raw","source":"train['comment_text'][0]"},{"metadata":{"_cell_guid":"9caf5da3-33bb-422d-81c4-fef20fbda1a8","_uuid":"b0d70e9d745411ea6228c95c5f19bd3a2ca6dd55","scrolled":true,"trusted":true},"cell_type":"raw","source":"train['comment_text'][2]"},{"metadata":{"_cell_guid":"2ea37597-02f7-43cf-ad16-a3d50aac1aba","_uuid":"5c4c716de98a4b1c2ecc0e516e67813b4fc1473e"},"cell_type":"markdown","source":"The length of the comments varies a lot."},{"metadata":{"_cell_guid":"fd3fe158-4d7f-4b30-ac15-42605240ea4f","_uuid":"9c1a3f81397199fa250a2b642edc7fbc5f9f504e","trusted":true},"cell_type":"raw","source":"lens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()"},{"metadata":{"_cell_guid":"d2e55012-4736-425f-84f3-c148ac1f4852","_uuid":"eb68f1c83a5ad11e652ca5f2150993a06d43edb4","trusted":true},"cell_type":"raw","source":"lens.hist();"},{"metadata":{"_cell_guid":"b8515824-b2dd-4c95-bbf9-dc74c80355db","_uuid":"0151ab55887071aed82d297acb2c6545ed964c2b"},"cell_type":"markdown","source":"We'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset."},{"metadata":{"_cell_guid":"c66f79d1-1d9f-4d94-82c1-8026af198f2a","_uuid":"4ba6ef86c82f073bf411785d971a694348c3efa9","trusted":true},"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain['none'] = 1-train[label_cols].max(axis=1)\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9f6316e3-7e29-431b-abef-73acf4a08637","_uuid":"b7b0d391248f929a026b16fc38936b7fc0176351","trusted":true},"cell_type":"raw","source":"len(train),len(test)"},{"metadata":{"_cell_guid":"1b221e62-e23f-422a-939d-6747edf2d613","_uuid":"bfdcf59624717b37ca4ffc0c99d2c28a2d419b06"},"cell_type":"markdown","source":"There are a few empty comments that we need to get rid of, otherwise sklearn will complain."},{"metadata":{"_cell_guid":"fdba531c-7ef2-4967-88e2-fc2b04f6f2ef","_uuid":"1e1229f403225f1889c7a7b4fc9be90fda818af5","trusted":true},"cell_type":"code","source":"COMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"480780f1-00c0-4f9a-81e5-fc1932516a80","_uuid":"f2e77e8e6df5e29b620c7a2a0add1438c35af932"},"cell_type":"markdown","source":"## Building the model\n\nWe'll start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper."},{"metadata":{"_cell_guid":"b7f11db7-5c12-4eb8-9f2d-0323d629fed9","_uuid":"b043a3fb66c443fab0129e863c134ec813dadb87","trusted":true},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bfdebf11-133c-4b12-8664-8bf64757d6cc","_uuid":"941759df15c71d42853515e4d1006f4ab000ce75"},"cell_type":"markdown","source":"It turns out that using TF-IDF gives even better priors than the binarized features used in the paper. I don't think this has been mentioned in any paper before, but it improves leaderboard score from 0.59 to 0.55."},{"metadata":{"_cell_guid":"31ad6c98-d054-426c-b3bd-b3b18f52eb6f","_uuid":"75f3f27d56fb2d7d539e65c292d9e77c92ceead3","trusted":true},"cell_type":"code","source":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train[COMMENT])\ntest_term_doc = vec.transform(test[COMMENT])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4cf3ec26-8237-452b-90c9-831cb0297955","_uuid":"6d215bc460e64d88b08f501d5c5a67c290e40635"},"cell_type":"markdown","source":"This creates a *sparse matrix* with only a small number of non-zero elements (*stored elements* in the representation  below)."},{"metadata":{"_cell_guid":"4c7bdbcc-4451-4477-944c-772e99bac777","_uuid":"8816cc35f66b9fed9c12978fbdef5bb68fae10f4","trusted":true},"cell_type":"code","source":"trn_term_doc, test_term_doc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59131479-a861-4f46-add9-b2af09a51976","_uuid":"5fc487461f4c6fdaea25f2cd471fc801856c6689"},"cell_type":"markdown","source":"Here's the basic naive bayes feature equation:"},{"metadata":{"_cell_guid":"45fc6070-ba13-455b-9274-5c2611e2809c","_uuid":"8b277f01cecd575ed4fcae2e630c0dd8ce979793","trusted":true},"cell_type":"code","source":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2299d24b-5515-4d37-92d9-e7f6b16a290a","_uuid":"926eaa2e40e588f4ef2b86e0a28f8e575c9ed5f4","trusted":true},"cell_type":"code","source":"x = trn_term_doc\ntest_x = test_term_doc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c0b494ac-0dfc-4faa-a909-0a6d7696d1fc","_uuid":"dc5cafeab86d17ac4f036d58658437636a885a87"},"cell_type":"markdown","source":"Fit a model for one dependent at a time:"},{"metadata":{"_cell_guid":"b756c889-a383-4952-9ee9-eca79fd3454f","_uuid":"8652ab2f5f84e77fa395252be9b60be1e44fd583","trusted":true},"cell_type":"code","source":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33fd5f8c-adfc-45a1-9fde-1769a0993e76","_uuid":"0fa103b5406aabdc36ea9ef21612d343e4982fc4","trusted":true},"cell_type":"raw","source":"preds = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_mdl(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"},{"metadata":{"_cell_guid":"1a99c4d9-916f-4189-9a25-fedcb7700336","_uuid":"5525045116474e6d12b6edc890250d30c0790f06"},"cell_type":"markdown","source":"And finally, create the submission file."},{"metadata":{"_cell_guid":"bc6a4575-fbbb-47ea-81ac-91fa702dc194","_uuid":"5dd033a93e6cf32cdbdaa0a8b05cd8d27de2b21d","collapsed":true,"trusted":true},"cell_type":"raw","source":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission.csv', index=False)"},{"metadata":{"_cell_guid":"1c345d02-b768-491c-8c03-8c3459a552a8","_uuid":"adbbfb0156952a6a43833e337b8a418ccac257aa","collapsed":true,"trusted":true},"cell_type":"markdown","source":"## Explain the 1norm"},{"metadata":{"_uuid":"335af051b6d7c6fccf0b66cda7af0a720aa357e7"},"cell_type":"markdown","source":"Many question about how `r` is computed. In the paper it's equation (2) which uses l1 norm, but the above used the length (`(y==y_i).sum()`). We take one label and break down the steps to calculate `r`. "},{"metadata":{"trusted":true,"_uuid":"c8e1b1db19db04754ffb9a6054d7c2eb83fd15e6"},"cell_type":"markdown","source":"Take the first label."},{"metadata":{"trusted":true,"_uuid":"00fe0586e177e8dc3336f15def7b15602d9feeef"},"cell_type":"code","source":"x = trn_term_doc\nj = label_cols[0]\nj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08f909ccae20fece889e161a9c01a8bf4032d4cc"},"cell_type":"code","source":"y = train[j]\ny = y.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a44addc04606a4b5aa2b79f48178f45cdd778c5"},"cell_type":"markdown","source":"Use `p` and `q` and the paper does."},{"metadata":{"trusted":true,"_uuid":"f8a352d29f02b0545611d72acf9434dc51ea1e51"},"cell_type":"code","source":"p = x[y==1].sum(0)+1\nq = x[y==0].sum(0)+1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e768960e011fd33e0b7710103e253cb6f6af35a"},"cell_type":"markdown","source":"This is the paper implementation of the L1 norms `p_n_bk` and `q_n_bk`. (`p` and `q` are positive so l1 norm is just the sum)."},{"metadata":{"trusted":true,"_uuid":"8a4d8a01a9227e8aa7b5f645379bd9f62365536b"},"cell_type":"code","source":"p_n_bk = p.sum()\nq_n_bk = q.sum()\nr_bk = np.log( (p/p_n_bk) / (q/q_n_bk))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6e11ede58641335838d3b454a74b905b178ad09"},"cell_type":"markdown","source":"We can see that because \"logAB=logA+logB\", `r` can be broken down to `np.log(p/q)` and `np.log(q_n_bk/p_n_bk)`, where the L1 norms only appears in the 2nd part."},{"metadata":{"trusted":true,"_uuid":"27c0fc00b7be7a2eb1947fcdd61402696ad15538"},"cell_type":"code","source":"np.allclose(r_bk, np.log(p/q) + np.log(q_n_bk/p_n_bk))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc7b7d1833b90a6fc4999a1e444733ac8d56642e"},"cell_type":"markdown","source":"This is JH's implementation in the kernel. Instead of the L1 norm, the normalizing terms are vector length `p_n_jh` and `q_n_jh`."},{"metadata":{"trusted":true,"_uuid":"9bb371e538df9211fa01a75eda27a355160b301f"},"cell_type":"code","source":"p_n_jh = (y==1).sum()+1\nq_n_jh = (y==0).sum()+1\nr_jh = np.log( (p/p_n_jh) / (q/q_n_jh))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c66c3d00fe288fdcd415a0afe981068478db5ed"},"cell_type":"markdown","source":"We can see that because \"logAB=logA+logB\", `r` can be broken down to `np.log(p/q)` and `np.log(q_n_jh/p_n_jh)`, where the normalizing terms only appears in the 2nd part."},{"metadata":{"trusted":true,"_uuid":"5afa68157e8a8ed46538883dfe6f3e7cbd673f1a","scrolled":true},"cell_type":"code","source":"np.allclose(r_jh, np.log(p/q) + np.log(q_n_jh/p_n_jh))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c10b2f60962f6b7dfd122023b18823b2ffa59a0"},"cell_type":"markdown","source":"Therefore the `r` from the paper and the `r` from JH's kernel only differs by a constant `np.log(q_n_bk/p_n_bk) - np.log(q_n_jh/p_n_jh)`"},{"metadata":{"trusted":true,"_uuid":"d6cf544e076a8182276648f1285b08bb94ce0a67"},"cell_type":"code","source":"cnst = np.log(q_n_jh/p_n_jh) - np.log(q_n_bk/p_n_bk)\ncnst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc350a5e986b9bc31a6af1b854a6bd6e1134e8c8","scrolled":false},"cell_type":"code","source":"np.allclose(r_jh, r_bk + cnst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b2f2aaec77f3fcfbd6831695276a6c7e6ff4a9e"},"cell_type":"markdown","source":"So when it coms to the elementwise product."},{"metadata":{"_uuid":"e1a2e6fd3f39cefe0ba622bae999b0b446307fd8"},"cell_type":"markdown","source":"Paper version `xm`"},{"metadata":{"trusted":true,"_uuid":"e3e9d461116652b6533fa6a47d1e81b33b0a1760"},"cell_type":"code","source":"xm = x.multiply(r_bk) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"309e3d07f7bd607804907a2aef13de2ed5fbe561"},"cell_type":"markdown","source":"JH kernel version `xmjh`"},{"metadata":{"trusted":true,"_uuid":"1ee4130d46a2fa90a3e6fdf071021b615b9c8f05"},"cell_type":"code","source":"xmjh = x.multiply(r_jh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d8de81b2f3fe60cf20e14439603eeee315850d0"},"cell_type":"markdown","source":"We have `xmjh = xm + x*cnst`"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7b653252d115773661429ed15dad607c725d201a"},"cell_type":"code","source":"np.allclose(xmjh.tocsr()[0].todense(), \n            (xm + x*cnst)[0].todense())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6574529d47f7119d98d44a3f3786bf583da586b5"},"cell_type":"markdown","source":"So `x*cnst` is all the difference that fed into the  logstic regression models. - hence JH's explanation: \"Normally yes, but here that is rolled into the bias term in the logistic regression automatically\".\n\n"},{"metadata":{"trusted":true,"_uuid":"05d150ab01d09236c24099e706cd1608a6223407"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}