{"cells":[{"metadata":{"_uuid":"a6e926edfd31362ac957d8a2a76f57fc78ebc433","_cell_guid":"5afdb577-5e12-4aae-ab95-697ce2fcb3b2","trusted":false,"collapsed":true},"cell_type":"code","source":"import csv\nimport re\nimport string\n\nimport operator\nimport os\nimport functools\nimport operator\nimport fastText\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Embedding, Input, InputSpec, GlobalMaxPool1D, GlobalAvgPool1D, Masking\nfrom keras.layers import LSTM, GRU, Bidirectional, Dropout, SpatialDropout1D, BatchNormalization\nfrom keras.layers import concatenate\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback, ReduceLROnPlateau\nfrom keras.optimizers import Adam, Nadam\nfrom keras import initializers, regularizers, constraints\nfrom tqdm import tqdm\nfrom collections import Counter\n\n\nimport keras.backend as K\nimport tensorflow as tf\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom scipy import sparse\n\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom textacy.preprocess import preprocess_text\n\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0b1d5a7bf690a70c1a7de1cfbbf0b69780843c9","_cell_guid":"5aa68170-5233-4fdc-b530-b920151b35e1"},"cell_type":"markdown","source":"# Embedding Files","outputs":[],"execution_count":null},{"metadata":{"_uuid":"03ca2d3e503ba8d3f46a67849a1b94578ae938bb","_cell_guid":"1a665f5c-6166-400b-bddb-4d08e57b2738","trusted":false,"collapsed":true},"cell_type":"code","source":"for i in sorted(os.scandir('../input/fasttext-pretrained-word-vectors-english'), key=lambda x: x.stat().st_size, reverse=True):\n    print(i.path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dbc6c0165cba74a63b9b31906ab832b0d4a1779","_cell_guid":"72483c23-8145-4860-b8d1-b9d5cf758404","trusted":false,"collapsed":true},"cell_type":"code","source":"max_features = 60000\nmaxlen = 250\nembed_size = 300\n\nfile_path = \"weights_base.best.hdf5\"\nemb_file = '../input/fasttext-pretrained-word-vectors-english/wiki.en.bin'\nunused = set([i.strip() for i in open('../input/unused-words/unused.txt') if i.strip()])\n\ntweet_tokenizer = TweetTokenizer(reduce_len=True)\nlem = WordNetLemmatizer()\neng_stopwords = set(stopwords.words(\"english\"))\n\nlist_classes = [\n    \"toxic\",\n    \"severe_toxic\",\n    \"obscene\",\n    \"threat\",\n    \"insult\",\n    \"identity_hate\",\n]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a1cc2a5a9c0fb8cb7125f450882ccddf87b8ea2","_cell_guid":"4d198ec4-6faf-4960-aa09-3d9013ca4a19","collapsed":true,"trusted":false},"cell_type":"code","source":"CONTEXT_DIM = 100\n\nclass Attention(Layer):\n\n    def __init__(self, regularizer=regularizers.l2(1e-10), **kwargs):\n        self.regularizer = regularizer\n        self.supports_masking = True\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3        \n        self.W = self.add_weight(name='W',\n                                 shape=(input_shape[-1], CONTEXT_DIM),\n                                 initializer='normal',\n                                 trainable=True, \n                                 regularizer=self.regularizer)\n        self.b = self.add_weight(name='b',\n                                 shape=(CONTEXT_DIM,),\n                                 initializer='normal',\n                                 trainable=True, \n                                 regularizer=self.regularizer)\n        self.u = self.add_weight(name='u',\n                                 shape=(CONTEXT_DIM,),\n                                 initializer='normal',\n                                 trainable=True, \n                                 regularizer=self.regularizer)        \n        super(Attention, self).build(input_shape)\n\n    @staticmethod\n    def softmax(x, dim):\n        \"\"\"Computes softmax along a specified dim. Keras currently lacks this feature.\n        \"\"\"\n        if K.backend() == 'tensorflow':\n            import tensorflow as tf\n            return tf.nn.softmax(x, dim)\n        elif K.backend() == 'theano':\n            # Theano cannot softmax along an arbitrary dim.\n            # So, we will shuffle `dim` to -1 and un-shuffle after softmax.\n            perm = np.arange(K.ndim(x))\n            perm[dim], perm[-1] = perm[-1], perm[dim]\n            x_perm = K.permute_dimensions(x, perm)\n            output = K.softmax(x_perm)\n\n            # Permute back\n            perm[dim], perm[-1] = perm[-1], perm[dim]\n            output = K.permute_dimensions(x, output)\n            return output\n        else:\n            raise ValueError(\"Backend '{}' not supported\".format(K.backend()))\n\n    def call(self, x, mask=None):\n        ut = K.tanh(K.bias_add(K.dot(x, self.W), self.b)) * self.u\n\n        # Collapse `attention_dims` to 1. This indicates the weight for each time_step.\n        ut = K.sum(ut, axis=-1, keepdims=True)\n\n        # Convert those weights into a distribution but along time axis.\n        # i.e., sum of alphas along `time_steps` axis should be 1.\n        self.at = self.softmax(ut, dim=1)\n        if mask is not None:\n            self.at *= K.cast(K.expand_dims(mask, -1), K.floatx())\n\n        # Weighted sum along `time_steps` axis.\n        return K.sum(x * self.at, axis=-2)\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n    \n    def get_config(self):\n        config = {}\n        base_config = super(Attention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_mask(self, inputs, mask):\n        return None\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fae26f773b7699a6c5347635a42b793f78aaed43","_cell_guid":"fffffe3c-8df6-4d6e-9f7f-ca176ee22cd0","collapsed":true,"trusted":false},"cell_type":"code","source":"def create_embedding(emb_file, word_index):\n    if emb_file.endswith('bin'):\n        embeddings_index = fastText.load_model(emb_file)\n    else:\n        embeddings_index = pd.read_table(emb_file,\n                                         sep=\" \",\n                                         index_col=0,\n                                         header=None,\n                                         quoting=csv.QUOTE_NONE,\n                                         usecols=range(embed_size + 1),\n                                         dtype={h: np.float32 for h in range(1, embed_size + 1)},\n                                         engine='c',\n        )\n\n    nb_words = min(max_features, len(word_index))\n\n    # Initialize Random Matrix\n    if emb_file.endswith('bin'):\n        mean, std = 0.007565171, 0.29283202\n    else:\n        mean, std = embeddings_index.values.mean(), embeddings_index.values.std()\n\n    embedding_matrix = np.random.normal(mean, std, (nb_words, embed_size))\n\n    with tqdm(total=nb_words, desc='Embeddings', unit=' words') as pbar:\n        for word, i in word_index.items():\n            if i >= nb_words:\n                continue\n            if emb_file.endswith('bin'):\n                if embeddings_index.get_word_id(word) != -1:\n                    embedding_matrix[i] = embeddings_index.get_word_vector(word).astype(np.float32)\n                    pbar.update()\n            else:\n                if word in embeddings_index.index:\n                    embedding_matrix[i] = embeddings_index.loc[word].values\n                    pbar.update()\n\n    return embedding_matrix\n\ndef get_embedding(emb_file):\n    return Embedding(min(max_features, len(tokenizer.word_index)), embed_size,\n                     weights=[create_embedding(emb_file, tokenizer.word_index)],\n                     input_length=maxlen,\n                     trainable=False\n    )\n\ndef tokenize(s):\n    return re.sub('([{}“”¨«»®´·º½¾¿¡§£₤‘’])'.format(string.punctuation), r' \\1 ', s).split()\n\ndef replace_numbers(s):\n    dictionary = {\n        '&': ' and ',\n        '@': ' at ',\n        '0': ' zero ',\n        '1': ' one ',\n        '2': ' two ',\n        '3': ' three ',\n        '4': ' four ',\n        '5': ' five ',\n        '6': ' six ',\n        '7': ' seven ',\n        '8': ' eight ',\n        '9': ' nine ',\n    }\n    for k, v in dictionary.items():\n        s = s.replace(k, v)\n    return s\n\ndef text_cleanup(s, remove_unused=True):\n    \"\"\"\n    This function receives ss and returns clean word-list\n    \"\"\"\n    # Remove leaky elements like ip, user, numbers, newlines\n    s = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"_ip_\", s)\n    s = re.sub(\"\\[\\[.*\\]\", \"\", s)\n    s = re.sub('\\n', ' ', s)\n    s = replace_numbers(s)\n\n    # Split the sentences into words\n    s = tweet_tokenizer.tokenize(s)\n\n    # Lemmatize\n    s = [lem.lemmatize(word, \"v\") for word in s]\n\n    # Remove Stopwords\n    s = ' '.join([w for w in s if not w in eng_stopwords])\n    \n    s = preprocess_text(s, fix_unicode=True,\n                           lowercase=True,\n                           no_currency_symbols=True,\n                           transliterate=True,\n                           no_urls=True,\n                           no_emails=True,\n                           no_contractions=True,\n                           no_phone_numbers=True,\n                           no_punct=True).strip()\n    \n    if remove_unused:\n        s = ' '.join([i for i in s.split() if i not in unused])\n    return s\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68cf14532fb4887f7ca908722f423a1befc51d63","_cell_guid":"d1c534c1-446d-4e2e-82b6-490f01cbeb1d","trusted":false,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv').sample(frac=1)\ntest  = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n\ntrain['comment_text'] = train.comment_text.fillna(\"_na_\").apply(text_cleanup)\ntest['comment_text']  = test.comment_text.fillna(\"_na_\").apply(text_cleanup)\n\nlist_sentences_train = train.comment_text.tolist()\nlist_sentences_test  = test.comment_text.tolist()\n\ny = train[list_classes].values\n\ntokenizer = text.Tokenizer(num_words=max_features, lower=True)\ntokenizer.fit_on_texts(list_sentences_train + list_sentences_test)\n\nX_t  = sequence.pad_sequences(tokenizer.texts_to_sequences(list_sentences_train), maxlen=maxlen)\nX_te = sequence.pad_sequences(tokenizer.texts_to_sequences(list_sentences_test),  maxlen=maxlen)\n\nX_train, X_val, y_train, y_val = train_test_split(X_t, y, test_size=0.1, random_state=1337)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c961648e9c1fd511fdc7ce34fcb9465300bc6581","_cell_guid":"375b51da-1e7e-4ded-9cac-268477c7e7ef","trusted":false,"collapsed":true},"cell_type":"code","source":"embedding = get_embedding(emb_file)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"563bce691e753f2f40d8692f68412b5844548ec0","_cell_guid":"9d6fa6c1-47db-45d7-ad20-324c9c1adaf9","collapsed":true,"trusted":false},"cell_type":"code","source":"class RocAucEvaluation(Callback):\n\n    def __init__(self, verbose=True):\n        super(RocAucEvaluation, self).__init__()\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs   = logs or {}\n        x_val  = self.validation_data[0]\n        y_val  = self.validation_data[1]\n        y_pred = self.model.predict(x_val, verbose=0)\n        try:\n            current  = roc_auc_score(y_val, y_pred)\n        except ValueError:\n            # Bug in AUC metric when TP = 100%\n            # https://github.com/scikit-learn/scikit-learn/issues/1257\n            current = 1.0\n\n        logs['roc_auc'] = current\n\n        if self.verbose:\n            print(\"val_roc_auc: {:.6f}\".format(current))\n\ndef create_model(embedding=None):\n    inp = Input(shape=(maxlen,))\n\n    x = embedding(inp)\n    x = Bidirectional(GRU(64, return_sequences=True))(x)\n    x = Attention()(x)\n    x = Dense(6, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=inp, outputs=x)\n\n    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3, clipnorm=4), metrics=['accuracy'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b83fb812dc4c15f3e818e5b5fb0a38dcb8b0c36e","_cell_guid":"ecadb92c-20f6-4726-82e3-c11bd7dff535","trusted":false,"collapsed":true},"cell_type":"code","source":"model = create_model(embedding)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebed2370d2a3f909d71a99ed243d7ddbc72399f9","_cell_guid":"6955df70-b504-48d2-81b2-393d56d9a49b","trusted":false,"collapsed":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffca917b8c73557477a27533cb3a1ca75c4f428d","_cell_guid":"f5ebda59-5eaf-4ae3-a655-debacce9c73d","trusted":false,"collapsed":true},"cell_type":"code","source":"batch_size = 256\nepochs = 1\n\nmodel.fit(X_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(X_val, y_val),\n          callbacks=[\n              RocAucEvaluation(verbose=True),\n              ModelCheckpoint(file_path,    monitor='roc_auc', mode='max', save_best_only=True),\n              EarlyStopping(patience=10,    monitor=\"roc_auc\", mode=\"max\"),\n              ReduceLROnPlateau(patience=0, monitor='roc_auc', mode='max', cooldown=2, min_lr=1e-7, factor=0.3)\n          ]\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec45d87d15ec16f126fdae0dd249664e4638b74a","_cell_guid":"bec86d64-6479-41a0-878f-d278149a7424","collapsed":true,"trusted":false},"cell_type":"code","source":"model.load_weights(file_path)\n\nsample_submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\nsample_submission[list_classes] = model.predict(X_te, verbose=True)\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1340e04736c2ec59b0eeb22759571334a2b18f5","_cell_guid":"01a68f6c-bd45-4026-b0a4-3d7425d2119e","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"toc":{"widenNotebook":false,"navigate_menu":true,"colors":{"navigate_text":"#333333","navigate_num":"#000000","hover_highlight":"#DAA520","sidebar_border":"#EEEEEE","running_highlight":"#FF0000","selected_highlight":"#FFD700","wrapper_background":"#FFFFFF"},"toc_window_display":false,"toc_section_display":"block","number_sections":true,"toc_cell":false,"sideBar":true,"nav_menu":{"height":"12px","width":"252px"},"moveMenuLeft":true,"threshold":4}},"nbformat":4,"nbformat_minor":1}