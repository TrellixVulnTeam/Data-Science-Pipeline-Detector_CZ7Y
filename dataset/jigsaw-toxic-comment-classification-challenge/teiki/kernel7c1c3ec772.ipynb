{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\npre_train_comment = pre_train['comment_text'].fillna('')\npre_test = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\npre_test_comment = pre_test['comment_text'].fillna('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pre_train = pd.read_csv('../input/processed/process_train.csv')\n# pre_train_comment = pre_train['comment_text'].fillna('')\n# pre_test = pd.read_csv('../input/processed/process_test.csv')\n# pre_test_comment = pre_test['comment_text'].fillna('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pre_train[labels].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,SpatialDropout1D, concatenate\nfrom keras.layers import Bidirectional,Bidirectional, GRU, GlobalAveragePooling1D,GlobalMaxPooling1D,GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom sklearn.model_selection import train_test_split\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 30000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(pre_train_comment))\npre_train_comment = tokenizer.texts_to_sequences(pre_train_comment)\npre_test_comment = tokenizer.texts_to_sequences(pre_test_comment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 100\nX_t = pad_sequences(pre_train_comment, maxlen=maxlen)\nX_te = pad_sequences(pre_test_comment, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from gensim.models import KeyedVectors\n# embedding_index = KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz', binary=True)\nfrom gensim.models import KeyedVectors\nembedding_index =  KeyedVectors.load_word2vec_format('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadEmbeddingMatrix(pre_vector):\n    embed_size = 300\n    embeddings_index = dict()\n    for word in pre_vector.wv.vocab:\n        embeddings_index[word] = pre_vector.word_vec(word)\n    print('Loaded %s word vectors.' % len(embeddings_index))\n    \n    gc.collect()\n    all_embs = np.stack(list(embeddings_index.values()))\n    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n    \n    nb_words = len(tokenizer.word_index)\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    gc.collect()\n\n    embeddedCount = 0\n    for word, i in tokenizer.word_index.items():\n        i -= 1\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            embeddedCount += 1\n    print('total embedded:', embeddedCount,'common words')\n    \n    del(embeddings_index)\n    gc.collect()\n\n    return embedding_matrix\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nembedding_matrix = loadEmbeddingMatrix(embedding_index)\npickle.dump(embedding_matrix, open(\"embedding_matrix.pkl\", \"wb\"))\n# import pickle\n# embedding_matrix = pickle.load(open('../input/weigths/embedding_matrix.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen, ))\nx = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)\n# x = SpatialDropout1D(0.2)(x)\nx = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\nx = Bidirectional(GRU(80, return_sequences=True))(x)\n# avg_pool = GlobalAveragePooling1D()(x)\n# max_pool = GlobalMaxPooling1D()(x)\n# conc = concatenate([avg_pool, max_pool])\n# outp = Dense(6, activation=\"sigmoid\")(conc)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation='relu')(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation='sigmoid')(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\ncheckpoint = ModelCheckpoint('weights_word2vec.best.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='max')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"max\", patience=20)\ncallbacks_list = [checkpoint, early]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n             optimizer='adam',\n             metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 2\n# with tf.device('/gpu:0'):\nhist = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('weights_word2vec.best.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = model.predict(X_te)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\nsample_submission[labels] = y_test\n\nsample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}