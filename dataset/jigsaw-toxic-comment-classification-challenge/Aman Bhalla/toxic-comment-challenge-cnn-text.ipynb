{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Toxic Comment Classification Challenge\nGiven a comment, identify the following labels associated with it:\n* Toxic \n* Severetoxic\n* Obscene\n* threat\n* Insult\n* Identityhate\n\nThis is a **multi-label classification**, meaning, that a comment can belong to zero or more labels at the same time.\nThe evauation metric used has been mean column-wise ROC AUC. \n\nThis notebook has been divided into following sections:\n\n1. Importing the libraries\n2. Reading the Training dataset\n3. Performing EDA\n4. Preprocessing of the Text\n5. Defining the Model\n6. Training\n7. Testing\n***\n### Importing the libraries  \n\nIn the block below, required libraries for this notebook has been imported."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"\"\"\"Importing the required libraries\"\"\"\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.layers import *\nfrom keras import backend, Model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the Training dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"\"\"\"Reading the training dataset and performing the EDA ( Exploratory Data Analysis ) in the upcoming cells\"\"\"\n\npath = '../input/'\ncomp = 'jigsaw-toxic-comment-classification-challenge/'\ndataset = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Performing EDA  \nEDA is Exploratory Data Analysis which is basically analysis of the data you have been given before you can start working on it.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"Performing EDA ( Exploratory Data Analysis ) \"\"\"\n\n\n#Having a look at the data\nprint(\"Number of rows in data = \", dataset.shape[0])\nprint(\"Number of columns in data = \", dataset.shape[1])\nprint(\"\\n\")\nprint(\"---Sample data---\")\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\nHaving a look at the plot of Comments vs. Label.  \nThis will give us the count of the comments belonging to each label.  \n**Note**: A comment can belong to several labels ( multi-label classification )"},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"Performing EDA ( Exploratory Data Analysis ) \"\"\"\n\n#Visualising the Training data : Number of comments vs. Label\nlabels = dataset.columns.values[2:]\nlabels_count = dataset.iloc[:, 2:].sum().values\nsns.set(font_scale = 2)\nplt.figure(figsize = (15, 8))\nfig = sns.barplot(labels, labels_count)\nplt.title(\"Comments vs. Label\", fontsize = 24)\nplt.ylabel('Number of comments', fontsize = 20)\nplt.xlabel('Label ', fontsize = 20)\nrects = fig.patches\nfor rect, label in zip(rects, labels_count):\n    height = rect.get_height()\n    fig.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha = 'center', va = 'bottom', fontsize = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing of the Text\n\nBetter the data, better the model. Henceforth, text will be preprocessed.  \nPreprocessing involves cleaning the data like, removing HTML tags, stop-words, non-ASCII characters, converting to lowercase etc."},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"Function for Text Preprocessing\"\"\"\n\n#Removing non-english symbols, HTML tags, converting to lower-case, lemmatizing, and finally removing the stop-words \nstop_words = set(stopwords.words(\"english\")) \nstop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(X):\n    processed = []\n    for text in X:\n        text = text[0]\n        text = re.sub(r'[^\\w\\s]', '',text, re.UNICODE)\n        text = re.sub('\\n', ' ',text, re.UNICODE)\n        text = re.sub('<.*?>', '', text)\n        text = text.lower()\n        text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n        text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n        text = [word for word in text if not word in stop_words]\n        processed.append(text)\n    return processed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing the data so as to prepare the Training set"},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"Getting the X and Y and preprocessing them\"\"\"\n\nX = dataset.iloc[:, 1:2].values\ny_train = dataset.iloc[:, 2:8].values\nX_train = clean_text(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenizing the comments and converting them into a sequence of integers of fixed length( maxlen ).  \nNeural networks are basically mathematical models, they can't accept characters."},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"Tokenization and Padding\"\"\"\nvocab_size = 10000\nmaxlen = 250\nembed_dim = 20\nbatch_size = 64\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\ntokenized_word_list = tokenizer.texts_to_sequences(X_train)\nX_train_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### EarlyStopping  \nIt can be used to prevent overfitting.It basically waits a few epochs (2), monitoring the loss for the validation dataset.If the loss doesn't decrease for 2 epochs, it stops the training process.\n\n#### ModelCheckpoint\nIt is used for saving the best model during training. After each epoch, it takes a look at the Validation accuracy, if it improves globally, this is the best model we have seen till now during the training process and hence, saves it."},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"EarlyStopping and ModelCheckpoint\"\"\"\n\nes = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 2)\nmc = ModelCheckpoint('model_best.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining the Model\nI have used TextCNN with 64 filters of size 3 and 5 each, followed by GlobalMaxPool (both 1-D) and concatenated their output\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"Creating TextCNN model for comment classification\"\"\"\n\n#Defining the Layers of the model\ninput_X = Input(shape=(maxlen, ))\nembed = Embedding(vocab_size, embed_dim)(input_X)\nconv_1 = Conv1D(filters = 32, kernel_size = 2, activation = 'relu', padding = 'valid')(embed)\nout_1 = GlobalMaxPooling1D()(conv_1)\nconv_2 = Conv1D(filters = 32, kernel_size = 4, activation = 'relu', padding = 'valid')(embed)\nout_2 = GlobalMaxPooling1D()(conv_2)\nconc = concatenate([out_1, out_2])\ndense1 = Dense(32, activation = 'relu')(conc)\nout = Dense(6, activation = 'sigmoid', name = 'output_layer')(dense1)\n\n#Defining the model now\nmodel = Model(input_X, out)\nmodel.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training\nTraining the Model using 20% of the training set as validation set"},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"Fitting the model\"\"\"\nX_train, X_val, y_train, y_val = train_test_split(X_train_padded, y_train, test_size = 0.2, random_state = 42, shuffle = True)\nmodel.fit(X_train, y_train, epochs = 10, batch_size = batch_size, verbose = 1, validation_data = [X_val, y_val], callbacks = [es, mc])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing\nImporting the test set and making it ready "},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"Importing the Test Data and making it ready to be passed to the Model\"\"\"\n\ndataset2 = pd.read_csv('../input/test.csv')\nX_test = dataset2.iloc[:, 1:2].values\nX_test = clean_text(X_test)\ntokenized_word_list = tokenizer.texts_to_sequences(X_test)\nX_test_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"Testing and creating the test results\"\"\"\n\nlabels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nmodel = load_model('model_best.h5')\ny_test = model.predict(X_test_padded, batch_size = 512, verbose = 1)\nsample_submission = pd.read_csv(f'{path}sample_submission.csv')\nsample_submission[labels] = y_test\nsample_submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}