{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import DataLoader\nfrom scipy.special import softmax\nfrom sklearn.metrics import precision_recall_curve, auc\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextEncodedDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_ds(df, tokenizer):\n    labels = df[\"toxic\"].values\n    text = df[\"comment_text\"].tolist()\n    encodings = tokenizer(text, truncation=True, padding=True)\n    ds = TextEncodedDataset(encodings, labels)\n    return ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_filt_full, val = train_test_split(df, test_size=0.2, random_state=0)\nval_ds = create_ds(val, tokenizer)\nval_loader = DataLoader(val_ds, batch_size=16, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(trainer, val_loader):\n    res = trainer.prediction_loop(val_loader, description=\"Preds\")\n    logits = res[0]\n    labels = res[1]\n    pred_probas = softmax(logits, axis=1)[:, 1]\n    precision, recall, thresholds = precision_recall_curve(labels, pred_probas)\n    auc_score = auc(recall, precision)\n    return {\"auc_score\": auc_score, \"pos_samples\": labels.sum()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_sample_list = [100, 500 , 2000, 10000, 50000] #, 100000]\n\n\nd_res = dict()\nfor n_samples in n_sample_list[:1]:\n    print(\"Now running\", n_samples)\n    train_filt = train_filt_full.sample(n_samples, random_state=0)\n    train_filt, test = train_test_split(train_filt, test_size=0.2, random_state=0)\n    train_dataset = create_ds(train_filt, tokenizer)\n    test_dataset = create_ds(test, tokenizer)\n\n    training_args = TrainingArguments(\n        output_dir='./results',          # output directory\n        num_train_epochs=100,              # total number of training epochs\n        per_device_train_batch_size=16,  # batch size per device during training\n        per_device_eval_batch_size=64,   # batch size for evaluation\n        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n        weight_decay=0.01,               # strength of weight decay\n        logging_dir='./logs',            # directory for storing logs\n        logging_steps=10,\n    )\n\n    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n\n    trainer = Trainer(\n        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n        args=training_args,                  # training arguments, defined above\n        train_dataset=train_dataset,         # training dataset\n        eval_dataset=test_dataset             # evaluation dataset\n    )\n\n    trainer.train()\n    d_res[n_samples] = evaluate(trainer, val_loader)\n     \n    with open(\"results.pkl\", \"wb\") as f:\n        pickle.dump(d_res, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tests = [\n    \"Please go home you stupid retard\",\n    \"I think this was very nice!\",\n    \"It's all about not doing things and I kind of like that\",\n    \"I think this is the most stupid shit I have seen\"\n]\nlabels = [0] * len(tests)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings = tokenizer(tests, truncation=True, padding=True)\nds = TextEncodedDataset(encodings, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = trainer.predict(ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"softmax(res.predictions, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}