{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df.comment_text.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Input,GlobalMaxPooling1D\nfrom keras.layers import Conv1D,MaxPooling1D,Embedding\nfrom keras.models import Sequential\nfrom sklearn.metrics import roc_auc_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some configuration\nmax_sequence_length = 100\nmax_vocab_size = 20000\nembedding_dim = 100\nvalidation_split = 0.2\nbatch_size = 128\nepoch = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lem = WordNetLemmatizer()\nps = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#corpus = []\n#for i in range(0,len(df)):\n #   review = re.sub('[^0-9a-zA-Z]',' ',df['comment_text'][i])\n  #  review = review.lower()\n   # review = review.split()\n    \n    #review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    #review = ' '.join(review)\n    #corpus.append(review)\n# doing the above would take a lot of time so we just load in the pretrained values using the glove file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load in pre-trained vectors\n# loading word vectors by using pre trained glove.6B.txt file\nprint('Loading word vectors...')\nword2vec = {}\nwith open(os.path.join('../input/glove6b/glove.6B.%sd.txt' % embedding_dim)) as f:\n    # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\n        print('Found %s word vectors,.' % len(word2vec))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare text samples and their lables\nprint('loading in comments...')\nsentences = df['comment_text'].values\n# sentences stores the comments in the form of an array\nsentences[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"possible_lables = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\ntargets = df[possible_lables].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('max sequence length:',max(len(s) for s in sentences))\nprint('min sequence length:',min(len(s) for s in sentences))\ns= sorted(len(s) for s in sentences)\nprint('median sequence length', s[len(s) // 2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the sentences into tokens/integers\ntokenizer = Tokenizer(num_words=max_vocab_size)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pad sequences so that we get a NxT matrix\ndata = pad_sequences(sequences,maxlen=max_sequence_length)\nprint('shape of our data tensor', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(max_vocab_size, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in word2idx.items():\n      if i < max_vocab_size:\n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n          # words not found in embedding index will be all zeros.\n          embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pre-trained word embeddings into an embeddding layer\nembedding_layer = Embedding(num_words,\n                           embedding_dim,\n                           weights=[embedding_matrix],\n                           input_length= max_sequence_length,\n                           trainable = False)\n\nprint('Building Model...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\ninput_ = Input(shape=(max_sequence_length,))\n\nmodel.add(Embedding(num_words,\n                           embedding_dim,\n                           weights=[embedding_matrix],\n                           input_length= max_sequence_length,\n                           trainable = False))\n\nmodel.add(Conv1D(128,3,activation='relu'))\nmodel.add(MaxPooling1D(3))\n\nmodel.add(Conv1D(128,3,activation='relu'))\nmodel.add(MaxPooling1D(3))\n\nmodel.add(Conv1D(128,3,activation='relu'))\nmodel.add(GlobalMaxPooling1D())\n\nmodel.add(Dense(len(possible_lables),activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n             optimizer='adam',\n             metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training model')\n\nr = model.fit(data,targets,\n              batch_size=batch_size,\n             epochs=epoch,\n             validation_split=validation_split,\n             callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_losses = pd.DataFrame(r.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_losses[['loss','val_loss']].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_losses[['accuracy','val_accuracy']].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = model.predict(data)\naucs = []\nfor j in range(6):\n    auc = roc_auc_score(targets[:,j],p[:,j])\n    aucs.append(auc)\nprint(np.mean(aucs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}