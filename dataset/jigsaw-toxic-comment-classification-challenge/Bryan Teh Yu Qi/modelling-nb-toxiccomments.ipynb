{"cells":[{"metadata":{},"cell_type":"markdown","source":"An NLP attempt at predicting toxic, obscene, threats, insults, identity hatred comments. First Text Processing, then Vectorized with Tf-Idf, then model with NB. \n\nThe score of the model on the test.csv dataset was 0.88.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Read and Analyze Data...","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\ndf = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ndf_test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Size of dataset: ',df.shape)\nprint(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Count of cases positive:')\npositive = []\npositive_perc = []\nlabel_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nfor i in label_cols:\n    print(i,'-',len(df[df[i]==1]),',',str(round(100*len(df[df[i]==1])/len(df),2))+'%')\n    positive.append(len(df[df[i]==1]))\n    positive_perc.append(len(df[df[i]==1])/len(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df\ntemp['length'] = temp.comment_text.apply(lambda x: len(x))\ntemp['length'] = temp.length.apply(lambda x: np.log(x))\nplt.figure(figsize=(20,16))\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nprint('Comments that are flagged tend to have shorter length')\n\nlabel_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nfor i in range(len(label_cols)):\n    plt.subplot(2,3,i+1)\n    sns.kdeplot(temp[temp[label_cols[i]]==0]['length'],label='ntf')\n    sns.kdeplot(temp[temp[label_cols[i]]==1]['length'],label='flagged')\n    plt.title(label_cols[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nfor j in range(len(label_cols)):\n    print(label_cols[j],'comments:')\n    for i in range(len(df)):\n        print(str(i+1),':',list(df[df[label_cols[j]]==1].comment_text)[i],'\\n')\n        if i == 1: break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Preprocessing Text...**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import wordnet\nfrom collections import Counter\n\ndef get_part_of_speech(word):\n  probable_part_of_speech = wordnet.synsets(word)\n  pos_counts = Counter()\n  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n  \n  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n  return most_likely_part_of_speech","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('checking')\nprint(list(df[df[label_cols[j]]==1].comment_text)[0])\nprint(list(df[df[label_cols[j]]==1].comment_text)[1])\nprint(list(df[df[label_cols[j]]==1].comment_text)[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing preprocessing methods on a sample:\n\n1. Remove non-words: punctuations\n2. tokenize - break up by individual words\n3. lowercase - no capital letters\n4. lemmatize - reduce to base form\n5. remove stopwords - reduce insignificant words(i, you, to)\n6. un-tokenize","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nprint('BEFORE:\\n',list(df[df[label_cols[j]]==1].comment_text)[0],'\\n')\n\ncleaned = re.sub('\\W+',' ',list(df[df[label_cols[j]]==1].comment_text)[0]) \nprint('AFTER clearing non-words:\\n',cleaned,'\\n')\n\nlowered = cleaned.lower()\nprint('AFTER setting lowercase:\\n',lowered,'\\n')\n\ntokenized = word_tokenize(lowered)\nprint('AFTER tokenizing:\\n',tokenized,'\\n')\n\nfiltered_stopwords = [w for w in tokenized if not w in stopwords.words('english')]\nprint('AFTER removing stopwords:\\n',filtered_stopwords)\n\nlemmatizer = WordNetLemmatizer()\nlemmatized = [lemmatizer.lemmatize(i,get_part_of_speech(i)) for i in filtered_stopwords]\nprint('AFTER lemmatizing:\\n',lemmatized)\n\nregroup = \" \".join(lemmatized)\nprint('AFTER regrouping:\\n',regroup)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing preprocessing computational time:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\ndef text_processing(text_list):\n    processed_text = []\n    for text in text_list:\n        cleaned = re.sub('\\W+',' ',text)\n        lowered = cleaned.lower()\n        tokenized = word_tokenize(lowered)\n        filtered_stopwords = [w for w in tokenized if not w in stopwords.words('english')]\n        lemmatized = [lemmatizer.lemmatize(i,get_part_of_speech(i)) for i in filtered_stopwords]\n        processed_text.append(\" \".join(lemmatized))\n    return processed_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nfrom datetime import timedelta\nimport time\n\nbefore = datetime.now()\nprocessed_text = text_processing(df['comment_text'][0:1000])\nafter = datetime.now()\ntime_delta = after - before\nseconds = time_delta.total_seconds()\nminutes = seconds/60\nprint(1000,'rows takes:',round(minutes,4),'minutes')\nprint(len(df)+len(df_test),'rows takes:',round((len(df)+len(df_test))*minutes/1000,4),'minutes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_text = text_processing(df['comment_text'])\ntest_processed_text = text_processing(df_test['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Option 1: Vectorize the training dataset\n* Option 2: Tdidf the training dataset\n\n** Future work: We run into memory problems with Option1. Use tdidf for now to ignore too frequent words","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Modelling: Multinomial Naive Bayes...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* undersampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"no_positive = len(df[df['toxic'] == 1])\nnegative_indices = df[df['toxic']==0].index\nrandom_negative_indices = np.random.choice(negative_indices,no_positive, replace=False)\npositive_indices = df[df['toxic'] == 1].index\nunder_sample_indices = np.concatenate([positive_indices,random_negative_indices])\nunder_sample = df.loc[under_sample_indices]\n\nretained_processed_text = []\nfor i in under_sample.index:\n    retained_processed_text.append(processed_text[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* splitting train set and test set within the 'train.csv'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train document with X_Test_y_test\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(retained_processed_text, under_sample.toxic, test_size = 0.2, random_state = 6)\nprint(len(x_train))\nprint(len(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* build tf-idf vectorizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_creator = TfidfVectorizer(max_df=0.60)\nx_train_tfidf = tfidf_creator.fit_transform(x_train)\nx_test_tfidf = tfidf_creator.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#not used for now CountVectorizer\n# option = 2 #1, 2\n# if option == 1:\n#     from sklearn.feature_extraction.text import CountVectorizer\n#     counter = CountVectorizer()\n#     counter.fit(processed_text) \n#     counter.vocabulary_\n#     vectorized_text = counter.transform(processed_text).toarray()\n# else:\n#     pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* model of nb","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(x_train_tfidf,np.array(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(x_train_tfidf)\nclassifier.predict_proba(x_train_tfidf)\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\nprint('Train dataset:')\n \nresult = pd.DataFrame(confusion_matrix(y_train, y_pred))\ndisplay(result)\nprint('accuracy:\\t',round(accuracy_score( y_train , y_pred),2))\nprint('recall:\\t\\t',round(recall_score( y_train , y_pred),2))\nprint('precision:\\t',round(precision_score( y_train , y_pred),2))\nprint('f1:\\t\\t',round(f1_score( y_train , y_pred),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(x_test_tfidf)\nclassifier.predict_proba(x_test_tfidf)\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n\nprint('Test dataset:')\nresult = pd.DataFrame(confusion_matrix(y_test, y_pred))\ndisplay(result)\nprint('accuracy:\\t',round(accuracy_score( y_test , y_pred),2))\nprint('recall:\\t\\t',round(recall_score( y_test , y_pred),2))\nprint('precision:\\t',round(precision_score( y_test , y_pred),2))\nprint('f1:\\t\\t',round(f1_score( y_test , y_pred),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compiling Submission...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Do it on whole training dataset and then test dataset for submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nsubmissions = df_test[['id']]\n\n#6 nb models to predict 6 flags\nfor i in label_cols:\n    \n    #undersampling for each nb model\n    no_positive = len(df[df[i] == 1])\n    negative_indices = df[df[i] == 0].index\n    random_negative_indices = np.random.choice(negative_indices,no_positive, replace=False)\n    positive_indices = df[df[i] == 1].index\n    under_sample_indices = np.concatenate([positive_indices,random_negative_indices])\n    under_sample = df.loc[under_sample_indices]\n\n    retained_processed_text = []\n    for j in under_sample.index:\n        retained_processed_text.append(processed_text[j])\n    \n    #vectorized for each nb model\n    tfidf_creator = TfidfVectorizer(max_df=0.60,binary=True)\n    x_train_tfidf = tfidf_creator.fit_transform(retained_processed_text)\n    x_test_tfidf = tfidf_creator.transform(test_processed_text)\n        \n    #nb model for each flags\n    classifier = MultinomialNB()\n    classifier.fit(x_train_tfidf,np.array(under_sample[i]))\n    y_pred = classifier.predict(x_test_tfidf)\n    submissions[i] = y_pred.reshape(-1,1)\n    print('done for',i)\n    \ndisplay(submissions.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissions.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}