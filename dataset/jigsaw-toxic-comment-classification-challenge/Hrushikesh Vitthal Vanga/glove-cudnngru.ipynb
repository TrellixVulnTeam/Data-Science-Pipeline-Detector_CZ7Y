{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 50000","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\nprint(train_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#toxic\nsns.countplot(train_data.toxic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#severe_toxic\nsns.countplot(train_data.severe_toxic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#obscene\nsns.countplot(train_data.obscene)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#threat\nsns.countplot(train_data.threat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#insult\nsns.countplot(train_data.insult)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#identity_hate\nsns.countplot(train_data.identity_hate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = train_data['comment_text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english')) \n\nt_data = list()\n\nfor i in range(len(train_text)):\n    \n    if i % 10000 == 0:\n        print(i)\n\n    words = nltk.word_tokenize(train_text[i])\n    \n    words = [word for word in words if word not in stop_words] \n\n    words=[word.lower() for word in words if word.isalpha()]\n    \n    # remove single character\n\n    words = [word for word in words if len(word) > 1]\n    \n    t_data.append(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer \n  \nlemmatizer = WordNetLemmatizer()\n\ndata_l = list()\nfor i in range(len(t_data)):\n    temp = list()\n    for j in t_data[i]:\n        temp.append(lemmatizer.lemmatize(j))\n    data_l.append(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = list()\n\nfor i in data_l:\n    for j in i:\n        vocab.append(j)\n# no of words in text\nlen(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no of unique words\n\nvocab = set(vocab)\nlen(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n# function to build a tokenizer\n\ndef tokenization(lines):\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\neng_tokens = tokenization(data_l)\neng_vocab_size = len(eng_tokens.word_index) + 1\nprint('English Vocabulary Size: %d' % eng_vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = list()\nfor i in range(len(data_l)):\n    m.append(len(data_l[i]))\nplt.plot(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n# encode and pad sequences\ndef encode_sequences(tokenizer,length,lines):\n    # integer encode sequences\n    seq = tokenizer.texts_to_sequences(lines)\n    # pad sequences with 0 values\n    seq = pad_sequences(seq, maxlen=length, padding='post')\n    return seq\n\nseq_data = encode_sequences(eng_tokens,400,data_l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove6b50dtxt/glove.6B.50d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = eng_tokens.word_index\nnb_words = min(eng_vocab_size - 1, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= eng_vocab_size - 1: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = train_data.iloc[:,2:].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import *\n\nmodel = Sequential()\nmodel.add(Embedding(eng_vocab_size - 1,\n                    embed_size,\n                    weights=[embedding_matrix],\n                    input_length=400,\n                    trainable=True))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Bidirectional(CuDNNGRU(128, return_sequences=True)))\nmodel.add(Bidirectional(CuDNNGRU(64)))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\nhistory = model.fit(seq_data, targets, epochs=3, batch_size=batch_size, verbose=1, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\nprint(test_data.shape)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = test_data['comment_text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_data_test = list()\n\nfor i in range(len(test_text)):\n    \n    if i % 10000 == 0:\n        print(i)\n\n    words = nltk.word_tokenize(test_text[i])\n    \n    words = [word for word in words if word not in stop_words] \n\n    words=[word.lower() for word in words if word.isalpha()]\n    \n    # remove single character\n\n    words = [word for word in words if len(word) > 1]\n    \n    t_data_test.append(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_l_test = list()\nfor i in range(len(t_data_test)):\n    temp = list()\n    for j in t_data_test[i]:\n        temp.append(lemmatizer.lemmatize(j))\n    data_l_test.append(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_data_test = encode_sequences(eng_tokens,400,data_l_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(seq_data_test,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sam_sub = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\nsam_sub.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_id = test_data['id'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_id = _id.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = np.array(np.concatenate((_id, pred), 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame(output,columns = ['id', 'toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}