{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\n\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nimport tokenizers\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, auc\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom tqdm import tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-11T08:14:34.934262Z","iopub.execute_input":"2022-05-11T08:14:34.934553Z","iopub.status.idle":"2022-05-11T08:14:34.948928Z","shell.execute_reply.started":"2022-05-11T08:14:34.934518Z","shell.execute_reply":"2022-05-11T08:14:34.9479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualization","metadata":{}},{"cell_type":"code","source":"train_table = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_table = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_tags = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\ntrain_table.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T08:14:37.466104Z","iopub.execute_input":"2022-05-11T08:14:37.466641Z","iopub.status.idle":"2022-05-11T08:14:40.307521Z","shell.execute_reply.started":"2022-05-11T08:14:37.466603Z","shell.execute_reply":"2022-05-11T08:14:40.306707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of Lost Values","metadata":{}},{"cell_type":"markdown","source":"We only choose 30000 data for training.","metadata":{}},{"cell_type":"code","source":"train_table = train_table.iloc[0:30000,:]\ntrain_table.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T08:14:58.794868Z","iopub.execute_input":"2022-05-11T08:14:58.795509Z","iopub.status.idle":"2022-05-11T08:14:58.806331Z","shell.execute_reply.started":"2022-05-11T08:14:58.79547Z","shell.execute_reply":"2022-05-11T08:14:58.805619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_table.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:02:58.946017Z","iopub.execute_input":"2022-05-11T07:02:58.946541Z","iopub.status.idle":"2022-05-11T07:02:58.967498Z","shell.execute_reply.started":"2022-05-11T07:02:58.946499Z","shell.execute_reply":"2022-05-11T07:02:58.966711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tags = train_table.drop(['id', 'comment_text'], axis = 1)\nlabel_counts = train_tags.sum()\ndf_counts = pd.DataFrame(label_counts)\ndf_counts.rename(columns = {0:'counts'}, inplace = True)\ndf_counts = df_counts.sort_values('counts', ascending = False)\ndf_counts","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:02:58.968597Z","iopub.execute_input":"2022-05-11T07:02:58.968795Z","iopub.status.idle":"2022-05-11T07:02:58.98396Z","shell.execute_reply.started":"2022-05-11T07:02:58.968771Z","shell.execute_reply":"2022-05-11T07:02:58.983186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could see the data is quite unbalanced. Among 30,000 data, only 10% are in class \"toxic\".","metadata":{}},{"cell_type":"code","source":"ax = df_counts.plot.barh(width=0.7, fontsize='24', figsize=(12,5));\nax.legend(bbox_to_anchor=(1, 0.3), fontsize='16');\nfor p in ax.patches:\n    w = p.get_width()\n    ax.annotate(f'{w:d}', (w * 1, p.get_y() + 0.1))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:02:58.985462Z","iopub.execute_input":"2022-05-11T07:02:58.98573Z","iopub.status.idle":"2022-05-11T07:02:59.263434Z","shell.execute_reply.started":"2022-05-11T07:02:58.985695Z","shell.execute_reply":"2022-05-11T07:02:59.262749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seem that data having tags are far smaller than those not","metadata":{}},{"cell_type":"code","source":"import random\n\ntrain_tags.loc[random.sample(range(train_tags.shape[0]),5)]","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:02:59.264617Z","iopub.execute_input":"2022-05-11T07:02:59.266007Z","iopub.status.idle":"2022-05-11T07:02:59.278161Z","shell.execute_reply.started":"2022-05-11T07:02:59.265966Z","shell.execute_reply":"2022-05-11T07:02:59.277483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Could tell from the table that a row could have multiple tags.","metadata":{}},{"cell_type":"code","source":"comment = train_table['comment_text']\ncomment.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:02:59.279633Z","iopub.execute_input":"2022-05-11T07:02:59.279918Z","iopub.status.idle":"2022-05-11T07:02:59.286109Z","shell.execute_reply.started":"2022-05-11T07:02:59.279882Z","shell.execute_reply":"2022-05-11T07:02:59.285357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idxs = random.sample(range(comment.shape[0]),5)\ntexts = comment.loc[idxs].reset_index(drop=True)\nfor i in range(5):\n    print(repr('%d: %s'%(i,list(texts)[i][:128])))\n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:02:59.287582Z","iopub.execute_input":"2022-05-11T07:02:59.288039Z","iopub.status.idle":"2022-05-11T07:02:59.298907Z","shell.execute_reply.started":"2022-05-11T07:02:59.288004Z","shell.execute_reply":"2022-05-11T07:02:59.297996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text preprocessing","metadata":{}},{"cell_type":"code","source":"import re\n\ndef text_preprocessing(text):\n    #lower case\n    text = text.lower()\n    \n    #pattern = [zero or more character]\n    text = re.sub('\\[.*?\\]', '', text)\n    \n    #pattern = (zero or more character)\n    text = re.sub('\\(.*?\\)', '', text)\n    \n    #pattern = with or without(http),://, one or more non-white space character, OR www, .,one or more non-white space character\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    \n    #pattern = @some characters + space\n    text = re.sub(r'(@.*?)[\\s]', ' ', text) \n    \n#     #pattern = num\n#     text = re.sub(r'[0-9]+' , '' ,text)\n    \n    #pattern = space+@+A-Za-z0-9_\n    text = re.sub(r'\\s([@][\\w_-]+)', '', text).strip()\n    \n    #pattern = &amp\n    text = re.sub(r'&amp;', '&', text)\n    \n    #pattern = multiple space\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    #replace #\n    text = text.replace(\"#\" , \" \")\n    \n    #pattern = any new line\n    text = re.sub('\\n', '', text)\n    \n    encoded_string = text.encode(\"ascii\", \"ignore\")\n    decode_string = encoded_string.decode()\n    return decode_string","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:02:59.302944Z","iopub.execute_input":"2022-05-11T07:02:59.303156Z","iopub.status.idle":"2022-05-11T07:02:59.311342Z","shell.execute_reply.started":"2022-05-11T07:02:59.303131Z","shell.execute_reply":"2022-05-11T07:02:59.310432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import re\n# import string\n\n# def clean_text(text):\n#     '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n#     and remove words containing numbers.'''\n#     #text = text.lower()\n    \n#     #pattern = [zero or more character]\n#     text = re.sub('\\[.*?\\]', '', text)\n    \n#     #pattern = with or without(http),://, one or more non-white space character, OR www, .,one or more non-white space character\n#     text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    \n#     #pattern = <, zero or more characters, >, (one or more occurance of >)\n#     text = re.sub('<.*?>+', '', text)\n    \n#     #pattern = any punctionation\n#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    \n#     #pattern = any new line\n#     text = re.sub('\\n', '', text)\n    \n#     #pattern = any from[a-zA-Z0-9_], any from[0-9], any from [a-zA-Z0-9_]\n#     text = re.sub('\\w*\\d\\w*', '', text)\n#     return text","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:02:59.31295Z","iopub.execute_input":"2022-05-11T07:02:59.313444Z","iopub.status.idle":"2022-05-11T07:02:59.320644Z","shell.execute_reply.started":"2022-05-11T07:02:59.313402Z","shell.execute_reply":"2022-05-11T07:02:59.319904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_table['processed_text'] = comment.apply(str).apply(lambda x: text_preprocessing(x))\ntrain_table['processed_text_length'] = train_table['processed_text'].apply(lambda x:len(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:02:59.323039Z","iopub.execute_input":"2022-05-11T07:02:59.32405Z","iopub.status.idle":"2022-05-11T07:03:01.144451Z","shell.execute_reply.started":"2022-05-11T07:02:59.324007Z","shell.execute_reply":"2022-05-11T07:03:01.143717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyze the length of label words","metadata":{}},{"cell_type":"code","source":"other_comment_length = train_table['processed_text_length'][(train_table[\"toxic\"] != 1) & (train_table[\"severe_toxic\"] != 1) & (train_table[\"obscene\"] != 1) & (train_table[\"threat\"] != 1) & (train_table[\"insult\"] != 1) & (train_table[\"identity_hate\"] != 1)].reset_index(drop = True)\nax1 = other_comment_length.plot.hist(title = \"other_comments_counts\")","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:01.146738Z","iopub.execute_input":"2022-05-11T07:03:01.147135Z","iopub.status.idle":"2022-05-11T07:03:01.37341Z","shell.execute_reply.started":"2022-05-11T07:03:01.1471Z","shell.execute_reply":"2022-05-11T07:03:01.372695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_comment_length = train_table['processed_text_length'][(train_table[\"toxic\"] == 1) | (train_table[\"severe_toxic\"] == 1) | (train_table[\"obscene\"] == 1) | (train_table[\"threat\"] == 1) | (train_table[\"insult\"] == 1) | (train_table[\"identity_hate\"] == 1)].reset_index(drop = True)\nax2 = toxic_comment_length.plot.hist(title = \"toxic_comments_counts\")","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:01.374737Z","iopub.execute_input":"2022-05-11T07:03:01.37509Z","iopub.status.idle":"2022-05-11T07:03:01.581617Z","shell.execute_reply.started":"2022-05-11T07:03:01.375052Z","shell.execute_reply":"2022-05-11T07:03:01.580966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyze most common word in text","metadata":{}},{"cell_type":"code","source":"all_comments = []\nfor item in train_table['processed_text']:\n    all_comments.append(item)\ncommonWord = ' '.join(all_comments)\n\nfrom wordcloud import WordCloud,STOPWORDS\ncommon_word_cloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(commonWord)\n\nplt.figure(1,figsize=(12, 12))\nplt.imshow(common_word_cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:01.583027Z","iopub.execute_input":"2022-05-11T07:03:01.583288Z","iopub.status.idle":"2022-05-11T07:03:24.374584Z","shell.execute_reply.started":"2022-05-11T07:03:01.583253Z","shell.execute_reply":"2022-05-11T07:03:24.373889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_comments = []\nfor item in train_table[\"processed_text\"][(train_table[\"toxic\"] == 1) | (train_table[\"severe_toxic\"] == 1) | (train_table[\"obscene\"] == 1) | (train_table[\"threat\"] == 1) | (train_table[\"insult\"] == 1) | (train_table[\"identity_hate\"] == 1)]:\n    toxic_comments.append(item)\ntoxicWord = ' '.join(toxic_comments)\n\ntoxic_word_cloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(toxicWord)\n\nplt.figure(1,figsize=(12, 12))\nplt.imshow(toxic_word_cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:24.375562Z","iopub.execute_input":"2022-05-11T07:03:24.375812Z","iopub.status.idle":"2022-05-11T07:03:41.746431Z","shell.execute_reply.started":"2022-05-11T07:03:24.375782Z","shell.execute_reply":"2022-05-11T07:03:41.7457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"markdown","source":"related_link: https://www.analyticsvidhya.com/blog/2021/07/performing-sentiment-analysis-with-naive-bayes-classifier/","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport joblib\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:41.747753Z","iopub.execute_input":"2022-05-11T07:03:41.748507Z","iopub.status.idle":"2022-05-11T07:03:41.75297Z","shell.execute_reply.started":"2022-05-11T07:03:41.748468Z","shell.execute_reply":"2022-05-11T07:03:41.752182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_table['easy_label'] = np.where(((train_table[\"toxic\"] == 1) | (train_table[\"severe_toxic\"] == 1) | (train_table[\"obscene\"] == 1) | (train_table[\"threat\"] == 1) | (train_table[\"insult\"] == 1) | (train_table[\"identity_hate\"] == 1)), 1, 0)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:41.754163Z","iopub.execute_input":"2022-05-11T07:03:41.754871Z","iopub.status.idle":"2022-05-11T07:03:41.769319Z","shell.execute_reply.started":"2022-05-11T07:03:41.754811Z","shell.execute_reply":"2022-05-11T07:03:41.768324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_data = train_table[[\"processed_text\", \"easy_label\"]]","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:41.771133Z","iopub.execute_input":"2022-05-11T07:03:41.771422Z","iopub.status.idle":"2022-05-11T07:03:41.783269Z","shell.execute_reply.started":"2022-05-11T07:03:41.771385Z","shell.execute_reply":"2022-05-11T07:03:41.7825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_x = nb_data['processed_text']\nnb_y = nb_data['easy_label']\nnb_x, nb_x_test, nb_y, nb_y_test = train_test_split(nb_x,nb_y, stratify=nb_y, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:41.784809Z","iopub.execute_input":"2022-05-11T07:03:41.785154Z","iopub.status.idle":"2022-05-11T07:03:41.808875Z","shell.execute_reply.started":"2022-05-11T07:03:41.785104Z","shell.execute_reply":"2022-05-11T07:03:41.808056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec = CountVectorizer(stop_words='english')\nnb_x = vec.fit_transform(nb_x).toarray()\nnb_x_test = vec.transform(nb_x_test).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:41.810142Z","iopub.execute_input":"2022-05-11T07:03:41.810984Z","iopub.status.idle":"2022-05-11T07:03:44.856257Z","shell.execute_reply.started":"2022-05-11T07:03:41.810943Z","shell.execute_reply":"2022-05-11T07:03:44.85546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_x.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:44.857716Z","iopub.execute_input":"2022-05-11T07:03:44.857994Z","iopub.status.idle":"2022-05-11T07:03:44.863386Z","shell.execute_reply.started":"2022-05-11T07:03:44.857959Z","shell.execute_reply":"2022-05-11T07:03:44.862675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nnb_model = MultinomialNB()\nnb_model.fit(nb_x, nb_y)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:03:44.864766Z","iopub.execute_input":"2022-05-11T07:03:44.865224Z","iopub.status.idle":"2022-05-11T07:04:33.766076Z","shell.execute_reply.started":"2022-05-11T07:03:44.865187Z","shell.execute_reply":"2022-05-11T07:04:33.765273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_model.score(nb_x_test, nb_y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:04:33.767567Z","iopub.execute_input":"2022-05-11T07:04:33.768069Z","iopub.status.idle":"2022-05-11T07:04:35.915126Z","shell.execute_reply.started":"2022-05-11T07:04:33.768025Z","shell.execute_reply":"2022-05-11T07:04:35.914295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, plot_roc_curve, RocCurveDisplay, ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(nb_model, nb_x_test, nb_y_test)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:04:35.919628Z","iopub.execute_input":"2022-05-11T07:04:35.922393Z","iopub.status.idle":"2022-05-11T07:04:37.863441Z","shell.execute_reply.started":"2022-05-11T07:04:35.922341Z","shell.execute_reply":"2022-05-11T07:04:37.862687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_auc_score = roc_auc_score(nb_y_test, nb_model.predict_proba(nb_x_test)[:,1])\nRocCurveDisplay.from_estimator(nb_model, nb_x_test, nb_y_test)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:04:37.8649Z","iopub.execute_input":"2022-05-11T07:04:37.865627Z","iopub.status.idle":"2022-05-11T07:04:41.116334Z","shell.execute_reply.started":"2022-05-11T07:04:37.865585Z","shell.execute_reply":"2022-05-11T07:04:41.115665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting to Tokens","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:04:41.117716Z","iopub.execute_input":"2022-05-11T07:04:41.118179Z","iopub.status.idle":"2022-05-11T07:04:47.844125Z","shell.execute_reply.started":"2022-05-11T07:04:41.118142Z","shell.execute_reply":"2022-05-11T07:04:47.843055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_data[\"tokenized_length\"] = nb_data[\"processed_text\"].apply(lambda x:len(tokenizer.tokenize(x)))\nnb_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:04:47.848525Z","iopub.execute_input":"2022-05-11T07:04:47.848995Z","iopub.status.idle":"2022-05-11T07:05:39.575455Z","shell.execute_reply.started":"2022-05-11T07:04:47.848961Z","shell.execute_reply":"2022-05-11T07:05:39.574716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\nax = sns.histplot(x = \"tokenized_length\", data=nb_data)\nax.set(xlim=(0,350))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:07:36.356294Z","iopub.execute_input":"2022-05-11T07:07:36.35656Z","iopub.status.idle":"2022-05-11T07:07:37.578079Z","shell.execute_reply.started":"2022-05-11T07:07:36.356533Z","shell.execute_reply":"2022-05-11T07:07:37.577368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(nb_data.loc[nb_data[\"tokenized_length\"]<=350])/len(nb_data[\"tokenized_length\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:07:39.625572Z","iopub.execute_input":"2022-05-11T07:07:39.626261Z","iopub.status.idle":"2022-05-11T07:07:39.639138Z","shell.execute_reply.started":"2022-05-11T07:07:39.626213Z","shell.execute_reply":"2022-05-11T07:07:39.63807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could see most length is within 350.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nmax_len = 350\n\nclass BertDataSet(Dataset):\n    \n    def __init__(self, sentences, toxic_labels):\n        self.sentences = sentences\n        #target is a matrix with shape [#1 x #6(toxic, obscene, etc)]\n        self.targets = toxic_labels.to_numpy()\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    \n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        bert_senten = tokenizer.encode_plus(\n                                            text = sentence, \n                                            add_special_tokens = True, # Add `[CLS]` and `[SEP]`\n                                            max_length = max_len, # Max length to truncate/pad\n                                            pad_to_max_length=True, # Pad sentence to max length\n                                            truncation = True, # truncate long sentence\n                                            return_attention_mask = True  # Return attention mask\n                                             )\n        ids = torch.tensor(bert_senten['input_ids'], dtype = torch.long)\n        mask = torch.tensor(bert_senten['attention_mask'], dtype = torch.long)\n        toxic_label = torch.tensor(self.targets[idx], dtype = torch.float)\n        \n        \n        return {\n            'ids' : ids,\n            'mask' : mask,\n            'toxic_label':toxic_label\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:07:42.974257Z","iopub.execute_input":"2022-05-11T07:07:42.974859Z","iopub.status.idle":"2022-05-11T07:07:42.98334Z","shell.execute_reply.started":"2022-05-11T07:07:42.974809Z","shell.execute_reply":"2022-05-11T07:07:42.982278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch.utils.data import DataLoader, Dataset\n\n# max_len = 350\n\n# Class BertDataSet(Dataset):\n#     def __init__(sentences, labels):\n#         self.sentences = sentences\n#         self.labels = labels.to_numpy()\n    \n#     def __len__(self):\n#         return len(self.sentences)\n    \n#     def __getitem__(self,idx):\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:05:39.966552Z","iopub.status.idle":"2022-05-11T07:05:39.966995Z","shell.execute_reply.started":"2022-05-11T07:05:39.96675Z","shell.execute_reply":"2022-05-11T07:05:39.966774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"@ohmeow you're loading the bert-base-cased checkpoint (which is a checkpoint that was trained using a similar architecture to BertForPreTraining) in a BertForSequenceClassification model.\n\nThis means that:\n\nThe layers that BertForPreTraining has, but BertForSequenceClassification does not have will be discarded\nThe layers that BertForSequenceClassification has but BertForPreTraining does not have will be randomly initialized.\nThis is expected, and tells you that you won't have good performance with your BertForSequenceClassification model before you fine-tune it 🙂.\n\n@fliptrail this warning means that during your training, you're not using the pooler in order to compute the loss. I don't know how you're finetuning your model, but if you're not using the pooler layer then there's no need to worry about that warning.","metadata":{}},{"cell_type":"markdown","source":"# Model & Training","metadata":{}},{"cell_type":"code","source":"Kfold = 5\nnb_data['kfold'] = nb_data.index % Kfold\nnb_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:07:48.172019Z","iopub.execute_input":"2022-05-11T07:07:48.173042Z","iopub.status.idle":"2022-05-11T07:07:48.187202Z","shell.execute_reply.started":"2022-05-11T07:07:48.172991Z","shell.execute_reply":"2022-05-11T07:07:48.186385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as func\n\nbatch_size = 16\nepochs = 3\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nloss_fn = nn.BCEWithLogitsLoss()\nloss_fn.to(device)\nscaler = torch.cuda.amp.GradScaler()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:07:50.627151Z","iopub.execute_input":"2022-05-11T07:07:50.62769Z","iopub.status.idle":"2022-05-11T07:07:50.686549Z","shell.execute_reply.started":"2022-05-11T07:07:50.627651Z","shell.execute_reply":"2022-05-11T07:07:50.685815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\ndef train(train_dataloader, model, optimizer, scheduler):\n    \n    model.train()\n    torch.backends.cudnn.benchmark = True\n    correct_predictions = 0\n    \n    for a in tqdm(train_dataloader):\n        losses = []\n        optimizer.zero_grad()\n        \n        #allpreds = []\n        #alltargets = []\n        \n        with torch.cuda.amp.autocast():\n            \n            ids = a['ids'].to(device, non_blocking = True)\n            mask = a['mask'].to(device, non_blocking = True) \n\n            output = model(ids, mask) #This gives model as output, however we want the values at the output\n            output = output['logits'].squeeze(-1).to(torch.float32)\n\n            output_probs = torch.sigmoid(output)\n            preds = torch.where(output_probs > 0.5, 1, 0)\n            \n            toxic_label = a['toxic_label'].to(device, non_blocking = True) \n            loss = loss_fn(output, toxic_label)            \n            \n            losses.append(loss.item())\n            #allpreds.append(output.detach().cpu().numpy())\n            #alltargets.append(toxic.detach().squeeze(-1).cpu().numpy())\n            correct_predictions += torch.sum(preds == toxic_label)\n        \n        scaler.scale(loss).backward() #Multiplies (‘scales’) a tensor or list of tensors by the scale factor.\n                                      #Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.\n        scaler.step(optimizer) #Returns the return value of optimizer.step(*args, **kwargs).\n        scaler.update() #Updates the scale factor.If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. \n                        #If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it\n        scheduler.step() # Update learning rate schedule\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n#     accuracy = corr_preds/(len(train_dataloader)*batch_size*6)\n    accuracy = corr_preds/(len(train_dataloader)*batch_size)\n    \n    return losses, accuracy\n\ndef validate(valid_dataloader, model):\n    \n    model.eval()\n    correct_predictions = 0\n    all_output_probs = []\n    \n    for a in valid_dataloader:\n        losses = []\n        ids = a['ids'].to(device, non_blocking = True)\n        mask = a['mask'].to(device, non_blocking = True)\n        output = model(ids, mask)\n        output = output['logits'].squeeze(-1).to(torch.float32)\n        output_probs = torch.sigmoid(output)\n        preds = torch.where(output_probs > 0.5, 1, 0)\n            \n        toxic_label = a['toxic_label'].to(device, non_blocking = True)\n        loss = loss_fn(output, toxic_label)\n        losses.append(loss.item())\n        all_output_probs.extend(output_probs.detach().cpu().numpy())\n        \n        correct_predictions += torch.sum(preds == toxic_label)\n        corr_preds = correct_predictions.detach().cpu().numpy()\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n#     accuracy = corr_preds/(len(valid_dataloader)*batch_size*6)\n    accuracy = corr_preds/(len(valid_dataloader)*batch_size)\n\n    \n    return losses, accuracy, all_output_probs\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:07:52.369732Z","iopub.execute_input":"2022-05-11T07:07:52.370142Z","iopub.status.idle":"2022-05-11T07:07:52.388156Z","shell.execute_reply.started":"2022-05-11T07:07:52.370104Z","shell.execute_reply":"2022-05-11T07:07:52.387354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\ndef train(train_dataloader, model, optimizer, scheduler):\n    \n    model.train()\n    torch.backends.cudnn.benchmark = True\n    correct_predictions = 0\n    \n    for a in tqdm(train_dataloader):\n        losses = []\n        optimizer.zero_grad()\n        \n        #allpreds = []\n        #alltargets = []\n        \n        with torch.cuda.amp.autocast():\n            \n            ids = a['ids'].to(device, non_blocking = True)\n            mask = a['mask'].to(device, non_blocking = True) \n\n            output = model(ids, mask) #This gives model as output, however we want the values at the output\n            output = output['logits'].squeeze(-1).to(torch.float32)\n\n            output_probs = torch.sigmoid(output)\n            preds = torch.where(output_probs > 0.5, 1, 0)\n            \n            toxic_label = a['toxic_label'].to(device, non_blocking = True) \n            loss = loss_fn(output, toxic_label)            \n            \n            losses.append(loss.item())\n            #allpreds.append(output.detach().cpu().numpy())\n            #alltargets.append(toxic.detach().squeeze(-1).cpu().numpy())\n            correct_predictions += torch.sum(preds == toxic_label)\n        \n        scaler.scale(loss).backward() #Multiplies (‘scales’) a tensor or list of tensors by the scale factor.\n                                      #Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.\n        scaler.step(optimizer) #Returns the return value of optimizer.step(*args, **kwargs).\n        scaler.update() #Updates the scale factor.If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. \n                        #If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it\n        scheduler.step() # Update learning rate schedule\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n    accuracy = corr_preds/(len(train_dataloader)*batch_size*6)\n    \n    return losses, accuracy\n\ndef validate(valid_dataloader, model):\n    \n    model.eval()\n    correct_predictions = 0\n    all_output_probs = []\n    \n    for a in valid_dataloader:\n        losses = []\n        ids = a['ids'].to(device, non_blocking = True)\n        mask = a['mask'].to(device, non_blocking = True)\n        output = model(ids, mask)\n        output = output['logits'].squeeze(-1).to(torch.float32)\n        output_probs = torch.sigmoid(output)\n        preds = torch.where(output_probs > 0.5, 1, 0)\n            \n        toxic_label = a['toxic_label'].to(device, non_blocking = True)\n        loss = loss_fn(output, toxic_label)\n        losses.append(loss.item())\n        all_output_probs.extend(output_probs.detach().cpu().numpy())\n        \n        correct_predictions += torch.sum(preds == toxic_label)\n        corr_preds = correct_predictions.detach().cpu().numpy()\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n    accuracy = corr_preds/(len(valid_dataloader)*batch_size*6)\n    \n    return losses, accuracy, all_output_probs\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:07:52.369732Z","iopub.execute_input":"2022-05-11T07:07:52.370142Z","iopub.status.idle":"2022-05-11T07:07:52.388156Z","shell.execute_reply.started":"2022-05-11T07:07:52.370104Z","shell.execute_reply":"2022-05-11T07:07:52.387354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_scores = []\n\nfor fold in range(5):\n    \n    Xtrain = nb_data[\"processed_text\"][nb_data['kfold'] != fold].reset_index(drop = True)\n    Ytrain = nb_data[\"easy_label\"][nb_data['kfold'] != fold].reset_index(drop = True)\n    Xvalid = nb_data[\"processed_text\"][nb_data['kfold'] == fold].reset_index(drop = True)\n    Yvalid = nb_data[\"easy_label\"][nb_data['kfold'] == fold].reset_index(drop = True)\n    \n    train_dataset = BertDataSet(Xtrain, Ytrain)\n    valid_dataset = BertDataSet(Xvalid, Yvalid)\n    \n    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 2, pin_memory = True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = True, num_workers = 2, pin_memory = True)\n    \n    model = transformers.BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 1) # num_labels!\n    model.to(device)\n    \n    LR = 2e-5\n    optimizer = AdamW(model.parameters(), LR, betas = (0.9, 0.999), weight_decay = 1e-2)\n    \n    train_steps = int(len(Xtrain)/batch_size * epochs)\n    num_steps = int(train_steps * 0.1)\n    \n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n    \n    best_score = 1000\n    train_accs = []\n    valid_accs = []\n    train_losses = []\n    valid_losses = []\n    best_valid_probs = []\n    \n    print(\"-------------- Fold = \" + str(fold) + \"-------------\")\n    \n    for epoch in range(epochs):\n        print(\"-------------- Epoch = \" + str(epoch) + \"-------------\")\n        \n        train_loss, train_acc = train(train_dataloader, model, optimizer, scheduler)\n        valid_loss, valid_acc, valid_probs = validate(valid_dataloader, model)\n\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        valid_losses.append(valid_loss)\n        valid_accs.append(valid_acc)\n        \n        print('train losses: %.4f' %(train_loss), 'train accuracy: %.3f' %(train_acc))\n        print('valid losses: %.4f' %(valid_loss), 'valid accuracy: %.3f' %(valid_acc))\n\n        if (valid_loss < best_score):\n\n            best_score = valid_loss\n            print(\"Found an improved model! :)\")\n\n            state = {'state_dict': model.state_dict(),\n                     'optimizer_dict': optimizer.state_dict(),\n                     'best_score':best_score\n                    }\n\n            torch.save(state, \"model\" + str(fold) + \".pth\")\n            best_valid_prob = valid_probs\n            torch.cuda.memory_summary(device = None, abbreviated = False)\n        else:\n            pass\n\n\n    best_scores.append(best_score)\n    best_valid_probs.append(best_valid_prob)\n    \n    ##Plotting the result for each fold\n    x = np.arange(epochs)\n    fig, ax = plt.subplots(1, 2, figsize = (15,4))\n    ax[0].plot(x, train_losses)\n    ax[0].plot(x, valid_losses)\n    ax[0].set_ylabel('Losses', weight = 'bold')\n    ax[0].set_xlabel('Epochs')\n    ax[0].grid(alpha = 0.3)\n    ax[0].legend(labels = ['train losses', 'valid losses'])\n\n    ax[1].plot(x, train_accs)\n    ax[1].plot(x, valid_accs)\n    ax[1].set_ylabel('Accuracy', weight = 'bold')\n    ax[1].set_xlabel('Epochs')\n    ax[1].legend(labels = ['train acc', 'valid acc'])\n\n    ax[1].grid(alpha = 0.3)\n    fig.suptitle('Fold = '+str(fold), weight = 'bold') \n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:07:54.951221Z","iopub.execute_input":"2022-05-11T07:07:54.951483Z","iopub.status.idle":"2022-05-11T07:48:41.704967Z","shell.execute_reply.started":"2022-05-11T07:07:54.951452Z","shell.execute_reply":"2022-05-11T07:48:41.703746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_scores","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:05:39.976543Z","iopub.status.idle":"2022-05-11T07:05:39.977028Z","shell.execute_reply.started":"2022-05-11T07:05:39.976755Z","shell.execute_reply":"2022-05-11T07:05:39.976779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean of',Kfold, 'folds for best loss in', epochs, 'epochs cross-validation folds is %.4f.' %(np.mean(best_scores)))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T07:05:39.978355Z","iopub.status.idle":"2022-05-11T07:05:39.978781Z","shell.execute_reply.started":"2022-05-11T07:05:39.978544Z","shell.execute_reply":"2022-05-11T07:05:39.978567Z"},"trusted":true},"execution_count":null,"outputs":[]}]}