{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Begin","metadata":{}},{"cell_type":"markdown","source":"The prediction function is at [this notebook](https://www.kaggle.com/seraphwedd18/jigsaw-toxic-severity-word-scoring-prediction) with a max LB score of `0.749` as of creation of this notebook.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-24T05:10:49.207391Z","iopub.execute_input":"2021-11-24T05:10:49.207695Z","iopub.status.idle":"2021-11-24T05:10:49.211807Z","shell.execute_reply.started":"2021-11-24T05:10:49.207667Z","shell.execute_reply":"2021-11-24T05:10:49.211172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be using multiple dataset from previous Jigsaw competitions as complementary train data.","metadata":{}},{"cell_type":"code","source":"train_1 = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ntrain_1","metadata":{"execution":{"iopub.status.busy":"2021-11-24T05:10:49.216894Z","iopub.execute_input":"2021-11-24T05:10:49.217105Z","iopub.status.idle":"2021-11-24T05:10:49.799366Z","shell.execute_reply.started":"2021-11-24T05:10:49.217079Z","shell.execute_reply":"2021-11-24T05:10:49.798526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the main train data, we will simple associate the column `less_toxic` with the score of `0` and the column `more_toxic` with the score of `1`. There are some text that are duplicates so we will filter out the uniques for each level of toxicity.","metadata":{}},{"cell_type":"code","source":"train_1_cleaned = pd.concat(\n    [pd.DataFrame({\"text\":train_1.less_toxic.unique(), \"score\":np.zeros(train_1.less_toxic.nunique())}),\n     pd.DataFrame({\"text\":train_1.more_toxic.unique(), \"score\":np.ones(train_1.more_toxic.nunique())})],\n    axis=0).reset_index(drop=True)\ntrain_1_cleaned","metadata":{"execution":{"iopub.status.busy":"2021-11-24T05:10:49.80109Z","iopub.execute_input":"2021-11-24T05:10:49.801316Z","iopub.status.idle":"2021-11-24T05:10:49.932098Z","shell.execute_reply.started":"2021-11-24T05:10:49.80129Z","shell.execute_reply":"2021-11-24T05:10:49.931259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_2 = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ntrain_2","metadata":{"execution":{"iopub.status.busy":"2021-11-24T05:10:49.933679Z","iopub.execute_input":"2021-11-24T05:10:49.934423Z","iopub.status.idle":"2021-11-24T05:11:15.412844Z","shell.execute_reply.started":"2021-11-24T05:10:49.934376Z","shell.execute_reply":"2021-11-24T05:11:15.412241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the data from `jigsaw-unintended-bias-in-toxicity-classification` we will only be using the `comment_text` and the `target` columns.","metadata":{}},{"cell_type":"code","source":"train_2_cleaned = pd.DataFrame({'text':train_2.comment_text, 'score':train_2.target})\ntrain_2_cleaned['score'] = train_2_cleaned['score'].apply(lambda x: 0 if x <= 0.05 else x)\ntrain_2_cleaned","metadata":{"execution":{"iopub.status.busy":"2021-11-24T05:11:15.413907Z","iopub.execute_input":"2021-11-24T05:11:15.414826Z","iopub.status.idle":"2021-11-24T05:11:16.559965Z","shell.execute_reply.started":"2021-11-24T05:11:15.414786Z","shell.execute_reply":"2021-11-24T05:11:16.559132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_3 = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\ntrain_3","metadata":{"execution":{"iopub.status.busy":"2021-11-24T05:11:16.562069Z","iopub.execute_input":"2021-11-24T05:11:16.562298Z","iopub.status.idle":"2021-11-24T05:11:18.643771Z","shell.execute_reply.started":"2021-11-24T05:11:16.562272Z","shell.execute_reply":"2021-11-24T05:11:18.642929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the data from `jigsaw-toxic-comment-classification-challenge`, we will be scoring them based on the average of the five columns present as targets. Similar to `train_2` we will be using base 50% once more.","metadata":{}},{"cell_type":"code","source":"score = np.mean(train_3[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']], axis=1)\ntrain_3_cleaned = pd.DataFrame({'text':train_3.comment_text, 'score':score})\ntrain_3_cleaned['score'] = train_3_cleaned['score'].apply(lambda x: 0 if x <= 0.05 else x)\ntrain_3_cleaned","metadata":{"execution":{"iopub.status.busy":"2021-11-24T05:11:18.645056Z","iopub.execute_input":"2021-11-24T05:11:18.64588Z","iopub.status.idle":"2021-11-24T05:11:18.772325Z","shell.execute_reply.started":"2021-11-24T05:11:18.645832Z","shell.execute_reply":"2021-11-24T05:11:18.77146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will merge all train data as one.","metadata":{}},{"cell_type":"code","source":"train_df = pd.concat(\n    [train_1_cleaned, train_2_cleaned, train_3_cleaned],\n    axis=0\n).reset_index(drop=True)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2021-11-24T05:11:18.773647Z","iopub.execute_input":"2021-11-24T05:11:18.77448Z","iopub.status.idle":"2021-11-24T05:11:18.94473Z","shell.execute_reply.started":"2021-11-24T05:11:18.774434Z","shell.execute_reply":"2021-11-24T05:11:18.943704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check one random comment for each unique score (based from a rounded cutoff or 2 decimal places) printing the first 100 raw chars of the corresponding text.","metadata":{}},{"cell_type":"code","source":"printed = []\nfor i in sorted(train_df.score.unique()):\n    n = np.round(i, 2) \n    if n in printed:\n        continue\n    printed.append(n)\n    print(f\"{len(printed):<3}: {i:.5f}\\t{repr(np.random.choice(train_df[train_df.score==i]['text']))[:100]}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-24T05:11:18.94644Z","iopub.execute_input":"2021-11-24T05:11:18.946768Z","iopub.status.idle":"2021-11-24T05:11:19.402817Z","shell.execute_reply.started":"2021-11-24T05:11:18.946726Z","shell.execute_reply":"2021-11-24T05:11:19.401913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\ntqdm.pandas()\nimport re\n\nadd_space_before_punc = lambda x: re.sub(r'(\\W|_)', r' \\1 ', x)\nremove_whitespaces = lambda x: re.sub(r'\\s+', ' ', x)\nremove_multiples = lambda x: re.sub(r'(.)\\1{2,}', r'\\1\\1', x) #Remove repeated char multiple times\n\ntrain_df['clean_text'] = train_df.text.progress_apply(\n    lambda x: remove_whitespaces(remove_multiples(add_space_before_punc(x)))\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T05:11:19.404457Z","iopub.execute_input":"2021-11-24T05:11:19.404827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Scoring\n\nWe will be scoring each word used based on frequency. Words used on non-toxic comments will be treated with low scores and words exclusively used on offensive/toxic comments will be scored high.","metadata":{}},{"cell_type":"code","source":"import json\nimport gc\n\nneutral_words = {}\ntoxic_words = {}\n\ndef get_words(all_words, toxic=False):\n    keys = set(neutral_words.keys())\n    print(\"Looping through all words...\")\n    for word in tqdm(all_words):\n        if toxic and not (word in keys):\n            toxic_words[word] = toxic_words.get(word, 0) + 1\n        elif not toxic:\n            neutral_words[word] = neutral_words.get(word, 0) + 1\n            \n    with open(f\"{'neutral' if not toxic else 'toxic'}_word_counts.json\", 'w') as f:\n        if toxic:\n            json.dump(toxic_words, f)\n        else:\n            json.dump(neutral_words, f)\n\n#Separate neutral from toxic\nprint(\"Separating words...\")\nneutral_df = (' '.join(train_df[train_df.score == 0]['clean_text'].values)).split()\ntoxic_df = (' '.join(train_df[train_df.score != 0]['clean_text'].values)).split()\nprint(\"Neutral words:\", len(neutral_df))\nprint(\"Toxic words:\", len(toxic_df))\n\nprint(\"Creating word count mapping...\")\nget_words(neutral_df)\nget_words(toxic_df, True)\nprint(len(neutral_words), len(toxic_words))\ndel neutral_df, toxic_df\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have the map of word and word counts, we will be scoring them. For neutral words, the more frequent words will have lower scores and rarer words will have higher scores. We will use the equation `word_score = 0.1 / word_count` to actualize this metric.\n\nFor toxic words, the less the frequency of the word, the higher the score. More frequent words will have lower score. We will use the equation of `word_score = 256 * 0.995 ** (word_count)` where `256` is the arbitrary limit of the toxicity score.","metadata":{}},{"cell_type":"code","source":"neutral_words = {word: 0.1/count for word,count in tqdm(neutral_words.items())}\ntoxic_words = {word: 256 * 0.995 ** count for word,count in tqdm(toxic_words.items())}\nprint(f\"Neutral min/max: {min(neutral_words.values()):.5f}, {max(neutral_words.values()):.5f}\")\nprint(f\"Toxic min/max: {min(toxic_words.values()):.5f}, {max(toxic_words.values()):.5f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"neutral_word_scores.json\", 'w') as f:\n    json.dump(neutral_words, f)\nwith open(\"toxic_word_scores.json\", 'w') as f:\n    json.dump(toxic_words, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def score_sentence(text):\n    all_words = text.split()\n    return sum(neutral_words[word] if neutral_words.get(word, 0) else\n              toxic_words[word] for word in all_words)\n\ntrain_df['text_score'] = train_df.clean_text.progress_apply(lambda x: score_sentence(x))\ntrain_df['text_score'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nBest Score on Public LB is: `0.716`","metadata":{}},{"cell_type":"code","source":"from scipy.stats import rankdata\n\ntest = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ntest['clean_text'] = test.text.progress_apply(\n    lambda x: remove_whitespaces(remove_multiples(add_space_before_punc(x)))\n)\n\npreds = test.clean_text.progress_apply(lambda x: score_sentence(x))\ndisplay(preds, preds.min(), preds.max())\nsub = pd.DataFrame({'comment_id':test.comment_id.values, 'score':rankdata(preds, method='ordinal')})\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}