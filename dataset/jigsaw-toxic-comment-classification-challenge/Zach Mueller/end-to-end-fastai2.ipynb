{"cells":[{"metadata":{},"cell_type":"markdown","source":"# End to End Toxic Comments in fastai2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook we're going to explore end-to-end training of the Toxic Comments multi-label dataset using fastai2 and the high-level DataBlock API. First let's install `fastai2`:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install fastai2 --quiet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since our task is a text problem, let's grab the `fastai2` text sub-library:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from fastai2.text.all import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path('../input/jigsaw-toxic-comment-classification-challenge')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To work with the data we need to unzip everything:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from zipfile import ZipFile\n\nwith ZipFile(path/'train.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('../output/kaggle/working')\n    \nwith ZipFile(path/'test.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('../output/kaggle/working')\n    \nwith ZipFile(path/'test_labels.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('../output/kaggle/working')\n    \nwith ZipFile(path/'sample_submission.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('../output/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_w = Path('../output/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_w.ls()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see our training data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(path_w/'train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have our `comment_text` and a one-hot-encoded `y-label` for multi-class classification. Let's look at how to build a `DataBlock` for this to use in the `fastai2` framework. First let's look at the `TextBlock`. This will be where we dictate what kind of tokenizer we use, if it's a language model, and so forth:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"blocks = (TextBlock.from_df(text_cols='comment_text', is_lm=True, res_col_name='text'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now before we actually do this, one trick is to train the langauge model on as much data as we possibly can, as it's unlabelled. We'll make another `DataFrame` for this specifically:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(path_w/'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df = pd.Series.append(df['comment_text'], test_df['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df = pd.DataFrame(text_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now you may notice there was a `res_col_name` parameter. This is where our *tokenized* text will be output to. Next we need to tell how to grab our `x` and how we want to split our data. We'll split by 10% randomly:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_x = ColReader('text')\nsplitter = RandomSplitter(0.1, seed=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that this `get_x` should be the same as our output column","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's build the `DataBlock` Pipeline:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_dblock = DataBlock(blocks=blocks,\n                     get_x=get_x,\n                     splitter=splitter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now the `DataLoaders`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_dls = lm_dblock.dataloaders(text_df, bs=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It will take abit to pre-process everything, `fastai` will tokenize for us beforehand (should take about 14 minutes). \n\nNext, as per the ULM-FiT technique, we'll want to train our langauge model. First let's build our `Learner`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_learn = language_model_learner(lm_dls, AWD_LSTM, pretrained=True, metrics=[accuracy, Perplexity()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`fastai` has a nice in-house `fit` called `fine_tune`, which follows the freezing/unfreezing transfer-learning protocol. We can pass in the number of frozen epochs and unfrozen, however Jeremy and Sylvain found that one was enough, so we'll pass in the number of *unfrozen* epochs as well as a learning rate:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_learn.to_fp16()\nlm_learn.fine_tune(10, 4e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_learn.save_encoder('fine_tuned')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have our pre-trained model, let's build our down-stream multi-label classification task:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Toxic Comment Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For our next part we'll want to make a `DataBlock` that uses the original vocab and sets us up for a multi-label classification problem:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ys = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n       'identity_hate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blocks = (TextBlock.from_df('comment_text', seq_len=lm_dls.seq_len, vocab=lm_dls.vocab), \n          MultiCategoryBlock(encoded=True, vocab=ys))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"toxic_clas = DataBlock(blocks=blocks,\n                      get_x=ColReader('text'),\n                      get_y=ColReader(ys),\n                      splitter=RandomSplitter())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can test if it works by calling `toxic_clas.summary()`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"toxic_clas.summary(df.iloc[:100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which it works just fine! So let's build our `DataLoaders`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dls = toxic_clas.dataloaders(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we'll make our `text_classification_learner`:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now this is where things get a bit tricky, as we want to know how to build our thresholds for `BCELossLogits` and `accuracy_multi` (as ideally we'd want them both to be the same). To make sure my model is very strong, I'll set their thresholds to activations > 0.8:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_func = BCEWithLogitsLossFlat(thresh=0.8)\nmetrics = [partial(accuracy_multi, thresh=0.8)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = text_classifier_learner(dls, AWD_LSTM, metrics=metrics, loss_func=loss_func)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then we just find the learning rate and fit!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll use the unfreezing methodology for the ULM-FiT model to train and for its learning rate:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load_encoder('fine_tuned');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.to_fp16()\n\nlr = 1e-2\nmoms = (0.8,0.7, 0.8)\nlr *= learn.dls.bs/128\nlearn.fit_one_cycle(1, lr, moms=moms, wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-2)\nlr/=2\nlearn.fit_one_cycle(1, slice(lr/(2.6**4), lr), moms=moms, wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-3)\nlr /=2\nlearn.fit_one_cycle(1, slice(lr/(2.6**4), lr), moms=moms, wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlr /= 5\nlearn.fit_one_cycle(2, slice(lr/(2.6**4),lr), moms=(0.8,0.7,0.8), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submitting the Predictions:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's try to submit some predictions. To make a test set we should call `learn.dls.test_dl` and pass in our `DataFrame` with the text to use:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dl = learn.dls.test_dl(test_df['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For getting the predictions, we call `learn.get_preds` and pass in this `DataLoader`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = learn.get_preds(dl=dl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see how our sample submission wants it?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(path_w/'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Easy enough, let's push those predictions to it. They come in the same order we passed them in as:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds[0][0].cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[ys] = preds[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now we can submit it!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}