{"cells":[{"metadata":{"collapsed":true,"_uuid":"13fe744afa6808cc88b5fb42220e4a18633e01c4"},"cell_type":"markdown","source":"# Final Project for the NLP Class (CogSci @ UW, summer 2018)\n#### Rados≈Çaw Jurczak\n***"},{"metadata":{"_uuid":"9c353d6bec13172feff9afabf26f8a93d471e767"},"cell_type":"markdown","source":"## The Jigsaw Toxic Comments Classification Challenge"},{"metadata":{"_uuid":"a3abbafe15f4e7c05b825dd8fb0298af990868d2"},"cell_type":"markdown","source":"---\n### Introduction & Task Description\nThis notebook contains full code of a solution to [Kaggle's Jigsaw Toxic Comments Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).\n\nThe Google/Jigsaw ConversationAI research group (a team focused mainly on automated web moderation and hate speech detection) posted a dataset of comments collected from Wikipedia page edit discussions. The comments had been hand-labelled with respect to different kinds of toxicity. Six binary labels were used:\n* toxic\n* severe toxic\n* obscene\n* threat\n* insult\n* identity hate.\nAs we can see, the labels are quite different from each other, ranging from very specific types of toxicity (threat, obscenity) to sort of \"generic\" toxicity (first & second label).\n\nThis is __not__ a simple single-label classification task. Each comment may belong to __more than one__ category. Of course, most of the comments are labelled 0 for all categories, i.e. they are not toxic. This problem is discussed in Section 1 of this notebook.\n\nFor each comment in the test set, the model is expected to predict probability for each of the six types of toxicity specified above. Submissions are scored using the __mean column-wise ROC AUC (area under the Receiver Operating Characteristic curve)__. In other words, the final score is the average of ROC AUC scores for each category.\n\nAs usual with Kaggle competitions, the final score is calculated on a larger dataset called the \"private\" test set; during the competition, submissions are only scored against a small fraction of the private set (this promotes more generalisable models). When developing my solution, I was using only the public (small) test set; I checked private (i.e. ranking-relevant) test set score only in the end, after selecting the final model. The whole work is therefore done fully in accordance with Kaggle competition rules.\n\n---\n### Solution Overview\nIn general, the solution is a deep recurrent network. Although this is a Kaggle competition, I chose not to use any form of blending/ensembling but to build and tune a single model only.\n\nThe network was trained on a single Nvidia GPU (16 GB) hosted remotely on [Paperspace](https://www.paperspace.com/) (per-hour charge). I was using a 13$ free credit for students; this allowed only for limited experimentation, so possibly the model could be further improved if the computational resources were available."},{"metadata":{"_uuid":"8001e07ef3eb2e86afa4909fa6bf171e42372f82"},"cell_type":"markdown","source":"---\n### 0. Loading\nFirst, we'll load all the necessary libraries. This may take a while."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"95937c2b38c534f821d2c0d62bb7f5472cd4f408"},"cell_type":"code","source":"# basics\nfrom collections import defaultdict\nimport re\n\n# Numerics, data processing & similar utils\nimport numpy as np\nimport pandas as pd\n\n# nlp tools\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\n\n# Machine learning (metrics from scikit-learn, model building tools from Keras on tensorflow backend)\nfrom keras import backend as K\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.layers import Conv1D, CuDNNGRU, CuDNNLSTM, GlobalAveragePooling1D, GlobalMaxPooling1D, Dense, Embedding, Flatten\nfrom keras.layers import Activation, BatchNormalization, Bidirectional, concatenate, Dropout, Input, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import Sequence, pad_sequences\nfrom keras.regularizers import l1_l2\nfrom keras.utils import plot_model\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n# Visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pydot\nfrom tqdm import tqdm, tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"1118f7d7b2e353a0840d65d769419e63e6f246b2"},"cell_type":"code","source":"nltk.download(\"stopwords\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"6c77bc91803756833b0e24c4dea9a6dc71479562"},"cell_type":"code","source":"tqdm_notebook().pandas()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6315770ae8c8a8ef30ff36d55d96b4e911b87a26"},"cell_type":"markdown","source":"Now load the data.\n\nI am using pre-trained 300-dimensional Fasttext word vectors downloaded from [Facebook Research's github account](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md). More information on the Fasttext embedding is included in Section 2 of this notebook."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"75210f9b95fa7483f6150e9f740584a87dea2186"},"cell_type":"code","source":"TRAIN_FILE = \"../data/train.csv\"\nTEST_DATA_FILE = \"../data/test.csv\"\nEMB_FASTTEXT_300_FILE = \"../data/wiki.en.vec\"\n\ntrain = pd.read_csv(TRAIN_FILE)\ntest_data = pd.read_csv(TEST_DATA_FILE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c155c6347e48894f666b9162702e3bba527f742f"},"cell_type":"markdown","source":"---\n### 1. (Very) fast-and-simple exploratory analysis\nLet's have a quick look at the data."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"378d0e86ccb2f6f6274b007dbc35fbbeb3097115"},"cell_type":"code","source":"print(\"Training set dimension: \" + str(train.shape))\nprint(\"Test set dimension: \" + str(test_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9d54797ba838aa7c867c191ef33aa0f35e0e4a98"},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d384956d3913eccd3f89c98da1602ba6ccadc77"},"cell_type":"markdown","source":"There are 159571 comments in the training set.\n\nIt looks as if there was about the same amount of data in the test set, but according to the competition description, only a fraction of the examples is actually relevant for testing and score calculation. The \"fake\"/unused test examples were labeled -1 for all label categories (this labelling remained unknown to competitors). Although the competition has ended recently and thus the fully labeled version of the test set has been published, I will not remove the unused examples to stick to a realistic Kaggle setting."},{"metadata":{"_uuid":"4c33da4cd7ccdb8971981e8634946e88ff8d4fc0"},"cell_type":"markdown","source":"As we can see, there are luckily no missing values in the training set, which will save us some work:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"447007f240d63eaa42882069567363b32294de18"},"cell_type":"code","source":"np.sum(train.isna())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"1baebb1865da1b487172e3984276697d640562d0"},"cell_type":"code","source":"train_size = train.shape[0]\ntest_size = test_data.shape[0]\nprint(f\"Number of examples in the training set: {train_size}\")\nprint(f\"Number of examples in the test set: {test_size}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a639d8873efad2eaf9bb37239e7d71e2a924812f"},"cell_type":"markdown","source":"Now let's see how many examples in the training data have been labelled as toxic (some way or another):"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a9021567f62a2836e9e9b13a5c0f3928db273502"},"cell_type":"code","source":"LABELS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\npercent_toxic = train[LABELS].sum().apply(lambda x: (x / train_size) * 100)\nprint(\"Toxic comments by category (as fractions of the whole dataset): \\n\")\nfor category, percent in percent_toxic.items():\n    print(category + \": \" + str(round(percent, 2)) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d0aace1d5a9dc825bc1c9dfa61ab06c62597c3e"},"cell_type":"markdown","source":"Two problems are obvious from what we have seen above. First, only a small fraction of the whole dataset consists of toxic comments. This will make model training harder.\n\nSecond, the toxicity distribution between categories is very unbalanced. This also poses a potential problem for training, as models might become more sensitive to some types of toxicity while neglecting others. Things will get even worse if the test distribution does not match the training distribution, which is quite probable for such peculiar data."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9b868efb6bcd8f90fdb1d3a78966c3eb339a1e59"},"cell_type":"code","source":"ts = train[LABELS].sum()\nprint(\"Total toxicity occurrences detected by human annotators: \" + str(ts.sum()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8db2b1fe18b7bce43abbf5d6c08b21e5986a081"},"cell_type":"markdown","source":"---\n### 2. Data preprocessing\nWhat we have in the training (and test) set is pretty much raw text written in informal internet language. Quite a lot of preprocessing is necessary to turn this into a reasonably clean dataset suitable for feeding a neural network."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"37c6e4f806b7f19bf04f9f1fde49ad48502bdc68"},"cell_type":"code","source":"train_target = train[LABELS]\ntrain_comments = train[\"comment_text\"]\ntest_comments = test_data[\"comment_text\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e319b10a0a9b78b94c18bf914d83f0ee6f74280"},"cell_type":"markdown","source":"In the cell below, I have defined a regex-based word replacement vocabulary. It's a revised and extended version of a similar dictionary that circulated among the competitors (I am unsure of the original author). the making of this vocabulary was probably the most linguistics-based part of my work, as well as one that took the longest (I needed to look through at least part of the toxic comments in the training set). The vocabulary covers the following issues:\n* all newline signs (\\n), IP addresses and user nicknames are removed;\n* common abbreviations like \"afaik\" ot \"imho\" are expanded to their full forms;\n* all the expressions of amusement and/or amazement and/or mockery like \"lol\", \"lel\", \"kek\" etc. are reduced to one common form \"lol\";\n* emoticons like ;), :) and the like are reduced to the token \"happy\"; similarly, the many varieties of :( emoticon are reduced to the token \"sad\";\n* numerous cases of deliberate or non-deliberate misspelling, esp. of \"toxic\" words like \"fuck\", \"bitch\" etc., are corrected and replaced with the correct word forms;\n* @ and & symbols are replaced with \"at\" and \"and\" prepositions, respectively;\n* \"in/in'\" word endings are replaced with the proper suffix \"ing\".\n\nForms like \"I'll\", \"he'd\" were deliberately left as they were; the reason for that is described below."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"fbea3a4a86d94ef709bb61b7106d3f93ac6267fa"},"cell_type":"code","source":"CLEANING_DICT = {\n    \" \": [\"\\n\", \"/\", \"(([0-9]{2,3}[^a-z0-9])+)+\", \"\\[\\[.*\\]\", \" +\"],\n    \" as far as i know \": [\"afaik\"],\n    \" in my opinion \": [\"imo\", \"imho\"],\n    \" sad \": [\"\\:\\(\", \"\\:\\'\\(\", \"\\:\\(+\"],\n    \" lol \": [\" (h[ae])+h? \", \" lel \", \" kek \", \"lu+lz?\", \"loo(o)*lz?\"],\n    \" happy \": [\"\\:\\)+\", \";\\)+\", \"\\:d+\"],\n    \" american \": [\"amerikan\"],\n    \" adolf \": [\"adolf\"],\n    \" hitler \": [\"hitler\"],\n    \"fuck\": [\"(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])\", \"(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)\",\n            \" f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k\", \"f u u c\", \"(f)(c|[^a-z ])(u|[^a-z ])(k)\", r\"f\\*\", \"feck \",\n            \" fux \", \"f\\*\\*\", \"f\\.u\\.\", \"f###\", \" fu \", \"f@ck\", \"f u c k\", \"f uck\", \"f ck\"],\n    \"fucking \": [\"f[u|a]c?king?\"],\n    \" ass \": [\"[^a-z]ass \", \"[^a-z]azz \", \"arrse\", \" arse \", \"@\\$\\$\", \"[^a-z]anus\", \" a\\*s\\*s\", \"[^a-z]ass[^a-z ]\",\n             \"a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]\", \"[^a-z]anal \", \"a s s\", \"butt \"],\n    \" asshole \": [\" a[s|z]*wipe\", \"a[s|z]*[w]*h[o|0]+[l]*e\", \"@\\$\\$hole\"],\n    \" bitch \": [\"b[w]*i[t]*ch\", \"b!tch\", \"bi\\+ch\", \"b!\\+ch\", \"(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)\",\n                \"biatch\", \"bi\\*\\*h\", \"bytch\", \"b i t c h\"],\n    \" bastard \": [\"ba[s|z]t[a|e]rd\"],\n    \" gay \": [\"gay\"],\n    \" cock \": [\"[^a-z]cock\", \"c0ck\", \"[^a-z]cok \", \"c0k\", \"[^a-z]cok[^aeiou]\", \" cawk\",\n               \"(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)\", \"c o c k\"],\n    \" dick \": [\" dick[aeiou]\", \"deek\", \"d i c k\"],\n    \" suck \": [\"(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)\", \"sucks\", \"5uck\", \"s u c k\"],\n    \" sucking \": [\"s[u|a]c?king?\"],\n    \" cunt \": [\"cunt\", \"c u n t\", \"c\\*\\*\\*\", \"c\\*\\*t\", \"c\\*nt\"],\n    \" bullshit \": [\"bullsh\\*t\", \"bull\\$hit\"],\n    \" homosexual \": [\"homo\"],\n    \" jerk \": [\"jerk\"],\n    \" idiot \": [\"i[d]+io[t]+\", \"(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)\", \"idiots\", \"i d i o t\"],\n    \" dumb \": [\"(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)\", \"d\\*mb\", \"$dumm\"],\n    \" shit \": [\"shitty\", \"(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)\", \"shite\", \"\\$hit\", \"s h i t\"],\n    \" shit hole \": [\"shythole\", \"shithole\"],\n    \" retard \": [\"returd\", \"retad\", \"retard\", \"ret[au]rded\", \"wiktard\", \"wikitard\", \"wikitud\"],\n    \" dumb ass\": [\"dumbass\", \"dubass\", \"du(m)+ass\"],\n    \" ass head \": [\"butthead\"],\n    \" sex \": [\"s3x\", \"s\\*x\"],\n    \" nigger \": [\"nigger\", \"ni[g]+a\", \" nigr \", \"negrito\", \"niguh\", \"n3gr\", \"n i g g e r\"],\n    \" shut the fuck up \": [\"stfu\"],\n    \" rape \": [\"reap\", \"rpe\"],\n    \" pussy \": [\"pussy[^c]\", \"pusy\", \"pussi[^l]\", \"pusses\"],\n    \" faggot \": [\"faggot\", \" fa[g]+[s]*[^a-z ]\", \"fagot\", \"f a g g o t\", \"faggit\",\n                 \"(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)\", \"fau[g]+ot\", \"fae[g]+ot\",\n                 \"fagg+az\", \"fagg+otz\"],\n    \" motherfucker \": [\" motha \", \"motha f \", \"moth[a|er]fucka?r\", \"mother f\", \"motherucker\"],\n    \" whore \": [\"w\\*\\*\\*(\\*)?\", \"whor\", \"w h o r e\"],\n    \" and \": [\"&\"],\n    \" at \": [\" @ \"],\n    \"ing \": [\"$in\\'\"]\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c0a9d01c6024d7d1a3cf55ef37489131b473573"},"cell_type":"markdown","source":"For preprocessing, I have used [SpaCy](https://spacy.io/), a very fast, production-oriented NLP library for Python. As a language model for preprocessing tasks, I loaded one of SpaCy's predefined English language models: en_core_web_md, which is a convolutional neural network capable of tokenization, lemmatization, POS tagging and dependency parsing, trained on CommonCrawl and OntoNotes. This model's performance on the advanced tasks like tagging or parsing is actually irrelevant, as I'm using only tokenization+lemmatization, which is of decent quality.\n\nIt is important to mention that SpaCy can automatically deal with abbreviations like \"I'd\", \"he'll\" and the like. That is why such patterns are not present in the replacement dictionary.\n\nThe whole preprocessing is contained within the clean_text() function defined below. It takes as input a single comment string and performs the following steps:\n* lowercase all words in the comment;\n* use regular expressions to apply the substitutions defined in the replacement dictionary;\n* tokenize the result;\n* lemmatize the result;\n* remove stopwords (I used the list of English stopwords available as part of [NLTK](https://www.nltk.org/));\n* return all remaining tokens joined in a single string.\n\nThis function is applied both to the training and test set.\n\nWARNING: __do not__ run the cleaning procedure (unless you want to check if it works). Even with SpaCy (which is really very fast in comparison to other similar tools), it does take time. Below you can find code that loads the preprocessed datasets directly into pandas DataFrame objects."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"15629fa2abbf8d609f70b4f30ee45feafc5cab1b"},"cell_type":"code","source":"STOPWORDS = stopwords.words(\"english\")\nSPACY_LANG_MODEL = spacy.load(\"en_core_web_md\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"6f148a277369deb944076ba705881856701feebb"},"cell_type":"code","source":"def clean_text(comment, replacement_dict=CLEANING_DICT, spacy_model=SPACY_LANG_MODEL, stopwds=STOPWORDS):\n    comment = comment.lower()\n    for base, patterns in replacement_dict.items():\n        for pattern in patterns:\n            comment = re.sub(pattern, base, comment)\n    tokenized = list(spacy_model(comment, disable=[\"tagger\", \"parser\", \"ner\"]))\n    lemmatized = [token.lemma_ if token.lemma_ != \"-PRON-\" else token.lower_ for token in tokenized]\n    lemmatized = [lem for lem in lemmatized if lem not in stopwds and lem != \" \"]\n    comment = \" \".join(lemmatized)\n    return comment","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"acbc556cd8b632e52e2e26c3f864fe1019392cf0"},"cell_type":"code","source":"train_clean = train_comments.progress_apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ccec46fca52ff8bc519e57c099f4d01226168e15"},"cell_type":"code","source":"train_clean.to_csv(\"../data/train_clean.csv\", index=False, header=[\"comment_text\"])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8f307ac5d5d9ffdc6a2cc3cba0bc9baf032f429d"},"cell_type":"code","source":"test_clean = test_comments.progress_apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"5d26ba2dcee283c2b4a5a7a3319e4aae0cad8c3f"},"cell_type":"code","source":"test_clean.to_csv(\"../data/test_clean.csv\", index=False, header=[\"comment_text\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"beb797789d518b1bc58e380d1a0dcbc8017740b4"},"cell_type":"markdown","source":"__Run the cell below to load preprocessed data:__"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d77fa3f2a8f54bea4ed749213a6453f8178b99ac"},"cell_type":"code","source":"train_clean = pd.read_csv(\"../data/train_clean.csv\", header=0)\ntest_clean = pd.read_csv(\"../data/test_clean.csv\", header=0)\ntrain_clean = train_clean[\"comment_text\"]\ntest_clean = test_clean[\"comment_text\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9375926f794e7784cfde70f90f6ff4cd9922e994"},"cell_type":"markdown","source":"The processed comments need to be tokenized to a Keras-feedable form. This is done with [Keras built-in tokenizer](https://keras.io/preprocessing/text/). It automatically removes special symbols, whitespaces and punctuation; it also returns a word index (a dictionary mapping words to their integer vocabulary indices).\n\nAs far as possible, I want to keep any information available in the comment text, so the model will use all the words that appear in the comments, hence the large MAX_FEATURES constant defining maximum vocabulary size. MAX_COMMENT_LEN is the cutoff comment length (longer comments will be trimmed, shorter ones will be zero-padded later)."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"69afdf4326b80bebe9b34d66fd70f30757e00c77"},"cell_type":"code","source":"MAX_FEATURES = 300000\nMAX_COMMENT_LEN = 900","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9fcdd65aa51d5605f87a72453050baea31c7fb7c"},"cell_type":"code","source":"def tokenize_for_keras(tr_set, test_set, max_features):\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(tr_set) + list(test_set))\n    train_tokenized = tokenizer.texts_to_sequences(tr_set)\n    test_tokenized = tokenizer.texts_to_sequences(test_set)\n    tokenizer_index = tokenizer.word_index\n    return (tokenizer_index, train_tokenized, test_tokenized)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"193dda40ecf08352e54b3c492aca87f64eaaea67"},"cell_type":"code","source":"train_feed = train_clean.fillna(\"fillna\")\ntest_feed = test_clean.fillna(\"fillna\")\nword_index, train_tokenized, test_tokenized = tokenize_for_keras(train_feed, test_feed, max_features=MAX_FEATURES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82b772512e9d5ae6481f145bdaa34336b270ffa1"},"cell_type":"markdown","source":"Now we need to load the Fasttext word vectors and form an embedding dictionary to pass to Keras embedding layer.\n\nBTW the advantage of Fasttext over other embeddings, e.g. GloVe or word2vec, is that they are capable of representing incomplete words (see [this publication by Bojanowski et al. (2016)](https://arxiv.org/abs/1607.04606)), which seems useful when working with informal internet language data."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b27cba0334e07ca6676601e42356ef07ed0f5d88"},"cell_type":"code","source":"def get_embedding_dict(emb_file=EMB_FASTTEXT_300_FILE):\n    emb_dict = defaultdict()\n    with open(emb_file) as file:\n        for line in file:\n            emb = line.rstrip().rsplit(\" \")\n            word = emb[0]\n            vec = np.asarray(emb[1:], dtype=\"float32\")\n            emb_dict[word] = vec\n    return emb_dict","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"7bc0f6fee866adc3a9dc2781cc96e751d0564639"},"cell_type":"code","source":"emb_dict_fasttext = get_embedding_dict()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"48285382d19e7a364df6afbcdd67fcfe75e4f0c5"},"cell_type":"code","source":"EMB_SIZE_FASTTEXT = 300","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef3b1542cf4ba8d87716113eb9ee1a16b1ea42f7"},"cell_type":"markdown","source":"The Keras-tokenized comments need to be zero-padded so that they all have the same length (defined above as MAX_COMMENT_LENGTH)."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"569bc8c61b883074cc4373f29a0ed672c423bef1"},"cell_type":"code","source":"train_tokenized = pad_sequences(train_tokenized, maxlen=MAX_COMMENT_LEN)\ntest_tokenized = pad_sequences(test_tokenized, maxlen=MAX_COMMENT_LEN)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d73afac1e33f878bf2418e989159f119a9fc695a"},"cell_type":"markdown","source":"In order to construct an Embedding layer in Keras, an embedding matrix (i.e. a dictionary mapping word indices and vectors) is needed. The function below builds embedding matrix based on given embedding dictionary, tokenizer index and parameters specifying maximum vocabulary size and the embedding vectors' dimension.\n\nThe matrix is initialized with 300-dimensional vectors filled with zeros, then filled with available embeddings. This means that words for which we do not have Fasttext embeddings are zero-initialized.\n\nThe function's optional boolean parameter \"gaussian_initialization\" specifies whether the out-of-vocabulary words should instead be initialized with random numbers drawn from the normal distribution with mean and standard deviation computed from all the embeddings available in the vocabulary. This solution improved score when combined with GloVe vectors (which I was initially using), but for Fasttext zero-initialized vectors tend to work better."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"53c75c6c322f0313141a17b5e7b3b482db5a22e5"},"cell_type":"code","source":"def get_embedding_matrix(emb_dict, word_index, max_features, emb_size, gaussian_initialization=False):\n    n_words = min(MAX_FEATURES, len(word_index))\n    \n    if gaussian_initialization:\n        stacked_embs = np.stack(emb_dict.values())\n        emb_mean, emb_std = (np.mean(stacked_embs), np.std(stacked_embs))\n        emb_matrix = np.random.normal(emb_mean, emb_std, size=(n_words, emb_size))\n    else:\n        emb_matrix = np.zeros((n_words, emb_size))\n    \n    for word, index in word_index.items():\n        if index >= max_features:\n            continue\n        vec = emb_dict.get(word)\n        if vec is not None:\n            emb_matrix[index] = vec\n    return emb_matrix","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"49989992ee7ce66f494098003c3074ca75ba85cb"},"cell_type":"code","source":"emb_matrix_fasttext = get_embedding_matrix(emb_dict=emb_dict_fasttext, word_index=word_index, max_features=MAX_FEATURES, emb_size=EMB_SIZE_FASTTEXT, gaussian_initialization=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4769216f1419856d9606b8cd1876b8834ee1e15"},"cell_type":"markdown","source":"### 2. Model building\nNow we can construct the model."},{"metadata":{"_uuid":"98280999185d622f275da65b09032932004c8b9d"},"cell_type":"markdown","source":"---\nFirst, a few utils are necessary.\n* The official rating metrics in the competition is the ROC AUC score; it is not, however, a differentiable loss function, so we need a reasonable proxy. I chose binary crossentropy, which is a standard loss function for multi-class classification.\n* It's better to track ROC AUC anyway. For that, I defined a custom Keras callback.\n* I also define a model checkpointer which creates backup by saving the current best model (in terms of ROC AUC) to file."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"80e85430d99da2b37033deff705c3985eac49724"},"cell_type":"code","source":"class ROC_AUC_Score(Callback):\n    \"\"\"Custom Keras callback class tracking ROC AUC.\n    After every epoch's end, it prints out current ROC AUC score for model's predictions\n    and saves the value to model's logs for potential use by other callbacks.\n    \"\"\"\n    \n    def __init__(self, validation_data=(), interval=1):\n        super().__init__()\n        self.interval = interval\n        self.val_X, self.val_target = validation_data\n     \n    def on_train_begin(self, logs={}):\n        self.aucs = []\n        self.losses = []\n    \n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            preds = self.model.predict(self.val_X, verbose=0)\n            auc = roc_auc_score(self.val_target, preds)\n            self.aucs.append(auc)\n            logs[\"roc_auc_val\"] = auc\n            print(f\"Epoch: {epoch+1} - ROC AUC score: {round(auc, 5)}\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c24757fe62682efc9fbdec4a09dd75c9103a3d1a"},"cell_type":"markdown","source":"The model is defined below. It has the following components:\n* Non-trainable Embedding layer (using Fasttext vectors for embedding as explained above). Trainable vectors tend to overfit terribly (the dataset is too small and also somehow prone to overfitting), so I decided not to unfreeze the embeddings.\n* Spatial dropout (rate 0.5) is applied to the embeddings. Spatial dropout cuts off entire feature maps along a dimension, which in the case of our model means that some positions on every vector are randomly \"switched off\" (the usual dropout way) from batch to batch during training. As every dropout layer, this has a regularizing effect. Adding the spatial dropout layer right after the embeddings turned out to make one of the biggest improvements in model's score.\n* Dense layer with 128 units and ReLU activation. The idea of including this layer is to provide a substitute to trainable vectors (the network can learn to \"preprocess\" the embedding layer if necessary).\n* Batch normalization and dropout (rate 0.5). All dense layers in the model are followed by the commonly used pattern of batchnorm+dropout. The first one normalizes the preceding layer's activations using current batch's mean and variance; this usually effects in learning speedup, decreases model's sensitivity to initial (randomly initialized) weight values and to possible differences between training/test feature distributions, allows for adding more layers (basically makes the layers more independent in a way) and has a small regularizing effect. Dropout is a standard regularization technique. I'm using quite high dropout rate (established empirically) because it's really easy to overfit on this dataset even for much sipler models than the final one.\n* After that, the main trick is applied: the model splits into two separate branches. Along the first branch, an LSTM layer with 64 units is applied (it's bidirectional, i.e. goes through the input both right-to-left and left-to-right). Th LSTM's output is then convolved with 128 filters of size 3 (in the 1D case, this means a moving \"horizontal\" window of length 3) with stride 1. The convolution's padding is valid, which means that the input dimension is reduced, not artificially kept as it was by use of padding. As usual, after the convolution comes a pooling layer; in this case, both max and average pooling is applied.\n* The left branch is similar, but it uses a greater (128) number of simpler GRU units instead of LSTM and less (64 instead of 128) convolutional filters.\n* After that, the branches are concatenated together; one more trick is to use both the results of max and average pooling over each branch, simply stacked into one long tensor. This allows the network to extract as much information about the input's properties as possible. Details of this approach will be described during the class presentation.\n* The concatenated results of pooling are then fed to another dense layer of 128 units with ReLU activation followed by batchnorm and 0.5 dropout. This allows the network to extract necessary features from the tensor obtained from pooling.\n* Finally, a simple 6-unit sigmoid-activated dense layer predicts the inputs.\nThe network uses an Adam (Adaptive learning rate with momentum) optimizer with initial learning rate of 0.001. One important thing is that gradient clipping is added to the optimizer. It helps to deal with the so-called exploding gradient problem, which is a common issue in recurrent networks and also empirically proved to be an issue in this particular task. Clipping the values, not the gradient norm, turned out to work a bit better.\n---\nThe schematic picture of the model (created with Keras built-in plot_model() function) looks as follows:\n\n![model](../models/final-model.png)"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"78237ddb18e24573c73f7c48243fdd7b83dadca6"},"cell_type":"code","source":"def build_model():\n    model_input = Input(shape=(MAX_COMMENT_LEN,))\n    X = Embedding(input_dim=MAX_FEATURES, output_dim=EMB_SIZE_FASTTEXT, weights=[emb_matrix_fasttext], trainable=False)(model_input)\n    X = SpatialDropout1D(rate=0.5)(X)\n    \n    X = Dense(units=128, activation=\"relu\")(X)\n    X = BatchNormalization()(X)\n    X = Dropout(rate=0.5)(X)\n    \n    Y = Bidirectional(CuDNNLSTM(units=64, return_sequences=True))(X)\n    Y = Conv1D(filters=128, kernel_size=3, strides=1, padding=\"valid\")(Y)\n    maxpool2 = GlobalMaxPooling1D()(Y)\n    avgpool2 = GlobalAveragePooling1D()(Y)\n    \n    X = Bidirectional(CuDNNGRU(units=128, return_sequences=True))(X)\n    X = Conv1D(filters=64, kernel_size=3, strides=1, padding=\"valid\")(X)\n    maxpool = GlobalMaxPooling1D()(X)\n    avgpool = GlobalAveragePooling1D()(X)\n    \n    X = concatenate([avgpool, maxpool, avgpool2, maxpool2])\n    \n    X = Dense(units=128, activation=\"relu\")(X)\n    X = BatchNormalization()(X)\n    X = Dropout(rate=0.5)(X)\n    X = Dense(units=6, activation=\"sigmoid\")(X)\n\n    model = Model(inputs=model_input, outputs=X)\n    opt = Adam(lr=0.001, clipvalue=1.0)\n    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51b2ec1e14cddaa7c753fea30fe7e1c0c2f2e9cd"},"cell_type":"markdown","source":"I couldn't do crossvalidation (it would take too much time on the computer I had), so I used the standard train-validation-test approach. The train/validation split is 90%/10%.\n\nThe model was set for 15 epochs of training. I've tried a few minibatch sizes; rather small 64-item minibatches turned out to work best, perhaps due to the slight regularizing effect they provide."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"467be3e98af90cb70a34ef22a406eec68143e271"},"cell_type":"code","source":"model = build_model()\nX_tr, X_val, y_train, y_val = train_test_split(train_tokenized, train_target, train_size=0.9, random_state=233)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d142c6e82cee6fd7e4508e13e13b1ff68c8b8f24"},"cell_type":"code","source":"# ROC AUC tracker, model checkpointer\nroc_auc = ROC_AUC_Score(validation_data=(X_val, y_val))\nmodel_path=\"final-model\"\ncheckpointer = ModelCheckpoint(model_path, monitor=\"roc_auc_val\", mode=\"max\", verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c4bbb7bd530b4c9d5fc18809da50f47a9baede9d"},"cell_type":"code","source":"model.fit(x=X_tr, y=y_train, batch_size=64, epochs=15, validation_data=(X_val, y_val), callbacks=[roc_auc, checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b738309b9d3644387d512dc615219160b260542"},"cell_type":"markdown","source":"__If needed, load the final model by running the cell below. DO NOT run the whole training routine. Training this network from scratch would take about 1.5h on a GPU, I don't even know how long on a CPU.__"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e87ea544e1ea6ba9a9233142dbe2d22fe646a5d4"},"cell_type":"code","source":"model = load_model(\"final-model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f0a7b31a77ff0e52cc6bf226226cf6f95d4a16c"},"cell_type":"markdown","source":"Make predictions and save them to file; backup the model."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a4b8a39e6cf4046417f4c7ffc2ef7679c6d67fe8"},"cell_type":"code","source":"pred_test = model.predict(x=[test_tokenized], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f823cd940ad6a524614c96d2f0d1edf619568317"},"cell_type":"code","source":"def predictions_to_csv(predictions, submission_name):\n    submission = pd.read_csv(\"../data/sample_submission.csv\")\n    submission[LABELS] = predictions\n    submission.to_csv(\"../output/\" + submission_name + \".csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"fb116712ccea72888eb0d5637ccca729016ca8d7"},"cell_type":"code","source":"predictions_to_csv(pred_test, \"final-submission\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"09de011296c91e60e6e73c99afbcf8a54946ca48"},"cell_type":"code","source":"def backup_model(model, model_name):\n    model_path = \"../models/\" + model_name + \".h5\"\n    model.save(model_path)\n    picture_path = \"../models/\" + model_name + \".png\"\n    plot_model(model, to_file=picture_path)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"03aef8c30b947c19d9b2df285225772f92fc0b45"},"cell_type":"code","source":"backup_model(model, \"final-model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5a5a8161541b686b5004c41153c8d4cc3eb20aa"},"cell_type":"markdown","source":"### 3. Results and prospects of improvement\n* The model scores __0.9801 ROC AUC__ on the private (full) test set (and 0.9803 on the public test set). This is a pretty good estimate of quality as well as the base for Kaggle scoring.\n* This is __0.0084 ROC AUC loss to the winning model__.\n\nOne way of improving the model I would like to try if I had time and resources is the trick the winning team used. It's a technique of data augmentation by translating each comment to German, French and Spanish using Google Translate's public API, then retranslating to English and adding the resulting text to the dataset. This method, however, comes with numerous difficulties I'll explain in detail during the presentation.\n\nWere it an active Kaggle competition, I'd probably begin ensembling (most of the top 5% solutions were ensembles), but here I wanted to keep things clean and build a single model.\n\nIf I had much more time or an own GPU, I would switch from train/validation/test framework to crossvalidation to get better estimates of model quality.\n\n\n"},{"metadata":{"_uuid":"bee6fe0d6c6e7be119e7dab486d0d3a17a998f4b"},"cell_type":"markdown","source":"-------------------------------------------------------------------------------------------------------------------------\n#### Sources:\n* The model is not based on any publicly available solution known to me. I have read the descriptions of 1st and 2nd place solutions, but could not apply much of what they mentioned. However, I did benefit greatly from reading various discussions on Kaggle forums, especially regarding the dataset's properties (warnings about easy overfitting, presence of IP addresses in the training data, etc.).\n* The Fasttext papers: [Bojanowski et al. (2016), Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606); [Joulin et al. (2017), Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759).\n* The idea of forking the model, applying recurrent, convolutional and pooling layers and then concatenating the results is loosely based on a similar idea from deep image processing networks which is called \"Inception blocks\". I'll be talking about it during the presentation; the original image processing paper is [Szegedy, Liu et al. (2014), Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842).\n* Much of what I learnt about recurrent networks when doing the project was from [Goodfellow, Bengio & Courville (2016), Deep Learning](https://mitpress.mit.edu/books/deep-learning). [Andrew Ng's lectures on sequence models](https://www.youtube.com/playlist?list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7) were also helpful in developing higher-level intuition of how the various layers behave."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"dfb15ecad26bb8ce0414943d4115e87e9e47f8d7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}