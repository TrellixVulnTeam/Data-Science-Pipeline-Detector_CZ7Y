{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-12T05:01:02.865847Z","iopub.execute_input":"2022-03-12T05:01:02.866188Z","iopub.status.idle":"2022-03-12T05:01:02.882042Z","shell.execute_reply.started":"2022-03-12T05:01:02.866149Z","shell.execute_reply":"2022-03-12T05:01:02.881332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"This is Rathod Mayur || 19BCE221 || This is my 3rd practical of IRS\")","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:02.883975Z","iopub.execute_input":"2022-03-12T05:01:02.88582Z","iopub.status.idle":"2022-03-12T05:01:02.892354Z","shell.execute_reply.started":"2022-03-12T05:01:02.88573Z","shell.execute_reply":"2022-03-12T05:01:02.891594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# I this fetch_20newsgroups dataset for make prediction using CountVectorizer.\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:02.894074Z","iopub.execute_input":"2022-03-12T05:01:02.895063Z","iopub.status.idle":"2022-03-12T05:01:02.912122Z","shell.execute_reply.started":"2022-03-12T05:01:02.895025Z","shell.execute_reply":"2022-03-12T05:01:02.911386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CountVectorizer","metadata":{}},{"cell_type":"code","source":"corpus = ['This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?',]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:02.914645Z","iopub.execute_input":"2022-03-12T05:01:02.915351Z","iopub.status.idle":"2022-03-12T05:01:02.921268Z","shell.execute_reply.started":"2022-03-12T05:01:02.915315Z","shell.execute_reply":"2022-03-12T05:01:02.92038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(X)\nvectorizer.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:02.924528Z","iopub.execute_input":"2022-03-12T05:01:02.927261Z","iopub.status.idle":"2022-03-12T05:01:02.945407Z","shell.execute_reply.started":"2022-03-12T05:01:02.927224Z","shell.execute_reply":"2022-03-12T05:01:02.94195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.toarray())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:02.94665Z","iopub.execute_input":"2022-03-12T05:01:02.949702Z","iopub.status.idle":"2022-03-12T05:01:02.954355Z","shell.execute_reply.started":"2022-03-12T05:01:02.94966Z","shell.execute_reply":"2022-03-12T05:01:02.953624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\nX2 = vectorizer2.fit_transform(corpus)\nprint(vectorizer2.get_feature_names_out())\nprint(\"***Vector Repersentation***\\n\",X2.toarray())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:02.955724Z","iopub.execute_input":"2022-03-12T05:01:02.956547Z","iopub.status.idle":"2022-03-12T05:01:02.965672Z","shell.execute_reply.started":"2022-03-12T05:01:02.956502Z","shell.execute_reply":"2022-03-12T05:01:02.964807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the Example for sklearn datasets.\n# In this is I perform CountVectorizer and then make Prediction.\n\n# Create our vectorizer\nvectorizer = CountVectorizer()\n\ntrain = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'))\ntest = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n\n# Get the training vectors\nvectors = vectorizer.fit_transform(train.data)\n\nclf = MultinomialNB(alpha=.01)\nclf.fit(vectors, train.target)\n\n# Get the test vectors\nvectors_test = vectorizer.transform(test.data)\n\n# Predict and score the vectors\npred = clf.predict(vectors_test)\nacc_score = metrics.accuracy_score(test.target, pred)\nf1_score = metrics.f1_score(test.target, pred, average='macro')\n\nprint('Total accuracy classification score: {}'.format(acc_score))\nprint('Total F1 classification score: {}'.format(f1_score))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:02.967187Z","iopub.execute_input":"2022-03-12T05:01:02.968105Z","iopub.status.idle":"2022-03-12T05:01:09.714087Z","shell.execute_reply.started":"2022-03-12T05:01:02.968071Z","shell.execute_reply":"2022-03-12T05:01:09.713308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TfidfVectorizer","metadata":{}},{"cell_type":"code","source":"txt = [\"He is ::having a great Time, at the park time?\",\n       \"She, unlike most women, is a big player on the park's grass.\",\n       \"she can't be going\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.716522Z","iopub.execute_input":"2022-03-12T05:01:09.717008Z","iopub.status.idle":"2022-03-12T05:01:09.721488Z","shell.execute_reply.started":"2022-03-12T05:01:09.716965Z","shell.execute_reply":"2022-03-12T05:01:09.7207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(txt)\nprint(vectorizer.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.722768Z","iopub.execute_input":"2022-03-12T05:01:09.723137Z","iopub.status.idle":"2022-03-12T05:01:09.739733Z","shell.execute_reply.started":"2022-03-12T05:01:09.723101Z","shell.execute_reply":"2022-03-12T05:01:09.739038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nX.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.740727Z","iopub.execute_input":"2022-03-12T05:01:09.742986Z","iopub.status.idle":"2022-03-12T05:01:09.754063Z","shell.execute_reply.started":"2022-03-12T05:01:09.742951Z","shell.execute_reply":"2022-03-12T05:01:09.753433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt1 = ['His smile was not perfect', 'His smile was not not not not so perfect', 'she not sang', 'she was not perfect']\nprint (\"The text: \", txt1)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.755118Z","iopub.execute_input":"2022-03-12T05:01:09.75542Z","iopub.status.idle":"2022-03-12T05:01:09.76108Z","shell.execute_reply.started":"2022-03-12T05:01:09.755384Z","shell.execute_reply":"2022-03-12T05:01:09.760347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This will print out a list of words used, and their index in the vectors\nprint('Vocabulary: ')\nprint(vectorizer.vocabulary_)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.762474Z","iopub.execute_input":"2022-03-12T05:01:09.762967Z","iopub.status.idle":"2022-03-12T05:01:09.77064Z","shell.execute_reply.started":"2022-03-12T05:01:09.762873Z","shell.execute_reply":"2022-03-12T05:01:09.769723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\ntxt_fitted = tf.fit(txt1)\ntxt_transformed = tf.fit_transform(txt1)\ntxt_transformed.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.77209Z","iopub.execute_input":"2022-03-12T05:01:09.772404Z","iopub.status.idle":"2022-03-12T05:01:09.783696Z","shell.execute_reply.started":"2022-03-12T05:01:09.772369Z","shell.execute_reply":"2022-03-12T05:01:09.782558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if we wanted to get the vector for one word:\nprint('Hot vector: ')\nprint(vectorizer.transform(['smile']).toarray())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.784787Z","iopub.execute_input":"2022-03-12T05:01:09.785579Z","iopub.status.idle":"2022-03-12T05:01:09.793696Z","shell.execute_reply.started":"2022-03-12T05:01:09.785543Z","shell.execute_reply":"2022-03-12T05:01:09.792976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idf = tf.idf_\nprint(dict(zip(txt_fitted.get_feature_names(), idf)))\n\n# We see that the tokens 'sang','she' have the most idf weight because \n# they are the only tokens that appear in one document only.\n# The token 'not' appears 6 times but it is also in all documents, so its idf is the lowest","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.795094Z","iopub.execute_input":"2022-03-12T05:01:09.795523Z","iopub.status.idle":"2022-03-12T05:01:09.802515Z","shell.execute_reply.started":"2022-03-12T05:01:09.795487Z","shell.execute_reply":"2022-03-12T05:01:09.80177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rr = dict(zip(txt_fitted.get_feature_names(), idf))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.806714Z","iopub.execute_input":"2022-03-12T05:01:09.807685Z","iopub.status.idle":"2022-03-12T05:01:09.812024Z","shell.execute_reply.started":"2022-03-12T05:01:09.807651Z","shell.execute_reply":"2022-03-12T05:01:09.811032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\ntoken_weight.columns=('token','weight')\ntoken_weight = token_weight.sort_values(by='weight', ascending=False)\ntoken_weight ","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.813467Z","iopub.execute_input":"2022-03-12T05:01:09.814528Z","iopub.status.idle":"2022-03-12T05:01:09.829551Z","shell.execute_reply.started":"2022-03-12T05:01:09.814492Z","shell.execute_reply":"2022-03-12T05:01:09.82884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot bar graph on this txt.\nsns.barplot(x='token', y='weight', data=token_weight)            \nplt.title(\"Inverse Document Frequency(idf) per token\")\nfig=plt.gcf()\nfig.set_size_inches(10,5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:09.830579Z","iopub.execute_input":"2022-03-12T05:01:09.831178Z","iopub.status.idle":"2022-03-12T05:01:10.130329Z","shell.execute_reply.started":"2022-03-12T05:01:09.831142Z","shell.execute_reply":"2022-03-12T05:01:10.129617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get feature names \nfeature_names = np.array(tf.get_feature_names())\nsorted_by_idf = np.argsort(tf.idf_)\n\nprint(\"Features with lowest idf:\\n{}\".format(feature_names[sorted_by_idf[:3]]))\nprint(\"\\nFeatures with highest idf:\\n{}\".format(feature_names[sorted_by_idf[-3:]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:10.131513Z","iopub.execute_input":"2022-03-12T05:01:10.131744Z","iopub.status.idle":"2022-03-12T05:01:10.141267Z","shell.execute_reply.started":"2022-03-12T05:01:10.131712Z","shell.execute_reply":"2022-03-12T05:01:10.140509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Work On Given Dataset","metadata":{}},{"cell_type":"code","source":"train_data  =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:10.142837Z","iopub.execute_input":"2022-03-12T05:01:10.143348Z","iopub.status.idle":"2022-03-12T05:01:11.4271Z","shell.execute_reply.started":"2022-03-12T05:01:10.143312Z","shell.execute_reply":"2022-03-12T05:01:11.426279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:11.428518Z","iopub.execute_input":"2022-03-12T05:01:11.428977Z","iopub.status.idle":"2022-03-12T05:01:11.473325Z","shell.execute_reply.started":"2022-03-12T05:01:11.428934Z","shell.execute_reply":"2022-03-12T05:01:11.472653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that over data highly imbalanced.","metadata":{}},{"cell_type":"code","source":"test_data=pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:01:11.474493Z","iopub.execute_input":"2022-03-12T05:01:11.474744Z","iopub.status.idle":"2022-03-12T05:01:12.991375Z","shell.execute_reply.started":"2022-03-12T05:01:11.474709Z","shell.execute_reply":"2022-03-12T05:01:12.990662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nimport pickle\nimport string\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score , accuracy_score , confusion_matrix , f1_score\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:14:00.41208Z","iopub.execute_input":"2022-03-12T05:14:00.412926Z","iopub.status.idle":"2022-03-12T05:14:00.419979Z","shell.execute_reply.started":"2022-03-12T05:14:00.41288Z","shell.execute_reply":"2022-03-12T05:14:00.419045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_comment(text):\n    text = text.lower()\n    text = re.sub('<.*?>','',text).strip() # remove html chars\n    text = re.sub('\\[|\\(.*\\]|\\)','', text).strip() # remove text in square brackets and parenthesis\n    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation marks\n    text = re.sub(\"(\\\\W)\",\" \",text).strip() # remove non-ascii chars\n    text = re.sub('\\S*\\d\\S*\\s*','', text).strip()  # remove words containing numbers\n    return text.strip()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:14:07.631753Z","iopub.execute_input":"2022-03-12T05:14:07.632413Z","iopub.status.idle":"2022-03-12T05:14:07.639291Z","shell.execute_reply.started":"2022-03-12T05:14:07.63237Z","shell.execute_reply":"2022-03-12T05:14:07.638208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First convert into string\ntrain_data.comment_text = train_data.comment_text.astype(str)\n\n# now clean the data using clean_comment.\ntrain_data.comment_text = train_data.comment_text.apply(clean_comment)\ntrain_data.comment_text.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:15:21.880714Z","iopub.execute_input":"2022-03-12T05:15:21.881617Z","iopub.status.idle":"2022-03-12T05:15:37.495391Z","shell.execute_reply.started":"2022-03-12T05:15:21.88157Z","shell.execute_reply":"2022-03-12T05:15:37.494529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.stem.snowball import SnowballStemmer\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\nsnow_stemmer = SnowballStemmer(language='english')\n\nstopwords = nlp.Defaults.stop_words\ndef apply_stemmer(text):\n    words = text.split()\n    sent = [snow_stemmer.stem(word) for word in words if not word in set(stopwords)]\n    return ' '.join(sent)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:16:09.403035Z","iopub.execute_input":"2022-03-12T05:16:09.403742Z","iopub.status.idle":"2022-03-12T05:16:10.148657Z","shell.execute_reply.started":"2022-03-12T05:16:09.403696Z","shell.execute_reply":"2022-03-12T05:16:10.147815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply Stemming\ntrain_data.comment_text = train_data.comment_text.apply(apply_stemmer)\ntrain_data.comment_text.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:16:13.171489Z","iopub.execute_input":"2022-03-12T05:16:13.171974Z","iopub.status.idle":"2022-03-12T05:18:09.320802Z","shell.execute_reply.started":"2022-03-12T05:16:13.171906Z","shell.execute_reply":"2022-03-12T05:18:09.319938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data.comment_text\ny = train_data.drop(['id','comment_text'],axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:18:09.322598Z","iopub.execute_input":"2022-03-12T05:18:09.323016Z","iopub.status.idle":"2022-03-12T05:18:09.3313Z","shell.execute_reply.started":"2022-03-12T05:18:09.322968Z","shell.execute_reply":"2022-03-12T05:18:09.330244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test =  train_test_split(X,y,test_size = 0.2,random_state = 45)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:18:09.332909Z","iopub.execute_input":"2022-03-12T05:18:09.333239Z","iopub.status.idle":"2022-03-12T05:18:09.397522Z","shell.execute_reply.started":"2022-03-12T05:18:09.333195Z","shell.execute_reply":"2022-03-12T05:18:09.396574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    strip_accents='unicode',     \n    analyzer='word',            \n    token_pattern=r'\\w{1,}',    \n    ngram_range=(1, 3),         \n    stop_words='english',\n    sublinear_tf=True)\n\nword_vectorizer.fit(x_train)    \ntrain_word_features = word_vectorizer.transform(x_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:18:09.401682Z","iopub.execute_input":"2022-03-12T05:18:09.402148Z","iopub.status.idle":"2022-03-12T05:19:20.565138Z","shell.execute_reply.started":"2022-03-12T05:18:09.402097Z","shell.execute_reply":"2022-03-12T05:19:20.564133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_transformed = word_vectorizer.transform(x_train)\nX_test_transformed = word_vectorizer.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:19:20.566612Z","iopub.execute_input":"2022-03-12T05:19:20.566971Z","iopub.status.idle":"2022-03-12T05:19:45.331172Z","shell.execute_reply.started":"2022-03-12T05:19:20.566913Z","shell.execute_reply":"2022-03-12T05:19:45.3303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.linear_model import LogisticRegression\nseed=100\n\nlog_reg = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=seed)\n\n# fit model\nclassifier_ovr_log = OneVsRestClassifier(log_reg)\nclassifier_ovr_log.fit(X_train_transformed, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:19:45.332502Z","iopub.execute_input":"2022-03-12T05:19:45.332802Z","iopub.status.idle":"2022-03-12T05:21:42.865588Z","shell.execute_reply.started":"2022-03-12T05:19:45.332746Z","shell.execute_reply":"2022-03-12T05:21:42.864902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred_proba = classifier_ovr_log.predict_proba(X_train_transformed)\ny_test_pred_proba = classifier_ovr_log.predict_proba(X_test_transformed)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:21:42.866795Z","iopub.execute_input":"2022-03-12T05:21:42.867063Z","iopub.status.idle":"2022-03-12T05:21:43.329568Z","shell.execute_reply.started":"2022-03-12T05:21:42.867026Z","shell.execute_reply":"2022-03-12T05:21:43.328802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_predictions(df,classifier):\n    df.comment_text = df.comment_text.apply(clean_comment)\n    df.comment_text = df.comment_text.apply(apply_stemmer)\n    X_test = df.comment_text\n    X_test_transformed = word_vectorizer.transform(X_test)\n    y_test_pred = classifier.predict_proba(X_test_transformed)\n    return y_test_pred","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:21:43.330902Z","iopub.execute_input":"2022-03-12T05:21:43.331644Z","iopub.status.idle":"2022-03-12T05:21:43.337156Z","shell.execute_reply.started":"2022-03-12T05:21:43.331601Z","shell.execute_reply":"2022-03-12T05:21:43.336515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=make_test_predictions(test_data,classifier_ovr_log)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:21:43.338412Z","iopub.execute_input":"2022-03-12T05:21:43.339063Z","iopub.status.idle":"2022-03-12T05:24:27.060644Z","shell.execute_reply.started":"2022-03-12T05:21:43.339008Z","shell.execute_reply":"2022-03-12T05:24:27.059706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_df = pd.DataFrame(y_pred,columns=y.columns)\ny_pred_df","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:24:27.063434Z","iopub.execute_input":"2022-03-12T05:24:27.063901Z","iopub.status.idle":"2022-03-12T05:24:27.083327Z","shell.execute_reply.started":"2022-03-12T05:24:27.06385Z","shell.execute_reply":"2022-03-12T05:24:27.082297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.concat([test_data.id, y_pred_df], axis=1)\nsubmission_df.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:24:45.62119Z","iopub.execute_input":"2022-03-12T05:24:45.621489Z","iopub.status.idle":"2022-03-12T05:24:47.838805Z","shell.execute_reply.started":"2022-03-12T05:24:45.621454Z","shell.execute_reply":"2022-03-12T05:24:47.837961Z"},"trusted":true},"execution_count":null,"outputs":[]}]}