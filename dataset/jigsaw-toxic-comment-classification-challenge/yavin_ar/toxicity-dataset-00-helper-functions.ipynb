{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier , GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score, confusion_matrix, f1_score,mean_squared_error, mean_absolute_error,mean_squared_log_error,max_error,r2_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, GridSearchCV, cross_validate\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nimport numpy as np\n#from sklearn.feature_selection import SelectFromModel\nimport xgboost\n#import lightgbm as lgbm\nfrom textblob import TextBlob as tb\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n\n\n\ndef quick_classifier_setup(grid,crossvalidation,score):\n    \n    estimators = []\n    mnb = MultinomialNB()\n    if grid == 1:\n        mnb_parameters = {'alpha': [0,0.5,1]}\n        mnb = GridSearchCV(mnb,mnb_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['mnb',mnb,mnb_parameters])\n    else: \n        estimators.append(['mnb',mnb])\n    mlp = MLPClassifier()\n    if grid == 1:\n        mlp_parameters = {'hidden_layer_sizes':[(10,),(5,5),(2,2),(50,50,50)]} #, (50,100,50), (100,), [100]*5]}\n        mlp = GridSearchCV(mlp,mlp_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['mlp',mlp,mlp_parameters])\n    else: \n        estimators.append(['mlp',mlp])\n    ada = AdaBoostClassifier() \n    if grid == 1:\n        ada_parameters = {'n_estimators':[15,25,50,75,100,250,500]}\n        ada = GridSearchCV(ada,ada_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['ada',ada,ada_parameters])\n    else: \n        estimators.append(['ada',ada])\n    svc = LinearSVC()\n    if grid == 1:\n        svc_parameters = {'C':[1, 10, 100, 1000], 'loss':('hinge','squared_hinge')}\n        svc = GridSearchCV(svc,svc_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['svc',svc,svc_parameters])\n    else: \n        estimators.append(['svc',svc])\n    gb = GradientBoostingClassifier()\n    if grid == 1:\n        gb_parameters = {'n_estimators' : [15, 50, 100,150,200],'max_features': ['auto','sqrt','log2', None],'loss':['deviance', 'exponential'] }\n        gb = GridSearchCV(gb,gb_parameters,cv=crossvalidation,scoring=score,return_train_score=True)     \n        estimators.append(['gb',gb,gb_parameters])\n    else: \n        estimators.append(['gb',gb])\n    xgb = xgboost.XGBClassifier(verbosity = 0)\n    if grid == 1:\n        xgb_parameters = {'max_depth':[2,3,4]}\n        xgb = GridSearchCV(xgb,xgb_parameters,cv=crossvalidation,scoring=score,return_train_score=True)     \n        estimators.append(['xgb',xgb,xgb_parameters])\n    else: \n        estimators.append(['xgb',xgb])\n    logr = LogisticRegression()\n    #parameters = {'solver': ['newton-cg','lbfgs','liblinear','sag','saga'],'max_iter':[100,200,300,500] }\n    if grid == 1:\n        logr_parameters = {'solver': ['newton-cg','lbfgs','liblinear','sag','saga'],'max_iter':[100,200,300,500],'C': [0.001, 0.01, 0.1, 1, 10, 100],'class_weight':['balanced',None,{0:0.01, 1:1.0},{0:0.05, 1:1.0}]}\n        logr = GridSearchCV(logr,logr_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['logr',logr,logr_parameters])\n    else: \n        estimators.append(['logr',logr])\n    rfc = RandomForestClassifier()\n    #parameters = {'n_estimators':[50, 100, 250],'max_depth':[1,2,5],'max_features':['auto','sqrt','log2'],'min_samples_split':[2, 3, 10],'bootstrap':[True, False],'criterion':[\"gini\", \"entropy\"]}\n    if grid == 1:\n        rfc_parameters = {'n_estimators' : [15, 20,50, 100,150,200],'criterion' : ['gini', 'entropy'],'max_features' : ['auto', 'sqrt', 'log2', None],'max_depth' : [None, 50],'min_samples_split' : [7, 11],'min_weight_fraction_leaf' : [0.0, 0.2],'max_leaf_nodes' : [15, 18, 20, 25] }\n        rfc = GridSearchCV(rfc,rfc_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['rfc',rfc,rfc_parameters])\n    else: \n        estimators.append(['rfc',rfc])\n    for estimator in estimators:\n        print(estimator[0])\n    return estimators\ndef quick_regressor_setup(grid,crossvalidation,score):\n\n    estimators = []\n    linreg = LinearRegression()\n    if grid == 1:\n        linreg_parameters = {'normalize':[False,True],'positive':[False,True]}\n        linreg = GridSearchCV(linreg,linreg_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['linr',linreg,linreg_parameters])\n    else:\n        estimators.append(['linr',linreg])\n    \n    elnet = ElasticNet()\n    if grid == 1:\n        elnet_parameters = {\"max_iter\": [1, 5, 10],\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n        elnet = GridSearchCV(elnet,elnet_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['elnet',elnet,elnet_parameters])\n    else:\n        estimators.append(['elnet',elnet])\n\n    ridge = Ridge()\n    if grid == 1:\n        ridge_parameters = {'alpha':[200, 230, 250,265, 270, 275, 290, 300, 500],\"fit_intercept\": [True, False], \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n        ridge = GridSearchCV(ridge,ridge_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['ridge',ridge,ridge_parameters])\n    else:\n        estimators.append(['ridge',ridge])\n\n    gb = GradientBoostingRegressor()\n    #parameters = {'loss':['ls','lad','huber','quantile'],'max_features':[None,'auto','sqrt','log2'],'criterion':['friedman_mse','mse','mae'],'min_samples_split':[2,4,6],'min_samples_leaf':[1,2,5],'max_depth':[2,3,4,5]}\n    if grid == 1:\n        gb_parameters = {'loss':['ls','lad','huber','quantile'],'criterion':['friedman_mse','mse','mae'],'max_depth':[2,3,4]}\n        gb = GridSearchCV(gb,gb_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['gb',gb,gb_parameters])\n    else: \n        estimators.append(['gb',gb])\n    dtr= DecisionTreeRegressor()\n    if grid == 1:\n            dtr_parameters = {\"criterion\": [\"mse\", \"mae\"],\"min_samples_split\": [10, 20, 40],\"max_depth\": [2, 6, 8],\"min_samples_leaf\": [20, 40, 100],\"max_leaf_nodes\": [5, 20, 100]}\n            dtr = GridSearchCV(dtr,dtr_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n            estimators.append(['dtr',dtr,dtr_parameters])\n    else: \n        estimators.append(['dtr',dtr])\n\n    xgb = xgboost.XGBRegressor()\n    if grid == 1:\n        xgb_parameters = {'max_depth':[2,3,4,5]}\n        xgb = GridSearchCV(xgb,xgb_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['xgb',xgb,xgb_parameters])\n    else: \n        estimators.append(['xgb',xgb])\n    rf = RandomForestRegressor()\n    if grid == 1:\n        rf_parameters = {\"n_estimators\" : [10,50,100,250,300],\"min_samples_split\": [10, 20, 40],\"max_depth\": [2, 6, 8],\"min_samples_leaf\": [20, 40, 100],\"max_leaf_nodes\": [5, 20, 100]}\n        rf = GridSearchCV(rf,rf_parameters,cv=crossvalidation,scoring=score,return_train_score=True)\n        estimators.append(['rf',rf,rf_parameters])\n    else: \n        estimators.append(['rf',rf])\n    for estimator in estimators:\n        print(estimator[0])\n    return estimators\ndef quick_sentiment(df,series):\n    df['textblob_score'] = df[series].apply(lambda x: tb(x).sentiment.polarity)\n    sia = SIA()\n    df['vader_score'] = [sia.polarity_scores(x)['compound'] for x in df[series]]\n    df['vader_neg'] = [sia.polarity_scores(x)['neg'] for x in df[series]]\n    df['vader_neu'] = [sia.polarity_scores(x)['neu'] for x in df[series]]\n    df['vader_pos'] = [sia.polarity_scores(x)['pos'] for x in df[series]]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##########################\n## TEXT PROCESSING FUNCTIONS\n##########################\ndef expand_contracted_expressions(text):\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub(r\"aren't\",\"are not\",text)\n    text = re.sub(r\"can't\",\"cannot\",text)\n    text = re.sub(r\"couldn't\",\"could not\",text)\n    text = re.sub(r\"didn't\",\"did not\",text)\n    text = re.sub(r\"doesn't\",\"does not\",text)\n    text = re.sub(r\"don't\",\"do not\",text)\n    text = re.sub(r\"hadn't\",\"had not\",text)\n    text = re.sub(r\"hasn't\",\"has not\",text)\n    text = re.sub(r\"haven't\",\"have not\",text)\n    text = re.sub(r\"he'd\",\"he would\",text)\n    text = re.sub(r\"he'll\",\"he will\",text)\n    text = re.sub(r\"he's\",\"he is\",text)\n    text = re.sub(r\"i'd\",\"I would\",text)\n    text = re.sub(r\"i'd\",\"I had\",text)\n    text = re.sub(r\"i'll\",\"I will\",text)\n    text = re.sub(r\"i'm\",\"I am\",text)\n    text = re.sub(r\"isn't\",\"is not\",text)\n    text = re.sub(r\"it's\",\"it is\",text)\n    text = re.sub(r\"it'll\",\"it will\",text)\n    text = re.sub(r\"i've\",\"I have\",text)\n    text = re.sub(r\"let's\",\"let us\",text)\n    text = re.sub(r\"mightn't\",\"might not\",text)\n    text = re.sub(r\"mustn't\",\"must not\",text)\n    text = re.sub(r\"shan't\",\"shall not\",text)\n    text = re.sub(r\"she'd\",\"she would\",text)\n    text = re.sub(r\"she'll\",\"she will\",text)\n    text = re.sub(r\"she's\",\"she is\",text)\n    text = re.sub(r\"shouldn't\",\"should not\",text)\n    text = re.sub(r\"that's\",\"that is\",text)\n    text = re.sub(r\"there's\",\"there is\",text)\n    text = re.sub(r\"they'd\",\"they would\",text)\n    text = re.sub(r\"they'll\",\"they will\",text)\n    text = re.sub(r\"they're\",\"they are\",text)\n    text = re.sub(r\"they've\",\"they have\",text)\n    text = re.sub(r\"we'd\",\"we would\",text)\n    text = re.sub(r\"we're\",\"we are\",text)\n    text = re.sub(r\"weren't\",\"were not\",text)\n    text = re.sub(r\"we've\",\"we have\",text)\n    text = re.sub(r\"what'll\",\"what will\",text)\n    text = re.sub(r\"what're\",\"what are\",text)\n    text = re.sub(r\"what's\",\"what is\",text)\n    text = re.sub(r\"what've\",\"what have\",text)\n    text = re.sub(r\"where's\",\"where is\",text)\n    text = re.sub(r\"who'd\",\"who would\",text)\n    text = re.sub(r\"who'll\",\"who will\",text)\n    text = re.sub(r\"who're\",\"who are\",text)\n    text = re.sub(r\"who's\",\"who is\",text)\n    text = re.sub(r\"who've\",\"who have\",text)\n    text = re.sub(r\"won't\",\"will not\",text)\n    text = re.sub(r\"wouldn't\",\"would not\",text)\n    text = re.sub(r\"you'd\",\"you would\",text)\n    text = re.sub(r\"you'll\",\"you will\",text)\n    text = re.sub(r\"you're\",\"you are\",text)\n    text = re.sub(r\"you've\",\"you have\",text)\n    text = re.sub(r\"'re\",\" are\",text)\n    text = re.sub(r\"wasn't\",\"was not\",text)\n    text = re.sub(r\"we'll\",\" will\",text)\n    text = re.sub(r\"didn't\",\"did not\",text)\n    text = re.sub(r\"tryin'\",\"trying\",text)\n    text = text.strip(' ')\n    return text\ndef clean_html(html):\n    dirty_string = re.compile('<.*?>')\n    clean_string = re.sub(dirty_string, '',html)\n    return clean_string\ndef text_cleaner(text):\n    text = unidecode.unidecode(text)\n    text = re.sub('\\n',' ',text)\n    text = re.sub('%20',' ',text)\n    text = re.sub('https?://\\S+|www\\.\\S+', 'url', text)\n    text = re.sub('[^A-Za-z0-9. ]+', '', text)\n    #text = re.sub('[.]', '. ', text)\n    text = str(text).lower()\n    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n    text = re.sub('  ',' ',text)\n    text = re.sub(' ',' ',text)\n    text = text.strip()\n    return text\n\ndef digit_transformer(text):\n\ttext = re.sub('[0-9]{5,}', 'largenumber', text)\n\t#text = re.sub('[0-9]{4}', '4digitnumber', text)\n\t#text = re.sub('[0-9]{3}', '3digitnumber', text)\n\t#text = re.sub('[0-9]{2}', '2digitnumber', text)  \n\treturn text\ndef remove_stopwords(text,language):\n\twords = text.lower().split()\n\tstops = set(stopwords.words(language))\n\twords = [w for w in words if not w in stops]\n    \n\ttext = \" \".join(words)\n\treturn text\n\ndef custom_tokenizer(text):\n\ttext = word_tokenize(text)\n\t#text = [w for w in text if len(w)>3 and not w in stopwords_english]\n\t#text = [w for w in text if len(w)>3 and not w in stopwords_spanish]\n\t#text = [wnlm.lemmatize(w) for w in text]\n\treturn text\ndef spanish_stemmer(token_list):\n    spanish_stemmer = SnowballStemmer(language='spanish')\n    token_list = [spanish_stemmer.stem(token) for token in token_list]\n    return token_list\ndef english_stemmer(token_list):\n    english_stemmer = SnowballStemmer(language='english')\n    token_list = [english_stemmer.stem(token) for token in token_list]\n    return token_list\n#def get_wordnet_pos(word):\n\t\t\t#\"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n\t\t\t#tag = nltk.pos_tag([word])[0][1][0].upper()\n\t\t\t#tag_dict = {\"J\": wordnet.ADJ,\n\t\t\t#\"N\": wordnet.NOUN,\n\t\t\t#\"V\": wordnet.VERB,\n\t\t\t#\"R\": wordnet.ADV}\n\t\t\t#return tag_dict.get(tag, wordnet.NOUN)\n#def tokenize_words(input):\n\t\t\t#lowercase everything to standardize it\n\t\t\t#input = input.lower()\n\t\t\t#instantiate the tokenizer\n\t\t\t#tokenizer = RegexpTokenizer(r'\\w+')\n\t\t\t#tokens = tokenizer.tokenize(input)\n\t\t\t#if the created token isn't in the stop words, make it part of \"filtered\"\n\t\t\t#filtered = filter(lambda token: token not in stopwords.words('spanish'), tokens)\n\t\t\t#return \" \".join(filtered)\n##########################\n## SIMILARITY FUNCTIONS\n##########################\ndef jaccard_similarity(query, document):\n    intersection = set(query).intersection(set(document))\n    union = set(query).union(set(document))\n    return 100.0*len(intersection)/len(union)\ndef word_cosine_similarity(base,compare):\n    basecounter = Counter(base)\n    comparecounter = Counter(compare)\n    terms = set(basecounter).union(comparecounter)\n    dotprod = sum(basecounter.get(k, 0) * comparecounter.get(k, 0) for k in terms)\n    magA = math.sqrt(sum(basecounter.get(k, 0)**2 for k in terms))\n    magB = math.sqrt(sum(comparecounter.get(k, 0)**2 for k in terms))\n    return 100.0*dotprod / (magA * magB)\ndef calculate_similarity(base_text,compare_text):\n    jaccard_sim = []\n    share_word_sim = []\n    cosine_sim = []\n    \n    for text in compare_text:\n        intersection = set(base_text.split()) & set(text.split())\n        jaccard_sim.append(round(jaccard_similarity(text,base_text),3))\n        share_word_sim.append(round(100.0*(len(intersection)/len(text.split())),3))\n        cosine_sim.append(round(word_cosine_similarity(custom_tokenizer(base_text),custom_tokenizer(text)),3))\n                        \n    return jaccard_sim, share_word_sim, cosine_sim                                  \ndef list_similarity(base,compare):\n    baseset = set(base)\n    compareset = set(compare)\n    overlap = baseset & compareset\n    universe = baseset | compareset\n    return 100.0*len(overlap)/(0.01+len(compareset))\n#def sklearn_vector_similarity(vectorizer,tokens1,tokens2):\n    #    vec1 = np.array(vectorizer.fit_transform(tokens1)).flatten()\n    #    vec2 = np.array(vectorizer.transform(tokens2)).flatten()\n    #    similarity = cosine_similarity(vec1.reshape(-1, 1),vec2.reshape(-1, 1)) \n    #    return similarity\n##########################\n## SPACY W2V FUNCTIONS\n##########################\ndef get_named_entities(text):\n    entities = []\n    for entity in text.ents:\n        #ent_list = [entity.text,entity.label_]\n        entities.append(entity.text)\n    return entities\ndef w2v_train_model(base_text,compare_text):\n    full_text = base_text\n    for text in compare_text:\n        full_text = full_text+' '+text\n        #full_text = full_text.replace('.','. ')\n    sentences = [custom_tokenizer(sentence) for sentence in full_text.split('.')]\n    w2v_model = Word2Vec(min_count=0,size=50,alpha=0.03,sample=6e-5,min_alpha=-0.007)\n    w2v_model.build_vocab(sentences,progress_per=10000)\n    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=5, report_delay=1)\n    #model_name = \"300features_40minwords_10context\"\n    #model.save(model_name)\n    return w2v_model\ndef w2v_get_similar_words(w2v_model,word):\n    word_list = w2v_model.wv.most_similar(positive=word)\n    #word_list2 = w2v_model.wv.most_similar_cosmul(positive=word)\n    word_list = [word[0] for word in word_list]\n    #word_list2 = [word[0] for word in word_list2]\n    #word_list = list(dict.fromkeys(word_list))\n    return word_list\ndef w2v_wordvector_similarity(w2v_model,text1,text2):\n    s1 = remove_periods(text1)\n    s2 = remove_periods(text2)\n    similarity = w2v_model.wv.n_similarity(s1.split(), s2.split())\n    return 100.0*similarity\ndef w2v_text_avg_vector(text, model, num_features):\n    vector = np.zeros(num_features,dtype=\"float32\")\n    nwords = 0\n    index2word_set = set(model.wv.index2word)\n    for word in text:\n        if word in index2word_set:\n            nwords = nwords + 1\n            vector = np.add(vector,model[word])\n    vector = np.divide(vector, nwords)\n    return vector\ndef w2v_vector_similarity(w2v_model,text1,text2,num_features):\n    vec1 = w2v_text_avg_vector(text1,w2v_model,num_features)\n    vec2 = w2v_text_avg_vector(text2,w2v_model,num_features)\n    similarity = float(cosine_similarity([vec1],[vec2]))\n    return 100.0*similarity\ndef export_features_to_df(dataset,base_named_ents,compare_named_ents,jaccard_sim,share_word_sim,cosine_sim,spacy_sim,links_sim,entities_sim,w2v_word_sim,w2v_text_sim):\n    df_base = dataset[:1]\n    ents_list = [base_ents for base_ents in base_named_ents]\n    df_base['named_entities'] = np.nan\n    df_base['named_entities'][0] = ents_list\n    df_base['jaccard_similarity'] = 100\n    df_base['share_word_similarity'] = 100\n    df_base['cosine_similarity'] = 100\n    df_base['spacy_similarity'] = 100\n    df_base['internal_links_similarity'] = 100\n    df_base['named_entities_similarity'] = 100\n    df_base['w2v_word_sim'] = 100\n    df_base['w2v_text_sim'] = 100\n    df_compare = dataset[1:]\n    ents_list = [compare_ents for compare_ents in compare_named_ents]\n    df_compare['named_entities'] = ents_list\n    df_compare['jaccard_similarity'] = jaccard_sim\n    df_compare['share_word_similarity'] = share_word_sim\n    df_compare['cosine_similarity'] = cosine_sim\n    df_compare['spacy_similarity'] = spacy_sim\n    df_compare['internal_links_similarity'] = links_sim\n    df_compare['named_entities_similarity'] = entities_sim\n    df_compare['w2v_word_sim'] = w2v_word_sim\n    df_compare['w2v_text_sim'] = w2v_text_sim\n    dataset = df_base.append(df_compare)\n    return dataset\n#def spacy_vector_similarity(sp,text1,text2):\n    #    vec1 = np.array([token.vector for token in sp(text1)])\n    #    vec1 = np.array([token.vector for token in sp(text2)])\n    #    similarity = cosine_similarity(vec1.reshape(-1, 1),vec2.reshape(-1, 1))\n    #    return similarity\n#\n\n\n##########################\n### IMPORTS\n##########################\n\npat1 = r'@[A-Za-z0-9]+'\npat2 = r'https?://[A-Za-z0-9./]+'\ncombined_pat = r'|'.join((pat1, pat2))\n\nfrom collections import Counter\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom gensim.models import doc2vec\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import TSNE\nfrom gensim.models import KeyedVectors\nimport gensim.downloader as api\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\nfrom numpy import dot\nfrom numpy.linalg import norm\nfrom os.path import isfile\nfrom scipy import spatial\nfrom scipy import stats\nfrom scipy.spatial import distance\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom spacy import displacy\nfrom string import punctuation\nfrom tqdm import tqdm\nfrom unidecode import unidecode\nimport codecs\nimport collections\nimport datetime\nimport itertools\nimport json\nimport keras\nimport math\nimport matplotlib\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport random\nimport re\nimport requests\nimport seaborn as sns\nimport spacy\nimport string\nimport sys\nimport time\nimport timeit\nimport unidecode\nimport warnings\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', 1000)\npd.set_option('display.max_rows', 500)\npd.set_option('display.width', 800)\nreddit_stopwords = ['edit','subreddit','moderator','edited','reddit','thread','post','comment','answer','question','mentioned','mention','content']\nstopwords_english = set(nltk.corpus.stopwords.words('english'))\nstopwords_spanish = set(nltk.corpus.stopwords.words('spanish'))\nwarnings.filterwarnings('ignore')\nwnlm = WordNetLemmatizer()\n","metadata":{},"execution_count":null,"outputs":[]}]}