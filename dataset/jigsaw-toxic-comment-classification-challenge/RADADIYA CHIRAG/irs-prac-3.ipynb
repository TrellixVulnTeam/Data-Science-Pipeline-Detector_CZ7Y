{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-12T07:57:21.669659Z","iopub.execute_input":"2022-03-12T07:57:21.669957Z","iopub.status.idle":"2022-03-12T07:57:21.678375Z","shell.execute_reply.started":"2022-03-12T07:57:21.669922Z","shell.execute_reply":"2022-03-12T07:57:21.677597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd ","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:57:21.679833Z","iopub.execute_input":"2022-03-12T07:57:21.680102Z","iopub.status.idle":"2022-03-12T07:57:21.692327Z","shell.execute_reply.started":"2022-03-12T07:57:21.680071Z","shell.execute_reply":"2022-03-12T07:57:21.691392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df  =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:57:42.218871Z","iopub.execute_input":"2022-03-12T07:57:42.219864Z","iopub.status.idle":"2022-03-12T07:57:46.651437Z","shell.execute_reply.started":"2022-03-12T07:57:42.219819Z","shell.execute_reply":"2022-03-12T07:57:46.650438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:58:01.038875Z","iopub.execute_input":"2022-03-12T07:58:01.039169Z","iopub.status.idle":"2022-03-12T07:58:01.062965Z","shell.execute_reply.started":"2022-03-12T07:58:01.039139Z","shell.execute_reply":"2022-03-12T07:58:01.062055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:58:16.678714Z","iopub.execute_input":"2022-03-12T07:58:16.679045Z","iopub.status.idle":"2022-03-12T07:58:16.689939Z","shell.execute_reply.started":"2022-03-12T07:58:16.67901Z","shell.execute_reply":"2022-03-12T07:58:16.689095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:58:35.11787Z","iopub.execute_input":"2022-03-12T07:58:35.118996Z","iopub.status.idle":"2022-03-12T07:58:35.19236Z","shell.execute_reply.started":"2022-03-12T07:58:35.118936Z","shell.execute_reply":"2022-03-12T07:58:35.191377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\n\ndef clean_review_text(text):\n    text = text.lower()  # covert the text to lowercase\n    text = re.sub('<.*?>','',text).strip() # remove html chars\n    text = re.sub('\\[|\\(.*\\]|\\)','', text).strip() # remove text in square brackets and parenthesis\n    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation marks\n    text = re.sub(\"(\\\\W)\",\" \",text).strip() # remove non-ascii chars\n    text = re.sub('\\S*\\d\\S*\\s*','', text).strip()  # remove words containing numbers\n    return text.strip()\n\ntrain_df.comment_text = train_df.comment_text.astype(str)\ntrain_df.comment_text = train_df.comment_text.apply(clean_review_text)\ntrain_df.comment_text.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:58:56.918543Z","iopub.execute_input":"2022-03-12T07:58:56.918862Z","iopub.status.idle":"2022-03-12T07:59:25.382848Z","shell.execute_reply.started":"2022-03-12T07:58:56.918828Z","shell.execute_reply":"2022-03-12T07:59:25.381727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import  matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nimport pickle\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score , accuracy_score , confusion_matrix , f1_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport en_core_web_sm\n\n\nnlp = en_core_web_sm.load()\nsnow_stemmer = SnowballStemmer(language='english')\nstopwords = nlp.Defaults.stop_words","metadata":{"execution":{"iopub.status.busy":"2022-03-12T08:05:49.12185Z","iopub.execute_input":"2022-03-12T08:05:49.122255Z","iopub.status.idle":"2022-03-12T08:05:49.953701Z","shell.execute_reply.started":"2022-03-12T08:05:49.12221Z","shell.execute_reply":"2022-03-12T08:05:49.952586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_stemmer(text):\n    words = text.split()\n    sent = [snow_stemmer.stem(word) for word in words if not word in set(stopwords)]\n    return ' '.join(sent)\n\ntrain_df.comment_text = train_df.comment_text.apply(apply_stemmer)\ntrain_df.comment_text.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:59:49.587584Z","iopub.execute_input":"2022-03-12T07:59:49.587872Z","iopub.status.idle":"2022-03-12T08:02:21.551828Z","shell.execute_reply.started":"2022-03-12T07:59:49.587842Z","shell.execute_reply":"2022-03-12T08:02:21.550821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.comment_text\ny = train_df.drop(['id','comment_text'],axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T08:05:55.857253Z","iopub.execute_input":"2022-03-12T08:05:55.857611Z","iopub.status.idle":"2022-03-12T08:05:55.871808Z","shell.execute_reply.started":"2022-03-12T08:05:55.85757Z","shell.execute_reply":"2022-03-12T08:05:55.870562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test =  train_test_split(X,y,test_size = 0.2,random_state = 45)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T08:06:01.089224Z","iopub.execute_input":"2022-03-12T08:06:01.089717Z","iopub.status.idle":"2022-03-12T08:06:01.136219Z","shell.execute_reply.started":"2022-03-12T08:06:01.089683Z","shell.execute_reply":"2022-03-12T08:06:01.135479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    strip_accents='unicode',     \n    analyzer='word',            \n    token_pattern=r'\\w{1,}',    \n    ngram_range=(1, 3),         \n    stop_words='english',\n    sublinear_tf=True)\n\nword_vectorizer.fit(x_train)    \ntrain_word_features = word_vectorizer.transform(x_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T08:06:05.428093Z","iopub.execute_input":"2022-03-12T08:06:05.42841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_transformed = word_vectorizer.transform(x_train)\nX_test_transformed = word_vectorizer.transform(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logistic_regression = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=100)\nclassifier_ovr_log = OneVsRestClassifier(logistic_regression)\nclassifier_ovr_log.fit(X_train_transformed, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T08:02:22.1209Z","iopub.status.idle":"2022-03-12T08:02:22.121855Z","shell.execute_reply.started":"2022-03-12T08:02:22.121611Z","shell.execute_reply":"2022-03-12T08:02:22.12164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred_proba = classifier_ovr_log.predict_proba(X_train_transformed)\ny_test_pred_proba = classifier_ovr_log.predict_proba(X_test_transformed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_predictions(df,classifier):\n    df.comment_text = df.comment_text.apply(clean_review_text)\n    df.comment_text = df.comment_text.apply(apply_stemmer)\n    X_test = df.comment_text\n    X_test_transformed = word_vectorizer.transform(X_test)\n    y_test_pred = classifier.predict_proba(X_test_transformed)\n    return y_test_pred\n\ny_pred=make_test_predictions(test_df,classifier_ovr_log)\ny_pred_df = pd.DataFrame(y_pred,columns=y.columns)\ny_pred_df","metadata":{"execution":{"iopub.status.busy":"2022-03-12T08:02:51.269476Z","iopub.execute_input":"2022-03-12T08:02:51.269803Z","iopub.status.idle":"2022-03-12T08:02:51.305631Z","shell.execute_reply.started":"2022-03-12T08:02:51.269765Z","shell.execute_reply":"2022-03-12T08:02:51.304447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.concat([test_df.id, y_pred_df], axis=1)\nsubmission_df.to_csv('submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]}]}