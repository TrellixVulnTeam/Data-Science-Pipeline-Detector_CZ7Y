{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Toxic Comment Classification Challenge\n## BERT - TensorFlow 2 & Hugging Face Transformers Library","execution_count":null},{"metadata":{"id":"SuY02mplBrDy","outputId":"7812630a-8a77-4cdf-b99b-892778ca3898","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_directory = '../input/jigsaw-toxic-comment-classification-challenge'","execution_count":null,"outputs":[]},{"metadata":{"id":"5lawOFYO_KUV","outputId":"282f1765-fc5a-4d88-e6f7-b68a843aba0a","trusted":true},"cell_type":"code","source":"!mkdir data\n!unzip {dataset_directory}/train.csv.zip -d data/\n!unzip {dataset_directory}/test.csv.zip  -d data/\n!unzip {dataset_directory}/test_labels.csv.zip  -d data/\n!unzip {dataset_directory}/sample_submission.csv.zip  -d data/","execution_count":null,"outputs":[]},{"metadata":{"id":"aGa2jmNmCjXf","trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Pipeline\n- Loading the datasets from CSVs\n- Preprocessing (Tokenization, Truncation & Padding)\n- Creating efficient data pipelines using tf.data","execution_count":null},{"metadata":{"id":"5FuPBMEX_Lxv","trusted":true},"cell_type":"code","source":"train_path = 'data/train.csv'\ntest_path = 'data/test.csv'\ntest_labels_path = 'data/test_labels.csv'\nsubm_path = 'data/sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"id":"50uZZZzH_NDV","outputId":"f925ad52-96d5-47a1-b573-db6fa12012ba","trusted":true},"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ndf_train = pd.read_csv(train_path)\ndf_test = pd.read_csv(test_path)\ndf_test_labels = pd.read_csv(test_labels_path)\ndf_test_labels = df_test_labels.set_index('id')\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"g-8WnPfD_35q","outputId":"ef31e72c-9dc5-4e7d-875e-875758466538","trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nbert_model_name = 'bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\nMAX_LEN = 128\n\ndef tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n\n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)\n\ninput_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nattention_masks = create_attention_masks(input_ids)","execution_count":null,"outputs":[]},{"metadata":{"id":"bMumeio9FFds","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nlabels =  df_train[label_cols].values\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n\ntrain_size = len(train_inputs)\nvalidation_size = len(validation_inputs)","execution_count":null,"outputs":[]},{"metadata":{"id":"H99t9YNDFcRq","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nNR_EPOCHS = 1\n\ndef create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n    if train:\n        dataset = dataset.shuffle(buffer_size=buffer_size)\n    dataset = dataset.repeat(epochs)\n    dataset = dataset.batch(batch_size)\n    if train:\n        dataset = dataset.prefetch(1)\n    \n    return dataset\n\ntrain_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\nvalidation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. BERT Model\n- Load the pretrained BERT base-model from Transformers library\n- Take the first hidden-state from BERT output (corresponding to CLS token) and feed it into a Dense layer with 6 neurons and sigmoid activation (Classifier). The outputs of this layer can be interpreted as probabilities for each of the 6 classes.","execution_count":null},{"metadata":{"id":"3Zp6X7meF_T5","trusted":true},"cell_type":"code","source":"from transformers import TFBertModel\nfrom tensorflow.keras.layers import Dense, Flatten\n\nclass BertClassifier(tf.keras.Model):    \n    def __init__(self, bert: TFBertModel, num_classes: int):\n        super().__init__()\n        self.bert = bert\n        self.classifier = Dense(num_classes, activation='sigmoid')\n        \n    @tf.function\n    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids,\n                               position_ids=position_ids,\n                               head_mask=head_mask)\n        cls_output = outputs[1]\n        cls_output = self.classifier(cls_output)\n                \n        return cls_output\n\nmodel = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Training Loop\n- Use BinaryCrossentropy as loss function (is calculated for each of the output 6 output neurons ...that's like training 6 binary classification tasks at the same time) \n- Use the AdamW optimizer with 1-cycle-policy from the Transformers library\n- AUC evaluation metrics","execution_count":null},{"metadata":{"id":"cJsVaX4qqoM0","outputId":"52dfd86b-a8f4-48f5-c481-a9db4f5c40fa","trusted":true},"cell_type":"code","source":"import time\nfrom transformers import create_optimizer\n\nsteps_per_epoch = train_size // BATCH_SIZE\nvalidation_steps = validation_size // BATCH_SIZE\n\n# | Loss Function\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\nvalidation_loss = tf.keras.metrics.Mean(name='test_loss')\n\n# | Optimizer (with 1-cycle-policy)\nwarmup_steps = steps_per_epoch // 3\ntotal_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\noptimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n\n# | Metrics\ntrain_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\nvalidation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n\n@tf.function\ndef train_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    with tf.GradientTape() as tape:\n        predictions = model(token_ids, attention_mask=masks)\n        loss = loss_object(labels, predictions)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n\n    train_loss(loss)\n\n    for i, auc in enumerate(train_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n        \n@tf.function\ndef validation_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    predictions = model(token_ids, attention_mask=masks, training=False)\n    v_loss = loss_object(labels, predictions)\n\n    validation_loss(v_loss)\n    for i, auc in enumerate(validation_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n                                              \ndef train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n    for epoch in range(epochs):\n        print('=' * 50, f\"EPOCH {epoch}\", '=' * 50)\n\n        start = time.time()\n\n        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n            train_step(model, token_ids, masks, labels)\n            if i % 1000 == 0:\n                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n                for i, label_name in enumerate(label_cols):\n                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n                    train_auc_metrics[i].reset_states()\n        \n        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n            validation_step(model, token_ids, masks, labels)\n\n        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n\n        for i, label_name in enumerate(label_cols):\n            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n            validation_auc_metrics[i].reset_states()\n\n        print('\\n')\n\n        \ntrain(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Run predictions on test-set & save submission","execution_count":null},{"metadata":{"id":"cdShHpPfH3es","outputId":"94b89833-244c-4cde-e6f6-637594ddef4b","trusted":true},"cell_type":"code","source":"test_input_ids = tokenize_sentences(df_test['comment_text'], tokenizer, MAX_LEN)\ntest_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntest_attention_masks = create_attention_masks(test_input_ids)","execution_count":null,"outputs":[]},{"metadata":{"id":"RUAKdp2EKYlZ","outputId":"29f3b4bc-2c0c-43cb-f873-1b0db86865d3","trusted":true},"cell_type":"code","source":"TEST_BATCH_SIZE = 32\ntest_steps = len(df_test) // TEST_BATCH_SIZE\n\ntest_dataset = create_dataset((test_input_ids, test_attention_masks), batch_size=TEST_BATCH_SIZE, train=False, epochs=1)\n\ndf_submission = pd.read_csv(subm_path, index_col='id')\n\nfor i, (token_ids, masks) in enumerate(tqdm(test_dataset, total=test_steps)):\n    sample_ids = df_test.iloc[i*TEST_BATCH_SIZE:(i+1)*TEST_BATCH_SIZE]['id']\n    predictions = model(token_ids, attention_mask=masks).numpy()\n\n    df_submission.loc[sample_ids, label_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"id":"JgAv8do3PYYX","trusted":true},"cell_type":"code","source":"df_submission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}