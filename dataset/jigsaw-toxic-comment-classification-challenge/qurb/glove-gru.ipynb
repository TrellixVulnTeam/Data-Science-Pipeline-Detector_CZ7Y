{"cells":[{"metadata":{"_cell_guid":"2f9b7a76-8625-443d-811f-8f49781aef81","_uuid":"598f965bc881cfe6605d92903b758778d400fa8b","trusted":true},"cell_type":"code","source":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input,LSTM, Embedding, Dropout, Activation, GRU, SpatialDropout1D, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional, CuDNNGRU, CuDNNLSTM\nfrom keras.layers import LeakyReLU,concatenate,GlobalAvgPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gensim.models.keyedvectors as word2vec\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"66a6b5fd-93f0-4f95-ad62-3253815059ba","collapsed":true,"_uuid":"729b0f0c2a02c678631b8c072d62ff46146a82ef","trusted":true},"cell_type":"code","source":"path = '../input/'\ncomp = 'jigsaw-toxic-comment-classification-challenge/'\nEMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\n# EMBEDDING_FILE = f'{path}fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nTRAIN_DATA_FILE=f'{path}{comp}train.csv.zip'\nTEST_DATA_FILE=f'{path}{comp}test.csv.zip'\nTEST_LABELS_FILE = f'{path}{comp}test_labels.csv.zip'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2807a0a5-2220-4af6-92d6-4a7100307de2","collapsed":true,"_uuid":"d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3","trusted":true},"cell_type":"code","source":"embed_size = 50 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 200 # max number of words in a comment to use","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv(TRAIN_DATA_FILE)\ntest = pd.read_csv(TEST_DATA_FILE)\ntest_l = pd.read_csv(TEST_LABELS_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().any(),test.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train.iloc[:,2:].sum()\n#plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"# per class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Multi-tag Check\nrowsums=train.iloc[:,2:].sum(axis=1)\nx=rowsums.value_counts()\n\n#plot\nplt.figure(figsize=(8,4))\nax = sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Multiple tags per comment\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('# of tags ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', text) # Replace ips\n    text = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ',text) # Isolate punctuation\n    text = re.sub(r'([\\;\\:\\|•«\\n])', ' ', text) # Remove some special characters\n    text = text.replace('&', ' and ') # Replace numbers and symbols with language\n    text = text.replace('@', ' at ')\n    text = text.replace('0', ' zero ')\n    text = text.replace('1', ' one ')\n    text = text.replace('2', ' two ')\n    text = text.replace('3', ' three ')\n    text = text.replace('4', ' four ')\n    text = text.replace('5', ' five ')\n    text = text.replace('6', ' six ')\n    text = text.replace('7', ' seven ')\n    text = text.replace('8', ' eight ')\n    text = text.replace('9', ' nine ')\n    text = text.strip(' ')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].map(lambda com : clean_text(com))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.comment_text.shape, test.comment_text.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ac2e165b-1f6e-4e69-8acf-5ad7674fafc3","collapsed":true,"_uuid":"8ab6dad952c65e9afcf16e43c4043179ef288780","trusted":true},"cell_type":"code","source":"list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79afc0e9-b5f0-42a2-9257-a72458e91dbb","collapsed":true,"_uuid":"c292c2830522bfe59d281ecac19f3a9415c07155","trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7d19392b-7750-4a1b-ac30-ed75b8a62d52","collapsed":true,"_uuid":"e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc","trusted":true},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n# embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d29d827-377d-4d2f-8582-4a92f9569719","_uuid":"96fc33012e7f07a2169a150c61574858d49a561b","trusted":true},"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# embeddings = pd.read_table(EMBEDDING_FILE, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n# emb_mean, emb_std = np.mean(embeddings.values), np.std(embeddings.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62acac54-0495-4a26-ab63-2520d05b3e19","_uuid":"574c91e270add444a7bc8175440274bdd83b7173","trusted":true,"collapsed":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f1aeec65-356e-4430-b99d-bb516ec90b09","_uuid":"237345510bd2e664b5c6983a698d80bac2732bc4"},"cell_type":"markdown","source":"Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."},{"metadata":{"_cell_guid":"0d4cb718-7f9a-4eab-acda-8f55b4712439","_uuid":"dc51af0bd046e1eccc29111a8e2d77bdf7c60d28","trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = SpatialDropout1D(0.2)(x)\nx = Bidirectional(CuDNNLSTM(50, return_sequences=True))(x)\nx = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n\navg_pool = GlobalAvgPool1D()(x)\nmax_pool = GlobalMaxPool1D()(x)\nconc = concatenate([avg_pool,max_pool])\nx = GlobalMaxPool1D()(x)\n\nx = Dense(50, activation='elu')(x)\nx = Dropout(0.25)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\n\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Dropout, Activation, LSTM, Bidirectional, GlobalMaxPool1D, Embedding,AveragePooling1D\nfrom keras import Input, Model\nfrom keras import layers, initializers, regularizers, constraints, optimizers\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#모델 저장\n\nfile_path = \"bi_lstm.best.hdf5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss',verbose=1, save_best_only=True, mode = 'min')\n\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\",patience=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_t.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 10\ncallbacks_list = [checkpoint,early]\nhistory = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"history.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,max(plt.ylim())])\nplt.title('Training and Validation Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"28ce30e3-0f21-48e5-af3c-7e5512c9fbdc","_uuid":"e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0","trusted":true,"collapsed":true},"cell_type":"code","source":"y_test = model.predict([X_te], batch_size=1024, verbose=1)\nsample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv.zip')\nsample_submission[list_classes] = y_test\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"617e974a-57ee-436e-8484-0fb362306db2","_uuid":"2b969bab77ab952ecd5abf2abe2596a0e23df251","trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"end\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"version":"3.6.4","name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":4}