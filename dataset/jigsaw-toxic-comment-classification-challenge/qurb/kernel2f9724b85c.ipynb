{"cells":[{"metadata":{"_cell_guid":"2f9b7a76-8625-443d-811f-8f49781aef81","_uuid":"598f965bc881cfe6605d92903b758778d400fa8b","trusted":true,"id":"NbGuadFDhpcr"},"cell_type":"code","source":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, Conv1D\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional, concatenate,GlobalAvgPool1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gensim.models.keyedvectors as word2vec\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/'\ncomp = 'jigsaw-toxic-comment-classification-challenge/'\n# EMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\n# EMBEDDING_FILE = f'{path}fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nTRAIN_DATA_FILE=f'{path}{comp}train.csv.zip'\nTEST_DATA_FILE=f'{path}{comp}test.csv.zip'\nTEST_LABELS_FILE = f'{path}{comp}test_labels.csv.zip'","execution_count":null,"outputs":[]},{"metadata":{"id":"V2tEEfAnkAQ0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(TRAIN_DATA_FILE)\ntest = pd.read_csv(TEST_DATA_FILE)\ntest_l = pd.read_csv(TEST_LABELS_FILE)\nsample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2807a0a5-2220-4af6-92d6-4a7100307de2","_uuid":"d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3","trusted":true,"id":"IR2yOHlLhpc0"},"cell_type":"code","source":"embed_size = 300 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 200 # max number of words in a comment to use","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"8wtbIgglhpc6","outputId":"c760b513-81dc-459b-90a3-58301ec9c39e"},"cell_type":"code","source":"train.isnull().any(),test.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"id":"9Zhtf2bgHIMC"},"cell_type":"markdown","source":"**Text Cleaning** "},{"metadata":{"trusted":true,"id":"cK2-FBHChpc9"},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', text) # Replace ips\n    text = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ',text) # Isolate punctuation\n    text = re.sub(r'([\\;\\:\\|•«\\n])', ' ', text) # Remove some special characters\n    text = text.replace('&', ' and ') # Replace numbers and symbols with language\n    text = text.replace('@', ' at ')\n    text = text.replace('0', ' zero ')\n    text = text.replace('1', ' one ')\n    text = text.replace('2', ' two ')\n    text = text.replace('3', ' three ')\n    text = text.replace('4', ' four ')\n    text = text.replace('5', ' five ')\n    text = text.replace('6', ' six ')\n    text = text.replace('7', ' seven ')\n    text = text.replace('8', ' eight ')\n    text = text.replace('9', ' nine ')\n    text = text.strip(' ')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"jk_1aiRQhpdB"},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].map(lambda com : clean_text(com))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2guLn_6JhpdE","outputId":"1dfd2d94-bb4c-4f17-acc9-d9c0a626cd59"},"cell_type":"code","source":"print(train.comment_text.shape, test.comment_text.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"EvEgq7A-kLXC","trusted":true},"cell_type":"code","source":"# to do\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_train = train[\"comment_text\"]\nlist_sentences_test = test[\"comment_text\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"NGqUhAOJHPWN","trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]},{"metadata":{"_cell_guid":"79afc0e9-b5f0-42a2-9257-a72458e91dbb","_uuid":"c292c2830522bfe59d281ecac19f3a9415c07155","trusted":true,"id":"odfmFt-MhpdJ"},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"id":"fqZVHzUyxrDG","trusted":true},"cell_type":"code","source":"comp = 'crawl300d2m/'\nembedding_path = f'{path}{comp}crawl-300d-2M.vec'","execution_count":null,"outputs":[]},{"metadata":{"id":"KtrrorQ-xqNo","trusted":true},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))","execution_count":null,"outputs":[]},{"metadata":{"id":"GLbTzcFh0E46","trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f8c4f6a3-3a19-40b1-ad31-6df2690bec8a","_uuid":"e1cb77629e35c2b5b28288b4d6048a86dda04d78","id":"LSrYmgyVhpdO"},"cell_type":"markdown","source":"Read the glove word vectors (space delimited strings) into a dictionary from word->vector."},{"metadata":{"_cell_guid":"7370416a-094a-4dc7-84fa-bdbf469f6579","_uuid":"20cea54904ac1eece20874e9346905a59a604985","id":"3ngMBJiehpdT"},"cell_type":"markdown","source":"Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."},{"metadata":{"trusted":true,"id":"yfbYSZX-hpdh","outputId":"7c72aba7-a4d0-4628-e405-a9a6d491cef0"},"cell_type":"code","source":"embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"id":"1R6uAcK3Hgq4"},"cell_type":"markdown","source":"## **Model - Bidirectional GRU**\n# with convolution layer"},{"metadata":{"id":"alJV1RU1xt3j","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, Activation, LSTM, Bidirectional, GlobalMaxPool1D, Embedding,AveragePooling1D\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras import layers, initializers, regularizers, constraints, optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nfrom tensorflow.compat.v1.keras.layers import CuDNNGRU, CuDNNLSTM","execution_count":null,"outputs":[]},{"metadata":{"id":"c5CevvtMkopR","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import LeakyReLU\nfrom keras.layers import GRU","execution_count":null,"outputs":[]},{"metadata":{"id":"xvHJeggVpq4y","trusted":true},"cell_type":"code","source":"x=0\nmodel = 0","execution_count":null,"outputs":[]},{"metadata":{"id":"ppqfD_B3W1FJ","outputId":"14f67d49-4944-49c6-c536-16fb0c0c861a","trusted":true},"cell_type":"code","source":"maxlen=200\n\ninp = Input(shape=(maxlen,)) # max_len = 100\n\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp) # max_features = 20000, embed_size = 50 / embedding_matrix ?\n\nx= SpatialDropout1D(0.2)(x)\n\nx = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n\nx = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n\navg_pool = GlobalAvgPool1D()(x)\nmax_pool = GlobalMaxPool1D()(x)\n\nconc = concatenate([avg_pool,max_pool])\n\nx = Dense(50)(conc)\nx = LeakyReLU(alpha = 0.01)(x)\n\nx = Dropout(0.25)(x)\n\nx = Dense(6, activation = \"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"ND1K4xk1yIRi","trusted":true},"cell_type":"code","source":"model = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"ojZJ81fmxdLn","trusted":true},"cell_type":"code","source":"#모델 저장\n\nfile_path = \"bi_gru.best.hdf5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss',verbose=1, save_best_only=True, mode = 'min')\n\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\",patience=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"7ysbhbMEY5qw","outputId":"5e797b7f-21c8-49d0-ce06-89fddaf7c6a8","trusted":true},"cell_type":"code","source":"X_t.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"vWBeg8UHxqTw","outputId":"8bea26e6-1560-4529-fdc8-414eb1737240","trusted":true},"cell_type":"code","source":"#NLP에서 epoch 수 많이 하면 overfitting의 문제 생길 가능성 높음\nbatch_size = 32\nepochs = 10 \ncallbacks_list = [checkpoint,early]\nhistory = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a624b55-3720-42bc-ad5a-7cefc76d83f6","_uuid":"e2a0e9ce12e1ff5ea102665e79de23df5caf5802","id":"cgMaaA53hpdq"},"cell_type":"markdown","source":"Now we're ready to fit out model! Use `validation_split` when not submitting."},{"metadata":{"_cell_guid":"333626f1-a838-4fea-af99-0c78f1ef5f5c","scrolled":false,"_uuid":"c1558c6b2802fc632edc4510c074555a590efbd8","trusted":true,"id":"yGGqLelkhpdr","outputId":"2cc0093c-f2a0-4b43-8638-d2f6105f2aa2"},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,max(plt.ylim())])\nplt.title('Training and Validation Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d6fa2ace-aa92-40cf-913f-a8f5d5a4b130","_uuid":"3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf","id":"TAcg-gTFhpdv"},"cell_type":"markdown","source":"And finally, get predictions for the test set and prepare a submission CSV:"},{"metadata":{"_cell_guid":"28ce30e3-0f21-48e5-af3c-7e5512c9fbdc","_uuid":"e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0","trusted":true,"id":"BHOQalkvhpdw","outputId":"f779a624-4363-4547-9ab4-d55c2c62a8db"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nbatch_size = 1024\nresults = model.predict(X_te, batch_size=batch_size,verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"617e974a-57ee-436e-8484-0fb362306db2","_uuid":"2b969bab77ab952ecd5abf2abe2596a0e23df251","trusted":true,"id":"qFzvlZi0hpdz"},"cell_type":"code","source":"sample_submission[list_classes] = results\nsample_submission.to_csv('fin.csv', index=False)\n\nfiles.download('fin.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"version":"3.6.4","name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python"},"colab":{"name":"Bi-GRU baseline_Fasttext + pool_conc + clean_text.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}