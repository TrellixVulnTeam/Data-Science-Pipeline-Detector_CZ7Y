{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n\n- toxic\n- severe_toxic\n- obscene\n- threat\n- insult\n- identity_hate\n\nYou must create a model which predicts a probability of each type of toxicity for each comment.\n\nFile descriptions\n\n- train.csv - the training set, contains comments with their binary labels\n- test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.\n- sample_submission.csv - a sample submission file in the correct format\n- test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntrain = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\nlabels = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\nsub = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categories "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ntargets = train[labels.drop('id',axis = 1).columns]\ntargets\nfig, axes = plt.subplots(figsize = (32,8))\nsns.countplot(targets.sum(axis =1 )) # axis = 1 adds everything along columns , axis = 0 adds everything along rows\naxes.set_xticklabels(targets.columns, rotation=40, ha=\"right\",fontsize = 30)    \n# b.set_xlabel(\"X Label\",fontsize=30)\n# b.set_ylabel(\"Y Label\",fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic model Predicting only Toxicity"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom sklearn import preprocessing\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import *\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, LSTM\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Input\nfrom keras.layers.merge import Concatenate\nfrom keras.utils import plot_model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def preprocess_text(sen):\n#     # removing punctuations and numbers\n#     sentence = re.sub('[^a-zA-Z]',' ',sen)\n#     # remove single character\n#     sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\",' ',sentence)\n#     # remove multiple spaces\n#     sentence = re.sub(r'\\s+',' ',sentence)\n    \n#     return sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x = []\n# sentences = list(train['comment_text'])\n# for i in sentences:\n#     x.append(preprocess_text(i))\n\n# test1 = []\n# sentences = list(test['comment_text'])\n# for i in sentences:\n#     test1.append(preprocess_text(i))\n    \n# y = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)\n\n# y_train = to_categorical(y_train)\n# y_test = to_categorical(y_test)\n\n\n# tokenizer = Tokenizer(num_words = 5000)\n# tokenizer.fit_on_texts(X_train)\n\n# X_train = tokenizer.texts_to_sequences(X_train)\n# X_test = tokenizer.texts_to_sequences(X_test)\n\n\n# test1 = tokenizer.texts_to_sequences(test1)\n\n\n\n# vocab_size = len(tokenizer.word_index) + 1\n# maxlen = 200\n\n# X_train = pad_sequences(X_train,padding = 'post',maxlen = maxlen)\n# X_test = pad_sequences(X_test,padding = 'post',maxlen = maxlen)\n\n# test1 = pad_sequences(test1,padding = 'post',maxlen = maxlen)\n\n\n\n\n# embeddings_dic = dict()\n# glove_file = open('../input/glove-word-embedding-twitter/glove.twitter.27B.50d.txt',encoding = \"utf8\")\n# for line in glove_file:\n#     records = line.split()\n#     word = records[0]\n#     vector_dimensions = asarray(records[1:],dtype = 'float32')\n#     embeddings_dic[word] = vector_dimensions\n    \n# glove_file.close()\n\n# embedding_matrix = zeros((vocab_size,50))\n# for word,index in tokenizer.word_index.items():\n#     embedding_vector = embeddings_dic.get(word)\n#     if embedding_vector is not None:\n#         embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# deep_inputs = Input(shape = (maxlen,))\n# embedding_layer = Embedding(vocab_size,50,weights = [embedding_matrix],trainable = False)(deep_inputs)\n# LSTM_Layer_1 = LSTM(128)(embedding_layer)\n# dense_layer_1 = Dense(2,activation = 'softmax')(LSTM_Layer_1)\n# dense_layer_2 = Dense(2,activation = 'softmax')(LSTM_Layer_1)\n# model1 = Model(inputs= deep_inputs,outputs = dense_layer_1)\n# model1.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['acc'])\n# print(model1.summary())\n\n# plot_model(model1,to_file = 'model_plot1.png',show_shapes = True,show_layer_names = True)\n# tox = model1.fit(X_train,y_train,batch_size = 128,epochs = 10,verbose = 1,validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_pred = model1.predict(test1)\n# submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip', index_col='id')\n# submission['prediction'] = [i[0] for i in test_pred]\n# submission.reset_index(drop=False, inplace=True)\n# submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Basic proper model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = []\nsentences = list(train['comment_text'])\nfor i in sentences:\n    x.append(preprocess_text(i))\n\ntest1 = []\nsentences = list(test['comment_text'])\nfor i in sentences:\n    test1.append(preprocess_text(i))\n    \ny = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom sklearn.feature_extraction import stop_words\n\nstop_words = stop_words.ENGLISH_STOP_WORDS\n\ndef clean(doc):\n    # Removal of punctuation marks (.,/\\][{} etc) and numbers\n    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n    # Removal of stopwords\n    doc = \" \".join([token for token in doc.split() if token not in stop_words])\n    return doc.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer(max_features = 5000,preprocessor = clean)\nX_train_dtm = vect.fit_transform(X_train)\nX_val_dtm = vect.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cols = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_dtm.shape, X_val_dtm.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nnav = MultiOutputClassifier(MultinomialNB()).fit(X_train_dtm,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\nsample = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\ndf_test = pd.merge(test,sample,on = 'id')\nx_test_dtm = vect.transform(df_test['comment_text'])\ny_preds = np.transpose(np.array(nav.predict_proba(x_test_dtm))[:,:,1])\ndf_test[y_cols] = y_preds\n\ndf_test.drop(['comment_text'],axis = 1,inplace = True)\n\ndf_test.to_csv('sample_submissiong.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}