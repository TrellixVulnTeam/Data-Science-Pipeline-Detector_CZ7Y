{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"df_train1 = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_train1[category] = df_train1[category] * cat_mtpl[category]\n\ndf_train1['score'] = df_train1.loc[:, 'toxic':'identity_hate'].mean(axis=1)\n\ndf_train1['y'] = df_train1['score']\n\nmin_len = (df_train1['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train1[df_train1['y'] == 0].sample(n=min_len*2, random_state=41)  # take non toxic comments\ndf_train1 = pd.concat([df_train1[df_train1['y'] > 0], df_y0_undersample])  # make new df\ndf_train1 = df_train1[['comment_text','y']].rename(columns={'comment_text':'text'})\n\nimport nltk\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndf_train1['text'] = df_train1['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndf_train1[\"text\"] = df_train1[\"text\"].apply(lambda text: lemmatize_words(text))\n\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nraw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\nraw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\nraw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n\nfrom datasets import Dataset\n\ndataset = Dataset.from_pandas(df_train1[['text']])\n\ndef get_training_corpus():\n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\n        \nraw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\nfrom transformers import PreTrainedTokenizerFast\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\n\ndef dummy_fun(doc):\n    return doc\ndef vec_train_valition_preds(df):\n    labels = df['y']\n    comments = df['text']\n    comments = [text.lower() for text in comments]\n    tokenized_comments = tokenizer(comments)['input_ids']\n    \n    vectorizer = TfidfVectorizer(\n    analyzer = 'word',\n    tokenizer = dummy_fun,\n    preprocessor = dummy_fun,\n    token_pattern = None)\n    comments_tr = vectorizer.fit_transform(tokenized_comments)\n    \n    model = Ridge(random_state=2021)\n    model.fit(comments_tr,labels)\n\n    less_toxic_comments = df_valid['less_toxic']\n    more_toxic_comments = df_valid['more_toxic']\n    \n    less_toxic_comments = tokenizer(less_toxic_comments.to_list())['input_ids']\n    more_toxic_comments = tokenizer(more_toxic_comments.to_list())['input_ids']\n    \n    less_toxic = vectorizer.transform(less_toxic_comments)\n    more_toxic = vectorizer.transform(more_toxic_comments)\n    \n    # make predictions\n    y_pred_less = model.predict(less_toxic)\n    y_pred_more = model.predict(more_toxic)\n\n    print((y_pred_less < y_pred_more).mean())\n    \n    texts = df_test['text']\n    texts = tokenizer(texts.to_list())['input_ids']\n    texts = vectorizer.transform(texts)\n    p = model.predict(texts)\n    return p\n\np = vec_train_valition_preds(df_train1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}