{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_colwidth', 400)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\nimport keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\nsubmission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data has the following features:  \n`id`, X: (`comment_text`) and Y: (`toxic`, `severe_toxic`, `obscene`, `threat`,`insult`, `identity_hate`) "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train['comment_text'].values\ny_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\nX_test = test['comment_text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_words = {word for sent in X_train for word in sent.split()}.union({word for sent in X_test for word in sent.split()})\nprint(f'number of unique words in the corpus {len(unique_words)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(map(lambda x: len(x.split()), X_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a lot of words! And some long comments.  \nLet's simplify the problem by truncating the number of words we use to `50,0000`. Keras Tokenizer automatically tokenizes elements based on their frequency, so it is sufficient to give `max_words` as a parameter  \nWe'll also specify the max length of our sequences to be `100`; Our embeddings will be `300` dimensional (fasttext.ai)"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = 40000\nmax_len = 100\nembedding_size = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nfor token, idx in zip(list(tokenizer.word_index.keys())[:5], list(tokenizer.word_index.values())[:5]):\n    print(f'tokenized {token} ------> {idx}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: because of parameter `num_words` any sentence that contains an unknown token will simply not be contained in the resulting sequence. for example,  \n`\"my name is far faraway asdasd\"` with `{'name': 1, 'is': 2, 'my': 3, 'far': 4, 'faraway': 5, 'asdasd': 6, 'your': 7}` and `num_words=4`\nwill be translated to `[3, 1, 2]`"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)\nfor text, tokenized in zip(X_train[:5], X_train_tokenized[:5]):\n    print(f'{text.split()[:5]}... ------> {tokenized[:5]}...')\nX_train = X_train_tokenized\nX_test = X_test_tokenized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('lengths of first five examples: ', list(map(lambda x: len(x), X_train[:5])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since Keras GRUs need Matrix inputs, we will need all the sentences to be padded or truncated to `max_len` we specified earlier.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_paddded = pad_sequences(X_train, maxlen=max_len, padding='post')\nX_test_paddded = pad_sequences(X_test, maxlen=max_len, padding='post')\nfor train_unpadded, train_padded in zip(X_train[:2], X_train_paddded[:2]):\n    print(f'{train_unpadded} ------> {len(train_unpadded)} values + {max_len - len(train_unpadded)} zeros')\nX_train = X_train_paddded\nX_test = X_test_paddded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we make the embedding matrix form our FastText's embeddings file.  \nTo do this we will get the `max_words` most frequent words into a set, and as we iterate through every entry in fasttext.ai's embeddings we will update the corresponding embedding table lookup entry."},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index # dictionary of word -> idx\ntokenized_words = set(list(word_index.keys())[:max_words]) # we only care about top `max_words` amount of words\nembedding_matrix = np.zeros((len(tokenized_words)+1, embedding_size)) # we add a +1 because '0' actually isn't an embedding that we will use. However, our lookup table will still need it\nfound = 0\nwith open(EMBEDDING_FILE) as f:\n    for line in f:\n        word, coord = line.split(' ', 1)\n        if word in tokenized_words:\n            embedding_matrix[word_index[word]] = np.asarray(coord.split(), dtype='float32')\n            found += 1\n            \nprint(f'found {found} of {len(tokenized_words)} words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we define a callback that will evaluate the roc-auc every epoch's end"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RocAucCallback(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n        \n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score - roc_auc_score(self.y_val, y_pred)\n            print(f'\\n ROC-AUC - epoch {epoch+1} - score: {score}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we padded our dataset with 0s, we need to override `GlobalAveragePooling1D` and `GlobalMeanPooling1D` classes to handle a mask. The mask will be described below.  \nWithout a mask, we would do the following:  \n\n| original      | max pool | average pool     | max pool with masking | average pool with masking |\n|---------------|----------|------------------|-----------------------|---------------------------|\n| [-1,-2,-3,0,0,0] | 0        | (-1-2-3)/6 = -0.66 | -1                     | (-1-2-3)/3 = -2             |"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n    def call(self, x, mask=None):\n        if mask != None:\n            # we basically only average over nonzero terms. \n            # Numerator does not change, but the denominator does\n            return K.sum(x, axis=1) / K.clip(K.tf.cast(K.tf.count_nonzero(x, axis=1), dtype=K.tf.float32), 1, K.int_shape(x)[1])\n        else:\n            return super().call(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No real change here, since max value won't be affected by masking.\n# There will be a bug if all the values of a row of x are negative, but I do not count on that \n# happening\nclass GlobalMaxPooling1DMasked(GlobalAveragePooling1D):\n    def call(self, x, mask=None):\n        if mask != None:\n            return K.max(x, axis=1)\n        else:\n            return super().call(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    inp = Input(shape=(max_len, ))\n    x = Embedding(max_words+1, embedding_size, weights=[embedding_matrix], mask_zero=True)(inp)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(GRU(80, return_sequences=True))(x)\n    avg_pool = GlobalAveragePooling1DMasked()(x)\n    max_pool = GlobalMaxPooling1DMasked()(x)\n    conc = concatenate([avg_pool, max_pool])\n    outp = Dense(6, activation=\"sigmoid\")(conc)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = get_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 4\n\nfile_path=\"weights_base.best.hdf5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n\n\ncallbacks_list = [checkpoint, early] #early\n\nX_tra, X_val, y_tra, y_val = train_test_split(X_train, y_train, train_size=0.95)\nRocAuc = RocAucCallback(validation_data=(X_val, y_val), interval=1)\n\nhist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n                 callbacks=callbacks_list)\n\nmodel.load_weights(file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test, batch_size=2048)\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}