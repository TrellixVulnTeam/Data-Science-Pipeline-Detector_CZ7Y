{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is explainable AI?\n\nWhenever we make any Machine learning or Neural Network Model we only judge a model by the prediction it makes.\n\nBut for most of the **non-linear model** we never know how our model has predicted Means we don't know what **features** contributed to our prediction.\n\nAnd if somehow we can extract those features for our prediction. We can answer very Impotant Question mostly asked in **Interviews**\n\n<font color='orange'>**Can you Explain this model like I am 5 ?**</font>\n\n![](https://i.pinimg.com/originals/55/5c/a3/555ca3f972ccb9faa88683a1e2c316eb.png)\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Why Explainable AI is Important?\n\n## <font  color ='orange'>1. Cross Validation can Fail</font>\n\nAlthough we can rely on cross validation for testing our  model before putting in production\n\nBut the problem with cross validation is  we never know what kind of testing data weâ€™ll get in \nproduction.\n\n## <font color='orange'> 2. A/B Testing  not the solution For  everyone </font>\n\nAlthough A/B testing is a  golden standard but the problem is\n\n2.1   Companies will have to expose Not the best quality product to  public testers.\n\n2.2   It can  be expensive as well.\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <font color='blue'> Why I am Stressing Upon It?</font>\n\n![](https://www.kdnuggets.com/images/xai-fig3-accuracy-vs-explainability-600.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## So for explainability purpose we will use\n\n## <font color='red'> ELI5 ---> Explain me Like i'm 5 ðŸ˜Š</font>\n\nFor refrence \n\n1. LIME  =Local Model Agnostic Explainability -----> https://arxiv.org/abs/1602.04938\n2. SHAPLEY = Shapley Additive Explainations   ---->https://arxiv.org/abs/1911.11888\n\n\n### <font color='orange'> If you like my work pls upvote my work</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  LET'S <font color='yellow'>LIME</font>\n![](https://miro.medium.com/max/818/1*Wwjnrq1G3e7_BCsN2BD06A.png)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  Let's Break It?\n\n## What is <font color='red'>LOCAL</font>?\n\n### Why did the model make a certain prediction for an instance?\n\n\nWe will zoom in on a single instance and examine what the model predicts for this input, and explain why. If you look at an individual prediction, the behavior of the otherwise complex model might behave more pleasantly. Locally, the prediction might only depend linearly or monotonically on some features, rather than having a complex dependence on them. For example, the value of a house may depend nonlinearly on its size. But if you are looking at only one particular 100 square meters house, there is a possibility that for that data subset, your model prediction depends linearly on the size. You can find this out by simulating how the predicted price changes when you increase or decrease the size by 10 square meters. Local explanations can therefore be more accurate than global explanations\n\n### What is <font color='red'>Model-Agnostic</font>?\n\nIt means any model till date can be applied to LIME and Lime can create local explanations for  them\nBy treating each model as <font color='orange'>Black box model</font>\nshould not make any assumptions about model while providing explanations.\n\n### Explanation is just an explanation ðŸ˜ŠðŸ˜Š\n\n\n##  TYPES OF INTERPRETABILITY\n\nThere are two main ways to look at a **classification** or a **regression** model:\n1. Inspect model parameters and try to figure out how the model works globally.\n\n2. Inspect an individual **prediction of a model** , try to figure out why the model makes the decision it makes.\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# What I wanted to Say?\n![](https://miro.medium.com/max/1124/1*vE3PUuhG6RRgK1J9oxg0nA.png)\n\nThe **black-box modelâ€™s** complex decision function f (unknown to LIME) is represented by the blue/pink background, which cannot be approximated well by a linear model. The bold red cross is the instance being explained. LIME samples instances, gets predictions using f, and weighs them by the proximity to the instance being explained (represented here by size). The dashed line is the learned explanation that is locally (but not globally) faithful.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What Makes <font color='yellow'>LIME</font> excellent ?\nLIME use a representation that is understood by the humans irrespective of the actual features used by the model. This is coined as interpretable representation. An interpretable representation would vary with the type of data that we are working with for example :\n1. For **text** : It represents presence/absence of words.\n2. For **image** : It represents presence/absence of super pixels ( contiguous patch of similar pixels ).\n3. For **tabular data** : It is a weighted combination of columns\n\n## LET's Do  SOME PRACTICAL WORK?","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import warnings\nimport gc,time\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest=pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\nsubmission=pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\ntest_labels=pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classes are imbalanced","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx=train.iloc[:,2:].sum()\nx#COLUMN wise sum\n\nfig = px.bar( x=x.index, y=x.values,\n             height=400)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color='blue'>Divided train and test set using stratified sampling</font>\n\n### We are working for a Binary Text Classification\n\nWe have used 20 percent of Clean and Toxic comments because from above we there is Huge class Imbalance bw clean and \n\nToxic comments. So to Handle that Imbalance we will over-sample Toxic comments but due to CPU limitations we have to use\n\n20 percent of data from other classes and then we will oversample Toxic comments class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_df=train.loc[(train.toxic==0) &  (train.severe_toxic==0) &(train.obscene==0) &\n                   (train.threat==0)  &(train.insult==0) &(train.identity_hate==0)] # clean  comments\n\ntoxic_df=train.loc[(train.toxic==1)]# toxic comments\n\n#creating test set\nclean_test=clean_df.iloc[:28669]# 20percent of  total clean comments which are  approximately 144000\ntoxic_test=toxic_df.iloc[:3059]# 20percent of Toxic  comments which are approximately 15000\n\ntest_set=clean_test.append(toxic_test,ignore_index=True).sample(frac=1)# appending 2 dataframes and shuffling them\ntest_set.drop(['id','severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)\nprint(test_set.shape)\n\n#creating train set\nclean_train=clean_df.iloc[28669:]\ntoxic_train=toxic_df.iloc[3059:]\ndf=clean_train.append(toxic_train,ignore_index=True).sample(frac=1)\n\n# df=clean_df.append(toxic_df,ignore_index=1).sample(frac=1)# appending 2 dataframes and shuffling them\ndf.drop(['id','severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)\ndf.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying a first round of text cleaning techniques\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n# Applying the cleaning function to both test and training datasets\ndf['comment_text'] = df['comment_text'].apply(lambda x: clean_text(x))\ntest_set['comment_text']=test_set['comment_text'].apply(lambda x:clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary_clean={0:'clean',1:'toxic'}\ndf['target_name']=df['toxic'].map(dictionary_clean)\ntest_set['target_name']=test_set['toxic'].map(dictionary_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import  SMOTE\n\n\nsmt = SMOTE(random_state=777, k_neighbors=1)\n\nvec = TfidfVectorizer(min_df=3,max_features=10000,strip_accents='unicode',\n                     analyzer='word',ngram_range=(1,2),token_pattern=r'\\w{1,}',use_idf=1,smooth_idf=1,sublinear_tf=1,\n                     stop_words='english')\n\nvec_fit=vec.fit_transform(df.comment_text)\n\nclf = LogisticRegressionCV()\n\n\n# Over Sampling\nX_SMOTE, y_SMOTE = smt.fit_sample(vec_fit, df.toxic)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n#we over sampled it \nprint(Counter(y_SMOTE))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=0.1, solver='sag')\nscores = cross_val_score(clf, X_SMOTE,y_SMOTE, cv=5,scoring='f1_weighted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X_SMOTE,y_SMOTE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ndef print_report1(df):\n    y_test =  df.toxic\n    test_features=vec.transform(df.comment_text)\n    y_pred = clf.predict(test_features)\n    report = metrics.classification_report(y_test, y_pred,\n        target_names=list(df.target_name.unique()))\n    print(report)\n    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))\n\nprint_report1(test_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### VERY IMPORTANT\nwhenever we use  Logistic regressionin  for  2 columns it treats it as Binary problem  hence we  get 1  table\n\nhttps://stackoverflow.com/questions/51659523/eli5-show-weights-with-two-labels","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Features Using Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\neli5.show_weights(clf, vec=vec, top=15,\n                  target_names=['clean','toxic'])\n\n#  if we got the BIAS term that occurs\n#because we are using Linear model for classification and the Intercept added to the equation is termed BIAS here\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explain me Results?\n\nSo above figure tells that green word contributed most to **Toxic** comments and **Red** words  contributed to opposite class that \n\nis **Clean comments** class","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Testing  time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_set.comment_text[0])\nprint('\\n')\nprint(test_set.toxic[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## and what Eli5 shows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\neli5.show_prediction(clf, test_set.comment_text[0], vec=vec,\n                     target_names=list(df.target_name.unique()),top=15)\n# it shows probability of each of  the 2 classes and then shows which features contributed the most and which\n# contributed the least in each class\n# top argument shows the  top n features that contibuted to the prediction of each class","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So THAT was it\n\n## <font color='red'>HOPE you liked my work. Please share and comment below ðŸ˜„</font>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}