{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project Description"},{"metadata":{},"cell_type":"markdown","source":"## Overall objective"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I want to use two state of the art Natural Language Processing (NLP) techniques which have sort of revolutionalized the area of NLP in Deep Learning.\n\nThese techniques are as follows:\n\n1. BERT (Deep Bidirectional Transformers for Language Understanding)\n2. Fastai ULMFiT (Universal Language Model Fine-tuning for Text Classification)\n\nBoth these techniques are very advanced and very recent NLP techniques (BERT was introduced by Google in 2018). Both of them incorporate the methods of Transfer Learning which is quite cool and are pre-trained on large corpuses of Wikipedia articles. I wanted to compare the overall performance of these two techniques.\n\nI really like using Fastai for my deep learning projects and can't thank enough for this amazing community and our mentors - Jeremy & Rachael for creating few wonderful courses on the matters pertaining to Deep Learning. Therefore one of my aims to work on this project was to **integrate BERT with Fastai**. This means power of BERT combined with the simplicity of Fastai. It was not an easy task especially implementing Discriminative Learning Rate technique of Fastai in BERT modelling. \n\nIn my project, below article helped me in understanding few of these integration techniques and I would like to extend my gratidue to the writer of this article:\n\n[https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/](http://)\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Data"},{"metadata":{},"cell_type":"markdown","source":"In this project, we will use Jigsaw's Toxic Comments dataset which has categorized each text item into 6 classes -\n\n1. Toxic\n2. Severe Toxic\n3. Obscene\n4. Threat\n5. Insult\n6. Identity Hate\n\nThis is a **multi-label text classification challenge**."},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries & Data Preparation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this section, we will import Fastai libraries and few other important libraries for our task"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pretrainedmodels\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n!pip install fastai==1.0.52\nimport fastai\n\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.text import *\n\nfrom torchvision.models import *\nimport pretrainedmodels\n\nfrom utils import *\nimport sys\n\nfrom fastai.callbacks.tracker import EarlyStoppingCallback\nfrom fastai.callbacks.tracker import SaveModelCallback","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's import Huggingface's \"pytorch-pretrained-bert\" model (this is now renamed as pytorch-transformers)\n\n[https://github.com/huggingface/pytorch-transformers](http://)\n\nThis is a brilliant repository of few of amazing NLP techniques and already pre-trained."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\npip install pytorch-pretrained-bert","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BERT has several flavours when it comes to Tokenization. For our modelling purposes, we will use the most common and standard method named as \"bert-case-uncased\".\n\nWe will name this as bert_tok"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert import BertTokenizer\nbert_tok = BertTokenizer.from_pretrained(\n    \"bert-base-uncased\",\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As mentioned in the article in first section, we will change the tokenizer of Fastai to incorporate BertTokenizer. One important thing to note here is to change the start and end of each token with [CLS] and [SEP] which is a requirement of BERT."},{"metadata":{"trusted":true},"cell_type":"code","source":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we move further, lets have a look at the Data on which we have to work.\n\nWe will split the train data into two parts: Train, Validation. However, for the purpose of this project, we will not be using Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT = Path(\"..\") / \"input\"\n\ntrain, test = [pd.read_csv(DATA_ROOT / fname) for fname in [\"train.csv\", \"test.csv\"]]\ntrain, val = train_test_split(train, shuffle=True, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In following code snippets, we need to wrap BERT vocab and BERT tokenizer with Fastai modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=256), pre_rules=[], post_rules=[])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can create our Databunch. Important thing to note here is to use BERT Tokenizer, BERT Vocab. And to and put include_bos and include_eos as False as Fastai puts some default values for these"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\ndatabunch_1 = TextDataBunch.from_df(\".\", train, val, \n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=\"comment_text\",\n                  label_cols=label_cols,\n                  bs=32,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Alternatively, we can pass our own list of Preprocessors to the databunch (this is effectively what is happening behind the scenes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass BertNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n\ndef get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for BERT\n    We remove sos/eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original BERT model.\n    \"\"\"\n    return [BertTokenizeProcessor(tokenizer=tokenizer),\n            NumericalizeProcessor(vocab=vocab)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertDataBunch(TextDataBunch):\n    @classmethod\n    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n        \"Create a `TextDataBunch` from DataFrames.\"\n        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n        # use our custom processors while taking tokenizer and vocab as kwargs\n        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n        return src.databunch(**kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this will produce a virtually identical databunch to the code above\ndatabunch_2 = BertDataBunch.from_df(\".\", train_df=train, valid_df=val,\n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  text_cols=\"comment_text\",\n                  label_cols=label_cols,\n                  bs=32,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path=Path('../input/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"databunch_2.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"databunch_1.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both Databunch_1 and Databunch_2 can be used for modelling purposes. In this project, we will be using Databunch_1 which is easier to create and use."},{"metadata":{},"cell_type":"markdown","source":"# BERT Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM\nbert_model_class = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loss function to be used is Binary Cross Entropy with Logistic Losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_func = nn.BCEWithLogitsLoss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering this is a multi-label classification problem, we cant use simple accuracy as metrics here. Instead, we will use accuracy_thresh with threshold of 25% as our metric here."},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_02 = partial(accuracy_thresh, thresh=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = bert_model_class","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets create learner function"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.callbacks import *\n\nlearner = Learner(\n    databunch_1, model,\n    loss_func=loss_func, model_dir='/temp/model', metrics=acc_02,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below code will help us in splitting the model into desirable parts which will be helpful for us in Discriminative Learning i.e. setting up different learning rates and weight decays for different parts of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_clas_split(self) -> List[nn.Module]:\n    \n    bert = model.bert\n    embedder = bert.embeddings\n    pooler = bert.pooler\n    encoder = bert.encoder\n    classifier = [model.dropout, model.classifier]\n    n = len(encoder.layer)//3\n    print(n)\n    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n    return groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = bert_clas_split(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's split the model now in 6 parts"},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.split([x[0], x[1], x[2], x[3], x[5]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.save('head')\nlearner.load('head')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will unfreeze last two last layers and train the model again"},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.freeze_to(-2)\nlearner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.save('head-2')\nlearner.load('head-2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now unfreeze the entire model and train it"},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.unfreeze()\nlearner.lr_find()\nlearner.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_one_cycle(2, slice(5e-6, 5e-5), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now see our model's prediction power"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = 'you are so sweet'\nlearner.predict(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = 'you are pathetic piece of shit'\nlearner.predict(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is awesome!\n\nWith few number of epochs, we are able to get the accuracy of around 98% on this multi-label classification task.\n\nNow, lets see how does Fastai ULMFiT fare on this task"},{"metadata":{},"cell_type":"markdown","source":"# Fastai - ULMFiT"},{"metadata":{},"cell_type":"markdown","source":"This will have two parts:\n\n1. Training the Language Model\n2. Training the Classifier Model"},{"metadata":{},"cell_type":"markdown","source":"## Language Model\n"},{"metadata":{},"cell_type":"markdown","source":"Important thing to remember in the Language Model is that we train it without label. The basic objective by training language model is to predict the next sentence / words in a sequence of text."},{"metadata":{"trusted":true},"cell_type":"code","source":"src_lm = ItemLists(path, TextList.from_df(train, path=\".\", cols = \"comment_text\"), \n                   TextList.from_df(val, path=\".\", cols = 'comment_text'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm = src_lm.label_for_lm().databunch(bs=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3, model_dir=\"/temp/model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(1, max_lr=slice(5e-4, 5e-3), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('fit_head')\nlearn.load('fit_head')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 1e-3), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4,  1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('fine-tuned')\nlearn.load('fine-tuned')\nlearn.save_encoder('fine-tuned')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT = \"He is a piece of\"\nN_WORDS = 10\nN_SENTENCES = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"src_clas = ItemLists(path, TextList.from_df( train, path=\".\", cols=\"comment_text\", vocab = data_lm.vocab),\n                    TextList.from_df( val, path=\".\", cols=\"comment_text\", vocab = data_lm.vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clas = src_clas.label_from_df(cols=label_cols).databunch(bs=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clas.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, model_dir='/temp/model', metrics=acc_02, loss_func=loss_func)\nlearn.load_encoder('fine-tuned')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(2, max_lr=slice(1e-3, 1e-2), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('first-head')\nlearn.load('first-head')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-2)\nlearn.fit_one_cycle(2, slice(5e-2/(2.6**4),5e-2), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('second')\nlearn.load('second')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-3)\nlearn.fit_one_cycle(2, slice(5e-2/(2.6**4),5e-2), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('third')\nlearn.load('third')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(2, slice(1e-4/(2.6**4),1e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.predict('she is so sweet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.predict('you are pathetic piece of shit')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}