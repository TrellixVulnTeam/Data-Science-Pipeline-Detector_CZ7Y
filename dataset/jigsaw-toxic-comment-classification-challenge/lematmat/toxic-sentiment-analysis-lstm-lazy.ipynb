{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -al /kaggle/input/jigsaw-toxic-comment-dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://deeplearningcourses.com/c/deep-learning-advanced-nlp\nfrom __future__ import print_function, division\nfrom builtins import range\n# Note: you may need to update your version of future\n# sudo pip install -U future\n\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, Input\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import roc_auc_score\n\nimport keras.backend as K\n#if len(K.tensorflow_backend._get_available_gpus()) > 0:\nfrom keras.layers import CuDNNLSTM as LSTM\nfrom keras.layers import CuDNNGRU as GRU\n\n\n# Download the data:\n# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n# Download the word vectors:\n# http://nlp.stanford.edu/data/glove.6B.zip\n\n\n# some configuration\nMAX_SEQUENCE_LENGTH = 100\nMAX_VOCAB_SIZE = 20000\nEMBEDDING_DIM = 50\nVALIDATION_SPLIT = 0.2\nBATCH_SIZE = 128\nEPOCHS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load Glove 6B pre-trained 50--D word vectors\nprint('Loading word vectors...')\nword2vec = {}\n#Open word vector text file\nwith open(os.path.join('/kaggle/input/glove-word-vector/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f: \n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vec = np.asarray(values[1:], dtype='float32')\n    word2vec[word] = vec\nprint('Found %s word vectors in Glove.' % len(word2vec))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare text samples and their labels\nprint('Loading in comments...')\n\n#train = pd.read_csv(\"../large_files/toxic-comment/train.csv\")\ntrain = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-dataset/train.csv')\ntest = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-dataset/test.csv\")\n\n#Get all sentences from train and test data\nsentences_train = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\nsentences_test = test[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\n\n# Get train sentences labels\npossible_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntrain_targets = train[possible_labels].values\n#test_targets = test[possible_labels].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the sentences (strings) into integers\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE) #Create tokenizers (to vectorized text corpora)\ntokenizer.fit_on_texts(sentences_train) #Updates internal vocabulary based on a list of texts (ie sentences_train)\nsentences_train = tokenizer.texts_to_sequences(sentences_train) # Transforms each text in sequences_train to a sequence of integers.\n\n#tokenizer.fit_on_texts(sentences_test)\nsentences_test = tokenizer.texts_to_sequences(sentences_test)\n\n\n# get word -> integer mapping number found by tokenizer in train sentences\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n\n# pad sequences so that we get a N x T matrix for all sequences as needed by keras\ndata_train = pad_sequences(sentences_train, maxlen=MAX_SEQUENCE_LENGTH)\ndata_test = pad_sequences(sentences_test, maxlen=MAX_SEQUENCE_LENGTH)\n\nprint('Shape of train tensor:', data_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prepare embedding matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare embedding matrix  by keeping Glove 6B word found in sentences_train and sentences_test\n# Result will be of (input_dim = MAX_VOCAB_SIZE, output_dim = EMBEDDING_DIM of glove 6B vectors)\nprint('Filling pre-trained embeddings...')\nnum_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx.items(): # word loop over unique words found by tokenizer\n  if i < MAX_VOCAB_SIZE: #  check that vocab size keep < MAX_VOCAB_SIZE\n    embedding_vector = word2vec.get(word) # return glove vector of word\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n      embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Embedding Layer**\n* Turn positive integers (indexes) into dense vectors of fixed size.\n* Especially usefull in NLP to transform word index in vectors\n* **input_dim** = How large is the vocabulary? How many categories are you encoding. This is the number of items in your \"lookup table\".\n* **output_dim** = How many numbers in the vector that you wish to return.\n* **input_length** = How many items are in the input feature vector that you need to transform?\n* **weights**: list of numpy arrays to set as initial weights of dim (input_dim, output_dim).\n* The embedding layer can only be used as the first layer in a model "},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed (as )\nembedding_layer = Embedding(\n  num_words,          # input_dim = How large is the vocabulary ? \n  EMBEDDING_DIM,      # output_dim = How many numbers in the vector that you wish to return ?\n  weights=[embedding_matrix], # weights: list of numpy arrays to set as initial weights of dim (input_dim, output_dim).\n  input_length=MAX_SEQUENCE_LENGTH, # input_length = How many items are in the input feature vector that you need to transform ?\n  trainable=False     \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Create bidirectionable LTSM\n"},{"metadata":{},"cell_type":"markdown","source":"**Create bidirectionable LTSM**\n\n- best LSTM model for NLP as it take into account full sequence (both forwardly and backwardly)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Building model...')\n\n# create an LSTM network with a single LSTM\ninput_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\nx = embedding_layer(input_)\n# x = LSTM(15, return_sequences=True)(x)\nx = Bidirectional(LSTM(15, return_sequences=True))(x)\n#x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\noutput = Dense(len(possible_labels), activation=\"sigmoid\")(x)\n\nmodel = Model(input_, output)\nmodel.compile(\n  loss='binary_crossentropy',\n  optimizer=Adam(lr=0.01),\n  metrics=['accuracy']\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print('Training model...')\nr = model.fit(\n  data_train,\n  train_targets,\n  batch_size=BATCH_SIZE,\n  epochs=EPOCHS,\n  validation_split=VALIDATION_SPLIT\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plot history loss  and accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot loss history \nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()\n\n# plot accuracies history\nplt.plot(r.history['accuracy'], label='accuracy')\nplt.plot(r.history['val_accuracy'], label='val_accuracy')\nplt.legend()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(data_test, batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-dataset/sample_submission.csv\")\ntest = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-dataset/test.csv\")\ntest_labels = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-dataset/test_labels.csv\")\ntrain = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-dataset/train.csv\")","execution_count":0,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\naucs = []\nfor j in range(6):\n    auc = roc_auc_score(targets[:,j], p[:,j])\n    aucs.append(auc)\nprint(np.mean(aucs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}