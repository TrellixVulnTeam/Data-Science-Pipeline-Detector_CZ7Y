{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport gensim.models.keyedvectors as word2vec\nimport gc\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\nembed_size=0\nprint(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## split to train and val\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"comment_text\"].fillna(\"_na_\").values\nval_X = val_df[\"comment_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"comment_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntrain_y = train_df[list_classes].values\nval_y = val_df[list_classes].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadEmbeddingMatrix(typeToLoad):\n        #load different embedding file from Kaggle depending on which embedding \n        #matrix we are going to experiment with\n        if(typeToLoad==\"glove\"):\n            EMBEDDING_FILE='../input/glove840b300dtxt/glove.840B.300d.txt'\n            embed_size = 25\n        elif(typeToLoad==\"word2vec\"):\n            word2vecDict = word2vec.KeyedVectors.load_word2vec_format(\"../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin\", binary=True)\n            embed_size = 300\n        elif(typeToLoad==\"fasttext\"):\n            EMBEDDING_FILE='../input/fasttext/wiki.simple.vec'\n            embed_size = 300\n\n        if(typeToLoad==\"glove\" or typeToLoad==\"fasttext\" ):\n            embeddings_index = dict()\n            #Transfer the embedding weights into a dictionary by iterating through every line of the file.\n            f = open(EMBEDDING_FILE)\n            for line in f:\n                #split up line into an indexed array\n                values = line.split()\n                #first index is word\n                word = values[0]\n                #store the rest of the values in the array as a new array\n                coefs = np.asarray(values[1:], dtype='float32')\n                embeddings_index[word] = coefs #50 dimensions\n            f.close()\n            print('Loaded %s word vectors.' % len(embeddings_index))\n        else:\n            embeddings_index = dict()\n            for word in word2vecDict.wv.vocab:\n                embeddings_index[word] = word2vecDict.word_vec(word)\n            print('Loaded %s word vectors.' % len(embeddings_index))\n            \n        gc.collect()\n        #We get the mean and standard deviation of the embedding weights so that we could maintain the \n        #same statistics for the rest of our own random generated weights. \n        all_embs = np.stack(list(embeddings_index.values()))\n        emb_mean,emb_std = all_embs.mean(), all_embs.std()\n        \n        nb_words = len(tokenizer.word_index)\n        #We are going to set the embedding size to the pretrained dimension as we are replicating it.\n        #the size will be Number of Words in Vocab X Embedding Size\n        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n        gc.collect()\n\n        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \n        #our own dictionary and loaded pretrained embedding. \n        embeddedCount = 0\n        for word, i in tokenizer.word_index.items():\n            i-=1\n            #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n            embedding_vector = embeddings_index.get(word)\n            #and store inside the embedding matrix that we will train later on.\n            if embedding_vector is not None: \n                embedding_matrix[i] = embedding_vector\n                embeddedCount+=1\n        print('total embedded:',embeddedCount,'common words')\n        \n        del(embeddings_index)\n        gc.collect()\n        \n        #finally, return the embedding matrix\n        return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = loadEmbeddingMatrix('word2vec')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen, )) #maxlen=200 By indicating an empty space after comma, we are telling Keras to infer the number automatically.\nx = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(train_X, train_y, batch_size=512, epochs=7, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = hist.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = hist.history['acc']\nval_acc = hist.history['val_acc']\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('word2vec.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}