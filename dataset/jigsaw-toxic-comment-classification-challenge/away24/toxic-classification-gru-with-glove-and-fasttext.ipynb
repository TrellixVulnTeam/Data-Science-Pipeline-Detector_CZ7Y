{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\n\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.callbacks import Callback\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences \n\nfrom textblob import TextBlob\nfrom textblob import Word\n\nimport re\nimport string\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unzip all local files\n!unzip -q \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/*.zip\"\n\n!dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_file = \"train.csv\"\ntest_data_file = \"test.csv\"\nsubmission_file = \"sample_submission.csv\"\n\ntrain_data = pd.read_csv(train_data_file)\ntest_data = pd.read_csv(test_data_file)\nsubmission_result = pd.read_csv(submission_file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing "},{"metadata":{"trusted":true},"cell_type":"code","source":"# set up paramters\nmax_len = 120 \nembedding_dim = 300\nvocabulary_size = 20000 #35000\nnum_tokens = vocabulary_size+1 #including 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocess comment texts\ndef preprocess(corpus):\n    \n# remove all non-English characters\n# and convert all letters to lower case\n    printable = set(string.printable)\n    corpus = ''.join(filter(lambda x: x in printable, corpus))    \n    corpus = corpus.lower()\n\n# change contracted words into possible non-contracted form\n    # specific\n    corpus = re.sub(r\"won't\", \"will not\", corpus)\n    corpus = re.sub(r\"can\\'t\", \"can not\", corpus)\n    # or could be 'are not' etc \n    corpus = re.sub(r\"ain\\'t\",\"is not\", corpus)\n    corpus = re.sub(r\"shan\\'t\", \"shall not\", corpus)\n    corputs = re.sub(r\"let\\'s\", \"let us\", corpus)\n\n    # general\n    corpus = re.sub(r\"n\\'t\", \" not\", corpus)\n    corpus = re.sub(r\"\\'re\", \" are\", corpus)\n    corpus = re.sub(r\"\\'s\", \" is\", corpus)\n    # or could be \\'d --> had \n    corpus = re.sub(r\"\\'d\", \" would\", corpus)\n    corpus = re.sub(r\"\\'ll\", \" will\", corpus)\n    corpus = re.sub(r\"\\'t\", \" not\", corpus)\n    corpus = re.sub(r\"\\'ve\", \" have\", corpus)\n    corpus = re.sub(r\"\\'m\", \" am\", corpus)\n    \n    # replace the rest \\' with ' '\n    corpus = re.sub(r\"\\'\", \" \", corpus)\n\n    correction_list = {\"youfuck\": \"you fuck\", \\\n                       \"fucksex\": \"fuck sex\",\\\n                       \"bitchbot\": \"bitch bot\",\\\n                       \"offfuck\": \"fuck off\",\\\n                       \"donkeysex\": \"donkey sex\",\\\n                      \"securityfuck\": \"security fuck\",\\\n                      \"ancestryfuck\": \"ancestry fuck\",\\\n                      \"turkeyfuck\": \"turkey fuck\",\\\n                      \"faggotgay\": \"faggot gay\",\\\n                       \"fuckbot\": \"fuck bot\",\\\n                       \"assfuckers\": \"ass fucker\",\\\n                       \"ckckck\": \"cock\",\\\n                       \"fuckfuck\": \"fuck\",\\\n                       \"lolol\": \"lol\",\\\n                       \"pussyfuck\": \"fuck\",\\\n                        \"gaygay\": \"gay\",\\\n                       \"haha\": \"ha\",\\\n                       \"sucksuck\": \"suck\"\n                      }\n    for old,new in correction_list.items():\n        corpus = corpus.replace(old,new)\n        \n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizer vocabulary_size words\n# we ignore all numbers\ntokenizer = Tokenizer(num_words = vocabulary_size+1,\\\n                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789',\\\n                      lower=True, split=' ')\n\n# extract comment texts from train_data and test_data\nX_train_raw = train_data[\"comment_text\"]\nX_test_raw = test_data[\"comment_text\"]\n\nbad_comment_cat = ['toxic', 'severe_toxic', 'obscene', 'threat',\\\n       'insult', 'identity_hate']\nY_train = train_data[bad_comment_cat]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocess data\nX_train_raw = X_train_raw.apply(lambda x: preprocess(str(x)))\nX_test_raw = X_test_raw.apply(lambda x: preprocess(str(x)))\n# example X_train_raw.loc[126]\n\n# tokenize comment text\ntokenizer.fit_on_texts(X_train_raw)\ntokenizer.fit_on_texts(X_test_raw)\n\nX_train = pad_sequences(tokenizer.texts_to_sequences(X_train_raw),\\\n                        maxlen = max_len, truncating = \"pre\")\nX_test = pad_sequences(tokenizer.texts_to_sequences(X_test_raw),\\\n                       maxlen = max_len, truncating = \"pre\")\n\n# shuffle training data and split it into a training part and a validation part\nx_train, x_val, y_train, y_val = train_test_split(X_train,Y_train,train_size=0.9, random_state=199)\nprint(x_train.shape,\" \",y_train.shape,\" \",x_val.shape,\" \",y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RNN models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_weights(embedding_vectors,embedding_dim):\n    global num_tokens,tokenizer\n    \n    # assign vectors to words using the pretrained model embedding_vectors\n    embedding_weights = np.zeros((num_tokens,embedding_dim))\n\n    # count how many words are not assigned with the pretrained model. \n    # By default, vectors associated to words are zero vectors.\n    misses = 0 \n\n    # the index in word_index starts with 1\n    for word, i in tokenizer.word_index.items():\n        vector = embedding_vectors.get(word)\n        # the word_index is ordered by word frequency\n        if i>=num_tokens :\n            break\n        elif vector is not None:\n            embedding_weights[i] = vector\n        else:\n            if len(word)<20:\n                word = Word(word)\n                word = word.spellcheck()[0][0]\n                vector = embedding_vectors.get(str(word))\n                if vector is not None:\n                    embedding_weights[i] = vector\n                else:\n                    misses +=1\n                    #print(word)\n            else:\n                misses +=1\n                #print(word)\n        \n    print(f\"The number of missed words is {misses}\")\n    \n    return embedding_weights ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the pretrained model fastText \nembedding_vectors_fasttext = {}\nwith open(\"/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\",\"r\") as file:\n    file.readline()\n    for line in file:\n        word , vector = line.split(maxsplit=1)\n        vector = np.fromstring(vector,\"float32\",sep=\" \")\n        embedding_vectors_fasttext[word] = vector        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign vectors to words using the pretrained model fasttext\nembedding_weights_fasttext = get_weights(embedding_vectors_fasttext,embedding_dim=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the pretrained model GloVe \nembedding_vectors_glove = {}\nwith open(\"/kaggle/input/glove6b/glove.6B.300d.txt\",\"r\") as file:\n    for line in file:\n        word , vector = line.split(maxsplit=1)\n        vector = np.fromstring(vector,\"float32\",sep=\" \")\n        embedding_vectors_glove[word] = vector        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign vectors to words using the pretrained model GloVe\nembedding_weights_glove = get_weights(embedding_vectors_glove,embedding_dim=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def GRU_model_glove():\n    global max_len,num_tokens,embedding_weights_glove\n    \n    inputs = layers.Input(shape=(max_len,))\n    \n    x = layers.Embedding(input_dim=num_tokens,\\\n                         output_dim=embedding_dim,\\\n                         embeddings_initializer=keras.initializers.Constant(embedding_weights_glove),\\\n                         trainable=True)(inputs)\n    \n    x = layers.SpatialDropout1D(0.3)(x)\n    \n    forward_layer = layers.GRU(42,return_sequences=True)\n    backward_layer = layers.GRU(42,activation=\"relu\",dropout=0.1,return_sequences=True,go_backwards=True)\n    x = layers.Bidirectional(forward_layer,backward_layer=backward_layer)(x)\n\n    x = layers.GlobalMaxPooling1D()(x)\n    \n    outputs = layers.Dense(units=6,activation='sigmoid')(x)\n    \n    model = keras.models.Model(inputs=inputs, outputs=outputs, name=\"GRU_model_glove\")\n    \n    model.compile(optimizer=tf.optimizers.Adam(),\\\n                  loss=tf.losses.BinaryCrossentropy(),\\\n                  metrics=['AUC'])\n    \n    return model\n\nGRU_model_glove = GRU_model_glove()\nGRU_model_glove.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = GRU_model_glove.fit(x_train, y_train, epochs=2,\\\n                              batch_size=128, validation_data=(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def GRU_model_fasttext():\n    global max_len,num_tokens,embedding_weights_fasttext\n    \n    inputs = layers.Input(shape=(max_len,))\n    \n    x = layers.Embedding(input_dim=num_tokens,\\\n                         output_dim=embedding_dim,\\\n                         embeddings_initializer=keras.initializers.Constant(embedding_weights_fasttext),\\\n                         trainable=False)(inputs)\n    \n    x = layers.SpatialDropout1D(0.3)(x)\n    \n    forward_layer = layers.GRU(64,return_sequences=True)\n    backward_layer = layers.GRU(64,activation=\"relu\",dropout=0.3,return_sequences=True,go_backwards=True)\n    x = layers.Bidirectional(forward_layer,backward_layer=backward_layer)(x)\n    \n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool,max_pool])\n    \n    outputs = layers.Dense(units=6,activation='sigmoid')(x)\n    \n    model = keras.models.Model(inputs=inputs, outputs=outputs, name=\"GRU_model\")\n    \n    model.compile(optimizer=tf.optimizers.Adam(),\\\n                  loss=tf.losses.BinaryCrossentropy(),\\\n                  metrics=['AUC'])\n    \n    return model\n\nGRU_model_fasttext = GRU_model_fasttext()\nGRU_model_fasttext.summary()\nhistory = GRU_model_fasttext.fit(x_train, y_train, \\\n                                 epochs=2, batch_size=32,\\\n                                 validation_data=(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_nums = 2\nsize1 = x_train.shape[0]\n\ny_train_pred = np.zeros((model_nums,size1,6),dtype=\"float32\")\ny_train_pred[0] = GRU_model_fasttext.predict(x_train)\ny_train_pred[1] = GRU_model_glove.predict(x_train)\n\nsize2 = X_test.shape[0]\ny_test_pred = np.zeros((model_nums,size2,6),dtype=\"float32\")\ny_test_pred[0] = GRU_model_fasttext.predict(X_test)\ny_test_pred[1] = GRU_model_glove.predict(X_test)\n\ny_pred = np.zeros((size2,6),dtype=\"float32\")\n\nfor i in range(6):\n    lg = LogisticRegression()\n    temp = np.zeros((size1,model_nums),dtype=\"float32\")\n    for j in range(model_nums):\n        temp[:,j] = y_train_pred[j,:,i]\n    lg.fit(temp,y_train[bad_comment_cat[i]])\n\n    temp = np.zeros((size2,model_nums),dtype=\"float32\")\n    for j in range(model_nums):\n        temp[:,j] = y_test_pred[j,:,i]\n    y_pred[:,i] = lg.predict_proba(temp)[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_result[bad_comment_cat] = y_pred\nsubmission_result.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Previous Models"},{"metadata":{},"cell_type":"markdown","source":"The idea is not implemented\n```\n# Not all reviews are English. \n# It is better to translate all of them into English in order to make the full use of data.\n!pip install --quiet google_trans_new\nfrom google_trans_new import google_translator  \ntranslator = google_translator()  \n#translator.translate(X_train_raw[i],lang_tgt=\"en\")\n```"},{"metadata":{},"cell_type":"markdown","source":"## An RNN model with pretrained word2vec and LSTM"},{"metadata":{},"cell_type":"markdown","source":"```\nimport spacy \nnlp = spacy.load('en_core_web_lg')\nnlp.meta['vectors']\n\n# Transform every comment into a vector of length 300.\n# Notice that in the following commands, spaCy first transforms every word in a comment\n# into a vector of length 300, then takes average of them to form the vector of \n# the comment. There could be other ways to form vectors of sentences, like TF-IDF.\n\nx_train_spacy = np.zeros(shape=(len(train_data.index),300,1),dtype=np.float32)\nfor i in train_data.index:\n    x_train_spacy[i]= np.reshape(nlp(train_data.loc[i,'comment_text']).vector,(300,1))\n\n# Define the model (architecture)\ndef model_spacy():\n    initializer = keras.initializers.HeNormal()\n    \n    model = keras.models.Sequential(name='with_pretraining_word2vec')\n    model.add(keras.Input(shape=(300,1),name='input'))\n    model.add(layers.LSTM(units=128, dropout=0.2,recurrent_regularizer=keras.regularizers.L2(0.1),\\\n                                return_sequences=True, kernel_initializer=initializer, name='layer_1'))\n    #recurrent_dropout=0.2\n    model.add(layers.Dense(units=1,name='layer_2'))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(units=6,activation='sigmoid',kernel_initializer=initializer,name='output')) \n    \n    model.compile(optimizer=tf.optimizers.Adam(),loss=tf.losses.BinaryCrossentropy(),metrics=['AUC'])\n    \n    return model_spacy\n    \nmodel_spacy = model_spacy()\nmodel_spacy.summary()\n\nmodel_spacy.fit(x_train_spacy,y_train,epochs=12,batch_size=512)\n\nx_test_spacy = np.zeros(shape=(len(test_data.index),300,1),dtype=np.float32)\nfor i in test_data_raw.index:\n    x_test_spacy[i] = np.reshape(nlp(test_data.loc[i,'comment_text']).vector,(300,1))\n    \nsubmission_result[bad_comment_cat] = model_spacy.predict(x_test_spacy)\nsubmission_result.info()\nsubmission_result.to_csv(\"submission.csv\",index=False)\n```"},{"metadata":{},"cell_type":"markdown","source":"## A Capsule model with fastText"},{"metadata":{},"cell_type":"markdown","source":"\n```\n#from https://github.com/bojone/Capsule/blob/master/Capsule_Keras.py\n#! -*- coding: utf-8 -*-\n# refer: https://kexue.fm/archives/5112\n\nfrom keras import activations\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\n\ndef squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    return scale * x\n\n\n#define our own softmax function instead of K.softmax\ndef softmax(x, axis=-1):\n    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n    return ex/K.sum(ex, axis=axis, keepdims=True)\n\n\n#A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, share_weights=True, activation='squash', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.share_weights = share_weights\n        if activation == 'squash':\n            self.activation = squash\n        else:\n            self.activation = activations.get(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        #final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:,:,:,0]) #shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            c = softmax(b, 1)\n            # o = K.batch_dot(c, u_hat_vecs, [2, 2])\n            o = tf.einsum('bin,binj->bij', c, u_hat_vecs)\n            if K.backend() == 'theano':\n                o = K.sum(o, axis=1)\n            if i < self.routings - 1:\n                o = K.l2_normalize(o, -1)\n                # b = K.batch_dot(o, u_hat_vecs, [2, 3])\n                b = tf.einsum('bij,binj->bin', o, u_hat_vecs)\n                if K.backend() == 'theano':\n                    b = K.sum(b, axis=1)\n\n        return self.activation(o)\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)  \n```\n\n```\ndef squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n```\n\n```\ndef capsule_model_fasttext():\n    global max_len,num_tokens,embedding_weights_fasttext\n    \n    inputs = layers.Input(shape=(max_len,))\n    \n    x = layers.Embedding(input_dim=num_tokens,\\\n                         output_dim=embedding_dim,\\\n                         embeddings_initializer=keras.initializers.Constant(embedding_weights_fasttext),\\\n                         trainable=False)(inputs)\n    \n    x = layers.SpatialDropout1D(0.3)(x)\n    \n    forward_layer = layers.GRU(64,dropout=0.2,return_sequences=True)\n    backward_layer = layers.GRU(64,dropout=0.3,return_sequences=True,go_backwards=True)\n    x = layers.Bidirectional(forward_layer,backward_layer=backward_layer)(x)\n    \n    x = Capsule(num_capsule=8, dim_capsule=16, routings=6,\n                      share_weights=True)(x)\n    \n    x = layers.Flatten()(x)\n    x = layers.Dropout(0.2)(x)\n    \n    outputs = layers.Dense(units=6,activation='sigmoid')(x)\n    \n    model = keras.models.Model(inputs=inputs, outputs=outputs, name=\"capsule_model_fasttext\")\n    \n    model.compile(optimizer=tf.optimizers.Adam(),\\\n                  loss=tf.losses.BinaryCrossentropy(),\\\n                  metrics=['AUC'])\n    \n    return model\n```\n\n```\ncapsule_model_fasttext = capsule_model_fasttext()\ncapsule_model_fasttext.summary()\n```\n```\nhistory = capsule_model_fasttext.fit(x_train, y_train, \\\n                                 epochs=4, batch_size=64,\\\n                                 validation_data=(x_val,y_val))\n```\n "},{"metadata":{},"cell_type":"markdown","source":"## BiLSTM+BiGRU+GloVe"},{"metadata":{},"cell_type":"markdown","source":"\n```\ndef GRU_model_glove():\n    global max_len,num_tokens,embedding_weights_glove\n    \n    inputs = layers.Input(shape=(max_len,))\n    \n    x = layers.Embedding(input_dim=num_tokens,\\\n                         output_dim=embedding_dim,\\\n                         embeddings_initializer=keras.initializers.Constant(embedding_weights_glove),\\\n                         trainable=False)(inputs)\n    \n    x = layers.SpatialDropout1D(0.3)(x)\n    x = layers.LSTM(32,return_sequences=True)(x)\n    x = layers.Bidirectional(layers.GRU(40,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))(x)\n    \n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool,max_pool])  \n    \n    outputs = layers.Dense(units=6,activation='sigmoid')(x)\n    \n    model = keras.models.Model(inputs=inputs, outputs=outputs, name=\"GRU_model_glove\")\n    \n    model.compile(optimizer=tf.optimizers.Adam(),\\\n                  loss=tf.losses.BinaryCrossentropy(),\\\n                  metrics=['AUC'])\n    \n    return model\n```\n```\nGRU_model_glove = GRU_model_glove()\nGRU_model_glove.summary()\nhistory = GRU_model_glove.fit(x_train, y_train, \\\n                               epochs=6, batch_size=128,\\\n                               validation_data=(x_val,y_val)\\\n                   )\n```"},{"metadata":{},"cell_type":"markdown","source":"## BiGRU+fastText"},{"metadata":{},"cell_type":"markdown","source":"```\ndef GRU_model_fasttext():\n    global max_len,num_tokens,embedding_weights_fasttext\n    \n    inputs = layers.Input(shape=(max_len,))\n    \n    x = layers.Embedding(input_dim=num_tokens,\\\n                         output_dim=embedding_dim,\\\n                         embeddings_initializer=keras.initializers.Constant(embedding_weights_fasttext),\\\n                         trainable=False)(inputs)\n    \n    x = layers.SpatialDropout1D(0.3)(x)\n    \n    forward_layer = layers.GRU(64,return_sequences=True)\n    backward_layer = layers.GRU(64,dropout=0.3,return_sequences=True,go_backwards=True)\n    x = layers.Bidirectional(forward_layer,backward_layer=backward_layer)(x)\n    \n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool,max_pool])\n    \n    outputs = layers.Dense(units=6,activation='sigmoid')(x)\n    \n    model = keras.models.Model(inputs=inputs, outputs=outputs, name=\"GRU_model_fasttext\")\n    \n    model.compile(optimizer=tf.optimizers.Adam(),\\\n                  loss=tf.losses.BinaryCrossentropy(),\\\n                  metrics=['AUC'])\n    \n    return model\n```\n\n```\nGRU_model_fasttext = GRU_model_fasttext()\nGRU_model_fasttext.summary()\nhistory = GRU_model_fasttext.fit(x_train, y_train, \\\n                                 epochs=4, batch_size=128,\\\n                                 validation_data=(x_val,y_val))\n```\n"},{"metadata":{},"cell_type":"markdown","source":"## Ensemble"},{"metadata":{},"cell_type":"markdown","source":"```\nmodel_nums = 3\nsize1 = x_train.shape[0]\n\ny_train_pred = np.zeros((model_nums,size1,6),dtype=\"float32\")\ny_train_pred[0] = GRU_model_fasttext.predict(x_train)\ny_train_pred[1] = GRU_model_glove.predict(x_train)\ny_train_pred[2] = capsule_model_fasttext(x_train)\n\nsize2 = X_test.shape[0]\ny_test_pred = np.zeros((model_nums,size2,6),dtype=\"float32\")\ny_test_pred[0] = GRU_model_fasttext.predict(X_test)\ny_test_pred[1] = GRU_model_glove.predict(X_test)\ny_test_pred[2] = capsule_model_fasttext(X_test)\n\ny_pred = np.zeros((size2,6),dtype=\"float32\")\n\nfor i in range(6):\n    lg = LogisticRegression()\n    temp = np.zeros((size1,model_nums),dtype=\"float32\")\n    for j in range(model_nums):\n        temp[:,j] = y_train_pred[j,:,i]\n    lg.fit(temp,y_train[bad_comment_cat[i]])\n    \n    temp = np.zeros((size2,model_nums),dtype=\"float32\")\n    for j in range(model_nums):\n        temp[:,j] = y_test_pred[j,:,i]\n    y_pred[:,i] = lg.predict_proba(temp)[:,1]\n```"},{"metadata":{},"cell_type":"markdown","source":"Other ensemble models (from earlier version)\n```\nensemble_weight = 0.7\ny_pred_fasttext = GRU_model_fasttext.predict(X_test)\ny_pred_glove = GRU_model_glove.predict(X_test)\ny_pred = y_pred_fasttext*ensemble_weight + y_pred_glove*(1-ensemble_weight)\n```"},{"metadata":{},"cell_type":"markdown","source":"combine two pretrained embedding models\n```\nembedding_weights = np.concatenate((embedding_weights_fasttext,embedding_weights_glove),axis=1)\n\ndef model():\n    global max_len,num_tokens,embedding_weights\n    \n    inputs = layers.Input(shape=(max_len,))\n    \n    x = layers.Embedding(input_dim=num_tokens,\\\n                         output_dim=embedding_weights.shape[1],\\\n                         embeddings_initializer=keras.initializers.Constant(embedding_weights),\\\n                         trainable=False)(inputs)\n    \n    x = layers.SpatialDropout1D(0.4)(x)\n    \n    x = layers.Bidirectional(layers.GRU(128,dropout=0.2,return_sequences=True))(x)\n    # layers.CuDNNGRU\n    \n    #forward_layer = layers.GRU(40,dropout=0.1,return_sequences=True)\n    #backward_layer = layers.GRU(40,dropout=0.2,return_sequences=True,go_backwards=True)\n    #x = layers.Bidirectional(forward_layer,backward_layer=backward_layer)(x)\n    \n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool,max_pool])\n    \n    x = layers.Dense(units=12,activation='relu')(x)\n    outputs = layers.Dense(units=6,activation='sigmoid')(x)\n    \n    model = keras.models.Model(inputs=inputs, outputs=outputs, name=\"mixed_model\")\n    \n    model.compile(optimizer=tf.optimizers.Adam(clipvalue=1),\\\n                  loss=tf.losses.BinaryCrossentropy(),\\\n                  metrics=['AUC'])\n    \n    return model\n    \nmodel = model()\nmodel.summary()\n```\n\n```\ncallback = keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=3)\n\nhistory = model.fit(x_train, y_train, \\\n                                 epochs=4, batch_size=64,\\\n                                 validation_data=(x_val,y_val),\n                       callbacks=[callback])\n```"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}