{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\n\nimport pickle \n#import mglearn\nimport time\nimport warnings\nimport string\n\n\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\nimport nltk\nfrom nltk import Text\nfrom nltk.tokenize import regexp_tokenize\nfrom nltk.tokenize import word_tokenize  \nfrom nltk.tokenize import sent_tokenize \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.metrics import roc_auc_score , accuracy_score , confusion_matrix , f1_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline","metadata":{"execution":{"iopub.status.busy":"2022-03-11T19:27:36.846617Z","iopub.execute_input":"2022-03-11T19:27:36.846876Z","iopub.status.idle":"2022-03-11T19:27:36.859917Z","shell.execute_reply.started":"2022-03-11T19:27:36.846846Z","shell.execute_reply":"2022-03-11T19:27:36.858951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CountVectorizer","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\ntxt = [\"He is ::having a great Time, at the park time?\",\n       \"She, unlike most women, is a big player on the park's grass.\",\n       \"she can't be going\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.318075Z","iopub.execute_input":"2022-03-11T18:54:42.318324Z","iopub.status.idle":"2022-03-11T18:54:42.322222Z","shell.execute_reply.started":"2022-03-11T18:54:42.318289Z","shell.execute_reply":"2022-03-11T18:54:42.321558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n\n# Transforms the data into a bag of words\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\nprint(\"Encoded Document is:\")\nprint(bag_of_words.toarray())\n\n# Print the first 10 features of the count_vec\nprint(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.323327Z","iopub.execute_input":"2022-03-11T18:54:42.323706Z","iopub.status.idle":"2022-03-11T18:54:42.340204Z","shell.execute_reply.started":"2022-03-11T18:54:42.323668Z","shell.execute_reply":"2022-03-11T18:54:42.339264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.342281Z","iopub.execute_input":"2022-03-11T18:54:42.342736Z","iopub.status.idle":"2022-03-11T18:54:42.350459Z","shell.execute_reply.started":"2022-03-11T18:54:42.3427Z","shell.execute_reply":"2022-03-11T18:54:42.349551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Doing CountVectorizer just with different N-Gram range","metadata":{}},{"cell_type":"code","source":"count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 5), max_df=1.0, min_df=1, max_features=None)\n\n# Transforms the data into a bag of words\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\nprint(bag_of_words.toarray())\n\n# Print the first 10 features of the count_vec\nprint(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.351554Z","iopub.execute_input":"2022-03-11T18:54:42.351807Z","iopub.status.idle":"2022-03-11T18:54:42.362968Z","shell.execute_reply.started":"2022-03-11T18:54:42.351774Z","shell.execute_reply":"2022-03-11T18:54:42.362289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Doing CountVectorizer and fetching features in the desired range of max and min Document Frequency","metadata":{}},{"cell_type":"code","source":"count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=0.5, min_df=0.3, max_features=None)\n\ncount_train = count_vec.fit_transform(txt)\n\nprint(count_vec.get_feature_names())\nprint('Displays terms whose document frequency is between 0.3 and 0.5')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.364267Z","iopub.execute_input":"2022-03-11T18:54:42.364635Z","iopub.status.idle":"2022-03-11T18:54:42.375475Z","shell.execute_reply.started":"2022-03-11T18:54:42.36458Z","shell.execute_reply":"2022-03-11T18:54:42.374765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fetching features only upto desired capacity specifed by user for learning vectorizer","metadata":{}},{"cell_type":"code","source":"count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=4)\n\ncount_train = count_vec.fit_transform(txt)\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.377126Z","iopub.execute_input":"2022-03-11T18:54:42.377406Z","iopub.status.idle":"2022-03-11T18:54:42.386176Z","shell.execute_reply.started":"2022-03-11T18:54:42.377373Z","shell.execute_reply":"2022-03-11T18:54:42.38513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF Vectorizer","metadata":{}},{"cell_type":"code","source":"txt1 = ['This is the first document.',\n        'This document is the second document.',\n        'And this is the third one.',\n        'Good this is nice',\n        'Is this the first document?']","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.387683Z","iopub.execute_input":"2022-03-11T18:54:42.388179Z","iopub.status.idle":"2022-03-11T18:54:42.391938Z","shell.execute_reply.started":"2022-03-11T18:54:42.388146Z","shell.execute_reply":"2022-03-11T18:54:42.391133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\ntxt_fitted = tf.fit(txt1)\ntxt_transformed = txt_fitted.transform(txt1)\nprint (\"The text: \", txt1)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.393582Z","iopub.execute_input":"2022-03-11T18:54:42.393916Z","iopub.status.idle":"2022-03-11T18:54:42.40435Z","shell.execute_reply.started":"2022-03-11T18:54:42.393867Z","shell.execute_reply":"2022-03-11T18:54:42.40346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.vocabulary_","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.408047Z","iopub.execute_input":"2022-03-11T18:54:42.40835Z","iopub.status.idle":"2022-03-11T18:54:42.415768Z","shell.execute_reply.started":"2022-03-11T18:54:42.408314Z","shell.execute_reply":"2022-03-11T18:54:42.414949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idf = tf.idf_\nprint(dict(zip(txt_fitted.get_feature_names(), idf))) #The one with the most value of TF-IDF is considered as discriminative","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.417367Z","iopub.execute_input":"2022-03-11T18:54:42.418082Z","iopub.status.idle":"2022-03-11T18:54:42.425263Z","shell.execute_reply.started":"2022-03-11T18:54:42.417982Z","shell.execute_reply":"2022-03-11T18:54:42.42428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rr = dict(zip(txt_fitted.get_feature_names(), idf))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.427117Z","iopub.execute_input":"2022-03-11T18:54:42.427822Z","iopub.status.idle":"2022-03-11T18:54:42.432277Z","shell.execute_reply.started":"2022-03-11T18:54:42.427781Z","shell.execute_reply":"2022-03-11T18:54:42.431314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\ntoken_weight.columns=('token','weight')\ntoken_weight \n\nsns.barplot(x='token', y='weight', data=token_weight)            \nplt.title(\"Inverse Document Frequency(idf) per token\")\nfig=plt.gcf()\nfig.set_size_inches(14,5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.433948Z","iopub.execute_input":"2022-03-11T18:54:42.434551Z","iopub.status.idle":"2022-03-11T18:54:42.694782Z","shell.execute_reply.started":"2022-03-11T18:54:42.434515Z","shell.execute_reply":"2022-03-11T18:54:42.694106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding features with lowest and highest IDF","metadata":{}},{"cell_type":"code","source":"feature_names = np.array(tf.get_feature_names())\nsorted_by_idf = np.argsort(tf.idf_)\nprint(\"Features with lowest idf:\\n{}\".format(\n       feature_names[sorted_by_idf[:3]]))\nprint(\"\\nFeatures with highest idf:\\n{}\".format(\n       feature_names[sorted_by_idf[-3:]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.696065Z","iopub.execute_input":"2022-03-11T18:54:42.696467Z","iopub.status.idle":"2022-03-11T18:54:42.704129Z","shell.execute_reply.started":"2022-03-11T18:54:42.696427Z","shell.execute_reply":"2022-03-11T18:54:42.703309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt_transformed.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.705341Z","iopub.execute_input":"2022-03-11T18:54:42.705652Z","iopub.status.idle":"2022-03-11T18:54:42.719064Z","shell.execute_reply.started":"2022-03-11T18:54:42.705615Z","shell.execute_reply":"2022-03-11T18:54:42.718362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding features with lowest and highest TFIDF","metadata":{}},{"cell_type":"code","source":"new1 = tf.transform(txt1)\n\n# find maximum value for each of the features over all of dataset:\nmax_val = new1.max(axis=0).toarray().ravel()\n\n#sort weights from smallest to biggest and extract their indices \nsort_by_tfidf = max_val.argsort()\n\nprint(\"Features with lowest tfidf:\\n{}\".format(\n      feature_names[sort_by_tfidf[:3]]))\n\nprint(\"\\nFeatures with highest tfidf: \\n{}\".format(\n      feature_names[sort_by_tfidf[-3:]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.720454Z","iopub.execute_input":"2022-03-11T18:54:42.720858Z","iopub.status.idle":"2022-03-11T18:54:42.73004Z","shell.execute_reply.started":"2022-03-11T18:54:42.720823Z","shell.execute_reply":"2022-03-11T18:54:42.729128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comment Classification","metadata":{}},{"cell_type":"code","source":"train_data  =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_data  =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_target =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:42.731662Z","iopub.execute_input":"2022-03-11T18:54:42.73192Z","iopub.status.idle":"2022-03-11T18:54:46.502392Z","shell.execute_reply.started":"2022-03-11T18:54:42.731864Z","shell.execute_reply":"2022-03-11T18:54:46.50165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_target.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:46.504328Z","iopub.execute_input":"2022-03-11T18:54:46.504527Z","iopub.status.idle":"2022-03-11T18:54:46.519774Z","shell.execute_reply.started":"2022-03-11T18:54:46.504502Z","shell.execute_reply":"2022-03-11T18:54:46.519141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:46.52103Z","iopub.execute_input":"2022-03-11T18:54:46.521707Z","iopub.status.idle":"2022-03-11T18:54:46.567364Z","shell.execute_reply.started":"2022-03-11T18:54:46.521671Z","shell.execute_reply":"2022-03-11T18:54:46.566651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:46.569443Z","iopub.execute_input":"2022-03-11T18:54:46.569908Z","iopub.status.idle":"2022-03-11T18:54:46.612384Z","shell.execute_reply.started":"2022-03-11T18:54:46.569858Z","shell.execute_reply":"2022-03-11T18:54:46.611754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments = train_data.drop(['id','comment_text'],axis = 1)\nfor i in comments.columns :\n    print(\"Percent of {0}s: \".format(i), round(100*comments[i].mean(),2), \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:46.613393Z","iopub.execute_input":"2022-03-11T18:54:46.613623Z","iopub.status.idle":"2022-03-11T18:54:46.628567Z","shell.execute_reply.started":"2022-03-11T18:54:46.61359Z","shell.execute_reply":"2022-03-11T18:54:46.627859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = {}\nfor i in list(comments.columns):\n    classes[i] =  comments[i].sum()\nn_classes = [classes[i] for i in list(classes.keys())]\nclasses = list(classes.keys())\nprint(n_classes) #Total toxic comments hold by Respective classes\nprint(sum(n_classes)) #Total number of rows out of 159570 which contain toxic comments","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:46.629675Z","iopub.execute_input":"2022-03-11T18:54:46.630392Z","iopub.status.idle":"2022-03-11T18:54:46.639273Z","shell.execute_reply.started":"2022-03-11T18:54:46.630358Z","shell.execute_reply":"2022-03-11T18:54:46.638559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,12))\nfig, ax = plt.subplots()\nax.bar(classes,n_classes,color='cyan')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:46.64047Z","iopub.execute_input":"2022-03-11T18:54:46.641328Z","iopub.status.idle":"2022-03-11T18:54:46.829397Z","shell.execute_reply.started":"2022-03-11T18:54:46.641295Z","shell.execute_reply":"2022-03-11T18:54:46.828741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Converting apostrophe characters into normal characters\ndef  clean_text(text):\n    text =  text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"\\r\", \"\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"that is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"n'\", \"ng\", text)\n    text = re.sub(r\"'bout\", \"about\", text)\n    text = re.sub(r\"'til\", \"until\", text)\n    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n    text = text.translate(str.maketrans('', '', string.punctuation)) \n    text = re.sub(\"(\\\\W)\",\" \",text) \n    text = re.sub('\\S*\\d\\S*\\s*','', text)\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:46.830622Z","iopub.execute_input":"2022-03-11T18:54:46.830849Z","iopub.status.idle":"2022-03-11T18:54:46.944718Z","shell.execute_reply.started":"2022-03-11T18:54:46.830819Z","shell.execute_reply":"2022-03-11T18:54:46.943828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying the above function in the dataset\ntrain_data.comment_text = train_data.comment_text.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:54:46.946152Z","iopub.execute_input":"2022-03-11T18:54:46.946436Z","iopub.status.idle":"2022-03-11T18:55:14.981113Z","shell.execute_reply.started":"2022-03-11T18:54:46.946397Z","shell.execute_reply":"2022-03-11T18:55:14.980182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:55:14.982644Z","iopub.execute_input":"2022-03-11T18:55:14.982949Z","iopub.status.idle":"2022-03-11T18:55:15.132649Z","shell.execute_reply.started":"2022-03-11T18:55:14.982898Z","shell.execute_reply":"2022-03-11T18:55:15.131639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sn = SnowballStemmer(language='english')\n\n\ndef stemmer(text):\n    words =  text.split()\n    train = [sn.stem(word) for word in words if not word in set(stopwords.words('english'))]\n    return ' '.join(train)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:55:15.133902Z","iopub.execute_input":"2022-03-11T18:55:15.13463Z","iopub.status.idle":"2022-03-11T18:55:15.140675Z","shell.execute_reply.started":"2022-03-11T18:55:15.134588Z","shell.execute_reply":"2022-03-11T18:55:15.139949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.comment_text = train_data.comment_text.apply(stemmer)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:55:15.14528Z","iopub.execute_input":"2022-03-11T18:55:15.146186Z","iopub.status.idle":"2022-03-11T19:18:01.511118Z","shell.execute_reply.started":"2022-03-11T18:55:15.146149Z","shell.execute_reply":"2022-03-11T19:18:01.510362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x =  train_data.comment_text\ny =  train_data.drop(['id','comment_text'],axis = 1)\n\nx_train,x_test,y_train,y_test =  train_test_split(x,y,test_size = 0.2,random_state = 45)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T19:18:01.512402Z","iopub.execute_input":"2022-03-11T19:18:01.512649Z","iopub.status.idle":"2022-03-11T19:18:01.547315Z","shell.execute_reply.started":"2022-03-11T19:18:01.512614Z","shell.execute_reply":"2022-03-11T19:18:01.546637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    strip_accents='unicode',     \n    analyzer='word',            \n    token_pattern=r'\\w{1,}',    \n    ngram_range=(1, 3),         \n    stop_words='english',\n    sublinear_tf=True)\n\nword_vectorizer.fit(x_train)    \ntrain_word_features = word_vectorizer.transform(x_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T19:18:01.548701Z","iopub.execute_input":"2022-03-11T19:18:01.548965Z","iopub.status.idle":"2022-03-11T19:18:52.26497Z","shell.execute_reply.started":"2022-03-11T19:18:01.548931Z","shell.execute_reply":"2022-03-11T19:18:52.264135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_transformed = word_vectorizer.transform(x_train)\nX_test_transformed = word_vectorizer.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T19:18:52.266404Z","iopub.execute_input":"2022-03-11T19:18:52.266658Z","iopub.status.idle":"2022-03-11T19:19:10.282568Z","shell.execute_reply.started":"2022-03-11T19:18:52.266623Z","shell.execute_reply":"2022-03-11T19:19:10.281821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_transformed)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T19:19:10.284022Z","iopub.execute_input":"2022-03-11T19:19:10.284276Z","iopub.status.idle":"2022-03-11T19:19:10.390846Z","shell.execute_reply.started":"2022-03-11T19:19:10.284242Z","shell.execute_reply":"2022-03-11T19:19:10.390118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_reg = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=45)\n\nclassifier = OneVsRestClassifier(log_reg)\nclassifier.fit(X_train_transformed, y_train)\n\n\ny_train_pred_proba = classifier.predict_proba(X_train_transformed)\ny_test_pred_proba = classifier.predict_proba(X_test_transformed)\n\n\nroc_auc_score_train = roc_auc_score(y_train, y_train_pred_proba,average='weighted')\nroc_auc_score_test = roc_auc_score(y_test, y_test_pred_proba,average='weighted')\n\nprint(\"ROC AUC Score Train:\", roc_auc_score_train)\nprint(\"ROC AUC Score Test:\", roc_auc_score_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-11T19:27:41.425272Z","iopub.execute_input":"2022-03-11T19:27:41.425796Z","iopub.status.idle":"2022-03-11T19:29:30.152435Z","shell.execute_reply.started":"2022-03-11T19:27:41.425754Z","shell.execute_reply":"2022-03-11T19:29:30.151781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_predictions(df,classifier):\n    df.comment_text = df.comment_text.apply(clean_text)\n    df.comment_text = df.comment_text.apply(stemmer)\n    X_test = df.comment_text\n    X_test_transformed = word_vectorizer.transform(X_test)\n    y_test_pred = classifier.predict_proba(X_test_transformed)\n    return y_test_pred\n    y_test_pred_df = pd.DataFrame(y_test_pred,columns=comments.columns) \n    submission_df = pd.concat([df.id, y_test_pred_df], axis=1)\n    submission_df.to_csv('19BCE226_Submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T19:31:59.052062Z","iopub.execute_input":"2022-03-11T19:31:59.052554Z","iopub.status.idle":"2022-03-11T19:31:59.059225Z","shell.execute_reply.started":"2022-03-11T19:31:59.052513Z","shell.execute_reply":"2022-03-11T19:31:59.058445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(test_data))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T19:34:16.158244Z","iopub.execute_input":"2022-03-11T19:34:16.158514Z","iopub.status.idle":"2022-03-11T19:34:16.163709Z","shell.execute_reply.started":"2022-03-11T19:34:16.158483Z","shell.execute_reply":"2022-03-11T19:34:16.162904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_test_predictions(test_data,classifier)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T19:34:25.956481Z","iopub.execute_input":"2022-03-11T19:34:25.956736Z","iopub.status.idle":"2022-03-11T19:54:45.645996Z","shell.execute_reply.started":"2022-03-11T19:34:25.956708Z","shell.execute_reply":"2022-03-11T19:54:45.645164Z"},"trusted":true},"execution_count":null,"outputs":[]}]}