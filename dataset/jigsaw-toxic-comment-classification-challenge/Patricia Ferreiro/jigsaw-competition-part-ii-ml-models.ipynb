{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n1. [Introduction](#1)\n2. [Import Libraries and Datasets](#2)\n3. [Training Data Pre-processing](#3)\n4. [Feature extraction: TD-IDF vectorization](#4)\n5. [Machine Learning Models](#5)\n - 5.1 [Logistic Regression](#6)\n - 5.2 [Support Vector Machines](#7)\n - 5.3 [Naive Bayes](#8)\n6. [Pipelining and Parameter Tuning](#9)\n7. [Conclusions](#10)"},{"metadata":{"_cell_guid":"d3b04218-0413-4e6c-8751-5d8a404d73a9","_uuid":"0bca9739b82d5d51e1229243e03ea1b6db35c17e"},"cell_type":"markdown","source":"## 1. Introduction\n\nThis kernel benchmarks classic **Machine Learning models for Text Classification**. Words are vectorized using TF-IDF and both character and word level n-grams of different sizes. The models implemented are Logistic Regression, Support Vector Machines and Naive Bayes. \n\nThe notebook follows these steps:\n\n - **Load train and test datasets**\n - **Feature extraction/vectorization of corpus with TD-IDF**\n - **Training of multi-class categorization ML models** for toxicity levels and type\n - **Models hyperparameter tuning**\n - **Performance metrics** and ML models comparison"},{"metadata":{},"cell_type":"markdown","source":"## 2. Import Libraries and Datasets"},{"metadata":{"_cell_guid":"ef06cd19-66b6-46bc-bf45-184e12d3f7d4","_uuid":"cca038ca9424a3f66e10262fc9129de807b5f855","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nfrom scipy import sparse\nimport gc, sys, warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import zipfile\n\n# unzip file to specified path\ndef import_zipped_data(file, output_path):\n    with zipfile.ZipFile(\"../input/\"+file+\".zip\",\"r\") as z:\n        z.extractall(\"/kaggle/working\")\n        \ndatasets = ['train.csv', 'test.csv', 'test_labels.csv', 'sample_submission.csv']\n\nkaggle_home = '/kaggle/working'\nfor dataset in datasets:\n    import_zipped_data(dataset, output_path = kaggle_home)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a494f561-0c2f-4a38-8973-6b60c22da357","_uuid":"f70ebe669fcf6b434c595cf6fb7a76120bf7809c","trusted":true,"collapsed":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/working/test.csv')\ntrain_df = pd.read_csv('/kaggle/working/train.csv')\nsample_input = pd.read_csv('/kaggle/working/sample_submission.csv')\ntest_labels = pd.read_csv('/kaggle/working/test_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5ddb337b-c9b2-4fec-9652-cb26769dc3c6","_uuid":"5f5269c56ea6ded273881b0d4dcdb6af83a3e089","scrolled":true,"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Training Data Pre-processing"},{"metadata":{"_cell_guid":"c66f79d1-1d9f-4d94-82c1-8026af198f2a","_uuid":"4ba6ef86c82f073bf411785d971a694348c3efa9","trusted":true,"collapsed":true},"cell_type":"code","source":"TEXT = 'comment_text'\nlabels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n# add label to mark non-toxic comments\ntrain_df['non-toxic'] = 1 - train_df[labels].max(axis=1)\n# replace na values with placeholder\ntrain_df[TEXT].fillna(\"unknown\", inplace=True)\ntest_df[TEXT].fillna(\"unknown\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"480780f1-00c0-4f9a-81e5-fc1932516a80","_uuid":"f2e77e8e6df5e29b620c7a2a0add1438c35af932"},"cell_type":"markdown","source":"## 4. Feature extraction: TD-IDF vectorization\n\nCreate a TD-IDF vectorization for both n-grams of character and words and stack the results in a single feature matrix.\n\nWe'll use the `TfidfVectorizer` function, which is equivalent to using `CountVectorizer` followed by `TfidfTransformer`. The tokenizing and filtering of stopwords are all included in `CountVectorizer`."},{"metadata":{},"cell_type":"markdown","source":"### Generate word n-grams from training data vocabulary"},{"metadata":{"_cell_guid":"31ad6c98-d054-426c-b3bd-b3b18f52eb6f","_uuid":"75f3f27d56fb2d7d539e65c292d9e77c92ceead3","trusted":true,"collapsed":true},"cell_type":"code","source":"# tokenizing and filtering of stopwords is included in CountVectorizer\n# apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)\n# max_df = 0.9, i.e. ignore words appearing in > 90% documents\nvec_words = TfidfVectorizer(stop_words='english', analyzer='word',\n                            min_df=3, max_df=0.9, strip_accents='unicode', sublinear_tf=1)\n# ngram_range=(1,2) # ideally added, increases training time\n\n# create vocabulary based on training data\nvec_words.fit_transform(train_df[TEXT])\n# vectorize train and test data for scoring\ntrain_vec_words = vec_words.transform(train_df[TEXT])\ntest_vec_words = vec_words.transform(test_df[TEXT])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate character n-grams from training data vocabulary"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"## OPTIONAL: n-grams at char level (VERY TIME CONSUMING!) ##\n# vectorizer for ngrams with characters\n#vec_chars = TfidfVectorizer(ngram_range=(4,5), stop_words='english', analyzer='char',\n#                            min_df=3, max_df=0.9, strip_accents='unicode', sublinear_tf=1)\n\n# create vocabulary based on training data\n#vec_chars.fit_transform(train_df[TEXT])\n# vectorize train and test data for scoring\n#train_vec_chars = vec_chars.transform(train_df[TEXT])\n#test_vec_chars = vec_chars.transform(test_df[TEXT])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concatenate n-grams into a single Feature Matrix"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# stack features in one matrix\n# features = sparse.hstack([train_vec_words, train_vec_chars])\n# test_features = sparse.hstack([test_vec_words, test_vec_chars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# clean up vectorizations - if matrix was created\n#del train_vec_words, test_vec_words, train_vec_chars, test_vec_chars\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"features, test_features = train_vec_words, test_vec_words\nlabels = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\ny = train_df[labels]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Machine Learning Models\nWe will train a model per toxicity type i.e. 6 models per algorithm. To keep the notebook concise, all Confusion Matrices and ROC plots will show the `threat` class classifier only. Comparison for performance in all classes is assessed via the AUC value.\n\n### 5.1 Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an array to store all the predictions\npredictions = np.zeros((test_features.shape[0],y.shape[1]))\n# fit a model per class\nfor i, label in enumerate(labels):\n    lr = LogisticRegression(C=2, random_state = i, class_weight = 'balanced')\n    print('Building {} model for column:{''}'.format(i, label)) \n    lr.fit(features, y[label])\n    predictions[:, i] = lr.predict_proba(test_features)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = 'threat'\npred =  lr.predict(features)\n# show confussion matrix for toxicity classification\nprint(confusion_matrix(y[label], pred))\nprint(classification_report(y[label], pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_prob_lr = lr.predict_proba(features)[:,1]\nfrp, trp, threshold = roc_curve(y[label], pred_prob_lr)\nauc_val = auc(frp, trp)\n\nplt.plot([0,1], [0,1], color='b')\nplt.plot(frp, trp, color='r', label= 'AUC = %.2f'%auc_val)\nplt.legend(loc='lower right')\nplt.xlabel('True Positive rate')\nplt.ylabel('False Positive rate')\nplt.title('ROC Curve')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Support Vector Machines\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.zeros((test_features.shape[0],y.shape[1]))\nfor i, label in enumerate(labels):\n    sgdc = SGDClassifier(loss='squared_loss', penalty='l2',\n                        alpha=1e-3, random_state=42,\n                        max_iter=5, tol=None)\n    print('Building {} model for column:{''}'.format(i, label)) \n    sgdc.fit(features, y[label])\n    pred[:, i] = sgdc.decision_function(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = sgdc.predict(features)\n# show confussion matrix for toxicity classification\nprint(confusion_matrix(y[label], pred))\nprint(classification_report(y[label], pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_prob_sgdc = sgdc.decision_function(features)\nfrp, trp, threshold = roc_curve(y[label], pred_prob_sgdc)\nauc_val = auc(frp, trp)\n\nplt.plot([0,1], [0,1], color='b')\nplt.plot(frp, trp, color='r', label= 'AUC = %.2f'%auc_val)\nplt.legend(loc='lower right')\nplt.xlabel('True Positive rate')\nplt.ylabel('False Positive rate')\nplt.title('ROC Curve')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"Several variants of Naive Bayes classifier exist. We will fit the multinomial variant, which is most suitable for word counts:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.zeros((test_features.shape[0],y.shape[1]))\nfor i, label in enumerate(labels):\n    nb = MultinomialNB(alpha = 0.0001)\n    print('Building {} model for column:{''}'.format(i, label)) \n    nb.fit(features, y[label])\n    pred[:, i] = nb.predict_proba(test_features)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred =  nb.predict(features)\nprint(confusion_matrix(y[label], pred))\nprint(classification_report(y[label], pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_prob_nb = nb.predict_proba(features)[:,1]\nfrp, trp, threshold = roc_curve(y[label], pred_prob_nb)\nauc_val = auc(frp, trp)\n\nplt.plot([0,1], [0,1], color='b')\nplt.plot(frp, trp, color='r', label= 'AUC = %.2f'%auc_val)\nplt.legend(loc='lower right')\nplt.xlabel('True Positive rate')\nplt.ylabel('False Positive rate')\nplt.title('ROC Curve')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Pipelining and Parameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"Using sklearn's `Pipeline` class we can define a compound classifier and stack all pre-processing, vectorization, feature extraction and model fitting operations. \n\nWe can easily try out the different classifiers we've defined throughout this notebook by passing the appropiate `clf` object at the last stage of the `Pipeline`. As with the default parameters both NB and SVM models were outperformed by the LR, we will try to tune them:"},{"metadata":{},"cell_type":"markdown","source":"#### Multinomial Naive Bayes Tuning"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"toxic_clf = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english', analyzer='word',\n                              min_df=3, max_df=0.9, strip_accents='unicode', sublinear_tf=1)),\n    ('clf', MultinomialNB())\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Pipelines also comes in handy when **tuning the hyperparameters**. We can run an exhaustive search of the best parameters on all stages of the pipeline. In this case, we'll use `GridSearch`: "},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"parameters = {\n    'clf__alpha': (0, 0.25, 0.5, 0.75, 1)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# n_jobs = -1 detects how many cores are installed and uses them all\nopt_toxic_clf = GridSearchCV(toxic_clf, parameters, cv=5, n_jobs=-1)\nopt_toxic_clf = opt_toxic_clf.fit(train_df[TEXT], y[label])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The object’s `best_score_` and `best_params_` attributes store the best mean score and the parameters setting corresponding to that score. A more detailed summary of the search is available at `cv_results_`."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(opt_toxic_clf.best_score_)\nfor p in sorted(parameters.keys()):\n    print(\"%s: %r\" % (p, opt_toxic_clf.best_params_[p]))\nprint(opt_toxic_clf.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best choice for the `alpha` smoothing factor is `0.25`.\n\n#### SVM Tuning"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"toxic_clf = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english', analyzer='word',\n                              min_df=3, max_df=0.9, strip_accents='unicode', sublinear_tf=1)),\n    ('clf', SGDClassifier())\n])\nparameters = {\n    'clf__alpha': (0.0001, 0.001, 0.01, 0.1)\n}\n# n_jobs = -1 detects how many cores are installed and uses them all\nopt_toxic_clf = GridSearchCV(toxic_clf, parameters, cv=5, n_jobs=-1)\nopt_toxic_clf = opt_toxic_clf.fit(train_df[TEXT], y[label])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(opt_toxic_clf.best_score_)\nfor p in sorted(parameters.keys()):\n    print(\"%s: %r\" % (p, opt_toxic_clf.best_params_[p]))\nprint(opt_toxic_clf.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best choice for the `alpha` constant that multiplies the regularization term is `0.0001`."},{"metadata":{},"cell_type":"markdown","source":"## 7. Conclusions"},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the ROC/AUC for all classifiers and toxicity types. We'll define a function to speed up the process:"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def str_to_class(classname):\n    return getattr(sys.modules[__name__], classname)\n\ndef compare_clfs(label):\n    plt.figure(0).clf()\n    # plot reference AUC 0.5 line\n    plt.plot([0,1], [0,1], color='k')\n    \n    # compute ROC for each classifier\n    classifiers = {'lr':'g', 'sgdc':'b', 'nb':'r'}\n    for c in classifiers.keys():\n        model = str_to_class(c)\n        if c == 'sgdc':\n            pred_prob = model.decision_function(features)\n        else:\n            pred_prob = model.predict_proba(features)[:,1]\n        frp, trp, threshold = roc_curve(y[label], pred_prob)\n        auc_val = auc(frp, trp)\n        plt.plot(frp, trp, color=classifiers[c], label= f'{c.upper()} AUC = %.2f'%auc_val)\n\n    plt.legend(loc=0)\n    plt.xlabel('True Positive rate')\n    plt.ylabel('False Positive rate')\n    plt.title(f'ROC Curve for {label.capitalize()} Class Classifiers')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_clfs('toxic')\ncompare_clfs('severe_toxic')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, severly toxic comments are better identified by all classifiers. For both levels of toxicity, LR yields the best results."},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_clfs('threat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LR also yields the best results for Threat type classification. Let's see if LR is still the most performant model for other classes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_clfs('identity_hate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Identity hate comments are classified with near 100% accuracy by both LR and SVM classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_clfs('obscene')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LR and SVM classifiers also show a consistent "},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_clfs('insult')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the default hyperparameters (see previous Kernel commit), Logistic Regression provided the best results for all classes. However, after tuning SVM and NB, the SVM model outperforms the other two in all toxicity types except `identity_hate`."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}