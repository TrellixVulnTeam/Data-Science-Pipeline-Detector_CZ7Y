{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**INTRODUCCIÓN**","metadata":{}},{"cell_type":"code","source":"# Daniel Revuelta González\n# 03/05/2021\n# Tarea 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importamos las librerías\n\nimport pandas as pd\nimport numpy as np\n\nimport gc\nimport time\nimport warnings\n\nfrom scipy import sparse\nimport scipy.stats as ss\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\n\nimport string\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer\n\nimport sys, os, re, csv, codecs\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cargamos el dataset\n\npath = '../input/'\ncomp = 'jigsaw-toxic-comment-classification-challenge/' # Conjunto de datos\nEMBEDDING_FILE = f'{path}glove6b50d/glove.6B.50d.txt' # Incluimos Glove, que utilizaremos posteriormente\ntrain_path = f'{path}{comp}train.csv.zip'\ntrain = pd.read_csv(train_path)\ntest_path = f'{path}{comp}test.csv.zip'\ntest = pd.read_csv(test_path)\n\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"] # Lista con las etiquetas\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANÁLISIS EXPLORATORIO**","metadata":{}},{"cell_type":"code","source":"train.tail(10) # Mostramos las 10 últimas observaciones del dataset de entreanmiento","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.tail(10) # Mostramos las 10 últimas observaciones del dataset de testeo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ejemplo de comentario \"obsceno\"\n\nprint(\"Comentario obsceno:\")\nprint(train[train.obscene==1].iloc[1,1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ejemplo de comentario \"tóxico\"\n\nprint(\"Comentario tóxico:\")\nprint(train[train.severe_toxic==1].iloc[3,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analizamos si hay datos incompletos\n\nprint(\"Comprobación de valores perdidos en el conjunto de entrenamiento\")\nnull_check=train.isnull().sum()\nprint(null_check)\nprint(\"Comprobación de valores perdidos en el conjunto de testeo\")\nnull_check=test.isnull().sum()\nprint(null_check)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# En este caso vemos que no hay ningún valor perdido, pero podemos sistematizar un par de sentencias que completen los datos si algún comentario no está disponible.\n\nprint(\"Completamos los datos no disponibles con \\\"_na_\\\"\")\nlist_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Marcamos las observaciones sin etiquetas como comentarios limpios\n\nrowsums=train.iloc[:,2:].sum(axis=1) # Sumamos horizontalmente los valores de las etiquetas de cada comentario.\ntrain['clean']=(rowsums==0) # Si la suma es 0, quiere decir que no tiene ninguna etiqueta asignada, por lo que se considera que es un comentario limpio. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total etiquetas\n\nx=train.iloc[:,2:].sum() # Sumamos los valores de las etiquetas de todos los comentarios.\nprint(\"Total etiquetas =\",x.sum())\n\n# Comentarios totales\n\nprint(\"Total comentarios = \",len(train))\n\n# Comentarios limpios\n\nprint(\"Total comentarios limpios = \",train['clean'].sum())\n\n# Comentarios en los que se detecta toxicidad (pertenecen a alguna de las categorías citadas antes)\n\nprint(\"Total comentarios con presencia de toxicidad = \",len(train)-train['clean'].sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Se observa que el set de entrenamiento no está equilibrado, pues hay 143.346 comentarios limpios y 16.225 catalogados con otras etiquetas.\n# Como vemos, hay más etiquetas que comentarios, por lo que se trata de un problema de clasificación \"multi-clase\" y \"multi-etiquetas\".","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Graficamos según el tipo de etiqueta que tienen los comentarios:\n\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Apariciones según la clase de comentario\")\nplt.ylabel('Nº de apariciones', fontsize=12)\nplt.xlabel('Tipo de etiqueta ', fontsize=12)\n\n# Añadimos las etiquetas\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# En el gráfico se aprecia cómo el dataset se encuentra desequilibrado, siendo la etiqueta \"clean\" la mayoritaria. Porcentualmente, la distribución sería:\n\nfor z in list_classes:\n    print('La clase',z, 'supone un: ',100*train[z].sum()/len(train), \"% del total\")\nprint('La clase clean supone un: ',100*train[\"clean\"].sum()/len(train), \"% del total\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Graficamos según el número de etiquetas\n\nx=rowsums.value_counts()\n\nplt.figure(figsize=(8,4))\nax = sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Cantidad de etiquetas por comentario\")\nplt.ylabel('Número de comentarios', fontsize=12)\nplt.xlabel('Número de etiquetas ', fontsize=12)\n\n# Añadimos las etiquetas\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hay 143.346 comentarios sin etiquetas (son limpios)\n# Hay 6.360 comentarios que pertenecen a una única categoría, 4.209 a dos, 3.480 a 3, 1.760 a 4 y 385 a 5.\n# Se aprecia que incluso hay 31 comentarios que están catalogados en todos los tipos de comentarios tóxicos.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobamos la relación entre etiquetas (en este caso, \"toxic\" contra el resto)\nmain_col=\"toxic\"\n\n# Aunque en esta ocasión se ha efectuado únicamente para la etiqueta \"toxic\" para reducir el tiempo de ejecución, se puede hacer con otras, pues sería similar el proceso. \n\ntemp_df=train.iloc[:,2:-1] # Quitamos los comentarios limpios porque no tienen etiqueta\n\n# Al ser variables categóricas, vamos a efectuarlo mediante tablas de contingencia\n\ncorr_mats=[]\nfor other_col in temp_df.columns[1:]:\n    confusion_matrix = pd.crosstab(temp_df[main_col], temp_df[other_col])\n    corr_mats.append(confusion_matrix)\nout = pd.concat(corr_mats,axis=1,keys=temp_df.columns[1:])\n\nprint('Matriz: ')\nprint(out)\n\n# También se puede hacer con una matriz de confusión y el estadístico de Cramer: #https://stackoverflow.com/questions/20892799/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix/39266194\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Graficamos un WordCloud con las palabras más mencionadas en comentarios \"limpios\"\n\nsubset=train[train.clean==True]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000).generate(\" \".join(text)) # Se fija un máximo de 2000 palabras\nplt.title(\"Palabras más utilizadas en comentarios limpios\", fontsize=12)\nplt.imshow(wc,interpolation='none')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Podemos hacer lo mismo con comentarios clasificados como \"obscenos\"\n\nsubset=train[train.obscene==True]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000).generate(\" \".join(text)) # Se fija un máximo de 2000 palabras\nplt.title(\"Palabras más utilizadas en comentarios obscenos\", fontsize=12)\nplt.imshow(wc,interpolation='none')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DESARROLLO DEL MODELO**","metadata":{}},{"cell_type":"code","source":"# Se fija su configuración básica\n\nembed_size = 50 # Cómo de grande va a ser el vector con el texto\nmax_features = 20000 # Cuántas palabras únicas se incorporan al vector\nmaxlen = 100 # Máximo número de palabras a usar en un comentario\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Se inicia el tokenizador, con el número máximo de palabras que hemos establecido (20000)\ntokenizer = Tokenizer(num_words=max_features)\n\n# Creamos el diccionario de índices y palabras del vocabulario según el orden de aparición de estas en los comentarios que se analizan.\n# Nota: el 0 se reserva para el padding.\ntokenizer.fit_on_texts(list(list_sentences_train)) \n\n# Transformamos los comentarios en una secuencia de números, sustituyendo cada palabra por el índice que ocupa en el diccionario.\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n\n# Convertimos las secuencias de números obtenidas en el paso anterior en cadena de igual longitud (en este caso, 100)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Leemos los vectores de palabras de Glove:\n\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Se utilizan los vectores para crear la matriz de embedding, con inicio alatorio para las palabras que no se encuentran en Glove. \n# Para esa inicialización aleatoria utilizaremos la media y la desviación estándar de los embedding de Glove.\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index # Diccionario con las palabras y su número asignado tras realizar el ajuste sobre el texto con el tokenizador (fit_on_texts) \nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) # Matriz de embedding\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Se construye el modelo:\n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp) \nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Entrenamos el modelo:\n\nmodel.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predecimos las variables de las etiquetas:\n\ny_test = model.predict([X_te], batch_size=1024, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos las 10 primeras predicciones:\n\ny_test[:10,]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Planteamos un modelo alternativo, al que se ha añadido otra capa densa para ver si ello mejora el desempeño del modelo.\n\n# Añadimos un Early Stopping como buena práctica, para que se interrumpa el entrenamiento si la función de pérdida sobre el conjunto de validación empeora durante 2 epochs:\ncallbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2)\n\n# Además, como otra buena práctica, representaremos gráficamente la evolución de las distintas funciones de pérdida y de precisión durante el proceso de entreanmiento para monitorizar qué ocurre","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Se construye el modelo:\n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ajustamos nuestro modelo alternativo:\n\nmodel_2 = model.fit(X_t, y, batch_size=32, epochs=5, callbacks=[callbacks], validation_split=0.1);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seleccionamos los datos de la función de pérdida y la precisión del set de entrenamiento y el de validación\n\nmodel_2_loss = model_2.history['loss']\nmodel_2_val_loss = model_2.history['val_loss']\nmodel_2_accuracy = model_2.history['accuracy']\nmodel_2_val_accuracy = model_2.history['val_accuracy']\n\n\n# Determinamos el número de epochs realizados durante el proceso de entrenamiento\nnum_epochs = range(1,len(model_2_loss)+1)\n\n# Graficamos la función de pérdida\nplt.plot(num_epochs,model_2_loss,label='Training_Loss')\nplt.plot(num_epochs,model_2_val_loss,label='Validation_Loss')\nplt.title('Pérdida en el entrenamiento y en la validación')\nplt.xlabel('Nº epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Graficamos la precisión\nplt.plot(num_epochs,model_2_accuracy,label='Training_Acc')\nplt.plot(num_epochs,model_2_val_accuracy,label='Validation_Acc')\nplt.title('Precisión en el entrenamiento y en la validación')\nplt.xlabel('Nº epoch')\nplt.ylabel('Precisión')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predecimos las etiquetas del conjunto de testeo:\n\ny_test_2 = model.predict([X_te], batch_size=1024, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos las 10 primeras predicciones:\n\ny_test_2[:10,]","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}