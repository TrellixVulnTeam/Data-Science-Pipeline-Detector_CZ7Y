{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Toxicity Classification: Word2Vec+TFIDF\n## Table of Contents\n* [1. Overview](#1.)\n* [2. Configuration](#2.)\n* [3. Setup](#3.)\n* [4. Import datasets](#4.)\n* [5. EDA & Preprocessing](#5.)\n    * [5.1 Text Preprocessing Function](#5.1)\n    * [5.2 TF-IDF Vectorization](#5.2)\n    * [5.3 Word2Vec Vectorization](#5.3)\n    * [5.4 Train Validation Split](#5.4)\n    * [5.5 Create TensorFlow Dataset](#5.5)\n* [6. Model Development](#6.)\n    * [6.1 FNet Encoder](#6.1)\n    * [6.2 Positional Embedding](#6.2)\n    * [6.3 Word2Vec FNet Model](#6.3)\n    * [6.4 TFIDF DNN Model](#6.4)\n    * [6.5 The Whole Model](#6.5)\n    * [6.6 Model Training](#6.6)\n    * [6.7 Evaluation](#6.7)\n* [7. Submission](#7.)\n* [8. References](#8.)","metadata":{"id":"QLQhXw_-L3bw","papermill":{"duration":0.059438,"end_time":"2021-11-17T03:42:41.235959","exception":false,"start_time":"2021-11-17T03:42:41.176521","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<font color=\"red\" size=\"3\">If you found it useful and would like to back me up, just upvote.</font>","metadata":{"papermill":{"duration":0.052528,"end_time":"2021-11-17T03:42:41.340324","exception":false,"start_time":"2021-11-17T03:42:41.287796","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"1.\"></a>\n## 1. Overview\nIn this notebook, I am going to build a Jigsaw Toxicity Classification Model with FNet using Word2Vec Vectorization and DNN using TFIDF Vectorization. \n\nI will keep track of three Model: Model with Best Accuracy, Model with Best AUC, and Latest Model. So I can use them for inference and try different ensemble method to get a better score.\n\nMany people like to create a training notebook and inference notebook to separate training and inference process. But to me I think that maintaining two notebooks is very inconvenient, instead I create an output dataset for this notebook. And I add two modes to this notebook, training and inference. During training mode, this notebook is responsible for training, it also infernece using output of the training. Everytime we finish training, we can choose the output to update the version of this output dataset. During inference mode, this notebook will skip training and inference using output dataset directly.","metadata":{"id":"3br121fsL3by","papermill":{"duration":0.050404,"end_time":"2021-11-17T03:42:41.441273","exception":false,"start_time":"2021-11-17T03:42:41.390869","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"2.\"></a>\n## 2. Configuration","metadata":{"id":"23-sSaFsL3bz","papermill":{"duration":0.051025,"end_time":"2021-11-17T03:42:41.543867","exception":false,"start_time":"2021-11-17T03:42:41.492842","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    tfidf_vocab_size = 20000\n    sequence_length = 100 # Length of sequence\n    batch_size = 1024\n    validation_split = 0.15\n    embed_dim = 256\n    latent_dim = 256\n    epochs = 10 # Number of Epochs to train\n    best_auc_model_path = \"model_best_auc.tf\"\n    best_acc_model_path = \"model_best_acc.tf\"\n    lastest_model_path = \"model_latest.tf\"\n    output_dataset_path = \"../input/toxicity-classification-word2vectfidf-output\"\n    labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n    modes = [\"training\", \"inference\"]\n    mode = modes[0]\nconfig = Config()","metadata":{"id":"gmp8ivzzL3bz","papermill":{"duration":0.075389,"end_time":"2021-11-17T03:42:41.669817","exception":false,"start_time":"2021-11-17T03:42:41.594428","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:57:51.733687Z","iopub.execute_input":"2021-11-18T12:57:51.734333Z","iopub.status.idle":"2021-11-18T12:57:51.741054Z","shell.execute_reply.started":"2021-11-18T12:57:51.734296Z","shell.execute_reply":"2021-11-18T12:57:51.74025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.\"></a>\n## 3. Setup","metadata":{"id":"98JWyaiDL3b0","papermill":{"duration":0.049986,"end_time":"2021-11-17T03:42:41.77083","exception":false,"start_time":"2021-11-17T03:42:41.720844","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport pathlib\nimport random\nimport string\nimport re\nimport sys\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import TweetTokenizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom scipy.stats import rankdata\nimport json","metadata":{"id":"oda23ThKL3b1","papermill":{"duration":6.302726,"end_time":"2021-11-17T03:42:48.11563","exception":false,"start_time":"2021-11-17T03:42:41.812904","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:53:13.170585Z","iopub.execute_input":"2021-11-18T12:53:13.171244Z","iopub.status.idle":"2021-11-18T12:53:13.181891Z","shell.execute_reply.started":"2021-11-18T12:53:13.171199Z","shell.execute_reply":"2021-11-18T12:53:13.180808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.\"></a>\n## 4. Import datasets","metadata":{"id":"7czW02EaL3b1","papermill":{"duration":0.077402,"end_time":"2021-11-17T03:42:48.227603","exception":false,"start_time":"2021-11-17T03:42:48.150201","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!unzip ../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n!unzip ../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n!unzip ../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n!unzip ../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip","metadata":{"execution":{"iopub.status.busy":"2021-11-18T12:53:15.609374Z","iopub.execute_input":"2021-11-18T12:53:15.609817Z","iopub.status.idle":"2021-11-18T12:53:20.027716Z","shell.execute_reply.started":"2021-11-18T12:53:15.609779Z","shell.execute_reply":"2021-11-18T12:53:20.02694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/working/train.csv\")\ntrain.head()","metadata":{"papermill":{"duration":1.71518,"end_time":"2021-11-17T03:42:50.526832","exception":false,"start_time":"2021-11-17T03:42:48.811652","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:53:23.889154Z","iopub.execute_input":"2021-11-18T12:53:23.889427Z","iopub.status.idle":"2021-11-18T12:53:24.795333Z","shell.execute_reply.started":"2021-11-18T12:53:23.889396Z","shell.execute_reply":"2021-11-18T12:53:24.794578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.\"></a>\n## 5. EDA & Preprocessing","metadata":{"papermill":{"duration":0.030876,"end_time":"2021-11-17T03:42:50.58883","exception":false,"start_time":"2021-11-17T03:42:50.557954","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"5.1\"></a>\n### 5.1 Text Preprocessing Function ","metadata":{"papermill":{"duration":0.029896,"end_time":"2021-11-17T03:42:50.909532","exception":false,"start_time":"2021-11-17T03:42:50.879636","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def custom_standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n    text = tf.strings.regex_replace(\n        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n    )\n    text = tf.strings.regex_replace(text, f\"[0-9]+\", \" \")\n    text = tf.strings.regex_replace(text, f\"[ ]+\", \" \")\n    text = tf.strings.strip(text)\n    return text","metadata":{"papermill":{"duration":0.038644,"end_time":"2021-11-17T03:42:50.978602","exception":false,"start_time":"2021-11-17T03:42:50.939958","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:53:27.843076Z","iopub.execute_input":"2021-11-18T12:53:27.843847Z","iopub.status.idle":"2021-11-18T12:53:27.849362Z","shell.execute_reply.started":"2021-11-18T12:53:27.843805Z","shell.execute_reply":"2021-11-18T12:53:27.848622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2\"></a>\n### 5.2 TF-IDF Vectorization","metadata":{"papermill":{"duration":0.030396,"end_time":"2021-11-17T03:42:51.039142","exception":false,"start_time":"2021-11-17T03:42:51.008746","status":"completed"},"tags":[]}},{"cell_type":"code","source":"tfidf_vectorizer = layers.TextVectorization(\n    standardize=custom_standardization, \n    max_tokens=config.tfidf_vocab_size, \n    output_mode=\"tf-idf\", \n    ngrams=2\n)\nwith tf.device(\"CPU\"):\n    # A bug that prevents this from running on GPU for now.\n    tfidf_vectorizer.adapt(list(train[\"comment_text\"]))","metadata":{"papermill":{"duration":41.618748,"end_time":"2021-11-17T03:43:32.688415","exception":false,"start_time":"2021-11-17T03:42:51.069667","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:53:31.207769Z","iopub.execute_input":"2021-11-18T12:53:31.208345Z","iopub.status.idle":"2021-11-18T12:54:18.358145Z","shell.execute_reply.started":"2021-11-18T12:53:31.208306Z","shell.execute_reply":"2021-11-18T12:54:18.35737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.3\"></a>\n### 5.3 Word2Vec Vectorization","metadata":{"papermill":{"duration":0.030692,"end_time":"2021-11-17T03:43:32.750597","exception":false,"start_time":"2021-11-17T03:43:32.719905","status":"completed"},"tags":[]}},{"cell_type":"code","source":"word2vec_vectorizer = layers.TextVectorization(\n    standardize=custom_standardization, \n    max_tokens=config.vocab_size, \n    output_sequence_length=config.sequence_length\n)\nwith tf.device(\"CPU\"):\n    word2vec_vectorizer.adapt(train[\"comment_text\"])","metadata":{"papermill":{"duration":12.096367,"end_time":"2021-11-17T03:43:44.877835","exception":false,"start_time":"2021-11-17T03:43:32.781468","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:30.545922Z","iopub.execute_input":"2021-11-18T12:54:30.54618Z","iopub.status.idle":"2021-11-18T12:54:51.138191Z","shell.execute_reply.started":"2021-11-18T12:54:30.546144Z","shell.execute_reply":"2021-11-18T12:54:51.136896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.4\"></a>\n### 5.4 Train Validation Split","metadata":{"papermill":{"duration":0.03156,"end_time":"2021-11-17T03:43:44.941531","exception":false,"start_time":"2021-11-17T03:43:44.909971","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train[\"comment_text\"], train[config.labels], test_size=config.validation_split)","metadata":{"papermill":{"duration":0.063485,"end_time":"2021-11-17T03:43:45.036267","exception":false,"start_time":"2021-11-17T03:43:44.972782","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:51.141494Z","iopub.execute_input":"2021-11-18T12:54:51.142061Z","iopub.status.idle":"2021-11-18T12:54:51.184715Z","shell.execute_reply.started":"2021-11-18T12:54:51.142023Z","shell.execute_reply":"2021-11-18T12:54:51.183889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_val.shape, y_val.shape","metadata":{"papermill":{"duration":0.039905,"end_time":"2021-11-17T03:43:45.10831","exception":false,"start_time":"2021-11-17T03:43:45.068405","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:51.185993Z","iopub.execute_input":"2021-11-18T12:54:51.18634Z","iopub.status.idle":"2021-11-18T12:54:51.193645Z","shell.execute_reply.started":"2021-11-18T12:54:51.1863Z","shell.execute_reply":"2021-11-18T12:54:51.192931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.5\"></a>\n### 5.5 Create TensorFlow Dataset","metadata":{"papermill":{"duration":0.03179,"end_time":"2021-11-17T03:43:45.172903","exception":false,"start_time":"2021-11-17T03:43:45.141113","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def make_dataset(X, y, batch_size, mode):\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    if mode == \"train\":\n       dataset = dataset.shuffle(256) \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.cache().prefetch(16).repeat(1)\n    return dataset","metadata":{"id":"KTzWxxsAL3b4","papermill":{"duration":0.039552,"end_time":"2021-11-17T03:43:45.24477","exception":false,"start_time":"2021-11-17T03:43:45.205218","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:51.195333Z","iopub.execute_input":"2021-11-18T12:54:51.195819Z","iopub.status.idle":"2021-11-18T12:54:51.202552Z","shell.execute_reply.started":"2021-11-18T12:54:51.195779Z","shell.execute_reply":"2021-11-18T12:54:51.201766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = make_dataset(X_train, y_train, batch_size=config.batch_size, mode=\"train\")\nvalid_ds = make_dataset(X_val, y_val, batch_size=config.batch_size, mode=\"valid\")","metadata":{"id":"oAP4kRzzL3b4","papermill":{"duration":0.122267,"end_time":"2021-11-17T03:43:45.398643","exception":false,"start_time":"2021-11-17T03:43:45.276376","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:51.203959Z","iopub.execute_input":"2021-11-18T12:54:51.204256Z","iopub.status.idle":"2021-11-18T12:54:51.311461Z","shell.execute_reply.started":"2021-11-18T12:54:51.20422Z","shell.execute_reply":"2021-11-18T12:54:51.310746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see what this data look like.","metadata":{"papermill":{"duration":0.03161,"end_time":"2021-11-17T03:43:45.462332","exception":false,"start_time":"2021-11-17T03:43:45.430722","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for batch in train_ds.take(1):\n    print(batch)","metadata":{"id":"BMjiEEa9L3b5","outputId":"598a1ca1-c02c-4be2-ec1e-d0632cda7d07","papermill":{"duration":0.070266,"end_time":"2021-11-17T03:43:45.563955","exception":false,"start_time":"2021-11-17T03:43:45.493689","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:51.312898Z","iopub.execute_input":"2021-11-18T12:54:51.31317Z","iopub.status.idle":"2021-11-18T12:54:51.349001Z","shell.execute_reply.started":"2021-11-18T12:54:51.313135Z","shell.execute_reply":"2021-11-18T12:54:51.3483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.\"></a>\n## 6. Model Development","metadata":{"id":"XSXAmMV0L3b6","papermill":{"duration":0.032727,"end_time":"2021-11-17T03:43:45.774255","exception":false,"start_time":"2021-11-17T03:43:45.741528","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"6.1\"></a>\n### 6.1 FNet Encoder","metadata":{"id":"n1qyTb9AL3b6","papermill":{"duration":0.0323,"end_time":"2021-11-17T03:43:45.839069","exception":false,"start_time":"2021-11-17T03:43:45.806769","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FNetEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, dropout_rate=0.1, **kwargs):\n        super(FNetEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n\n    def call(self, inputs):\n        # Casting the inputs to complex64\n        inp_complex = tf.cast(inputs, tf.complex64)\n        # Projecting the inputs to the frequency domain using FFT2D and\n        # extracting the real part of the output\n        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n        proj_input = self.layernorm_1(inputs + fft)\n        proj_output = self.dense_proj(proj_input)\n       \n        layer_norm = self.layernorm_2(proj_input + proj_output)\n        return layer_norm","metadata":{"id":"fRXW_RaML3b6","papermill":{"duration":0.043048,"end_time":"2021-11-17T03:43:45.91502","exception":false,"start_time":"2021-11-17T03:43:45.871972","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:51.352358Z","iopub.execute_input":"2021-11-18T12:54:51.352609Z","iopub.status.idle":"2021-11-18T12:54:51.410839Z","shell.execute_reply.started":"2021-11-18T12:54:51.352575Z","shell.execute_reply":"2021-11-18T12:54:51.410037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2\"></a>\n### 6.2 Positional Embedding","metadata":{"id":"ISzHta1dL3b7","papermill":{"duration":0.032047,"end_time":"2021-11-17T03:43:45.979414","exception":false,"start_time":"2021-11-17T03:43:45.947367","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","metadata":{"id":"kUiNiGv6L3b7","papermill":{"duration":0.042295,"end_time":"2021-11-17T03:43:46.053898","exception":false,"start_time":"2021-11-17T03:43:46.011603","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:51.413464Z","iopub.execute_input":"2021-11-18T12:54:51.413669Z","iopub.status.idle":"2021-11-18T12:54:51.427517Z","shell.execute_reply.started":"2021-11-18T12:54:51.413645Z","shell.execute_reply":"2021-11-18T12:54:51.426511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.3\"></a>\n### 6.3 Word2Vec FNet Model","metadata":{"id":"2VUKFd9sL3b9","papermill":{"duration":0.032867,"end_time":"2021-11-17T03:43:46.119462","exception":false,"start_time":"2021-11-17T03:43:46.086595","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_word2vec_model(config, inputs):\n    x = word2vec_vectorizer(inputs)\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(x)\n    x = FNetEncoder(config.embed_dim, config.latent_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.5)(x)\n    for i in range(3):\n        x = layers.Dense(100, activation=\"relu\")(x)\n        x = layers.Dropout(0.3)(x)\n    return x","metadata":{"papermill":{"duration":0.040146,"end_time":"2021-11-17T03:43:46.192518","exception":false,"start_time":"2021-11-17T03:43:46.152372","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:51.431067Z","iopub.execute_input":"2021-11-18T12:54:51.431269Z","iopub.status.idle":"2021-11-18T12:54:51.4391Z","shell.execute_reply.started":"2021-11-18T12:54:51.431245Z","shell.execute_reply":"2021-11-18T12:54:51.438274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.4\"></a>\n### 6.4 TFIDF DNN Model","metadata":{"papermill":{"duration":0.032351,"end_time":"2021-11-17T03:43:46.257715","exception":false,"start_time":"2021-11-17T03:43:46.225364","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_tfidf_model(config, inputs):\n    x = tfidf_vectorizer(inputs)\n    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(x)\n    return x","metadata":{"papermill":{"duration":0.038828,"end_time":"2021-11-17T03:43:46.328525","exception":false,"start_time":"2021-11-17T03:43:46.289697","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:51.440632Z","iopub.execute_input":"2021-11-18T12:54:51.441411Z","iopub.status.idle":"2021-11-18T12:54:51.452174Z","shell.execute_reply.started":"2021-11-18T12:54:51.441384Z","shell.execute_reply":"2021-11-18T12:54:51.451444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.5\"></a>\n### 6.5 The Whole Model","metadata":{"papermill":{"duration":0.032162,"end_time":"2021-11-17T03:43:46.392994","exception":false,"start_time":"2021-11-17T03:43:46.360832","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_model(config):\n    inputs = keras.Input(shape=(None, ), dtype=\"string\", name=\"inputs\")\n    word2vec_x = get_word2vec_model(config, inputs)\n    tfidf_x = get_tfidf_model(config, inputs)\n    x = layers.Concatenate()([word2vec_x, tfidf_x])\n    output = layers.Dense(6, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, output, name=\"model\")\n    return model","metadata":{"id":"dp19cOEqL3b9","papermill":{"duration":0.039835,"end_time":"2021-11-17T03:43:46.465233","exception":false,"start_time":"2021-11-17T03:43:46.425398","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:57.237067Z","iopub.execute_input":"2021-11-18T12:54:57.237621Z","iopub.status.idle":"2021-11-18T12:54:57.243317Z","shell.execute_reply.started":"2021-11-18T12:54:57.237582Z","shell.execute_reply":"2021-11-18T12:54:57.242638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(config)\nmodel.summary()","metadata":{"id":"T-QhEpKoL3b-","papermill":{"duration":0.427814,"end_time":"2021-11-17T03:43:46.925194","exception":false,"start_time":"2021-11-17T03:43:46.49738","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:54:59.612299Z","iopub.execute_input":"2021-11-18T12:54:59.612889Z","iopub.status.idle":"2021-11-18T12:54:59.987828Z","shell.execute_reply.started":"2021-11-18T12:54:59.612849Z","shell.execute_reply":"2021-11-18T12:54:59.987095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the Model.","metadata":{"id":"x_O7FfflL3b-","papermill":{"duration":0.032446,"end_time":"2021-11-17T03:43:47.074163","exception":false,"start_time":"2021-11-17T03:43:47.041717","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keras.utils.plot_model(model, show_shapes=True)","metadata":{"id":"6Tm1nG0CL3b-","outputId":"0cd78731-3c68-4fdf-f8c7-9b0b2512013d","papermill":{"duration":1.295254,"end_time":"2021-11-17T03:43:48.402955","exception":false,"start_time":"2021-11-17T03:43:47.107701","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:55:03.589816Z","iopub.execute_input":"2021-11-18T12:55:03.590864Z","iopub.status.idle":"2021-11-18T12:55:04.568593Z","shell.execute_reply.started":"2021-11-18T12:55:03.590818Z","shell.execute_reply":"2021-11-18T12:55:04.567755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.6\"></a>\n### 6.6 Model Training","metadata":{"id":"NFtSL6-3L3b-","papermill":{"duration":0.036552,"end_time":"2021-11-17T03:43:48.476657","exception":false,"start_time":"2021-11-17T03:43:48.440105","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.compile(\n    \"adam\", loss=\"binary_crossentropy\", metrics=[\"categorical_accuracy\", keras.metrics.AUC()]\n)","metadata":{"id":"3wMy6AcIL3b_","papermill":{"duration":0.064855,"end_time":"2021-11-17T03:43:48.577896","exception":false,"start_time":"2021-11-17T03:43:48.513041","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:55:09.794617Z","iopub.execute_input":"2021-11-18T12:55:09.795137Z","iopub.status.idle":"2021-11-18T12:55:09.820112Z","shell.execute_reply.started":"2021-11-18T12:55:09.7951Z","shell.execute_reply":"2021-11-18T12:55:09.81932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config.mode == config.modes[0]:\n    acc_checkpoint = keras.callbacks.ModelCheckpoint(config.best_acc_model_path, monitor=\"val_categorical_accuracy\",save_weights_only=True, save_best_only=True)\n    auc_checkpoint = keras.callbacks.ModelCheckpoint(config.best_auc_model_path, monitor=\"val_auc\",save_weights_only=True, save_best_only=True)\n    early_stopping = keras.callbacks.EarlyStopping(patience=10)\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(patience=5, min_delta=1e-4, min_lr=1e-6)\n    model.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[acc_checkpoint, auc_checkpoint, reduce_lr])\n    model.save_weights(config.lastest_model_path)","metadata":{"id":"YKjayqSSL3b_","outputId":"0a6677a7-60a0-4eda-e992-f0807e4d8f6e","papermill":{"duration":946.073818,"end_time":"2021-11-17T03:59:34.688459","exception":false,"start_time":"2021-11-17T03:43:48.614641","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:55:59.508093Z","iopub.execute_input":"2021-11-18T12:55:59.508358Z","iopub.status.idle":"2021-11-18T12:55:59.515284Z","shell.execute_reply.started":"2021-11-18T12:55:59.50833Z","shell.execute_reply":"2021-11-18T12:55:59.514538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.7\"></a>\n### 6.7 Evaluation","metadata":{"papermill":{"duration":1.82236,"end_time":"2021-11-17T03:59:38.643906","exception":false,"start_time":"2021-11-17T03:59:36.821546","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Classification Report","metadata":{"papermill":{"duration":1.840635,"end_time":"2021-11-17T03:59:42.312555","exception":false,"start_time":"2021-11-17T03:59:40.47192","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if config.mode == config.modes[0]:\n    from sklearn.metrics import classification_report\n    y_pred = np.array(model.predict(valid_ds) > 0.5, dtype=int)\n    cls_report = classification_report(y_val, y_pred)\n    print(cls_report)","metadata":{"papermill":{"duration":3.890371,"end_time":"2021-11-17T03:59:48.070742","exception":false,"start_time":"2021-11-17T03:59:44.180371","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T12:59:08.199316Z","iopub.execute_input":"2021-11-18T12:59:08.200113Z","iopub.status.idle":"2021-11-18T12:59:08.206827Z","shell.execute_reply.started":"2021-11-18T12:59:08.20006Z","shell.execute_reply":"2021-11-18T12:59:08.205219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.\"></a>\n## 7. Submission","metadata":{"papermill":{"duration":1.829614,"end_time":"2021-11-17T03:59:51.750795","exception":false,"start_time":"2021-11-17T03:59:49.921181","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/working/test.csv\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T12:59:11.718635Z","iopub.execute_input":"2021-11-18T12:59:11.719229Z","iopub.status.idle":"2021-11-18T12:59:12.484826Z","shell.execute_reply.started":"2021-11-18T12:59:11.719188Z","shell.execute_reply":"2021-11-18T12:59:12.484143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/working/sample_submission.csv\")\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T12:59:19.395328Z","iopub.execute_input":"2021-11-18T12:59:19.395604Z","iopub.status.idle":"2021-11-18T12:59:19.552297Z","shell.execute_reply.started":"2021-11-18T12:59:19.395573Z","shell.execute_reply":"2021-11-18T12:59:19.551522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = []\ntest_ds = tf.data.Dataset.from_tensor_slices((test[\"comment_text\"])).batch(config.batch_size).cache().prefetch(1)\nfor path in [config.best_auc_model_path]:\n#for path in [config.best_acc_model_path, config.best_auc_model_path, config.lastest_model_path]:\n    if config.mode == config.modes[1]:\n        path = config.output_dataset_path + \"/\" + path\n    model.load_weights(path)\n    score = model.predict(test_ds)\n    scores.append(score)\nscore = np.mean(scores, axis=0)\nprint(score.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:02:07.677591Z","iopub.execute_input":"2021-11-18T13:02:07.678155Z","iopub.status.idle":"2021-11-18T13:02:57.839061Z","shell.execute_reply.started":"2021-11-18T13:02:07.678116Z","shell.execute_reply":"2021-11-18T13:02:57.8382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[config.labels] = score\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","metadata":{"papermill":{"duration":5.267665,"end_time":"2021-11-17T03:59:58.978023","exception":false,"start_time":"2021-11-17T03:59:53.710358","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-18T13:02:57.840754Z","iopub.execute_input":"2021-11-18T13:02:57.841228Z","iopub.status.idle":"2021-11-18T13:02:59.348518Z","shell.execute_reply.started":"2021-11-18T13:02:57.841187Z","shell.execute_reply":"2021-11-18T13:02:59.347708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<a id=\"8.\"></a>\n## 8. References\n- [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824v3)\n- [Attention Is All You Need](https://arxiv.org/abs/1706.03762v5)\n- [Text Generation using FNet](https://keras.io/examples/nlp/text_generation_fnet/)\n- [English-Spanish Translation: FNet](https://www.kaggle.com/lonnieqin/english-spanish-translation-fnet)","metadata":{"id":"uKmcuxr1L3cH","papermill":{"duration":1.855045,"end_time":"2021-11-17T04:00:02.658482","exception":false,"start_time":"2021-11-17T04:00:00.803437","status":"completed"},"tags":[]}}]}