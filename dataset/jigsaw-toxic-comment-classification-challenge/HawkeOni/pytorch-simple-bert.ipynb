{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"! pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nfrom typing import Tuple, List\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertPreTrainedModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are reading the data into the DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"! unzip ../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip;\n! unzip ../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip;\n! unzip ../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip;\n! unzip ../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"./\"\nbert_model_name = 'bert-base-cased'\n# path = \"../input/jigsaw-toxic-comment-classification-challenge/\"\ndevice = torch.device('cpu')\nif torch.cuda.is_available():\n    device = torch.device('cuda:0')\ntokenizer = BertTokenizer.from_pretrained(bert_model_name)\nassert tokenizer.pad_token_id == 0, \"Padding value used in masks is set to zero, please change it everywhere\"\ntrain_df = pd.read_csv(os.path.join(path, 'train.csv'))\n# training on a part of data for speed\n# train_df = train_df.sample(frac=0.33)\ntrain_df, val_df = train_test_split(train_df, test_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we create a Dataset and iterators to it for training and validation. It is not truly lazy, as it has dataframe in memory, but they are not converted to tensors."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToxicDataset(Dataset):\n    \n    def __init__(self, tokenizer: BertTokenizer, dataframe: pd.DataFrame, lazy: bool = False):\n        self.tokenizer = tokenizer\n        self.pad_idx = tokenizer.pad_token_id\n        self.lazy = lazy\n        if not self.lazy:\n            self.X = []\n            self.Y = []\n            for i, (row) in tqdm(dataframe.iterrows()):\n                x, y = self.row_to_tensor(self.tokenizer, row)\n                self.X.append(x)\n                self.Y.append(y)\n        else:\n            self.df = dataframe        \n    \n    @staticmethod\n    def row_to_tensor(tokenizer: BertTokenizer, row: pd.Series) -> Tuple[torch.LongTensor, torch.LongTensor]:\n        tokens = tokenizer.encode(row[\"comment_text\"], add_special_tokens=True)\n        if len(tokens) > 120:\n            tokens = tokens[:119] + [tokens[-1]]\n        x = torch.LongTensor(tokens)\n        y = torch.FloatTensor(row[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]])\n        return x, y\n        \n    \n    def __len__(self):\n        if self.lazy:\n            return len(self.df)\n        else:\n            return len(self.X)\n\n    def __getitem__(self, index: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n        if not self.lazy:\n            return self.X[index], self.Y[index]\n        else:\n            return self.row_to_tensor(self.tokenizer, self.df.iloc[index])\n            \n\ndef collate_fn(batch: List[Tuple[torch.LongTensor, torch.LongTensor]], device: torch.device) \\\n        -> Tuple[torch.LongTensor, torch.LongTensor]:\n    x, y = list(zip(*batch))\n    x = pad_sequence(x, batch_first=True, padding_value=0)\n    y = torch.stack(y)\n    return x.to(device), y.to(device)\n\ntrain_dataset = ToxicDataset(tokenizer, train_df, lazy=True)\ndev_dataset = ToxicDataset(tokenizer, val_df, lazy=True)\ncollate_fn = partial(collate_fn, device=device)\nBATCH_SIZE = 32\ntrain_sampler = RandomSampler(train_dataset)\ndev_sampler = RandomSampler(dev_dataset)\ntrain_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\ndev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE, sampler=dev_sampler, collate_fn=collate_fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simple Bert model for classification of whole sequence."},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertClassifier(nn.Module):\n    \n    def __init__(self, bert: BertModel, num_classes: int):\n        super().__init__()\n        self.bert = bert\n        self.classifier = nn.Linear(bert.config.hidden_size, num_classes)\n        \n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                \n            labels=None):\n        outputs = self.bert(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids,\n                               position_ids=position_ids,\n                               head_mask=head_mask)\n        cls_output = outputs[1] # batch, hidden\n        cls_output = self.classifier(cls_output) # batch, 6\n        cls_output = torch.sigmoid(cls_output)\n        criterion = nn.BCELoss()\n        loss = 0\n        if labels is not None:\n            loss = criterion(cls_output, labels)\n        return loss, cls_output\n\nmodel = BertClassifier(BertModel.from_pretrained(bert_model_name), 6).to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training and evaluation loops"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, iterator, optimizer, scheduler):\n    model.train()\n    total_loss = 0\n    for x, y in tqdm(iterator):\n        optimizer.zero_grad()\n        mask = (x != 0).float()\n        loss, outputs = model(x, attention_mask=mask, labels=y)\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    print(f\"Train loss {total_loss / len(iterator)}\")\n\ndef evaluate(model, iterator):\n    model.eval()\n    pred = []\n    true = []\n    with torch.no_grad():\n        total_loss = 0\n        for x, y in tqdm(iterator):\n            mask = (x != 0).float()\n            loss, outputs = model(x, attention_mask=mask, labels=y)\n            total_loss += loss\n            true += y.cpu().numpy().tolist()\n            pred += outputs.cpu().numpy().tolist()\n    true = np.array(true)\n    pred = np.array(pred)\n    for i, name in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n        print(f\"{name} roc_auc {roc_auc_score(true[:, i], pred[:, i])}\")\n    print(f\"Evaluate loss {total_loss / len(iterator)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\nEPOCH_NUM = 2\n# triangular learning rate, linearly grows untill half of first epoch, then linearly decays \nwarmup_steps = 10 ** 3\ntotal_steps = len(train_iterator) * EPOCH_NUM - warmup_steps\noptimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\nscheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=total_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(EPOCH_NUM):\n    print('=' * 50, f\"EPOCH {i}\", '=' * 50)\n    train(model, train_iterator, optimizer, scheduler)\n    evaluate(model, dev_iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\ntest_df = pd.read_csv(os.path.join(path, 'test.csv'))\nsubmission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\ncolumns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nfor i in tqdm(range(len(test_df) // BATCH_SIZE + 1)):\n    batch_df = test_df.iloc[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n    assert (batch_df[\"id\"] == submission[\"id\"][i * BATCH_SIZE: (i + 1) * BATCH_SIZE]).all(), f\"Id mismatch\"\n    texts = []\n    for text in batch_df[\"comment_text\"].tolist():\n        text = tokenizer.encode(text, add_special_tokens=True)\n        if len(text) > 120:\n            text = text[:119] + [tokenizer.sep_token_id]\n        texts.append(torch.LongTensor(text))\n    x = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n    mask = (x != tokenizer.pad_token_id).float().to(device)\n    with torch.no_grad():\n        _, outputs = model(x, attention_mask=mask)\n    outputs = outputs.cpu().numpy()\n    submission.iloc[i * BATCH_SIZE: (i + 1) * BATCH_SIZE][columns] = outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}