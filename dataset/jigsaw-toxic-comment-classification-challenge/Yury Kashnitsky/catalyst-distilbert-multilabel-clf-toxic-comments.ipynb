{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center> DistilBERT for multilabel classification with Catalyst and HuggingFace\n##  <center>  Toxic comments classification","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Python \nimport os\nimport warnings\nimport logging\nfrom typing import Mapping, List, Union, Optional, Tuple\nfrom pprint import pprint\n\n# Numpy, Pandas, Sklearn\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# PyTorch \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Transformers \nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n\n# Catalyst\nimport catalyst\nfrom catalyst.dl import SupervisedRunner\n# this will appear in Catalyst 20.08\n# from catalyst.dl.callbacks.metrics.accuracy import MultiLabelAccuracyCallback\nfrom catalyst.dl.callbacks import OptimizerCallback, CheckpointCallback, InferCallback\nfrom catalyst.dl.utils import plot_metrics\nfrom catalyst.utils import set_global_seed, prepare_cudnn","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"catalyst.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Setup**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"MODEL_NAME = 'distilbert-base-uncased' # pretrained model from Transformers\nLOG_DIR = \"./logdir\"                   # for training logs and tensorboard visualizations\nNUM_EPOCHS = 3                         # smth around 2-6 epochs is typically fine when finetuning transformers\nBATCH_SIZE = 96                        # depends on your available GPU memory (in combination with max seq length)\nMAX_SEQ_LENGTH = 256                   # depends on your available GPU memory (in combination with batch size)\nLEARN_RATE = 3e-5                      # learning rate is typically ~1e-5 for transformers\nACCUM_STEPS = 4                        # one optimization step for that many backward passes\nSEED = 17                              # random seed for reproducibility","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dataset**\n\nToxic comments - [competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).\nGiven text of a comment, we need to classify it into several toxicity categories: 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', or 'identity_hate'.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# to reproduce, download the data and customize this path\nPATH_TO_DATA = '../input/jigsaw-toxic-comment-classification-challenge/'\nTEXT_FIELD = 'comment_text'\nTARGET_FIELDS = ['toxic','severe_toxic','obscene','threat','insult', 'identity_hate']\nNUM_CLASSES = len(TARGET_FIELDS)\nPRED_THRES = 0.4   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = pd.read_csv(PATH_TO_DATA + 'train.csv.zip', index_col='id')\ntest_df = pd.read_csv(PATH_TO_DATA + 'test.csv.zip', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train_df[TEXT_FIELD],\n                                                      train_df[TARGET_FIELDS], \n                                                      test_size=0.1, \n                                                      random_state=17)\nX_test = test_df[TEXT_FIELD]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(X_train), len(X_valid), len(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Torch Dataset\n\nThis is left for user to be defined. Catalyst will take care of the rest. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"class TextClassificationDataset(Dataset):\n    \"\"\"\n    Wrapper around Torch Dataset to perform text classification\n    \"\"\"\n    def __init__(self,\n                 texts: List[str],\n                 labels: np.ndarray = None,\n                 max_seq_length: int = 512,\n                 model_name: str = 'distilbert-base-uncased'):\n        \"\"\"\n        Args:\n            texts (List[str]): a list with texts to classify or to train the\n                classifier on\n            labels List[str]: \n            max_seq_length (int): maximal sequence length in tokens,\n                texts will be stripped to this length\n            model_name (str): transformer model name, needed to perform\n                appropriate tokenization\n\n        \"\"\"\n\n        self.texts = texts\n        self.labels = labels\n        self.max_seq_length = max_seq_length\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        # suppresses tokenizer warnings\n        logging.getLogger(\n            \"transformers.tokenization_utils\").setLevel(logging.FATAL)\n\n    def __len__(self):\n        \"\"\"\n        Returns:\n            int: length of the dataset\n        \"\"\"\n        return len(self.texts)\n\n    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n        \"\"\"Gets element of the dataset\n\n        Args:\n            index (int): index of the element in the dataset\n        Returns:\n            Single element by index\n        \"\"\"\n\n        # encoding the text\n        x = self.texts[index]\n        \n        # a dictionary with `input_ids` and `attention_mask` as keys\n        output_dict = self.tokenizer.encode_plus(\n            x,\n            add_special_tokens=True,\n            pad_to_max_length=True,\n            max_length=self.max_seq_length,\n            return_tensors=\"pt\",\n            return_attention_mask=True\n        )\n        \n        # for Catalyst, there needs to be a key called features\n        output_dict['features'] = output_dict['input_ids'].squeeze(0)\n        del output_dict['input_ids']\n        \n        # encoding target\n        if self.labels is not None:\n            output_dict[\"targets\"] = torch.from_numpy(self.labels[index]).float()\n\n        return output_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create Torch Datasets with train, validation, and test data.**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_dataset = TextClassificationDataset(\n    texts=X_train.values.tolist(),\n    labels=y_train.values,\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)\n\nvalid_dataset = TextClassificationDataset(\n    texts=X_valid.values.tolist(),\n    labels=y_valid.values,\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)\n\ntest_dataset = TextClassificationDataset(\n    texts=X_test.values.tolist(),\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the training dataset instances:","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pprint(train_dataset[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finally, we define standard PyTorch loaders. This dictionary will be fed to Catalyst.**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_val_loaders = {\n    \"train\": DataLoader(dataset=train_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=True),\n    \"valid\": DataLoader(dataset=valid_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=False)    \n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"class BertForSequenceClassification(nn.Module):\n    \"\"\"\n    Simplified version of the same class by HuggingFace.\n    See transformers/modeling_distilbert.py in the transformers repository.\n    \"\"\"\n\n    def __init__(self, pretrained_model_name: str, num_classes: int = None, dropout: float = 0.3):\n        \"\"\"\n        Args:\n            pretrained_model_name (str): HuggingFace model name.\n                See transformers/modeling_auto.py\n            num_classes (int): the number of class labels\n                in the classification task\n        \"\"\"\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(\n            pretrained_model_name, num_labels=num_classes)\n\n        self.model = AutoModel.from_pretrained(pretrained_model_name,\n                                                    config=config)\n#         self.pre_classifier = nn.Linear(config.hidden_size, config.hidden_size)\n        self.classifier = nn.Linear(config.hidden_size, num_classes)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, features, attention_mask=None, head_mask=None):\n        \"\"\"Compute class probabilities for the input sequence.\n\n        Args:\n            features (torch.Tensor): ids of each token,\n                size ([bs, seq_length]\n            attention_mask (torch.Tensor): binary tensor, used to select\n                tokens which are used to compute attention scores\n                in the self-attention heads, size [bs, seq_length]\n            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n                we keep the head, size: [num_heads]\n                or [num_hidden_layers x num_heads]\n        Returns:\n            PyTorch Tensor with predicted class probabilities\n        \"\"\"\n        assert attention_mask is not None, \"attention mask is none\"\n        \n        bert_output = self.model(input_ids=features,\n                                            attention_mask=attention_mask,\n                                            head_mask=head_mask)\n        # we only need the hidden state here and don't need\n        # transformer output, so index 0\n        seq_output = bert_output[0]  # (bs, seq_len, dim)\n        # mean pooling, i.e. getting average representation for all tokens\n        pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n        logits = self.classifier(pooled_output)  # (bs, dim)\n\n        return logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model = BertForSequenceClassification(pretrained_model_name=MODEL_NAME,\n                                      num_classes=NUM_CLASSES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d = next(iter(train_val_loaders['train']))\n# p = model(d['features'], d['attention_mask'])\n# criterion(p, d['targets'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model training\n\nFirst we specify optimizer and scheduler (pure PyTorch). Then Catalyst stuff.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"criterion = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Implementation of multilabel accuracy is under development in Catalyst, so we've copied the code for `MultiLabelAccuracyCallback` and dependencies here (branch [metrics-update-2](https://github.com/catalyst-team/catalyst/blob/feature/metrics-update-2/catalyst/dl/callbacks/metrics/accuracy.py)).","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from catalyst.core import MetricCallback\nfrom catalyst.utils.torch import get_activation_fn\n\ndef preprocess_multi_label_metrics(\n    outputs: torch.Tensor,\n    targets: torch.Tensor,\n    weights: Optional[torch.Tensor] = None,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    General preprocessing and check for multi-label-based metrics.\n    Args:\n        outputs (torch.Tensor): NxK tensor that for each of the N examples\n            indicates the probability of the example belonging to each of\n            the K classes, according to the model.\n        targets (torch.Tensor): binary NxK tensor that encodes which of the K\n            classes are associated with the N-th input\n            (eg: a row [0, 1, 0, 1] indicates that the example is\n            associated with classes 2 and 4)\n        weights (torch.Tensor): importance for each sample\n    Returns:\n        processed ``outputs`` and ``targets``\n        with [batch_size; num_classes] shape\n    \"\"\"\n    if not torch.is_tensor(outputs):\n        outputs = torch.from_numpy(outputs)\n    if not torch.is_tensor(targets):\n        targets = torch.from_numpy(targets)\n    if weights is not None:\n        if not torch.is_tensor(weights):\n            weights = torch.from_numpy(weights)\n        weights = weights.squeeze()\n\n    if outputs.dim() == 1:\n        outputs = outputs.view(-1, 1)\n    else:\n        assert outputs.dim() == 2, (\n            \"wrong `outputs` size \"\n            \"(should be 1D or 2D with one column per class)\"\n        )\n\n    if targets.dim() == 1:\n        targets = targets.view(-1, 1)\n    else:\n        assert targets.dim() == 2, (\n            \"wrong `targets` size \"\n            \"(should be 1D or 2D with one column per class)\"\n        )\n\n    if weights is not None:\n        assert weights.dim() == 1, \"Weights dimension should be 1\"\n        assert weights.numel() == targets.size(\n            0\n        ), \"Weights dimension 1 should be the same as that of target\"\n        assert torch.min(weights) >= 0, \"Weight should be non-negative only\"\n\n    assert torch.equal(\n        targets ** 2, targets\n    ), \"targets should be binary (0 or 1)\"\n\n    return outputs, targets, weights\n\ndef multi_label_accuracy(\n    outputs: torch.Tensor,\n    targets: torch.Tensor,\n    threshold: Union[float, torch.Tensor],\n    activation: Optional[str] = None,\n) -> torch.Tensor:\n    \"\"\"\n    Computes multi-label accuracy for the specified activation and threshold.\n    Args:\n        outputs (torch.Tensor): NxK tensor that for each of the N examples\n            indicates the probability of the example belonging to each of\n            the K classes, according to the model.\n        targets (torch.Tensor): binary NxK tensort that encodes which of the K\n            classes are associated with the N-th input\n            (eg: a row [0, 1, 0, 1] indicates that the example is\n            associated with classes 2 and 4)\n        threshold (float): threshold for for model output\n        activation (str): activation to use for model output\n    Returns:\n        computed multi-label accuracy\n    \"\"\"\n    outputs, targets, _ = preprocess_multi_label_metrics(\n        outputs=outputs, targets=targets\n    )\n    activation_fn = get_activation_fn(activation)\n    outputs = activation_fn(outputs)\n\n    outputs = (outputs > threshold).long()\n    output = (targets.long() == outputs.long()).sum().float() / np.prod(\n        targets.shape\n    )\n    return output\n\n\nclass MultiLabelAccuracyCallback(MetricCallback):\n    \"\"\"Accuracy metric callback.\n    Computes multi-class accuracy@topk for the specified values of `topk`.\n    .. note::\n        For multi-label accuracy please use\n        `catalyst.dl.callbacks.metrics.MultiLabelAccuracyCallback`\n    \"\"\"\n\n    def __init__(\n        self,\n        input_key: str = \"targets\",\n        output_key: str = \"logits\",\n        prefix: str = \"multi_label_accuracy\",\n        threshold: float = None,\n        activation: str = \"Sigmoid\",\n    ):\n        \"\"\"\n        Args:\n            input_key (str): input key to use for accuracy calculation;\n                specifies our `y_true`\n            output_key (str): output key to use for accuracy calculation;\n                specifies our `y_pred`\n            prefix (str): key for the metric's name\n            threshold (float): threshold for for model output\n            activation (str): An torch.nn activation applied to the outputs.\n                Must be one of ``\"none\"``, ``\"Sigmoid\"``, or ``\"Softmax\"``\n        \"\"\"\n        super().__init__(\n            prefix=prefix,\n            metric_fn=multi_label_accuracy,\n            input_key=input_key,\n            output_key=output_key,\n            threshold=threshold,\n            activation=activation,\n        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To run Deep Learning experiments, Catalyst resorts to the [`Runner`](https://catalyst-team.github.io/catalyst/api/dl.html#catalyst.dl.core.runner.Runner) abstraction, in particular, to [`SupervisedRunner`](https://catalyst-team.github.io/catalyst/api/dl.html#module-catalyst.dl.runner.supervised).\n\n`SupervisedRunner` implements the following methods:\n - `train` - starts the training process of the model\n - `predict_loader` - makes a prediction on the whole loader with the specified model\n - `infer` - makes the inference on the model\n \nTo train the model within this interface you pass the following to the `train` method:\n - model (`torch.nn.Module`) – PyTorch model to train\n - criterion (`nn.Module`) – PyTorch criterion function for training\n - optimizer (`optim.Optimizer`) – PyTorch optimizer for training\n - loaders (dict) – dictionary containing one or several `torch.utils.data.DataLoader` for training and validation\n - logdir (str) – path to output directory. There Catalyst will write logs, will dump the best model and the actual code to train the model\n - callbacks – list of Catalyst callbacks\n - scheduler (`optim.lr_scheduler._LRScheduler`) – PyTorch scheduler for training\n - ...\n \nIn our case we'll pass the created `DistilBertForSequenceClassification` model, cross-entropy criterion, Adam optimizer, scheduler and data loaders that we created earlier. Also, we'll be tracking accuracy and thus will need `AccuracyCallback`. To perform batch accumulation, we'll be using `OptimizationCallback`.\n\nThere are many more useful [callbacks](https://catalyst-team.github.io/catalyst/api/dl.html#module-catalyst.dl.callbacks.checkpoint) implemented, also check out [Catalyst examples](https://github.com/catalyst-team/catalyst/tree/master/examples/notebooks).","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"    # can be changed in case of multiple GPUs onboard\nset_global_seed(SEED)                       # reproducibility\nprepare_cudnn(deterministic=True)           # reproducibility","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"%%time\n# here we specify that we pass masks to the runner. So model's forward method will be called with\n# these arguments passed to it. \nrunner = SupervisedRunner(\n    input_key=(\n        \"features\",\n        \"attention_mask\"\n    )\n)\n\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=train_val_loaders,\n    callbacks=[\n        MultiLabelAccuracyCallback(threshold=PRED_THRES),\n        OptimizerCallback(accumulation_steps=ACCUM_STEPS)\n    ],\n    logdir=LOG_DIR,\n    num_epochs=NUM_EPOCHS,\n    verbose=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!du -hc $LOG_DIR","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!ls $LOG_DIR/checkpoints","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot metrics\n\n<img src=\"https://habrastorage.org/webt/ki/ib/hy/kiibhyp373r65zriwruroiqitky.jpeg\" width=30% />\n\nThere are at least 4 ways to monitor training:\n\n### 1. Good old tqdm\nThere above it's set with a flag `verbose` in `runner.train`. Actually, it's not that bad :)\n\n<img src='https://habrastorage.org/webt/ta/1s/98/ta1s988ghabz412weaq0lgs_cke.png'> \n\n\n### 2. Weights & Biases\n\nBefore launching training, you can run [Weighs & Biases](https://app.wandb.ai/) inititialization for this project. Execute `wandb init` in a separate terminal window (from the same directory where this notebook is running). `wandb` will ask your API key from https://app.wandb.ai/authorize and project name. The rest will be picked up by Catalyst's `SupervisedWandbRunner` (so you'll need to import this instead of `SupervisedRunner`). \nFollowing the links printed above (smth. like  https://app.wandb.ai/yorko/catalyst-nlp-bert) we can keep track of loss and metrics.\n\n### 3. Tensorboard\nDuring training, logs are written to `LOG_DIR` specified above. \nSimiltaneously with training, you can run `tensorboard --logdir $LOG_DIR` (in another terminal tab, in case of training on a server, I also had to add a `--bind_all` flag),\nand you'll get a nice dashboard. Here we see how accuracy and loss change during training.\n\n<img src=\"https://habrastorage.org/webt/2a/sx/mo/2asxmoizgcpf2fnhjjkfhvf70aw.png\" width=50% />\n\n### 4. Offline metric plotting\n\nIf your training is pretty fast and/or you're not interested in tracking training progress, you can just plot losses and metrics once the training is done. Looks like it won't work in Kernels though but try it locally.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_metrics(\n    logdir=LOG_DIR,\n    step='epoch',\n    metrics=['accuracy']\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference for the test set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's create a Torch loader for the test set and launch `infer` to actually make predictions fot the test set. First, we load the best model checkpoint, then make inference with this model.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"test_loaders = {\n    \"test\": DataLoader(dataset=test_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=False) \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nrunner.infer(\n    model=model,\n    loaders=test_loaders,\n    callbacks=[\n        CheckpointCallback(\n            resume=f\"{LOG_DIR}/checkpoints/best.pth\"\n        ),\n        InferCallback(),\n    ],   \n    verbose=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted_probs = runner.callbacks[0].predictions['logits']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted_probs.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have predicted probabilities, let's finally create a submission file.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_sub_df = pd.read_csv(PATH_TO_DATA + 'sample_submission.csv.zip',\n                           index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_sub_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_sub_df[TARGET_FIELDS] = predicted_probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_sub_df.to_csv('submissions.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!head -3 submissions.csv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}