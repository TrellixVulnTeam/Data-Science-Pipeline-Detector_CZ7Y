{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"input files","metadata":{}},{"cell_type":"code","source":"!unzip -o '/kaggle/input/jigsaw-toxic-comment-classification-challenge/*.zip' -d /kaggle/working > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:13:27.872916Z","iopub.execute_input":"2021-07-04T12:13:27.873319Z","iopub.status.idle":"2021-07-04T12:13:30.220516Z","shell.execute_reply.started":"2021-07-04T12:13:27.873202Z","shell.execute_reply":"2021-07-04T12:13:30.2195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader\nfrom torch.nn import BCEWithLogitsLoss, Sigmoid\nfrom tqdm.notebook import tqdm, trange\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n#from bs4 import BeautifulSoup\nimport re","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:17:38.326252Z","iopub.execute_input":"2021-07-04T12:17:38.326666Z","iopub.status.idle":"2021-07-04T12:17:38.332814Z","shell.execute_reply.started":"2021-07-04T12:17:38.326635Z","shell.execute_reply":"2021-07-04T12:17:38.33119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"other installations","metadata":{}},{"cell_type":"code","source":"!pip install BeautifulSoup4","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:18:12.345923Z","iopub.execute_input":"2021-07-04T12:18:12.34627Z","iopub.status.idle":"2021-07-04T12:18:20.452827Z","shell.execute_reply.started":"2021-07-04T12:18:12.346238Z","shell.execute_reply":"2021-07-04T12:18:20.451965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install pytorch_pretrained_bert pytorch-nlp -q","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:18:31.38057Z","iopub.execute_input":"2021-07-04T12:18:31.38091Z","iopub.status.idle":"2021-07-04T12:18:38.16264Z","shell.execute_reply.started":"2021-07-04T12:18:31.38088Z","shell.execute_reply":"2021-07-04T12:18:38.161524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\ndf_train = pd.read_csv(\"train.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:18:56.477181Z","iopub.execute_input":"2021-07-04T12:18:56.477553Z","iopub.status.idle":"2021-07-04T12:18:57.2632Z","shell.execute_reply.started":"2021-07-04T12:18:56.477519Z","shell.execute_reply":"2021-07-04T12:18:57.262374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:19:26.060487Z","iopub.execute_input":"2021-07-04T12:19:26.060871Z","iopub.status.idle":"2021-07-04T12:19:26.109937Z","shell.execute_reply.started":"2021-07-04T12:19:26.060839Z","shell.execute_reply":"2021-07-04T12:19:26.108861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"test.csv\")\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:19:54.130731Z","iopub.execute_input":"2021-07-04T12:19:54.131086Z","iopub.status.idle":"2021-07-04T12:19:54.680506Z","shell.execute_reply.started":"2021-07-04T12:19:54.131055Z","shell.execute_reply":"2021-07-04T12:19:54.679494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"sample_submission.csv\")\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:20:28.877095Z","iopub.execute_input":"2021-07-04T12:20:28.877437Z","iopub.status.idle":"2021-07-04T12:20:29.03222Z","shell.execute_reply.started":"2021-07-04T12:20:28.877406Z","shell.execute_reply":"2021-07-04T12:20:29.031066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape,df_test.shape,sample_submission.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:21:19.370981Z","iopub.execute_input":"2021-07-04T12:21:19.371314Z","iopub.status.idle":"2021-07-04T12:21:19.376643Z","shell.execute_reply.started":"2021-07-04T12:21:19.371271Z","shell.execute_reply":"2021-07-04T12:21:19.375784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"check null values","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:22:45.257805Z","iopub.execute_input":"2021-07-04T12:22:45.258128Z","iopub.status.idle":"2021-07-04T12:22:45.311739Z","shell.execute_reply.started":"2021-07-04T12:22:45.258092Z","shell.execute_reply":"2021-07-04T12:22:45.310317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:23:04.441547Z","iopub.execute_input":"2021-07-04T12:23:04.441901Z","iopub.status.idle":"2021-07-04T12:23:04.480189Z","shell.execute_reply.started":"2021-07-04T12:23:04.441871Z","shell.execute_reply":"2021-07-04T12:23:04.478826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"text preprocessing","metadata":{}},{"cell_type":"code","source":"def textPre(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    text = re.sub('\\[[^]]*\\]', '', soup.get_text())\n    pattern=r\"[^a-zA-z0-9\\s,']\"\n    text=re.sub(pattern,'',text)\n    return text\n\ndf_train[\"comment_text\"] = df_train[\"comment_text\"].apply(textPre)\ndf_test[\"comment_text\"] = df_test[\"comment_text\"].apply(textPre)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:27:08.44585Z","iopub.execute_input":"2021-07-04T12:27:08.4462Z","iopub.status.idle":"2021-07-04T12:27:32.297629Z","shell.execute_reply.started":"2021-07-04T12:27:08.446168Z","shell.execute_reply":"2021-07-04T12:27:32.296503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Processed sentence :'  , df_train['comment_text'][3])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:29:09.696423Z","iopub.execute_input":"2021-07-04T12:29:09.69674Z","iopub.status.idle":"2021-07-04T12:29:09.701572Z","shell.execute_reply.started":"2021-07-04T12:29:09.696711Z","shell.execute_reply":"2021-07-04T12:29:09.700684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change the sentences for input to BERT","metadata":{}},{"cell_type":"code","source":"df_train_sentence = df_train[\"comment_text\"]\ndf_test_sentence = df_test[\"comment_text\"]\ndf_train_sentence = [\"[SCL] \"+ i + \" [SEP]\"for i in df_train_sentence]\ndf_test_sentence = [\"[SCL] \"+ i + \" [SEP]\"for i in df_test_sentence]\ndf_train_sentence[0], df_test_sentence[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:32:36.936592Z","iopub.execute_input":"2021-07-04T12:32:36.936903Z","iopub.status.idle":"2021-07-04T12:32:37.129788Z","shell.execute_reply.started":"2021-07-04T12:32:36.936875Z","shell.execute_reply":"2021-07-04T12:32:37.128677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenizing and mapping to the index","metadata":{}},{"cell_type":"code","source":"from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n\ntrain_tokenizer_texts = list(map(lambda t: tokenizer.tokenize(t)[:510], tqdm(df_train_sentence)))\n\ntest_tokenizer_texts = list(map(lambda t: tokenizer.tokenize(t)[:510], tqdm(df_test_sentence)))\n\nnp.array(train_tokenizer_texts[0]), np.array(test_tokenizer_texts[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:35:43.307879Z","iopub.execute_input":"2021-07-04T12:35:43.308211Z","iopub.status.idle":"2021-07-04T12:44:00.120965Z","shell.execute_reply.started":"2021-07-04T12:35:43.308182Z","shell.execute_reply":"2021-07-04T12:44:00.119897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\n\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tqdm(train_tokenizer_texts)]\ninput_ids = pad_sequences(sequences = input_ids, maxlen = MAX_LEN, dtype = 'long', padding='post', truncating='post')\n\ntest_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tqdm(test_tokenizer_texts)]\ntest_input_ids = pad_sequences(sequences = test_input_ids, maxlen = MAX_LEN, dtype = 'long', padding='post', truncating='post')\n\n\ninput_ids[0], test_input_ids[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:44:00.122629Z","iopub.execute_input":"2021-07-04T12:44:00.12298Z","iopub.status.idle":"2021-07-04T12:44:11.553367Z","shell.execute_reply.started":"2021-07-04T12:44:00.122943Z","shell.execute_reply":"2021-07-04T12:44:11.552637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating an attention mask - For actual tokens its set to 1, for padding tokens its set to 0\ndef create_attention_masks(input_ids):\n    attention_masks = []\n    for seq in tqdm(input_ids):\n        seq_mask = [float(i>0) for i in seq]\n        attention_masks.append(seq_mask)\n    return np.array(attention_masks)\n\nattention_masks = create_attention_masks(input_ids)\ntest_attention_masks = create_attention_masks(test_input_ids)\nattention_masks[0], test_attention_masks[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:44:59.554998Z","iopub.execute_input":"2021-07-04T12:44:59.555356Z","iopub.status.idle":"2021-07-04T12:45:46.615498Z","shell.execute_reply.started":"2021-07-04T12:44:59.555317Z","shell.execute_reply":"2021-07-04T12:45:46.614562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = df_train[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].to_numpy()\nX_train, X_val, y_train, y_val = train_test_split(input_ids, labels, random_state = 123, test_size = 0.20)\nattention_masks_train, attention_masks_val = train_test_split(attention_masks, random_state = 123, test_size = 0.20)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:46:38.038928Z","iopub.execute_input":"2021-07-04T12:46:38.039276Z","iopub.status.idle":"2021-07-04T12:46:38.19545Z","shell.execute_reply.started":"2021-07-04T12:46:38.039245Z","shell.execute_reply":"2021-07-04T12:46:38.194418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = torch.tensor(X_train)\nX_val = torch.tensor(X_val)\ny_train = torch.tensor(y_train) \ny_val = torch.tensor(y_val)\nattention_masks_train = torch.tensor(attention_masks_train)\nattention_masks_val = torch.tensor(attention_masks_val)\n\ntest_input_ids = torch.tensor(test_input_ids)\ntest_attention_masks = torch.tensor(test_attention_masks)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:47:00.471652Z","iopub.execute_input":"2021-07-04T12:47:00.471965Z","iopub.status.idle":"2021-07-04T12:47:00.841749Z","shell.execute_reply.started":"2021-07-04T12:47:00.471935Z","shell.execute_reply":"2021-07-04T12:47:00.840744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\n#Dataset wrapping tensors.\ntrain_data = TensorDataset(X_train, attention_masks_train, y_train)\nval_data = TensorDataset(X_val, attention_masks_val, y_val)\ntest_data = TensorDataset(test_input_ids, test_attention_masks)\n#Samples elements randomly. If without replacement(default), then sample from a shuffled dataset.\ntrain_sampler = RandomSampler(train_data)\nval_sampler = SequentialSampler(val_data)\ntest_sampler = SequentialSampler(test_data)\n#represents a Python iterable over a dataset\ntrain_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = BATCH_SIZE)\ntest_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:47:23.373126Z","iopub.execute_input":"2021-07-04T12:47:23.37357Z","iopub.status.idle":"2021-07-04T12:47:23.38153Z","shell.execute_reply.started":"2021-07-04T12:47:23.373537Z","shell.execute_reply":"2021-07-04T12:47:23.380675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Inititaing a BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 6)\nmodel.cuda()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:47:35.363249Z","iopub.execute_input":"2021-07-04T12:47:35.363613Z","iopub.status.idle":"2021-07-04T12:48:04.486773Z","shell.execute_reply.started":"2021-07-04T12:47:35.36358Z","shell.execute_reply":"2021-07-04T12:48:04.485977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {\n        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n        'weight_decay_rate': 0.01\n    },\n    {\n        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n        'weight_decay_rate': 0.0\n    }\n]\n\noptimizer = BertAdam(optimizer_grouped_parameters, lr = 2e-5, warmup = .1)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:48:51.991002Z","iopub.execute_input":"2021-07-04T12:48:51.991334Z","iopub.status.idle":"2021-07-04T12:48:52.000995Z","shell.execute_reply.started":"2021-07-04T12:48:51.991287Z","shell.execute_reply":"2021-07-04T12:48:52.000139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#freeing up memory\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:49:10.582982Z","iopub.execute_input":"2021-07-04T12:49:10.583342Z","iopub.status.idle":"2021-07-04T12:49:11.261689Z","shell.execute_reply.started":"2021-07-04T12:49:10.583291Z","shell.execute_reply":"2021-07-04T12:49:11.260769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Empty the GPU memory as it might be memory and CPU intensive while training\ntorch.cuda.empty_cache()\n#Number of times the whole dataset will run through the network and model is fine-tuned\nepochs = 2\n#Iterate over number of epochs\nfor _ in trange(epochs, desc = \"Epoch\"):\n    #Switch model to train phase where it will update gradients\n    model.train()\n    #Initaite train and validation loss, number of rows passed and number of batches passed\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    val_loss = 0\n    nb_val_examples, nb_val_steps = 0, 0\n    #Iterate over batches within the same epoch\n    for batch in tqdm(train_dataloader):\n        #Shift the batch to GPU for computation\n        batch = tuple(t.to(device) for t in batch)\n        #Load the input ids and masks from the batch\n        b_input_ids, b_input_mask, b_labels = batch\n        #Initiate gradients to 0 as they tend to add up\n        optimizer.zero_grad()\n        #Forward pass the input data\n        logits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n        #We will be using the Binary Cross entropy loss with added sigmoid function after that in BCEWithLogitsLoss\n        loss_func = BCEWithLogitsLoss()\n        #Calculate the loss between multilabel predicted outputs and actuals\n        loss = loss_func(logits, b_labels.type_as(logits))\n        #Backpropogate the loss and calculate the gradients\n        loss.backward()\n        #Update the weights with the calculated gradients\n        optimizer.step()\n        #Add the loss of the batch to the final loss, number of rows and batches\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n    #Print the current training loss \n    print(\"Train Loss: {}\".format(tr_loss/nb_tr_examples))\n    #Switch the model to evaluate stage at which the gradients wont be updated\n    model.eval()\n    #Iterate over the validation data\n    for step, batch in enumerate(val_dataloader):\n        #Shift the validation data to GPUs for computation\n        batch = tuple(t.to(device) for t in batch)\n        #We dont want to update the gradients\n        with torch.no_grad():\n            #Load the input ids and masks from the batch\n            b_input_ids, b_input_mask, b_labels = batch\n            #Forward pass the input data\n            logits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n            #We will be using the Binary Cross entropy loss with added sigmoid function after that in BCEWithLogitsLoss\n            loss_func = BCEWithLogitsLoss()\n            #Calculate the loss between multilabel predicted outputs and actuals\n            loss = loss_func(logits, b_labels.type_as(logits))\n            #Add the loss of the batch to the final loss, number of rows and batches\n            val_loss += loss.item()\n            nb_val_examples += b_input_ids.size(0)\n            nb_val_steps += 1\n    #Print the current validation loss     \n    print(\"Valid Loss: {}\".format(val_loss/nb_val_examples))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T12:51:42.358991Z","iopub.execute_input":"2021-07-04T12:51:42.359333Z","iopub.status.idle":"2021-07-04T14:07:23.141806Z","shell.execute_reply.started":"2021-07-04T12:51:42.35929Z","shell.execute_reply":"2021-07-04T14:07:23.140868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = []\n\n#Iterate over the test_loader \nfor step, batch in enumerate(test_dataloader):\n        #Transfer batch to GPUs\n        batch = tuple(t.to(device) for t in batch)\n        #We dont need to update gradients as we are just predicting\n        with torch.no_grad():\n            #Bring up the next batch of input_texts and attention_masks \n            b_input_ids, b_input_mask = batch\n            #Forward propogate the inputs and get output as logits\n            logits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n            #Pass the outputs through a sigmoid function to get the multi-label preditions\n            s = Sigmoid()\n            out = s(logits).to('cpu').numpy()    \n            #Add the predictions for this batch to the final list\n            outputs.extend(out)\n\ndf_test = pd.merge(df_test, sample_submission, on = \"id\")\n#Assign the predictions to the toxic_output columns\ndf_test[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]] = outputs\n#Drop text data as it is not expected in the submission file\ndf_test.drop([\"comment_text\"], axis = 1, inplace = True)\n#Saving the submission dataframe\ndf_test.to_csv(\"sample_submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T14:43:22.082294Z","iopub.execute_input":"2021-07-04T14:43:22.082678Z","iopub.status.idle":"2021-07-04T14:54:44.499892Z","shell.execute_reply.started":"2021-07-04T14:43:22.082644Z","shell.execute_reply":"2021-07-04T14:54:44.498778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('sample_submission.csv')\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T14:59:35.583551Z","iopub.execute_input":"2021-07-04T14:59:35.583881Z","iopub.status.idle":"2021-07-04T14:59:35.82248Z","shell.execute_reply.started":"2021-07-04T14:59:35.583851Z","shell.execute_reply":"2021-07-04T14:59:35.821388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}