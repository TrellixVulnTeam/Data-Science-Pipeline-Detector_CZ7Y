{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip', low_memory=False , sep=',') \nprint(df)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#List the fields in our dataframe\nprint(df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# below line causes shuffling of indices, to avoid using train_test_split later\ndf = df.reindex(np.random.permutation(df.index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Separate the Comment field data and outcome lables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"comment = df['comment_text']\nprint(comment.head())\ncomment = comment.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = df[['toxic', 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate']]\nprint(label.head())\nlabel = label.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find out the frequency of occurence of multilabelled data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ct1 counts samples having atleast one label\n# ct2 counts samples having 2 or more than 2 labels\nct1,ct2 = 0,0\nfor i in range(label.shape[0]):\n    ct = np.count_nonzero(label[i])\n    if ct :\n        ct1 = ct1+1\n    if ct>1 :\n        ct2 = ct2+1\nprint(ct1)\nprint(ct2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Visualisations**\n\nAnalyse the no. of comments having lengths varying from 0 to 1200"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [len(comment[i]) for i in range(comment.shape[0])]\n\nprint('average length of comment: {:.3f}'.format(sum(x)/len(x)) )\nbins = [1,200,400,600,800,1000,1200]\nplt.hist(x, bins=bins)\nplt.xlabel('Length of comments')\nplt.ylabel('Number of comments')       \nplt.axis([0, 1200, 0, 90000])\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comments classified as toxic,severe_toxic,.. etc depending on numbers of comments and their lengths"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.zeros(label.shape)\nfor ix in range(comment.shape[0]):\n    l = len(comment[ix])\n    if label[ix][0] :\n        y[ix][0] = l\n    if label[ix][1] :\n        y[ix][1] = l\n    if label[ix][2] :\n        y[ix][2] = l\n    if label[ix][3] :\n        y[ix][3] = l\n    if label[ix][4] :\n        y[ix][4] = l\n    if label[ix][5] :\n        y[ix][5] = l\n\nlabelsplt = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\ncolor = ['red','green','blue','yellow','orange','chartreuse']        \nplt.hist(y,bins = bins,label = labelsplt,color = color)\nplt.axis([0, 1200, 0, 8000])\nplt.xlabel('Length of comments')\nplt.ylabel('Number of comments') \nplt.legend()\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remove excessive length comments**\n\nThreshold = 400 words"},{"metadata":{"trusted":true},"cell_type":"code","source":"comments = []\nlabels = []\n\nfor i in range(comment.shape[0]):\n    if len(comment[i])<=400:\n        comments.append(comment[i])\n        labels.append(label[i])\n\nlabels = np.asarray(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(comments))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Preprocessing**\n\nPreprocessing involves the following steps\n\n* Removing Punctuations and other special characters\n* Splitting the comments into individual words\n* Removing Stop Words\n* Stemming and Lemmatising\n* Applying Count Vectoriser\n* Splitting dataset into Training and Testing\n\n**Removing Punctuations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nprint(string.punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing apostrophe character(') to prevent, words won't,don't.. to be coverted into wont,dont... and adding \"0123456789\". \npunctuation_edit = string.punctuation.replace('\\'','') +\"0123456789\"\nprint (punctuation_edit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# maketrans() returns a translation table that maps each character in the punctuation_edit into the character at the same position in the outtab string.\nouttab = \"                                         \"\ntrantab = str.maketrans(punctuation_edit, outtab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Removing Stop Words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n# Initialize the stopwords\nstop_words = stopwords.words('english')\nprint(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words.append('')\nfor x in range(ord('b'), ord('z')+1):\n    stop_words.append(chr(x))\nprint(stop_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stemming and Lemmatizing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer, WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create objects for stemmer and lemmatizer\nlemmatiser = WordNetLemmatizer()\nstemmer = PorterStemmer()\n#download words from wordnet library\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can now, loop once through all the comments applying :**\n\n* punctuation removal\n* splitting the words by space\n* applying stemmer and lemmatizer\n* recombining the words again for further processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(comments)):\n    comments[i] = comments[i].lower().translate(trantab)\n    l = []\n    for word in comments[i].split():\n        l.append(stemmer.stem(lemmatiser.lemmatize(word,pos=\"v\")))\n    comments[i] = \" \".join(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(comments), len(comments)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Applying Count Vectorizer\n\nHere we can finally convert our comments into a matrix of token counts, which signifies the number of times it occurs.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import required library\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#create object supplying our custom stop words\ncount_vector = CountVectorizer(stop_words=stop_words)\n\ntf = count_vector.fit_transform(comments).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting dataset into training and testing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def shuffle(matrix, target, test_proportion):\n    ratio = int(matrix.shape[0]/test_proportion)\n    X_train = matrix[ratio:,:]\n    X_test =  matrix[:ratio,:]\n    Y_train = target[ratio:,:]\n    Y_test =  target[:ratio,:]\n    return X_train, X_test, Y_train, Y_test\n\nX_train, X_test, Y_train, Y_test = shuffle(tf, labels,3)\n\nprint(X_test.shape)\nprint(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finalising Evaluation Metric - Example based metrics**\n\n**1. Label based metrics**\n\nIt includes one-error, average precision, etc. These are calculated separately for each of the labels, and then averaged for all without taking into account any relation between the labels.\n\n**2. Example based metrics**\n\nIt include accuracy, hamming loss, etc.These are calculated for each example and then averaged across the test set.\n\n**defining the evaluation metrics**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import hamming_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\ndef evaluate_score(Y_test,predict): \n    loss = hamming_loss(Y_test,predict)\n    print(\"Hamming_loss : {}\".format(loss*100))\n    accuracy = accuracy_score(Y_test,predict)\n    print(\"Accuracy : {}\".format(accuracy*100))\n    try : \n        loss = log_loss(Y_test,predict)\n    except :\n        loss = log_loss(Y_test,predict.toarray())\n    print(\"Log_loss : {}\".format(loss))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Applying algorithmic techniques to build a multi-label classifier**\n\n**1. Problem transformation methods** like binary relevance method, label power set, classifier chain and random k-label sets (RAKEL) algorithm \n\n**2. Adaptation algorithms** like the AdaBoost MH, AdaBoost MR, k-nearest neighbours, decision trees and back propagation-multi label neural networks(BP-MLL).\n\n**I. Problem Transformation Methods**"},{"metadata":{},"cell_type":"markdown","source":"( Using scikit-multilearn library is used for implementing the various methods. eg.: Multinomial Naive Bayes, Gaussian Naive Bayes and SVC.)"},{"metadata":{},"cell_type":"markdown","source":"**1. Binary Relevance (BR) Method with MultinomialNB classifiers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install scikit-multilearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n#clf will be the list of the classifiers for all the 6 labels\n# each classifier is fit with the training data and corresponding classifier\nclf = []\nfor ix in range(6):\n    clf.append(MultinomialNB())\n    clf[ix].fit(X_train,Y_train[:,ix])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict list contains the predictions, it is transposed later to get the proper shape\npredict = []\nfor ix in range(6):\n    predict.append(clf[ix].predict(X_test))\n\npredict = np.asarray(np.transpose(predict))\nprint(predict.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_score(Y_test,predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. BR Method with SVM classifier (from scikit-multilearn)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create and fit classifier\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.svm import SVC\nclassifier = BinaryRelevance(classifier = SVC(), require_dense = [False, True])\nclassifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\npredictions = classifier.predict(X_test)\n#calculate scores\nevaluate_score(Y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. BR Method with Multinomial classifier (from scikit-multilearn)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create and fit classifier\nclassifier = BinaryRelevance(classifier = MultinomialNB(), require_dense = [False, True])\nclassifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\npredictions = classifier.predict(X_test)\n#calculate scores\nevaluate_score(Y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. BR Method with GausseanNB classifier (from scratch)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n#create and fit classifiers\nclf = []\nfor ix in range(6):\n    clf.append(GaussianNB())\n    clf[ix].fit(X_train,Y_train[:,ix])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\npredict = []\nfor ix in range(6):\n    predict.append(clf[ix].predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate scores\npredict = np.asarray(np.transpose(predict))\nevaluate_score(Y_test,predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. Classifier chain with MultinomialNB classifier (from scikit-multilearn)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create and fit classifier\nfrom skmultilearn.problem_transform import ClassifierChain\nclassifier = ClassifierChain(MultinomialNB())\nclassifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\npredictions = classifier.predict(X_test)\n#calculate scores\nevaluate_score(Y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. Label Powerset with MultinomialNB classifier (from scikit-multilearn)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create and fit classifier\nfrom skmultilearn.problem_transform import LabelPowerset\nclassifier = LabelPowerset(MultinomialNB())\nclassifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\npredictions = classifier.predict(X_test)\nevaluate_score(Y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**II. Adaptation Algorithms**\n\n**7. MLkNN with k=2 (from scikit-multilearn) (Multi label version of K-nearest neighbours)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create and fit classifier\nfrom skmultilearn.adapt import MLkNN\nclassifier = MLkNN(k=2)\nclassifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\npredictions = classifier.predict(X_test)\n#calculate scores\nevaluate_score(Y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**8. BP-MLL Neural Networks (from scratch) (Back propagation Multi-label Neural Networks)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define model architecture\nmodel = Sequential()\nmodel.add(Dense(4, activation='relu', input_dim = X_train.shape[1]))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(6, activation='softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compile model with all parameters set\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit using check pointer\nfrom keras.callbacks import ModelCheckpoint  \n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.myneural.h5py', \n                               verbose=1, save_best_only=True)\nmodel.fit(X_train, Y_train, epochs=10, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\npredict = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate score\nloss = log_loss(Y_test,predict)\nprint(\"Log_loss : {}\".format(loss))\npredict = np.round(predict)\nloss = hamming_loss(Y_test,predict)\nprint(\"Hamming_loss : {}\".format(loss*100))\naccuracy = accuracy_score(Y_test,predict)\nprint(\"Accuracy : {}\".format(accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**improving the BP-MLL model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras import optimizers\n\n#define parameters for using in param grid\nnodes = [16, 32, 64] # number of nodes in the hidden layer\nlrs = [0.001, 0.002, 0.003] # learning rate, default = 0.001\nepochs = [10,20,30]\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(nodes=10,lr=0.001):\n    model = Sequential()\n    model.add(Dense(nodes, activation='relu', input_dim = X_train.shape[1]))\n    model.add(Dropout(0.3))\n    model.add(Dense(6, activation='softmax'))\n    opt = optimizers.RMSprop(lr=lr)\n    model.compile(optimizer=opt,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n    return model\n\nmodel = KerasClassifier(build_fn=create_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#start fitting process\nparam_grid = dict(epochs=epochs,nodes=nodes, lr=lrs)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1,refit=True,verbose=2)\ngrid_result = grid.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best estimator : {}'.format (grid.best_estimator_))\nprint('Best score : {}'.format(grid.best_score_))\nprint('Best params : {}'.format(grid.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\npredictions = grid.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\npredict = grid.predict_proba(X_test)\nprint(predict.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate score\nloss = log_loss(Y_test,predict)\nprint(\"Log_loss : {}\".format(loss))\npredict = np.round(predict)\nloss = hamming_loss(Y_test,predict)\nprint(\"Hamming_loss : {}\".format(loss*100))\naccuracy = accuracy_score(Y_test,predict)\nprint(\"Accuracy : {}\".format(accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualisation**\n\nLet us have a plot showing the hamming-loss and log-loss of different models, which we selected."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.cm as cm\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hamming Loss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = ['BR-MultNB','BR-GausNB','BR-SVC','CC-MultNB','LP-MultNB','BP-MLL-ini','BP-MLL-fin']\ny = [3.27,20.74,4.26,3.56,3.17,13.96,15.158]\ncolors = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\nplt.ylabel('Hamming-Loss')\nplt.xlabel('Model-details')\nplt.xticks(rotation=90)\nfor i in range(len(y)):\n    plt.bar(x[i], y[i], color=next(colors))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Log Loss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = ['BR-MultNB','BR-GausNB','BR-SVC','CC-MultNB','LP-MultNB','BP-MLL-ini','BP-MLL-fin']\ny = [1.92,1.422,0.46,1.5,1.47,0.36,0.35]\ncolors = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\nplt.ylabel('Log-Loss')\nplt.xlabel('Model-details')\nplt.xticks(rotation=90)\nfor i in range(len(y)):\n    plt.bar(x[i], y[i], color=next(colors))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}