{"cells":[{"metadata":{},"cell_type":"markdown","source":"Introduction\n\nThe concept of Long Short Term Memory networks were first envisioned by Sepp Hochreiter and JÃ¼rgen Schmidhuber in 1997 which provided a huge advancement within the world of Reccurent Neural Networks. LSTMs utilize gates which learn to pass on certain parts of a given input to be utilized when determining an output.\n\nQuora is a website which hosts an online platform where questions can be asked and then be asnwered by the online community. The the community intends to answer questions posed by curious individuals from topics ranging from relgion to technology. A majority of the answers provided are well intedned, but infrequently, insencere responses are posted which provide no value to the question asked. If Quora is capable of identifying these types of responses, it can make sure it's community is a benefical environment for all users. The following analysis intends to identify inscencere questions that have been posted within the Quora Community by utilizing Convolutional Networks in tangent with LSTMs.\n\nBelow, we import the required modules for the analysis.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\n\nfrom sklearn.metrics import roc_curve, auc,  f1_score\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport keras\nfrom sklearn import metrics\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D, BatchNormalization, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D,CuDNNLSTM\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, TimeDistributed\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.preprocessing import text, sequence\n\nfrom nltk.corpus import stopwords\nimport string \n\nfrom keras.callbacks import ModelCheckpoint\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!du -l ../input/*","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Preproccessing\nThe train data set consists of 1.31 million records for the training data where the commentary is preclassified as toxic or non toxic. The test data set consists of 376 thousand records.\nIn order for computers to be able to process english text, they must convert the sentences to vectors. We will first tokenize the sentences where each word is given a value. Each sentence must be the same length before entering the neural network which is why we utilize the pad_sequences function.\nWe will now read in the data and split it into train and test, tokenize the comments, and pad the comments for preprocessing."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\n#df['non_toxic'] = df.apply(lambda x: 1 if x.toxic == 0 & x.severe_toxic == 0 & x.obscene == 0 & x.threat == 0 & x.insult == 0 & x.identity_hate == 0 else 0 , axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\n#test_y =   pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv')\n#print(test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\ndf['comment_text'] = df.apply(lambda x: x.comment_text.lower(), axis=1)\ndf['comment_text'] = df.apply(lambda x: re.sub(r'\\d+', '', x.comment_text), axis=1)\ndf['comment_text'] = df.apply(lambda x: x.comment_text.translate(str.maketrans('', '', string.punctuation)),axis=1)\ndf['comment_text'] = df.apply(lambda x: x.comment_text.strip(),axis=1)\ndf['comment_text'] = df.apply(lambda x: x.comment_text.rstrip(),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['comment_text'] = test_df.apply(lambda x: x.comment_text.lower(), axis=1)\ntest_df['comment_text'] = test_df.apply(lambda x: re.sub(r'\\d+', '', x.comment_text), axis=1)\ntest_df['comment_text'] = test_df.apply(lambda x: x.comment_text.translate(str.maketrans('', '', string.punctuation)),axis=1)\ntest_df['comment_text'] = test_df.apply(lambda x: x.comment_text.strip(),axis=1)\ntest_df['comment_text'] = test_df.apply(lambda x: x.comment_text.rstrip(),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filename = '../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz'\n#model = KeyedVectors.load_word2vec_format(filename, binary=True)\n\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=2018)\n#test_df =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\n\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"comment_text\"].fillna(\"_na_\").values\nval_X = val_df[\"comment_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"comment_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n\n## Get the target values\ntrain_y = train_df[['toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate']].values\nval_y = val_df[['toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate']].values\n#test_y = test_df[['toxic', 'severe_toxic', 'obscene', 'threat',\n#       'insult', 'identity_hate','non_toxic']].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nEMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n\nmax_features= 50000\nmaxlen = 100\nembed_size = 300\n\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n    \ndef get_embed_mat(EMBEDDING_FILE, max_features,embed_dim):\n    # word vectors\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n    print('Found %s word vectors.' % len(embeddings_index))\n\n    # embedding matrix\n    word_index = tokenizer.word_index\n    num_words = min(max_features, len(word_index) + 1)\n    all_embs = np.stack(embeddings_index.values()) #for random init\n    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n                                        (num_words, embed_dim))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    max_features = embedding_matrix.shape[0]\n    \n    return embedding_matrix\n\nembedding_matrix = get_embed_mat(EMBEDDING_FILE,max_features,300)\nprint(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_sizes = [1,2,3,5,10]\nnum_filters = 128\n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Reshape((maxlen, embed_size, 1))(x)\n\nmaxpool_pool = []\nfor i in range(len(filter_sizes)):\n    conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n                                 kernel_initializer='glorot_uniform', activation='relu')(x) \n    maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n\nz = Concatenate(axis=1)(maxpool_pool) \nz = TimeDistributed(Bidirectional(CuDNNLSTM(256)))(z)\nz = SpatialDropout1D(.5)(z)\nz = Bidirectional(CuDNNLSTM(256))(z)\nz = Dropout(.5)(z)\nz = BatchNormalization()(z)\n#z = Flatten()(z)\nz = Dense(1000, activation=\"relu\")(z)\nz = Dropout(.5)(z)\nz = Dense(500, activation=\"relu\")(z)\nz = Dropout(.5)(z)\nz = Dense(100, activation=\"relu\")(z)\n\noutp = Dense(6, activation=\"softmax\")(z)\n\nmodel = Model(inputs=inp, outputs=outp)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\n#from keras.callbacks import EarlyStopping, ModelCheckpoint\n#earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\nmcp_save = ModelCheckpoint('../input/mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\nhistory = model.fit(train_X, train_y, batch_size=512, epochs=30, validation_data=(val_X, val_y), callbacks = [mcp_save])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(test_X)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = pd.DataFrame(np.concatenate((np.array(test_df.id).reshape(-1,1),y_pred[:,:6]),axis=1), columns = ['id','toxic','severe_toxic','obscene','threat','insult','identity_hate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.to_csv('toxic_v2.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}