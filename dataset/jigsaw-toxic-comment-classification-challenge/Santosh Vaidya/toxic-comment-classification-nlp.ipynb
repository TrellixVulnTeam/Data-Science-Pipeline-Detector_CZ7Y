{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action=\"ignore\")\n\nimport time \n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_union\nfrom sklearn.linear_model import LogisticRegression\n\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\n\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\n%matplotlib inline\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comment_text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import regexp_tokenize   # module for tokenizing strings\nfrom nltk.tokenize import TreebankWordTokenizer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comment_text']=test['comment_text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    \n    words = [word for word in text if word not in stopwords.words('english')]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"        # Checking the count of the various types of words\n\ncols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntargets = train[cols].values\n\ntrain_df = train['comment_text']\ntest_df = test['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_counts = train[cols].sum()\n\nplt.figure(figsize=(8,5))\nax = sns.barplot(val_counts.index, val_counts.values, alpha=0.8)\n\nplt.title(\"Comments per Classes\")\nplt.xlabel(\"Various Comments Type\")\nplt.ylabel(\"Counts of the Comments\")\n\nrects = ax.patches\nlabels = val_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height+5, label, ha=\"center\", va=\"bottom\")\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nwords = ' '.join([text for text in train['comment_text'] ])\n\n\nword_cloud = WordCloud(\n                       width=1600,\n                       height=800,\n                       #colormap='PuRd', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\").generate(words)\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.title('Comments and their Nature', fontsize = 40)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwords = ' '.join([text for text in test['comment_text'] ])\n\n\nword_cloud = WordCloud(\n                       width=1600,\n                       height=800,\n                       #colormap='PuRd', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\").generate(words)\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"bilinear\")\nplt.title('Comments and their Nature', fontsize = 40)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    ngram_range=(1, 1),\n    max_features=30000)\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    ngram_range=(1, 4),\n    max_features=30000)\nvectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.fit(all_text)\ntrain_features = vectorizer.transform(train_df)\ntest_features = vectorizer.transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nsubmission = pd.DataFrame.from_dict({'id': test['id']})\nfor class_name in cols:\n    train_target = train[class_name]\n    classifier = LogisticRegression(solver='sag')\n\n    cv_score = np.mean(cross_val_score(\n        classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n    scores.append(cv_score)\n    print('CV score for class {} is {}'.format(class_name, cv_score))\n\n    classifier.fit(train_features, train_target)\n    submission[class_name] = classifier.predict_proba(test_features)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total CV score is {}'.format(np.mean(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}