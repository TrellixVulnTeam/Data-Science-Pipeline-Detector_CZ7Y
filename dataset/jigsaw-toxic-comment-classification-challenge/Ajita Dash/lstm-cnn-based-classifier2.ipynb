{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport os\nimport sys\nfrom keras import backend as K\nfrom keras.layers import Dense,Input, LSTM, Bidirectional, Embedding, TimeDistributed, SpatialDropout1D,GRU,CuDNNGRU,Dropout\nfrom keras.layers import Conv1D,GlobalMaxPooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras import losses\nfrom keras import initializers as initializers, regularizers, constraints\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nimport nltk\nimport re\nfrom keras.engine.topology import Layer\n# Any results you write to the current directory are saved as output.","execution_count":59,"outputs":[{"output_type":"stream","text":"['sample_submission.csv', 'test_labels.csv', 'train.csv', 'test.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(\"../input/train.csv\")\ndf_test=pd.read_csv(\"../input/test.csv\")\nprint(df_train.shape)\nprint(df_test.shape)","execution_count":2,"outputs":[{"output_type":"stream","text":"(159571, 8)\n(153164, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X=df_train['comment_text']\ntest_X=df_test['comment_text']\ntrain_Y=df_train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['threat']==1]['comment_text'].head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"79      Hi! I am back again!\\nLast warning!\\nStop undo...\n176     I think that your a Fagget get a oife and burn...\n600     I'm also a sock puppet of this account...SUPRI...\n802     Fuck you, Smith. Please have me notified when ...\n1017    WOULDN'T BE THE FIRST TIME BITCH. FUCK YOU I'L...\nName: comment_text, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation\n\ncontractions = {\n\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"can not\",\"can't've\": \"can not have\",\"'cause\": \"because\",\"could've\": \"could have\",\n\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he had\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n\"i'd\": \"I would\",\"i'd've\": \"I would have\",\"i'll\": \"I shall / I will\",\"i'll've\": \"I shall have / I will have\",\"i'm\": \"I am\",\n\"i've\": \"I have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they had\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\n\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\n\"who'll've\":\"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\n\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\",\n\"&lt;3\": \" good \",\":d\": \" good \",\":dd\": \" good \",\":p\": \" good \",\"8)\": \" good \",\":-)\": \" good \", \":)\": \" good \",\";)\": \" good \",\n \"(-:\": \" good \",\"(:\": \" good \",\"yay!\": \" good \",\"yay\": \" good \",\"yaay\": \" good \",\"yaaay\": \" good \",\"yaaaay\": \" good \",\n\"yaaaaay\": \" good \",\":/\": \" bad \",\":&gt;\": \" sad \",\":')\": \" sad \",\":-(\": \" bad \",\":(\": \" bad \", \":s\": \" bad \",\":-s\": \" bad \",\n\"&lt;3\": \" heart \",\":d\": \" smile \",\":p\": \" smile \",\":dd\": \" smile \",\"8)\": \" smile \", \":-)\": \" smile \", \":)\": \" smile \",\n\";)\": \" smile \",\"(-:\": \" smile \",\"(:\": \" smile \",\":/\": \" worry \",\":&gt;\": \" angry \", \":')\": \" sad \",\":-(\": \" sad \",\":(\": \" sad \",\n\":s\": \" sad \", \":-s\": \" sad \",r\"\\br\\b\": \"are\",r\"\\bu\\b\": \"you\",r\"\\bhaha\\b\": \"ha\",r\"\\bhahaha\\b\": \"ha\",r\"\\bdon't\\b\": \"do not\",\nr\"\\bdoesn't\\b\": \"does not\",r\"\\bdidn't\\b\": \"did not\",r\"\\bhasn't\\b\": \"has not\",r\"\\bhaven't\\b\": \"have not\",r\"\\bhadn't\\b\": \"had not\",\nr\"\\bwon't\\b\": \"will not\",r\"\\bwouldn't\\b\": \"would not\",r\"\\bcan't\\b\": \"can not\",r\"\\bcannot\\b\": \"can not\",r\"\\bi'm\\b\": \"i am\",\n\"m\": \"am\",\"r\": \"are\",\"u\": \"you\",\"haha\": \"ha\",\"hahaha\": \"ha\",\"m\": \"am\"}\n\ndef remove_punctuation(sent):\n    l=[]\n    for char in sent:\n        if char not in punctuation:\n            l.append(char)\n        else:\n            l.append(' ')\n    return ''.join(l)\n\ndef lowerr(sent):\n    l=[]\n    doc=sent.split()\n    for word in doc:\n        if not word.islower():\n            word=word.lower()\n        if word[:4]=='http' or word[:3]=='www':\n            continue\n        if word in contractions.keys():\n            word=contractions[word]\n            l.extend(word.split())\n        else:\n            l.append(word)\n    return ' '.join(l)\n\ndef remove_non_ascii(sent):\n    return sent.encode('ascii', 'ignore').decode('ascii')\n\ndef remove_noise(input_text):\n    text = re.sub('\\(talk\\)(.*)\\(utc\\)','',input_text)\n    text = text.split()\n    text = [re.sub('[\\d]+','',x) for x in text]\n    return ' '.join(text)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_sent=[]\nfor sent in train_X:\n    sent=remove_non_ascii(sent)\n    sent=lowerr(sent)\n    sent=remove_noise(sent)\n    sent=remove_punctuation(sent)\n    processed_sent.append(sent)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_sent[:10]","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"['explanation why the edits made under my username hardcore metallica fan were reverted  they were not vandalisms  just closure on some gas after i voted at new york dolls fac  and please do not remove the template from the talk page since I am retired now    ',\n 'd aww  he matches this background colour I am seemingly stuck with  thanks ',\n 'hey man  I am really not trying to edit war  it is just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page  he seems to care more about the formatting than the actual info ',\n '  more i can not make any real suggestions on improvement   i wondered if the section statistics should be later on  or a subsection of   types of accidents    i think the references may need tidying so that they are all in the exact same format ie date format etc  i can do that later on  if no one else does first   if you have any preferences for formatting style on references or want to do it yourself please let me know  there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up  it is listed in the relevant form eg wikipedia good article nominations transport  ',\n 'you  sir  are my hero  any chance you remember what page that is on ',\n '  congratulations from me as well  use the tools well  talk  ',\n 'cocksucker before you piss around on my work',\n 'your vandalism to the matt shirvington article has been reverted  please do not do it again  or you will be banned ',\n 'sorry if the word  nonsense  was offensive to you  anyway  I am not intending to write anything in the article wow they would jump on me for vandalism   I am merely requesting that it be more encyclopedic so one can use it for school as a reference  i have been to the selective breeding page but it is almost a stub  it points to  animal breeding  which is a short messy article that gives you no info  there must be someone around with expertise in eugenics     ',\n 'alignment on this subject and which are contrary to those of dulithgow']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_sent2=[]\nfor sent in test_X:\n    sent=remove_non_ascii(sent)\n    sent=lowerr(sent)\n    sent=remove_noise(sent)\n    sent=remove_punctuation(sent)\n    processed_sent2.append(sent)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxwords=100000\ntok=text.Tokenizer(maxwords)\ntok.fit_on_texts(list(train_X)+list(test_X))\ntrain_X_sent=tok.texts_to_sequences(processed_sent)\ntest_X_sent=tok.texts_to_sequences(processed_sent2)\nmaxwords=min(maxwords,len(tok.word_index)+1)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['token_comments']=train_X_sent\ndf_train['len_token_comments']=[len(sent) for sent in train_X_sent]\ndf_train['len_token_comments'].replace(0,np.nan,inplace=True)\nprint(np.sum(df_train.isnull()))","execution_count":23,"outputs":[{"output_type":"stream","text":"id                     0\ncomment_text           0\ntoxic                  0\nsevere_toxic           0\nobscene                0\nthreat                 0\ninsult                 0\nidentity_hate          0\ntoken_comments         0\nlen_token_comments    35\ndtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2_train=df_train.dropna()\ndf2_train['len_token_comments']=[len(doc) for doc in df2_train['comment_text']]","execution_count":25,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(max(df2_train['len_token_comments']))\nprint(min(df2_train['len_token_comments']))\nprint(np.mean(df2_train['len_token_comments']))","execution_count":28,"outputs":[{"output_type":"stream","text":"5000\n6\n394.1470514492027\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_senten_len=400\ntrain_X_sent=df2_train['token_comments']\ntrain_X_sent=sequence.pad_sequences(train_X_sent,maxlen=max_senten_len)\ntest_X_sent=sequence.pad_sequences(test_X_sent,maxlen=max_senten_len)","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Y=df2_train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n#train_Y=np.reshape(train_Y,(train_Y.shape[0],1,train_Y.shape[1]))\nprint(train_Y.shape)","execution_count":47,"outputs":[{"output_type":"stream","text":"(159536, 6)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMB_DIM=300\nemb_layer=Embedding(maxwords,EMB_DIM,input_length=max_senten_len)\nInp=Input((max_senten_len,))\nsent=emb_layer(Inp)\nsent=Conv1D(128,5)(sent)\nsent=GlobalMaxPooling1D()(sent)\n#sent=Bidirectional(CuDNNGRU(128,return_sequences=False))(sent)\n#sent=Dropout(rate=0.2)(sent)\nsent=Dense(32,activation='relu')(sent)\npred=Dense(6,activation='sigmoid')(sent)\nmodel=Model(Inp,pred)","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X_sent.shape","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"(159571, 400)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nmodel.fit(train_X_sent,train_Y,epochs=5,validation_split=0.2,batch_size=128)","execution_count":62,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nTrain on 127628 samples, validate on 31908 samples\nEpoch 1/5\n127628/127628 [==============================] - 30s 234us/step - loss: 0.0725 - acc: 0.9748 - val_loss: 0.0463 - val_acc: 0.9825\nEpoch 2/5\n127628/127628 [==============================] - 27s 215us/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0460 - val_acc: 0.9822\nEpoch 3/5\n127628/127628 [==============================] - 28s 216us/step - loss: 0.0302 - acc: 0.9881 - val_loss: 0.0481 - val_acc: 0.9820\nEpoch 4/5\n127628/127628 [==============================] - 27s 215us/step - loss: 0.0230 - acc: 0.9909 - val_loss: 0.0544 - val_acc: 0.9819\nEpoch 5/5\n127628/127628 [==============================] - 27s 215us/step - loss: 0.0175 - acc: 0.9934 - val_loss: 0.0660 - val_acc: 0.9818\n","name":"stdout"},{"output_type":"execute_result","execution_count":62,"data":{"text/plain":"<keras.callbacks.History at 0x7fb9181a53c8>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X_sent=np.asarray(test_X_sent)\ny_pred=model.predict(test_X_sent,batch_size=1024,verbose=1)","execution_count":63,"outputs":[{"output_type":"stream","text":"153164/153164 [==============================] - 4s 26us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub=pd.read_csv(\"../input/sample_submission.csv\")\ndf_sub[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]=y_pred","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.to_csv(\"submission.csv\",index=False)","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}