{"cells":[{"metadata":{"id":"wmI3qlP4lSiN","outputId":"05e7496f-808a-475f-e0af-9cea4dc7fc4f","trusted":false},"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","execution_count":null,"outputs":[]},{"metadata":{"id":"Z1G_QFzRlkLo","outputId":"828d1e6c-3abe-4c1a-f528-9b5fbcf0b592","trusted":false},"cell_type":"code","source":"import imblearn","execution_count":null,"outputs":[]},{"metadata":{"id":"pkTbZEXwlr0k","outputId":"d985b32d-16e1-4339-a975-f67311bc6603","trusted":false},"cell_type":"code","source":"pip install Unidecode","execution_count":null,"outputs":[]},{"metadata":{"id":"qy5qM72XltXF","outputId":"05ca76cc-cd75-43b6-cb4d-9610d98942c0","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport os\n\nimport collections\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nfrom nltk.tokenize import word_tokenize\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import KFold \nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, fbeta_score, multilabel_confusion_matrix, confusion_matrix, plot_confusion_matrix, f1_score","execution_count":null,"outputs":[]},{"metadata":{"id":"CxpYIqJLlvgp","trusted":false},"cell_type":"code","source":"data = pd.read_csv('/content/drive/My Drive/train.csv/train.csv')\ndata_dum = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"2ShA07a0lxpH","trusted":false},"cell_type":"code","source":"comments = data_dum['comment_text'].to_numpy()\nlabels = data_dum[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"id":"W2dcGcGIl0TW","trusted":false},"cell_type":"code","source":"data_ = []\nfor ind in range(len(labels)):\n    num = np.count_nonzero(labels[ind])\n    if num == 0:\n      data_.append([comments[ind],0])\n    else:\n      data_.append([comments[ind],1])","execution_count":null,"outputs":[]},{"metadata":{"id":"S8OhFHXql2NN","trusted":false},"cell_type":"code","source":"df = pd.DataFrame(data_,columns=['comment','label'])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4vr1g1zXl4ji","outputId":"6e14a0b5-aa3e-46e7-afd1-067d832fd9a9","trusted":false},"cell_type":"code","source":"df['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"QPkrRhb3l6IY","trusted":false},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n                   \"this's\": \"this is\",\n                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n                       \"here's\": \"here is\",\n                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } ","execution_count":null,"outputs":[]},{"metadata":{"id":"DByjEZcYl8dV","trusted":false},"cell_type":"code","source":"import codecs\nimport unidecode\nimport re\nimport spacy\nnlp = spacy.load('en')\n\ndef spacy_cleaner(text):\n    try:\n        decoded = unidecode.unidecode(codecs.decode(text, 'unicode_escape'))\n    except:\n        decoded = unidecode.unidecode(text)\n    apostrophe_handled = re.sub(\"â€™\", \"'\", decoded)\n    expanded = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\n    parsed = nlp(expanded)\n    final_tokens = []\n    for t in parsed:\n        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\n            pass\n        else:\n            if t.lemma_ == '-PRON-':\n                final_tokens.append(str(t))\n            else:\n                sc_removed = re.sub(\"[^a-zA-Z]\", '', str(t.lemma_))\n                if len(sc_removed) > 1:\n                    final_tokens.append(sc_removed)\n    joined = ' '.join(final_tokens)\n    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\n    return spell_corrected","execution_count":null,"outputs":[]},{"metadata":{"id":"l76kkEEUl_FB","trusted":false},"cell_type":"code","source":"df['clean_text'] = [spacy_cleaner(t) for t in df.comment]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression","execution_count":null},{"metadata":{"id":"vaxQV9yamCK0","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntvec = TfidfVectorizer(stop_words=None, max_features=100000, ngram_range=(1, 3))\nlr = LogisticRegression()\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef lr_cv(splits, X, Y, pipeline, average_method):\n    \n    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)\n    accuracy = []\n    precision = []\n    recall = []\n    f1 = []\n    for train, test in kfold.split(X, Y):\n        lr_fit = pipeline.fit(X[train], Y[train])\n        prediction = lr_fit.predict(X[test])\n        scores = lr_fit.score(X[test],Y[test])\n        \n        accuracy.append(scores * 100)\n        precision.append(precision_score(Y[test], prediction, average=average_method)*100)\n        print('              non-toxic    toxic')\n        print('precision:',precision_score(Y[test], prediction, average=None))\n        recall.append(recall_score(Y[test], prediction, average=average_method)*100)\n        print('recall:   ',recall_score(Y[test], prediction, average=None))\n        f1.append(f1_score(Y[test], prediction, average=average_method)*100)\n        print('f1 score: ',f1_score(Y[test], prediction, average=None))\n        print('-'*50)\n\n    print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(accuracy), np.std(accuracy)))\n    print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n    print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n    print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1), np.std(f1)))","execution_count":null,"outputs":[]},{"metadata":{"id":"cKUJRBKAydmn","outputId":"7c27c9d7-06e0-46f5-d6ef-97733173483f","trusted":false},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\noriginal_pipeline = Pipeline([\n    ('vectorizer', tvec),\n    ('classifier', lr)\n])\nlr_cv(3, df.clean_text, df.label, original_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{"id":"7Njd2J0fyh76","trusted":false},"cell_type":"code","source":"from imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\nROS_pipeline = make_pipeline(tvec, RandomOverSampler(random_state=777),lr)\nSMOTE_pipeline = make_pipeline(tvec, SMOTE(random_state=777),lr)","execution_count":null,"outputs":[]},{"metadata":{"id":"RbOm_qTvzj-U","outputId":"8034cf9d-5d74-4203-ffda-1db45ad0e8c8","trusted":false},"cell_type":"code","source":"lr_cv(3, df.clean_text, df.label, ROS_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{"id":"wG24Z6nLzomm","outputId":"c3f8c61f-7ec3-4164-e2f7-e25019530053","trusted":false},"cell_type":"code","source":"lr_cv(3, df.clean_text, df.label, SMOTE_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{"id":"U-34xy4m07yJ","trusted":false},"cell_type":"code","source":"from imblearn.under_sampling import NearMiss, RandomUnderSampler\nRUS_pipeline = make_pipeline(tvec, RandomUnderSampler(random_state=777),lr)","execution_count":null,"outputs":[]},{"metadata":{"id":"QGk5jRFy2kKz","outputId":"3dee947c-749c-49bd-e147-8bc7434ce75d","trusted":false},"cell_type":"code","source":"lr_cv(3, df.clean_text, df.label, RUS_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM","execution_count":null},{"metadata":{"id":"-l2e43Pq2npn","trusted":false},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntvec = TfidfVectorizer(stop_words=None, max_features=100000, ngram_range=(1, 3))\nlr = svm.SVC(kernel='linear')\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef lr_v(splits, X, Y, pipeline, average_method):\n    \n    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)\n    accuracy = []\n    precision = []\n    recall = []\n    f1 = []\n    for train, test in kfold.split(X, Y):\n        lr_fit = pipeline.fit(X[train], Y[train])\n        prediction = lr_fit.predict(X[test])\n        scores = lr_fit.score(X[test],Y[test])\n        \n        accuracy.append(scores * 100)\n        precision.append(precision_score(Y[test], prediction, average=average_method)*100)\n        print('              non-toxic    toxic')\n        print('precision:',precision_score(Y[test], prediction, average=None))\n        recall.append(recall_score(Y[test], prediction, average=average_method)*100)\n        print('recall:   ',recall_score(Y[test], prediction, average=None))\n        f1.append(f1_score(Y[test], prediction, average=average_method)*100)\n        print('f1 score: ',f1_score(Y[test], prediction, average=None))\n        print('-'*50)\n\n    print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(accuracy), np.std(accuracy)))\n    print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n    print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n    print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1), np.std(f1)))","execution_count":null,"outputs":[]},{"metadata":{"id":"O3erwB__2rpj","outputId":"627f5599-924a-4995-ee74-6a9165ec81a6","trusted":false},"cell_type":"code","source":"lr_v(3, df.clean_text, df.label, original_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{"id":"7Z9S31TL2rn2","outputId":"cba3e606-d4dd-4ccc-ec00-e693b34c7aad","trusted":false},"cell_type":"code","source":"lr_v(3, df.clean_text, df.label, ROS_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{"id":"WdBg0oQI2rmn","trusted":false},"cell_type":"code","source":"lr_v(3, df.clean_text, df.label, SMOTE_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{"id":"RsGPfCMZ2rki","trusted":false},"cell_type":"code","source":"lr_v(3, df.clean_text, df.label, RUS_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes\n","execution_count":null},{"metadata":{"id":"g5VluHAQ2rho","trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntvec = TfidfVectorizer(stop_words=None, max_features=100000, ngram_range=(1, 3))\nlr = GaussianNB()\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef lr_(splits, X, Y, pipeline, average_method):\n    \n    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)\n    accuracy = []\n    precision = []\n    recall = []\n    f1 = []\n    for train, test in kfold.split(X, Y):\n        lr_fit = pipeline.fit(X[train], Y[train])\n        prediction = lr_fit.predict(X[test])\n        scores = lr_fit.score(X[test],Y[test])\n        \n        accuracy.append(scores * 100)\n        precision.append(precision_score(Y[test], prediction, average=average_method)*100)\n        print('              non-toxic    toxic')\n        print('precision:',precision_score(Y[test], prediction, average=None))\n        recall.append(recall_score(Y[test], prediction, average=average_method)*100)\n        print('recall:   ',recall_score(Y[test], prediction, average=None))\n        f1.append(f1_score(Y[test], prediction, average=average_method)*100)\n        print('f1 score: ',f1_score(Y[test], prediction, average=None))\n        print('-'*50)\n\n    print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(accuracy), np.std(accuracy)))\n    print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n    print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n    print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1), np.std(f1)))","execution_count":null,"outputs":[]},{"metadata":{"id":"wFuQD7iR2rei","trusted":false},"cell_type":"code","source":"lr_(3, df.clean_text, df.label, original_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{"id":"1Ew495li2rZa","trusted":false},"cell_type":"code","source":"lr_(3, df.clean_text, df.label, ROS_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{"id":"lDJ1G0WX3YUy","trusted":false},"cell_type":"code","source":"lr_(3, df.clean_text, df.label, SMOTE_pipeline, 'macro')","execution_count":null,"outputs":[]},{"metadata":{"id":"cG6utetX3YPf","trusted":false},"cell_type":"code","source":"lr_(3, df.clean_text, df.label, RUS_pipeline, 'macro')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}