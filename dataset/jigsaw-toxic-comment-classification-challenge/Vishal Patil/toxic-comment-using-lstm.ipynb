{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip '/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip'\n!unzip '/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip'\n!unzip '/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip'\n!unzip '/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Import Libraries \nimport re\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom sklearn.svm import LinearSVC\nimport seaborn as sns\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Input\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Reading csv data files using pandas dataframe \n\ntrain = pd.read_csv(\"./train.csv\", encoding = \"ISO-8859-1\")\ntest = pd.read_csv(\"./test.csv\", encoding = \"ISO-8859-1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Checking nulls in both the datasets\n\nprint(\"Nulls in training dataset\",train.isnull().sum())\nprint(\"\\nNulls in test dataset\",test.isnull().sum())\n\n### There are no missing values in both the datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Before cleaning the dataset I would like to perform EDA(Exploratory data analysis) by performing data visualization to understand\n### the distribution of different classes. I will be performing EDA on training dataset\n\ncategorywise_data = train.drop(['id', 'comment_text'], axis=1)     ### Removed unnecessary columns - id and comment_text\ncounts_category = []                                               ### A list that contains tuple which consists of class label and number of comments for that particular class \ncategories = list(categorywise_data.columns.values)\nfor i in categories:\n    counts_category.append((i, categorywise_data[i].sum()))\n    \ndataframe = pd.DataFrame(counts_category, columns=['Labels', 'number_of_comments'])   ### Dataframe made up of category and total number of comments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Visualization 1\n\n### Bar graph of total number of comments per label \n### This visualization is helpful in identifying the total number of comments per label\ndataframe.plot(x='Labels', y='number_of_comments', kind='bar',figsize=(8,8))\nplt.title(\"Number of comments per category\")\nplt.ylabel('No. of Occurrences', fontsize=12)\nplt.xlabel('Labels', fontsize=12)\n\n### From the below graph we can observe that most of the comments having toxic label. \n### Threat label is having lowest no. of comments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Visualization 2\n\n### Bar graph of Total No. of labels in a sentence against Total no. of sentences\n### This visualization is helpful in identifying whether a sentence belongs to only one category or many categories\n\ndataframe = pd.DataFrame(pd.DataFrame(train[train.columns[2:]].sum(axis=1)).reset_index()[0].value_counts())\ndataframe[\"Total no. of sentences\"]=dataframe[0]\ndataframe[\"Total No. of labels in a sentence\"]=dataframe.index\ndataframe.plot(x=\"Total No. of labels in a sentence\", y=\"Total no. of sentences\", kind='bar',figsize=(8,8))\nplt.title(\"No of comments based on the count of labels\")\nplt.ylabel('Total no. of sentences', fontsize=12)\nplt.xlabel('Total No. of labels in a sentence', fontsize=12)\ndataframe\n\n### From the below graph we can see that 1,43,346 out of 1,59,571 sentences does not have any labels(class 0).\n### we can observe that a single sentence can have multiple labels. It can be a toxic sentence or it can be a toxic as well as obscene senetence.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Visualization 3\n\n### Graph of individual class against the total no. of labelled and unlabelled sentences for the same class\n\nfig, plots = plt.subplots(2,3,figsize=(15,12))\nplot1, plot2, plot3, plot4, plot5, plot6 = plots.flatten()\nsns.countplot(train['obscene'], ax = plot1)\nsns.countplot(train['threat'], ax = plot2)\nsns.countplot(train['insult'], ax = plot3)\nsns.countplot(train['identity_hate'], ax = plot4)\nsns.countplot(train['toxic'], ax = plot5)\nsns.countplot(train['severe_toxic'], ax = plot6)\n\n### From this graph it can be concluded that every class(category) is having higher no. of unlabelled sentences(0) as compared\n### to labelled sentences(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Visualization 4\n\n### Correlation between different variables\n### Correlation helps us finding relationship/dependency between different variables. \n\ntarget_data = train.drop(['id', 'comment_text'], axis=1)\ncorrMatrix = target_data.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()\n\n### Correlation coefficient ranges from -1 to 1. Values always range between -1 imply strong negative relationship\n### between variables and +1 imply a strong positive relationship between variables. \n### Values at or close to zero imply weak or no linear relationship. \n### From the correlation matrix(graph), it can be concluded that some labels are highely correlated. Those varibles are mentioned below \n### (Correlation coefficient for insult-obscene is 0.74, Correlation coefficient for toxic-obscene is 0.68 and Correlation coefficient for toxic-insult is 0.65).","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### From visualization we can conclude following things\n### 1) A single sentence can have multiple labels (categories)\n### 2) Some labels are highely correlated to each other\n### 3) This is multi label classification problem ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Data cleaning/Preparation \n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)      ### conversion of contraction words to expanded words\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)                                                 ### removing non-word characters\n    text = re.sub('[^A-Za-z\\' ]+', '',text)                                        ### removing all non-alphanumeric values(Except single quotes)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    text = ' '.join([word for word in text.split() if word not in (stop_words)])    ### Stopwords removal\n    return text\n\ntrain[\"comment_text\"] = train[\"comment_text\"].apply(clean_text)\ntest[\"comment_text\"] = test[\"comment_text\"].apply(clean_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Splitting up the labels and data\n### Training dataset is splitted into 2 parts. 1st part includes the training data(train_data) and 2nd part includes labels(train_label) \n### associated with the training data\n### Test dataset is having only 1 part i.e. test data which is used to predict the labels. \n\ntrain_data = train[\"comment_text\"]\ntest_data = test[\"comment_text\"]\ntrain_label=train[['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Creating corpus of words and coverting it into integer and then susbstituing it in sentences - prepare tokenizer\ntokenizer = Tokenizer(num_words = 40000) #40000 words are used here\ntokenizer.fit_on_texts(train_data)\n\n#convert each text into array of integers with help of tokenizer.\ntrain_final = tokenizer.texts_to_sequences(train_data)\ntest_final = tokenizer.texts_to_sequences(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Padding - Every sentence is unequal in length. We need to have all the sentence equal in lengths\n### If length of sentence is less than 150 then padding will increase sentence length to 150 by adding zeros, if its greater\n### than 150 then it will reduce the length of sentence to 150 by trimming the words\n\ntrain_padded =pad_sequences(train_final, maxlen=150)\ntest_padded =pad_sequences(test_final, maxlen=150)\nprint(\"Shape of training data\",train_padded.shape)\nprint(\"Shape of testing data\",test_padded.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Building a model\n### I have built a sequential model using LSTM. Initial layer contains a word embedding layer having 128 hidden units. \n### Input to the LSTM model is given in the shape of an array of (159571, 150). There are 6 main classes and every class is having 2 sub classes.\n### For every Input observation, there are 6 labels. \n### I have used 2 LSTM layer with 64 hidden units. 1 Dense layer which is an output layer. Dropout probability is set to 0.2 for\n### 2 lSTM layers which will dropout the 20 percent of neuron automatically while training.Dropout resolves the problem of overfitting. \n### Output layer contains 6 hidden units. I have used sigmoid activation function in the output layer. The reason behind using\n### sigmoid activation function in the output layer is because we have multi label classification problem.\n### Every class(label) is having 2 more classes i.e.0 and 1 to predict. \n\nmodel = Sequential()\nmodel.add(Embedding(40000, 128))\nmodel.add(LSTM(units = 64, dropout = 0.2,return_sequences=True))\nmodel.add(LSTM(units = 64, dropout = 0.2))\nmodel.add(Dense(units = 6, activation = 'sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Compiling the model and training the model using training dataset\n### We need to define loss function to measure the loss. Our problem is multi label multi classification problem.\n### Hence I decided to take loss function as binary_crossentropy. (Reference - https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/)\n### Adam optimizer is best for this problem. I tried RMSprop also but didn't get good accuracy. So decided to go with adam optimizer\n### accuracy metric has been used. Dataset is divided into training and validation set. \n### We got an accuracy of 97.79% on validation dataset. batch_size is 32 . \n### These are the Number of samples that goes through the network at a time and calculates the loss by updating the parameters.\n### I used only 1 epoch because I got almost 97% accuracy in 1 iteration.\n### Submission from the competition was evaluated on the basis of AUC. Hence i decided to use AUC metrics.\n### AUC is area under the curve. It can be calculated from receiver operating curve\n\nmodel.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"AUC\"])\nx_train, x_val, y_train, y_val = train_test_split(train_padded, train_label, shuffle = True, random_state = 123)\nmodel.fit(x_train, y_train, batch_size = 32, epochs = 1, validation_data = (x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Prediction for test data\npredict = model.predict(test_padded)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted values are\",predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### predicted probabilities of each id in the test set, for all the target columns\n\ntarget_cols = np.array(['toxic','severe_toxic','obscene', 'threat','insult', 'identity_hate'])\nfinal_predict_test = pd.concat([pd.DataFrame(predict, columns=target_cols)], 1)\nt1 = test['id']\nfinal_predict_test = pd.concat([t1,final_predict_test],1)\nfinal_predict_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Complete description of classification system using neural networks\n\n## This classification system is built using LSTM which is a special type of RNN's. I have used Keras library to build neural network.\n## Recurrent Neural networks are used mostly for sequential data. We have sequential data(sentences). RNN's store information in their \"memory\".\n## Recurrent Neural networks stores the information about the computations which has already been performed.\n## Variety of RNN achitectures are available based on the need of the user. I have used LSTM which is a special type of RNN.\n## Even though RNN are good for sequence modelling, it still have issues and it tends to have vanishing gradient problem.\n## Vanishing gradient problem is faced by neural networks which uses activation function having small gradient(0 to 1).\n## During backpropagation phase, these gradients either tend to become 0 or they vanishes. This happens because RNN's are not\n## capable of handling long-term dependencies. In order to avoid vanishing gradient problem LSTM are developed. \n## LSTM comes with LSTM cell. It helps to model and keep track of long term dependencies. \n\n### Process performed in this classification model are mentioned below.\n## Reading the dataset using pandas dataframe and checking whether it contains any null observations or not. There are no nulls in this dataset.\n## Many people clean the data first and then perform visualizations. I performed visualizations first. \n## I created 4 visualizations. \n## 1st visualization is a Bar graph of total number of comments per label. It is helpful in identifying the total number of comments per label\n## 2nd visualization is a Bar graph of Total No. of labels in a sentence against Total no. of sentences. \n## From 2nd visualization we can observe whether a sentence belongs to only one category or many categories. We can say that\n## there are many sentences which are not classified(label 0) but many sentences falls under more than 1 category.\n## 3rd Visualization is a Graph of individual class against the total no. of labelled and unlabelled sentences for the same class. \n## 4th Visualization is a Correlation graph(matrix) between different variables\n\n## From visualization we can conclude that,  1) A single sentence can have multiple labels (categories).\n## 2) Some labels are highely correlated to each other.\n## 3) This is multi label classification problem. Hence we can use sigmoid function in the output layer.\n\n### Next part includes cleaning the data. Data cleaning part is performed on the input text. \n## I created a function, which performs below activities: \n## 1) conversion of contraction words to expanded words , 2) removing stopwords , 3) lowercasing the words\n## 4) removing the non - alphanumeric characters   5) removing the extra spaces between words\n\n### Once we have cleaned the data, next part is creating input feature vectors.  \n## Tokenizer class from keras library is used. 40000 words has been selected based on the frequency of the words and assigned a number using keras tokenizer\n## Training dataset consists of sentences. Every sentence is made up of words. The number given to the words by the keras \n## tokenizer are used to build up a sentence. Padding is used to fix the sentence length to 150 words per sentence. \n\n\n### Input features are generated. Now its time to build a model. Model contains an Input embedding layer of 128 Hidden units.\n## Models contains 2 LSTM layers of 64 Hidden units and 1 output layer of 6 Hiddent units having sigmoid activation function.\n##Input to the neural network is given in the shape of (159571, 150).\n\n### Next part is compiling the model and training it on training dataset. Training dataset is divided in training and validation dataset\n##  Binary_crossentropy is used as a loss function. Adam optimizer is used. Accuracy metric has been used.\n## batch_size is 32 . These are the Number of samples that goes through the network at a time and calculates the loss by\n## updating the parameters.I used only 1 epoch because I got almost 97% accuracy in 1 iteration.\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}