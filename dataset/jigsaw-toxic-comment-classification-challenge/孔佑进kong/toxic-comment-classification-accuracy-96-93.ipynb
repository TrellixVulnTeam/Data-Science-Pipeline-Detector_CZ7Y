{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile as zf\nimport re\nimport random\nimport math\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bert-for-tf2\n!pip install sentencepiece","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import load_model\nimport bert\nfrom transformers import AutoTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_data = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()#查看数据类型及其分布","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_list = [f for f in train.columns if train.dtypes[f] != 'object']\ndf = pd.DataFrame(columns=column_list)\nfor col in column_list:\n    df.loc[0,col] = train[train[col] == 1][col].sum()#计算每种类型评论的数量\ndf['non_hate'] = train.shape[0] - df.sum(axis=1)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pie, ax = plt.subplots(figsize=[12,10])\nlabels = df.keys()\nplt.pie(x=df.values[0], autopct=\"%.1f\", explode=[0.05]*len(df.values[0]), labels=labels, pctdistance=0.5)\nplt.title(\"Types of Toxic Comments\", fontsize=14);\npie.savefig(\"ToxicCommentsChart.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train.drop(columns=['id'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Cleaning","metadata":{}},{"cell_type":"code","source":"def clean_comment(comment):\n    # Removing the @\n    comment = re.sub(r\"@[A-Za-z0-9]+\", ' ', comment)\n    # Removing the URL links\n    comment = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', comment)\n    # Keeping only letters\n    comment = re.sub(r\"[^a-zA-Z.!?']\", ' ', comment)\n    # Removing additional whitespaces\n    comment = re.sub(r\" +\", ' ', comment)             \n    return comment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['comment_text'] = df['comment_text'].apply(lambda x: clean_comment(x))#置换一些特殊符号","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_clean = df.comment_text.values\ndata_labels = df.drop(columns=['comment_text'],axis=1).values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"FullTokenizer = bert.bert_tokenization.FullTokenizer\n#FullTokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n                            trainable=False)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = FullTokenizer(vocab_file, do_lower_case)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_sentence(sent):\n    return [\"[CLS]\"] + tokenizer.tokenize(sent)[:510] + [\"[SEP]\"]#编码","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_inputs = [encode_sentence(sentence) for sentence in data_clean]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Creation","metadata":{}},{"cell_type":"code","source":"def get_ids(tokens):\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef get_mask(tokens):\n    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n\ndef get_segments(tokens):\n    seg_ids = []\n    current_seg_id = 0\n    for tok in tokens:\n        seg_ids.append(current_seg_id)\n        if tok == \"[SEP]\":\n            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n    return seg_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_with_len = [[sent, data_labels[i], len(sent)]\n                 for i, sent in enumerate(data_inputs)]#enumerate将其组成一个索引序列，利用它可以同时获得索引和值\nrandom.shuffle(data_with_len)#随机排序\ndata_with_len.sort(key=lambda x: x[2])#按照第二位元素的最大值排序\nsorted_all = [([get_ids(sent_lab[0]),\n                get_mask(sent_lab[0]),\n                get_segments(sent_lab[0])],\n               sent_lab[1])\n              for sent_lab in data_with_len if sent_lab[2] > 7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_with_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A list is a type of iterator so it can be used as generator for a dataset\nall_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n                                             output_types=(tf.int32, tf.int32))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nall_batched = all_dataset.padded_batch(BATCH_SIZE,\n                                       padded_shapes=((3, None), (6)),\n                                       padding_values=(0, 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\nNB_BATCHES_TEST = NB_BATCHES // 10   #  //表示整除\nall_batched.shuffle(NB_BATCHES)  #随机排序\ntest_dataset = all_batched.take(NB_BATCHES_TEST)\ntrain_dataset = all_batched.skip(NB_BATCHES_TEST)#数据集划分为测试集和训练集","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"class DCNNBERTEmbedding(tf.keras.Model):\n    \n    def __init__(self,\n                 nb_filters=50,\n                 FFN_units=512,\n                 nb_classes=6,\n                 dropout_rate=0.1,\n                 name=\"dcnn\"):\n        super(DCNNBERTEmbedding, self).__init__(name=name)\n        \n        self.bert_layer = hub.KerasLayer(\n            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n            trainable=False)\n\n        self.bigram = layers.Conv1D(filters=nb_filters,\n                                    kernel_size=2,\n                                    padding=\"valid\",\n                                    activation=\"relu\")\n        self.trigram = layers.Conv1D(filters=nb_filters,\n                                     kernel_size=3,\n                                     padding=\"valid\",\n                                     activation=\"relu\")\n        self.fourgram = layers.Conv1D(filters=nb_filters,\n                                      kernel_size=4,\n                                      padding=\"valid\",\n                                      activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D()\n        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n#        if nb_classes == 2:\n        self.last_dense = layers.Dense(units=nb_classes,\n                                           activation=\"sigmoid\")\n#        else:\n#            self.last_dense = layers.Dense(units=nb_classes,\n#                                           activation=\"softmax\")\n    \n    def embed_with_bert(self, all_tokens):\n        _, embs = self.bert_layer([all_tokens[:, 0, :],\n                                   all_tokens[:, 1, :],\n                                   all_tokens[:, 2, :]])\n        return embs\n\n    def call(self, inputs, training):\n        x = self.embed_with_bert(inputs)\n\n        print(x.shape)\n\n        x_1 = self.bigram(x)\n        x_1 = self.pool(x_1)\n        x_2 = self.trigram(x)\n        x_2 = self.pool(x_2)\n        x_3 = self.fourgram(x)\n        x_3 = self.pool(x_3)\n        \n        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n        merged = self.dense_1(merged)\n        merged = self.dropout(merged, training)\n        output = self.last_dense(merged)\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"NB_FILTERS = 100\nFFN_UNITS = 256\nNB_CLASSES = 6\n\nDROPOUT_RATE = 0.5\n\nBATCH_SIZE = 32\nNB_EPOCHS = 2  #超参数值","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n                         FFN_units=FFN_UNITS,\n                         nb_classes=NB_CLASSES,\n                         dropout_rate=DROPOUT_RATE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dcnn.compile(loss=\"binary_crossentropy\",\n            optimizer=\"adam\",\n            metrics=[\"accuracy\"])  #编译模型","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = \"./\"\n\nckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print(\"Latest checkpoint restored!!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyCustomCallback(tf.keras.callbacks.Callback):\n\n    def on_epoch_end(self, epoch, logs=None):\n        ckpt_manager.save()\n        print(\"Checkpoint saved at {}.\".format(checkpoint_path))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dcnn.fit(train_dataset,\n         epochs=NB_EPOCHS,\n         callbacks=[MyCustomCallback()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dcnn.save('Bert_Dcnn_model1',save_format='tf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = load_model('Bert_Dcnn_model1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"results = Dcnn.evaluate(test_dataset)\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}