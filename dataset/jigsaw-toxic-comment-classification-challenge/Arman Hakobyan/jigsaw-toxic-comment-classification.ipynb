{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install contractions\nimport contractions\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nimport pickle\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier\n# import xgboost as xgb\nfrom sklearn.calibration import CalibratedClassifierCV\nimport re \nimport string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nnltk.download(\"stopwords\")\nnltk.download('punkt')\nnltk.download('wordnet')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_label = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\nsub=pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-15T13:38:28.265519Z","iopub.execute_input":"2021-12-15T13:38:28.26623Z","iopub.status.idle":"2021-12-15T13:38:34.862823Z","shell.execute_reply.started":"2021-12-15T13:38:28.266118Z","shell.execute_reply":"2021-12-15T13:38:34.861952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ntrain_text = train['comment_text']\ntest_text = test['comment_text']\nall_text = pd.concat([train_text, test_text])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T13:38:43.261611Z","iopub.execute_input":"2021-12-15T13:38:43.261907Z","iopub.status.idle":"2021-12-15T13:38:43.32351Z","shell.execute_reply.started":"2021-12-15T13:38:43.261867Z","shell.execute_reply":"2021-12-15T13:38:43.322796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Определим список стоп-слов, из из них удалим те стоп-слова, которые означают отрицание","metadata":{}},{"cell_type":"code","source":"contractions.fix( \"couldn't\")\nnot_stop = set(['don', 'hadn','couldn','haven','wasn','doesn', \n            'hasn', 'aren', 'shouldn', 'mightn', 'don', 'needn',\n            'weren','didn','wouldn','isn', \n           ])\nstop_words = set(stopwords.words('english'))\nstop_words  = stop_words.difference(not_stop)\n# Раскрываем сокрощения типа haven't -> have not и убираем из стоп-слов\nstop_words = [st for st in stop_words if 'not' not in contractions.fix(st)]\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T13:38:43.336463Z","iopub.execute_input":"2021-12-15T13:38:43.337023Z","iopub.status.idle":"2021-12-15T13:38:43.347787Z","shell.execute_reply.started":"2021-12-15T13:38:43.336984Z","shell.execute_reply":"2021-12-15T13:38:43.347057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words=stop_words,\n    ngram_range=(1, 1),\n    max_features=10000)#10k\n\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n#     stop_words='english',\n    ngram_range=(2, 5),#2-6\n    max_features=50000)#50k\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)\n\ntrain_features = hstack([train_char_features, train_word_features])\ntest_features = hstack([test_char_features, test_word_features])\n\nif True:\n    with open('word_vectorizer.pk', 'wb') as f:\n        pickle.dump(word_vectorizer, f)\n    with open('char_vectorizer.pk', 'wb') as f:\n        pickle.dump(char_vectorizer, f)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T13:38:43.349382Z","iopub.execute_input":"2021-12-15T13:38:43.34966Z","iopub.status.idle":"2021-12-15T13:53:49.93996Z","shell.execute_reply.started":"2021-12-15T13:38:43.349626Z","shell.execute_reply":"2021-12-15T13:53:49.939082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    word_vectorizer = pickle.load(open(\"word_vectorizer.pk\", \"rb\"))\n    char_vectorizer = pickle.load(open(\"char_vectorizer.pk\", \"rb\"))\n\n    train_word_features = word_vectorizer.transform(train_text)\n    test_word_features = word_vectorizer.transform(test_text)\n\n    train_char_features = char_vectorizer.transform(train_text)\n    test_char_features = char_vectorizer.transform(test_text)\n\n    train_features = hstack([train_char_features, train_word_features])\n    test_features = hstack([test_char_features, test_word_features])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T13:53:49.944909Z","iopub.execute_input":"2021-12-15T13:53:49.947201Z","iopub.status.idle":"2021-12-15T13:53:49.956006Z","shell.execute_reply.started":"2021-12-15T13:53:49.947155Z","shell.execute_reply":"2021-12-15T13:53:49.955163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_word_features, test_word_features,train_char_features, test_char_features,\ndel word_vectorizer, char_vectorizer","metadata":{"execution":{"iopub.status.busy":"2021-12-15T13:53:49.960246Z","iopub.execute_input":"2021-12-15T13:53:49.962512Z","iopub.status.idle":"2021-12-15T13:53:50.965823Z","shell.execute_reply.started":"2021-12-15T13:53:49.962464Z","shell.execute_reply":"2021-12-15T13:53:50.964907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nscores = []\nsubmission = pd.DataFrame.from_dict({'id': test['id']})\nfor class_name in class_names:\n    train_target = train[class_name]\n    classifier = LogisticRegression(solver='sag',n_jobs=-1)\n#     svm = LinearSVC(dual=False)\n#     classifier = AdaBoostClassifier(cv, algorithm='SAMME', n_estimators=100)\n#     svm = LinearSVC()\n#     classifier = CalibratedClassifierCV(svm) \n#     classifier = xgb.XGBClassifier(objective='multi:softprob', learning_rate=1,\n#                                    tree_method='auto',\n#                                    max_depth = 12,    \n#                                    silent=True, \n#                                    n_estimators=100, \n#                                    num_class=2, )\n\n#     print('class')\n    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n    scores.append(cv_score)\n    print('CV score for class {} is {}'.format(class_name, cv_score))\n\n    classifier.fit(train_features, train_target)\n    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n    with open(f'logreg_{class_name}.pk', 'wb') as f:\n        pickle.dump(classifier, f)\n\nprint('Total CV score is {}'.format(np.mean(scores)))\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T13:04:59.134009Z","iopub.execute_input":"2021-12-15T13:04:59.134577Z","iopub.status.idle":"2021-12-15T13:15:53.896834Z","shell.execute_reply.started":"2021-12-15T13:04:59.134535Z","shell.execute_reply":"2021-12-15T13:15:53.895998Z"},"trusted":true},"execution_count":null,"outputs":[]}]}