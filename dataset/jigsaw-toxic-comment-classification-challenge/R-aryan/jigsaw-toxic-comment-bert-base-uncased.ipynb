{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-05T09:36:11.439212Z","iopub.execute_input":"2021-07-05T09:36:11.439583Z","iopub.status.idle":"2021-07-05T09:36:11.454511Z","shell.execute_reply.started":"2021-07-05T09:36:11.439507Z","shell.execute_reply":"2021-07-05T09:36:11.453253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install transformers","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:36:11.456431Z","iopub.execute_input":"2021-07-05T09:36:11.457207Z","iopub.status.idle":"2021-07-05T09:36:11.461651Z","shell.execute_reply.started":"2021-07-05T09:36:11.457166Z","shell.execute_reply":"2021-07-05T09:36:11.460466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nimport numpy as np\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport time\nimport datetime\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport re                                  \nimport string  \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nimport torch\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import EarlyStoppingCallback\nfrom transformers import BertModel, BertConfig\n\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler","metadata":{"execution":{"iopub.status.busy":"2021-07-05T12:46:50.435519Z","iopub.execute_input":"2021-07-05T12:46:50.435914Z","iopub.status.idle":"2021-07-05T12:46:57.325544Z","shell.execute_reply.started":"2021-07-05T12:46:50.43583Z","shell.execute_reply":"2021-07-05T12:46:57.324702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# n_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T12:46:57.326884Z","iopub.execute_input":"2021-07-05T12:46:57.327218Z","iopub.status.idle":"2021-07-05T12:46:57.379191Z","shell.execute_reply.started":"2021-07-05T12:46:57.327184Z","shell.execute_reply":"2021-07-05T12:46:57.377297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\n\n\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\",\"r\") as z:\n    z.extractall(\".\")\n    \nwith zipfile.ZipFile(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\",\"r\") as z:\n    z.extractall(\".\")\n    \nwith zipfile.ZipFile(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\",\"r\") as z:\n    z.extractall(\".\")","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:36:18.205979Z","iopub.execute_input":"2021-07-05T09:36:18.206659Z","iopub.status.idle":"2021-07-05T09:36:20.794731Z","shell.execute_reply.started":"2021-07-05T09:36:18.206618Z","shell.execute_reply":"2021-07-05T09:36:20.793794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1=pd.read_csv(\"./train.csv\")\ntrain_df1.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:36:20.796088Z","iopub.execute_input":"2021-07-05T09:36:20.796426Z","iopub.status.idle":"2021-07-05T09:36:21.577125Z","shell.execute_reply.started":"2021-07-05T09:36:20.79639Z","shell.execute_reply":"2021-07-05T09:36:21.57615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Settings:\n#     PROJ_NAME = 'Jigsaw-Toxic-Comment-Classification'\n#     root_path = os.getcwd().split(PROJ_NAME)[0] + \"/\" + PROJ_NAME + \"/\"\n    MAX_LEN = 512\n    TRAIN_BATCH_SIZE = 16\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 1\n    RANDOM_STATE = 42\n    MODEL_PATH = 'toxic_model.bin'\n    TRAIN_NUM_WORKERS = 4\n    VAL_NUM_WORKERS = 1\n\n    # training data directory\n#     TRAIN_DATA = root_path + \"/training/data/train.csv\"\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    input_dim = 768\n    hidden_dim = 50\n    output_dim = 6\n    bert_model_name = 'bert-base-uncased'\n\n    # mapping of columns\n    columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:36:21.57957Z","iopub.execute_input":"2021-07-05T09:36:21.579935Z","iopub.status.idle":"2021-07-05T09:36:21.586734Z","shell.execute_reply.started":"2021-07-05T09:36:21.579898Z","shell.execute_reply":"2021-07-05T09:36:21.585834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTDataset:\n    def __init__(self, texts, targets):\n        self.settings = Settings\n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n                                                       do_lower_case=True)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        sent = str(self.texts[item])\n        inputs = self.tokenizer.encode_plus(\n            text=sent,\n            add_special_tokens=True,\n            max_length=self.settings.MAX_LEN,\n            pad_to_max_length = True,\n            return_attention_mask=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return {\n            'input_ids': torch.tensor(ids),\n            'attention_mask': torch.tensor(mask),\n            'token_type_ids': torch.tensor(token_type_ids),\n            'targets': torch.tensor(self.targets[item])\n        }","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:36:21.588684Z","iopub.execute_input":"2021-07-05T09:36:21.589227Z","iopub.status.idle":"2021-07-05T09:36:21.600196Z","shell.execute_reply.started":"2021-07-05T09:36:21.589184Z","shell.execute_reply":"2021-07-05T09:36:21.599308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Engine:\n    def __init__(self):\n        pass\n\n    def loss_fn(self, outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 6))\n\n    def accuracy_threshold(self, y_pred, y_true, thresh: float = 0.5, sigmoid: bool = True):\n        if sigmoid:\n            y_pred = y_pred.sigmoid()\n        return ((y_pred > thresh) == y_true.byte()).float().mean().item()\n\n    def set_seed(self, seed_value=42):\n        random.seed(seed_value)\n        np.random.seed(seed_value)\n        torch.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n\n    def train_fn(self, data_loader, model, optimizer, device, schedular):\n        print(\"Starting training...\\n\")\n        # Reset the total loss for this epoch.\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n        t0_epoch, t0_batch = time.time(), time.time()\n        model.train()\n        for step, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n            batch_counts += 1\n            b_input_ids = data['input_ids']\n            b_attn_mask = data['attention_mask']\n            b_labels = data['targets']\n\n            # moving tensors to device\n            b_input_ids = b_input_ids.to(device,dtype=torch.long)\n            b_attn_mask = b_attn_mask.to(device,dtype=torch.long)\n            b_labels = b_labels.to(device,dtype=torch.float)\n\n            # optimizer.zero_grad()\n\n            # Always clear any previously calculated gradients before performing a\n            # backward pass. PyTorch doesn't do this automatically because\n            # accumulating the gradients is \"convenient while training RNNs\".\n            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n            model.zero_grad()\n\n            logits = model(b_input_ids, b_attn_mask)\n\n            loss = self.loss_fn(logits, b_labels.float())\n            batch_loss += loss.item()\n            # Accumulate the training loss over all of the batches so that we can\n            # calculate the average loss at the end. `loss` is a Tensor containing a\n            # single value; the `.item()` function just returns the Python value\n            # from the tensor.\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate the gradients.\n            loss.backward()\n            # Clip the norm of the gradients to 1.0.\n            # This is to help prevent the \"exploding gradients\" problem.\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            # Update parameters and take a step using the computed gradient.\n            # The optimizer dictates the \"update rule\"--how the parameters are\n            # modified based on their gradients, the learning rate, etc\n            optimizer.step()\n            # Update the learning rate\n            schedular.step()\n\n            if step % 500 == 0 and not step == 0:\n                # Calculate elapsed time in minutes.\n                elapsed = self.format_time(time.time() - t0_epoch)\n                # Report progress.\n                print(' \\n Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(data_loader), elapsed))\n\n        # Calculate the average loss over all of the batches.\n        avg_train_loss = total_loss / len(data_loader)\n        # Measure how long this epoch took.\n        training_time = self.format_time(time.time() - t0_epoch)\n\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n\n    def eval_fn(self, data_loader, model, device):\n        print(\"Starting evaluation...\\n\")\n        t0 = time.time()\n        model.eval()\n        val_accuracy = []\n        val_loss = []\n        with torch.no_grad():\n            for step, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n                b_input_ids = data['input_ids']\n                b_attn_mask = data['attention_mask']\n                b_labels = data['targets']\n\n                # moving tensors to device\n                b_input_ids = b_input_ids.to(device,dtype=torch.long)\n                b_attn_mask = b_attn_mask.to(device,dtype=torch.long)\n                b_labels = b_labels.to(device,dtype=torch.float)\n\n                logits = model(b_input_ids, b_attn_mask)\n                loss = self.loss_fn(logits, b_labels.float())\n                val_loss.append(loss.item())\n                accuracy = self.accuracy_threshold(logits.view(-1, 6), b_labels.view(-1, 6))\n                val_accuracy.append(accuracy)\n\n        val_loss = np.mean(val_loss)\n        val_accuracy = np.mean(val_accuracy)\n        validation_time = self.format_time(time.time() - t0)\n\n        print(\"  Average Validation Loss: {0:.2f}\".format(val_loss))\n        print(\"  Average Validation Accuracy: {0:.2f}\".format(val_accuracy))\n        print(\"  Validation took: {:}\".format(validation_time))\n\n        return val_loss, val_accuracy\n\n    def format_time(self, elapsed):\n        '''\n        Takes a time in seconds and returns a string hh:mm:ss\n        '''\n        # Round to the nearest second.\n        elapsed_rounded = int(round(elapsed))\n\n        # Format as hh:mm:ss\n        return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:36:21.601639Z","iopub.execute_input":"2021-07-05T09:36:21.602036Z","iopub.status.idle":"2021-07-05T09:36:21.625618Z","shell.execute_reply.started":"2021-07-05T09:36:21.601998Z","shell.execute_reply":"2021-07-05T09:36:21.6243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTClassifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(BERTClassifier, self).__init__()\n        self.settings = Settings\n        self.input_dim = self.settings.input_dim\n        self.hidden_dim = self.settings.hidden_dim\n        self.output_dim = self.settings.output_dim\n\n        # loading the bert model\n        self.bert = BertModel.from_pretrained(self.settings.bert_model_name)\n\n        # adding custom layers according to the problem statement\n        self.classifier = nn.Sequential(\n            nn.Linear(self.input_dim, self.hidden_dim),\n            nn.ReLU(),\n            nn.Linear(self.hidden_dim, self.output_dim)\n        )\n\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None):\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids)\n\n        last_hidden_state_cls = outputs[0][:, 0, :]\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits\n\n    # def print_model_details(self):\n    #     # Get all of the model's parameters as a list of tuples.\n    #     params = list(self.bert.named_parameters())\n    #\n    #     print('The BERT Base Uncased Model Has {:} different named parameters.\\n'.format(len(params)))\n    #\n    #     print('==== Embedding Layer ====\\n')\n    #\n    #     for p in params[0:5]:\n    #         print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n    #\n    #     print('\\n==== First Transformer ====\\n')\n    #\n    #     for p in params[5:21]:\n    #         print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n    #\n    #     print('\\n==== Output Layer ====\\n')\n    #\n    #     for p in params[-4:]:\n    #         print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:36:21.627309Z","iopub.execute_input":"2021-07-05T09:36:21.62772Z","iopub.status.idle":"2021-07-05T09:36:21.640069Z","shell.execute_reply.started":"2021-07-05T09:36:21.627683Z","shell.execute_reply":"2021-07-05T09:36:21.6391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocess:\n    def __init__(self):\n        pass\n\n    def clean_text(self, text):\n        text = text.lower()\n        text = re.sub('\\[.*?\\]', '', text)\n        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n        text = re.sub('<.*?>+', '', text)\n        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n        text = re.sub('\\n', '', text)\n        text = re.sub('\\w*\\d\\w*', '', text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:36:21.641493Z","iopub.execute_input":"2021-07-05T09:36:21.641962Z","iopub.status.idle":"2021-07-05T09:36:21.652495Z","shell.execute_reply.started":"2021-07-05T09:36:21.641925Z","shell.execute_reply":"2021-07-05T09:36:21.651702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Train:\n    def __init__(self):\n        # initialize required class\n        self.settings = Settings\n        self.engine = Engine()\n        self.preprocess = Preprocess()\n\n        # initialize required variables\n        self.bert_classifier = None\n        self.optimizer = None\n        self.scheduler = None\n        self.train_data_loader = None\n        self.val_data_loader = None\n        self.total_steps = None\n        self.best_accuracy = 0\n\n    def __initialize(self):\n        # Instantiate Bert Classifier\n        self.bert_classifier = BERTClassifier(freeze_bert=False)\n        self.bert_classifier.to(self.settings.DEVICE)\n\n        # Create the optimizer\n        self.optimizer = AdamW(self.bert_classifier.parameters(),\n                               lr=5e-5,  # Default learning rate\n                               eps=1e-8  # Default epsilon value\n                               )\n        # Set up the learning rate scheduler\n        self.scheduler = get_linear_schedule_with_warmup(self.optimizer,\n                                                         num_warmup_steps=0,  # Default value\n                                                         num_training_steps=self.total_steps)\n\n    def crete_data_loaders(self, dataset):\n        pass\n\n    def load_data(self):\n        train_df = train_df1.fillna(\"none\")\n        train_df['comment_text'] = train_df['comment_text'].apply(lambda x: self.preprocess.clean_text(x))\n        X = list(train_df['comment_text'])\n        y = np.array(train_df.loc[:, 'toxic':])\n\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=self.settings.RANDOM_STATE)\n\n        # training dataset\n        train_dataset = BERTDataset(X_train, y_train)\n\n        # validation dataset\n        val_dataset = BERTDataset(X_val, y_val)\n\n        self.train_data_loader = DataLoader(train_dataset,\n                                            batch_size=self.settings.TRAIN_BATCH_SIZE,\n                                            shuffle=True,\n                                            num_workers=self.settings.TRAIN_NUM_WORKERS)\n\n        self.val_data_loader = DataLoader(val_dataset,\n                                          batch_size=self.settings.VALID_BATCH_SIZE,\n                                          shuffle=True,\n                                          num_workers=self.settings.VAL_NUM_WORKERS)\n\n        self.total_steps = int(len(X_train) / self.settings.TRAIN_BATCH_SIZE * self.settings.EPOCHS)\n\n    def train(self):\n        for epochs in range(self.settings.EPOCHS):\n          print(\"\\n\")\n          print(\"-\"*70)\n          print(\"Epoch--- \",str(epochs))\n          print(\"-\"*70)\n          print(\"\\n\")\n\n          # calling the training function in engine.py file\n          self.engine.train_fn(data_loader=self.train_data_loader,\n                                model=self.bert_classifier,\n                                optimizer=self.optimizer,\n                                device=self.settings.DEVICE,\n                                schedular=self.scheduler)\n\n          # calling the evaluation function from the engine.py file to compute evaluation\n          val_loss, val_accuracy = self.engine.eval_fn(data_loader=self.val_data_loader,\n                                                        model=self.bert_classifier,\n                                                        device=self.settings.DEVICE)\n\n          # updating the accuracy\n          if val_accuracy > self.best_accuracy:\n              torch.save(self.bert_classifier.state_dict(), self.settings.MODEL_PATH)\n              self.best_accuracy = val_accuracy\n\n    def run(self):\n        try:\n            print(\"Loading and Preparing the Dataset-----!! \")\n            self.load_data()\n            print(\"Dataset Successfully Loaded and Prepared-----!! \")\n            print()\n            print(\"-\" * 70)\n            print(\"Loading and Initializing the Bert Model -----!! \")\n            self.__initialize()\n            print(\"Model Successfully Loaded and Initialized-----!! \")\n            print()\n            print(\"-\" * 70)\n            print(\"------------------Starting Training-----------!!\")\n            self.engine.set_seed()\n            self.train()\n            print(\"Training complete-----!!!\")\n\n        except BaseException as ex:\n            print(\"Following Exception Occurred---!! \", str(ex))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:36:21.654208Z","iopub.execute_input":"2021-07-05T09:36:21.654713Z","iopub.status.idle":"2021-07-05T09:36:21.675044Z","shell.execute_reply.started":"2021-07-05T09:36:21.654671Z","shell.execute_reply":"2021-07-05T09:36:21.674081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t =Train()\nt.run()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:36:21.676266Z","iopub.execute_input":"2021-07-05T09:36:21.676706Z","iopub.status.idle":"2021-07-05T11:43:18.383025Z","shell.execute_reply.started":"2021-07-05T09:36:21.67667Z","shell.execute_reply":"2021-07-05T11:43:18.381826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference Code","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('./test.csv')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-07-05T11:43:18.384916Z","iopub.execute_input":"2021-07-05T11:43:18.385298Z","iopub.status.idle":"2021-07-05T11:43:19.038222Z","shell.execute_reply.started":"2021-07-05T11:43:18.385254Z","shell.execute_reply":"2021-07-05T11:43:19.037174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL = BERTClassifier()\nMODEL.load_state_dict(torch.load(Settings.MODEL_PATH,map_location=torch.device(Settings.DEVICE)))\nMODEL.to(Settings.DEVICE)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T11:50:22.91551Z","iopub.execute_input":"2021-07-05T11:50:22.915955Z","iopub.status.idle":"2021-07-05T11:50:25.931837Z","shell.execute_reply.started":"2021-07-05T11:50:22.915913Z","shell.execute_reply":"2021-07-05T11:50:25.931033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-05T11:43:22.247438Z","iopub.execute_input":"2021-07-05T11:43:22.247803Z","iopub.status.idle":"2021-07-05T11:43:22.253831Z","shell.execute_reply.started":"2021-07-05T11:43:22.247764Z","shell.execute_reply":"2021-07-05T11:43:22.252862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T11:43:22.255413Z","iopub.execute_input":"2021-07-05T11:43:22.25578Z","iopub.status.idle":"2021-07-05T11:43:40.025641Z","shell.execute_reply.started":"2021-07-05T11:43:22.255745Z","shell.execute_reply":"2021-07-05T11:43:40.024747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_predict(model, test_dataloader):\n    model.eval()\n    all_logits = []\n    for batch in test_dataloader:\n        b_input_ids, b_attn_mask = tuple(t.to(Settings.DEVICE) for t in batch)[:2]\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    all_logits = torch.cat(all_logits, dim=0)\n    probs = all_logits.sigmoid().cpu().numpy()\n    \n    return probs","metadata":{"execution":{"iopub.status.busy":"2021-07-05T11:43:40.029212Z","iopub.execute_input":"2021-07-05T11:43:40.029482Z","iopub.status.idle":"2021-07-05T11:43:40.035181Z","shell.execute_reply.started":"2021-07-05T11:43:40.029455Z","shell.execute_reply":"2021-07-05T11:43:40.034233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ndef preprocessing_for_bert(data):\n    input_ids = []\n    attention_masks = []\n    for sent in data:\n        encoded_sent = tokenizer.encode_plus(\n            text = sent,\n            add_special_tokens = True,\n            max_length = Settings.MAX_LEN,\n            pad_to_max_length = True,\n            return_attention_mask = True\n        )\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n    \n    return input_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2021-07-05T11:43:40.03717Z","iopub.execute_input":"2021-07-05T11:43:40.037532Z","iopub.status.idle":"2021-07-05T11:43:40.621894Z","shell.execute_reply.started":"2021-07-05T11:43:40.037496Z","shell.execute_reply":"2021-07-05T11:43:40.621035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_inputs, test_masks = preprocessing_for_bert(test_df.comment_text)\ntest_dataset = TensorDataset(test_inputs, test_masks)\ntest_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T11:43:40.623114Z","iopub.execute_input":"2021-07-05T11:43:40.623436Z","iopub.status.idle":"2021-07-05T11:49:04.38526Z","shell.execute_reply.started":"2021-07-05T11:43:40.623402Z","shell.execute_reply":"2021-07-05T11:49:04.384376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = bert_predict(MODEL, test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T11:50:39.880244Z","iopub.execute_input":"2021-07-05T11:50:39.880569Z","iopub.status.idle":"2021-07-05T12:34:24.455036Z","shell.execute_reply.started":"2021-07-05T11:50:39.880538Z","shell.execute_reply":"2021-07-05T12:34:24.454118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(probs,columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\ntest_df[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]=submission\nfinal_sub = test_df[['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']]\nfinal_sub.head()\n                    ","metadata":{"execution":{"iopub.status.busy":"2021-07-05T12:35:40.883809Z","iopub.execute_input":"2021-07-05T12:35:40.884222Z","iopub.status.idle":"2021-07-05T12:35:40.915421Z","shell.execute_reply.started":"2021-07-05T12:35:40.884188Z","shell.execute_reply":"2021-07-05T12:35:40.914261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T12:35:49.874961Z","iopub.execute_input":"2021-07-05T12:35:49.875339Z","iopub.status.idle":"2021-07-05T12:35:52.172737Z","shell.execute_reply.started":"2021-07-05T12:35:49.875306Z","shell.execute_reply":"2021-07-05T12:35:52.171861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}