{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-18T21:47:27.238033Z","iopub.execute_input":"2021-07-18T21:47:27.23834Z","iopub.status.idle":"2021-07-18T21:47:27.251903Z","shell.execute_reply.started":"2021-07-18T21:47:27.238255Z","shell.execute_reply":"2021-07-18T21:47:27.250893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:27.253577Z","iopub.execute_input":"2021-07-18T21:47:27.254025Z","iopub.status.idle":"2021-07-18T21:47:29.454267Z","shell.execute_reply.started":"2021-07-18T21:47:27.253986Z","shell.execute_reply":"2021-07-18T21:47:29.453464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data[\"toxic\"]==1]","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:29.456182Z","iopub.execute_input":"2021-07-18T21:47:29.456542Z","iopub.status.idle":"2021-07-18T21:47:29.495772Z","shell.execute_reply.started":"2021-07-18T21:47:29.456507Z","shell.execute_reply":"2021-07-18T21:47:29.49504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_data[\"comment_text\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:29.497358Z","iopub.execute_input":"2021-07-18T21:47:29.497673Z","iopub.status.idle":"2021-07-18T21:47:29.502404Z","shell.execute_reply.started":"2021-07-18T21:47:29.49764Z","shell.execute_reply":"2021-07-18T21:47:29.501442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:29.504037Z","iopub.execute_input":"2021-07-18T21:47:29.504396Z","iopub.status.idle":"2021-07-18T21:47:34.526953Z","shell.execute_reply.started":"2021-07-18T21:47:29.504363Z","shell.execute_reply":"2021-07-18T21:47:34.524686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:34.530274Z","iopub.execute_input":"2021-07-18T21:47:34.530563Z","iopub.status.idle":"2021-07-18T21:47:34.537823Z","shell.execute_reply.started":"2021-07-18T21:47:34.530535Z","shell.execute_reply":"2021-07-18T21:47:34.536896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_data.iloc[:, 2:]\ny_train","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:34.539483Z","iopub.execute_input":"2021-07-18T21:47:34.540052Z","iopub.status.idle":"2021-07-18T21:47:34.567074Z","shell.execute_reply.started":"2021-07-18T21:47:34.540012Z","shell.execute_reply":"2021-07-18T21:47:34.566252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.values\ny_train","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:34.568652Z","iopub.execute_input":"2021-07-18T21:47:34.569055Z","iopub.status.idle":"2021-07-18T21:47:34.57966Z","shell.execute_reply.started":"2021-07-18T21:47:34.569015Z","shell.execute_reply":"2021-07-18T21:47:34.578707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The text contains some line breaks \"\\n\" so we remove it with regular expressions","metadata":{}},{"cell_type":"code","source":"import re","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:34.583527Z","iopub.execute_input":"2021-07-18T21:47:34.583866Z","iopub.status.idle":"2021-07-18T21:47:34.59039Z","shell.execute_reply.started":"2021-07-18T21:47:34.583834Z","shell.execute_reply":"2021-07-18T21:47:34.589366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(r\"\\n\", \" \", text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:34.592715Z","iopub.execute_input":"2021-07-18T21:47:34.593332Z","iopub.status.idle":"2021-07-18T21:47:34.600951Z","shell.execute_reply.started":"2021-07-18T21:47:34.593276Z","shell.execute_reply":"2021-07-18T21:47:34.600015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.apply(clean_text)\nX_train","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:34.602058Z","iopub.execute_input":"2021-07-18T21:47:34.6026Z","iopub.status.idle":"2021-07-18T21:47:35.013634Z","shell.execute_reply.started":"2021-07-18T21:47:34.602558Z","shell.execute_reply":"2021-07-18T21:47:35.012447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now to turn English letters into numbers (word vectors) so we can work with them","metadata":{}},{"cell_type":"code","source":"tokenizer = keras.preprocessing.text.Tokenizer(num_words = 100000, oov_token='<oov>')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:35.015157Z","iopub.execute_input":"2021-07-18T21:47:35.015558Z","iopub.status.idle":"2021-07-18T21:47:35.020553Z","shell.execute_reply.started":"2021-07-18T21:47:35.015501Z","shell.execute_reply":"2021-07-18T21:47:35.019688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:35.021922Z","iopub.execute_input":"2021-07-18T21:47:35.022549Z","iopub.status.idle":"2021-07-18T21:47:45.817225Z","shell.execute_reply.started":"2021-07-18T21:47:35.02251Z","shell.execute_reply":"2021-07-18T21:47:45.816291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pad sequences with 0's so that all sequences are of the same length","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = max([len(x) for x in np.array(X_train)])\n\ndef preprocess_to_sequences(dataset, fitted_tokenizer, maxlen):\n    dataset = tokenizer.texts_to_sequences(dataset)\n    dataset = pad_sequences(dataset, padding=\"pre\", truncating=\"pre\", maxlen=maxlen)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:45.81873Z","iopub.execute_input":"2021-07-18T21:47:45.81912Z","iopub.status.idle":"2021-07-18T21:47:45.922097Z","shell.execute_reply.started":"2021-07-18T21:47:45.819079Z","shell.execute_reply":"2021-07-18T21:47:45.921133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:45.923469Z","iopub.execute_input":"2021-07-18T21:47:45.923847Z","iopub.status.idle":"2021-07-18T21:47:45.932853Z","shell.execute_reply.started":"2021-07-18T21:47:45.923807Z","shell.execute_reply":"2021-07-18T21:47:45.931731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tokenized = preprocess_to_sequences(X_train, tokenizer, maxlen)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:45.934521Z","iopub.execute_input":"2021-07-18T21:47:45.93498Z","iopub.status.idle":"2021-07-18T21:47:58.646385Z","shell.execute_reply.started":"2021-07-18T21:47:45.934939Z","shell.execute_reply":"2021-07-18T21:47:58.645504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split 80% train 20% validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_tokenized, X_val_tokenized, y_train, y_val = train_test_split(X_train_tokenized, y_train, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:47:58.647667Z","iopub.execute_input":"2021-07-18T21:47:58.648005Z","iopub.status.idle":"2021-07-18T21:48:00.095166Z","shell.execute_reply.started":"2021-07-18T21:47:58.64797Z","shell.execute_reply":"2021-07-18T21:48:00.094296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download pretrained GloVe embeddings to use them as the first layer in our models\n# Embeddings map the word vectors into a vector space, similar words are grouped closely together in this space. They also apply a mask onto the padded sequences in order to ignore the 0's after padding","metadata":{}},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:48:00.096415Z","iopub.execute_input":"2021-07-18T21:48:00.096758Z","iopub.status.idle":"2021-07-18T21:51:02.290047Z","shell.execute_reply.started":"2021-07-18T21:48:00.096724Z","shell.execute_reply":"2021-07-18T21:51:02.28878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_glove_file = \"./glove.6B.100d.txt\"\n\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:51:02.295202Z","iopub.execute_input":"2021-07-18T21:51:02.295549Z","iopub.status.idle":"2021-07-18T21:51:14.294915Z","shell.execute_reply.started":"2021-07-18T21:51:02.295515Z","shell.execute_reply":"2021-07-18T21:51:14.292058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_tokens = len(tokenizer.word_index) + 1\nembedding_dim = 100\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:51:14.296349Z","iopub.execute_input":"2021-07-18T21:51:14.296717Z","iopub.status.idle":"2021-07-18T21:51:14.649638Z","shell.execute_reply.started":"2021-07-18T21:51:14.296666Z","shell.execute_reply":"2021-07-18T21:51:14.648636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Embedding\nembedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:51:14.650952Z","iopub.execute_input":"2021-07-18T21:51:14.651548Z","iopub.status.idle":"2021-07-18T21:51:14.6704Z","shell.execute_reply.started":"2021-07-18T21:51:14.6515Z","shell.execute_reply":"2021-07-18T21:51:14.669607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bidirectional LSTM with Conv1D, pre-trained 100d GloVe Embedding\n'''tf.random.set_seed(17)\nmodel = tf.keras.Sequential([embedding_layer,\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),\n    tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform'),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(6, activation='sigmoid')])'''","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:51:14.674131Z","iopub.execute_input":"2021-07-18T21:51:14.674455Z","iopub.status.idle":"2021-07-18T21:51:14.681252Z","shell.execute_reply.started":"2021-07-18T21:51:14.67442Z","shell.execute_reply":"2021-07-18T21:51:14.679927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bidirectional LSTM with Conv1D\n'''tf.random.set_seed(17)\nmodel = tf.keras.Sequential([tf.keras.layers.Embedding(150000, 50),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),\n    tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform'),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(6, activation='sigmoid')])\n'''","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:51:14.702051Z","iopub.execute_input":"2021-07-18T21:51:14.702824Z","iopub.status.idle":"2021-07-18T21:51:14.709845Z","shell.execute_reply.started":"2021-07-18T21:51:14.702781Z","shell.execute_reply":"2021-07-18T21:51:14.708475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2 GRU Layers\n'''tf.random.set_seed(17)\nmodel = keras.models.Sequential([\n    tf.keras.layers.Embedding(150000, 50),\n    keras.layers.GRU(128, return_sequences=True),\n    keras.layers.GRU(128),\n    keras.layers.Dense(6, activation=\"sigmoid\")\n])'''","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:51:14.744827Z","iopub.execute_input":"2021-07-18T21:51:14.745392Z","iopub.status.idle":"2021-07-18T21:51:14.751258Z","shell.execute_reply.started":"2021-07-18T21:51:14.745354Z","shell.execute_reply":"2021-07-18T21:51:14.750095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:51:14.772113Z","iopub.execute_input":"2021-07-18T21:51:14.772829Z","iopub.status.idle":"2021-07-18T21:51:14.789433Z","shell.execute_reply.started":"2021-07-18T21:51:14.772792Z","shell.execute_reply":"2021-07-18T21:51:14.788452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bidirectional GRU\ntf.random.set_seed(17)\nmodel = keras.models.Sequential([\n    embedding_layer,\n    keras.layers.Bidirectional(keras.layers.GRU(64, return_sequences=True)),\n    keras.layers.GRU(128),\n    keras.layers.Dense(6, activation=\"sigmoid\")\n])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:51:14.790651Z","iopub.execute_input":"2021-07-18T21:51:14.791315Z","iopub.status.idle":"2021-07-18T21:51:17.766091Z","shell.execute_reply.started":"2021-07-18T21:51:14.791265Z","shell.execute_reply":"2021-07-18T21:51:17.765218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:51:17.767988Z","iopub.execute_input":"2021-07-18T21:51:17.768636Z","iopub.status.idle":"2021-07-18T21:51:17.777894Z","shell.execute_reply.started":"2021-07-18T21:51:17.768594Z","shell.execute_reply":"2021-07-18T21:51:17.777064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"AUC\"])\nmodel.fit(X_train_tokenized, y_train, epochs=2, batch_size=200)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T21:51:17.779519Z","iopub.execute_input":"2021-07-18T21:51:17.780359Z","iopub.status.idle":"2021-07-18T22:10:06.016877Z","shell.execute_reply.started":"2021-07-18T21:51:17.780321Z","shell.execute_reply":"2021-07-18T22:10:06.016115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's predict some text and see what the model thinks of Gordon Ramsay","metadata":{}},{"cell_type":"code","source":"sample_text = \"You've got the nerve to tell me that some of them are fine. Wishy washy, not even seasoned, and you know what, more importantly, they're boiled. You donkey!\"\n# Quote from Gordon Ramsay, Hell's Kitchen\n\n# function that prints the probabilities of 6 labels of one sample text\ndef predict_print_text(sample_text):\n    sample_text = preprocess_to_sequences([sample_text], tokenizer, maxlen) # preprocesses text into number representations using a tokenizer\n    prediction = model.predict(sample_text) # predict using the trained model, consists of a GloVe 100d Embedding, a bidirectional GRU, a regular GRU and sigmoid unit\n    labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"] # 6 labels from the data\n    for idx, label in enumerate(labels):\n        print(f\"{label} = {prediction[0][idx]:.2f}\") # print probabilities for each label\n    return\n\npredict_print_text(sample_text)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T22:52:06.756918Z","iopub.execute_input":"2021-07-18T22:52:06.757258Z","iopub.status.idle":"2021-07-18T22:52:06.948942Z","shell.execute_reply.started":"2021-07-18T22:52:06.757224Z","shell.execute_reply":"2021-07-18T22:52:06.947973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n\nModel|Embedding|AUC|Total parameters|Time per epoch (seconds)|\n-----|-----|-----|-----|-----|\nBidirectional LSTM with Conv1D|300d|0.9848|45,542,054|889\nBidirectional LSTM with Conv1D|100d|0.9847|15,337,254|808\nBidirectional LSTM with Conv1D|50d|0.9842|7,786,054|659\nBidirectional LSTM with Conv1D|GloVe 100d|0.9718|21,371,154|625\nBidirectional LSTM with Conv1D|GloVe 50d|0.9720|10,803,004|584\n2 GRU Layers|300d|0.9877|45,264,966|1178\n2 GRU Layers|100d|0.9873|15,188,166|660\n2 GRU Layers|50d|0.9871|7,668,966|527\n2 GRU Layers|GloVe 100d|0.9780|21,222,066|471\n2 GRU Layers|GloVe 50d|0.9749|10,685,916|455\nBidirectional GRU|300d|0.9879|45,240,390|1303\nBidirectional GRU|100d|0.9877|15,163,590|755\nBidirectional GRU|50d|0.9871|7,644,390|599\nBidirectional GRU|GloVe 100d|0.9797|21,197,490|565\nBidirectional GRU|GloVe 50d|0.9761|10,661,340|527\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}