{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Toxic Comment Classification with ConvNets\nAuthor: Amit R. Baroi.\n\nThis notebook which was [implemented on Kaggle](https://www.kaggle.com/code/amitrobertbaroi/project-toxic-comment-classification), is based on [previous work by C. MATTHEWS](https://www.kaggle.com/code/charlesmatthews/toxic-twitter-with-keras-gru-1d-conv).","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.layers import Dense, Input, Bidirectional, Conv1D, GRU\nfrom tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-11T00:31:10.968632Z","iopub.execute_input":"2022-04-11T00:31:10.969366Z","iopub.status.idle":"2022-04-11T00:31:15.301168Z","shell.execute_reply.started":"2022-04-11T00:31:10.969002Z","shell.execute_reply":"2022-04-11T00:31:15.300232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Load data and GloVe embedding file\nWe will use the pre-trained GLOVE embedding, so load that in along with the test and training data.","metadata":{}},{"cell_type":"code","source":"# Training data\ntrain = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n\n# Testing data\ntest_labels = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\")\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest = test.merge(test_labels, on=\"id\")\n\n# Label column names\nlabels = list(train.columns[2:])\n\n# GloVe embedding\nglove_file = '../input/glove840b300dtxt/glove.840B.300d.txt'\n\n# Make output directory\nout_dir = \"outputs\"\nos.makedirs(out_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:31:15.303752Z","iopub.execute_input":"2022-04-11T00:31:15.304323Z","iopub.status.idle":"2022-04-11T00:31:17.817821Z","shell.execute_reply.started":"2022-04-11T00:31:15.304035Z","shell.execute_reply":"2022-04-11T00:31:17.816995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Label values with -1 mean that this row was not used for scoring in the competition","metadata":{}},{"cell_type":"code","source":"# Drop all rows with -1 in label values\ndrop_idxs = test[\n    (test.toxic == -1) | (test.severe_toxic == -1) | (test.obscene == -1) | \n    (test.threat == -1) | (test.insult == -1) | (test.identity_hate == -1)\n].index\ntest = test.drop(drop_idxs, axis=\"rows\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:31:17.819656Z","iopub.execute_input":"2022-04-11T00:31:17.820074Z","iopub.status.idle":"2022-04-11T00:31:17.899906Z","shell.execute_reply.started":"2022-04-11T00:31:17.820014Z","shell.execute_reply":"2022-04-11T00:31:17.898912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seperate features (text data) from labels (six types of toxic).","metadata":{}},{"cell_type":"code","source":"# Features\nX_train = train[\"comment_text\"].str.lower()\nX_test = test[\"comment_text\"].str.lower()\n\n# Labels\ny_train = train[labels].values\ny_test = test[labels].values\n\n# Training data\nprint(f\"X_train.shape = {X_train.shape}\")\nprint(f\"y_train.shape = {y_train.shape}\")\n# Testing data\nprint(f\"X_test.shape = {X_test.shape}\")\nprint(f\"y_test.shape = {y_test.shape}\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-04-11T00:31:17.902257Z","iopub.execute_input":"2022-04-11T00:31:17.902959Z","iopub.status.idle":"2022-04-11T00:31:18.296886Z","shell.execute_reply.started":"2022-04-11T00:31:17.90261Z","shell.execute_reply":"2022-04-11T00:31:18.295906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class label distribution in train and test data","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(16, 6))\nfor col, ax in zip(labels, axes.flatten()):\n    ax.hist(train[col], label=\"train\")\n    ax.hist(test[col], label=\"test\")\n    ax.set_title(f\"{col}\")\n    ax.set_yscale(\"log\")\n    ax.legend()\nplt.tight_layout()\nplt.savefig(f\"{out_dir}/class-label-distribution.jpg\", dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:31:18.301935Z","iopub.execute_input":"2022-04-11T00:31:18.302285Z","iopub.status.idle":"2022-04-11T00:31:24.479706Z","shell.execute_reply.started":"2022-04-11T00:31:18.302227Z","shell.execute_reply":"2022-04-11T00:31:24.478579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Tokenize\nWe then tokenize each of the test and training entries.\n\nThe embedding is 300 dimensional, but we have options for the maximum length of comments, and the max number of features we shall include. Here we use 150k features and cap the length to 200 words.","metadata":{}},{"cell_type":"code","source":"max_features = 150000\nmaxlen = 200\nembed_size = 300\n\ntok = text.Tokenizer(num_words=max_features, lower=True)\ntok.fit_on_texts(list(X_train)+list(X_test))\nX_train = tok.texts_to_sequences(X_train)\nX_test = tok.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:31:24.486793Z","iopub.execute_input":"2022-04-11T00:31:24.490101Z","iopub.status.idle":"2022-04-11T00:32:06.370784Z","shell.execute_reply.started":"2022-04-11T00:31:24.490015Z","shell.execute_reply":"2022-04-11T00:32:06.369748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Embedding\n### 3.1 Prepare embedding matrix\nWe assemble the embedding matrix by looping through GLOVE and adding the vector to the dictionary (2.2M words).","metadata":{}},{"cell_type":"code","source":"embeddings_index = {}\nwith open(glove_file, encoding='utf8') as f:\n    for line in tqdm(f):\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:32:06.372722Z","iopub.execute_input":"2022-04-11T00:32:06.373125Z","iopub.status.idle":"2022-04-11T00:37:40.445974Z","shell.execute_reply.started":"2022-04-11T00:32:06.373051Z","shell.execute_reply":"2022-04-11T00:37:40.444891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Map word tokens to embedding vector\nWe next map each of the tokens to the embedding vector. If a word is not found then we use the Spacy library to lemmatize the word, and see if we can find that instead. If we still can't find it, then we replace it with a random vector.","metadata":{}},{"cell_type":"code","source":"! python -m spacy.en.download all","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:37:40.447528Z","iopub.execute_input":"2022-04-11T00:37:40.44788Z","iopub.status.idle":"2022-04-11T00:37:42.869117Z","shell.execute_reply.started":"2022-04-11T00:37:40.447822Z","shell.execute_reply":"2022-04-11T00:37:42.86816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nlp = spacy.load('en'); # Depricated\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\nword_index = tok.word_index\n\nnum_words = min(max_features, len(word_index) + 1)\nembedding_matrix = np.random.randn(num_words, embed_size)/4\nkk = 0\nmoo = 0\nfor word, i in tqdm(word_index.items()): \n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        kk += 1\n    else: \n        for x in nlp(word,disable=['parser', 'ner']):\n            embedding_vector = embeddings_index.get(x.lemma_)\n            if embedding_vector is not None: \n                embedding_matrix[i] = embedding_vector \n                kk += 1\n                break","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:37:42.871568Z","iopub.execute_input":"2022-04-11T00:37:42.872058Z","iopub.status.idle":"2022-04-11T00:40:57.542496Z","shell.execute_reply.started":"2022-04-11T00:37:42.871988Z","shell.execute_reply":"2022-04-11T00:40:57.541416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Implement Models\n### 4.1 Paper Model\nWe first implement the model from the Georgakopoulos et al., (2018) research paper.","metadata":{}},{"cell_type":"code","source":"model_input = Input(shape=(maxlen, ))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(model_input)\nx = Conv1D(128, kernel_size=3)(x)\nx = Conv1D(128, kernel_size=4)(x)\nx = Conv1D(128, kernel_size=5)(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nx = concatenate([avg_pool, max_pool])\npaper_model_preds = Dense(6, activation=\"sigmoid\")(x)\npaper_model = Model(model_input, paper_model_preds, name=\"paper_model\")\npaper_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:40:57.544145Z","iopub.execute_input":"2022-04-11T00:40:57.544776Z","iopub.status.idle":"2022-04-11T00:40:59.566227Z","shell.execute_reply.started":"2022-04-11T00:40:57.544702Z","shell.execute_reply":"2022-04-11T00:40:59.565241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Paper model training\nThe research paper model is less complex, so it trains faster. Each iteration takes around 25 seconds so 10 iterations would take a bit more than 4 minutes.","metadata":{}},{"cell_type":"code","source":"batch_size = 128\nepochs = 10\n\npaper_model.compile(loss='binary_crossentropy',optimizer=Adam(lr=2e-4),metrics=['accuracy'])\nhistory_paper_model = paper_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:40:59.570875Z","iopub.execute_input":"2022-04-11T00:40:59.571182Z","iopub.status.idle":"2022-04-11T00:44:54.152526Z","shell.execute_reply.started":"2022-04-11T00:40:59.571118Z","shell.execute_reply":"2022-04-11T00:44:54.151508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(12, 6))\n\n# summarize history for accuracy\nax[0].plot(history_paper_model.history['accuracy'])\nax[0].set_title('paper model accuracy')\nax[0].set_ylabel('accuracy')\nax[0].set_xlabel('epoch')\nax[0].set_xticks(np.arange(0, epochs))\n# summarize history for loss\nax[1].plot(history_paper_model.history['loss'], c=\"orange\")\nax[1].set_title('paper model loss')\nax[1].set_ylabel('loss')\nax[1].set_xlabel('epoch')\nax[1].set_xticks(np.arange(0, epochs))\nplt.savefig(f\"{out_dir}/paper-model-training.jpg\", dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:44:54.154665Z","iopub.execute_input":"2022-04-11T00:44:54.15504Z","iopub.status.idle":"2022-04-11T00:44:55.119535Z","shell.execute_reply.started":"2022-04-11T00:44:54.154953Z","shell.execute_reply":"2022-04-11T00:44:55.118352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"y_pred_paper = paper_model.predict(x_test, batch_size=1024, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:44:55.121333Z","iopub.execute_input":"2022-04-11T00:44:55.122059Z","iopub.status.idle":"2022-04-11T00:45:00.739351Z","shell.execute_reply.started":"2022-04-11T00:44:55.121972Z","shell.execute_reply":"2022-04-11T00:45:00.738384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"tests = pd.DataFrame(y_test, columns=labels)\npreds = pd.DataFrame(y_pred_paper, columns=labels).round()  # We have to round the probabilities (0-1 inclusive) to binary (0 or 1)\n\nacc_dict_paper = {}\nf1s_dict_paper = {}\nfig, axes = plt.subplots(ncols=3, nrows=2, figsize=(12, 6))\nfor col, ax in zip(labels, axes.flatten()):\n    true_label = tests[col]\n    prediction = preds[col]\n    # Record accuracy and F1 score\n    acc = acc_dict_paper[col] = accuracy_score(true_label, prediction)\n    f1 = f1s_dict_paper[col] = f1_score(true_label, prediction)\n    # Confusion matrix\n    (tn, fp), (fn, tp) = conf_matt = confusion_matrix(true_label, prediction)\n    sns.heatmap(conf_matt, annot=True, cbar=False, fmt=\".2f\", ax=ax)\n    ax.set_title(f\"{col}\\naccuracy: {acc:.3f}\\nF1-score: {f1:.3f}\")\n    ax.set_xlabel(\"Prediction\")\n    ax.set_ylabel(\"True\")\nplt.tight_layout()\nplt.savefig(f\"{out_dir}/paper-model-confusion-matrix.jpg\", dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:45:00.741341Z","iopub.execute_input":"2022-04-11T00:45:00.741688Z","iopub.status.idle":"2022-04-11T00:45:05.205261Z","shell.execute_reply.started":"2022-04-11T00:45:00.74163Z","shell.execute_reply":"2022-04-11T00:45:05.203979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize evaluation metric comparison dataframe\neval_df = pd.DataFrame(\n    index=[\"paper_model\",\"project_model\"],\n    columns=[\"acc_mean\", \"acc_sd\", \"f1_mean\", \"f1_sd\"]\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:45:05.211209Z","iopub.execute_input":"2022-04-11T00:45:05.214486Z","iopub.status.idle":"2022-04-11T00:45:05.230766Z","shell.execute_reply.started":"2022-04-11T00:45:05.214379Z","shell.execute_reply":"2022-04-11T00:45:05.229262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy\naccuracies_paper = list(acc_dict_paper.values())\nmu = eval_df[\"acc_mean\"][\"paper_model\"] = np.mean(accuracies_paper)\nsd = eval_df[\"acc_sd\"][\"paper_model\"] = np.std(accuracies_paper)\nprint(f\"Avg accuracy: {mu:.3f} ± {sd:.3f}\")\n\n# f1-score\nf1_scores_paper = list(f1s_dict_paper.values())\nmu = eval_df[\"f1_mean\"][\"paper_model\"] = np.mean(f1_scores_paper)\nsd = eval_df[\"f1_sd\"][\"paper_model\"] = np.std(f1_scores_paper)\nprint(f\"Avg F1-score: {mu:.3f} ± {sd:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:45:05.238329Z","iopub.execute_input":"2022-04-11T00:45:05.241816Z","iopub.status.idle":"2022-04-11T00:45:05.258562Z","shell.execute_reply.started":"2022-04-11T00:45:05.241742Z","shell.execute_reply":"2022-04-11T00:45:05.257324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Project Model Definition\nNow we define the model. We use the following:\n\n* Glove embedding layer\n* Dropout layer\n* Bidirectional GRU layer\n* 1D Convolution layer \n* Average & Max pooling layer\n* Dense layer \n* Six category output, using binary cross entropy\n\nSome experimentation has gone into parameterizing the model, but due to a lack of resources it can be likely parameterized further.","metadata":{}},{"cell_type":"code","source":"model_input = Input(shape=(maxlen, )) \nx = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(model_input)\nx = SpatialDropout1D(0.1)(x)\nx = Bidirectional(GRU(200, return_sequences=True,dropout=0.25,recurrent_dropout=0.25,implementation=1))(x)\nx = Conv1D(128, kernel_size = 3)(x)   \navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nx = concatenate([avg_pool, max_pool])   \npreds = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(model_input, preds, name=\"project_model\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:45:05.260527Z","iopub.execute_input":"2022-04-11T00:45:05.261274Z","iopub.status.idle":"2022-04-11T00:45:06.816988Z","shell.execute_reply.started":"2022-04-11T00:45:05.261213Z","shell.execute_reply":"2022-04-11T00:45:06.8132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Project Model Training\nFinally, we fit the model to the entire test set, and run for 10 epochs. Each epoch, on a Kaggle CPU, takes around half an hour, so 10 iterations takes around 5 hours.","metadata":{}},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',optimizer=Adam(lr=2e-4),metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:45:06.822277Z","iopub.execute_input":"2022-04-11T00:45:06.825372Z","iopub.status.idle":"2022-04-11T04:09:15.988396Z","shell.execute_reply.started":"2022-04-11T00:45:06.825269Z","shell.execute_reply":"2022-04-11T04:09:15.987434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(12, 6))\n\n# summarize history for accuracy\nax[0].plot(history.history['accuracy'])\nax[0].set_title('project model accuracy')\nax[0].set_ylabel('accuracy')\nax[0].set_xlabel('epoch')\nax[0].set_xticks(np.arange(0, epochs))\n# summarize history for loss\nax[1].plot(history.history['loss'], c=\"orange\")\nax[1].set_title('project model loss')\nax[1].set_ylabel('loss')\nax[1].set_xlabel('epoch')\nax[1].set_xticks(np.arange(0, epochs))\nplt.savefig(f\"{out_dir}/project-model-training.jpg\", dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T04:09:15.99022Z","iopub.execute_input":"2022-04-11T04:09:15.99056Z","iopub.status.idle":"2022-04-11T04:09:16.889718Z","shell.execute_reply.started":"2022-04-11T04:09:15.990488Z","shell.execute_reply":"2022-04-11T04:09:16.888258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Prediction\nWe next make our prediction and save the output.","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(x_test, batch_size=1024, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T04:09:16.891387Z","iopub.execute_input":"2022-04-11T04:09:16.891784Z","iopub.status.idle":"2022-04-11T04:09:32.815247Z","shell.execute_reply.started":"2022-04-11T04:09:16.891705Z","shell.execute_reply":"2022-04-11T04:09:32.814351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"tests = pd.DataFrame(y_test, columns=labels)\npreds = pd.DataFrame(y_pred, columns=labels).round() # We have to round the probabilities (0-1 inclusive) to binary (0 or 1)\n\nacc_dict_proj = {}\nf1s_dict_proj = {}\nfig, axes = plt.subplots(ncols=3, nrows=2, figsize=(12, 6))\nfor col, ax in zip(labels, axes.flatten()):\n    true_label = tests[col]\n    prediction = preds[col]\n    # Record accuracy and F1 score\n    acc = acc_dict_proj[col] = accuracy_score(true_label, prediction)\n    f1 = f1s_dict_proj[col] = f1_score(true_label, prediction)\n    # Confusion matrix\n    (tn, fp), (fn, tp) = conf_matt = confusion_matrix(true_label, prediction)\n    sns.heatmap(conf_matt, annot=True, cbar=False, fmt=\".2f\", ax=ax)\n    ax.set_title(f\"{col}\\naccuracy: {acc:.3f}\\nF1-score: {f1:.3f}\")\n    ax.set_xlabel(\"Prediction\")\n    ax.set_ylabel(\"True\")\nplt.tight_layout()\nplt.savefig(f\"{out_dir}/project-model-confusion-matrix.jpg\", dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T04:09:32.818972Z","iopub.execute_input":"2022-04-11T04:09:32.819284Z","iopub.status.idle":"2022-04-11T04:09:37.331922Z","shell.execute_reply.started":"2022-04-11T04:09:32.819226Z","shell.execute_reply":"2022-04-11T04:09:37.330823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy\naccuracies_proj = list(acc_dict_proj.values())\nmu = eval_df[\"acc_mean\"][\"project_model\"] = np.mean(accuracies_proj)\nsd = eval_df[\"acc_sd\"][\"project_model\"] = np.std(accuracies_proj)\nprint(f\"Avg accuracy: {mu:.3f} ± {sd:.3f}\")\n\n# f1-score\nf1_scores_proj = list(f1s_dict_proj.values())\nmu = eval_df[\"f1_mean\"][\"project_model\"] = np.mean(f1_scores_proj)\nsd = eval_df[\"f1_sd\"][\"project_model\"] = np.std(f1_scores_proj)\nprint(f\"Avg F1-score: {mu:.3f} ± {sd:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T04:09:37.333708Z","iopub.execute_input":"2022-04-11T04:09:37.33444Z","iopub.status.idle":"2022-04-11T04:09:37.349337Z","shell.execute_reply.started":"2022-04-11T04:09:37.334352Z","shell.execute_reply":"2022-04-11T04:09:37.347553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save final evaluation metrics\nSave evaluation metric comparison dataframe to file","metadata":{}},{"cell_type":"code","source":"eval_file = f\"{out_dir}/model-performance-comparison.csv\"\neval_df.to_csv(eval_file)\nprint(\"Evaluation metric comparison saved at:\", eval_file)\neval_df","metadata":{"execution":{"iopub.status.busy":"2022-04-11T04:09:37.351103Z","iopub.execute_input":"2022-04-11T04:09:37.351598Z","iopub.status.idle":"2022-04-11T04:09:37.539098Z","shell.execute_reply.started":"2022-04-11T04:09:37.35153Z","shell.execute_reply":"2022-04-11T04:09:37.537844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# eval_df[\"accuracy_mean\"][\"project_model\"] = \n# eval_df[\"accuracy_sd\"][\"project_model\"] = \n\n# eval_df[\"f1_mean\"][\"project_model\"] = \n# eval_df[\"f1_sd\"][\"project_model\"] = ","metadata":{"execution":{"iopub.status.busy":"2022-04-11T04:09:37.540966Z","iopub.execute_input":"2022-04-11T04:09:37.541771Z","iopub.status.idle":"2022-04-11T04:09:37.546901Z","shell.execute_reply.started":"2022-04-11T04:09:37.54169Z","shell.execute_reply":"2022-04-11T04:09:37.54522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}