{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #visualization\n\nfrom keras.preprocessing.text import Tokenizer # tokenize\nfrom keras.preprocessing.sequence import pad_sequences # padding\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Embedding # model layers \nfrom keras.models import Model # model\nfrom sklearn.metrics import roc_auc_score # model evaluation score metrics\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#in this notebook, pretrained GloVe is used as the word vector for the model\n\n#word vector link\nword_vec_link = 'http://nlp.standford.edu/data/glove.6B.zip' #specify link for the GloVe\n!wget http://nlp.stanford.edu/data/glove.6B.zip #download the word vector\n!unzip glove*.zip #unzip the word vector\n\n#unzip dataset\n!unzip ../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set default config\nMAX_SEQUENCE_LENGTH = 100\nMAX_VOCAB_SIZE  = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\nBATCH_SIZE = 128\nEPOCHS = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load glove vector\n\nprint('loading word vectors...')\nword2vec = {}\nwith open(os.path.join('./glove.6B.%sd.txt' %EMBEDDING_DIM)) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype = 'float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('./train.csv') #load dataset\nsentences = train['comment_text'].fillna('DUMMY_VALUE').values #take only the comment part and fill the missing value\npossible_labels = ['toxic', 'severe_toxic', 'obscene','threat', 'insult', 'identity_hate'] #identify labels column\ntargets = train[possible_labels].values #take out the output label from the dataset from the label column \n\nprint('max_sequence_length:', max(len(s) for s in sentences))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert sentence to integer\n\ntokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE) #specify the number of word that need to be used\ntokenizer.fit_on_texts(sentences) #create a dictionary of size max of words arranged by most recurrence word exist in the dataset \nsequences = tokenizer.texts_to_sequences(sentences) #tokenize the dataset('sentences') and convert each word based on created dictionary\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample output\nprint('actual sentence:')\nprint(sentences[0])\nprint('')\nprint('after convert using tokenizer:')\nprint(sequences[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the total number of words exist in the tokenizer \nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n\n#dont get confused with the specified 'num_words' when creating the tokenizer before because: \n#1) tokenizer made all word index for all unique word exist in the dataset\n#2) the 'num_words' specify before is to specify on how many words that want to use from all word index created in the tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pad sequences to make all the sentences to have same length\ndata = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor: ', data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample shape padding\nprint(data[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare embedding matrix and replace the each word in the tokenizer according to the weight specify from the word embedding\n#note that the output may be bigger from the word because the word in the word embedding have a bigger shape\n\nprint('Filling pre_trained embedding....')\nnum_words = min(MAX_VOCAB_SIZE, len(word2idx) +1) #specify the num_words\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) #create embedding matrix np array with shape (num_words, embedding dimenstion) '20 000 x 100'\nfor word, i in word2idx.items(): #fill the embedding matrix with the pretrained glove\n    if i < MAX_VOCAB_SIZE:\n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\nprint('Done!!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample output shape for word 'the'\n\nprint('word \"the\"')\nprint()\nprint('\"the\" in transform tokenizer')\nprint(word2idx['the'])\nprint()\nprint('\"the\" in transform GloVe vector')\nprint(word2vec.get('the'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#specify the Embedding layer\n\nembedding_layer = Embedding(\n    num_words,\n    EMBEDDING_DIM,\n    weights = [embedding_matrix],\n    input_length = MAX_SEQUENCE_LENGTH,\n    trainable =  False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#build the model\nprint('Building model...')\n\n#train a 1D convnet with global maxpooling\n\ninput_ = Input(shape = (MAX_SEQUENCE_LENGTH,))\nx = embedding_layer(input_)\nx = Conv1D(128,3,activation = 'relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128,3,activation = 'relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128,3,activation = 'relu')(x)\nx = GlobalMaxPooling1D()(x)\noutput = Dense(len(possible_labels), activation = 'sigmoid')(x)\n\nmodel = Model(input_,output)\nmodel.compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = ['accuracy']\n)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training model....')\nr = model.fit(\n    data,\n    targets,\n    batch_size = BATCH_SIZE,\n    epochs = EPOCHS,\n    validation_split = VALIDATION_SPLIT\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot loss\nplt.plot(r.history['loss'], label = 'loss')\nplt.plot(r.history['val_loss'], label = 'val_loss')\nplt.legend()\nplt.show()\n\n\n#plot accuracy\nplt.plot(r.history['accuracy'], label = 'acc')\nplt.plot(r.history['val_accuracy'], label = 'val_acc')\nplt.legend()\nplt.show()\n\n#plot mean AUC over each label\np = model.predict(data)\naucs = []\nfor j in range(6):\n    auc = roc_auc_score(targets[:,j], p[:,j])\n    aucs.append(auc)\nprint(np.mean(aucs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}