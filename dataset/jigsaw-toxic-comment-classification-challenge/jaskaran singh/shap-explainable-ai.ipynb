{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is Explainable AI?\n\n**Explainable AI** is an emerging field in machine learning that aims to address how black box decisions of AI are made. This area inspects and tries to understand the steps & models involved in making decisions. It answers questions like - \n\n1. Why did the AI system make a specific prediction or decision?\n2. Why didnâ€™t the AI system do something else?\n3. When do AI system give enough confidence in the decision that one can trust it, how can the AI system correct    errors that arise?\n\nTo promote explainable AI, researchers have been developing tools and techniques such as What-if Tool, LIME,SHAP TreeInterpreter and many more.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Shapley value\n\nA prediction can be explained by assuming that each feature value of the instance is a \"player\" in a game where the prediction is the payout. Shapley values -- a method from coalitional game theory -- tells us how to fairly distribute the \"payout\" among the features.\n\nLloyd shapley Idea:- Members should recieve payments or share propotionals to their marginal contribution.\n\nThe Shapley value is the only attribution method that satisfies the properties Efficiency, Symmetry, Dummy and Additivity, which together can be considered a definition of a fair payout.\n\nTo get deeper into the definition and computation of the Shapley value , you can refer this link https://christophm.github.io/interpretable-ml-book/shapley.html\n\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\nres_df1=df.loc[(df['toxic'] == 1) | (df['severe_toxic'] == 1) | (df['obscene'] == 1) | (df['threat'] == 1) | (df['insult'] == 1) | (df['identity_hate'] == 1)] \nres_df1.loc[:,'label'] = 1\nres_df2=df.loc[(df['toxic'] == 0) & (df['severe_toxic'] == 0) & (df['obscene'] == 0) & (df['threat'] == 0) & (df['insult'] == 0) & (df['identity_hate'] == 0)] \nres_df2.loc[:,'label'] = 0\ndf=res_df1.append(res_df2)\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df['comment_text'],df['label'], test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf= TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,1),max_features=10000,stop_words='english')\nfeatures= tfidf.fit_transform(X_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=features.todense()\ntest_tfidf= tfidf.transform(X_test)\ntest_tfidf=test_tfidf.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#random\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=15, n_estimators=40)\nclf.fit(features, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data=X_test.tolist()\ntest_data[12445]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Working with Shap Library","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we use Force Plot to visualize the contribution of various features(words) which makes the sentence as toxic or non toxic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nexplainer = shap.TreeExplainer(clf)\nchoosen_instance = test_tfidf[12445]\nchoosen_instance = np.squeeze(np.asarray(choosen_instance))\nshap_values = explainer.shap_values(choosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0],choosen_instance,feature_names=tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can see the contribution of **thanks** word is quite high as compare to other features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data[37]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"choosen_instance = test_tfidf[37]\nchoosen_instance = np.squeeze(np.asarray(choosen_instance))\nshap_values = explainer.shap_values(choosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1],choosen_instance,feature_names=tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here Words like fuck, balls , ass have contribution which makes a toxic sentence.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data[75]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"choosen_instance = test_tfidf[75]\nchoosen_instance = np.squeeze(np.asarray(choosen_instance))\nshap_values = explainer.shap_values(choosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1],choosen_instance,feature_names=tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"another toxic example","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}