{"cells":[{"metadata":{},"cell_type":"markdown","source":"### References\nDo the ensemble and add weights for the logistic regression of the following notebooks:\n* Notebook 1: [Link](https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta)\n* Notebook 2: [Link](https://www.kaggle.com/swannnn/jigsaw-tpu-xlm-roberta-e3ad07)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re, string\nfrom scipy.special import softmax\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_union\nfrom scipy.sparse import hstack","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip').fillna(' ')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip').fillna(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = train['comment_text']\ntest_text = test['comment_text']\nall_text = pd.concat([train_text, test_text])\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vec = TfidfVectorizer(\n                    sublinear_tf=True,\n                    strip_accents='unicode',\n                    analyzer='word',\n                    token_pattern=r'\\w{1,}',\n                    ngram_range=(1, 2),\n                    max_features=30000)\n\nchar_vec = TfidfVectorizer(\n                    sublinear_tf=True,\n                    strip_accents='unicode',\n                    analyzer='char',\n                    ngram_range=(1, 3),\n                    max_features=30000)\n\nvec1 = make_union(word_vec, char_vec)\nvec1.fit(all_text)\ntrain_features = vec1.transform(train_text)\ntest_features = vec1.transform(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): \n    return re_tok.sub(r' \\1 ', s).split()\n\nvec2 = TfidfVectorizer(ngram_range=(1,2), \n                      tokenizer=tokenize,\n                      min_df=3, \n                      max_df=0.9, \n                      strip_accents='unicode', \n                      use_idf=1,\n                      smooth_idf=1, \n                      sublinear_tf=1)\ntrain_term_doc = vec2.fit_transform(train_text)\ntest_term_doc = vec2.transform(test_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build and Run model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# weighted logistic regression\nscores1 = []\nsub1 = pd.DataFrame.from_dict({'id': test['id']})\nfor class_name in class_names:\n    train_target = train[class_name]\n    classifier = LogisticRegression(C = 0.1, solver='saga', class_weight='balanced')\n\n    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n    scores1.append(cv_score)\n    print('CV score for class {} is {:.4%}'.format(class_name, cv_score))\n\n    classifier.fit(train_features, train_target)\n    sub1[class_name] = classifier.predict_proba(test_features)[:, 1]\n\n#print('Total WLR CV score is {:.4%}'.format(np.mean(scores1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nb-svm\ntrain_x = train_term_doc\ntest_x = test_term_doc\n\ndef pr(y_i, y):\n    p = train_x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)\n\ndef get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, solver='liblinear', dual=True)\n    x_nb = train_x.multiply(r)\n    return m.fit(x_nb, y), r\n\nscores2 = []\nsub2 = pd.DataFrame.from_dict({'id': test['id']})\nfor i, j in enumerate(class_names):\n    m,r = get_mdl(train[j])\n    sub2[j] = m.predict_proba(test_x.multiply(r))[:,1]\n    y_pred = m.predict_proba(train_x.multiply(r))[:,1]\n    print('fit score for class {} is {:.4%}'.format(j, roc_auc_score(train_target, y_pred)))\n    scores2.append(roc_auc_score(train_target, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sub1.copy()\nfor i, j in enumerate(class_names):\n    pb = np.array([scores1[i], scores2[i]])\n    weights = lambda x: x/sum(x)\n    w = weights(pb)\n    print(w)\n    sub[j] = sub1[j]*w[0] + sub2[j]*w[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}