{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-09T08:49:03.419638Z","iopub.execute_input":"2022-02-09T08:49:03.420028Z","iopub.status.idle":"2022-02-09T08:49:03.437962Z","shell.execute_reply.started":"2022-02-09T08:49:03.419967Z","shell.execute_reply":"2022-02-09T08:49:03.436714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\nThere is lot to learn from these notebooks. I took some time and writing this kernel, really interesting kernels and very useful with real life datasets.\n* [NLP with Disaster Tweets - EDA, Cleaning and BERT](https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n* [Improve your Score with Text Preprocessing -- V2](https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2/notebook)\n* [How to: Preprocessing when using embeddings](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:34:59.876009Z","iopub.execute_input":"2022-02-06T07:34:59.876539Z","iopub.status.idle":"2022-02-06T07:34:59.880073Z","shell.execute_reply.started":"2022-02-06T07:34:59.876498Z","shell.execute_reply":"2022-02-06T07:34:59.879294Z"}}},{"cell_type":"code","source":"import gc\nimport re\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom tqdm import tqdm\n\ntqdm.pandas()\n\nfrom wordcloud import STOPWORDS\nfrom plotly.subplots import make_subplots\n\nrandom_seed = 73","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:03.443936Z","iopub.execute_input":"2022-02-09T08:49:03.444353Z","iopub.status.idle":"2022-02-09T08:49:04.682339Z","shell.execute_reply.started":"2022-02-09T08:49:03.444318Z","shell.execute_reply":"2022-02-09T08:49:04.681622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:04.683622Z","iopub.execute_input":"2022-02-09T08:49:04.68392Z","iopub.status.idle":"2022-02-09T08:49:06.361844Z","shell.execute_reply.started":"2022-02-09T08:49:04.683886Z","shell.execute_reply":"2022-02-09T08:49:06.360873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:06.36438Z","iopub.execute_input":"2022-02-09T08:49:06.364665Z","iopub.status.idle":"2022-02-09T08:49:07.800692Z","shell.execute_reply.started":"2022-02-09T08:49:06.364632Z","shell.execute_reply":"2022-02-09T08:49:07.79952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:34:45.13425Z","iopub.execute_input":"2022-02-05T10:34:45.135412Z","iopub.status.idle":"2022-02-05T10:34:47.302643Z","shell.execute_reply.started":"2022-02-05T10:34:45.135356Z","shell.execute_reply":"2022-02-05T10:34:47.301413Z"}}},{"cell_type":"code","source":"train_df['characters length'] = train_df['comment_text'].apply(len)\ntrain_df['words length'] = train_df['comment_text'].apply(lambda x: len(x.split()))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:07.802195Z","iopub.execute_input":"2022-02-09T08:49:07.802448Z","iopub.status.idle":"2022-02-09T08:49:08.737904Z","shell.execute_reply.started":"2022-02-09T08:49:07.80242Z","shell.execute_reply":"2022-02-09T08:49:08.736628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['characters length'] = test_df['comment_text'].apply(len)\ntest_df['words length'] = test_df['comment_text'].apply(lambda x: len(x.split()))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:08.739019Z","iopub.execute_input":"2022-02-09T08:49:08.739264Z","iopub.status.idle":"2022-02-09T08:49:09.59546Z","shell.execute_reply.started":"2022-02-09T08:49:08.739236Z","shell.execute_reply":"2022-02-09T08:49:09.594479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Characters Length Distribution Train","metadata":{}},{"cell_type":"code","source":"print(train_df['characters length'].describe())\nfig = px.histogram(train_df, x='characters length', marginal='box')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:09.596763Z","iopub.execute_input":"2022-02-09T08:49:09.597126Z","iopub.status.idle":"2022-02-09T08:49:11.074005Z","shell.execute_reply.started":"2022-02-09T08:49:09.597088Z","shell.execute_reply":"2022-02-09T08:49:11.072898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Characters Length Distribution Test","metadata":{}},{"cell_type":"code","source":"print(test_df['characters length'].describe())\nfig = px.histogram(test_df, x='characters length', marginal='box')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:11.075383Z","iopub.execute_input":"2022-02-09T08:49:11.075686Z","iopub.status.idle":"2022-02-09T08:49:11.939687Z","shell.execute_reply.started":"2022-02-09T08:49:11.075651Z","shell.execute_reply":"2022-02-09T08:49:11.93864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Length Distribution Train","metadata":{}},{"cell_type":"code","source":"print(train_df['words length'].describe())\nfig = px.histogram(train_df, x='words length', marginal='box')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:11.941012Z","iopub.execute_input":"2022-02-09T08:49:11.941263Z","iopub.status.idle":"2022-02-09T08:49:12.985556Z","shell.execute_reply.started":"2022-02-09T08:49:11.941232Z","shell.execute_reply":"2022-02-09T08:49:12.98485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Length Distribution Test","metadata":{}},{"cell_type":"code","source":"print(test_df['words length'].describe())\nfig = px.histogram(test_df, x='words length', marginal='box')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:12.986776Z","iopub.execute_input":"2022-02-09T08:49:12.987188Z","iopub.status.idle":"2022-02-09T08:49:13.82841Z","shell.execute_reply.started":"2022-02-09T08:49:12.987146Z","shell.execute_reply":"2022-02-09T08:49:13.827743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target Distributions","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=3)\nk = 2\nfor i in range(2):\n    for j in range(3):\n        fig.add_trace(px.histogram(train_df, x=train_df.columns[k], text_auto=True)['data'][0],\n                      row=i+1, col=j+1)\n        fig.update_layout(bargap=0.2)\n        k += 1\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:13.829433Z","iopub.execute_input":"2022-02-09T08:49:13.829839Z","iopub.status.idle":"2022-02-09T08:49:19.244495Z","shell.execute_reply.started":"2022-02-09T08:49:13.829799Z","shell.execute_reply":"2022-02-09T08:49:19.243344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Each class is also highly unbalacned","metadata":{}},{"cell_type":"markdown","source":"## Maximum number of target assigned to text","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(train_df.iloc[:, 2:-2].sum(axis=1), barmode='group', text_auto=True)\nfig.update_layout(bargap=0.3)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:19.252075Z","iopub.execute_input":"2022-02-09T08:49:19.252627Z","iopub.status.idle":"2022-02-09T08:49:20.223457Z","shell.execute_reply.started":"2022-02-09T08:49:19.252582Z","shell.execute_reply":"2022-02-09T08:49:20.222615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can see that most of text data is not toxic\n* Data is multi label and second highest is with single label only","metadata":{}},{"cell_type":"markdown","source":"## Build Vocab","metadata":{}},{"cell_type":"code","source":"def build_vocab(sentences):\n    vocab = {}\n    for sentence in tqdm(sentences):\n        for word in sentence:\n            vocab[word] = vocab.get(word, 0) + 1\n    return vocab","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:20.227056Z","iopub.execute_input":"2022-02-09T08:49:20.227305Z","iopub.status.idle":"2022-02-09T08:49:20.232601Z","shell.execute_reply.started":"2022-02-09T08:49:20.227277Z","shell.execute_reply":"2022-02-09T08:49:20.231478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = train_df['comment_text'].apply(lambda x: x.split()).to_numpy()\nvocab = build_vocab(sentences)\nsorted(vocab.items(), key=lambda x: x[1], reverse=True)[:10]","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:20.234069Z","iopub.execute_input":"2022-02-09T08:49:20.234927Z","iopub.status.idle":"2022-02-09T08:49:26.818433Z","shell.execute_reply.started":"2022-02-09T08:49:20.234877Z","shell.execute_reply":"2022-02-09T08:49:26.817374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# N-grams","metadata":{}},{"cell_type":"code","source":"def generate_ngram_vocab(sentences, n_gram=1):\n    def generate_ngrams(sentence, n_gram):\n        tokens = [token for token in sentence if token != '' if token not in STOPWORDS]\n        ngrams = zip(*[tokens[i:] for i in range(n_gram)])\n        ngrams_list = []\n        for ngram in ngrams:\n            ngrams_list.append(\" \".join(ngram))\n        return ngrams_list\n\n    n_gram_dict = {}\n    for sentence in sentences:\n        for token in generate_ngrams(sentence, n_gram):\n            n_gram_dict[token] = n_gram_dict.get(token, 0) + 1\n    return n_gram_dict","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:26.819736Z","iopub.execute_input":"2022-02-09T08:49:26.819992Z","iopub.status.idle":"2022-02-09T08:49:26.828422Z","shell.execute_reply.started":"2022-02-09T08:49:26.819962Z","shell.execute_reply":"2022-02-09T08:49:26.82733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unigrams, Bigrams and Trigrams","metadata":{}},{"cell_type":"markdown","source":"### Unigram Analysis","metadata":{}},{"cell_type":"code","source":"toxic_sent = train_df[train_df['toxic'] == 1]['comment_text'].apply(lambda x: x.lower().split()).to_numpy()\nsevere_toxic_sent = train_df[train_df['severe_toxic'] == 1]['comment_text'].apply(lambda x: x.lower().split()).to_numpy()\nobscene_sent = train_df[train_df['obscene'] == 1]['comment_text'].apply(lambda x: x.lower().split()).to_numpy()\nthreat_sent = train_df[train_df['threat'] == 1]['comment_text'].apply(lambda x: x.lower().split()).to_numpy()\ninsult_sent = train_df[train_df['insult'] == 1]['comment_text'].apply(lambda x: x.lower().split()).to_numpy()\nidentity_hate_sent = train_df[train_df['identity_hate'] == 1]['comment_text'].apply(lambda x: x.lower().split()).to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:26.830462Z","iopub.execute_input":"2022-02-09T08:49:26.831295Z","iopub.status.idle":"2022-02-09T08:49:27.173947Z","shell.execute_reply.started":"2022-02-09T08:49:26.831244Z","shell.execute_reply":"2022-02-09T08:49:27.172684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_unigram_dict = generate_ngram_vocab(toxic_sent, n_gram=1)\nsevere_toxic_unigram_dict = generate_ngram_vocab(severe_toxic_sent, n_gram=1)\nobscene_unigram_dict = generate_ngram_vocab(obscene_sent, n_gram=1)\nthreat_unigram_dict = generate_ngram_vocab(threat_sent, n_gram=1)\ninsult_unigram_dict = generate_ngram_vocab(insult_sent, n_gram=1)\nidentity_hate_unigram_dict = generate_ngram_vocab(identity_hate_sent, n_gram=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:27.175275Z","iopub.execute_input":"2022-02-09T08:49:27.175526Z","iopub.status.idle":"2022-02-09T08:49:28.102722Z","shell.execute_reply.started":"2022-02-09T08:49:27.175497Z","shell.execute_reply":"2022-02-09T08:49:28.101755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_unigrams_df = pd.DataFrame(sorted(toxic_unigram_dict.items(), key=lambda x: x[1])[::-1])\nsevere_toxic_unigrams_df = pd.DataFrame(sorted(severe_toxic_unigram_dict.items(), key=lambda x: x[1])[::-1])\nobscene_unigrams_df = pd.DataFrame(sorted(obscene_unigram_dict.items(), key=lambda x: x[1])[::-1])\nthreat_unigrams_df = pd.DataFrame(sorted(threat_unigram_dict.items(), key=lambda x: x[1])[::-1])\ninsult_unigrams_df = pd.DataFrame(sorted(insult_unigram_dict.items(), key=lambda x: x[1])[::-1])\nidentity_hate_unigrams_df = pd.DataFrame(sorted(identity_hate_unigram_dict.items(), key=lambda x: x[1])[::-1])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:28.104096Z","iopub.execute_input":"2022-02-09T08:49:28.104349Z","iopub.status.idle":"2022-02-09T08:49:28.305433Z","shell.execute_reply.started":"2022-02-09T08:49:28.10432Z","shell.execute_reply":"2022-02-09T08:49:28.304039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=6, figsize=(18, 50), dpi=100)\nplt.tight_layout()\n\nN = 100\nsns.barplot(y=toxic_unigrams_df[0].values[:N], x=toxic_unigrams_df[1].values[:N], ax=axes[0])\nsns.barplot(y=severe_toxic_unigrams_df[0].values[:N], x=severe_toxic_unigrams_df[1].values[:N], ax=axes[1])\nsns.barplot(y=obscene_unigrams_df[0].values[:N], x=obscene_unigrams_df[1].values[:N], ax=axes[2])\nsns.barplot(y=threat_unigrams_df[0].values[:N], x=threat_unigrams_df[1].values[:N], ax=axes[3])\nsns.barplot(y=insult_unigrams_df[0].values[:N], x=insult_unigrams_df[1].values[:N], ax=axes[4])\nsns.barplot(y=identity_hate_unigrams_df[0].values[:N], x=identity_hate_unigrams_df[1].values[:N], ax=axes[5])\n\nfor i in range(6):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} Toxic Comments', fontsize=15)\naxes[1].set_title(f'Top {N} Severe Toxic Comments', fontsize=15)\naxes[2].set_title(f'Top {N} Obsece Comments', fontsize=15)\naxes[3].set_title(f'Top {N} Threat Comments', fontsize=15)\naxes[4].set_title(f'Top {N} Insult Comments', fontsize=15)\naxes[5].set_title(f'Top {N} Identity Hate Comments', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:28.307939Z","iopub.execute_input":"2022-02-09T08:49:28.308687Z","iopub.status.idle":"2022-02-09T08:49:37.901766Z","shell.execute_reply.started":"2022-02-09T08:49:28.30863Z","shell.execute_reply":"2022-02-09T08:49:37.900831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bigram Analysis","metadata":{}},{"cell_type":"code","source":"toxic_bigram_dict = generate_ngram_vocab(toxic_sent, n_gram=2)\nsevere_toxic_bigram_dict = generate_ngram_vocab(severe_toxic_sent, n_gram=2)\nobscene_bigram_dict = generate_ngram_vocab(obscene_sent, n_gram=2)\nthreat_bigram_dict = generate_ngram_vocab(threat_sent, n_gram=2)\ninsult_bigram_dict = generate_ngram_vocab(insult_sent, n_gram=2)\nidentity_hate_bigram_dict = generate_ngram_vocab(identity_hate_sent, n_gram=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:37.903116Z","iopub.execute_input":"2022-02-09T08:49:37.903382Z","iopub.status.idle":"2022-02-09T08:49:38.977222Z","shell.execute_reply.started":"2022-02-09T08:49:37.903349Z","shell.execute_reply":"2022-02-09T08:49:38.976366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_bigrams_df = pd.DataFrame(sorted(toxic_bigram_dict.items(), key=lambda x: x[1])[::-1])\nsevere_toxic_bigrams_df = pd.DataFrame(sorted(severe_toxic_unigram_dict.items(), key=lambda x: x[1])[::-1])\nobscene_bigrams_df = pd.DataFrame(sorted(obscene_bigram_dict.items(), key=lambda x: x[1])[::-1])\nthreat_bigrams_df = pd.DataFrame(sorted(threat_bigram_dict.items(), key=lambda x: x[1])[::-1])\ninsult_bigrams_df = pd.DataFrame(sorted(insult_bigram_dict.items(), key=lambda x: x[1])[::-1])\nidentity_hate_bigrams_df = pd.DataFrame(sorted(identity_hate_bigram_dict.items(), key=lambda x: x[1])[::-1])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:38.97846Z","iopub.execute_input":"2022-02-09T08:49:38.979198Z","iopub.status.idle":"2022-02-09T08:49:39.576154Z","shell.execute_reply.started":"2022-02-09T08:49:38.979155Z","shell.execute_reply":"2022-02-09T08:49:39.574812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=6, figsize=(18, 50), dpi=100)\nplt.tight_layout()\n\nN = 100\nsns.barplot(y=toxic_bigrams_df[0].values[:N], x=toxic_bigrams_df[1].values[:N], ax=axes[0])\nsns.barplot(y=severe_toxic_bigrams_df[0].values[:N], x=severe_toxic_bigrams_df[1].values[:N], ax=axes[1])\nsns.barplot(y=obscene_bigrams_df[0].values[:N], x=obscene_bigrams_df[1].values[:N], ax=axes[2])\nsns.barplot(y=threat_bigrams_df[0].values[:N], x=threat_bigrams_df[1].values[:N], ax=axes[3])\nsns.barplot(y=insult_bigrams_df[0].values[:N], x=insult_bigrams_df[1].values[:N], ax=axes[4])\nsns.barplot(y=identity_hate_bigrams_df[0].values[:N], x=identity_hate_bigrams_df[1].values[:N], ax=axes[5])\n\nfor i in range(6):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} Toxic Comments', fontsize=15)\naxes[1].set_title(f'Top {N} Severe Toxic Comments', fontsize=15)\naxes[2].set_title(f'Top {N} Obsece Comments', fontsize=15)\naxes[3].set_title(f'Top {N} Threat Comments', fontsize=15)\naxes[4].set_title(f'Top {N} Insult Comments', fontsize=15)\naxes[5].set_title(f'Top {N} Identity Hate Comments', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:39.57798Z","iopub.execute_input":"2022-02-09T08:49:39.578718Z","iopub.status.idle":"2022-02-09T08:49:50.898001Z","shell.execute_reply.started":"2022-02-09T08:49:39.578664Z","shell.execute_reply":"2022-02-09T08:49:50.89727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trigrams Analysis","metadata":{}},{"cell_type":"code","source":"toxic_trigram_dict = generate_ngram_vocab(toxic_sent, n_gram=3)\nsevere_toxic_trigram_dict = generate_ngram_vocab(severe_toxic_sent, n_gram=3)\nobscene_trigram_dict = generate_ngram_vocab(obscene_sent, n_gram=3)\nthreat_trigram_dict = generate_ngram_vocab(threat_sent, n_gram=3)\ninsult_trigram_dict = generate_ngram_vocab(insult_sent, n_gram=3)\nidentity_hate_trigram_dict = generate_ngram_vocab(identity_hate_sent, n_gram=3)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:50.89908Z","iopub.execute_input":"2022-02-09T08:49:50.899842Z","iopub.status.idle":"2022-02-09T08:49:52.003955Z","shell.execute_reply.started":"2022-02-09T08:49:50.899795Z","shell.execute_reply":"2022-02-09T08:49:52.002487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_trigrams_df = pd.DataFrame(sorted(toxic_trigram_dict.items(), key=lambda x: x[1])[::-1])\nsevere_toxic_trigrams_df = pd.DataFrame(sorted(severe_toxic_trigram_dict.items(), key=lambda x: x[1])[::-1])\nobscene_trigrams_df = pd.DataFrame(sorted(obscene_trigram_dict.items(), key=lambda x: x[1])[::-1])\nthreat_trigrams_df = pd.DataFrame(sorted(threat_trigram_dict.items(), key=lambda x: x[1])[::-1])\ninsult_trigrams_df = pd.DataFrame(sorted(insult_trigram_dict.items(), key=lambda x: x[1])[::-1])\nidentity_hate_trigrams_df = pd.DataFrame(sorted(identity_hate_trigram_dict.items(), key=lambda x: x[1])[::-1])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:52.005349Z","iopub.execute_input":"2022-02-09T08:49:52.005676Z","iopub.status.idle":"2022-02-09T08:49:52.569942Z","shell.execute_reply.started":"2022-02-09T08:49:52.005638Z","shell.execute_reply":"2022-02-09T08:49:52.569086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=6, figsize=(18, 50), dpi=100)\nplt.tight_layout()\n\nN = 100\nsns.barplot(y=toxic_trigrams_df[0].values[:N], x=toxic_trigrams_df[1].values[:N], ax=axes[0])\nsns.barplot(y=severe_toxic_trigrams_df[0].values[:N], x=severe_toxic_trigrams_df[1].values[:N], ax=axes[1])\nsns.barplot(y=threat_trigrams_df[0].values[:N], x=threat_trigrams_df[1].values[:N], ax=axes[2])\nsns.barplot(y=threat_trigrams_df[0].values[:N], x=threat_trigrams_df[1].values[:N], ax=axes[3])\nsns.barplot(y=insult_trigrams_df[0].values[:N], x=insult_trigrams_df[1].values[:N], ax=axes[4])\nsns.barplot(y=identity_hate_trigrams_df[0].values[:N], x=identity_hate_trigrams_df[1].values[:N], ax=axes[5])\n\nfor i in range(6):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} Toxic Comments', fontsize=15)\naxes[1].set_title(f'Top {N} Severe Toxic Comments', fontsize=15)\naxes[2].set_title(f'Top {N} Obsece Comments', fontsize=15)\naxes[3].set_title(f'Top {N} Threat Comments', fontsize=15)\naxes[4].set_title(f'Top {N} Insult Comments', fontsize=15)\naxes[5].set_title(f'Top {N} Identity Hate Comments', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:49:52.571041Z","iopub.execute_input":"2022-02-09T08:49:52.572219Z","iopub.status.idle":"2022-02-09T08:50:05.871819Z","shell.execute_reply.started":"2022-02-09T08:49:52.572165Z","shell.execute_reply":"2022-02-09T08:50:05.870648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* we can see threat sentences real threate words\n* hate sentences also have clear difference from other type of sentences\n* There are repitition of particular words more than once and exclamtion marks as well.We can try to remove those with tokenizers.","metadata":{}},{"cell_type":"markdown","source":"# Embeddings and Cleaning Text","metadata":{}},{"cell_type":"code","source":"def load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:05.873312Z","iopub.execute_input":"2022-02-09T08:50:05.873719Z","iopub.status.idle":"2022-02-09T08:50:05.880515Z","shell.execute_reply.started":"2022-02-09T08:50:05.873684Z","shell.execute_reply":"2022-02-09T08:50:05.879538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import operator\n\ndef check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:05.882198Z","iopub.execute_input":"2022-02-09T08:50:05.882493Z","iopub.status.idle":"2022-02-09T08:50:05.903593Z","shell.execute_reply.started":"2022-02-09T08:50:05.882457Z","shell.execute_reply":"2022-02-09T08:50:05.902682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove_path = \"../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl\"\nglove_emb = np.load(glove_path, allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:05.905097Z","iopub.execute_input":"2022-02-09T08:50:05.905649Z","iopub.status.idle":"2022-02-09T08:50:17.513996Z","shell.execute_reply.started":"2022-02-09T08:50:05.905606Z","shell.execute_reply":"2022-02-09T08:50:17.512931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Glove 840 : \")\noov_glove = check_coverage(vocab, glove_emb)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:17.515411Z","iopub.execute_input":"2022-02-09T08:50:17.516284Z","iopub.status.idle":"2022-02-09T08:50:18.415242Z","shell.execute_reply.started":"2022-02-09T08:50:17.516216Z","shell.execute_reply":"2022-02-09T08:50:18.414135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(oov_glove[:10])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:18.416769Z","iopub.execute_input":"2022-02-09T08:50:18.417129Z","iopub.status.idle":"2022-02-09T08:50:18.425006Z","shell.execute_reply.started":"2022-02-09T08:50:18.417079Z","shell.execute_reply":"2022-02-09T08:50:18.423556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences_low = train_df['comment_text'].apply(lambda x: x.lower().split()).to_numpy()\nvocab_low = build_vocab(sentences_low)\nsorted(vocab_low.items(), key=lambda x: x[1], reverse=True)[:10]","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:18.426935Z","iopub.execute_input":"2022-02-09T08:50:18.427204Z","iopub.status.idle":"2022-02-09T08:50:25.318455Z","shell.execute_reply.started":"2022-02-09T08:50:18.427175Z","shell.execute_reply":"2022-02-09T08:50:25.317458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Glove : \")\noov_glove = check_coverage(vocab_low, glove_emb)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:25.319908Z","iopub.execute_input":"2022-02-09T08:50:25.320182Z","iopub.status.idle":"2022-02-09T08:50:26.201585Z","shell.execute_reply.started":"2022-02-09T08:50:25.320147Z","shell.execute_reply":"2022-02-09T08:50:26.2005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In both cases after making lowers we lost a significant amount of embeddings","metadata":{}},{"cell_type":"code","source":"print(oov_glove[:10])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:26.203042Z","iopub.execute_input":"2022-02-09T08:50:26.203327Z","iopub.status.idle":"2022-02-09T08:50:26.209162Z","shell.execute_reply.started":"2022-02-09T08:50:26.203292Z","shell.execute_reply":"2022-02-09T08:50:26.207996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Contractions and special characters are causing the problem","metadata":{}},{"cell_type":"code","source":"def add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:26.210609Z","iopub.execute_input":"2022-02-09T08:50:26.211145Z","iopub.status.idle":"2022-02-09T08:50:26.222925Z","shell.execute_reply.started":"2022-02-09T08:50:26.211106Z","shell.execute_reply":"2022-02-09T08:50:26.221692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Glove : \")\nadd_lower(glove_emb, vocab)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:26.224816Z","iopub.execute_input":"2022-02-09T08:50:26.225382Z","iopub.status.idle":"2022-02-09T08:50:26.589766Z","shell.execute_reply.started":"2022-02-09T08:50:26.225334Z","shell.execute_reply":"2022-02-09T08:50:26.588513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contraction_mapping = {\n    \"ain't\": \"is not\", \n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\", \n    \"'cause\": \"because\", \n    \"could've\": \"could have\", \n    \"couldn't\": \"could not\", \n    \"didn't\": \"did not\",  \n    \"doesn't\": \"does not\", \n    \"don't\": \"do not\", \n    \"hadn't\": \"had not\", \n    \"hasn't\": \"has not\", \n    \"haven't\": \"have not\", \n    \"he'd\": \"he would\",\n    \"he'll\": \"he will\", \n    \"he's\": \"he is\", \n    \"how'd\": \"how did\", \n    \"how'd'y\": \"how do you\", \n    \"how'll\": \"how will\", \n    \"how's\": \"how is\",  \n    \"I'd\": \"I would\", \n    \"I'd've\": \"I would have\", \n    \"I'll\": \"I will\", \n    \"I'll've\": \"I will have\",\n    \"I'm\": \"I am\", \n    \"I've\": \"I have\", \n    \"i'd\": \"i would\", \n    \"i'd've\": \"i would have\", \n    \"i'll\": \"i will\",  \n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\", \n    \"i've\": \"i have\", \n    \"isn't\": \"is not\", \n    \"it'd\": \"it would\", \n    \"it'd've\": \"it would have\", \n    \"it'll\": \"it will\", \"it'll've\": \n    \"it will have\",\"it's\": \"it is\", \n    \"let's\": \"let us\", \n    \"ma'am\": \"madam\", \n    \"mayn't\": \"may not\", \n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\", \n    \"must've\": \"must have\", \n    \"mustn't\": \"must not\", \n    \"mustn't've\": \"must not have\", \n    \"needn't\": \"need not\", \n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\", \n    \"oughtn't\": \"ought not\", \n    \"oughtn't've\": \"ought not have\", \n    \"shan't\": \"shall not\", \n    \"sha'n't\": \"shall not\", \n    \"shan't've\": \"shall not have\", \n    \"she'd\": \"she would\", \n    \"she'd've\": \"she would have\", \n    \"she'll\": \"she will\", \n    \"she'll've\": \"she will have\", \n    \"she's\": \"she is\", \n    \"should've\": \"should have\", \n    \"shouldn't\": \"should not\", \n    \"shouldn't've\": \"should not have\", \n    \"so've\": \"so have\",\n    \"so's\": \"so as\", \n    \"this's\": \"this is\",\n    \"that'd\": \"that would\", \n    \"that'd've\": \"that would have\", \n    \"that's\": \"that is\", \n    \"there'd\": \"there would\", \n    \"there'd've\": \"there would have\", \n    \"there's\": \"there is\", \n    \"here's\": \"here is\",\n    \"they'd\": \"they would\", \n    \"they'd've\": \"they would have\", \n    \"they'll\": \"they will\", \n    \"they'll've\": \"they will have\", \n    \"they're\": \"they are\", \n    \"they've\": \"they have\", \n    \"to've\": \"to have\", \n    \"wasn't\": \"was not\", \n    \"we'd\": \"we would\", \n    \"we'd've\": \"we would have\", \n    \"we'll\": \"we will\", \n    \"we'll've\": \"we will have\", \n    \"we're\": \"we are\", \n    \"we've\": \"we have\", \n    \"weren't\": \"were not\", \n    \"what'll\": \"what will\", \n    \"what'll've\": \"what will have\", \n    \"what're\": \"what are\",  \n    \"what's\": \"what is\", \n    \"what've\": \"what have\", \n    \"when's\": \"when is\", \n    \"when've\": \"when have\", \n    \"where'd\": \"where did\", \n    \"where's\": \"where is\", \n    \"where've\": \"where have\", \n    \"who'll\": \"who will\", \n    \"who'll've\": \"who will have\", \n    \"who's\": \"who is\", \n    \"who've\": \"who have\", \n    \"why's\": \"why is\", \n    \"why've\": \"why have\", \n    \"will've\": \"will have\", \n    \"won't\": \"will not\", \n    \"won't've\": \"will not have\", \n    \"would've\": \"would have\", \n    \"wouldn't\": \"would not\", \n    \"wouldn't've\": \"would not have\", \n    \"y'all\": \"you all\", \n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\", \n    \"you'd've\": \"you would have\", \n    \"you'll\": \"you will\", \n    \"you'll've\": \"you will have\", \n    \"you're\": \"you are\", \n    \"you've\": \"you have\" }","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:26.592433Z","iopub.execute_input":"2022-02-09T08:50:26.592747Z","iopub.status.idle":"2022-02-09T08:50:26.63157Z","shell.execute_reply.started":"2022-02-09T08:50:26.592712Z","shell.execute_reply":"2022-02-09T08:50:26.630077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:26.63343Z","iopub.execute_input":"2022-02-09T08:50:26.633826Z","iopub.status.idle":"2022-02-09T08:50:26.654469Z","shell.execute_reply.started":"2022-02-09T08:50:26.633774Z","shell.execute_reply":"2022-02-09T08:50:26.652914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"- Known Contractions -\")\nprint(\"   Glove :\")\nprint(known_contractions(glove_emb))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:26.656067Z","iopub.execute_input":"2022-02-09T08:50:26.656981Z","iopub.status.idle":"2022-02-09T08:50:26.671591Z","shell.execute_reply.started":"2022-02-09T08:50:26.656934Z","shell.execute_reply":"2022-02-09T08:50:26.669445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:26.673481Z","iopub.execute_input":"2022-02-09T08:50:26.674487Z","iopub.status.idle":"2022-02-09T08:50:26.692117Z","shell.execute_reply.started":"2022-02-09T08:50:26.674391Z","shell.execute_reply":"2022-02-09T08:50:26.688745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['comment_text'] = train_df['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\ntest_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:26.700593Z","iopub.execute_input":"2022-02-09T08:50:26.701775Z","iopub.status.idle":"2022-02-09T08:50:32.174887Z","shell.execute_reply.started":"2022-02-09T08:50:26.701724Z","shell.execute_reply":"2022-02-09T08:50:32.173771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = build_vocab(train_df['comment_text'].apply(lambda x: x.split()))\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, glove_emb)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:32.177973Z","iopub.execute_input":"2022-02-09T08:50:32.178302Z","iopub.status.idle":"2022-02-09T08:50:40.642609Z","shell.execute_reply.started":"2022-02-09T08:50:32.178265Z","shell.execute_reply":"2022-02-09T08:50:40.641423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:40.644637Z","iopub.execute_input":"2022-02-09T08:50:40.645017Z","iopub.status.idle":"2022-02-09T08:50:40.650799Z","shell.execute_reply.started":"2022-02-09T08:50:40.644967Z","shell.execute_reply":"2022-02-09T08:50:40.649615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:40.652674Z","iopub.execute_input":"2022-02-09T08:50:40.653065Z","iopub.status.idle":"2022-02-09T08:50:40.665231Z","shell.execute_reply.started":"2022-02-09T08:50:40.65303Z","shell.execute_reply":"2022-02-09T08:50:40.664387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Glove :\")\nprint(unknown_punct(glove_emb, punct))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:40.666756Z","iopub.execute_input":"2022-02-09T08:50:40.667023Z","iopub.status.idle":"2022-02-09T08:50:40.679739Z","shell.execute_reply.started":"2022-02-09T08:50:40.666991Z","shell.execute_reply":"2022-02-09T08:50:40.678996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:40.681297Z","iopub.execute_input":"2022-02-09T08:50:40.681583Z","iopub.status.idle":"2022-02-09T08:50:40.692987Z","shell.execute_reply.started":"2022-02-09T08:50:40.681523Z","shell.execute_reply":"2022-02-09T08:50:40.691938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n        \n    text = re.sub(\"(\\\\W)\",\" \",text).strip() # remove non-ascii chars\n    text = re.sub('\\S*\\d\\S*\\s*','', text).strip()  # remove words containing numbers\n    text = re.sub(' +', ' ', text)\n    \n    return text.strip()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:40.695885Z","iopub.execute_input":"2022-02-09T08:50:40.696612Z","iopub.status.idle":"2022-02-09T08:50:40.706653Z","shell.execute_reply.started":"2022-02-09T08:50:40.696525Z","shell.execute_reply":"2022-02-09T08:50:40.705839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['comment_text'] = train_df['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ntest_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:50:40.708504Z","iopub.execute_input":"2022-02-09T08:50:40.70944Z","iopub.status.idle":"2022-02-09T08:51:49.852583Z","shell.execute_reply.started":"2022-02-09T08:50:40.709386Z","shell.execute_reply":"2022-02-09T08:51:49.851775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['comment_text']","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:51:49.855222Z","iopub.execute_input":"2022-02-09T08:51:49.855826Z","iopub.status.idle":"2022-02-09T08:51:49.865741Z","shell.execute_reply.started":"2022-02-09T08:51:49.855771Z","shell.execute_reply":"2022-02-09T08:51:49.864748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = build_vocab(train_df['comment_text'].apply(lambda x: x.split()))\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, glove_emb)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:51:49.867643Z","iopub.execute_input":"2022-02-09T08:51:49.868019Z","iopub.status.idle":"2022-02-09T08:51:57.087991Z","shell.execute_reply.started":"2022-02-09T08:51:49.867972Z","shell.execute_reply":"2022-02-09T08:51:57.086899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(oov_glove))\noov_glove[:150]","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:51:57.089337Z","iopub.execute_input":"2022-02-09T08:51:57.089601Z","iopub.status.idle":"2022-02-09T08:51:57.105577Z","shell.execute_reply.started":"2022-02-09T08:51:57.08957Z","shell.execute_reply":"2022-02-09T08:51:57.104625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Replace Miss Spell Words and Combined Words","metadata":{}},{"cell_type":"code","source":"map_wrong_words = {\n    'fucksex': 'fuck sex',\n    'yourselfgo': 'your self ego',\n    'BeCauSe': 'because',\n    'DENEID': 'DENIED',\n    '\\u200e': '',\n    'CriminalWar': 'criminal war',\n    'PaTHeTiC': 'pathetic',\n    'POLITCAL': 'political',\n    'talk2me': 'talk to me',\n    'shitFuck': 'shift fuck',\n    'BabyWhat': 'baby what',\n    'Sockpuppetry': 'sock puppetry',\n    'Bastered': 'bastard',\n    'PHILIPPINESLONG': 'philippines long',\n    'SuPeRTR0LL': 'supertroll',\n    'FUCKBAGS': 'fuck bags',\n    'peNis': 'penis',\n    'pensnsnnienSNsn': 'penis',\n    'pneis': 'penis',\n    'FooL': 'fool',\n    'pennnis': 'penis',\n    'PenIS': 'penis',\n    'itsuck': 'it suck',\n    'deletionist': 'delete',\n    'ReSPeCT': 'respect'\n}\n\ndef clean_wrong_spell_words(text, mapping):\n    for word in mapping:\n        text = text.replace(word, mapping[word])\n    \n    return text\n\ntrain_df['comment_text'] = train_df['comment_text'].apply(lambda x: clean_wrong_spell_words(x, map_wrong_words))\ntest_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_wrong_spell_words(x, map_wrong_words))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:51:57.106972Z","iopub.execute_input":"2022-02-09T08:51:57.107241Z","iopub.status.idle":"2022-02-09T08:52:00.640893Z","shell.execute_reply.started":"2022-02-09T08:51:57.107207Z","shell.execute_reply":"2022-02-09T08:52:00.639993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = build_vocab(train_df['comment_text'].apply(lambda x: x.split()))\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, glove_emb)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:52:00.642347Z","iopub.execute_input":"2022-02-09T08:52:00.642922Z","iopub.status.idle":"2022-02-09T08:52:07.752619Z","shell.execute_reply.started":"2022-02-09T08:52:00.642874Z","shell.execute_reply":"2022-02-09T08:52:07.75146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del glove_emb, oov_glove, vocab_low\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:52:07.754463Z","iopub.execute_input":"2022-02-09T08:52:07.754861Z","iopub.status.idle":"2022-02-09T08:52:09.962821Z","shell.execute_reply.started":"2022-02-09T08:52:07.754811Z","shell.execute_reply":"2022-02-09T08:52:09.961764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base Model Count Vectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:52:09.964585Z","iopub.execute_input":"2022-02-09T08:52:09.964893Z","iopub.status.idle":"2022-02-09T08:52:10.015692Z","shell.execute_reply.started":"2022-02-09T08:52:09.964861Z","shell.execute_reply":"2022-02-09T08:52:10.014676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(ngram_range=(1, 3),\n                                   stop_words='english',\n                                   strip_accents='unicode',\n                                   token_pattern=r'\\w{1,}',\n                                   lowercase=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:52:10.016979Z","iopub.execute_input":"2022-02-09T08:52:10.017224Z","iopub.status.idle":"2022-02-09T08:52:10.022723Z","shell.execute_reply.started":"2022-02-09T08:52:10.017195Z","shell.execute_reply":"2022-02-09T08:52:10.021643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:52:10.024405Z","iopub.execute_input":"2022-02-09T08:52:10.025439Z","iopub.status.idle":"2022-02-09T08:52:10.082613Z","shell.execute_reply.started":"2022-02-09T08:52:10.025382Z","shell.execute_reply":"2022-02-09T08:52:10.081519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Snowball stemmer\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\n\nsnow_stemmer = SnowballStemmer(language='english')\n\ndef apply_stemmer(text):\n    words = text.split()\n    sent = [snow_stemmer.stem(word) for word in words if not word in set(STOPWORDS)]\n    return ' '.join(sent)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:52:10.084215Z","iopub.execute_input":"2022-02-09T08:52:10.085761Z","iopub.status.idle":"2022-02-09T08:52:10.276891Z","shell.execute_reply.started":"2022-02-09T08:52:10.085703Z","shell.execute_reply":"2022-02-09T08:52:10.275811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = train_df['comment_text'].apply(apply_stemmer).values, train_df[train_df.columns[2:-2]].values","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:52:10.278755Z","iopub.execute_input":"2022-02-09T08:52:10.279101Z","iopub.status.idle":"2022-02-09T08:54:30.237314Z","shell.execute_reply.started":"2022-02-09T08:52:10.279056Z","shell.execute_reply":"2022-02-09T08:54:30.236117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train_df['comment_text'].values, train_df[train_df.columns[2:-2]].values, test_size=0.3, random_state=random_seed)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:54:30.238823Z","iopub.execute_input":"2022-02-09T08:54:30.239113Z","iopub.status.idle":"2022-02-09T08:54:30.285445Z","shell.execute_reply.started":"2022-02-09T08:54:30.23908Z","shell.execute_reply":"2022-02-09T08:54:30.284641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \nX_train = count_vectorizer.fit_transform(X_train)\nX_valid = count_vectorizer.transform(X_valid)\nX_test = count_vectorizer.transform(test_df['comment_text'])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:54:30.286653Z","iopub.execute_input":"2022-02-09T08:54:30.28753Z","iopub.status.idle":"2022-02-09T08:56:00.750309Z","shell.execute_reply.started":"2022-02-09T08:54:30.287481Z","shell.execute_reply":"2022-02-09T08:56:00.749292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# logistic regression\nlog_reg = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=random_seed)\n\none_vs_rest = OneVsRestClassifier(log_reg)\none_vs_rest.fit(X_train, y_train)\n\ny_train_pred_proba = one_vs_rest.predict_proba(X_train)\ny_valid_pred_proba = one_vs_rest.predict_proba(X_valid)\n\n\nroc_auc_score_train = roc_auc_score(y_train, y_train_pred_proba,average='weighted')\nroc_auc_score_test = roc_auc_score(y_valid, y_valid_pred_proba,average='weighted')\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:56:00.751582Z","iopub.execute_input":"2022-02-09T08:56:00.751865Z","iopub.status.idle":"2022-02-09T09:06:50.305635Z","shell.execute_reply.started":"2022-02-09T08:56:00.751831Z","shell.execute_reply":"2022-02-09T09:06:50.304629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train ROC AUC Score:\", roc_auc_score_train)\nprint(\"test ROC AUC Score:\", roc_auc_score_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:06:50.307912Z","iopub.execute_input":"2022-02-09T09:06:50.30869Z","iopub.status.idle":"2022-02-09T09:06:50.315164Z","shell.execute_reply.started":"2022-02-09T09:06:50.308642Z","shell.execute_reply":"2022-02-09T09:06:50.314243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = one_vs_rest.predict_proba(X_test)\ny_test","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:14:44.95568Z","iopub.execute_input":"2022-02-09T09:14:44.956022Z","iopub.status.idle":"2022-02-09T09:14:45.325729Z","shell.execute_reply.started":"2022-02-09T09:14:44.955988Z","shell.execute_reply":"2022-02-09T09:14:45.32458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'toxic': y_test[:, 0],\n    'severe_toxic': y_test[:, 1],\n    'obscene': y_test[:, 2],\n    'threat': y_test[:, 3],\n    'insult': y_test[:, 4],\n    'identity_hate': y_test[:, 5]\n    \n})\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:19:34.257998Z","iopub.execute_input":"2022-02-09T09:19:34.258372Z","iopub.status.idle":"2022-02-09T09:19:34.334022Z","shell.execute_reply.started":"2022-02-09T09:19:34.25834Z","shell.execute_reply":"2022-02-09T09:19:34.332787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:20:17.297305Z","iopub.execute_input":"2022-02-09T09:20:17.297671Z","iopub.status.idle":"2022-02-09T09:20:19.726885Z","shell.execute_reply.started":"2022-02-09T09:20:17.297635Z","shell.execute_reply":"2022-02-09T09:20:19.725293Z"},"trusted":true},"execution_count":null,"outputs":[]}]}