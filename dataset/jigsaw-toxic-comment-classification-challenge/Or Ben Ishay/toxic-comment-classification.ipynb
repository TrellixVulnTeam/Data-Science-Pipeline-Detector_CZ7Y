{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Toxic Comment Classification:**"},{"metadata":{},"cell_type":"markdown","source":"**Pipeline:**\n\n* import libraries\n* Pre-processing:\n    * lower case \n    * decide if to replace \\n by space or period\n    * remove words that have numbers inside them \n    * remove any other non a-z charachter\n* RE:\n    * Remove non letter characters\n    * Convert to lowercase\n* We may want to use a trained model instead of training on this limied dataset.  \n\n\n\n\n1. Which word embedding methodology to use? BOW (Bag Of Word),Word2Vec,Doc2Vec,FastText,TFIDF\n2. Strategy\\plan how to clean the vocabulary\n3. Direct feaures from the text\n4. Classification of 6 categories\n5. Apply word vectors on comments\n6. EDA: correlation between the columns\n7. Kaggle collaborate on notebook\n\nDirect features:\n1. contains question Mark\n2. contains exclamation mark (especially multiple - and count them)\n3. contains words with all capital\n4. begins with hi (or similar)\n5. begins with 'I'\n6. begins with 'You'\n7. begins with a capital or small letter\n8. contains parenthesis\n9. conains curses from a glosary: fuck, shit, ass, looser, dick, bitch, suck, Nazi,stupid, bullshit, piss, cock, jew, die, cunt, rape, gay, cut, looser, nigger\n10. contains negative words: bad, terrible, kill, block, stop, wrong, destroy, absurd, don't, mom, mother, fat\n12. conains A$$HOLE, NIGGA, FVCK, cra*p, fxxk\n13. abbreviations: OMG, LMAO, WTF, FFS\n\nRemove:\n1. Multiple spaces\n2. Usernames: JacobBarnett52698\n3. email addresses\n4. '=', '-', \n    \nTerms:\n* tockens\n* corpus\n* NLP\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General tools\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport seaborn as sns\nimport re\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import Counter\nimport tensorflow as tf\nfrom sklearn.manifold import TSNE # t-distributed Stochastic Neighbor Embedding; to visualize high-dimensional data. \nfrom sklearn.feature_extraction.text import TfidfVectorizer # Convert a collection of raw documents to a matrix of TF-IDF features.\nimport gensim\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom wordcloud import WordCloud, STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Get the data:**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.\n\ntraincomments = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntraincomments['comment_characters']=traincomments['comment_text'].str.len()\ntraincomments['comment_words']=traincomments['comment_text'].str.split().str.len()\ntraincomments['number_categories']= traincomments.iloc[:, 2:8].sum(axis=1)\ntraincomments['exclamation_mark_count']=traincomments['comment_text'].str.count('!')\ntraincomments['question_mark_count']=traincomments['comment_text'].str.count('\\?')\n\ndef find_exclamation_mark_cosecutive y=re.search(['e']{2,}\n                                                 \ntraincomments['exclamation_mark_cosecutive']=traincomments['comment_text'].apply(find_exclamation_mark_cosecutive(x))\n                                                                                 \ntraincomments['all_upper']=traincomments['comment_text'].str.split().apply(lambda x: np.any([True if w.isupper() else False for w in x]))\ntraincomments['begins_greeting'] = traincomments['comment_text'].apply(lambda x: True if re.match('^hey|hi|hello', x, re.IGNORECASE) else False)\ntraincomments['begins_I'] = traincomments['comment_text'].apply(lambda x: True if re.match('^I ', x, re.IGNORECASE) else False)\ntraincomments['begins_You'] = traincomments['comment_text'].apply(lambda x: True if re.match('^You', x, re.IGNORECASE) else False)\ntraincomments['begins_capital'] = traincomments['comment_text'].apply(lambda x: True if re.match('^[A-Z].*', x) else False)\ntraincomments['contains_parenthesis'] = traincomments['comment_text'].apply(lambda x: True if re.search('\\(|\\)', x) else False)\ntraincomments['contains_curse'] = traincomments['comment_text'].apply(lambda x: True if re.search('fuck|shit |ass| looser | dick|bitch|suck|Nazi|stupid|bullshit| piss | cock|jew| die | cunt| rape | gay | nigger | pussy|wank|faggot|dumb', x,re.IGNORECASE) else False)\ntraincomments['contains_curse_replacement'] = traincomments['comment_text'].apply(lambda x: True if re.search('A$$HOLE|NIGGA|FVCK|cra*p|fxxk|b*tch', x,re.IGNORECASE) else False)\ntraincomments['contains_negative_terms'] = traincomments['comment_text'].apply(lambda x: True if re.search(' bad | terrible | kill | block | stop | wrong | destroy | absurd | don\\'t | mom | mother | fat | cut | burn | ugly | kick | liar ', x,re.IGNORECASE) else False)\ntraincomments['contains_abbreviations'] = traincomments['comment_text'].apply(lambda x: True if re.search(' OMG | LMAO |WTF|FFS', x,re.IGNORECASE) else False)\n\ntraincomments.head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Data Exploration:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"traincomments.shape\ntraincomments.nunique()\ntraincomments.info()\ntraincomments['toxic'].value_counts().head(10) #Count entries per value:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Preliminary EDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sums=traincomments[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].sum(axis=0)\nsums_df=pd.DataFrame(data=sums, columns=['number'])\n\n\nfig = plt.figure()\nfig.set_size_inches(12,10)\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\nax3 = fig.add_subplot(223)\nax4 = fig.add_subplot(224)\n\nax1.set_title(\"Comment # characters distribution\", x=0.4, y=0.95, ha='center', fontsize='xx-large');\nax2.set_title(\"Category breakdown\", x=0.4, y=0.95, ha='center', fontsize='xx-large');\nax3.set_title(\"Comment # words distribution\", x=0.4, y=0.95, ha='center', fontsize='xx-large');\nax4.set_title(\"Number of categories per comment\", x=0.4, y=0.95, ha='center', fontsize='xx-large');\n\nsns.distplot(traincomments.comment_characters, bins=100, kde=True, color='blue', ax=ax1);\nax1.set_xlim(-200,1000)\nsums_df.sort_values(by='number',ascending=False).plot.bar(ax=ax2);\nsns.distplot(traincomments.comment_words, bins=100, kde=True, color='blue', ax=ax3);\nax3.set_xlim(-25,300)\ntraincomments[traincomments['number_categories']>0]['number_categories'].value_counts().sort_index(ascending=True).plot.bar(ax=ax4);\n\n\nplt.show();\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_df = traincomments[traincomments['number_categories']>0]\ncorr=corr_df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True, mask=mask, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traincomments_toxic=traincomments[traincomments['toxic']==1]\ntraincomments_severe_toxic=traincomments[traincomments['severe_toxic']==1]\ntraincomments_obscene=traincomments[traincomments['obscene']==1]\ntraincomments_threat=traincomments[traincomments['threat']==1]\ntraincomments_insult=traincomments[traincomments['insult']==1]\ntraincomments_identity_hate=traincomments[traincomments['identity_hate']==1]\n\ntext = traincomments_identity_hate['comment_text'].values\nwordcloud = WordCloud(width = 3000,height = 2000,background_color = 'black',stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(figsize = (8, 8), facecolor = 'k',edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = traincomments['comment_text'].values\n\nwordcloud = WordCloud(width = 3000,height = 2000,background_color = 'black',stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(figsize = (8, 8), facecolor = 'k',edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Word2Vector - Gensim**"},{"metadata":{"trusted":true},"cell_type":"code","source":"words_with_numbers_pattern = re.compile(r'\\w*[\\d]+\\w*') # letter\\s (0 or more), digit (1 or more), letter\\s (0 or more)\nspecial_chars_pattern = re.compile(r'[,.\"!?:;=&*\\\\/()\\'$^#]+')\ndef clean(s):\n    s = re.sub(words_with_numbers_pattern, '', s).strip()  # get rid of numbers or words with numbers \n    s = re.sub(special_chars_pattern, '', s)  # remove special chars without removing words\n    # ''.join([char.strip(',.\"!?:;=&*\\\\/()\\'$^#') for char in s])  # remove these characters - this is so slow-- change to re\n    return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_list = traincomments[:159571]['comment_text'].values # gives a list of all comments\ncomment_list=[c.lower() for c in comment_list] # need to do with \"map\" before\nsentences_list = [re.split('[.\\n!?]',line) for line in comment_list] # gives a list of list (each list is a list of sentences corresponding to 1 comment)\nsentences = [s for sentence in sentences_list for s in sentence] # gives a list of all sentences; we loose the comment context\n# print(len(sentences))  # after this split we have ~1.4M sentences\nsentences = [clean(s) for s in sentences if len(s) > 1]\n# print(len(sentences))  # now we have ~954K\nsentences = [s.split() for s in sentences] # list of list; words within sentences\nsentences = [words_list for words_list in sentences if len(words_list)>1] # remove lists with 1 word or less\n# print(len(sentences))  # now we have ~771K\n\n# maybe we want to change \\n to space before\n# we can use Pandas functions (map) to do the splits\n# remove empty words \n# remove words with no letters\n# clean special charachters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec(sentences, size=100, window=5, min_count=3, workers=4) # train the model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sims=model.wv.most_similar('kill')\nsims_df=pd.DataFrame(data=sims, columns=['word','score'])\nsims_df.sort_values(by='score',ascending=False).plot.bar('word','score',ylim=(0.62,0.82));\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(model.wv.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordered_vocab = [(term, voc.index, voc.count) for term, voc in model.wv.vocab.items()] # build tokens and counts list\nordered_vocab = sorted(ordered_vocab, key=lambda k: -k[2]) # sort by tokens counts desc\nordered_terms, term_indices, term_counts = zip(*ordered_vocab) # unzip the tokens, indices and counts into separate lists\nword_vectors = pd.DataFrame(model.wv.syn0norm[term_indices, :], index=ordered_terms) # create a DataFrame\nprint(word_vectors[:15].index.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **TSNE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"how_many_to_use = 700\nX = word_vectors[:how_many_to_use].to_numpy() # will use top most frequent\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)\ndf = pd.DataFrame(X_tsne, index=word_vectors[:how_many_to_use].index.tolist(), columns=['x', 'y'])\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(1, 1, 1)\nax.scatter(df['x'], df['y'])\nfor word, pos in df.iterrows():\n    ax.annotate(word, pos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# code to add vector per comment to training df\n# need to clean comment_list before this\n\nwvc=word_vectors.columns.tolist()\ncols=traincomments.columns.tolist()\ncols.extend(wvc)\n\ntraincomments=traincomments.reindex(columns=cols)\ntraincomments.head()\n# print(traincomments.columns.tolist())\n\nfor i, comment in enumerate(comment_list[0:100]):\n    org_words = clean(comment).split()\n    words = [w for w in org_words if w in word_vectors.index]  #remove words not in model\n    comment_vector=word_vectors.loc[words].mean(axis=0)   # average the word vectors for the comment (cum then normalize for word nmber)\n    traincomments.loc[i, wvc]=comment_vector  # add the features to the df\n\ntraincomments.head(23)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_related_terms(token, topn=10):\n    \"\"\"\n    look up the top most similar terms to token\n    and print them as a formatted list\n    \"\"\"\n\n    for word, similarity in model.most_similar(positive=[token], topn=topn):\n        print (word, round(similarity, 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Word2Vector - TensorFlow**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_raw = 'He is the king . The king is royal . She is the royal  queen '\ncorpus_raw = corpus_raw.lower() # convert to lower case","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **TFIDF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = TfidfVectorizer()\n\nX_train = v.fit_transform(train['comment_text'])\n# X_test = v.transform(test['comment_text'])\n\nfor label in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    y = train[label]\n    model = LogisticRegression()\n    model.fit(X_train, y)\n    test[label] = model.predict_proba(X_test)[:, 1]\n    \ntest.drop('comment_text', axis=1, inplace=True)\ntest.to_csv('simplest.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Bag of Words**"},{"metadata":{},"cell_type":"markdown","source":"### **TFIDF**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}