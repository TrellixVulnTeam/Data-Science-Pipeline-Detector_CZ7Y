{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nfrom keras.callbacks import Callback\nfrom keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# all global parameters\nconf = {\n    'EMBEDDING_FILE' : '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt',\n    'train_file': '/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv',\n    'test_file': '/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv',\n    'test_label': '/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv',\n    \n    'max_features':10000,\n    'max_len': 150,\n    'embed_size' : 300,\n\n    'batch_size': 128,\n    'epochs': 4,\n    'train_size': 0.9\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(conf['train_file'])\ntest = pd.read_csv(conf['test_file'])\n\ntrain.comment_text.fillna(' ')\ntest.comment_text.fillna(' ')\n\ntags = list(train.columns)[2:]\n\nX_train = train.comment_text.str.lower()\ny_train = train[tags].values\n\nX_test = test.comment_text.str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n  Abstract base class used to build new callbacks.\n  Attributes:\n      params: dict. Training parameters\n          (eg. verbosity, batch size, number of epochs...).\n      model: instance of `keras.models.Model`.\n          Reference of the model being trained.\n  The `logs` dictionary that callback methods\n  take as argument will contain keys for quantities relevant to\n  the current batch or epoch.\n  \nIn Keras, Callback is a python class meant to be subclassed to provide specific functionality, \nwith a set of methods called at various stages of training (including batch/epoch start and ends), \ntesting, and predicting. \nCallbacks are useful to get a view on internal states and statistics of the model during training.\n\"\"\"\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data = (), internal = 1):\n        super(Callback, self).__init__()\n        # ?\n        self.internal = internal\n        self.X_val, self.y_val = validation_data\n        \n    def on_epoch_end(self, epoch, logs={}):\n        \"\"\"\n        Called at the end of an epoch.\n        Arguments:\n            epoch: integer, index of epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        if epoch % self.internal == 0:\n            # verbose: Integer. 0, 1, or 2. Verbosity mode.\n            # 0 = silent, 1 = progress bar, 2 = one line per epoch.\n#             Returns:\n#         A `History` object. Its `History.history` attribute is\n#         a record of training loss values and metrics values\n#         at successive epochs, as well as validation loss values\n#         and validation metrics values (if applicable).\n            y_pred = self.model.predict(self.X_val, verbose = 0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC_AUC - epoch: {:d} - scoe: {:.6f}\".format(epoch+1, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# num_words: the maximum number of words to keep, based on word frequency. \n# Only the most common num_words-1 words will be kept.\n\ntok = text.Tokenizer(num_words = conf['max_features'])\n# with default filter\n# These sequences are then split into lists of tokens. They will then be indexed or vectorized.\n\ntok.fit_on_texts(list(X_train) + list(X_test))\n# Updates internal vocabulary based on a list of texts.\n# Required before using texts_to_sequences or texts_to_matrix.\n\nX_train = tok.texts_to_sequences(X_train)\nX_test = tok.texts_to_sequences(X_test)\n# Transforms each text in texts to a sequence of integers.\n# Only top num_words-1 most frequent words will be taken into account. \n# Only words known by the tokenizer will be taken into account.\n\nx_train = sequence.pad_sequences(X_train, maxlen = conf['max_len'])\nx_test = sequence.pad_sequences(X_test, maxlen = conf['max_len'])\n# Pads sequences to the same length.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split a sentence into a list of words.\n# keras.preprocessing.text.text_to_word_sequence(text, \n#    filters=base_filter(), lower=True, split=\" \")\nx_train[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # {word : embedding}\nembedding_index = {}\nwith open(conf['EMBEDDING_FILE'], encoding = 'utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        # rsplit: except for spliting from right, same as split\n        \n        word = values[0]\n        vec = np.asarray(values[1:], dtype = 'float32')\n        embedding_index [word] = vec\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dictionary: {words : rank/index}. Only set after fit_on_texts was called.\nword_index = tok.word_index\n\n#  initializeing embedding matrix\nnum_words = min(conf['max_features'], len(word_index) + 1)\nembedding_matrix = np.zeros((num_words,conf['embed_size']))\n\n# what we want is index: embedding\nfor word, i in word_index.items():\n    if i >= conf['max_features']:\n        continue\n    embedding_vec = embedding_index.get(word)\n    if embedding_vec is not None:\n        embedding_matrix[i] = embedding_vec\n        \n# add embedding matrix into para dict\nconf['embedding_matrix'] = embedding_matrix\n\ndel embedding_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build model\n\ninp = Input(shape = (conf['max_len'],))\n# this returns a tensor\n# a layer instance is callable on a tensor, and returns a tensor\n\nx = Embedding(num_words, conf['embed_size'], weights = [conf['embedding_matrix']], trainable = False)(inp)\n# Turns positive integers (indexes) into dense vectors of fixed size.\n# This embedding layer will encode the input sequence into a sequence of dense embed_size vectors\n\nx = SpatialDropout1D(0.2)(x)\n# spatial drop out, drop entire 1d feature maps instead of individual elements.\n# promote independence between feature maps\n\nx = Bidirectional(GRU(128, return_sequences = True, dropout = 0.1 ))(x)\n# bidirectional wrapper of rnn layer\n# gated recurrent unit\n\nx = Conv1D(64, kernel_size = 3, padding = 'valid', kernel_initializer = 'glorot_uniform')(x)\n# 1d convolution layer \n\navg_pool  = GlobalAveragePooling1D()(x)\n# global averaging pooling\nmax_pool = GlobalMaxPooling1D()(x)\n# global max pooling\nx = concatenate([avg_pool, max_pool]) \n\npreds = Dense(6, activation=\"sigmoid\")(x) # six classes\n# Dense implements the operation: output = activation(dot(input, kernel) + bias)\n\nmodel = Model(inp, preds)\n# Model groups layers into an object with training and inference features.\n\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n# loss: what else?\n    \n# Optimizer that implements the Adam algorithm, set the learing rate\n# metrics: what else?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use a validation set to train the model\n# should add a cross validation instead of just one model\nconf['train_size'] = 0.9\nconf['random_state'] = 0\nX_tr, X_val, y_tr, y_val = train_test_split(x_train, y_train, train_size = conf['train_size'], random_state = conf['random_state'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath=\"weights_base.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n# Save the best model after every epoch.\n\nearly = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n# Stop training when a monitored quantity has stopped improving.\n\nra_val = RocAucEvaluation(validation_data=(X_val, y_val), internal = 1)\n# use the validation set(from training set) to check the model\n\ncallbacks_list = [ra_val,checkpoint, early]\n# print out when 1 epoch ends","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_tr, y_tr, batch_size = conf['batch_size'], epochs = conf['epochs'], \n          validation_data = (X_val, y_val), callbacks = callbacks_list, verbose = 1)\nmodel.load_weights(filepath)\nprint('predicting...')\ny_pred = model.predict(x_test, batch_size = 1024, verbose = 1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}