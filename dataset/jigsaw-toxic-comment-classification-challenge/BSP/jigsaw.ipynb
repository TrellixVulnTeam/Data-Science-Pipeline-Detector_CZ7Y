{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# A dependency of the preprocessing for BERT inputs\n!pip install -q tensorflow-text\n\n#!pip install -q tf-models-official","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n#import tensorflow_text as text\n#from official.nlp import optimization  # to create AdamW optmizer\n\nimport matplotlib.pyplot as plt\n\ntf.get_logger().setLevel('ERROR')\n\nimport nltk\nfrom nltk.tokenize import  word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.util import ngrams\nimport re\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\nbatch_size = 32\nseed = 42\n\n\nPATH='../input/jigsaw-toxic-comment-classification-challenge'\n\n\nraw_train_ds = pd.read_csv(PATH+'/train.csv.zip').sample(frac=1)\nraw_test=pd.read_csv(PATH+'/test.csv.zip')\nraw_train_ds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for line in raw_train_ds['comment_text'][0:10]:\n    print(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopWords = stopwords.words('english')\nraw_train_ds['comment_token'] =raw_train_ds['comment_text'].map(lambda x: word_tokenize(x)) \nraw_train_ds['comment_token_stop']=raw_train_ds['comment_token'].apply(lambda x: [item for item in x if item not in stopWords])\n\nlemmatizer = WordNetLemmatizer() \nraw_train_ds['comment_token_lemm']=raw_train_ds['comment_token_stop'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_test['comment_token'] =raw_test['comment_text'].map(lambda x: word_tokenize(x)) \nraw_test['comment_token_stop']=raw_test['comment_token'].apply(lambda x: [item for item in x if item not in stopWords])\n\n\nraw_test['comment_token_lemm']=raw_test['comment_token_stop'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the clean_text on train set\n\nraw_train_ds['comment_text_reg'] = raw_train_ds['comment_token_lemm'].apply(lambda x: [clean_text(item) for item in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_test['comment_text_reg'] = raw_test['comment_token_lemm'].apply(lambda x: [clean_text(item) for item in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x=raw_train_ds['comment_text_reg'][:round(len(raw_train_ds['comment_text_reg'])*.77)].reset_index()\ntest_x=raw_train_ds['comment_text_reg'][round(len(raw_train_ds['comment_text_reg'])*.77):].reset_index()\n\ntrain_y=raw_train_ds[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].iloc[:round(len(raw_train_ds['comment_text_reg'])*.77),:].reset_index()\ntest_y=raw_train_ds[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].iloc[round(len(raw_train_ds['comment_text_reg'])*.77):,:].reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input=tf.data.Dataset.from_tensor_slices(train_x['comment_text_reg'].apply(lambda x:\" \".join(x) ))\ntrain_target=tf.data.Dataset.from_tensor_slices(train_y[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].apply(lambda x: x ))\n\ntest_input=tf.data.Dataset.from_tensor_slices(test_x['comment_text_reg'].apply(lambda x:\" \".join(x) ))\ntest_target=tf.data.Dataset.from_tensor_slices(test_y[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].apply(lambda x: x ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_test=raw_test['comment_text_reg'].apply(lambda x:\" \".join(x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE = 10000\nBATCH_SIZE = 64\ntrain=tf.data.Dataset.zip((train_input, train_target)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest=tf.data.Dataset.zip((test_input, test_target)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for example, label in train.take(1):\n  print('texts: ', example.numpy()[:3])\n  print()\n  print('labels: ', label.numpy()[:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE=1000\n\nencoder =tf.keras.layers.experimental.preprocessing.TextVectorization(\n    max_tokens=VOCAB_SIZE, \n    ngrams=2,\n    )\nencoder.adapt(train.map(lambda text, label: text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nvocab = np.array(encoder.get_vocabulary())\nvocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_example = encoder(example)[:3].numpy()\nencoded_example","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n in range(3):\n  print(\"Original: \", example[n].numpy())\n  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n  print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=64,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(6)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print([layer.supports_masking for layer in model.layers])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on a sample text without padding.\n\nsample_text = ('You fucking arrogent')\npredictions = model.predict(np.array([sample_text]))\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on a sample text with padding\n\npadding = \"the \" * 2000\npredictions = model.predict(np.array([sample_text, padding]))\nprint(predictions[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train, epochs=10,\n                    validation_data=test, \n                    validation_steps=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampl=pd.read_csv(PATH+'/sample_submission.csv.zip')\nsampl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampl.columns[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_test[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=tf.nn.sigmoid(model.predict(real_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampl[sampl.columns[1:]]=tf.squeeze(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampl.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampl.to_csv('./sample_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}