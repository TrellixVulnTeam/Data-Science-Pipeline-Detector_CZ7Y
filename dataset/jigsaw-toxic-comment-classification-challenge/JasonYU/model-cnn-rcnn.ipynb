{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport time\nimport re\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom subprocess import check_output\nfrom sklearn.model_selection import StratifiedKFold,KFold\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.autograd import Variable\nimport torch.utils.data\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"maxlen = 100\nmax_features = 100000\nembed_size = 300\nembedding_path = \"../input/glove840b300dtxt/glove.840B.300d.txt\"\n#embedding_path = \"../input/glove6b100dtxt/glove.6B.100d.txt\"\n\npatience = 5\nn_epochs = 50\nbatch_size = 512\nseed=1029","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\nsubm = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n\ntrain_test_boundary = len(train['id'])\ndf = pd.concat([train.drop(['id','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],axis=1),test.drop('id',axis=1)])\n\nlabel_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain['none'] = 1-train[label_cols].max(axis=1)\ntrain['multi'] = train[label_cols].sum(axis=1)\n\ndf['lowered_comment'] = df['comment_text'].apply(lambda x: x.lower())\n\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ndef clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\ndf['treated_question'] = df['lowered_comment'].apply(lambda x: clean_contractions(x, contraction_mapping))\n\ndef clean_text(x):\n    x = str(x)\n    for punct in \"/-'—\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    x = re.sub('[0-9]{5,}', ' ##### ', x)\n    x = re.sub('[0-9]{4}', ' #### ', x)\n    x = re.sub('[0-9]{3}', ' ### ', x)\n    x = re.sub('[0-9]{2}', ' ## ', x)\n    return x\ndf['treated_question'] = df['lowered_comment'].apply(lambda x: clean_text(x))\n\nmispell_dict = {\"youfuck\":\"you fuck\",\"niggors\":\"niggers\",\"néger\":'niger',\"fucksex\":\"fuck sex\",\"yourselfgo\":\"yourself go\",\"bitchbot\":\"bitch bot\",\"donkeysex\":\"donkey sex\",\"mothjer\":\"mother\",\"niggerjew\":\"nigger jew\",\"gayyour\":\"gay your\",\"motherfuckerdie\":\"motherfucker die\",\"radicalnigger\":\"radical nigger\",\"philippineslong\":\"philippines long\",'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\ndf['treated_comment'] = df['treated_question'].apply(lambda x: correct_spelling(x, mispell_dict))\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nlist_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ny_train = train[list_classes].values\nlist_sentences_train = df[\"treated_comment\"].iloc[:train_test_boundary]\nlist_sentences_test = df[\"treated_comment\"].iloc[train_test_boundary:]\ndel df\n\n#max_features = 100000\n## Whole vocab 200000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n\nX_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen)\n\n#######################################\n### How to tackle with the unknown word\n\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore'))\nall_embs = np.stack(embedding_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\n#emb_mean,emb_std = -0.005838499, 0.48782197\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Early Stopping Class"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport torch\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### CNN text model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class CNN_Text(nn.Module):\n    def __init__(self, args):\n        super(CNN_Text, self).__init__()\n        self.args = args\n        \n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1 # input_channel\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n\n        if args.max_norm is not None:\n            print(\"max_norm = {} \".format(args.max_norm))\n            self.embed = nn.Embedding(V, D, max_norm=5, scale_grad_by_freq=True)\n        else:\n            print(\"max_norm = {} \".format(args.max_norm))\n            self.embed = nn.Embedding(V, D, scale_grad_by_freq=True)\n        \n        if args.pre_word_Embedding:\n            self.embed.weight.data.copy_(torch.tensor(embedding_matrix))\n            # fixed the word embedding\n            self.embed.weight.requires_grad = True\n        print(\"dddd {} \".format(self.embed.weight.data.size()))\n\n        if args.wide_conv is True:\n            print(\"using wide convolution\")\n            self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), stride=(1, 1),\n                                     padding=(K//2, 0), dilation=1, bias=False) for K in Ks]\n        else:\n            print(\"using narrow convolution\")\n            self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), bias=True) for K in Ks]\n        print(self.convs1)\n\n        if args.init_weight:\n            print(\"Initing W .......\")\n            for conv in self.convs1:\n                torch.nn.init.xavier_normal_(conv.weight.data, gain=np.sqrt(args.init_weight_value))\n                fan_in, fan_out = CNN_Text.calculate_fan_in_and_fan_out(conv.weight.data)\n                print(\" in {} out {} \".format(fan_in, fan_out))\n                std = np.sqrt(args.init_weight_value) * np.sqrt(2.0 / (fan_in + fan_out))\n        # for cnn cuda\n        if args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n\n        self.dropout = nn.Dropout(args.dropout)\n        self.dropout_embed = nn.Dropout(args.dropout_embed)\n        \n        in_fea = len(Ks) * Co\n        self.fc = nn.Linear(in_features=in_fea, out_features=C, bias=True)\n        # whether to use batch normalizations\n        if args.batch_normalizations is True:\n            print(\"using batch_normalizations in the model......\")\n            self.fc1 = nn.Linear(in_features=in_fea, out_features=in_fea//2, bias=True)\n            self.fc2 = nn.Linear(in_features=in_fea//2, out_features=C, bias=True)\n            self.convs1_bn = nn.BatchNorm2d(num_features=Co, momentum=args.bath_norm_momentum,\n                                            affine=args.batch_norm_affine)\n            self.fc1_bn = nn.BatchNorm1d(num_features=in_fea//2, momentum=args.bath_norm_momentum,\n                                         affine=args.batch_norm_affine)\n            self.fc2_bn = nn.BatchNorm1d(num_features=C, momentum=args.bath_norm_momentum,\n                                         affine=args.batch_norm_affine)\n\n    def calculate_fan_in_and_fan_out(tensor):\n        dimensions = tensor.ndimension()\n        if dimensions < 2:\n            raise ValueError(\"Fan in and fan out can not be computed for tensor with less than 2 dimensions\")\n\n        if dimensions == 2:  # Linear\n            fan_in = tensor.size(1)\n            fan_out = tensor.size(0)\n        else:\n            num_input_fmaps = tensor.size(1)\n            num_output_fmaps = tensor.size(0)\n            receptive_field_size = 1\n            if tensor.dim() > 2:\n                receptive_field_size = tensor[0][0].numel()\n            fan_in = num_input_fmaps * receptive_field_size\n            fan_out = num_output_fmaps * receptive_field_size\n        return fan_in, fan_out\n\n    def forward(self, x):\n        x = self.embed(x)  # (N,W,D)\n        x = self.dropout_embed(x)\n        x = x.unsqueeze(1)  # (N,Ci,W,D)\n        if self.args.batch_normalizations is True:\n            x = [self.convs1_bn(torch.tanh(conv(x))).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n            x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n        else:\n            x = [F.elu(conv(x)).squeeze(3) for conv in self.convs1]  #[(N,Co,W), ...]*len(Ks)\n            x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n        x = torch.cat(x, 1)\n        x = self.dropout(x)  # (N,len(Ks)*Co)\n        if self.args.batch_normalizations is True:\n            x = self.fc1_bn(self.fc1(x))\n            logit = self.fc2_bn(self.fc2(torch.tanh(x)))\n        else:\n            logit = self.fc(x)\n        return logit\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RCNN Text model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class RCNN_Text(nn.Module):\n    def __init__(self, args):\n        super(RCNN_Text, self).__init__()\n        self.args = args\n        \n        self.output_size = args.output_size\n        self.hidden_size = args.hidden_size\n        self.vocab_size = args.vocab_size\n        self.embedding_size = args.embedding_size\n        \n        if args.max_norm is not None:\n            print(\"max_norm = {} \".format(args.max_norm))\n            self.embed = nn.Embedding(self.vocab_size, self.embedding_size, max_norm=5, scale_grad_by_freq=True)\n        else:\n            print(\"max_norm = {} \".format(args.max_norm))\n            self.embed = nn.Embedding(self.vocab_size, self.embedding_size, scale_grad_by_freq=True)\n        \n        if args.pre_word_Embedding:\n            self.embed.weight.data.copy_(torch.tensor(embedding_matrix))\n            # fixed the word embedding\n            self.embed.weight.requires_grad = True\n        print(\"dddd {} \".format(self.embed.weight.data.size()))\n        \n        self.dropout = nn.Dropout(args.dropout)\n        self.dropout_embed = nn.Dropout(args.dropout_embed)\n        \n        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, bidirectional=True)\n        self.W2 = nn.Linear(2 * self.hidden_size + self.embedding_size, self.hidden_size)\n        self.label = nn.Linear(self.hidden_size, self.output_size)\n        \n    def forward(self,x):\n            \n        x = self.embed(x)  # (N,W,D)\n        x = self.dropout_embed(x)\n        batch_size = x.size(0)\n           \n        x = x.permute(1, 0, 2) # x.size() = (num_sequences, batch_size, embedding_size)\n        h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n        c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n\n        output, (final_hidden_state, final_cell_state) = self.lstm(x, (h_0, c_0))\n        final_encoding = torch.cat((output, x), 2).permute(1, 0, 2)\n        y = self.W2(final_encoding) # y.size() = (batch_size, num_sequences, hidden_size)\n        y = y.permute(0, 2, 1) # y.size() = (batch_size, hidden_size, num_sequences)\n        y = F.max_pool1d(y, y.size()[2]) # y.size() = (batch_size, hidden_size, 1)\n        y = y.squeeze(2)\n        logits = self.label(y)\n        return logits\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RNN self attention model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"### Ref:https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/selfAttention.py\nclass RNN_selfatten(nn.Module):\n    def __init__(self,args):\n        super(RNN_selfatten, self).__init__()\n        \n        self.args = args\n        self.output_size = args.output_size\n        self.hidden_size = args.hidden_size\n        self.vocab_size = args.vocab_size\n        self.embedding_size = args.embedding_size\n        \n        # We will use da = 350, r = 30 & penalization_coeff = 1\n        # as per given in the self-attention original ICLR paper\n        da = 350\n        r = 30\n        \n        self.W_s1 = nn.Linear(2*self.hidden_size, da)\n        self.W_s2 = nn.Linear(da, r)\n        self.fc_layer = nn.Linear(r*2*self.hidden_size, 2000)\n        self.label = nn.Linear(2000, self.output_size)\n        \n        if args.max_norm is not None:\n            print(\"max_norm = {} \".format(args.max_norm))\n            self.embed = nn.Embedding(self.vocab_size, self.embedding_size, max_norm=5, scale_grad_by_freq=True)\n        else:\n            print(\"max_norm = {} \".format(args.max_norm))\n            self.embed = nn.Embedding(self.vocab_size, self.embedding_size, scale_grad_by_freq=True)\n        \n        if args.pre_word_Embedding:\n            self.embed.weight.data.copy_(torch.tensor(embedding_matrix))\n            # fixed the word embedding\n            self.embed.weight.requires_grad = True\n        print(\"dddd {} \".format(self.embed.weight.data.size()))\n        \n        self.dropout = nn.Dropout(args.dropout)\n        self.dropout_embed = nn.Dropout(args.dropout_embed)\n        self.bilstm = nn.LSTM(self.embedding_size, self.hidden_size, bidirectional=True)\n        \n\n    def attention_net(self, lstm_output):\n        attn_weight_matrix = self.W_s2(torch.tanh(self.W_s1(lstm_output)))\n        attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n        attn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n        return attn_weight_matrix\n    \n    def forward(self,x):\n        x = self.embed(x)  # (N,W,D)\n        x = self.dropout_embed(x)\n        batch_size = x.size(0)\n        \n        x = x.permute(1, 0, 2) # x.size() = (num_sequences, batch_size, embedding_size)\n        h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n        c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n\n        output, (h_n, c_n) = self.bilstm(x, (h_0, c_0))\n        output = output.permute(1, 0, 2)  \n        # output.size() = (batch_size, num_seq, 2*hidden_size)\n        # h_n.size() = (1, batch_size, hidden_size)\n        # c_n.size() = (1, batch_size, hidden_size)\n        attn_weight_matrix = self.attention_net(output)\n        # attn_weight_matrix.size() = (batch_size, r, num_seq)\n        # output.size() = (batch_size, num_seq, 2*hidden_size)\n        hidden_matrix = torch.bmm(attn_weight_matrix, output)\n        # hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n        # Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n        fc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n        logits = self.label(fc_out)\n        # logits.size() = (batch_size, output_size)\n\n        return logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RNN Attention model (soft attention)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"### soft attention\nclass RNN_atten(nn.Module):\n    def __init__(self,args):\n        super(RNN_atten, self).__init__()\n        \n        self.args = args\n        self.output_size = args.output_size\n        self.hidden_size = args.hidden_size\n        self.vocab_size = args.vocab_size\n        self.embedding_size = args.embedding_size\n        \n        if args.max_norm is not None:\n            print(\"max_norm = {} \".format(args.max_norm))\n            self.embed = nn.Embedding(self.vocab_size, self.embedding_size, max_norm=5, scale_grad_by_freq=True)\n        else:\n            print(\"max_norm = {} \".format(args.max_norm))\n            self.embed = nn.Embedding(self.vocab_size, self.embedding_size, scale_grad_by_freq=True)\n        \n        if args.pre_word_Embedding:\n            self.embed.weight.data.copy_(torch.tensor(embedding_matrix))\n            # fixed the word embedding\n            self.embed.weight.requires_grad = True\n        print(\"dddd {} \".format(self.embed.weight.data.size()))\n        \n        self.dropout = nn.Dropout(args.dropout)\n        self.dropout_embed = nn.Dropout(args.dropout_embed)\n        \n        self.label = nn.Linear(self.hidden_size, self.output_size)\n        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, bidirectional=False)\n\n    def attention_net(self, lstm_output, final_state):\n        \"\"\"  \n        Tensor Size :\n                    final_state.size() = (1,batch_size, hidden_size)\n                    hidden.size() = (batch_size, hidden_size)\n                    attn_weights.size() = (batch_size, num_seq)\n                    soft_attn_weights.size() = (batch_size, num_seq)\n                    new_hidden_state.size() = (batch_size, hidden_size)\n        \"\"\"\n        hidden = final_state.squeeze(0)\n        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n        soft_attn_weights = F.softmax(attn_weights, 1)\n        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n        return new_hidden_state\n    \n    def forward(self,x):\n        x = self.embed(x)  # (N,W,D)\n        x = self.dropout_embed(x)\n        batch_size = x.size(0)\n        \n        x = x.permute(1, 0, 2) # x.size() = (num_sequences, batch_size, embedding_size)\n        h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n        c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n        \n        output, (final_hidden_state, final_cell_state) = self.lstm(x, (h_0, c_0)) # final_hidden_state.size() = (1, batch_size, hidden_size) \n        output = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n        \n        attn_output = self.attention_net(output, final_hidden_state)\n        logits = self.label(attn_output)\n        \n        return logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Char - CNN model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Ref:https://github.com/dreamgonfly/deep-text-classification-pytorch/blob/master/dictionaries.py\nclass CharCNNDictionary:\n    def __init__(self):\n        self.ALPHABET = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\" + '\\n'\n        self.PAD_TOKEN = '<PAD>'\n        \n    def build_dictionary(self):\n        self.vocab_chars = list(self.ALPHABET) + [self.PAD_TOKEN]\n        self.char2idx = {char:idx for idx, char in enumerate(self.vocab_chars)}\n        self.vocabulary_size = len(self.vocab_chars)\n        self._build_weight()\n    \n    # One under slash \"_\" means half private which means using that inside the class \n    def _build_weight(self):\n        # one hot embedding plus all-zero vector\n        onehot_matrix = np.eye(self.vocabulary_size, self.vocabulary_size - 1)\n        self.embedding = onehot_matrix\n\n    def indexer(self, char):\n        try:\n            return self.char2idx[char]\n        except:\n            char = self.PAD_TOKEN\n            return self.char2idx[char]\n\ndef pad_text(text, pad, min_length=None, max_length=None):\n    length = len(text)\n    if min_length is not None and length < min_length:\n        return text + [pad]*(min_length - length)\n    if max_length is not None and length > max_length:\n        return text[:max_length]\n    return text\n\nclass TextDataset(Dataset):\n    \n    def __init__(self, texts, dictionary, sort=False, min_length=None, max_length=None):\n\n        PAD_IDX = dictionary.indexer(dictionary.PAD_TOKEN)\n        \n        self.texts = [[dictionary.indexer(token) for token in text]\n                          for text in texts]\n\n        if min_length or max_length:\n            self.texts = [pad_text(text, PAD_IDX, min_length, max_length) \n                          for text in self.texts]\n\n        if sort:\n            self.texts = sorted(self.texts, key=lambda x: len(x[0]))\n    \n    def get_all(self):\n        return self.texts\n        \n    def __getitem__(self, index):\n        tokens = self.texts[index]\n        return tokens\n        \n    def __len__(self):\n        return len(self.texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"### char CNN model\nclass char_CNN(nn.Module):\n    def __init__(self,args):\n        super(char_CNN, self).__init__()\n        self.args = args\n        \n        vocabulary_size = args.vocabulary_size\n        embed_size = vocabulary_size - 1 # except for padding\n        embedding_weight = args.embedding\n        \n        n_classes = args.output_size\n        max_length = args.max_length\n        \n        if args.mode == 'large':\n            conv_features = 1024\n            linear_features = 2048\n        elif args.mode == 'small':\n            conv_features = 256\n            linear_features = 1024\n        else:\n            raise NotImplementedError()\n        \n        # quantization\n        self.embedding = nn.Embedding(vocabulary_size, embed_size)\n        if embedding_weight is not None:\n            self.embedding.weight = nn.Parameter(torch.FloatTensor(embedding_weight), requires_grad=False)\n        \n        conv1 = nn.Sequential(\n            nn.Conv1d(in_channels=embed_size, out_channels=conv_features, kernel_size=7),\n            nn.MaxPool1d(kernel_size=3),\n            nn.ReLU()\n        )\n        conv2 = nn.Sequential(\n            nn.Conv1d(in_channels=conv_features, out_channels=conv_features, kernel_size=7),\n            nn.MaxPool1d(kernel_size=3),\n            nn.ReLU()\n        )\n        conv3 = nn.Sequential(\n            nn.Conv1d(in_channels=conv_features, out_channels=conv_features, kernel_size=3),\n            nn.ReLU()\n        )\n        conv4 = nn.Sequential(\n            nn.Conv1d(in_channels=conv_features, out_channels=conv_features, kernel_size=3),\n            nn.ReLU()\n        )\n        conv5 = nn.Sequential(\n            nn.Conv1d(in_channels=conv_features, out_channels=conv_features, kernel_size=3),\n            nn.ReLU()\n        )\n        conv6 = nn.Sequential(\n            nn.Conv1d(in_channels=conv_features, out_channels=conv_features, kernel_size=3),\n            nn.MaxPool1d(kernel_size=3),\n            nn.ReLU()\n        )\n         \n        # (max_length - 96) // 27 is the output size of conv6 , before feed it to the fc layer, there is a \"view\" opt.\n        initial_linear_size = (max_length - 96) // 27 * conv_features\n        \n        linear1 = nn.Sequential(\n            nn.Linear(initial_linear_size, linear_features),\n            nn.Dropout(),\n            nn.ReLU()\n        )\n        linear2 = nn.Sequential(\n            nn.Linear(linear_features, linear_features),\n            nn.Dropout(),\n            nn.ReLU()\n        )\n        linear3 = nn.Linear(linear_features, n_classes)\n        \n        self.convolution_layers = nn.Sequential(conv1, conv2, conv3, conv4, conv5, conv6)\n        self.linear_layers = nn.Sequential(linear1, linear2, linear3)\n        \n        \n        self.dropout = nn.Dropout(args.dropout)\n        self.dropout_embed = nn.Dropout(args.dropout_embed)\n        \n    def forward(self, sentences):\n#         print(sentences.shape)\n        x = self.embedding(sentences)\n#         print(x.shape)\n        x = x.transpose(1,2)\n#         print(x.shape)\n        x = self.convolution_layers(x)\n#         print(x.shape)\n        x = x.view(x.size(0), -1)\n#         print(x.shape)\n        x = self.linear_layers(x)\n#         print(x.shape)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"#### C-LSTM model - \"A C-LSTM Neural Network for Text Classification\""},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class C_LSTM(nn.Module):\n    def __init__(self, args):\n        super(C_LSTM, self).__init__()\n        self.args = args\n        \n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1 # input_channel\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n        \n        self.hidden_size = args.hidden_size\n\n        if args.max_norm is not None:\n            print(\"max_norm = {} \".format(args.max_norm))\n            self.embed = nn.Embedding(V, D, max_norm=5, scale_grad_by_freq=True)\n        else:\n            print(\"max_norm = {} \".format(args.max_norm))\n            self.embed = nn.Embedding(V, D, scale_grad_by_freq=True)\n        \n        if args.pre_word_Embedding:\n            self.embed.weight.data.copy_(torch.tensor(embedding_matrix))\n            # fixed the word embedding\n            self.embed.weight.requires_grad = True\n        print(\"dddd {} \".format(self.embed.weight.data.size()))\n\n        if args.wide_conv is True:\n            print(\"using wide convolution\")\n            self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), stride=(1, 1),\n                                     padding=(K//2, 0), dilation=1, bias=False) for K in Ks]\n        else:\n            print(\"using narrow convolution\")\n            self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), bias=True) for K in Ks]\n        print(self.convs1)\n\n        if args.init_weight:\n            print(\"Initing W .......\")\n            for conv in self.convs1:\n                torch.nn.init.xavier_normal_(conv.weight.data, gain=np.sqrt(args.init_weight_value))\n                fan_in, fan_out = C_LSTM.calculate_fan_in_and_fan_out(conv.weight.data)\n                print(\" in {} out {} \".format(fan_in, fan_out))\n                std = np.sqrt(args.init_weight_value) * np.sqrt(2.0 / (fan_in + fan_out))\n        # for cnn cuda\n        \n        if args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n        \n        self.dropout = nn.Dropout(args.dropout)\n        self.dropout_embed = nn.Dropout(args.dropout_embed)\n        \n        self.lstm = nn.LSTM(Co, self.hidden_size, bidirectional=True)\n        \n        in_fea = 2 * self.hidden_size\n        self.fc = nn.Linear(in_features=in_fea, out_features=C, bias=True)\n        # whether to use batch normalizations\n        if args.batch_normalizations:\n            print(\"using batch_normalizations in the model......\")\n            self.fc1 = nn.Linear(in_features=in_fea, out_features=in_fea//2, bias=True)\n            self.fc2 = nn.Linear(in_features=in_fea//2, out_features=C, bias=True)\n            self.convs1_bn = nn.BatchNorm2d(num_features=Co, momentum=args.bath_norm_momentum,\n                                            affine=args.batch_norm_affine)\n            self.fc1_bn = nn.BatchNorm1d(num_features=in_fea//2, momentum=args.bath_norm_momentum,\n                                         affine=args.batch_norm_affine)\n            self.fc2_bn = nn.BatchNorm1d(num_features=C, momentum=args.bath_norm_momentum,\n                                         affine=args.batch_norm_affine)\n\n    def calculate_fan_in_and_fan_out(tensor):\n        dimensions = tensor.ndimension()\n        if dimensions < 2:\n            raise ValueError(\"Fan in and fan out can not be computed for tensor with less than 2 dimensions\")\n\n        if dimensions == 2:  # Linear\n            fan_in = tensor.size(1)\n            fan_out = tensor.size(0)\n        else:\n            num_input_fmaps = tensor.size(1)\n            num_output_fmaps = tensor.size(0)\n            receptive_field_size = 1\n            if tensor.dim() > 2:\n                receptive_field_size = tensor[0][0].numel()\n            fan_in = num_input_fmaps * receptive_field_size\n            fan_out = num_output_fmaps * receptive_field_size\n        return fan_in, fan_out\n\n    def forward(self, x):\n        x = self.embed(x)  # (N,W,D)\n        x = self.dropout_embed(x)\n        x = x.unsqueeze(1)  # (N,Ci,W,D)\n        \n        batch_size = x.size(0)\n        \n        if self.args.batch_normalizations:\n            x = [self.convs1_bn(torch.tanh(conv(x))).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n            x = [i.permute(2, 0,1) for i in x]\n        else:\n            x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n            x = [i.permute(2, 0,1) for i in x] #[(N,Co), ...]*len(Ks)\n            \n        x = x[0]\n        \n        h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n        c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n        output, (final_hidden_state, final_cell_state) = self.lstm(x, (h_0, c_0))\n        \n        x = torch.cat([final_hidden_state[i] for i in range(final_hidden_state.size(0))],1)\n        if self.args.batch_normalizations:\n            x = self.fc1_bn(self.fc1(x))\n            logit = self.fc2_bn(self.fc2(torch.tanh(x)))\n        else:\n            logit = self.fc(x)\n        return logit\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transformer encoder for classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\nimport copy\nimport math\n\ndef gelu(x):\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT_FNS = {\n    'relu': nn.ReLU,\n    'swish': swish,\n    'gelu': gelu\n}\n\nclass LayerNorm(nn.Module):\n    \"Construct a layernorm module in the OpenAI style (epsilon inside the square root).\"\n\n    def __init__(self, n_state, e=1e-5):\n        super(LayerNorm, self).__init__()\n        self.g = nn.Parameter(torch.ones(n_state))\n        self.b = nn.Parameter(torch.zeros(n_state))\n        self.e = e\n\n    # x:(batch*emb)\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)\n        s = (x - u).pow(2).mean(-1, keepdim=True)\n        x = (x - u) / torch.sqrt(s + self.e)\n        return self.g * x + self.b\n\n\nclass Conv1D(nn.Module):\n    def __init__(self, nf, rf, nx):\n        super(Conv1D, self).__init__()\n        self.rf = rf\n        self.nf = nf\n        if rf == 1:  # faster 1x1 conv\n            w = torch.empty(nx, nf)\n            nn.init.normal_(w, std=0.02)\n            self.w = Parameter(w)\n            self.b = Parameter(torch.zeros(nf))\n        else:  # was used to train LM\n            raise NotImplementedError\n\n    def forward(self, x):\n        if self.rf == 1:\n            size_out = x.size()[:-1] + (self.nf,)\n            x = torch.addmm(self.b, x.view(-1, x.size(-1)), self.w)\n            x = x.view(*size_out)\n        else:\n            raise NotImplementedError\n        return x\n\nclass Attention(nn.Module):\n    def __init__(self, nx, n_ctx, cfg, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n        assert n_state % cfg.n_head == 0\n        self.register_buffer('b', torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = cfg.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.c_attn = Conv1D(n_state * 3, 1, nx)\n        self.c_proj = Conv1D(n_state, 1, nx)\n        self.attn_dropout = nn.Dropout(cfg.attn_pdrop)\n        self.resid_dropout = nn.Dropout(cfg.resid_pdrop)\n\n    def _attn(self, q, k, v):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        # w = w * self.b + -1e9 * (1 - self.b)  # TF implem method: mask_attn_weights\n        # XD: self.b may be larger than w, so we need to crop it\n        b = self.b[:, :, :w.size(-2), :w.size(-1)]\n        w = w * b + -1e9 * (1 - b)\n\n        w = nn.Softmax(dim=-1)(w)\n        w = self.attn_dropout(w)\n        return torch.matmul(w, v)\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n        \n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n        if k:\n            return x.permute(0, 2, 3, 1)\n        else:\n            return x.permute(0, 2, 1, 3)\n\n    def forward(self, x):\n        # x : batch * seq * embed\n        x = self.c_attn(x)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n        a = self._attn(query, key, value)\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n        a = self.resid_dropout(a)\n        return a\n\nclass MLP(nn.Module):\n    def __init__(self, n_state, cfg):  # in MLP: n_state=3072 (4 * n_embd)\n        super(MLP, self).__init__()\n        nx = cfg.n_embd\n        self.c_fc = Conv1D(n_state, 1, nx)\n        self.c_proj = Conv1D(nx, 1, n_state)\n        self.act = ACT_FNS[cfg.ACT_FNS]\n        self.dropout = nn.Dropout(cfg.resid_pdrop)\n\n    def forward(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return self.dropout(h2)\n\nclass Block(nn.Module):\n    def __init__(self, n_ctx, cfg, scale=False):\n        super(Block, self).__init__()\n        nx = cfg.n_embd\n        self.attn = Attention(nx, n_ctx, cfg, scale)\n        self.ln_1 = LayerNorm(nx)\n        self.mlp = MLP(4 * nx, cfg)\n        self.ln_2 = LayerNorm(nx)\n\n    def forward(self, x):\n        a = self.attn(x)\n        n = self.ln_1(x + a)\n        m = self.mlp(n)\n        h = self.ln_2(n + m)\n        return h\n\n\nclass PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        \n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                             -(math.log(10000) / d_model))\n        pe[:, 0::2] = torch.sin(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))\n        pe[:, 1::2] = torch.cos(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        x = x + Variable(self.pe[:, :x.size(1)], \n                         requires_grad=False)\n        return x\n\nclass TransformerModel(nn.Module):\n    \"\"\" Transformer model \"\"\"\n    def __init__(self, cfg):\n        super(TransformerModel, self).__init__()\n        self.vocab = cfg.vocab\n        self.n_ctx = cfg.n_ctx\n        self.embed = nn.Embedding(self.vocab+1, cfg.n_embd)\n        self.PE = PositionalEncoding(cfg.n_embd,self.n_ctx)\n        nn.init.normal_(self.embed.weight, std=0.02)\n        if cfg.pre_word_Embedding:\n            self.embed.weight.data.copy_(torch.tensor(cfg.embedding_matrix))\n            # fixed the word embedding\n            self.embed.weight.requires_grad = True\n        \n        self.drop = nn.Dropout(cfg.embd_pdrop)\n        block = Block(self.n_ctx, cfg, scale=True)\n        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(cfg.n_layer)])\n        \n        self.fc = nn.Linear(in_features=cfg.n_embd, out_features=cfg.C, bias=True)\n        \n\n    def forward(self, x):\n        #x = x.view(-1, x.size(-2), x.size(-1))\n        #print ('after:',x.shape)\n        x = self.embed(x)\n        x = self.PE(x)\n        h = self.drop(x)\n        # Add the position information to the input embeddings\n        for block in self.h:\n            h = block(h)\n        logit = self.fc(h[:,-1,:])\n        return logit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = torch.tensor([[0,1,1]], dtype=torch.float32)  # 3 classes, batch size = 2\noutput = torch.tensor([[0.1,0.7,0.8]], dtype=torch.float32)  # A prediction (logit)\n#pos_weight = torch.ones([64])  # All weights are equal to 1\ncriterion = torch.nn.BCEWithLogitsLoss()#pos_weight=pos_weight)\ncriterion(output, target)  # -log(sigmoid(0.999))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m=nn.Sigmoid()\nm(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1+np.exp(-x))\n-np.log((1-sigmoid(0.1))*(sigmoid(0.7))*(sigmoid(0.8)))/3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self,pos_weight,alpha=1, gamma=2,logits=True, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n        \n        self.BCELossLogits = torch.nn.BCEWithLogitsLoss(reduction='none',pos_weight=pos_weight)\n        self.BCELoss = torch.nn.BCELoss(reduction='none' )\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = self.BCELossLogits(inputs, targets)\n        else:\n            BCE_loss = self.BCELoss(inputs, targets)\n    \n        pt = torch.exp(-BCE_loss)\n        F_loss = (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n\n\nclass FocalLoss_new(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n        super(FocalLoss_new, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=None)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=None)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def train_model(model, x_train, y_train, x_val, y_val, validate=True):\n    #optimizer = torch.optim.Adam(model.parameters())\n    optimizer = torch.optim.RMSprop(model.parameters(),lr=0.001)\n    #optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n    #optimizer = torch.optim.Adadelta(model.parameters())\n    # scheduler = CosineAnnealingLR(optimizer, T_max=5)\n    # scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n    \n    train = torch.utils.data.TensorDataset(x_train, y_train)\n    valid = torch.utils.data.TensorDataset(x_val, y_val)\n    \n    train_loss,valid_loss,valid_auc = [],[],[]\n    \n    early_stopping = EarlyStopping(patience=patience, verbose=True)\n    \n    pos_label = y_train.cpu().sum(0)\n    #whole = y_train.size(0)*torch.ones(pos_label.shape)\n    #pos_weight = (whole-pos_label) / pos_label\n    #loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean',pos_weight=pos_weight).cuda()\n    #loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n    \n    pos_weight = 0.25 * torch.ones(pos_label.shape)\n    \n    loss_fn = FocalLoss(pos_weight=pos_weight).cuda()\n    \n    best_score = -np.inf\n    \n    for epoch in range(n_epochs):\n        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n        start_time = time.time()\n        \n        avg_loss = 0.\n        \n        model.train()\n        for x_batch, y_batch in tqdm(train_loader, disable=True):\n            y_pred = model(x_batch)\n            y_batch = y_batch.squeeze(dim=1)\n            loss = loss_fn(y_pred, y_batch)\n            '''\n            lambda_2 = torch.tensor(0.1).cuda()\n            l2_reg = torch.tensor(0.0).cuda()\n            for name, param in model.named_parameters():\n                #print (name)\n                if name == \"fc1.weight\":\n                    l2_reg += torch.norm(param).cuda()\n            loss += lambda_2 * l2_reg\n            '''\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n         \n        model.eval()\n        valid_preds=None\n        valid_labels = None\n        \n        if validate:\n            avg_val_loss = 0.\n            for i, (x_batch, y_batch) in enumerate(valid_loader):\n                y_pred = model(x_batch).detach()\n                y_batch = y_batch.squeeze(dim=1)\n                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n                #valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n                if valid_preds is None:\n                    valid_preds = sigmoid(y_pred.cpu().numpy())\n                    valid_labels = y_batch.cpu().numpy()\n                else:\n                    valid_preds = np.concatenate((valid_preds,sigmoid(y_pred.cpu().numpy())),axis=0)\n                    valid_labels = np.concatenate((valid_labels,y_batch.cpu().numpy()),axis=0)\n                    \n            #search_result = threshold_search(y_val.cpu().numpy(), valid_preds)\n            #val_f1, val_threshold = search_result['f1'], search_result['threshold']\n            \n            avg_auc = roc_auc_score(valid_labels,valid_preds)\n            elapsed_time = time.time() - start_time\n            \n            train_loss.append(avg_loss)\n            valid_loss.append(avg_val_loss)\n            valid_auc.append(avg_auc)\n            \n            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_auc={:.4f} time={:.2f}s'.format(\n                epoch + 1, n_epochs, avg_loss, avg_val_loss, avg_auc, elapsed_time))    \n            \n            early_stopping(avg_val_loss, model)\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n                \n        else:\n            train_loss.append(avg_loss)\n            \n            elapsed_time = time.time() - start_time\n            print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n                epoch + 1, n_epochs, avg_loss, elapsed_time))\n    \n    '''\n    valid_preds = np.zeros((x_val_fold.size(0)))\n    avg_val_loss = 0.\n    for i, (x_batch, y_batch) in enumerate(valid_loader):\n        y_pred = model(x_batch).detach()\n        y_batch = y_batch.squeeze(dim=1)\n        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n        valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n    print('Validation loss: ', avg_val_loss)\n    '''\n    \n    model.load_state_dict(torch.load('checkpoint.pt'))\n    test_preds = np.zeros((len(test_loader.dataset),len(label_cols)))\n    for i, (x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n        test_preds[i * batch_size:(i+1) * batch_size,:] = sigmoid(y_pred.cpu().numpy())\n    \n    return valid_preds, test_preds,train_loss,valid_loss,valid_auc#, test_preds_local","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"x_test_cuda = torch.tensor(X_test, dtype=torch.long).cuda()\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\nimport os\n\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Args Class"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\nclass Args:\n    #modelType = \"charCNN\"\n    #modelType = \"CLSTM\"\n    #modelType = \"TextCNN\"\n    #modelType = \"RCNN_Text\"\n    #modelType = \"RNN_selfAtten\"\n    modelType = \"RNN_atten\"\n    max_norm = None\n    embed_num = max_features+1\n    embed_dim = embed_size\n    pre_word_Embedding = True\n    init_weight = True\n    init_weight_value  = 1 ##  gain = sqrt(init_weight_value) \n    cuda = True\n    class_num = 6\n    dropout = 0.1\n    dropout_embed = 0.6\n    \n    ### CNN_text Param\n    kernel_num = 32 #400 \n    kernel_sizes = [1,2,3,5]#[3,4,5]\n    wide_conv = False\n    \n    batch_normalizations = False\n    bath_norm_momentum = 0.1\n    batch_norm_affine = False\n    \n    ### RCNN_text/ RNN_selfatten Param\n    output_size = class_num\n    hidden_size = 100\n    vocab_size = embed_num\n    embedding_size = embed_dim\n    \n    ### char_CNN\n    if modelType == \"charCNN\":\n        charDict = CharCNNDictionary()\n        charDict.build_dictionary()\n        charDict\n        vocabulary_size = charDict.vocabulary_size\n        embedding = charDict.embedding\n        mode = \"small\"\n        max_length = 300 # length of char\n    \n    ### CLSTM\n    if modelType == \"CLSTM\":\n        kernel_num = 400\n        kernel_sizes = [3]\n        wide_conv = True\n    \nargs=Args()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config():\n    vocab = max_features\n    n_ctx = maxlen\n    n_head = 6\n    n_embd = embed_size\n    ACT_FNS = 'gelu'\n    n_layer = 1\n    C = 6 # class_num\n    \n    pre_word_Embedding = True\n    embedding_matrix = embedding_matrix\n    \n    \n    embd_pdrop = 0.5\n    attn_pdrop = 0.5\n    resid_pdrop = 0.5\n    \ncfg = Config()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if args.modelType == \"charCNN\":\n    x_train = list_sentences_train.map(lambda x:x[:args.max_length])\n    x_test = list_sentences_test.map(lambda x:x[:args.max_length])\n    \n    X_train = TextDataset(x_train,args.charDict,False,args.max_length,args.max_length).get_all()\n    X_test = TextDataset(x_test,args.charDict,False,args.max_length,args.max_length).get_all()\n    \n    X_train = np.asarray(X_train)\n    X_test = np.asarray(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splits = list(KFold(n_splits=5, shuffle=True, random_state=10).split(X_train, y_train))\ntrain_preds = np.zeros(len(X_train))\ntest_preds = np.zeros((len(X_test), len(splits)))\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score\nfrom functools import reduce\n \nfor size in [1]:\n    print (\"The kernel size is {}\".format(size))\n    tmp = []\n    for i, (train_idx, valid_idx) in enumerate(splits):    \n        x_train_fold = torch.tensor(X_train[train_idx], dtype=torch.long).cuda()\n        y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n        x_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.long).cuda()\n        y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n\n        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n\n        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n\n        print(f'Fold {i + 1}')\n\n        seed_everything(seed + i)\n        #model = CNN_Text(args)\n        #model = RCNN_Text(args)\n        #model = RNN_selfatten(args)\n        model = RNN_atten(args)\n        ##model = char_CNN(args)\n        ##model = C_LSTM(args)\n        #model = TransformerModel(cfg)\n        model.cuda()\n\n        valid_preds_fold, test_preds_fold,train_loss,valid_loss,valid_auc = train_model(model,\n                                                        x_train_fold, \n                                                        y_train_fold, \n                                                        x_val_fold, \n                                                        y_val_fold, validate=True)\n        tmp.append(test_preds_fold/len(splits))\n    \n    test_preds_fold = reduce(lambda x,y:x+y,tmp)\n        \n    submid = pd.DataFrame({'id': subm[\"id\"]})\n    submission = pd.concat([submid, pd.DataFrame(test_preds_fold, columns = label_cols)], axis=1)\n    submission.to_csv('submission'+str(size)+\".csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import graph objects as \"go\"\nimport plotly.offline as ply\nimport plotly.graph_objs as go\nfrom plotly.tools import make_subplots\nfrom plotly.plotly import iplot\nply.init_notebook_mode(connected=True)\n\nimport plotly.plotly as py\npy.sign_in('redinton', 'jQfx5zOJGz7GNQa50ISx')\n\nt = np.linspace(0,len(train_loss),len(train_loss))\n# Creating trace1\ntrace1 = go.Scatter(\n                    x = t,\n                    y = train_loss,\n                    mode = \"lines\",\n                    name = \"train_loss\",\n                    marker = dict(color = 'rgba(16, 112, 2, 0.8)'))\n                    #text= df.university_name)\n# Creating trace2\ntrace2 = go.Scatter(\n                    x = t,\n                    y = valid_loss,\n                    mode = \"lines+markers\",\n                    name = \"valid_loss\",\n                    marker = dict(color = 'rgba(80, 26, 80, 0.8)'))#,\n                    #text= df.university_name)\ndata = [trace1, trace2]\nlayout = dict(title = 'The curve of training loss and validing loss',\n              xaxis= dict(title= 'Epoch',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nt = np.linspace(0,len(train_loss),len(train_loss))\n\nplt.plot(t,train_loss,label='train_loss')\nplt.plot(t,valid_loss,label='valid_loss')\nplt.legend(loc='upper right')\nplt.show()\npd.DataFrame(valid_auc).plot()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}