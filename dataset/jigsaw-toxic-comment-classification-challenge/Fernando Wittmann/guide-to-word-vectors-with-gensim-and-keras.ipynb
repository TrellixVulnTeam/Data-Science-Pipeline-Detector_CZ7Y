{"cells":[{"metadata":{},"cell_type":"markdown","source":"Reference: https://www.depends-on-the-definition.com/guide-to-word-vectors-with-gensim-and-keras/?unapproved=1211&moderation-hash=75c72936db8b4f12b5036f7ce93fc9d5#comment-1211"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!unzip /kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip /kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = ''\n\nTRAIN_DATA_FILE = path + 'train.csv'\nTEST_DATA_FILE = path + 'test.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_DATA_FILE)\ntest_df = pd.read_csv(TEST_DATA_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Processing text dataset')\nfrom nltk.tokenize import WordPunctTokenizer\nfrom collections import Counter\nfrom string import punctuation, ascii_lowercase\nimport regex as re\nfrom tqdm import tqdm\n\n# replace urls\nre_url = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\\n                    .([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",\n                    re.MULTILINE|re.UNICODE)\n# replace ips\nre_ip = re.compile(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\")\n\n# setup tokenizer\ntokenizer = WordPunctTokenizer()\n\nvocab = Counter()\n\ndef text_to_wordlist(text, lower=False):\n    # replace URLs\n    text = re_url.sub(\"URL\", text)\n    \n    # replace IPs\n    text = re_ip.sub(\"IPADDRESS\", text)\n    \n    # Tokenize\n    text = tokenizer.tokenize(text)\n    \n    # optional: lower case\n    if lower:\n        text = [t.lower() for t in text]\n    \n    # Return a list of words\n    vocab.update(text)\n    return text\n\ndef process_comments(list_sentences, lower=False):\n    comments = []\n    for text in tqdm(list_sentences):\n        txt = text_to_wordlist(text, lower=lower)\n        comments.append(txt)\n    return comments\n\n\nlist_sentences_train = list(train_df[\"comment_text\"].fillna(\"NAN_WORD\").values)\nlist_sentences_test = list(test_df[\"comment_text\"].fillna(\"NAN_WORD\").values)\n\ncomments = process_comments(list_sentences_train + list_sentences_test, lower=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The vocabulary contains {} unique tokens\".format(len(vocab)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec(comments, size=100, window=5, min_count=5, workers=16, sg=0, negative=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectors = model.wv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_NB_WORDS = len(word_vectors.vocab)\nMAX_SEQUENCE_LENGTH = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_NB_WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nword_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = [[word_index.get(t, 0) for t in comment]\n             for comment in comments[:len(list_sentences_train)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_sentences_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"' '.join(comments[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"' '.join([str(s) for s in sequences[0]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = [[word_index.get(t, 0)  for t in comment] \n                  for comment in comments[len(list_sentences_train):]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n                     padding=\"pre\", truncating=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df[list_classes].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n                          truncating=\"post\")\nprint('Shape of test_data tensor:', test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WV_DIM = 100\nnb_words = min(MAX_NB_WORDS, len(word_vectors.vocab))\n# we initialize the matrix with random numbers\nwv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wv_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word, i = 'the', 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_vector = word_vectors[word]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WV_DIM = 100\nnb_words = min(MAX_NB_WORDS, len(word_vectors.vocab))\n# we initialize the matrix with random numbers\nwv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0\nfor word, i in word_index.items():\n    if i >= MAX_NB_WORDS:\n        continue\n    try:\n        embedding_vector = word_vectors[word]\n        # words not found in embedding index will be all-zeros.\n        wv_matrix[i] = embedding_vector\n    except:\n        pass  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional, LSTM\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\nfrom keras.layers.normalization import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(nb_words, WV_DIM, weights=[wv_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n\ninput_array = np.random.randint(nb_words, size=(32, MAX_SEQUENCE_LENGTH))\n\nmodel.compile('rmsprop', 'mse')\noutput_array = model.predict(data[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wv_layer = Embedding(nb_words,\n                     WV_DIM,\n                     mask_zero=False,\n                     weights=[wv_matrix],\n                     input_length=MAX_SEQUENCE_LENGTH,\n                     trainable=False)\n\n# Inputs\ncomment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = wv_layer(comment_input)\n\n# biGRU\nembedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\nx = Bidirectional(CuDNNLSTM(64, return_sequences=False))(embedded_sequences)\n\n# Output\nx = Dropout(0.2)(x)\nx = BatchNormalization()(x)\npreds = Dense(6, activation='sigmoid')(x)\n\n# build the model\nmodel = Model(inputs=[comment_input], outputs=preds)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=Adam(lr=0.001, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n              metrics=[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit([data], y, validation_split=0.1,\n                 epochs=10, batch_size=256, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = pd.DataFrame(hist.history)\nplt.figure(figsize=(12,12));\nplt.plot(history[\"loss\"]);\nplt.plot(history[\"val_loss\"]);\nplt.title(\"Loss with pretrained word vectors\");\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}