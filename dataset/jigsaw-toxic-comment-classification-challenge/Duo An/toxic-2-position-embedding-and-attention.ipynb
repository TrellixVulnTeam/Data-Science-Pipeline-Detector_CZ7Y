{"cells":[{"metadata":{"_uuid":"bcaa4fce4eca66a3c7f3852ade4780971842520d"},"cell_type":"markdown","source":"## 基于Position_Embedding和 Attention机制进行文本分类\n\n# toxic comment classification via postion_embedding and attention.\n\n注意力机制即 Attention mechanism在序列学习任务上具有巨大的提升作用，在编解码器框架内，通过在编码段加入Attention模型，对源数据序列进行数据加权变换，或者在解码端引入Attention 模型，对目标数据进行加权变化，可以有效提高序列对序列的自然方式下的系统表现。\n\n### 什么是Attention： \nAttention模型的基本表述可以这样理解成： 当我们人在看一样东西的时候，我们当前时刻关注的一定是我们当前正在看的这样东西的某一地方，换句话说，当我们目光移到别处时，注意力随着目光的移动也在转移。 这意味着，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。 这一点在如下情形下同样成立：当我们试图描述一件事情，我们当前时刻说到的单词和句子和正在描述的该事情的对应某个片段最先关，而其他部分随着描述的进行，相关性也在不断地改变。\n\n从上面两种情形来看，对于 Attention的作用角度出发，我们就可以从两个角度来分类 Attention种类： 空间注意力 Spatial Attention 时间注意力 Temporal Attention 这样的分类更多的是从应用层面上，而从 Attention的作用方法上，可以将其分为 Soft Attention 和 Hard Attention，这既我们所说的， Attention输出的向量分布是一种one-hot的独热分布还是soft的软分布，这直接影响对于上下文信息的选择作用。\n\n### 为什么要加入Attention： \n当输入序列非常长时，模型难以学到合理的向量表示 序列输入时，随着序列的不断增长，原始根据时间步的方式的表现越来越差，这是由于原始的这种时间步模型设计的结构有缺陷，即所有的上下文输入信息都被限制到固定长度，整个模型的能力都同样收到限制，我们暂且把这种原始的模型称为简单的编解码器模型。 编解码器的结构无法解释，也就导致了其无法设计。\n\n### 长输入序列带来的问题： \n使用传统编码器-解码器的RNN模型先用一些LSTM单元来对输入序列进行学习，编码为固定长度的向量表示；然后再用一些LSTM单元来读取这种向量表示并解码为输出序列。 采用这种结构的模型在许多比较难的序列预测问题（如文本翻译）上都取得了最好的结果，因此迅速成为了目前的主流方法。 这种结构在很多其他的领域上也取得了不错的结果。然而，它存在一个问题在于：输入序列不论长短都会被编码成一个固定长度的向量表示，而解码则受限于该固定长度的向量表示。 这个问题限制了模型的性能，尤其是当输入序列比较长时，模型的性能会变得很差（在文本翻译任务上表现为待翻译的原始文本长度过长时翻译质量较差）。\n\n“一个潜在的问题是，采用编码器-解码器结构的神经网络模型需要将输入序列中的必要信息表示为一个固定长度的向量，而当输入序列很长时则难以保留全部的必要信息（因为太多），尤其是当输入序列的长度比训练数据集中的更长时。” — Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015\n\n### 如何使用Attention机制：\nAttention机制的基本思想是：打破了传统编码器-解码器结构在编解码时都依赖于内部一个固定长度向量的限制。 Attention机制的实现是 通过保留LSTM编码器对输入序列的中间输出结果，然后训练一个模型来对这些输入进行选择性的学习并且在模型输出时将输出序列与之进行关联。\n\n换一个角度而言，输出序列中的每一项的生成概率取决于在输入序列中选择了哪些项。\n\nAttention-based Model 其实就是一个相似性的度量，当前的输入与目标状态约相似，那么在当前的输入的权重就会越大。就是在原有的model上加入了Attention的思想。\n\n**没有attention机制的encoder-decoder结构通常把encoder的最后一个状态作为decoder的输入（可能作为初始化，也可能作为每一时刻的输入），但是encoder的state毕竟是有限的，存储不了太多的信息，对于decoder过程，每一个步骤都和之前的输入都没有关系了，只与这个传入的state有关。attention机制的引入之后，decoder根据时刻的不同，让每一时刻的输入都有所不同。**\n"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:57:32.722117Z","start_time":"2018-12-10T14:57:32.705184Z"},"trusted":false,"_uuid":"2c9d2b5801ed782dcc3a46db9daa282368a1b12a"},"cell_type":"code","source":"import os\nos.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:57:33.794667Z","start_time":"2018-12-10T14:57:32.72467Z"},"trusted":false,"_uuid":"663ab9359ffa8ec498464e9ca038bbbf53f98f15"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:57:33.801038Z","start_time":"2018-12-10T14:57:33.797953Z"},"trusted":false,"_uuid":"cef6a5baa26f665f0affce084668ea8078d9f3c8"},"cell_type":"code","source":"np.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:57:35.597085Z","start_time":"2018-12-10T14:57:33.80667Z"},"trusted":false,"_uuid":"344d1abe7491b6db487f4d9fbff6832ae9867383"},"cell_type":"code","source":"# read data to dataframe\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n\nprint('train shape {} rows, {} cols'.format(*df_train.shape))\nprint('test shape {} rows, {} cols'.format(*df_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:57:35.628899Z","start_time":"2018-12-10T14:57:35.599462Z"},"trusted":false,"_uuid":"caa124b1fb730325dfba19f3cb0683e4f5ac1686"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:57:35.649055Z","start_time":"2018-12-10T14:57:35.633585Z"},"trusted":false,"_uuid":"4f5ff85851a068f9a0976a7eddb424f72e932220"},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:57:35.688674Z","start_time":"2018-12-10T14:57:35.65234Z"},"trusted":false,"_uuid":"9aec092289740bc68ff987f69f9da7302f134b00"},"cell_type":"code","source":"target_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nfor label in target_cols:\n  print(label)\n  print(df_train[label].value_counts())\n  print('*'*80)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:57:39.942188Z","start_time":"2018-12-10T14:57:35.691049Z"},"trusted":false,"_uuid":"cbb7fd805f095293d71ebedc22ce950e7ece291e"},"cell_type":"code","source":"from keras.preprocessing import text, sequence","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:57:40.005891Z","start_time":"2018-12-10T14:57:39.944178Z"},"trusted":false,"_uuid":"6ddc73c9173b53ea115af573f72db16df25fba95"},"cell_type":"code","source":"X_train = df_train[\"comment_text\"].fillna(\"fillna\").values\n\ny_train = df_train[target_cols].values\nX_test = df_test[\"comment_text\"].fillna(\"fillna\").values","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:58:20.662938Z","start_time":"2018-12-10T14:57:40.008048Z"},"trusted":false,"_uuid":"01dcd9f5bae77522483990606b8822b16d4bc17b"},"cell_type":"code","source":"max_features = 30000\nmaxlen = 100\nembed_size = 300\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\n\nX_train = tokenizer.texts_to_sequences(X_train)\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n\nX_test = tokenizer.texts_to_sequences(X_test)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:58:20.681119Z","start_time":"2018-12-10T14:58:20.664369Z"},"trusted":false,"_uuid":"67ef97d5a98d8c9a81a951d0338c4132ba064eb2"},"cell_type":"code","source":"from keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras.models import Model\nfrom keras.layers import *\n\n\nclass Position_Embedding(Layer):\n\n  def __init__(self, size=None, mode='sum', **kwargs):\n    self.size = size  # 必须为偶数\n    self.mode = mode\n    super(Position_Embedding, self).__init__(**kwargs)\n\n  def call(self, x):\n    if (self.size == None) or (self.mode == 'sum'):\n      self.size = int(x.shape[-1])\n    batch_size, seq_len = K.shape(x)[0], K.shape(x)[1]\n    position_j = 1. / K.pow(10000., 2 * K.arange(self.size / 2, dtype='float32') / self.size)\n    position_j = K.expand_dims(position_j, 0)\n    position_i = K.cumsum(K.ones_like(x[:, :, 0]),\n                          1) - 1  # K.arange不支持变长，只好用这种方法生成\n    position_i = K.expand_dims(position_i, 2)\n    position_ij = K.dot(position_i, position_j)\n    position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n    if self.mode == 'sum':\n      return position_ij + x\n    elif self.mode == 'concat':\n      return K.concatenate([position_ij, x], 2)\n\n  def compute_output_shape(self, input_shape):\n    if self.mode == 'sum':\n      return input_shape\n    elif self.mode == 'concat':\n      return (input_shape[0], input_shape[1], input_shape[2] + self.size)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:58:20.70449Z","start_time":"2018-12-10T14:58:20.683837Z"},"trusted":false,"_uuid":"5b6c302c2b01a089e68b0b08fb863e24c462506d"},"cell_type":"code","source":"class Attention(Layer):\n\n  def __init__(self, nb_head, size_per_head, **kwargs):\n    self.nb_head = nb_head\n    self.size_per_head = size_per_head\n    self.output_dim = nb_head * size_per_head\n    super(Attention, self).__init__(**kwargs)\n\n  def build(self, input_shape):\n    self.WQ = self.add_weight(\n        name='WQ',\n        shape=(input_shape[0][-1], self.output_dim),\n        initializer='glorot_uniform',\n        trainable=True)\n    self.WK = self.add_weight(\n        name='WK',\n        shape=(input_shape[1][-1], self.output_dim),\n        initializer='glorot_uniform',\n        trainable=True)\n    self.WV = self.add_weight(\n        name='WV',\n        shape=(input_shape[2][-1], self.output_dim),\n        initializer='glorot_uniform',\n        trainable=True)\n    super(Attention, self).build(input_shape)\n\n  def Mask(self, inputs, seq_len, mode='mul'):\n    if seq_len == None:\n      return inputs\n    else:\n      mask = K.one_hot(seq_len[:, 0], K.shape(inputs)[1])\n      mask = 1 - K.cumsum(mask, 1)\n      for _ in range(len(inputs.shape) - 2):\n        mask = K.expand_dims(mask, 2)\n      if mode == 'mul':\n        return inputs * mask\n      if mode == 'add':\n        return inputs - (1 - mask) * 1e12\n\n  def call(self, x):\n    # 如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n    # 如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n    if len(x) == 3:\n      Q_seq, K_seq, V_seq = x\n      Q_len, V_len = None, None\n    elif len(x) == 5:\n      Q_seq, K_seq, V_seq, Q_len, V_len = x\n    # 对Q、K、V做线性变换\n    Q_seq = K.dot(Q_seq, self.WQ)\n    Q_seq = K.reshape(Q_seq,\n                      (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n    Q_seq = K.permute_dimensions(Q_seq, (0, 2, 1, 3))\n    K_seq = K.dot(K_seq, self.WK)\n    K_seq = K.reshape(K_seq,\n                      (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n    K_seq = K.permute_dimensions(K_seq, (0, 2, 1, 3))\n    V_seq = K.dot(V_seq, self.WV)\n    V_seq = K.reshape(V_seq,\n                      (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n    V_seq = K.permute_dimensions(V_seq, (0, 2, 1, 3))\n    # 计算内积，然后mask，然后softmax\n    A = K.batch_dot(Q_seq, K_seq, axes=[3, 3]) / self.size_per_head**0.5\n    A = K.permute_dimensions(A, (0, 3, 2, 1))\n    A = self.Mask(A, V_len, 'add')\n    A = K.permute_dimensions(A, (0, 3, 2, 1))\n    A = K.softmax(A)\n    # 输出并mask\n    O_seq = K.batch_dot(A, V_seq, axes=[3, 2])\n    O_seq = K.permute_dimensions(O_seq, (0, 2, 1, 3))\n    O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n    O_seq = self.Mask(O_seq, Q_len, 'mul')\n    return O_seq\n\n  def compute_output_shape(self, input_shape):\n    return (input_shape[0][0], input_shape[0][1], self.output_dim)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:58:21.769394Z","start_time":"2018-12-10T14:58:20.709775Z"},"trusted":false,"_uuid":"807fb14ea33188fda35065c5aff602ffd9c4f557"},"cell_type":"code","source":"from keras.callbacks import *\nfrom sklearn.metrics import roc_auc_score\n\n\nclass RocAucEvaluation(Callback):\n\n  def __init__(self, validation_data=(), interval=1):\n    super(Callback, self).__init__()\n    self.interval = interval\n    self.X_val, self.y_val = validation_data\n\n  def on_epoch_end(self, epoch, logs={}):\n    if epoch % self.interval == 0:\n      y_pred = self.model.predict(self.X_val, verbose=0)\n      score = roc_auc_score(self.y_val, y_pred)\n      print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch + 1, score))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:58:21.989041Z","start_time":"2018-12-10T14:58:21.771683Z"},"trusted":false,"_uuid":"3776632527637e30a6523b9c660092e9e6f75f8e"},"cell_type":"code","source":"S_inputs = Input(shape=(None,), dtype='int32')\nembeddings = Embedding(max_features, 128)(S_inputs)\nembeddings = Position_Embedding()(embeddings)\nO_seq = Attention(8, 16)([embeddings, embeddings, embeddings])\nO_seq = GlobalMaxPooling1D()(O_seq)\nO_seq = Dropout(0.8)(O_seq)\noutputs = Dense(6, activation='sigmoid')(O_seq)\nmodel = Model(inputs=S_inputs, outputs=outputs)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:58:22.118054Z","start_time":"2018-12-10T14:58:21.992043Z"},"trusted":false,"_uuid":"40fd92f9d3d1d43e9a0a1020b204c4d95f7b6546"},"cell_type":"code","source":"from IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:58:22.166615Z","start_time":"2018-12-10T14:58:22.121369Z"},"trusted":false,"_uuid":"8a69ba2f478b6157f76bb786272fb6c2788b430c"},"cell_type":"code","source":"model.compile(\n    loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:58:22.174155Z","start_time":"2018-12-10T14:58:22.168443Z"},"trusted":false,"_uuid":"73d0fba989ac860626a0d23c46c1b8ec8b3961b6"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T14:58:22.311947Z","start_time":"2018-12-10T14:58:22.176996Z"},"trusted":false,"_uuid":"a9a6c86559670b9ff2997067f3f2411cc688560b"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_tra, X_val, y_tra, y_val = train_test_split(\n    x_train, y_train, test_size=0.3, random_state=233)\nroc_auc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T15:01:01.42492Z","start_time":"2018-12-10T15:00:49.231487Z"},"trusted":false,"_uuid":"af3299fa2e56ddc6d7b71bebbdb88c55c0d25bbe"},"cell_type":"code","source":"hist = model.fit(\n    X_tra,\n    y_tra,\n    callbacks=[EarlyStopping(patience=10), roc_auc],\n    batch_size=32,\n    epochs=1000,\n    validation_data=(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dc8b0e6c10b74b26cce4411784ab98e2609ef8cf"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.plot(hist.history['acc'])\nplt.plot(hist.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6e3097b593d7e4503143dcaf04e9999c9a61c23e"},"cell_type":"code","source":"y_pred = model.predict(x_test, batch_size=1024)\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission[target_cols] = y_pred\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}