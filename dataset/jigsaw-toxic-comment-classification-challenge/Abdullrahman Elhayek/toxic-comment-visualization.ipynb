{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Toxic Comment Exploration Notebook \n* Loading the Data \n* Running essential EDA \n* Word Cloud Visualization \n* Knowldage Graph Visualization \n* Tensorboard Visualization \n"},{"metadata":{},"cell_type":"markdown","source":"## Models and Scores "},{"metadata":{},"cell_type":"markdown","source":"* Preceptorn Nueral Network + Word embedding Kaggle Score : 0.90994 [Press Here](https://www.kaggle.com/ahayek84/toxic-comment-classification-challenge)\n* GRU with Pooling + Word embeding Kaggle Score : 0.95 [Press Here](https://www.kaggle.com/ahayek84/fork-of-toxic-comment-classification-gru)\n* BERT pre-trained model as proof of concept  [Press Here](https://www.kaggle.com/ahayek84/toxic-comment-bert-tf1-proof-of-concept)\n* Bidirectional GRU with Pooling + Glove Kaggle Score : 0.98112 [Press Here](https://www.kaggle.com/ahayek84/toxic-comment-gru-glove)\n* Bidirectional LSTM with Pooling + Glove Kaggle Score : 0.98 [Press Here](https://www.kaggle.com/ahayek84/toxic-comment-lstm-glove)\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Loading the Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start with loading all necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\nimport csv\n\n\nimport collections\nprint(os.listdir(\"../working/\"))\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load in the dataframe\ndf = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Running essential EDA "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of rows in data =\",df.shape[0])\nprint(\"Number of columns in data =\",df.shape[1])\nprint(\"\\n\")\nprint(\"**Sample data:**\")\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} observations and {} features in this dataset. \\n\".format(df.shape[0],df.shape[1]))\n\nprint(\"There are {} words in this dataset such as {}... \\n\".format(len(df.comment_text.unique()),\n                                                                           \", \".join(df.comment_text.unique()[0:1])))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"comment_text\"]].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we count the number of comments under each label. (For detailed code, please refer to the GitHub link of this project.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = list(df.columns.values)\nsns.set(font_scale = 2)\nplt.figure(figsize=(15,8))\nax= sns.barplot(categories[2:], df.iloc[:,2:].sum().values)\nplt.title(\"Comments in each category\", fontsize=24)\nplt.ylabel('Number of comments', fontsize=18)\nplt.xlabel('Comment Type ', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = df.iloc[:,2:].sum().values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Counting the number of comments having multiple labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"rowSums = df.iloc[:,2:].sum(axis=1)\nmultiLabel_counts = rowSums.value_counts()\nmultiLabel_counts = multiLabel_counts.iloc[1:]\nsns.set(font_scale = 2)\nplt.figure(figsize=(15,8))\nax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\nplt.title(\"Comments having multiple labels \")\nplt.ylabel('Number of comments', fontsize=18)\nplt.xlabel('Number of labels', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = multiLabel_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Cloud visualization "},{"metadata":{},"cell_type":"markdown","source":"WordCloud representation of most used words in each category of comments."},{"metadata":{},"cell_type":"markdown","source":"#### Utility functions for text cleaning "},{"metadata":{"trusted":true},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()\ndef clean(s): return re_tok.sub(r' \\1 ', s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\nimport sys\nimport warnings\ndata = df\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\ndef cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(sentence))\n    return cleantext\ndef cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent\ndata['comment_text'] = data['comment_text'].str.lower()\ndata['comment_text'] = data['comment_text'].apply(cleanHtml)\ndata['comment_text'] = data['comment_text'].apply(cleanPunc)\ndata['comment_text'] = data['comment_text'].apply(keepAlpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start with one review:\ntext = df.comment_text[0]\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color=\"white\").generate(text)\n# Display the generated image:\nplt.figure(figsize=(30,50))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_text(text_col):\n    ## decide vocab size\n    text = text_col\n    words = []\n    for t in text:\n        words.extend(tokenize(t))\n    ##print(words[:100])\n    vocab = list(set(words))\n    ##print(len(words), len(vocab))\n    words_str1 = ' '.join(str(e) for e in words)  \n    \n    # lower max_font_size, change the maximum number of word and lighten the background:\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                              collocations=False,\n                              width=2500,\n                              height=1800, background_color=\"white\").generate(words_str1)\n    plt.figure(figsize=(30,50))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_text(df['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### word cloud for each category "},{"metadata":{"trusted":true},"cell_type":"code","source":"toxic_comments = df.loc[df.toxic != 0]['comment_text']\nsevere_toxic_comments = df.loc[df.severe_toxic != 0]['comment_text']\nobscene_comments = df.loc[df.obscene != 0]['comment_text']\nthreat_comments = df.loc[df.threat != 0]['comment_text']\ninsult_comments = df.loc[df.insult != 0]['comment_text']\nidentity_hate_comments = df.loc[df.identity_hate != 0]['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_text(toxic_comments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_text(severe_toxic_comments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_text(obscene_comments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_text(threat_comments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_text(insult_comments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_text(identity_hate_comments)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Knowledge Graph Visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \n\nimport networkx as nx\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"candidate_sentences = df['comment_text']\ncandidate_sentences.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(\"The 22-year-old recently won ATP Challenger tournament.\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(\"Nagal won the first set.\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(\"the drawdown process is governed by astm standard d823\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Entity Pairs Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_entities(sent):\n  ## chunk 1\n  ent1 = \"\"\n  ent2 = \"\"\n\n  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n  prv_tok_text = \"\"   # previous token in the sentence\n\n  prefix = \"\"\n  modifier = \"\"\n\n  #############################################################\n  \n  for tok in nlp(sent):\n    ## chunk 2\n    # if token is a punctuation mark then move on to the next token\n    if tok.dep_ != \"punct\":\n      # check: token is a compound word or not\n      if tok.dep_ == \"compound\":\n        prefix = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          prefix = prv_tok_text + \" \"+ tok.text\n      \n      # check: token is a modifier or not\n      if tok.dep_.endswith(\"mod\") == True:\n        modifier = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          modifier = prv_tok_text + \" \"+ tok.text\n      \n      ## chunk 3\n      if tok.dep_.find(\"subj\") == True:\n        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n        prefix = \"\"\n        modifier = \"\"\n        prv_tok_dep = \"\"\n        prv_tok_text = \"\"      \n\n      ## chunk 4\n      if tok.dep_.find(\"obj\") == True:\n        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n        \n      ## chunk 5  \n      # update variables\n      prv_tok_dep = tok.dep_\n      prv_tok_text = tok.text\n  #############################################################\n\n  return [ent1.strip(), ent2.strip()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_entities(\"the film had 200 patents\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#entity_pairs = []\n\n#for i in tqdm(candidate_sentences):\n#  entity_pairs.append(get_entities(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## save paires\n#filename = 'entity_pairs.csv'\n#import csv\n#with open(filename, 'w') as f:\n#   writer = csv.writer(f, delimiter=',')\n#   writer.writerows(entity_pairs)  #considering my_list is a list of lists.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## load paires \nl_entity_pairs = []\ne_file = \"../input/saved-relations/entity_pairs.csv\" ## read preloaded entity_paires\n#e_file = \"hm_data/toxic_data/entity_pairs.csv\" ## read session written entity paires\nwith open(e_file, 'r') as csvfile:\n    entity_pairs_file = csv.reader(csvfile, delimiter=',')\n    for row in entity_pairs_file:\n        for re in row:\n            re = re.replace('\"','')\n            re = eval(re)\n            l_entity_pairs.append(re)      \nentity_pairs = l_entity_pairs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"entity_pairs[10:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Relation / Predicate Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_relation(sent):\n\n  doc = nlp(sent)\n\n  # Matcher class object \n  matcher = Matcher(nlp.vocab)\n\n  #define the pattern \n  pattern = [{'DEP':'ROOT'}, \n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},  \n            {'POS':'ADJ','OP':\"?\"}] \n\n  matcher.add(\"matching_1\", None, pattern) \n\n  matches = matcher(doc)\n  k = len(matches) - 1\n\n  span = doc[matches[k][1]:matches[k][2]] \n\n  return(span.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #relations = [get_relation(i) for i in tqdm(candidate_sentences)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## save paires\n#filename = 'relations.csv'\n#import csv\n#with open(filename, 'w') as f:\n#   writer = csv.writer(f, delimiter=',')\n#   writer.writerows(relations)  #considering my_list is a list of lists.\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## load relations \ne_file = \"../input/saved-relations/relations.csv\" ## read preloaded entity_paires\n#e_file = \"hm_data/toxic_data/relations.csv\" ## read session written entity paires\nl_relations = []\nwith open(e_file, 'r') as csvfile:\n    relations_file = csv.reader(csvfile, delimiter=',')\n    for row in relations_file:\n        for re in row:\n            l_relations.append(re)\n        #l_relations.append(''.join(row))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(l_relations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_relations[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relations = l_relations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = pd.Series(relations).value_counts()\ns[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## get as much as you want from verbs and their number of links\nprint(s[30:50])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build a Knowledge Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract subject\nsource = [i[0] for i in entity_pairs]\n\n# extract object\ntarget = [i[1] for i in entity_pairs]\n\nkg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a directed-graph from a dataframe\nG=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.figure(figsize=(12,12))\n\n#pos = nx.spring_layout(G)\n#nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore Key words in the Corpus related Entities"},{"metadata":{},"cell_type":"markdown","source":"#### Word : nigger"},{"metadata":{"trusted":true},"cell_type":"code","source":"## incoming \nG=nx.from_pandas_edgelist(kg_df[kg_df['source']==\"nigger\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## outgoing\nG=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"kiss\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### word : wikipedia"},{"metadata":{"trusted":true},"cell_type":"code","source":"## outging \nG=nx.from_pandas_edgelist(kg_df[kg_df['source']==\"wikipedia\"][:20], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5)\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## incoming \nG=nx.from_pandas_edgelist(kg_df[kg_df['target']==\"wikipedia\"][:20], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5)\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore Key verbs (relations) in the Corpus related Entities"},{"metadata":{},"cell_type":"markdown","source":"### word: fuck"},{"metadata":{"trusted":true},"cell_type":"code","source":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"fuck\"][:30], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### word : suck"},{"metadata":{"trusted":true},"cell_type":"code","source":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"suck\"][:25], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5)\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tensorboard Visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"## type at local computer \n# python -m tensorboard.main --logdir=models\n## at bti_tf1 enviroment and project folder\n### copy http://localhost:6006/ to your browser ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![test](https://raw.githubusercontent.com/ahayek84/bti_tf1/master/image/2019-10-25_22h03_11.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}