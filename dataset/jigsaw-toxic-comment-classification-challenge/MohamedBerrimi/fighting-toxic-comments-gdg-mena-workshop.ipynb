{"cells":[{"metadata":{"_uuid":"740ebff6-c24b-40e0-abc5-d95d1d494afc","_cell_guid":"4c555af0-799e-4dbe-bb87-5825f9b655d2","trusted":true},"cell_type":"code","source":"\"\"\"\nI wil use a simple RNN architecture withData with preprocessed version with no Embedding weights.\n\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom keras.layers import Dense, Input, LSTM, Bidirectional, Conv1D\nfrom keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D,Dropout,Flatten\nfrom keras.layers import Dropout, Embedding\nfrom keras.preprocessing import text, sequence\nfrom keras.models import Model\n\ntrain_x = pd.read_csv('../input/cleaned-toxic-comments/train_preprocessed.csv').fillna(\" \")\ntest_x = pd.read_csv('../input/cleaned-toxic-comments/test_preprocessed.csv').fillna(\" \")\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tesSt_x = pd.read_csv('../input/cleaned-toxic-comments/test_preprocessed.csv').fillna(\" \")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmax_features=50000\nmaxlen=150\nembed_size=300\n\ntrain_x['comment_text'].fillna(' ')\ntest_x['comment_text'].fillna(' ')\ntrain_y = train_x[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\ntrain_x = train_x['comment_text'].str.lower()\n\ntest_x = test_x['comment_text'].str.lower()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytest=tesSt_x[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorize text \ntokenizer = text.Tokenizer(num_words=max_features, lower=True)\ntokenizer.fit_on_texts(list(train_x))\n\ntrain_x = tokenizer.texts_to_sequences(train_x)\ntest_x = tokenizer.texts_to_sequences(test_x)\n\ntrain_x = sequence.pad_sequences(train_x, maxlen=maxlen)\ntest_x = sequence.pad_sequences(test_x, maxlen=maxlen)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Build Model\ninp = Input(shape=(maxlen,))\n\nx = Embedding(max_features, embed_size, trainable=True)(inp)\nx = Dropout(0.3)(x)\n\nx = Bidirectional(LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(x)\nx = Flatten()(x)\nout = Dense(6, activation='softmax')(x)\n\nmodel = Model(inp, out)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Prediction\nbatch_size = 250\nepochs = 2\n\nmodel.fit(train_x, train_y, batch_size=batch_size, validation_split=0.1 ,epochs=epochs, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predictions = model.predict(test_x, batch_size=batch_size, verbose=1)\n\nsubmission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\nsubmission[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] = predictions\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras import backend as K, initializers, regularizers, constraints\nfrom keras.engine.topology import Layer\n\n\n# modified based on `https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2`\nclass Attention(Layer):\n    \"\"\"\n    Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n e: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification/regression) or whatever...\n    \"\"\"\n\n    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None, W_constraint=None,\n                 u_constraint=None, b_constraint=None, use_W=True, use_bias=False, return_self_attend=False,\n                 return_attend_weight=True, **kwargs):\n        self.supports_masking = True\n\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.use_W = use_W\n        self.use_bias = use_bias\n        self.return_self_attend = return_self_attend    # whether perform self attention and return it\n        self.return_attend_weight = return_attend_weight    # whether return attention weight\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        if self.use_W:\n            self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),  initializer=self.init,\n                                     name='{}_W'.format(self.name), regularizer=self.W_regularizer,\n                                     constraint=self.W_constraint)\n        if self.use_bias:\n            self.b = self.add_weight(shape=(input_shape[1],), initializer='zero', name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer, constraint=self.b_constraint)\n\n        self.u = self.add_weight(shape=(input_shape[-1],), initializer=self.init, name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer, constraint=self.u_constraint)\n        \n        super(Attention, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        if self.use_W:\n            x = K.tanh(K.dot(x, self.W))\n\n        ait = Attention.dot_product(x, self.u)\n        if self.use_bias:\n            ait += self.b\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        if self.return_self_attend:\n            attend_output = K.sum(x * K.expand_dims(a), axis=1)\n            if self.return_attend_weight:\n                return [attend_output, a]\n            else:\n                return attend_output\n        else:\n            return a\n\n    def compute_output_shape(self, input_shape):\n        if self.return_self_attend:\n            if self.return_attend_weight:\n                return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[1])]\n            else:\n                return input_shape[0], input_shape[-1]\n        else:\n            return input_shape[0], input_shape[1]\n\n    @staticmethod\n    def dot_product(x, kernel):\n        \"\"\"\n        Wrapper for dot product operation, in order to be compatible with both\n        Theano and Tensorflow\n        Args:\n            x (): input\n            kernel (): weights\n        Returns:\n        \"\"\"\n        if K.backend() == 'tensorflow':\n            return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n        else:\n            return K.dot(x, kernel)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Build Model\ninp = Input(shape=(maxlen,))\n\nx = Embedding(max_features, embed_size, trainable=True)(inp)\nx = Dropout(0.3)(x)\n\nx = Bidirectional(LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(x)\nx= Attention()(x)\n\nout = Dense(6, activation='softmax')(x)\n\nmodel = Model(inp, out)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Prediction\nbatch_size = 250\nepochs = 2\n\nmodel.fit(train_x, train_y, batch_size=batch_size, validation_split=0.1 ,epochs=epochs, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}