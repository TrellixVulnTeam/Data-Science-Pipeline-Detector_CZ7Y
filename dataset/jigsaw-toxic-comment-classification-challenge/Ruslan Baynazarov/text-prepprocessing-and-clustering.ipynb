{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Toxic comments detection\nThe adjective 'toxic' is the [word of the year 2018 by Oxford](https://languages.oup.com/word-of-the-year/2018/).\n> The Oxford Word of the Year is a word or expression that is judged to reflect the ethos, mood, or preoccupations of the passing year, and have lasting potential as a term of cultural significance.\n> \n>  \n> \n> In 2018, toxic added many strings to its poisoned bow becoming an intoxicating descriptor for the yearâ€™s most talked about topics. It is the sheer scope of its application, as found by our research, that made toxic the stand-out choice for the Word of the Year title.\n\nToxic means: `extremely harsh, malicious, or harmful`.  \nexample: toxic sarcasm","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm_notebook as tqdm\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/jigsaw-toxic-comment-classification-challenge\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load the dataset\ntrain = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntarget = train['toxic']\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(target, kde=False)\nprint(target.mean())\nprint('Minimum accuracy:', max(target.mean(), 1 - target.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# define preprocessing function\nimport string\n\ndef preprocess(doc):\n    # lowercasing\n    doc = doc.lower()\n    # remove punctuation and different kinds of whitespaces e.g. newlines and tabs\n    for p in string.punctuation + string.whitespace:\n        doc = doc.replace(p, ' ')\n    # remove unneeded spaces\n    doc = doc.strip()\n    doc = ' '.join([w for w in doc.split(' ') if w != ''])\n    return doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessed text corpus\ncorpus = train['comment_text'].map(preprocess)\ncorpus.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nvectorizer = TfidfVectorizer(max_features=30000)\nsvd = TruncatedSVD(n_components=100)\n\nX_tfidf = vectorizer.fit_transform(corpus)\nprint(X_tfidf.shape)\nX_svd = svd.fit_transform(X_tfidf)\nprint(X_svd.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.utils.testing import ignore_warnings\n\ndef eval_on_trainset(X, y, model_names=None):\n    models = {\n        'SVM_rbf': SVC(C=100, kernel='rbf'),\n        'SVM_linear': SVC(C=100, kernel='linear'),\n        'Log regression': LogisticRegression(),\n        'naive bayes': GaussianNB(),\n        'random forest': RandomForestClassifier(),\n        'KNN': KNeighborsClassifier(),\n    }\n    scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    \n    print('Dummy accuracy:', max(y.mean(), 1 - y.mean()))\n    print()\n    \n    try:\n        X = X.toarray()\n    except:\n        pass\n    \n    for name in model_names or sorted(models):\n        model = models[name]\n        with ignore_warnings():\n            scores_svc = cross_validate(model, X, y, cv=3, scoring=scoring)\n        for sc in scoring:\n            mean = scores_svc['test_' + sc].mean()\n            std = scores_svc['test_' + sc].std()\n            print(name, sc, '{:.03} +- {:.03}'.format(mean, std))\n        print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Small part of tfidf, because of time and memory usage\n%time eval_on_trainset(X_tfidf[:3000], target[:3000], ['Log regression', 'naive bayes', 'random forest'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Part of data, because of time usage\n%time eval_on_trainset(X_svd[:10000], target[:10000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Eval on full data, for fastest algorythms\n%time eval_on_trainset(X_svd, target, ['Log regression', 'naive bayes', 'random forest'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVD Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_components = 5\n\nplot_data = pd.DataFrame(X_svd[:1000, :n_components], columns=['f{}'.format(i) for i in range(n_components)])\nplot_data['target'] = target[:1000]\nsns.pairplot(plot_data, hue='target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text preprocessing low-level code","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenize\nimport nltk\n\ntok_corpus = []\nfor sent in tqdm(corpus):\n#     tok_corpus.append(nltk.word_tokenize(sent))\n    tok_corpus.append(sent.split())\n\ntok_corpus[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create dictionary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# count words\ncounter = {}\nfor sent in tqdm(tok_corpus):\n    for word in sent:\n        counter[word] = counter.get(word, 0) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.plot(sorted([np.log10(v) for v in counter.values()], reverse=True))\nplt.xlabel('Word id')\nplt.ylabel('log(frequency)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More than half of the words have frequency 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make vocab\nvocab = sorted(list(counter), key=counter.get,reverse=True)\nprint('Length:', len(vocab))\n# Take only frequent words\nmin_count = 10\nvocab = [word for word in vocab if counter[word] >= min_count]\nprint('Length:', len(vocab))\n# Add <UNK> token\nvocab.append('<UNK>')\nprint(vocab[:5])\n\n# Make word index\nword2idx = {word: idx for (idx, word) in enumerate(vocab)}\nprint(word2idx['the'], word2idx['to'], word2idx['hello'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode corpus","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_corpus = [[word2idx[word] for word in sent if word in word2idx] for sent in tqdm(tok_corpus)]\nencoded_corpus[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load word2vec","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\n\nw2v_google = gensim.models.KeyedVectors.load_word2vec_format(\"../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin\", binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec = w2v_google['hello']\nprint(type(vec))\nprint(vec.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_google.most_similar([vec])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_google.most_similar(['hello'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_google.most_similar(positive=['woman', 'king'], negative=['man'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_google.most_similar(positive=['father', 'woman'], negative=['man'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Task:** try to get word \"moscow\" using the same principle","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE\n# w2v_google.most_similar(positive=['...', '...'], negative=['...'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check consistency with our corpus","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_google.most_similar(['fuck'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_google.most_similar(['ass'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train on word2vec bag of words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bag of words\nX_w2v = [np.sum([np.zeros(vec.shape)] + [w2v_google[w] for w in sent if w in w2v_google], axis=0) for sent in tqdm(tok_corpus)]\nnormalize = lambda x: x / np.sqrt(np.sum(x**2) + 1e-8)\nX_w2v = [normalize(x) for x in tqdm(X_w2v)]\nX_w2v = np.array(X_w2v)\nX_w2v.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time eval_on_trainset(X_w2v[:1000], target[:1000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time eval_on_trainset(X_w2v[:10000], target[:10000], ['Log regression', 'random forest'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. # Word2vec visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_components = 5\n\nplot_data = pd.DataFrame(X_w2v[:1000, :n_components], columns=['f{}'.format(i) for i in range(n_components)])\nplot_data['target'] = target[:1000]\nsns.pairplot(plot_data, hue='target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_components = 5\n\nplot_data = pd.DataFrame(TruncatedSVD(n_components=n_components).fit_transform(X_w2v)[:1000], columns=['f{}'.format(i) for i in range(n_components)])\nplot_data['target'] = target[:1000]\nsns.pairplot(plot_data, hue='target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KMeans Clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans, MeanShift, AgglomerativeClustering, DBSCAN\nfrom sklearn.mixture import GaussianMixture\n\nfrom sklearn.metrics import silhouette_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def distance(point1, point2):\n    return np.sqrt(np.sum((point1 - point2)**2))\n\ndists = []\nscores = []\n\nfor k in tqdm(range(2, 21)):\n    kmeans = MiniBatchKMeans(k)\n    kmeans.fit(X_w2v)\n    centers = kmeans.cluster_centers_\n    labels = kmeans.predict(X_w2v)\n    # Mean squared distance\n    mean_dist = np.sum([distance(x, centers[label])**2 for x, label in zip(X_w2v, labels)])\n    dists.append(mean_dist)\n    # Silhouette\n    score = silhouette_score(X_w2v[:2000], labels[:2000], metric='euclidean')\n    scores.append(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 8))\nplt.plot(np.arange(2, 21), dists)\nplt.ylabel('Sum of squared distance')\nplt.xlabel('Number of clusters k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elbow can be considered at k value from 3 to 6.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 8))\nplt.plot(np.arange(2, 21), scores)\nplt.ylabel('Silhouette score')\nplt.xlabel('Number of clusters k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 7\nkmeans = MiniBatchKMeans(k, random_state=2)\nkmeans.fit(X_w2v)\ntrain['KMeans'] = kmeans.predict(X_w2v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.predict(X_w2v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,6))\nsns.barplot(x='KMeans' , y='toxic' , data=train)\nplt.ylabel(\"Toxic\")\nplt.title(\"Toxic as function of KMeans\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the most toxic sentence\nam = np.argmin(np.sum((X_w2v - kmeans.cluster_centers_[0].reshape([1, 300]))**2, axis=1))\ncorpus[am]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the least toxic sentence\nam = np.argmin(np.sum((X_w2v[target == 1] - kmeans.cluster_centers_[1].reshape([1, 300]))**2, 1))\ncorpus[target == 1].iloc[am]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 7\nkmeans = MiniBatchKMeans(k, random_state=0)\nkmeans.fit(X_svd)\ntrain['KMeans_SVD'] = kmeans.predict(X_svd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,6))\nsns.barplot(x='KMeans_SVD' , y='toxic' , data=train)\nplt.ylabel(\"Toxic\")\nplt.title(\"Toxic as function of KMeans\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the most toxic sentence\nam = np.argmin(np.sum((X_svd - kmeans.cluster_centers_[0].reshape([1, 100]))**2, 1))\ncorpus[am]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction based entirely on one cluster:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = (train['KMeans'] == 0).astype('int')\nprint('accuracy', accuracy_score(preds, target))\nprint('precision', precision_score(target, preds))\nprint('recall', recall_score(target, preds))\nprint('f1', f1_score(target, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MeanShift","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmeanshift = MeanShift(bandwidth=0.9)\nmeanshift.fit(X_w2v[:1000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['MeanShift'] = meanshift.predict(X_w2v)\nprint(len(train['MeanShift'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,6))\nsns.barplot(x='MeanShift' , y='toxic' , data=train)\nplt.ylabel(\"Toxic\")\nplt.title(\"Toxic as function of Mean Shift\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = (train['MeanShift'] == 10).astype('int')\nprint('accuracy', accuracy_score(preds, target))\nprint('precision', precision_score(target, preds))\nprint('recall', recall_score(target, preds))\nprint('f1', f1_score(target, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Mixture","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 7\ngmix = GaussianMixture(k, random_state=2)\n%time gmix.fit(X_w2v[:10000])\ntrain['GMixture'] = gmix.predict(X_w2v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,6))\nsns.barplot(x='GMixture' , y='toxic' , data=train)\nplt.ylabel(\"Toxic\")\nplt.title(\"Toxic as function of GMixture\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = (train['GMixture'] == 1).astype('int')\nprint('accuracy', accuracy_score(preds, target))\nprint('precision', precision_score(target, preds))\nprint('recall', recall_score(target, preds))\nprint('f1', f1_score(target, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try to train models on clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_cluterizers = 4\n\ntrain_cl = train.iloc[:, -n_cluterizers:]\nfor cl in train_cl.columns:\n    print(cl, train_cl[cl].unique())\n    for i in sorted(train_cl[cl].unique()):\n        train_cl[cl + '_%i' % i] = (train_cl[cl] == i).astype(int)\ntrain_cl = train_cl.iloc[:, n_cluterizers:]\ntrain_cl.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time eval_on_trainset(train_cl.iloc[:10000, :], target[:10000])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}