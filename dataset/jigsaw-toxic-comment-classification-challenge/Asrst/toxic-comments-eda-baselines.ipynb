{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Machine Learning to Identify Toxic Comments: EDA & Baselines</h1>\n\n### Breif Overview of Problem Statement:\n\nWith too much freedom in social media, profanity in comments (i.e. comments that are disrespectful, rude or otherwise likely to make someone leave a discussion) can be a serious issue if not addressed. Moreover, with surge in number users accessing social media these days, this task cannot be done manually with help of moderators. So to automate the identification of toxic comments on social media platforms, The Conversation AI team had come up with an interesting kaggle competition to build a roboust machine learning model that can do this task.\n\nFor this competition, we are provided a annotated dataset of comments from Wikipediaâ€™s talk page edits. Using this as training data, we are challenged to build a multi-headed machine learning model that is capable of detecting different types of of toxicity like obscenity, threats, insults, and identity-based hate better than existing machine learning models (yes, conversational AI team already has a model to do this).\n\nMore details of the competition can be found here: \n> source: [kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n\n\n### Data & Evaluation metrics:\n\n`train.csv` contains a large number of Wikipedia comments (around 100K+) which have been labeled by human raters for toxic behavior. The types of toxicity are:\n\n`toxic\n severe_toxic\n obscene\n threat\n insult\n identity_hate`\n \n- Apart from the above mentioned classes there will be also completely clean comments.\n- The trained model will be tested on `test.csv` data using mean column-wise ROC AUC as evaluation metric.\n- **Mean column-wise ROC AUC = Average of the individual ROC AUC scores of each predicted class. **\n\nSubmission File Format:\n\nFor each id in the test set, you must predict a probability for each of the six possible types of comment toxicity (toxic, severetoxic, obscene, threat, insult, identityhate). The columns must be in the same order as shown below. The file should contain a header and have the following format:\n\n`id,toxic,severe_toxic,obscene,threat,insult,identity_hate\n 00001cee341fdb12,0.5,0.5,0.5,0.5,0.5,0.5\n 0000247867823ef7,0.5,0.5,0.5,0.5,0.5,0.5\n etc.`"},{"metadata":{},"cell_type":"markdown","source":"#### Objective of this Notebook: \n- Explore the data, perform analysis & build baselines.\n\n### Reading the Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\ndata_paths = {}\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        data_paths[filename] = os.path.join(dirname, filename)\n        # print(os.path.join(dirname, filename))\n        \ntrain_df = pd.read_csv(data_paths['train.csv'])\ntest_df = pd.read_csv(data_paths['test.csv'])\nsub_df = pd.read_csv(data_paths['sample_submission.csv'])\nprint('Train data shape:', train_df.shape)\nprint('Columns in Train:', train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The provided `train.csv` is having 159571 rows and 8 columns. Let us now sample to take 5 rows from the data to get visual understanding of what is present."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.sample(5, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- This is going to be text classification problem...we are given a comment and we need to classify whether it is toxic.\n- The column `comment_text` contains the comments text and it will be our main feature.\n- All the remaining columns except `id` will be our target variable to predict.\n- Target varaibles are binary encoded (one hot encoded) & all zeros represents a clean comment.\n- A single comments can belong to multiple toxicity classes\n\nNow lets look into few complete random comments from the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [34, 55345, 124786]:\n    display(train_df.loc[i, 'comment_text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, comments are largely varying in lengths. Let us see how their distrubution looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_lens = train_df['comment_text'].str.len()\nprint('Central Tendencies on lengths of comment_text\\n', comment_lens.describe())\nax = comment_lens.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above outputs we can say these points:\n\n- Comments have no null values.\n- Mininum length of a comment is only 6 chars while maximum length can be 5000 characters.\n- Mean (394 chars) and median (205 chars) are not close indicating the skewness - as a result histogram is left sided\n\nNow we will look how many of given comments are clean & how many are toxic (in other words, distrubution of classes to be predicted)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndrop_col = ['id', 'is_clean']  # columns not neccessary - can be dropped\ntext_col = ['comment_text']  # text feature\nlabel_col = [col for col in train_df.columns if col not in text_col + drop_col] # target variables\n\nlabels_per_comment = train_df[label_col].sum(axis = 1) # clac no.of labels for each comment\n\n# add a new column to indicate if a comment is toxic (bad) or not (clean).\ntrain_df['is_clean'] = 0\ntrain_df.loc[labels_per_comment == 0, 'is_clean'] = 1\n# train_df['is_clean'].value_counts()\n\nprint(\"Total Clean comments (All 0's in a target row) in train:\",len(train_df[train_df['is_clean'] == 1]))\nprint(\"Total unclean/bad comments (atleast one 1 in a target row)in train:\",len(train_df[train_df['is_clean'] != 1]))\nprint(\"Total label tags (total counts of 1's in target columns):\",train_df[label_col].sum().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importantly we need to notice that total bad comments & total label tags are not equal, so said intially there are multiple labels for many comments. On Average, there are 2 labels for each comment - but let us see - Now let us see how multiple labels are divided:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_count = labels_per_comment.value_counts()\n\n# plotting the label counts\nplt.figure(figsize=(8,4))\nax = sns.barplot(tags_count.index, tags_count.values, alpha=0.7)\nplt.title(\"Tags Counts v/s Occurences in Train Data\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('# Tag Count', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = tags_count.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 10, \n            label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- maximum no.of tags for a bad comment is 6 (i.e it will belong to all six classes).\n- This we can be viewed as highericial tagging. (Ex: A comment which is `severe_toxic` will be also a `toxic`).\n\nLets visualize how the main six classes are distributed (counts of classes) among the 35098 occurences"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_counts = train_df[label_col].sum()\n\n# plotting the label counts\nplt.figure(figsize=(8,4))\nax = sns.barplot(label_counts.index, label_counts.values, alpha=0.7)\nplt.title(\"Counts Per Class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Label', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = label_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 10, \n            label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above figure tells us that Toxicity type is not evenly spreadout - `toxic` comments are more common while `threat` is least occuring class. This is interesting & a bit scary too. In total, there is a high imbalance b/w the clean & unclean comments. Again with in unclean comments, there is imbalance b/w the types of toxicity!\n\nTill now we only analyzed the target varaibles, now we will look at one comment each from the all 6 types of toxic classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfor label in label_col:\n    label_df = train_df[train_df[label]==1].reset_index(drop = 1)\n    print('\\n' + label + ' - comment sample :')\n    print(label_df.loc[random.randint(0, len(label_df)-1), 'comment_text'])\n    print('\\n' + '-'*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Aah...that's a lot of profanity...!\n- Comments are having puntuctions & things user want to highlight in captials (or) quotes.\n- As usually, Contractions are always there. In some cases (very less), there is also text from other languages in b/w english.\n- comments include lots of chatting/social shortcuts which we generally use on social media.\n- few comments seems to contain numbers, time and also IP address (might be of user's).\n- In few cases there are also Device Id's and Urls posted by users.\n- Some comments are spam - repeating same things 10's of times.\n- In few rare cases, I found the word `unblock` before the comment - this might indicate that comment is blocked as it is toxic."},{"metadata":{},"cell_type":"markdown","source":"Now, let us start exploring into comment texts with some questions which relates to our objective. Our objective here is to classify the bad comments into different to toxicity types.\n\n- Are Longer comments more toxic ?\n- did Presence of special characters vary with Toxicity ?\n- Are spammer more toxic ?\n\n\nThese questions will answer how Toxicity of comments vary with different features of comment_text like length, presence of punctuations, capital letters, words, or sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\n#Total chars:\ntrain_df['total_len'] = train_df['comment_text'].apply(len)\ntest_df['total_len'] = test_df['comment_text'].apply(len)\n\n#Sentence count in comment: '\\n' is split & count number of sentences in each comment\ntrain_df['sent_count'] = train_df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\ntest_df['sent_count'] = test_df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n\n#Word count in each comment:\ntrain_df['word_count'] = train_df[\"comment_text\"].apply(lambda x: len(str(x).split()))\ntest_df['word_count'] = test_df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n\n\nplt.figure(figsize=(18,6))\nplt.suptitle(\"Are longer comments more toxic?\",fontsize=18)\nplt.tight_layout()\n\n# total lengths (characters)\nplt.subplot(131)\nax=sns.kdeplot(train_df[train_df.is_clean == 0].total_len, label=\"UnClean\",shade=True,color='r')\nax=sns.kdeplot(train_df[train_df.is_clean == 1].total_len, label=\"Clean\")\nplt.legend()\nplt.ylabel('Number of occurances', fontsize=12)\nplt.xlabel('# of Chars', fontsize=12)\n# plt.title(\"# Chars v/s Toxicity\", fontsize=12)\n\n# words\nplt.subplot(132)\nax=sns.kdeplot(train_df[train_df.is_clean == 0].word_count, label=\"UnClean\",shade=True,color='r')\nax=sns.kdeplot(train_df[train_df.is_clean == 1].word_count, label=\"Clean\")\nplt.legend()\nplt.xlabel('# of Words', fontsize=12)\n# plt.title(\"# Words v/s comment Toxicity\", fontsize=12)\n\n## sentences\nplt.subplot(133)\nax=sns.kdeplot(train_df[train_df.is_clean == 0].sent_count, label=\"UnClean\",shade=True,color='r')\nax=sns.kdeplot(train_df[train_df.is_clean == 1].sent_count, label=\"Clean\")\nplt.legend()\nplt.xlabel('# of Sentences', fontsize=12)\n# plt.title(\"# Sentences v/s comment Toxicity\", fontsize=12)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- most comments are having less than 25 sentences & less than 250 words\n- unclean comments are having more no.of words in less no.of sentences.\n- The distrubution plots of clean & unclean of all three plots are very much overlapping with each others, indicating these features are going to be less significant in differentiating them."},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\n#Captial letters:\ntrain_df['capitals'] = train_df['comment_text'].apply(lambda x: sum(1 for c in x if c.isupper()))\ntest_df['capitals'] = test_df['comment_text'].apply(lambda x: sum(1 for c in x if c.isupper()))\n\n# #Captials ratio:\n# train_df['capitals_percent'] = train_df['capitals']/train_df['total_len'] * 100\n# test_df['capitals_percent'] = test_df['capitals']/train_df['total_len'] * 100\n\n# punct count:\ntrain_df['punct_count'] = train_df['comment_text'].apply(lambda x: sum(1 for c in x if c in string.punctuation))\ntest_df['punct_count'] = test_df['comment_text'].apply(lambda x: sum(1 for c in x if c in string.punctuation))\n\n# smilies:\nsmilies = (':-)', ':)', ';-)', ';)')\ntrain_df['smilies_count'] = train_df['comment_text'].apply(lambda comment: sum(comment.count(s) for s in smilies))\ntest_df['smilies_count'] = test_df['comment_text'].apply(lambda comment: sum(comment.count(s) for s in smilies))\n\n#----------plotting------------\n\nplt.figure(figsize=(18,6))\nplt.suptitle(\"did Presence of special characters vary with Toxicity ?\\n\",fontsize=18)\nplt.tight_layout()\n\n# words\nplt.subplot(131)\nax=sns.kdeplot(train_df[train_df.is_clean == 0].capitals, label=\"UnClean\",shade=True,color='r')\nax=sns.kdeplot(train_df[train_df.is_clean == 1].capitals, label=\"Clean\")\nplt.legend()\nplt.ylabel('Number of occurances', fontsize=12)\nplt.xlabel('# Capital letters', fontsize=12)\n# plt.title(\"# Captials v/s Toxicity\", fontsize=12)\n\n# words\nplt.subplot(132)\nax=sns.kdeplot(train_df[train_df.is_clean == 0].punct_count, label=\"UnClean\",shade=True,color='r')\nax=sns.kdeplot(train_df[train_df.is_clean == 1].punct_count, label=\"Clean\")\nplt.legend()\nplt.xlabel('# of Punctuations', fontsize=12)\n# plt.title(\"#Punctuations v/s comment Toxicity\", fontsize=12)\n\n## sentences\nplt.subplot(133)\nax=sns.kdeplot(train_df[train_df.is_clean == 0].smilies_count, label=\"UnClean\",shade=True,color='r')\nax=sns.kdeplot(train_df[train_df.is_clean == 1].smilies_count, label=\"Clean\")\nplt.legend()\nplt.xlabel('# of Smilies', fontsize=12)\n# plt.title(\"#Smilies v/s comment Toxicity\", fontsize=12)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- presence of captial letters is more in case of unclean comments, but the distrbutions are overlapping making it a difficult feature for models to extract information.\n- most of the clean comments are having punctuations less than 100 while for unclean comments it spread to max of 5000 punctuations.\n- no.of smilies in unclean v/s clean comments is very much similar and unclean comments are having more comments with no.of smilies = 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unique word count:\ntrain_df['unique_word_count'] = train_df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\ntest_df['unique_word_count'] = test_df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n\n#Unique ratio:\ntrain_df['unique_word_percent'] = train_df['unique_word_count']/train_df['word_count'] * 100\ntest_df['unique_word_percent'] = test_df['unique_word_count']/train_df['word_count'] * 100\n\n#----------plotting------------\n\n# comments with unique word count percentage < 25%...they can be spam/referal links/marketing links\n\nplt.figure(figsize=(15,5))\nplt.suptitle(\"Comments with less-unique-words(spam) are more toxic?\",fontsize = 18)\n\nplt.subplot(121)\nplt.title(\"% of unique words in comments\")\nax=sns.kdeplot(train_df[train_df.is_clean == 0].unique_word_percent, label=\"UnClean\",shade=True,color='r')\nax=sns.kdeplot(train_df[train_df.is_clean == 1].unique_word_percent, label=\"Clean\")\nplt.legend()\nplt.ylabel('Number of occurances', fontsize=12)\nplt.xlabel('Percent unique words', fontsize=12)\n\nplt.subplot(122)\nsns.violinplot(y = 'unique_word_count',x='is_clean', data = train_df[train_df['unique_word_percent'] < 25], \n               split=True,inner=\"quart\")\nplt.xlabel('is_Clean', fontsize=12)\nplt.ylabel('# of words', fontsize=12)\nplt.title(\"# unique words v/s Toxicity\")\nplt.show()\n\n# train_df[train_df['word_unique_percent'] < 25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There is a wide spread area for unclean points in the unique word percentage range of 1-10%, Interesting there are clean comments as well with lesser number of unique words. \n- This feature seems carry some significance especially incase of sentences with less unique words.\n- lets once see how text in clean-spam & unclean-spam comments look like"},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets have a look how clean & unclean spam comment looks like\n\nprint(\"Clean Spam example:\")\nprint(train_df[train_df['unique_word_percent'] < 10][train_df['is_clean'] == 1].comment_text.iloc[3])\nprint('-'*50)\nprint(\"Toxic Spam example:\")\nprint(train_df[train_df['unique_word_percent'] < 10][train_df['is_clean'] == 0].comment_text.iloc[25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we definitely need to penalize the over repeating words (or) there will be chance for models to learn as words like 'George Bush' as indicator as toxicity. so comments with lesser unique words should be preprocessed carefully.\n\n- Tfidf is well known for this penalizing effect.\n-  TF-IDF Vectorizer\n      - TF - Term Frequency -- Count of the words(Terms) in the text corpus (same of Count Vect)\n      - IDF - Inverse Document Frequency -- Penalizes words that are too frequent. We can think of this as regularization"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_df.to_csv('train_feateng.csv', index = None)\ntest_df.to_csv('test_feateng.csv', index = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# from nltk.corpus import stopwords\n# from nltk import pos_tag\n# from nltk.stem.wordnet import WordNetLemmatizer \n# from nltk.tokenize import word_tokenize # Tweet tokenizer does not split at apostophes which is what we want\n# from nltk.tokenize import TweetTokenizer\n\n\n# lemma = WordNetLemmatizer()\n# tokenizer=TweetTokenizer()\n# eng_stopwords = list(stopwords.words('english'))\n\n# def simple_preprocess(comment):\n#     \"\"\"\n#     This function receives comments and returns clean word-list\n#     \"\"\"\n#     #Convert to lower case \n#     comment=comment.lower()\n#     #remove \\n\n#     comment=re.sub(\"\\\\n\",\"\",comment)\n\n#     #Split the sentences into words and lemmatize\n#     words=tokenizer.tokenize(comment)\n#     words=[lemma.lemmatize(word, \"v\") for word in words]\n#     words = [w for w in words if not w in eng_stopwords]\n    \n#     clean_sent=\" \".join(words)\n    \n#     return(clean_sent)\n\n# train_df['comment_text'] = train_df['comment_text'].apply(lambda x: simple_preprocess(x))\n# test_df['comment_text'] = test_df['comment_text'].apply(lambda x: simple_preprocess(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_topn_tfidf_feat_byClass(X_tfidf, y_train, feature_names, labels, topn):\n    \n    feat_imp_dfs = {}\n    \n    for label in labels:\n        # get indices of rows where label is true\n        label_ids = y_train.index[y_train[label] == 1]\n        # get subset of rows\n        label_rows = X_tfidf[label_ids].toarray()\n        # calc mean feature importance\n        feat_imp = label_rows.mean(axis = 0)\n        # sort by column dimension and get topn feature indices\n        topn_ids = np.argsort(feat_imp)[::-1][:topn]\n        # combine tfidf value with feature name\n        topn_features = [(feature_names[i], feat_imp[i]) for i in topn_ids]\n        # df\n        topn_df = pd.DataFrame(topn_features, columns = ['word_feature', 'tfidf_value'])\n        # save \n        feat_imp_dfs[label] = topn_df\n    return feat_imp_dfs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transforming Text Data into TD-IDF vectors & Analysing TF-IDF values \n\n#### 1. Visualising Bigram features per Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize # Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer\n\n\ntfidf = TfidfVectorizer(ngram_range = (1,1), min_df = 100, \n                        strip_accents='unicode', analyzer='word',\n                        use_idf=1,smooth_idf=1,sublinear_tf=1,\n                        stop_words = 'english')\nX_unigrams = tfidf.fit_transform(train_df['comment_text'])\nX_unigrams.shape, len(tfidf.get_feature_names())\n\n\nfeature_names = np.array(tfidf.get_feature_names())\nimp_dfs = get_topn_tfidf_feat_byClass(X_unigrams, train_df, feature_names, label_col, topn = 10)\n\nplt.figure(figsize=(15,10))\n\nfor i, label in enumerate(label_col):\n    plt.subplot(3, 2, i + 1)\n    sns.barplot(imp_dfs[label].word_feature[:10], imp_dfs[label].tfidf_value[:10], alpha = 0.8)\n    plt.title(\"Important UniGrams for the class:{}\".format(label))\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- few words like `fuck` seems to be in every class, but again as this is multi-label classification (multiple tags for each comment) there will be that overlapping.\n- especially `threat` class is standing apart with words like `kill`, `die`, `death`.\n- Interestingly, due to high tf-idf value, word `wikipedia` has stands in top10 features for the `toxic` class. which model should not learn."},{"metadata":{},"cell_type":"markdown","source":"#### 2. Visualising Bigram features per Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(ngram_range = (2,2), min_df = 100, \n                        strip_accents='unicode', analyzer='word',\n                        use_idf=1,smooth_idf=1,sublinear_tf=1,\n                        stop_words = 'english')\nX_bigrams = tfidf.fit_transform(train_df['comment_text'])\nX_bigrams.shape, len(tfidf.get_feature_names())\n\nfeature_names = np.array(tfidf.get_feature_names())\nimp_dfs = get_topn_tfidf_feat_byClass(X_bigrams, train_df, feature_names, label_col, topn = 10)\n\nplt.figure(figsize=(15,12))\n\nfor i, label in enumerate(label_col):\n    plt.subplot(3, 2, i + 1)\n    by_class = sns.barplot(imp_dfs[label].word_feature[:10], imp_dfs[label].tfidf_value[:10], alpha = 0.8)\n    plt.title(\"Important BiGrams for the class:{}\".format(label))\n    for item in by_class.get_xticklabels():\n        item.set_rotation(45)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- the top 10 tfidf features in case are not much differentiating, every class are having almost same phrases.\n- remember, the `comment_text` is completely unprocessed for now.\n- There is high chance for Names of persons like famous names `george bush`, `mitt romney` (if mentioned repeatedly in the spam) to appear in the bigram features for any class. To avoid this we can preprocess spam comments (comments with < 25% unique words) differently like removing usernames/person names from the comments using regular expressions or POS tags."},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into Training & Validation sets:\n\nBefore testing the model on the actual test data, we neeed to cross-validate internally that model is trained in a right way. Validation set will also help to tune model properly. I am using sklearn's `train_test_split` utility function to do this random splitting. I am using `random_state = 2019`as seed for purpose of reproducability & `test_size` = 0.2 meaning 20% of data points in the train set will be used as validation data and remaining 80% of data will be used to train the model.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_df['comment_text'], \n                                                  train_df[label_col], test_size=0.2, random_state=2019)\nX_test = test_df['comment_text']\nprint('Data points in train data after splitting:', len(X_train))\nprint('Data points in valiadtion data:', len(X_val))\nprint('Data points in test data:', len(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Baselines:\n\nBefore getting to actual modelling & algorithms. let me build some naive baseline on validation set to get minimal optimal score. These scores will be useful for analysing & selection of the models we will be building.\n\n#### naive baseline 1: Random guessing probabilities b/w 0 & 1.\n\nThis time instead having common probility...we take random value for probability for every column in row. What good is a machine learing if its results are not even better than a random prediction ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss, roc_auc_score\ny_val_naive1 = np.random.rand(y_val.shape[0], y_val.shape[1])\nprint('Naive Baseline:', 'Random Guessing')\nprint('ROC-AUC score :', roc_auc_score(y_val, y_val_naive1))\nprint('Log Loss:', log_loss(y_val, y_val_naive1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Naive baseline 2: Predicting all probabilities as 0.5\n- Here we are guessing more intelligently...as every comments in validation set as clean. From data analysi we found that train data is highly imbalanced there is high chance that validation set will be also have same distrbutions and predicting every comment as clean will actually give us 90% accuracy...this is also reason why accuracy is not an evalution metric for this problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val_naive2 = np.zeros(y_val.shape)\ny_val_naive2[:] = 0.5\nprint('Naive Baseline:', 'Random Guessing')\nprint('ROC-AUC score :', roc_auc_score(y_val, y_val_naive2))\nprint('Log Loss:', log_loss(y_val, y_val_naive2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- So the machine model we are going to build should atleast have validation log loss < 0.40 & roc-auc > 0.5. which seems not that tough to achieve.\n- Before starting the modelling,I am transforming the train, valid & test data splits into tfidf features with both unigrams & bigrams."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_df['comment_text'], \n                                                  train_df[label_col], test_size=0.2, random_state=2019)\nX_test = test_df['comment_text']\n\ntfidf = TfidfVectorizer(ngram_range = (1,2), min_df = 9, strip_accents='unicode', analyzer='word',\n                        use_idf=1, smooth_idf=1, sublinear_tf=1,stop_words = 'english')\nX_train_tf = tfidf.fit_transform(X_train)\nX_val_tf = tfidf.transform(X_val)\nX_test_tf = tfidf.transform(X_test)\nfeature_names = tfidf.get_feature_names()\n\nprint('Final Data dimensions after transformations:', X_train_tf.shape, y_train.shape, X_val_tf.shape, y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Modelling: Baselines\n\nGenerally, I will begin with least complex models & will move to complex models based on their time/space complexity.\n\n#### 1. Naive Bayes\n- Naive Bayes is well know for its Text Sentiment Classification tasks. It is also simple to interpret the model so I am starting with it.\n- If works well, added advantages includes speed - as worst case complexity is O(Nd) is comparatively low.\n      - N = No.of data points\n      - d = dimensions (features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = OneVsRestClassifier(MultinomialNB(), n_jobs = -1)\nmodel.fit(X_train_tf, y_train)\nprint('model: Naive Bayes')\nprint('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))\ny_pred_nb = model.predict_proba(X_val_tf)\nprint('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- so what is OneVsRestClassifier doing here? \n\n- This is basically wrapper that converts the problem of 6 class multi-label problem into 6 individual binary classifications .i.e it builds 6 multinomialNB models. Let me write this implement what I said & verify my understanding."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MultinomialNB()\n\ntrain_rocs = []\nvalid_rocs = []\n\npreds_train = np.zeros(y_train.shape)\npreds_valid = np.zeros(y_val.shape)\npreds_test = np.zeros((len(test_df), len(label_col)))\nprint('model: Naive Bayes')\nfor i, label_name in enumerate(label_col):\n    print('\\nClass:= '+label_name)\n    # fit\n    model.fit(X_train_tf,y_train[label_name])\n    \n    # train\n    preds_train[:,i] = model.predict_proba(X_train_tf)[:,1]\n    train_roc_class = roc_auc_score(y_train[label_name],preds_train[:,i])\n    print('Train ROC AUC:', train_roc_class)\n    train_rocs.append(train_roc_class)\n\n    # valid\n    preds_valid[:,i] = model.predict_proba(X_val_tf)[:,1]\n    valid_roc_class = roc_auc_score(y_val[label_name],preds_valid[:,i])\n    print('Valid ROC AUC:', valid_roc_class)\n    valid_rocs.append(valid_roc_class)\n    \n    # test predictions\n    preds_test[:,i] = model.predict_proba(X_test_tf)[:,1]\n    \nprint('\\nmean column-wise ROC AUC on Train data: ', np.mean(train_rocs))\nprint('mean column-wise ROC AUC on Val data:', np.mean(valid_rocs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- And the results of both implementations are excatly same..\n- Naive Bayes performed pretty well in its zone with 0.9047... \n- This looks like a good score only until we ecperiment with other models...as they might score a way higher lets see...how linear models like Logistic regression & linear SVM perform with default parameters\n- From here on I will be using the onevsrest classifier."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### 2. Logisitc Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = OneVsRestClassifier(LogisticRegression(), n_jobs = -1)\nmodel.fit(X_train_tf, y_train)\nprint('model: Logistic Regression')\nprint('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))\ny_pred_log = model.predict_proba(X_val_tf)\nprint('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_log))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Linear SVM\n\n- Linear SVM will not support probability prediction as hinge loss is unstable, the way around is using a calibration classifer wrapper and it wont support OnevsRest classifers. so we I am using manual python loop again.\n\n- Notice that I fitted a new `CalibratedClassifierCV` wrapper afer fitting the `LinearSVC` classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\n\nmodel = LinearSVC()\n\ntrain_rocs = []\nvalid_rocs = []\n\npreds_train = np.zeros(y_train.shape)\npreds_valid = np.zeros(y_val.shape)\npreds_test = np.zeros((len(test_df), len(label_col)))\nprint('model: Linear SVM')\nfor i, label_name in enumerate(label_col):\n    print('\\nClass:= '+label_name)\n    \n    # fit\n    model.fit(X_train_tf,y_train[label_name])\n    \n    # calibration classifier fit\n    model = CalibratedClassifierCV(model, cv = 'prefit')\n    model.fit(X_train_tf, y_train[label_name])\n    \n    # train\n    preds_train[:,i] = model.predict_proba(X_train_tf)[:,1]\n    train_roc_class = roc_auc_score(y_train[label_name],preds_train[:,i])\n    print('Train ROC AUC:', train_roc_class)\n    train_rocs.append(train_roc_class)\n\n    # valid\n    preds_valid[:,i] = model.predict_proba(X_val_tf)[:,1]\n    valid_roc_class = roc_auc_score(y_val[label_name],preds_valid[:,i])\n    print('Valid ROC AUC:', valid_roc_class)\n    valid_rocs.append(valid_roc_class)\n    \n    # test predictions\n    preds_test[:,i] = model.predict_proba(X_test_tf)[:,1]\n    \nprint('\\nmean column-wise ROC AUC on Train data: ', np.mean(train_rocs))\nprint('mean column-wise ROC AUC on Val data:', np.mean(valid_rocs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Performance of Linear Models:\n\n- Both models Logistic & SVM are performing better than naive bayes models.\n- Although Logistic is better with default parameters, I need to experiment & see if fine tuning helps Linear SVM.\n- Moreover, these models are interpretable & not very complex in terms of time & space required.\n- Further with prerocessing & fine tuning of models I am hoping to reach ROC-AUC of 0.98.\n\n\nNow I will be moving to baselines of non-linear decision tree models namely RandomForests, XGB and Lightgbm.\n#### 4. RandomForests:\n\nwhy not single decision tree: \n\n- In my practice I found RandomForest always outperforms decision trees...in other words where ever decision trees work well..certainly randomforests will also do as its base learner is a decision tree.\n\n- Randomforests also outperforms as it averages the decision from 100's of decision tree models. Moreover, decision tree easily tends to overfit relying only on a single decision tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = OneVsRestClassifier(RandomForestClassifier(), n_jobs = -1)\nprint('model: Random Forest')\nmodel.fit(X_train_tf, y_train)\nprint('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))\ny_pred_rf = model.predict_proba(X_val_tf)\nprint('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5. LightGBM\n\n- optimized version and generally takes lesser time than XGBoost."},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nmodel = OneVsRestClassifier(LGBMClassifier(), n_jobs = -1)\nprint('model: Lightgbm')\nmodel.fit(X_train_tf, y_train)\nprint('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))\ny_pred_log = model.predict_proba(X_val_tf)\nprint('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_log))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nmodel = OneVsRestClassifier(XGBClassifier(), n_jobs = -1)\nprint('model: XGBoost')\nmodel.fit(X_train_tf, y_train)\nprint('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))\ny_pred_lgb = model.predict_proba(X_val_tf)\nprint('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_lgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Random Forest are serverly overfitting with train ROC-AUC at 0.99 and validation at 0.87\n- All these non-linear models might be failing case of classes like 'threat' with only around 500 samples for this class which could be a reasoning - bagging & boosting requires more data ?\n- Non-linear models in this case are overfitting a bit, when compared to linear models.\n- Guessing, Non linear models can be improved by reducing features using selecting only Top K best or by limiting max_depth, increasing num_trees and tuning other hyperparameters . But ensmebles are also more time complex so sticking with simple linear models is better in this case."},{"metadata":{},"cell_type":"markdown","source":"### Conclusions:\n- Linear models are very well suited for this problem\n- Logistic Regression is having better baseline score than any other model\n- Logistic also has an edge with lower time/space complexity compared to ensembles and also, we can impove this further by text preprocessing & hyper parameter tuning.\n- So rather than trying complex models, I will settle with linear models & will try to improve the performance using preprocessing, finetuning & feature engineering. My next target is to improve to score to more than 0.98 using only linear models.\n\n### how about using deep neural nets, CNNs & LSTMs ?\n- with amount of data we are having it is possible for deep neural nets might provide better results.\n- But Let say after preprocessig & tuning our final Logistic model is having ROC-AUC of 0.98 on average & for some LSTM model is having ROC-AUC of 0.99..I am question myself, is it worth training & deploying such complex model which we can't interpret for an improvement of just 0.01 score ? If would **NO** but in the kaggle competitions, every improvement in 0.0001 score matters...so may be we can try LSTMs after all to improve our knowledge..but in production I would prefer to say NO Again it ultimately depends on serverity of the problem & several other factors."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_val = train_df['comment_text']\ny_train_val = train_df[label_col]\n\nX_train_val = tfidf.fit_transform(train_df['comment_text'])\nX_test = tfidf.transform(test_df['comment_text'])\n\n\nmodel = OneVsRestClassifier(LogisticRegression(), n_jobs = -1)\nmodel.fit(X_train_val, y_train_val)\nprint('model: Logistic Regression')\nprint('mean ROC-AUC on train set:', roc_auc_score(y_train_val, model.predict_proba(X_train_val)))\ny_test_pred = model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## making a submission file\nsub_df.iloc[:,1:] = y_test_pred\nsub_df.head()\nfrom IPython.display import FileLink\nsub_df.to_csv('submission.csv', index = None)\nFileLink('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# sorted_features = sorted(zip(model.coef_.ravel(), feature_names))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}