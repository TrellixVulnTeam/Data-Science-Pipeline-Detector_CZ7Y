{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://deeplearningcourses.com/c/deep-learning-advanced-nlp\nfrom __future__ import print_function, division\nfrom builtins import range\n# Note: you may need to update your version of future\n# sudo pip install -U future\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom sklearn.metrics import roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-26T13:32:14.828234Z","iopub.execute_input":"2022-01-26T13:32:14.828631Z","iopub.status.idle":"2022-01-26T13:32:21.137105Z","shell.execute_reply.started":"2022-01-26T13:32:14.828529Z","shell.execute_reply":"2022-01-26T13:32:21.13617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some configuration\nMAX_SEQUENCE_LENGTH = 100\nMAX_VOCAB_SIZE = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\nBATCH_SIZE = 128\nEPOCHS = 10","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:32:21.138785Z","iopub.execute_input":"2022-01-26T13:32:21.139048Z","iopub.status.idle":"2022-01-26T13:32:21.143244Z","shell.execute_reply.started":"2022-01-26T13:32:21.139011Z","shell.execute_reply":"2022-01-26T13:32:21.142528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load in pre-trained word vectors\nprint('Loading word vectors...')\nword2vec = {}\nwith open(os.path.join('../input/glove-global-vectors-for-word-representation/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n    \n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:36:45.643907Z","iopub.execute_input":"2022-01-26T13:36:45.644327Z","iopub.status.idle":"2022-01-26T13:36:56.79305Z","shell.execute_reply.started":"2022-01-26T13:36:45.644296Z","shell.execute_reply":"2022-01-26T13:36:56.792174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare text samples and their labels\nprint('Loading in comments...')\n\ntrain = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\nsentences = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\npossible_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntargets = train[possible_labels].values\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:37:34.653735Z","iopub.execute_input":"2022-01-26T13:37:34.654696Z","iopub.status.idle":"2022-01-26T13:37:37.463197Z","shell.execute_reply.started":"2022-01-26T13:37:34.654641Z","shell.execute_reply":"2022-01-26T13:37:37.462314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:37:37.464893Z","iopub.execute_input":"2022-01-26T13:37:37.465434Z","iopub.status.idle":"2022-01-26T13:37:37.48299Z","shell.execute_reply.started":"2022-01-26T13:37:37.465388Z","shell.execute_reply":"2022-01-26T13:37:37.482162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the sentences (strings) into integers\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\n# print(\"sequences:\", sequences); exit()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:37:37.484128Z","iopub.execute_input":"2022-01-26T13:37:37.484355Z","iopub.status.idle":"2022-01-26T13:37:55.9419Z","shell.execute_reply.started":"2022-01-26T13:37:37.48433Z","shell.execute_reply":"2022-01-26T13:37:55.941315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"max sequence length:\", max(len(s) for s in sequences))\nprint(\"min sequence length:\", min(len(s) for s in sequences))\ns = sorted(len(s) for s in sequences)\nprint(\"median sequence length:\", s[len(s) // 2])\n\nprint(\"max word index:\", max(max(seq) for seq in sequences if len(seq) > 0))","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:37:55.943223Z","iopub.execute_input":"2022-01-26T13:37:55.943592Z","iopub.status.idle":"2022-01-26T13:37:56.255974Z","shell.execute_reply.started":"2022-01-26T13:37:55.943552Z","shell.execute_reply":"2022-01-26T13:37:56.255028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:38:09.969938Z","iopub.execute_input":"2022-01-26T13:38:09.970539Z","iopub.status.idle":"2022-01-26T13:38:09.975691Z","shell.execute_reply.started":"2022-01-26T13:38:09.970498Z","shell.execute_reply":"2022-01-26T13:38:09.974589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:38:15.489216Z","iopub.execute_input":"2022-01-26T13:38:15.489886Z","iopub.status.idle":"2022-01-26T13:38:16.634138Z","shell.execute_reply.started":"2022-01-26T13:38:15.489843Z","shell.execute_reply":"2022-01-26T13:38:16.633623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install gensim","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:29:31.194795Z","iopub.execute_input":"2022-01-26T13:29:31.195117Z","iopub.status.idle":"2022-01-26T13:29:44.113227Z","shell.execute_reply.started":"2022-01-26T13:29:31.195085Z","shell.execute_reply":"2022-01-26T13:29:44.112047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx.items():\n    if i < MAX_VOCAB_SIZE:\n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n          embedding_matrix[i] = embedding_vector\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:38:20.487984Z","iopub.execute_input":"2022-01-26T13:38:20.489005Z","iopub.status.idle":"2022-01-26T13:38:20.567024Z","shell.execute_reply.started":"2022-01-26T13:38:20.48896Z","shell.execute_reply":"2022-01-26T13:38:20.566151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  num_words,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:39:08.750898Z","iopub.execute_input":"2022-01-26T13:39:08.751346Z","iopub.status.idle":"2022-01-26T13:39:08.782583Z","shell.execute_reply.started":"2022-01-26T13:39:08.751302Z","shell.execute_reply":"2022-01-26T13:39:08.781879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Building model...')\n\n# train a 1D convnet with global maxpooling\ninput_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\nx = embedding_layer(input_)\nx = Conv1D(128, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128, 3, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation='relu')(x)\noutput = Dense(len(possible_labels), activation='sigmoid')(x)\n\nmodel = Model(input_, output)\nmodel.compile(\n  loss='binary_crossentropy',\n  optimizer='rmsprop',\n  metrics=['accuracy']\n)\n\nprint('Training model...')\nr = model.fit(\n  data,\n  targets,\n  batch_size=BATCH_SIZE,\n  epochs=EPOCHS,\n  validation_split=VALIDATION_SPLIT\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:39:26.955911Z","iopub.execute_input":"2022-01-26T13:39:26.957024Z","iopub.status.idle":"2022-01-26T13:48:17.959019Z","shell.execute_reply.started":"2022-01-26T13:39:26.956969Z","shell.execute_reply":"2022-01-26T13:48:17.958336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot some data\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:48:17.960468Z","iopub.execute_input":"2022-01-26T13:48:17.961171Z","iopub.status.idle":"2022-01-26T13:48:18.482709Z","shell.execute_reply.started":"2022-01-26T13:48:17.96113Z","shell.execute_reply":"2022-01-26T13:48:18.481786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracies\nplt.plot(r.history['accuracy'], label='acc')\nplt.plot(r.history['val_accuracy'], label='val_acc')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:48:18.48369Z","iopub.execute_input":"2022-01-26T13:48:18.483882Z","iopub.status.idle":"2022-01-26T13:48:18.673239Z","shell.execute_reply.started":"2022-01-26T13:48:18.483859Z","shell.execute_reply":"2022-01-26T13:48:18.672678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the mean AUC over each label\np = model.predict(data)\naucs = []\nfor j in range(6):\n    auc = roc_auc_score(targets[:,j], p[:,j])\n    aucs.append(auc)\nprint(np.mean(aucs))","metadata":{"execution":{"iopub.status.busy":"2022-01-26T13:48:18.674537Z","iopub.execute_input":"2022-01-26T13:48:18.675034Z","iopub.status.idle":"2022-01-26T13:48:46.280774Z","shell.execute_reply.started":"2022-01-26T13:48:18.675005Z","shell.execute_reply":"2022-01-26T13:48:46.279307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}