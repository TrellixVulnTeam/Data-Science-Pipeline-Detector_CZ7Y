{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-02T17:24:17.647889Z","iopub.execute_input":"2021-11-02T17:24:17.648559Z","iopub.status.idle":"2021-11-02T17:24:17.672092Z","shell.execute_reply.started":"2021-11-02T17:24:17.648385Z","shell.execute_reply":"2021-11-02T17:24:17.670664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Exploration","metadata":{}},{"cell_type":"code","source":"import zipfile\nzip_ref = zipfile.ZipFile('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip', 'r')\nzip_ref.extractall('/kaggle/temp')\nzip_ref.close()\n\nos.listdir('/kaggle/temp/')","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:24:17.674821Z","iopub.execute_input":"2021-11-02T17:24:17.675619Z","iopub.status.idle":"2021-11-02T17:24:18.401667Z","shell.execute_reply.started":"2021-11-02T17:24:17.675556Z","shell.execute_reply":"2021-11-02T17:24:18.400327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/temp/train.csv')\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:24:18.403349Z","iopub.execute_input":"2021-11-02T17:24:18.40374Z","iopub.status.idle":"2021-11-02T17:24:19.408166Z","shell.execute_reply.started":"2021-11-02T17:24:18.403687Z","shell.execute_reply":"2021-11-02T17:24:19.406688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:24:19.409962Z","iopub.execute_input":"2021-11-02T17:24:19.410473Z","iopub.status.idle":"2021-11-02T17:24:19.42715Z","shell.execute_reply.started":"2021-11-02T17:24:19.410396Z","shell.execute_reply":"2021-11-02T17:24:19.425987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef summarize_data(corpus):\n    \"\"\"\n    print statements and visualizations to summarize the corpus\n    \"\"\"\n    \n    # get the documents size\n    df_doc_size = pd.Series([len(str(doc).split(\" \")) for doc in corpus])\n    \n    # get the tokens in the corpus\n    df_tokens = pd.Series([token for doc in corpus for token in str(doc).split(\" \")])\n    \n    print(\"---------------------------\")\n    print(\"num docs\", len(corpus))\n    print(\"median tokens\", df_doc_size.median())\n    print(\"num tokens\", len(df_tokens))\n    print(\"unique tokens\", len(df_tokens.value_counts()))\n    print(\"---------------------------\")\n    \n    # make plots\n    fig = plt.figure(figsize=(14,6))\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    \n    df_doc_size.plot.hist(ax=ax1, title='Document Sizes')\n    df_tokens.value_counts().plot.hist(ax=ax2, title='Tokens Counts')\n    \nsummarize_data(df.comment_text.values.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:24:19.430839Z","iopub.execute_input":"2021-11-02T17:24:19.431642Z","iopub.status.idle":"2021-11-02T17:24:29.372811Z","shell.execute_reply.started":"2021-11-02T17:24:19.431597Z","shell.execute_reply":"2021-11-02T17:24:29.371491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore the Target","metadata":{}},{"cell_type":"code","source":"df[df.drop(['id','comment_text'], axis=1).sum(axis=1)>1]","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:24:29.374456Z","iopub.execute_input":"2021-11-02T17:24:29.376218Z","iopub.status.idle":"2021-11-02T17:24:29.405863Z","shell.execute_reply.started":"2021-11-02T17:24:29.376145Z","shell.execute_reply":"2021-11-02T17:24:29.404632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\ndf.drop(['id','comment_text'], axis=1).sum().sort_values(ascending=False).plot.bar(title='Classes Counts')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:24:29.407414Z","iopub.execute_input":"2021-11-02T17:24:29.407803Z","iopub.status.idle":"2021-11-02T17:24:29.650465Z","shell.execute_reply.started":"2021-11-02T17:24:29.407761Z","shell.execute_reply":"2021-11-02T17:24:29.649533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_set, valid_set = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(train_set.shape)\nprint(valid_set.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:24:29.653024Z","iopub.execute_input":"2021-11-02T17:24:29.653872Z","iopub.status.idle":"2021-11-02T17:24:29.742473Z","shell.execute_reply.started":"2021-11-02T17:24:29.653827Z","shell.execute_reply":"2021-11-02T17:24:29.741476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(14,6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\ntrain_set.drop(['id','comment_text'], axis=1).sum().sort_values(ascending=False).plot.bar(title='Classes Counts | Train', ax=ax1)\nvalid_set.drop(['id','comment_text'], axis=1).sum().sort_values(ascending=False).plot.bar(title='Classes Counts | Valid', ax=ax2)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:24:29.744287Z","iopub.execute_input":"2021-11-02T17:24:29.7448Z","iopub.status.idle":"2021-11-02T17:24:30.15601Z","shell.execute_reply.started":"2021-11-02T17:24:29.744757Z","shell.execute_reply":"2021-11-02T17:24:30.154889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = \"/kaggle/temp/train_set.csv\"\nvalid_path = \"/kaggle/temp/valid_set.csv\" \n\ntrain_set.to_csv(train_path, index=False)\nvalid_set.to_csv(valid_path, index=False)\n\nos.listdir('/kaggle/temp/')","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:24:30.157958Z","iopub.execute_input":"2021-11-02T17:24:30.15856Z","iopub.status.idle":"2021-11-02T17:24:33.043186Z","shell.execute_reply.started":"2021-11-02T17:24:30.158515Z","shell.execute_reply":"2021-11-02T17:24:33.042055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# keras Text preprocessing with TextVectorization","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nvocab_size = 100000\nsequence_length = 150\n\ntrain_sentences = train_set.comment_text.values.tolist()\nvalid_sentences = valid_set.comment_text.values.tolist()\n\nvectorize_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\nvectorize_layer.adapt(train_sentences)\n\nvectorizer = tf.keras.models.Sequential()\nvectorizer.add(tf.keras.Input(shape=(1,), dtype=tf.string))\nvectorizer.add(vectorize_layer)\n\ntrain_sequences = vectorizer.predict(train_sentences)\nvalid_sequences = vectorizer.predict(valid_sentences)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:24:33.044834Z","iopub.execute_input":"2021-11-02T17:24:33.04552Z","iopub.status.idle":"2021-11-02T17:25:19.142842Z","shell.execute_reply.started":"2021-11-02T17:24:33.045459Z","shell.execute_reply":"2021-11-02T17:25:19.141401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_sentences[:3])\nprint(train_sequences[:3])","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:25:19.144835Z","iopub.execute_input":"2021-11-02T17:25:19.145317Z","iopub.status.idle":"2021-11-02T17:25:19.155941Z","shell.execute_reply.started":"2021-11-02T17:25:19.145246Z","shell.execute_reply":"2021-11-02T17:25:19.154603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(vectorize_layer.get_vocabulary()))\nprint(vectorize_layer.get_vocabulary()[:10])","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:25:19.157931Z","iopub.execute_input":"2021-11-02T17:25:19.158658Z","iopub.status.idle":"2021-11-02T17:25:24.827805Z","shell.execute_reply.started":"2021-11-02T17:25:19.158614Z","shell.execute_reply":"2021-11-02T17:25:24.826695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Classification model","metadata":{}},{"cell_type":"code","source":"labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n# input layer\ninputs = tf.keras.layers.Input(shape=(sequence_length,))\n\n# embeddings\nembed = tf.keras.layers.Embedding(vocab_size, 100, input_length=sequence_length, mask_zero=True)(inputs)\n\n# lstm layers\nz = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embed)\nz = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(z)\n\n# output block\nclass OutputBlock(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(32, activation='relu')\n        self.dropout = tf.keras.layers.Dropout(0.5)\n        self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n        \n    def call(self, inputs):\n        Z = inputs\n        Z = self.dense(Z)\n        Z = self.dropout(Z)\n        return self.out(Z)\n    \noutput_blocks = [OutputBlock(name=label) for label in labels]\noutputs = []\nfor block in output_blocks:\n    outputs.append(block(z))\n\nmodel = tf.keras.models.Model(inputs=[inputs], outputs=outputs)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:25:24.832844Z","iopub.execute_input":"2021-11-02T17:25:24.833109Z","iopub.status.idle":"2021-11-02T17:25:29.866365Z","shell.execute_reply.started":"2021-11-02T17:25:24.83308Z","shell.execute_reply":"2021-11-02T17:25:29.865388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = [train_set[label].values for label in labels]\ny_valid = [valid_set[label].values for label in labels]","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:25:29.868264Z","iopub.execute_input":"2021-11-02T17:25:29.868626Z","iopub.status.idle":"2021-11-02T17:25:29.877503Z","shell.execute_reply.started":"2021-11-02T17:25:29.868584Z","shell.execute_reply":"2021-11-02T17:25:29.87611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = tf.keras.backend\nK.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel.compile(loss={label:\"binary_crossentropy\" for label in labels},\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics={label:tf.keras.metrics.AUC(name='auc') for label in labels})\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5)\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"nlp.h5\", save_best_only=True)\n\nhistory = model.fit(train_sequences, y_train, epochs=30, validation_data=(valid_sequences, y_valid), batch_size=128,\n                    callbacks=[early_stopping_cb, checkpoint_cb])","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:25:29.881433Z","iopub.execute_input":"2021-11-02T17:25:29.881844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss=history.history['loss']\nval_loss=history.history['val_loss']\nepochs=range(len(loss)) # Get number of epochs\n\n# Plot training and validation loss per epoch\nplt.plot(epochs, loss, 'r', label=\"Training Loss\")\nplt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Predictions","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.load_model('nlp.h5', custom_objects={'OutputBlock':OutputBlock}) # rollback to the best model\nmodel.evaluate(valid_sequences, y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_ref = zipfile.ZipFile(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\", 'r')\nzip_ref.extractall('/kaggle/temp')\n\nzip_ref = zipfile.ZipFile(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\", 'r')\nzip_ref.extractall('/kaggle/temp')\n\nzip_ref = zipfile.ZipFile(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\", 'r')\nzip_ref.extractall('/kaggle/temp')\n\nzip_ref.close()\n\nos.listdir('/kaggle/temp')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('/kaggle/temp/sample_submission.csv').head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = pd.read_csv('/kaggle/temp/test.csv')\ntest_sentences = test_set.comment_text.values.tolist()\ntest_sequences = vectorizer.predict(test_sentences)\npredictions = model.predict(test_sequences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for label, y_pred in zip(labels, predictions):\n    test_set[label] = y_pred\n    \ntest_set.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set.drop(['comment_text'], axis=1).to_csv('submission.csv', index=False)\npd.read_csv('submission.csv').head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}