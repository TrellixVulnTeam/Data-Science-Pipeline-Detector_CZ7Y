{"cells":[{"metadata":{"trusted":true,"_uuid":"c7e4909894bfaf9e2201375704d69ce78d31eab1"},"cell_type":"code","source":"#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n\n#misc\nimport gc\nimport time\nimport warnings\n\n#stats\nfrom scipy.misc import imread\nfrom scipy import sparse\nimport scipy.stats as ss\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \n\n\n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom scipy import sparse\n\n\n\n\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3a92dd8bf300d7a7badc8ebc181899d40a97340"},"cell_type":"code","source":"train=pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest=pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\nsubm = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48a223c7b29cdb0fea255b7eb065222795814196"},"cell_type":"code","source":"#take a peak\ntrain.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c439a1ef93b36e0a98f8a1f0bad1c481b77c72f9"},"cell_type":"code","source":"nrow_train=train.shape[0]\nnrow_test=test.shape[0]\nsum=nrow_train+nrow_test\nprint(\"       : train : test\")\nprint(\"rows   :\",nrow_train,\":\",nrow_test)\nprint(\"perc   :\",round(nrow_train*100/sum),\"   :\",round(nrow_test*100/sum))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24cb611ed1fd1d9f66fd05ac0589bfe52892e3db"},"cell_type":"markdown","source":"There is a 30:70 train: test split and the test set might change in the future too.\n\nLet's take a look at the class imbalance in the train set."},{"metadata":{"_uuid":"8f9429a8b9356dd102deeeb7e603cb23703ea9ce"},"cell_type":"markdown","source":"## Class Imbalance:"},{"metadata":{"trusted":true,"_uuid":"fdd680c1f27b4c984218203df240621b81ab719a"},"cell_type":"code","source":"x=train.iloc[:,2:].sum()\n#marking comments without any tags as \"clean\"\nrowsums=train.iloc[:,2:].sum(axis=1)\ntrain['clean']=(rowsums==0)\n#count number of clean entries\ntrain['clean'].sum()\nprint(\"Total comments = \",len(train))\nprint(\"Total clean comments = \",train['clean'].sum())\nprint(\"Total tags =\",x.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d82d9f0641e224f6b4e5a92c7346a916b090f613"},"cell_type":"code","source":"print(\"Check for missing values in Train dataset\")\nnull_check=train.isnull().sum()\nprint(null_check)\nprint(\"Check for missing values in Test dataset\")\nnull_check=test.isnull().sum()\nprint(null_check)\nprint(\"filling NA with \\\"unknown\\\"\")\ntrain[\"comment_text\"].fillna(\"unknown\", inplace=True)\ntest[\"comment_text\"].fillna(\"unknown\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3456af2b61e54879493ba58aaa3d2e400f7573b8"},"cell_type":"code","source":"x=train.iloc[:,2:].sum()\n#plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"# per class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d46864f12625e791d0927fcd7e0c68296e53f62d"},"cell_type":"markdown","source":"* The toxicity is not evenly spread out across classes. Hence we might face class imbalance problems\n* There are ~95k comments in the training dataset and there are ~21 k tags and ~86k clean comments!?\n* This is only possible when multiple tags are associated with each comment (eg) a comment can be classified as both toxic and obscene."},{"metadata":{"_uuid":"68b90c26cbac19e265f5c87abeb75428dd35521a"},"cell_type":"markdown","source":"## Multi-tagging:"},{"metadata":{"_uuid":"06f57fd099d247e389bd903cd74f3f6f4a89138a"},"cell_type":"markdown","source":"Let's check how many comments have multiple tags."},{"metadata":{"trusted":true,"_uuid":"29616bee857588a8bbf4531f07837cfda92e97e5"},"cell_type":"code","source":"x=rowsums.value_counts()\n\n#plot\nplt.figure(figsize=(8,4))\nax = sns.barplot(x.index, x.values, alpha=0.8,color=color[2])\nplt.title(\"Multiple tags per comment\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('# of tags ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7299eea3325de7c3df2d63358f79fc1f7b3638dc"},"cell_type":"markdown","source":"Only ~10% of the total comments have some sort of toxicity in them. There are certain comments(20) that are marked as all of the above."},{"metadata":{"_uuid":"7f14f7df98502aeab18779a1674af2c5b15da8cb"},"cell_type":"markdown","source":"## Which tags go together?"},{"metadata":{"_uuid":"dba6dbf84fd9031facdd07fbe465c97181048ddf"},"cell_type":"markdown","source":"Now let's have a look at how often the tags occur together. A good indicator of that would be a correlation plot."},{"metadata":{"trusted":true,"_uuid":"5a1a11605c143195f41049235c46e49f61991c94"},"cell_type":"code","source":"temp_df=train.iloc[:,2:-1]\n# filter temp by removing clean comments\n# temp_df=temp_df[~train.clean]\n\ncorr=temp_df.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b372ba01152b3ea6eda8885354d0565f9bdb557"},"cell_type":"markdown","source":"The above plot indicates a pattern of co-occurance but Pandas's default Corr function which uses Pearson correlation does not apply here, since the variables invovled are Categorical (binary) variables."},{"metadata":{"_uuid":"b963f146cedea1552909d98b8b66368ceffbe99c"},"cell_type":"markdown","source":"So, to find a pattern between two categorical variables we can use other tools like"},{"metadata":{"_uuid":"46d3a56fce5a7c3e3b2e92acf2d929122ace8b17"},"cell_type":"markdown","source":"* Confusion matrix/Crosstab\n* Cramer's V Statistic\n* Cramer's V stat is an extension of the chi-square test where the extent/strength of association is also measured"},{"metadata":{"trusted":true,"_uuid":"c12efe5511d74224920ad9337d45f2d224976356"},"cell_type":"code","source":"# https://pandas.pydata.org/pandas-docs/stable/style.html\ndef highlight_min(data, color='yellow'):\n    '''\n    highlight the maximum in a Series or DataFrame\n    '''\n    attr = 'background-color: {}'.format(color)\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_min = data == data.min()\n        return [attr if v else '' for v in is_min]\n    else:  # from .apply(axis=None)\n        is_max = data == data.min().min()\n        return pd.DataFrame(np.where(is_min, attr, ''),\n                            index=data.index, columns=data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44d1379aeeafc5e797162995c72f9f6398136a19"},"cell_type":"code","source":"#Crosstab\n# Since technically a crosstab between all 6 classes is impossible to vizualize, lets take a \n# look at toxic with other tags\nmain_col=\"toxic\"\ncorr_mats=[]\nfor other_col in temp_df.columns[1:]:\n    confusion_matrix = pd.crosstab(temp_df[main_col], temp_df[other_col])\n    corr_mats.append(confusion_matrix)\nout = pd.concat(corr_mats,axis=1,keys=temp_df.columns[1:])\n\n#cell highlighting\nout = out.style.apply(highlight_min,axis=0)\nout","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"053cecf487af8bc3f3ff2c572a1da648994adc7d"},"cell_type":"markdown","source":"The above table represents the Crosstab/ consufion matix of Toxic comments with the other classes.\n\nSome interesting observations:\n* A Severe toxic comment is always toxic.\n* Other classes seem to be a subset of toxic barring a few exceptions."},{"metadata":{"trusted":true,"_uuid":"6741c2064507123c8516513437086462b779d934"},"cell_type":"code","source":"def cramers_corrected_stat(confusion_matrix):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n    rcorr = r - ((r-1)**2)/(n-1)\n    kcorr = k - ((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"378b53825537b6892c307b5e59d43b8c0fca7faa"},"cell_type":"code","source":"#Checking for Toxic and Severe toxic for now\nimport pandas as pd\ncol1=\"toxic\"\ncol2=\"severe_toxic\"\nconfusion_matrix = pd.crosstab(temp_df[col1], temp_df[col2])\nprint(\"Confusion matrix between toxic and severe toxic:\")\nprint(confusion_matrix)\nnew_corr=cramers_corrected_stat(confusion_matrix)\nprint(\"The correlation between Toxic and Severe toxic using Cramer's stat=\",new_corr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5821599153ba59673fd6e3f632451e85344681da"},"cell_type":"markdown","source":"# Example Comments:"},{"metadata":{"trusted":true,"_uuid":"522819e1de0586c258c6e893989f477bf94c2fbb"},"cell_type":"code","source":"print(\"toxic:\")\nprint(train[train.severe_toxic==1].iloc[3,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b607243413eb57b45347c1a442de3272841d367"},"cell_type":"code","source":"print(\"severe_toxic:\")\nprint(train[train.severe_toxic==1].iloc[4,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b485003cfbb4ef3f2cb5f04d32c465afdc49117c"},"cell_type":"code","source":"print(\"Threat:\")\nprint(train[train.threat==1].iloc[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04a6dd15dbf64acbc86d31ee2458674170446f04"},"cell_type":"code","source":"print(\"Obscene:\")\nprint(train[train.obscene==1].iloc[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9beb3998e42eebe705736aef1021f2b2a355934d"},"cell_type":"code","source":"print(\"identity_hate:\")\nprint(train[train.identity_hate==1].iloc[4,1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd74a7070ea8a4fee3864f0ee280996dd62b9b46"},"cell_type":"markdown","source":"That was a whole lot of toxicity. Some weird observations:\n\n* Some of the comments are extremely and mere copy paste of the same thing.\n* Comments can still contain IP addresses(eg:62.158.73.165), usernames(eg:ARKJEDI10) and some mystery numbers(i assume is article-IDs)\nPoint 2 can cause huge overfitting."},{"metadata":{"_uuid":"a531e2de50ef526cbf9f2cd600ba0c33db04a551"},"cell_type":"markdown","source":"# Wordclouds - Frequent words:"},{"metadata":{"_uuid":"4ce199cadfcd86b7ba0764f4d1d99831805e66a9"},"cell_type":"markdown","source":"Now, let's take a look at words that are associated with these classes.\n\nChart Desc: The visuals here are word clouds (ie) more frequent words appear bigger. A cool way to create word clouds with funky pics is given here. It involves the following steps."},{"metadata":{"_uuid":"1eefec1a2835e22bc865eb4b0579e6092b5dabfa"},"cell_type":"markdown","source":"* Search for an image and its base 64 encoding\n* Paste encoding in a cell and convert it using codecs package to image\n* Create word cloud with the new image as a mask"},{"metadata":{"_uuid":"8b7860ee8765c70e60265a310451b703e3af3364"},"cell_type":"markdown","source":"A simpler way would be to create a new kaggle dataset and import images from there."},{"metadata":{"trusted":true,"_uuid":"0c9d8f9acce98c372f4b08d764d95d778731386e"},"cell_type":"code","source":"!ls ../input/images\nstopword=set(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"523efca03bc0fa7a0ab17bac7fa79c50b535d81a"},"cell_type":"markdown","source":"## Clean Comments"},{"metadata":{"trusted":true,"_uuid":"1438f39ccf7810f718dbb636057a215187e14e50"},"cell_type":"code","source":"#clean comments\nclean_mask=np.array(Image.open(\"../input/images/safe-zone.png\"))\nclean_mask=clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset=train[train.clean==True]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=clean_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Clean Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4c0154aaebad1266922de0f9514cb2f8cce96d0"},"cell_type":"markdown","source":"### Toxic Comments"},{"metadata":{"trusted":true,"_uuid":"bbd4210d9ec9ae36e199140af54bab877212c408"},"cell_type":"code","source":"toxic_mask=np.array(Image.open(\"../input/images/toxic-sign.png\"))\ntoxic_mask=toxic_mask[:,:,1]\n#wordcloud for clean comments\nsubset=train[train.toxic==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=4000,mask=toxic_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,20))\nplt.subplot(221)\nplt.axis(\"off\")\nplt.title(\"Words frequented in Toxic Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42efbd77889d51ca55b345e5e74aa895e8fcb914"},"cell_type":"markdown","source":"### Severely Toxic Comments"},{"metadata":{"trusted":true,"_uuid":"63be1eecba8ef9379b0f9fd36a28e79c7997949e"},"cell_type":"code","source":"#Severely toxic comments\nplt.subplot(222)\nsevere_toxic_mask=np.array(Image.open(\"../input/images/bomb.png\"))\nsevere_toxic_mask=severe_toxic_mask[:,:,1]\nsubset=train[train.severe_toxic==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=severe_toxic_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Severe Toxic Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'Reds' , random_state=244), alpha=0.98)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e8a1b5c2e35f1d821775d78a7b3a3fefa9c84dc"},"cell_type":"markdown","source":"### Threat Comments"},{"metadata":{"trusted":true,"_uuid":"7c81148d3a4de8aa521c473d31d414388c17c9ef"},"cell_type":"code","source":"#Threat comments\nplt.subplot(223)\nthreat_mask=np.array(Image.open(\"../input/images/anger.png\"))\nthreat_mask=threat_mask[:,:,1]\nsubset=train[train.threat==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=threat_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Threatening Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'summer' , random_state=2534), alpha=0.98)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4c827ec0fa0c3ecc5b1e7e0d098f01d9a946ae8"},"cell_type":"markdown","source":"### Insult"},{"metadata":{"trusted":true,"_uuid":"d181156606e7e3c60068d81ef29334084cccbb2e"},"cell_type":"code","source":"plt.subplot(224)\ninsult_mask=np.array(Image.open(\"../input/images/swords.png\"))\ninsult_mask=insult_mask[:,:,1]\nsubset=train[train.insult==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=insult_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.title(\"Words frequented in insult Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'Paired_r' , random_state=244), alpha=0.98)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3f7f71b1ceaddb518f1b48bdda7bc302ebdbe2b"},"cell_type":"markdown","source":"# Corpus Cleaning:"},{"metadata":{"_uuid":"98f839b18a14f9bc6592b3954a9c0687b0f9d3f6"},"cell_type":"markdown","source":"It's important to use a clean dataset before creating count features."},{"metadata":{"trusted":true,"_uuid":"d4acd754bc24b3bbabb7938d19775ba96f08ad25"},"cell_type":"code","source":"merge=pd.concat([train.iloc[:,0:2],test.iloc[:,0:2]])\ndf=merge.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4604f55e4961c47a246e90d2596b9e95997655d8"},"cell_type":"code","source":"corpus=merge.comment_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7a23dd6013f1788fa775d9266a1260de3dbbb5b"},"cell_type":"code","source":"#https://drive.google.com/file/d/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM/view\n# Aphost lookup dict\nAPPO = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a73a216dbcb491d041b69e5dd754db6f2bc0fa8c"},"cell_type":"code","source":"def clean(comment):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    #Convert to lower case , so that Hi and hi are the same\n    comment=comment.lower()\n    #remove \\n\n    comment=re.sub(\"\\\\n\",\"\",comment)\n    # remove leaky elements like ip,user\n    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n    #removing usernames\n    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n    \n    #Split the sentences into words\n    words=tokenizer.tokenize(comment)\n    \n    # (')aphostophe  replacement (ie)   you're --> you are  \n    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n    words=[APPO[word] if word in APPO else word for word in words]\n    words=[lem.lemmatize(word, \"v\") for word in words]\n    words = [w for w in words if not w in eng_stopwords]\n    \n    clean_sent=\" \".join(words)\n    # remove any non alphanum,digit character\n    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n    return(clean_sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18612da889566b51786a8e68da8d26147a93894c"},"cell_type":"code","source":"corpus.iloc[12235]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"559b84448b51ba3b003ba834debd47e4f5751e8c"},"cell_type":"code","source":"clean(corpus.iloc[12235])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a08d6a4d583a0a2066b658475e3924cc27faf5f"},"cell_type":"code","source":"clean_corpus=corpus.apply(lambda x :clean(x))\n\nend_time=time.time()\nprint(\"total time till Cleaning\",end_time-start_time)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3e77958a72837cb6b38beee0d20107666d4a456"},"cell_type":"markdown","source":"## Direct features:"},{"metadata":{"_uuid":"250a7f3de9b356b31e31f2f0135f8d7630690b86"},"cell_type":"markdown","source":"### 1)Count based features(for unigrams):\nLets create some features based on frequency distribution of the words. Initially lets consider taking words one at a time (ie) Unigrams\n\nPython's SKlearn provides 3 ways of creating count features.All three of them first create a vocabulary(dictionary) of words and then create a sparse matrix of word counts for the words in the sentence that are present in the dictionary. A brief description of them:\n\n* CountVectorizer\n  * Creates a matrix with frequency counts of each word in the text corpus\n* TF-IDF Vectorizer\n  * TF - Term Frequency -- Count of the words(Terms) in the text corpus (same of Count Vect)\n  * IDF - Inverse Document Frequency -- Penalizes words that are too frequent. We can think of this as regularization\n* HashingVectorizer\n  * Creates a hashmap(word to number mapping based on hashing technique) instead of a dictionary for vocabulary\n  * This enables it to be more scalable and faster for larger text coprus\n  * Can be parallelized across multiple threads\nUsing TF-IDF here. Note: Using the concatenated dataframe \"merge\" which contains both text from train and test dataset to ensure that the vocabulary that we create does not missout on the words that are unique to testset."},{"metadata":{"trusted":true,"_uuid":"09a6faeb5d323922ee781c1b7167048748e77c2f"},"cell_type":"code","source":"merge=pd.concat([train.iloc[:,0:2],test.iloc[:,0:2]])\ndf=merge.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7f7fe36569726dc47bc7eefe0f3a2affcbd00c1"},"cell_type":"code","source":"## Indirect features\n\n#Sentense count in each comment:\n    #  '\\n' can be used to count the number of sentences in each comment\ndf['count_sent']=df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#Word count in each comment:\ndf['count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n#Unique word count\ndf['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n#Letter count\ndf['count_letters']=df[\"comment_text\"].apply(lambda x: len(str(x)))\n#punctuation count\ndf[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#upper case words count\ndf[\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n#title case words count\ndf[\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\ndf[\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Average length of the words\ndf[\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df1c8a0f32ac4ae57ebded9054f4742b1bec8a92"},"cell_type":"code","source":"#derived features\n#Word count percent in each comment:\ndf['word_unique_percent']=df['count_unique_word']*100/df['count_word']\n#derived features\n#Punct percent in each comment:\ndf['punct_percent']=df['count_punctuations']*100/df['count_word']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37b6e9c0357010ddf45d8aca2138ead9afd8d7a6"},"cell_type":"code","source":"#serperate train and test features\ntrain_feats=df.iloc[0:len(train),]\ntest_feats=df.iloc[len(train):,]\n#join the tags\ntrain_tags=train.iloc[:,2:]\ntrain_feats=pd.concat([train_feats,train_tags],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d97ca7169a8aca9acae26b312cf1066637348ce"},"cell_type":"code","source":"start_unigrams=time.time()\ntfv = TfidfVectorizer(min_df=200,  max_features=1000, \n            strip_accents='unicode', analyzer='word',ngram_range=(1,1),\n            use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\ntfv.fit(clean_corpus)\nfeatures = np.array(tfv.get_feature_names())\n\ntrain_unigrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\ntest_unigrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2328d5eb5d289e72eb0bbb46a57ba0c02797b463"},"cell_type":"code","source":"#https://buhrmann.github.io/tfidf-analysis.html\ndef top_tfidf_feats(row, features, top_n=25):\n    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\ndef top_feats_in_doc(Xtr, features, row_id, top_n=25):\n    ''' Top tfidf features in specific document (matrix row) '''\n    row = np.squeeze(Xtr[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)\n\ndef top_mean_feats(Xtr, features, grp_ids, min_tfidf=0.1, top_n=25):\n    ''' Return the top n features that on average are most important amongst documents in rows\n        indentified by indices in grp_ids. '''\n    \n    D = Xtr[grp_ids].toarray()\n\n    D[D < min_tfidf] = 0\n    tfidf_means = np.mean(D, axis=0)\n    return top_tfidf_feats(tfidf_means, features, top_n)\n\n# modified for multilabel milticlass\ndef top_feats_by_class(Xtr, features, min_tfidf=0.1, top_n=20):\n    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n        calculated across documents with the same class label. '''\n    dfs = []\n    cols=train_tags.columns\n    for col in cols:\n        ids = train_tags.index[train_tags[col]==1]\n        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n        feats_df.label = label\n        dfs.append(feats_df)\n    return dfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"303c08a7fed4efa9c683b1a0162973eedd9634f9"},"cell_type":"code","source":"#get top n for unigrams\ntfidf_top_n_per_lass=top_feats_by_class(train_unigrams,features)\n\nend_unigrams=time.time()\n\nprint(\"total time in unigrams\",end_unigrams-start_unigrams)\nprint(\"total time till unigrams\",end_unigrams-start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fc9b3e2b04ead005b63c0cb830d8b2a50df59a2"},"cell_type":"code","source":"plt.figure(figsize=(16,22))\nplt.suptitle(\"TF_IDF Top words per class(unigrams)\",fontsize=20)\ngridspec.GridSpec(4,2)\nplt.subplot2grid((4,2),(0,0))\nsns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:9],tfidf_top_n_per_lass[0].tfidf.iloc[0:9],color=color[0])\nplt.title(\"class : Toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.subplot2grid((4,2),(0,1))\nsns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:9],tfidf_top_n_per_lass[1].tfidf.iloc[0:9],color=color[1])\nplt.title(\"class : Severe toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(1,0))\nsns.barplot(tfidf_top_n_per_lass[2].feature.iloc[0:9],tfidf_top_n_per_lass[2].tfidf.iloc[0:9],color=color[2])\nplt.title(\"class : Obscene\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(1,1))\nsns.barplot(tfidf_top_n_per_lass[3].feature.iloc[0:9],tfidf_top_n_per_lass[3].tfidf.iloc[0:9],color=color[3])\nplt.title(\"class : Threat\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(2,0))\nsns.barplot(tfidf_top_n_per_lass[4].feature.iloc[0:9],tfidf_top_n_per_lass[4].tfidf.iloc[0:9],color=color[4])\nplt.title(\"class : Insult\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(2,1))\nsns.barplot(tfidf_top_n_per_lass[5].feature.iloc[0:9],tfidf_top_n_per_lass[5].tfidf.iloc[0:9],color=color[5])\nplt.title(\"class : Identity hate\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(3,0),colspan=2)\nsns.barplot(tfidf_top_n_per_lass[6].feature.iloc[0:19],tfidf_top_n_per_lass[6].tfidf.iloc[0:19])\nplt.title(\"class : Clean\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9afbbe80e0eac4e428a9153b775313f30ca0944b"},"cell_type":"code","source":"#temp settings to min=150 to facilitate top features section to run in kernals\n#change back to min=10 to get better results\ntfv = TfidfVectorizer(min_df=150,  max_features=1000, \n            strip_accents='unicode', analyzer='word',ngram_range=(2,2),\n            use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\ntfv.fit(clean_corpus)\nfeatures = np.array(tfv.get_feature_names())\ntrain_bigrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\ntest_bigrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])\n#get top n for bigrams\ntfidf_top_n_per_lass=top_feats_by_class(train_bigrams,features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2154a7d4a0b70ff6dd06ac7783f37cdd3c1d70d"},"cell_type":"code","source":"plt.figure(figsize=(16,22))\nplt.suptitle(\"TF_IDF Top words per class(Bigrams)\",fontsize=20)\ngridspec.GridSpec(4,2)\nplt.subplot2grid((4,2),(0,0))\nsns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:5],tfidf_top_n_per_lass[0].tfidf.iloc[0:5],color=color[0])\nplt.title(\"class : Toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.subplot2grid((4,2),(0,1))\nsns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:5],tfidf_top_n_per_lass[1].tfidf.iloc[0:5],color=color[1])\nplt.title(\"class : Severe toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(1,0))\nsns.barplot(tfidf_top_n_per_lass[2].feature.iloc[0:5],tfidf_top_n_per_lass[2].tfidf.iloc[0:5],color=color[2])\nplt.title(\"class : Obscene\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(1,1))\nsns.barplot(tfidf_top_n_per_lass[3].feature.iloc[0:5],tfidf_top_n_per_lass[3].tfidf.iloc[0:5],color=color[3])\nplt.title(\"class : Threat\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(2,0))\nsns.barplot(tfidf_top_n_per_lass[4].feature.iloc[0:5],tfidf_top_n_per_lass[4].tfidf.iloc[0:5],color=color[4])\nplt.title(\"class : Insult\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(2,1))\nsns.barplot(tfidf_top_n_per_lass[5].feature.iloc[0:5],tfidf_top_n_per_lass[5].tfidf.iloc[0:5],color=color[5])\nplt.title(\"class : Identity hate\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(3,0),colspan=2)\nsns.barplot(tfidf_top_n_per_lass[6].feature.iloc[0:9],tfidf_top_n_per_lass[6].tfidf.iloc[0:9])\nplt.title(\"class : Clean\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4e1ec0e33055ea534e38bae8b9eb2362a11a54f"},"cell_type":"code","source":"end_time=time.time()\nprint(\"total time till bigrams\",end_time-start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9d97cf1b62cbaa955e14ab52f3d801e514dcf7c"},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=100,  max_features=1000, \n            strip_accents='unicode', analyzer='char',ngram_range=(1,4),\n            use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\ntfv.fit(clean_corpus)\nfeatures = np.array(tfv.get_feature_names())\ntrain_charngrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\ntest_charngrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])\nend_time=time.time()\nprint(\"total time till charngrams\",end_time-start_time)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"889abec1cb57ade1659dad2fb4a4e2d425505dc4"},"cell_type":"markdown","source":"Building the model"},{"metadata":{"trusted":true,"_uuid":"5a1552cf3e1f6eb01f0d08ff05a4183d3c943858"},"cell_type":"code","source":"re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2345fd73f2b7176d6747ee7e89905eb25ee6f39"},"cell_type":"code","source":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train[\"comment_text\"])\ntest_term_doc = vec.transform(test[\"comment_text\"])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de069fb8d9006942241962f67d8b90064c116cc7"},"cell_type":"markdown","source":"This creates a sparse matrix with only a small number of non-zero elements (stored elements in the representation below).\n\n"},{"metadata":{"trusted":true,"_uuid":"d9c0cd6c602c7544f90e80efa48fb9dfd2312e07"},"cell_type":"code","source":"trn_term_doc, test_term_doc\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d654feb2bfada606b8fdc48f7496a3b45bb828c9"},"cell_type":"markdown","source":"Here's the basic naive bayes feature equation:"},{"metadata":{"trusted":true,"_uuid":"87037852fde8715f7204cc2a6a9f9a2b90461a6b"},"cell_type":"code","source":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdb64773edb37d6049c6f3c0e985af2198326040"},"cell_type":"code","source":"x = trn_term_doc\ntest_x = test_term_doc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f7c5ce861e8978ce970b8c2f2793a4b7b629cec"},"cell_type":"markdown","source":"Fit a model for one dependent at a time:"},{"metadata":{"trusted":true,"_uuid":"f0db9b1501e006a1626ea61cf12640259eba350e"},"cell_type":"code","source":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"244e88c914c59bd92c73e9163b1451d583af9dc5"},"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\npreds = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_mdl(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d011e9b55daa818dac6fc1bbb869b3de504d2442"},"cell_type":"markdown","source":"And finally, create the submission file.\n\n"},{"metadata":{"trusted":true,"_uuid":"1179c823da834118da89886af6c26e5d0284216d"},"cell_type":"code","source":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}