{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.special import logit, expit\n\nimport copy\nimport re\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom nltk import WordNetLemmatizer\n\nclass BaseTokenizer(object):\n    def process_text(self, text):\n        raise NotImplemented\n\n    def process(self, texts):\n        for text in texts:\n            yield self.process_text(text)\n\n\nRE_PATTERNS = {\n    ' american ':\n        [\n            'amerikan'\n        ],\n\n    ' adolf ':\n        [\n            'adolf'\n        ],\n\n\n    ' hitler ':\n        [\n            'hitler'\n        ],\n\n    ' fuck':\n        [\n            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n            'feck ', ' fux ', 'f\\*\\*', \n            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n        ],\n\n    ' ass ':\n        [\n            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n        ],\n\n    ' ass hole ':\n        [\n            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n        ],\n\n    ' bitch ':\n        [\n            'b[w]*i[t]*ch', 'b!tch',\n            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n        ],\n\n    ' bastard ':\n        [\n            'ba[s|z]+t[e|a]+rd'\n        ],\n\n    ' trans gender':\n        [\n            'transgender'\n        ],\n\n    ' gay ':\n        [\n            'gay'\n        ],\n\n    ' cock ':\n        [\n            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n        ],\n\n    ' dick ':\n        [\n            ' dick[^aeiou]', 'deek', 'd i c k'\n        ],\n\n    ' suck ':\n        [\n            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n        ],\n\n    ' cunt ':\n        [\n            'cunt', 'c u n t'\n        ],\n\n    ' bull shit ':\n        [\n            'bullsh\\*t', 'bull\\$hit'\n        ],\n\n    ' homo sex ual':\n        [\n            'homosexual'\n        ],\n\n    ' jerk ':\n        [\n            'jerk'\n        ],\n\n    ' idiot ':\n        [\n            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n                                                                                      'i d i o t'\n        ],\n\n    ' dumb ':\n        [\n            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n        ],\n\n    ' shit ':\n        [\n            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n        ],\n\n    ' shit hole ':\n        [\n            'shythole'\n        ],\n\n    ' retard ':\n        [\n            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n        ],\n\n    ' rape ':\n        [\n            ' raped'\n        ],\n\n    ' dumb ass':\n        [\n            'dumbass', 'dubass'\n        ],\n\n    ' ass head':\n        [\n            'butthead'\n        ],\n\n    ' sex ':\n        [\n            'sexy', 's3x', 'sexuality'\n        ],\n\n\n    ' nigger ':\n        [\n            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n        ],\n\n    ' shut the fuck up':\n        [\n            'stfu'\n        ],\n\n    ' pussy ':\n        [\n            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n        ],\n\n    ' faggot ':\n        [\n            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n        ],\n\n    ' mother fucker':\n        [\n            ' motha ', ' motha f', ' mother f', 'motherucker',\n        ],\n\n    ' whore ':\n        [\n            'wh\\*\\*\\*', 'w h o r e'\n        ],\n}\n\n\nclass PatternTokenizer(BaseTokenizer):\n    def __init__(self, lower=True, initial_filters=r\"[^a-z0-9!@#\\$%\\^\\&\\*_\\-,\\.' ]\", patterns=RE_PATTERNS,\n                 remove_repetitions=True):\n        self.lower = lower\n        self.patterns = patterns\n        self.initial_filters = initial_filters\n        self.remove_repetitions = remove_repetitions\n\n    def process_text(self, text):\n        x = self._preprocess(text)\n        for target, patterns in self.patterns.items():\n            for pat in patterns:\n                x = re.sub(pat, target, x)\n        x = re.sub(r\"[^a-z' ]\", ' ', x)\n        return x.split()\n\n    def process_ds(self, ds):\n        ### ds = Data series\n\n        # lower\n        ds = copy.deepcopy(ds)\n        if self.lower:\n            ds = ds.str.lower()\n        # remove special chars\n        if self.initial_filters is not None:\n            ds = ds.str.replace(self.initial_filters, ' ')\n        # fuuuuck => fuck\n        if self.remove_repetitions:\n            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n            ds = ds.str.replace(pattern, r\"\\1\")\n\n        for target, patterns in self.patterns.items():\n            for pat in patterns:\n                ds = ds.str.replace(pat, target)\n\n        ds = ds.str.replace(r\"[^a-z' ]\", ' ')\n\n        return ds.str.split()\n\n    def _preprocess(self, text):\n        # lower\n        if self.lower:\n            text = text.lower()\n\n        # remove special chars\n        if self.initial_filters is not None:\n            text = re.sub(self.initial_filters, ' ', text)\n\n        # fuuuuck => fuck\n        if self.remove_repetitions:\n            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n            text = pattern.sub(r\"\\1\", text)\n        return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10eac4e9aa14a239e43786b01478482939af7f3e"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\ntokenizer = PatternTokenizer()\ntrain_text = tokenizer.process_ds(train[\"comment_text\"]).str.join(sep=\" \")\ntest_text = tokenizer.process_ds(test[\"comment_text\"]).str.join(sep=\" \")\n\nall_text = pd.concat([train_text, test_text])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ac8f9fedd0d26e5389450e79f3c575bbc667bdd"},"cell_type":"code","source":"classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nword_vectorizer = CountVectorizer(stop_words = 'english',analyzer='word')\nword_vectorizer.fit(all_text)\ntrain_features = word_vectorizer.transform(train_text)\ntest_features = word_vectorizer.transform(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"beb3d9d3badb78530eb1417882a18a753ab336bd"},"cell_type":"code","source":"losses = []\npredictions = {'id': test['id']}\nfor class_name in classes:\n    train_target = train[class_name]\n    classifier = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=10, max_features=\"sqrt\", max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=3, min_samples_split=8,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n            oob_score=True, random_state=None, verbose=1,\n            warm_start=False)\n    \n    results = cross_val_score(classifier, train_features, train_target, cv=3, scoring='f1_micro')\n    cv_loss = results.mean()\n    losses.append(cv_loss)\n    print('CV score for class {} is {}'.format(class_name, cv_loss))\n    print(\"CV accuracy score: {:.2f}%\".format(results.mean()*100))\n    \n    classifier.fit(train_features, train_target)\n    predictions[class_name] = expit(logit(classifier.predict_proba(test_features)[:, 1]))\n\nprint('Total CV score is {}'.format(np.mean(losses)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}