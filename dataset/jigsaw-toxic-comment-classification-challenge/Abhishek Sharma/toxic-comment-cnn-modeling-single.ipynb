{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installing bayesian-optimization package\n!pip install bayesian-optimization\n# Installing for Stratified Split\n!pip install iterative-stratification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Dropout, Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras import backend as K\n# For hyperparameter tuning\nfrom bayes_opt import BayesianOptimization\n# For Splitting Data\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE=64\nTOTAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\nprint(\"Total Batch Size:\",TOTAL_BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading Training set\ndf_train=pd.read_json('/kaggle/input/toxic-comment-cnn-cleaned/train.json')\nprint('Shape=>',df_train.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading Test set\ndf_test=pd.read_json('/kaggle/input/toxic-comment-cnn-cleaned/test.json')\nprint('Shape=>',df_test.shape)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenizing Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\n#creating index for words\ntokenizer.fit_on_texts(df_train['cleaned'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Vocabulary Size=>',len(tokenizer.word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting word sequence to integer sequence\ntrain_seq = tokenizer.texts_to_sequences(df_train['cleaned']) \ntest_seq = tokenizer.texts_to_sequences(df_test['cleaned'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Padding with zero\ntrain_seq=pad_sequences(train_seq,maxlen=100,padding='post')\ntest_seq=pad_sequences(test_seq,maxlen=100,padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary=len(tokenizer.word_index)+1\nprint('Vocabulary Size=>',vocabulary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of train_sequence=>',train_seq.shape)\nprint('Shape of test_sequence=>',test_seq.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train=df_train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Data for TPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_seq, y_train))\n    .shuffle(42)\n    .batch(TOTAL_BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_seq)\n    .batch(TOTAL_BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_dataset)\nprint(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_model(filters,dropout):\n    input_1=Input(shape=(100,))\n    embedding_1=Embedding(vocabulary,100)(input_1)\n    conv_1=Conv1D(filters=int(round(filters)),kernel_size=7,padding=\"same\")(embedding_1)\n    dropout_1=Dropout(dropout)(conv_1)\n    pool_1=GlobalMaxPooling1D()(dropout_1)\n\n    dense=Dense(128,activation='relu')(pool_1)\n    output=Dense(6,activation='sigmoid')(dense)\n\n    model=Model(inputs=[input_1],outputs=output)\n    # Compile Model\n    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),loss=tf.keras.losses.BinaryCrossentropy(),metrics=[\"accuracy\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def evaluate_network(filters,dropout):\n#     boot=MultilabelStratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n#     score_list=[]\n    \n#     # Loop through samples\n#     for train, test in boot.split(train_seq,y_train):\n#         # Creating Train Set\n#         x_train_split,y_train_split=train_seq[train],y_train[train]\n#         train_split=(\n#             tf.data.Dataset\n#             .from_tensor_slices((x_train_split, y_train_split))\n#             .batch(TOTAL_BATCH_SIZE)\n#             .cache()\n#             .prefetch(AUTO)\n#         )\n#         # Creating Test Set\n#         x_test_split,y_test_split=train_seq[test],y_train[test]\n#         test_split=(\n#             tf.data.Dataset\n#             .from_tensor_slices((x_test_split,y_test_split))\n#             .batch(TOTAL_BATCH_SIZE)\n#             .cache()\n#             .prefetch(AUTO)\n#         )\n        \n#         with strategy.scope():\n#             model=generate_model(filters,dropout)\n#         es=EarlyStopping(monitor='val_loss', mode='min', verbose=0,patience=5,min_delta=1e-5)\n#         model.fit(train_split, batch_size=512,\n#                   epochs=100, verbose=0, validation_data=test_split, callbacks=[es])\n        \n#         # Validating on test split\n#         pred=model.predict(test_split)\n#         #Evaluating on ROC-AUC\n#         score=roc_auc_score(y_test_split,pred)\n#         score_list.append(score)\n#         #Clearing session\n#         K.clear_session()\n        \n#     return np.mean(score_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Bounding region for parameter space\n# param_space={\n#     \"filters\":(320,370),\n#     \"dropout\":(0,0.1)\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# # Running Bayesian Optimizer\n# optimizer=BayesianOptimization(f=evaluate_network,pbounds=param_space,random_state=42,verbose=2)\n# optimizer.maximize(init_points=2, n_iter=20)\n# max_param=optimizer.max\n# print(max_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model=generate_model(352,0.06675)\nes=EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=5,min_delta=1e-5)\nmc = ModelCheckpoint(\"/kaggle/working/model.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nmodel.fit(train_seq, y_train, batch_size=512,\n          epochs=100, verbose=1, validation_split=0.2, callbacks=[es,mc])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In-sample Evaluation\ntrain_pred=model.predict(train_seq)\nprint('In-sample Evaluation ROC-AUC Score:\\n',roc_auc_score(y_train,train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred=model.predict(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataframe for final probabilties\nprob=pd.DataFrame(columns=['id','toxic','severe_toxic','obscene','threat','insult','identity_hate'],index=df_test.index)\nprob['id']=df_test['id']\nfor index,value in enumerate(['toxic','severe_toxic','obscene','threat','insult','identity_hate']):\n    prob[value]=final_pred[:,index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob.to_csv('submission-CNN-single-2-100-100-7-opt-opt.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}