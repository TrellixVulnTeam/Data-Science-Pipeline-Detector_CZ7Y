{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import logging\nimport time\nfrom platform import python_version\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom sklearn.metrics import roc_auc_score\nfrom torch.autograd import Variable\n\nprint(\"python version==%s\" % python_version())\nprint(\"pandas==%s\" % pd.__version__)\nprint(\"numpy==%s\" % np.__version__)\nprint(\"torch==%s\" % torch.__version__)\nprint(\"sklearn==%s\" % sklearn.__version__)\nprint(\"transformers==%s\" % transformers.__version__)\nprint(\"matplotlib==%s\" % matplotlib.__version__)\n\nlogging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/mydata/data/train.csv/train.csv')\n\nnp.random.seed(42)\ndf = df.sample(frac=1)\ndf = df.reset_index(drop=True)\n\ntarget_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ndf.iloc[[103]][target_columns]\n\ndf_train = df[:1000].reset_index(drop=True)\ndf_val = df[1000:1100].reset_index(drop=True)\ndf_test = df[1100:1300].reset_index(drop=True)\n\nmodel_class = transformers.BertModel\ntokenizer_class = transformers.BertTokenizer\npretrained_weights='bert-base-uncased'\n\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nbert_model = model_class.from_pretrained(pretrained_weights)\n\nmax_seq = 100\ndef tokenize_text(df, max_seq):\n    return [\n        tokenizer.encode(text, add_special_tokens=True)[:max_seq] for text in df.comment_text.values\n    ]\n\n\ndef pad_text(tokenized_text, max_seq):\n    return np.array([el + [0] * (max_seq - len(el)) for el in tokenized_text])\n\n\ndef tokenize_and_pad_text(df, max_seq):\n    tokenized_text = tokenize_text(df, max_seq)\n    padded_text = pad_text(tokenized_text, max_seq)\n    return torch.tensor(padded_text)\n\n\ndef targets_to_tensor(df, target_columns):\n    return torch.tensor(df[target_columns].values, dtype=torch.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_indices = tokenize_and_pad_text(df_train, max_seq)\nval_indices = tokenize_and_pad_text(df_val, max_seq)\ntest_indices = tokenize_and_pad_text(df_test, max_seq)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    x_train = bert_model(train_indices)[0]  # Models outputs are tuples\n    x_val = bert_model(val_indices)[0]\n    x_test = bert_model(test_indices)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = targets_to_tensor(df_train, target_columns)\ny_val = targets_to_tensor(df_val, target_columns)\ny_test = targets_to_tensor(df_test, target_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KimCNN(nn.Module):\n    def __init__(self, embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static):\n        super(KimCNN, self).__init__()\n\n        V = embed_num\n        D = embed_dim\n        C = class_num\n        Co = kernel_num\n        Ks = kernel_sizes\n        \n        self.static = static\n        self.embed = nn.Embedding(V, D)\n        self.convs1 = nn.ModuleList([nn.Conv2d(1, Co, (K, D)) for K in Ks])\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(len(Ks) * Co, C)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, x):\n        if self.static:\n            x = Variable(x)\n\n        x = x.unsqueeze(1)  # (N, Ci, W, D)\n\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n\n        x = torch.cat(x, 1)\n        x = self.dropout(x)  # (N, len(Ks)*Co)\n        logit = self.fc1(x)  # (N, C)\n        output = self.sigmoid(logit)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_num = x_train.shape[1]\nembed_dim = x_train.shape[2]\nclass_num = y_train.shape[1]\nkernel_num = 3\nkernel_sizes = [2, 3, 4]\ndropout = 0.5\nstatic = True\nmodel = KimCNN(\n    embed_num=embed_num,\n    embed_dim=embed_dim,\n    class_num=class_num,\n    kernel_num=kernel_num,\n    kernel_sizes=kernel_sizes,\n    dropout=dropout,\n    static=static,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 10\nbatch_size = 10\nlr = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nloss_fn = nn.BCELoss()\ndef generate_batch_data(x, y, batch_size):\n    i, batch = 0, 0\n    for batch, i in enumerate(range(0, len(x) - batch_size, batch_size), 1):\n        x_batch = x[i : i + batch_size]\n        y_batch = y[i : i + batch_size]\n        yield x_batch, y_batch, batch\n    if i + batch_size < len(x):\n        yield x[i + batch_size :], y[i + batch_size :], batch + 1\n    if batch == 0:\n        yield x, y, 1\ntrain_losses, val_losses = [], []\n\nfor epoch in range(n_epochs):\n    start_time = time.time()\n    train_loss = 0\n\n    model.train(True)\n    for x_batch, y_batch, batch in generate_batch_data(x_train, y_train, batch_size):\n        y_pred = model(x_batch)\n        optimizer.zero_grad()\n        loss = loss_fn(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    train_loss /= batch\n    train_losses.append(train_loss)\n    elapsed = time.time() - start_time\n\n    model.eval() # disable dropout for deterministic output\n    with torch.no_grad(): # deactivate autograd engine to reduce memory usage and speed up computations\n        val_loss, batch = 0, 1\n        for x_batch, y_batch, batch in generate_batch_data(x_val, y_val, batch_size):\n            y_pred = model(x_batch)\n            loss = loss_fn(y_pred, y_batch)\n            val_loss += loss.item()\n        val_loss /= batch\n        val_losses.append(val_loss)\n\n    print(\n        \"Epoch %d Train loss: %.2f. Validation loss: %.2f. Elapsed time: %.2fs.\"\n        % (epoch + 1, train_losses[-1], val_losses[-1], elapsed)\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_losses, label=\"Training loss\")\nplt.plot(val_losses, label=\"Validation loss\")\nplt.legend()\nplt.title(\"Losses\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval() # disable dropout for deterministic output\nwith torch.no_grad(): # deactivate autograd engine to reduce memory usage and speed up computations\n    y_preds = []\n    batch = 0\n    for x_batch, y_batch, batch in generate_batch_data(x_test, y_test, batch_size):\n        y_pred = model(x_batch)\n        y_preds.extend(y_pred.cpu().numpy().tolist())\n    y_preds_np = np.array(y_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_np = df_test[target_columns].values\ny_test_np[100:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}