{"cells":[{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Defintion:\" data-toc-modified-id=\"Problem-Defintion:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem Defintion:</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Dealing-with-the-data\" data-toc-modified-id=\"Dealing-with-the-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dealing with the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Making-necessary-imports-and-installations\" data-toc-modified-id=\"Making-necessary-imports-and-installations-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Making necessary imports and installations</a></span></li><li><span><a href=\"#Viewing-the-data\" data-toc-modified-id=\"Viewing-the-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Viewing the data</a></span></li><li><span><a href=\"#Defining-some-constant-terms-that-we'll-be-using-later\" data-toc-modified-id=\"Defining-some-constant-terms-that-we'll-be-using-later-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Defining some constant terms that we'll be using later</a></span></li><li><span><a href=\"#Loading-the-pretrained-word-embeddings-into-the--notebook-as-a-dictionary\" data-toc-modified-id=\"Loading-the-pretrained-word-embeddings-into-the--notebook-as-a-dictionary-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Loading the pretrained word embeddings into the  notebook as a dictionary</a></span></li><li><span><a href=\"#Preparing-the-data-to-feed-to-model\" data-toc-modified-id=\"Preparing-the-data-to-feed-to-model-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Preparing the data to feed to model</a></span></li><li><span><a href=\"#Converting-sentences-into-numbers-(sequences)\" data-toc-modified-id=\"Converting-sentences-into-numbers-(sequences)-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Converting sentences into numbers (sequences)</a></span></li><li><span><a href=\"#Padding-the-sequences\" data-toc-modified-id=\"Padding-the-sequences-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Padding the sequences</a></span></li><li><span><a href=\"#Preparing-the-embedding-matrix-corresponding-to-our-dataset-using-the-pretrained-embeddings\" data-toc-modified-id=\"Preparing-the-embedding-matrix-corresponding-to-our-dataset-using-the-pretrained-embeddings-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Preparing the embedding matrix corresponding to our dataset using the pretrained embeddings</a></span></li><li><span><a href=\"#Loading-the-embeddings-we-obtained-into-a-keras-Embedding-Layer\" data-toc-modified-id=\"Loading-the-embeddings-we-obtained-into-a-keras-Embedding-Layer-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>Loading the embeddings we obtained into a keras Embedding Layer</a></span></li></ul></li><li><span><a href=\"#Modelling\" data-toc-modified-id=\"Modelling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modelling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-model\" data-toc-modified-id=\"Baseline-model-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Baseline model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Defining-a-1D-Convolutional-Neural-Network\" data-toc-modified-id=\"Defining-a-1D-Convolutional-Neural-Network-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Defining a 1D Convolutional Neural Network</a></span></li><li><span><a href=\"#Compiling-the-model\" data-toc-modified-id=\"Compiling-the-model-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Compiling the model</a></span></li><li><span><a href=\"#Fitting-the-model-to-0.8-split-of-total-data-and-validating-on-the-0.2-part\" data-toc-modified-id=\"Fitting-the-model-to-0.8-split-of-total-data-and-validating-on-the-0.2-part-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Fitting the model to 0.8 split of total data and validating on the 0.2 part</a></span></li><li><span><a href=\"#Reviewing-loss-and-accuracy-path-of-model\" data-toc-modified-id=\"Reviewing-loss-and-accuracy-path-of-model-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Reviewing loss and accuracy path of model</a></span></li></ul></li><li><span><a href=\"#Conv1D-model-with-adam-optimizer\" data-toc-modified-id=\"Conv1D-model-with-adam-optimizer-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Conv1D model with adam optimizer</a></span></li><li><span><a href=\"#Introducing-Dropout\" data-toc-modified-id=\"Introducing-Dropout-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Introducing Dropout</a></span></li><li><span><a href=\"#Using-callbacks-on-our-model\" data-toc-modified-id=\"Using-callbacks-on-our-model-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Using callbacks on our model</a></span></li></ul></li><li><span><a href=\"#Evaluating-model-on-train-data\" data-toc-modified-id=\"Evaluating-model-on-train-data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Evaluating model on train data</a></span></li><li><span><a href=\"#Evaluating-model-on-test-data\" data-toc-modified-id=\"Evaluating-model-on-test-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Evaluating model on test data</a></span></li><li><span><a href=\"#Creating-submission-file\" data-toc-modified-id=\"Creating-submission-file-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Creating submission file</a></span></li></ul></div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Problem Defintion:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* To given a negative comment in English Language, we must be able to classify its toxicity\n* This is a multilabel problem.m\n* So the output for each example should be a six dimensional vector","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  Data\nThe data is available on kaggle at\n\nhttps://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n\nThe data comprises of\n\n* 159571 comments for train data and\n* 153164 test comments","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The model is evaluated on the mean column-wise ROC AUC. \n* In other words, the score is the average of the individual AUCs of each predicted column.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Dealing with the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Making necessary imports and installations","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import sklearn\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,Input,MaxPooling1D,GlobalMaxPooling1D,Conv1D,Embedding\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Viewing the data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train=pd.read_csv(\"train.csv\",low_memory=False)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test=pd.read_csv(\"test.csv\",low_memory=False)\ntest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining some constant terms that we'll be using later","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"MAX_SEQ_LENGTH=100\nMAX_VOCAB_SIZE=20000 #This is the maximum number of unique words that will be tokenized\nEMBEDDING_DIM=100 # Each word will be represented as 100 dim vector\\\nVALIDATION_SPLIT=0.2 #Useful while training\nBATCH_SIZE=128\nEPOCHS=10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the pretrained word embeddings into the  notebook as a dictionary","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"word2vec={}\nwith open(os.path.join(\"../large_data/glove.6B/glove.6B.%sd.txt\" % EMBEDDING_DIM),encoding=\"utf-8\") as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        embed=np.asarray(values[1:],dtype=\"float32\")\n        word2vec[word]=embed\nprint(\"Found \",len(word2vec),\" word vectors\")   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the data to feed to model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sentences=train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values   #.values returns a numpy array\npossible_labels=[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\ntargets=train[possible_labels].values # returns a one hot encoded label vector for each example in train data\ntargets.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting sentences into numbers (sequences)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"tokenizer=Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences)\nsequences=tokenizer.texts_to_sequences(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"word2idx=tokenizer.word_index\nprint(\"Found %s unique tokens \" % len(word2idx))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Padding the sequences","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data=pad_sequences(sequences,maxlen=MAX_SEQ_LENGTH) # padding is pre by default\nprint(\"shape of data is \",data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the embedding matrix corresponding to our dataset using the pretrained embeddings","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"word2idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"num_words=min(MAX_VOCAB_SIZE,len(word2idx)+1) # Num of words should be less than or equal to MAX_VOCAB_SIZE\n\n# The +1 term indicates that the tokenizer indexing begins from 1\n\nembedding_matrix=np.zeros((num_words,EMBEDDING_DIM))\nfor word,pos_from_start in word2idx.items():\n    if pos_from_start<MAX_VOCAB_SIZE:\n        embedding_vector=word2vec.get(word) #we use get method instead of indexing because it helps if the word is not present in the dictionary\n        if embedding_vector is not None:\n            embedding_matrix[pos_from_start]=embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: In the embedding matrix ,vectors corresponding to words that are present in the data but not in the tokenizer are all zeros","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Loading the embeddings we obtained into a keras Embedding Layer","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Also we set Trainable as False for this layer as we have already loaded pretrained weights(embeddings)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"embedding_layer=Embedding(num_words,\n                          EMBEDDING_DIM,\n                          weights=[embedding_matrix],\n                          input_length=MAX_SEQ_LENGTH,\n                          trainable=False\n                         )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Baseline model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Defining a 1D Convolutional Neural Network","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"input_=Input(shape=(MAX_SEQ_LENGTH,))\nx=embedding_layer(input_)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=MaxPooling1D(3)(x)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=MaxPooling1D(3)(x)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=GlobalMaxPooling1D()(x) # max from every input channel\n# it also indicates which timestep value in that sequnce was most influential for classification\nx=Dense(128,activation=\"relu\")(x)\noutput=Dense(len(possible_labels),activation=\"sigmoid\")(x)\n# we use sigmoid classifier so that each of the 6 units in the last layer act as a linear classifier(y/n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compiling the model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"model1=Model(input_,output)\nmodel1.compile(loss=\"binary_crossentropy\",\n             optimizer=\"rmsprop\",\n             metrics=[\"accuracy\"]\n              )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the model to 0.8 split of total data and validating on the 0.2 part","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"history=model1.fit(\n                data,\n                targets,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                validation_split=VALIDATION_SPLIT\n                )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reviewing loss and accuracy path of model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_curves(history):\n    fig,(ax0,ax1)=plt.subplots(2,1,figsize=(8,8))\n    ax0.plot(history.history[\"loss\"],label=\"loss\")\n    ax0.plot(history.history[\"val_loss\"],label=\"val_loss\")\n    ax0.legend()\n    ax1.plot(history.history[\"accuracy\"],label=\"accuracy\")\n    ax1.plot(history.history[\"val_accuracy\"],label=\"val_accuracy\")\n    ax1.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_curves(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we see the validation accuracy started dropping later.\n\nIt indicates that the model started overfitting the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conv1D model with adam optimizer","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"model2=Model(input_,output)\nmodel2.compile(loss=\"binary_crossentropy\",\n             optimizer=\"adam\",\n             metrics=[\"accuracy\"]\n              )\nhistory=model2.fit(\n                data,\n                targets,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                validation_split=VALIDATION_SPLIT\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_curves(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introducing Dropout","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"input_=Input(shape=(MAX_SEQ_LENGTH,))\nx=embedding_layer(input_)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=MaxPooling1D(3)(x)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=MaxPooling1D(3)(x)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=GlobalMaxPooling1D()(x) # max from every input channel\n# it also indicates which timestep value in that sequnce was most influential for classification\nx=Dense(128,activation=\"relu\")(x)\nx=tf.keras.layers.Dropout(0.3)(x)\noutput=Dense(len(possible_labels),activation=\"sigmoid\")(x)\n# we use sigmoid classifier so that each of the 6 units in the last layer act as a linear classifier(y/n)\n\nmodel3=Model(input_,output)\nmodel3.compile(loss=\"binary_crossentropy\",\n             optimizer=\"adam\",\n             metrics=[\"accuracy\"]\n              )\nhistory=model3.fit(\n                data,\n                targets,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                validation_split=VALIDATION_SPLIT\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_curves(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using callbacks on our model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"early_stopping=tf.keras.callbacks.EarlyStopping(patience=5,monitor=\"val_accuracy\")\nmodel_checkpoint=tf.keras.callbacks.ModelCheckpoint(\"model3.h5\",monitor=\"val_accuracy\",save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"EPOCHS=100\nhistory=model3.fit(\n                data,\n                targets,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                validation_split=VALIDATION_SPLIT,\n                callbacks=[early_stopping,model_checkpoint]\n                )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating model on train data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"model=tf.keras.models.load_model(\"model3.h5\")\np=model.predict(data)\naucs = []\nfor j in range(6):\n    auc = roc_auc_score(targets[:,j], p[:,j])\n    aucs.append(auc)\nprint(np.mean(aucs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"p.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating model on test data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"test_sentences=test[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\ntest_sequences=tokenizer.texts_to_sequences(test_sentences)\ntest_data=pad_sequences(test_sequences,maxlen=MAX_SEQ_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred=model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred[:,0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"possible_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating submission file","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"submit1=pd.DataFrame(columns=[\"id\",\"toxic\",\"severe_toxic\",\"threat\",\"insult\",\"identity_hate\"])\nsubmit1[\"id\"]=test[\"id\"]\ni=0\nfor col in possible_labels:\n    submit1[col]=pred[:,i]\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submit1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submit1.index = submit1.index+1\nsubmit1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submit1.to_csv(\"submission1.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"a=pd.read_csv(\"submission1.csv\")\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}