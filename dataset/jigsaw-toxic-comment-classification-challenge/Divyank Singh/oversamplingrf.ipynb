{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\nimport re\nimport nltk\n\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_data = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comments and labels as numpy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"comments = data['comment_text'].to_numpy()\nlabels = data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating the dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = []\nfor index in range(len(labels)):\n    num = np.count_nonzero(labels[index])\n    if(num == 0):\n        dataframe.append([comments[index], 0])\n    else:\n        dataframe.append([comments[index], 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(dataframe, columns = ['comment', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking imbalancing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.subplot()\n\ng = sns.countplot(df.label)\ng.set_xticklabels(['Toxic', 'Not Toxic'])\ng.set_yticklabels(['Count'])\n\n# function to show values on bars\ndef show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() / 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.0f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\nshow_values_on_bars(ax)\n\nsns.despine(left=True, bottom=True)\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Distribution of Classes', fontsize=30)\nplt.tick_params(axis = 'x', which='major', labelsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considerable imbalance {0 : 143346, 1 : 16225}\n\nNow lets check the performance of provided data without training the model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Resampling Data\n\n## Oversampling minority class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count_class_toxic, count_class_non_toxic = df.label.value_counts()\n\nclass_toxic = df[df['label'] == 0]\nclass_not_toxic = df[df['label'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_not_toxic_over = class_not_toxic.sample(count_class_toxic, replace = True)\ntest_over = pd.concat([class_toxic, class_not_toxic_over], axis = 0)\nprint(test_over.label.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Resampled data equally","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_over.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_over.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now performing the following operations on resampled data:\n1. Data Cleaning\n2. Vectorization\n3. Train Test split\n\n# Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_filter = test_over[\"comment\"] != \"\"\ntest_over = test_over[new_filter]\ntest_over = test_over.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing_text(sen):\n    # Remove punctuations and numbers\n    sent = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Single character removal\n    sent = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sent)\n\n    # Removing multiple spaces\n    sent = re.sub(r'\\s+', ' ', sent)\n\n    return sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = []\nnew_sentences = list(test_over[\"comment\"])\nfor sents in new_sentences:\n    X_new.append(preprocessing_text(sents))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_stopwords = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_new = TfidfVectorizer(stop_words = new_stopwords, use_idf = True)\nbag_of_words_new = vectorizer_new.fit_transform(X_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = bag_of_words_new\ny = test_over['label']\nX_train_old, X_test_old, Y_train_old, Y_test_old = train_test_split(x, y, test_size = 0.25, random_state = 27)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf_resampled = RandomForestClassifier(25)\nrf_clf_resampled.fit(X_train_old, Y_train_old)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_predict_resampled = rf_clf_resampled.predict(X_test_old)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_cm_resampled = confusion_matrix(Y_test_old, rf_predict_resampled)\nplot_confusion_matrix(rf_cm_resampled, [0, 1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Performance Measure","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy Score:', accuracy_score(Y_test_old, rf_predict_resampled))\nprint('Precision:', precision_score(Y_test_old, rf_predict_resampled))\nprint('Recall:',recall_score(Y_test_old, rf_predict_resampled))\nprint('F1 Score:', f1_score(Y_test_old, rf_predict_resampled))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ROC curve evaluation","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"Area under curve for Random Forest:\", roc_auc_score(Y_test_old, rf_predict_resampled))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For an ideal classifier the area under curve is 1.0 so Random Forest is close enough to being called an ideal classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score_rf = cross_val_score(rf_clf_resampled, x, y, cv = 5)\nprint(\"CV score {}\".format(np.mean(score_rf)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}