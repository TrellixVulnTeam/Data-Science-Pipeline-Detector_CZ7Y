{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\n\nfrom nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\nimport nltk\nfrom nltk import Text\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-25T04:53:02.608999Z","iopub.execute_input":"2022-03-25T04:53:02.609404Z","iopub.status.idle":"2022-03-25T04:53:04.575275Z","shell.execute_reply.started":"2022-03-25T04:53:02.609317Z","shell.execute_reply":"2022-03-25T04:53:04.574153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt = [\"He is ::having a great Time, at the park time?\",\n       \"She, unlike most women, is a big player on the park's grass.\",\n       \"she can't be going\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:53:15.018516Z","iopub.execute_input":"2022-03-25T04:53:15.018828Z","iopub.status.idle":"2022-03-25T04:53:15.022504Z","shell.execute_reply.started":"2022-03-25T04:53:15.018798Z","shell.execute_reply":"2022-03-25T04:53:15.021668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize a CountVectorizer object: count_vectorizer\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n\n# Transforms the data into a bag of words\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\n# Print the first 10 features of the count_vec\nprint(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))\nprint(\"\\nEvery 3rd feature:\\n{}\".format(count_vec.get_feature_names_out()[::3]))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:53:28.379709Z","iopub.execute_input":"2022-03-25T04:53:28.380425Z","iopub.status.idle":"2022-03-25T04:53:28.401303Z","shell.execute_reply.started":"2022-03-25T04:53:28.38033Z","shell.execute_reply":"2022-03-25T04:53:28.399924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:54:35.763632Z","iopub.execute_input":"2022-03-25T04:54:35.764316Z","iopub.status.idle":"2022-03-25T04:54:35.769281Z","shell.execute_reply.started":"2022-03-25T04:54:35.764266Z","shell.execute_reply":"2022-03-25T04:54:35.768406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#N=2\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:54:52.644634Z","iopub.execute_input":"2022-03-25T04:54:52.644918Z","iopub.status.idle":"2022-03-25T04:54:52.652633Z","shell.execute_reply.started":"2022-03-25T04:54:52.644891Z","shell.execute_reply":"2022-03-25T04:54:52.651856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#N=3\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 3), max_df=1.0, min_df=1, max_features=None)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:54:54.933986Z","iopub.execute_input":"2022-03-25T04:54:54.934419Z","iopub.status.idle":"2022-03-25T04:54:54.941283Z","shell.execute_reply.started":"2022-03-25T04:54:54.934375Z","shell.execute_reply":"2022-03-25T04:54:54.940771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#N=4\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 4), max_df=1.0, min_df=1, max_features=None)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:54:57.153378Z","iopub.execute_input":"2022-03-25T04:54:57.153654Z","iopub.status.idle":"2022-03-25T04:54:57.162Z","shell.execute_reply.started":"2022-03-25T04:54:57.153619Z","shell.execute_reply":"2022-03-25T04:54:57.161337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Min_df\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=0.6, max_features=None)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:54:59.368419Z","iopub.execute_input":"2022-03-25T04:54:59.368953Z","iopub.status.idle":"2022-03-25T04:54:59.377598Z","shell.execute_reply.started":"2022-03-25T04:54:59.368915Z","shell.execute_reply":"2022-03-25T04:54:59.376482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Max_df\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=0.50, min_df=1, max_features=None)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:01.149543Z","iopub.execute_input":"2022-03-25T04:55:01.149993Z","iopub.status.idle":"2022-03-25T04:55:01.15811Z","shell.execute_reply.started":"2022-03-25T04:55:01.149955Z","shell.execute_reply":"2022-03-25T04:55:01.157258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Max_features\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=4)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:03.256581Z","iopub.execute_input":"2022-03-25T04:55:03.256866Z","iopub.status.idle":"2022-03-25T04:55:03.265303Z","shell.execute_reply.started":"2022-03-25T04:55:03.256838Z","shell.execute_reply":"2022-03-25T04:55:03.264668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tfidf\ntxt1 = ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']\ntf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\ntxt_fitted = tf.fit(txt1)\ntxt_transformed = txt_fitted.transform(txt1)\nprint (\"The text: \", txt1)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:05.456935Z","iopub.execute_input":"2022-03-25T04:55:05.457377Z","iopub.status.idle":"2022-03-25T04:55:05.468123Z","shell.execute_reply.started":"2022-03-25T04:55:05.457348Z","shell.execute_reply":"2022-03-25T04:55:05.467124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.vocabulary_","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:07.304607Z","iopub.execute_input":"2022-03-25T04:55:07.305149Z","iopub.status.idle":"2022-03-25T04:55:07.313167Z","shell.execute_reply.started":"2022-03-25T04:55:07.305114Z","shell.execute_reply":"2022-03-25T04:55:07.312174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idf = tf.idf_\nprint(dict(zip(txt_fitted.get_feature_names(), idf)))\nprint(\"\\nWe see that the tokens 'sang','she' have the most idf weight because \\\nthey are the only tokens that appear in one document only.\")\nprint(\"\\nThe token 'not' appears 6 times but it is also in all documents, so its idf is the lowest\")","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:09.02467Z","iopub.execute_input":"2022-03-25T04:55:09.024972Z","iopub.status.idle":"2022-03-25T04:55:09.032177Z","shell.execute_reply.started":"2022-03-25T04:55:09.024943Z","shell.execute_reply":"2022-03-25T04:55:09.031271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rr = dict(zip(txt_fitted.get_feature_names(), idf))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:11.25686Z","iopub.execute_input":"2022-03-25T04:55:11.25736Z","iopub.status.idle":"2022-03-25T04:55:11.261146Z","shell.execute_reply.started":"2022-03-25T04:55:11.257307Z","shell.execute_reply":"2022-03-25T04:55:11.260405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\ntoken_weight.columns=('token','weight')\ntoken_weight = token_weight.sort_values(by='weight', ascending=False)\ntoken_weight \n\nsns.barplot(x='token', y='weight', data=token_weight)            \nplt.title(\"Inverse Document Frequency(idf) per token\")\nfig=plt.gcf()\nfig.set_size_inches(10,5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:14.825593Z","iopub.execute_input":"2022-03-25T04:55:14.826333Z","iopub.status.idle":"2022-03-25T04:55:15.060839Z","shell.execute_reply.started":"2022-03-25T04:55:14.826284Z","shell.execute_reply":"2022-03-25T04:55:15.060142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get feature names\nfeature_names = np.array(tf.get_feature_names_out())\nsorted_by_idf = np.argsort(tf.idf_)\nprint(\"Features with lowest idf:\\n{}\".format(\n       feature_names[sorted_by_idf[:3]]))\nprint(\"\\nFeatures with highest idf:\\n{}\".format(\n       feature_names[sorted_by_idf[-3:]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:18.407632Z","iopub.execute_input":"2022-03-25T04:55:18.409982Z","iopub.status.idle":"2022-03-25T04:55:18.422619Z","shell.execute_reply.started":"2022-03-25T04:55:18.409935Z","shell.execute_reply":"2022-03-25T04:55:18.421476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1\\\n its weight is 0 because it does not appear there.\")\ntxt_transformed.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:20.136742Z","iopub.execute_input":"2022-03-25T04:55:20.137366Z","iopub.status.idle":"2022-03-25T04:55:20.145607Z","shell.execute_reply.started":"2022-03-25T04:55:20.137309Z","shell.execute_reply":"2022-03-25T04:55:20.144603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new1 = tf.transform(txt1)\n\n# find maximum value for each of the features over all of dataset:\nmax_val = new1.max(axis=0).toarray().ravel()\n\n#sort weights from smallest to biggest and extract their indices \nsort_by_tfidf = max_val.argsort()\n\nprint(\"Features with lowest tfidf:\\n{}\".format(\n      feature_names[sort_by_tfidf[:3]]))\n\nprint(\"\\nFeatures with highest tfidf: \\n{}\".format(\n      feature_names[sort_by_tfidf[-3:]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:25.013694Z","iopub.execute_input":"2022-03-25T04:55:25.014138Z","iopub.status.idle":"2022-03-25T04:55:25.022323Z","shell.execute_reply.started":"2022-03-25T04:55:25.014108Z","shell.execute_reply":"2022-03-25T04:55:25.021353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = pd.read_csv('../input/train.csv')\n# holdout = pd.read_csv('../input/test.csv').fillna(' ') \ntrain  =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\nholdout  =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n#test_target =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:29.204445Z","iopub.execute_input":"2022-03-25T04:55:29.205157Z","iopub.status.idle":"2022-03-25T04:55:33.655341Z","shell.execute_reply.started":"2022-03-25T04:55:29.205098Z","shell.execute_reply":"2022-03-25T04:55:33.654551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Lemmatizing and stemming gives us a lower ROC-AUC score. So we will only clean \\\\n's, Username, IP and http links\"\"\"\n\nstart_time=time.time()\n# remove '\\\\n'\ntrain['comment_text'] = train['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n    \n# remove any text starting with User... \ntrain['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n    \n# remove IP addresses or user IDs\ntrain['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n    \n#remove http links in the text\ntrain['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n\nend_time=time.time()\nprint(\"total time\",end_time-start_time)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:41.319558Z","iopub.execute_input":"2022-03-25T04:55:41.320086Z","iopub.status.idle":"2022-03-25T04:55:50.65511Z","shell.execute_reply.started":"2022-03-25T04:55:41.320054Z","shell.execute_reply":"2022-03-25T04:55:50.654171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove '\\\\n'\nholdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n    \n# remove any text starting with User... \nholdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n    \n# remove IP addresses or user IDs\nholdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n    \n#remove http links in the text\nholdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\nx = train['comment_text']\ny = train.iloc[:, 2:8]  ","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:55:53.123717Z","iopub.execute_input":"2022-03-25T04:55:53.124262Z","iopub.status.idle":"2022-03-25T04:56:01.572507Z","shell.execute_reply.started":"2022-03-25T04:55:53.124226Z","shell.execute_reply":"2022-03-25T04:56:01.571463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:56:09.473552Z","iopub.execute_input":"2022-03-25T04:56:09.473868Z","iopub.status.idle":"2022-03-25T04:56:09.526736Z","shell.execute_reply.started":"2022-03-25T04:56:09.473833Z","shell.execute_reply":"2022-03-25T04:56:09.526077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the vectorizer\nword_vectorizer = TfidfVectorizer(\n    stop_words='english',\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n    ngram_range=(1, 1),\n    max_features=30000)\n\n# fit and transform on it the training features\nword_vectorizer.fit(X_train)\nX_train_word_features = word_vectorizer.transform(X_train)\n\n#transform the test features to sparse matrix\ntest_features = word_vectorizer.transform(X_test)\n\n# transform the holdout text for submission at the end\nholdout_text = holdout['comment_text']\nholdout_word_features = word_vectorizer.transform(holdout_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:56:13.043299Z","iopub.execute_input":"2022-03-25T04:56:13.043953Z","iopub.status.idle":"2022-03-25T04:56:48.781271Z","shell.execute_reply.started":"2022-03-25T04:56:13.043908Z","shell.execute_reply":"2022-03-25T04:56:48.780264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']\n\nlosses = []\nauc = []\n\nfor class_name in class_names:\n    #call the labels one column at a time so we can run the classifier on them\n    train_target = y_train[class_name]\n    test_target = y_test[class_name]\n    classifier = LogisticRegression(solver='sag', C=10)\n\n    cv_loss = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='neg_log_loss'))\n    losses.append(cv_loss)\n    print('CV Log_loss score for class {} is {}'.format(class_name, cv_loss))\n\n    cv_score = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='accuracy'))\n    print('CV Accuracy score for class {} is {}'.format(class_name, cv_score))\n    \n    classifier.fit(X_train_word_features, train_target)\n    y_pred = classifier.predict(test_features)\n    y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n    auc_score = metrics.roc_auc_score(test_target, y_pred_prob)\n    auc.append(auc_score)\n    auc.append(auc_score)\n    print(\"CV ROC_AUC score {}\\n\".format(auc_score))\n    \n    print(confusion_matrix(test_target, y_pred))\n    print(classification_report(test_target, y_pred))\n\nprint('Total average CV Log_loss score is {}'.format(np.mean(losses)))\nprint('Total average CV ROC_AUC score is {}'.format(np.mean(auc)))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T04:56:58.10978Z","iopub.execute_input":"2022-03-25T04:56:58.110095Z","iopub.status.idle":"2022-03-25T04:59:55.266137Z","shell.execute_reply.started":"2022-03-25T04:56:58.110056Z","shell.execute_reply":"2022-03-25T04:59:55.265036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vectorize, Classify (with parameter tuning)\nx = train['comment_text']\ny = train.iloc[:, 2:8]  \nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)\nstart_time=time.time()\n\npipe = make_pipeline(TfidfVectorizer(\n                                    stop_words='english',\n                                    strip_accents='unicode',\n                                    token_pattern=r'\\w{1,}', #accept tokens that have 1 or more characters\n                                    analyzer='word',\n                                    ngram_range=(1, 1),\n                                    min_df=5),\n                     OneVsRestClassifier(LogisticRegression()))\nparam_grid = {'tfidfvectorizer__max_features': [10000, 30000],\n              'onevsrestclassifier__estimator__solver': ['liblinear', 'sag'],\n             } \ngrid = GridSearchCV(pipe, param_grid, cv=3, scoring='roc_auc')\n\ngrid3 = grid.fit(X_train, y_train)\n\nend_time=time.time()\nprint(\"total time\",end_time-start_time)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T05:00:23.44336Z","iopub.execute_input":"2022-03-25T05:00:23.44411Z","iopub.status.idle":"2022-03-25T05:04:07.106894Z","shell.execute_reply.started":"2022-03-25T05:00:23.44406Z","shell.execute_reply":"2022-03-25T05:04:07.105843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid3.best_estimator_.named_steps['onevsrestclassifier'])\nprint(grid3.best_estimator_.named_steps['tfidfvectorizer'])\ngrid3.best_params_\n\n\ngrid3.best_score_\npredicted_y_test = grid3.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T05:04:37.585122Z","iopub.execute_input":"2022-03-25T05:04:37.585446Z","iopub.status.idle":"2022-03-25T05:04:40.368921Z","shell.execute_reply.started":"2022-03-25T05:04:37.585411Z","shell.execute_reply":"2022-03-25T05:04:40.367967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Toxic Confusion Matrixs: \\n{}\".format(confusion_matrix(y_test['toxic'], predicted_y_test[:,0])))\nprint(\"\\nSevere Toxic: \\n{}\".format(confusion_matrix(y_test['severe_toxic'], predicted_y_test[:,1])))\nprint(\"\\nObscene: \\n{}\".format(confusion_matrix(y_test['obscene'], predicted_y_test[:,2])))\nprint(\"\\nThreat: \\n{}\".format(confusion_matrix(y_test['threat'], predicted_y_test[:,3])))\nprint(\"\\nInsult: \\n{}\".format(confusion_matrix(y_test['insult'], predicted_y_test[:,4])))\nprint(\"\\nIdentity Hate: \\n{}\".format(confusion_matrix(y_test['identity_hate'], predicted_y_test[:,5])))\n\nprint(\"\\nToxic Classification report: \\n{}\".format(classification_report(y_test['toxic'], predicted_y_test[:,0])))\nprint(\"\\nSevere Toxic: \\n{}\".format(classification_report(y_test['severe_toxic'], predicted_y_test[:,1])))\nprint(\"\\nObscene: \\n{}\".format(classification_report(y_test['obscene'], predicted_y_test[:,2])))\nprint(\"\\nThreat: \\n{}\".format(classification_report(y_test['threat'], predicted_y_test[:,3])))\nprint(\"\\nInsult: \\n{}\".format(classification_report(y_test['insult'], predicted_y_test[:,4])))\nprint(\"\\nIdentity Hate: \\n{}\".format(classification_report(y_test['identity_hate'], predicted_y_test[:,5])))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T05:04:56.489889Z","iopub.execute_input":"2022-03-25T05:04:56.490532Z","iopub.status.idle":"2022-03-25T05:04:56.771428Z","shell.execute_reply.started":"2022-03-25T05:04:56.49049Z","shell.execute_reply":"2022-03-25T05:04:56.770533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = grid3.best_estimator_.named_steps[\"tfidfvectorizer\"]\n# transform the training dataset:\nX_test_set = vectorizer.transform(X_test)\n\n\n# find maximum value for each of the features over dataset:\nmax_value = X_test_set.max(axis=0).toarray().ravel()\nsorted_by_tfidf = max_value.argsort()\n\n# get feature names\nfeature_names = np.array(vectorizer.get_feature_names_out())\n\nprint(\"Features with lowest tfidf:\\n{}\".format(\n      feature_names[sorted_by_tfidf[:20]]))\n\nprint(\"\\nFeatures with highest tfidf: \\n{}\".format(\n      feature_names[sorted_by_tfidf[-20:]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T05:05:00.58363Z","iopub.execute_input":"2022-03-25T05:05:00.583964Z","iopub.status.idle":"2022-03-25T05:05:03.438063Z","shell.execute_reply.started":"2022-03-25T05:05:00.583925Z","shell.execute_reply":"2022-03-25T05:05:03.437139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_by_idf = np.argsort(vectorizer.idf_)\nprint(\"Features with lowest idf:\\n{}\".format(\n       feature_names[sorted_by_idf[:100]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T05:05:06.58344Z","iopub.execute_input":"2022-03-25T05:05:06.584273Z","iopub.status.idle":"2022-03-25T05:05:06.59241Z","shell.execute_reply.started":"2022-03-25T05:05:06.584227Z","shell.execute_reply":"2022-03-25T05:05:06.591543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holdout_comments = holdout['comment_text']\n# holdoutComments are automatically transformed throguh the grid3 pipeline before prodicting probabilities\ntwod = grid3.predict_proba(holdout_comments)\nholdout_predictions = {}\nholdout_predictions = {'id': holdout['id']}  \n\nholdout_predictions['toxic']=twod[:,0]\nholdout_predictions['severe_toxic']=twod[:,1]\nholdout_predictions['obscene']=twod[:,2]\nholdout_predictions['threat']=twod[:,3]\nholdout_predictions['insult']=twod[:,4]\nholdout_predictions['identity_hate']=twod[:,5]\n    \nsubmission = pd.DataFrame.from_dict(holdout_predictions)\nsubmission = submission[['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']] #rearrange columns\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T05:05:08.913475Z","iopub.execute_input":"2022-03-25T05:05:08.914312Z","iopub.status.idle":"2022-03-25T05:05:23.551324Z","shell.execute_reply.started":"2022-03-25T05:05:08.914263Z","shell.execute_reply":"2022-03-25T05:05:23.550373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}