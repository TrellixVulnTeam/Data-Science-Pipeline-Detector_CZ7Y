{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom nltk import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nimport string\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nimport numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\ntest = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\")\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"maxlen = 200\nmax_features = 20000\n\ndef remove_punctuation(text):\n    no_punc = \"\".join([c for c in text if c not in string.punctuation])\n    return no_punc\n\nregtok = RegexpTokenizer(r'\\w+')\n\ndef remove_stop_words(text):\n    output = [c for c in text if c not in stopwords.words('english')]\n    return output\n\nlemmatizer = WordNetLemmatizer()\n\ndef word_lemmatizer(text):\n    lem_text = \" \".join([lemmatizer.lemmatize(i) for i in text])\n    return lem_text\n\nstemmer = PorterStemmer()\n\ndef word_stemmer(text):\n    stem_text = \" \".join([stemmer.stem(i) for i in text])\n    return stem_text\n    \ndef preprocess_text(data):\n    data = data.apply(lambda x: remove_punctuation(x))\n    data = data.apply(lambda x: regtok.tokenize(x.lower()))\n    data = data.apply(lambda x: remove_stop_words(x))\n    data = data.apply(lambda x: word_lemmatizer(x))\n    \n    return data\n\ndef prepare_data_for_training(train, test, tok):\n    train = tok.texts_to_sequences(train)\n    test = tok.texts_to_sequences(test)\n    word_index = tok.word_index\n    print('Found %s unique tokens.' % len(word_index))\n    \n    train = pad_sequences(train, maxlen=maxlen)\n    test = pad_sequences(test, maxlen=maxlen)\n\n    return train, test\n\ndef estimator(X_t, y):\n    inp = Input(shape=(maxlen, ))\n    embed_size = 128\n    x = Embedding(max_features, embed_size)(inp)\n    x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dropout(0.1)(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(6, activation=\"sigmoid\")(x)\n    \n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy', tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.AUC()])\n    batch_size = 32\n    epochs = 5\n    history = model.fit(X_t,y, batch_size=batch_size, epochs=epochs)\n    \n    return model, history\n\ndef evaluate_model(model, features, predictions):\n    results = model.evaluate(features, predictions)\n    \n    for i in range(len(model.metrics_names)):\n        print(model.metrics_names[i], results[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Without Preprocessing\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_train = train[\"comment_text\"]\nlist_sentences_test = test[\"comment_text\"]\n\n# # SHORT DATASET FOR DEBUGGING\n# list_sentences_train = list_sentences_train[0:500]\n# y = y[0:500]\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(list_sentences_train, y, test_size=0.2, random_state=42)\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\n\nsentences_train, sentences_test = prepare_data_for_training(X_train, X_test, tokenizer)\nmodel, hist = estimator(sentences_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\n# Evaluate on test data')\nevaluate_model(model, sentences_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With preprocessing.\nX_train_pp = preprocess_text(X_train)\nX_test_pp = preprocess_text(X_test)\n\ntokenizer_pp = Tokenizer(num_words=max_features)\ntokenizer_pp.fit_on_texts(list(X_train_pp))\n\nsentences_train_pp, sentences_test_pp = prepare_data_for_training(X_train_pp, X_test_pp, tokenizer_pp)\nmodel_pp, hist_pp = estimator(sentences_train_pp, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\n# Evaluate on test data')\nevaluate_model(model_pp, sentences_test_pp, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadEmbeddingMatrix_wv(word_index):\n    embed_size = 100\n    embeddings_index = dict()\n    \n    for word in wv_model.wv.vocab:\n        embeddings_index[word] = wv_model.wv[word]\n    print('Loaded %s word vectors.' % len(embeddings_index))\n    \n    gc.collect()\n    all_embs = np.stack(list(embeddings_index.values()))\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    \n    nb_words = len(word_index) + 1\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    gc.collect()\n  \n    embeddedCount = 0\n    for word, i in word_index.items():\n        i-=1\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n            embeddedCount+=1\n    print('total embedded:',embeddedCount,'common words')\n\n    del(embeddings_index)\n    gc.collect()\n\n    return embedding_matrix\n\ndef estimator_embedding(X_t, y, embedding_matrix, total_words):\n    inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n    x = Embedding(total_words, embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)\n    x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dropout(0.1)(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(6, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy', tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.AUC()])\n\n    batch_size = 32\n    epochs = 5\n    history = model.fit(X_t,y, batch_size=batch_size, epochs=epochs)\n    \n    return model, history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With word embeddings\nimport gc\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import RegexpTokenizer\n\n# Training custom word2vec model\nlist_sentences_tok = X_train.apply(lambda x: regtok.tokenize(x))\nwv_model = Word2Vec(list_sentences_tok, min_count=1)\nembedding_matrix = loadEmbeddingMatrix_wv(tokenizer.word_index)\n\nmodel_w2v, hist_w2v = estimator_embedding(sentences_train, y_train, embedding_matrix, len(tokenizer.word_index) + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\n# Evaluate on test data')\nevaluate_model(model_w2v, sentences_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training custom word2vec model - Preprocessed\nlist_sentences_tok_pp = X_train_pp.apply(lambda x: regtok.tokenize(x))\nwv_model = Word2Vec(list_sentences_tok_pp, min_count=1)\nembedding_matrix = loadEmbeddingMatrix_wv(tokenizer_pp.word_index)\n\nmodel_w2v, hist_w2v = estimator_embedding(sentences_train_pp, y_train, embedding_matrix, len(tokenizer_pp.word_index) + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\n# Evaluate on test data')\nevaluate_model(model_w2v, sentences_test_pp, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}