{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gensim.models.keyedvectors as word2vec\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().any(),test.isnull().any()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_train = train[\"comment_text\"]\nlist_sentences_test = test[\"comment_text\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_tokenized_train[:1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 200\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Embedding matrix for pre-trained embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadEmbeddingMatrix(typeToLoad):\n        if(typeToLoad==\"word2vec\"):\n            word2vecDict = word2vec.KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\", binary=True)\n            embed_size = 300\n            \n            embeddings_index = dict()\n            for word in word2vecDict.wv.vocab:\n                embeddings_index[word] = word2vecDict.word_vec(word)\n            print('Loaded %s word vectors.' % len(embeddings_index))\n            \n        gc.collect()\n        #We get the mean and standard deviation of the embedding weights so that we could maintain the \n        #same statistics for the rest of our own random generated weights. \n        all_embs = np.stack(list(embeddings_index.values()))\n        emb_mean,emb_std = all_embs.mean(), all_embs.std()\n        \n        nb_words = len(tokenizer.word_index)\n\n        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n        gc.collect()\n\n        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \n        #our own dictionary and loaded pretrained embedding. \n        embeddedCount = 0\n        for word, i in tokenizer.word_index.items():\n            i-=1\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None: \n                embedding_matrix[i] = embedding_vector\n                embeddedCount+=1\n        print('total embedded:',embeddedCount,'common words')\n        \n        del(embeddings_index)\n        gc.collect()\n        \n        return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = loadEmbeddingMatrix('word2vec')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bidirectional LSTM with Pre-trained embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\n\nx = GlobalMaxPool1D()(x)\n\nx = Dropout(0.1)(x)\n\nx = Dense(50, activation=\"relu\")(x)\n\nx = Dense(6, activation=\"sigmoid\")(x)\n\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 4\nhist = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_metrics(accuracy):\n    print(\"Simple cnn model performance\")\n    print('Accuracy: ', np.round(accuracy, 4))\n    print('\\n')\n    \nloss, accuracy = model.evaluate(X_t, y)\nprint_metrics(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Baseline_model"},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_size = 128\nx = Embedding(max_features, embed_size)(inp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n\nx = GlobalMaxPool1D()(x)\n\nx = Dropout(0.1)(x)\n\nx = Dense(50, activation=\"relu\")(x)\n\nx = Dropout(0.1)(x)\n\nx = Dense(6, activation=\"sigmoid\")(x)\n\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 1\nmodel.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_metrics(accuracy):\n    print(\"Simple cnn model performance\")\n    print('Accuracy: ', np.round(accuracy, 4))\n    print('\\n')\n    \nloss, accuracy = model.evaluate(X_t, y)\nprint_metrics(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def toxicity_level(string):\n    \"\"\"\n    Return toxicity probability based on inputed string.\n    \"\"\"\n    # Process string\n    new_string = [string]\n    new_string = tokenizer.texts_to_sequences(new_string)\n    new_string = pad_sequences(new_string, maxlen=200, padding='post', truncating='post')\n    \n    # Predict\n    prediction = model.predict(new_string)\n    \n    # Print output\n    print(\"Toxicity levels for '{}':\".format(string))\n    print('Toxic:         {:.0%}'.format(prediction[0][0]))\n    print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n    print('Obscene:       {:.0%}'.format(prediction[0][2]))\n    print('Threat:        {:.0%}'.format(prediction[0][3]))\n    print('Insult:        {:.0%}'.format(prediction[0][4]))\n    print('Identity Hate: {:.0%}'.format(prediction[0][5]))\n    print()\n    \n    return\n\ntoxicity_level('go jump off a bridge jerk')\ntoxicity_level('i will kill you')\ntoxicity_level('have a nice day')\ntoxicity_level('hola, como estas')\ntoxicity_level('hola mierda joder')\ntoxicity_level('fuck off!!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_losses = {\n'word2vec_loss': [0.084318213647104789,\n  0.057314205012433353,\n  0.051338302593577821,\n  0.047672802178572039],\n 'word2vec_val_loss': [0.063002561892695971,\n  0.057253835496480658,\n  0.051085027624451551,\n  0.049801279793734249],\n 'baseline_loss': [0.053304489498915865,\n  0.044464004045674786,\n  0.039113874651286124,\n  0.034330130175761134],\n 'baseline_val_loss': [0.049044281075148932,\n  0.047314051594414926,\n  0.047658757860843601,\n  0.050186043558285421]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochRange = np.arange(1,5,1)\nplt.plot(epochRange,all_losses['word2vec_loss'])\nplt.plot(epochRange,all_losses['baseline_loss'])\nplt.title('Training loss for different embeddings')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Word2Vec', 'Baseline'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochRange = np.arange(1,5,1)\nplt.plot(epochRange,all_losses['baseline_loss'])\nplt.plot(epochRange,all_losses['baseline_val_loss'])\nplt.title('Training Vs Validation loss for baseline model')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Training', 'Validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ((ax1, ax2)) = plt.subplots(1, 2, sharex='col', sharey='row',figsize=(7, 7))\n\nplt.title('Training Vs Validation loss for all embeddings')\nax1.plot(epochRange,all_losses['baseline_loss'])\nax1.plot(epochRange,all_losses['baseline_val_loss'])\nax1.set_title('Baseline')\nax1.set_ylim(0.03, 0.10)\n\nax2.plot(epochRange,all_losses['word2vec_loss'])\nax2.plot(epochRange,all_losses['word2vec_val_loss'])\nax2.set_title('Word2Vec')\nax2.set_ylim(0.03, 0.10)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}