{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! unzip ../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip \n! unzip ../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip \n! unzip ../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip \n! unzip ../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip","metadata":{"_uuid":"0d89fec7-5d44-4336-9da5-469346fa2def","_cell_guid":"b5e152ff-6613-4ecf-a299-d475fca64685","execution":{"iopub.status.busy":"2022-01-25T13:16:58.936985Z","iopub.execute_input":"2022-01-25T13:16:58.93778Z","iopub.status.idle":"2022-01-25T13:17:03.371869Z","shell.execute_reply.started":"2022-01-25T13:16:58.937679Z","shell.execute_reply":"2022-01-25T13:17:03.371105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install transformers \n# save the best weight ,means less loss valued trained batch \n# do some warm up step initially to make it more effiecient","metadata":{"_uuid":"a399aa52-641d-4b0b-ae94-428c6c4e20ff","_cell_guid":"76f4bbc5-95f6-4206-9caa-51d8fbd1b46b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-25T13:17:03.374241Z","iopub.execute_input":"2022-01-25T13:17:03.374816Z","iopub.status.idle":"2022-01-25T13:17:11.983383Z","shell.execute_reply.started":"2022-01-25T13:17:03.374776Z","shell.execute_reply":"2022-01-25T13:17:11.982567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nimport transformers\nfrom tqdm.notebook import tqdm","metadata":{"_uuid":"262ea2b4-8965-457a-ac3f-36f3956394aa","_cell_guid":"f6f21706-c522-41f0-8eee-ec41acbad32e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-25T13:17:11.984927Z","iopub.execute_input":"2022-01-25T13:17:11.985183Z","iopub.status.idle":"2022-01-25T13:17:17.547361Z","shell.execute_reply.started":"2022-01-25T13:17:11.985148Z","shell.execute_reply":"2022-01-25T13:17:17.546644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\") \nbert_model.trainable = False","metadata":{"_uuid":"6bdfa15e-a5d6-4c58-b132-4e686af50782","_cell_guid":"b49e404e-c101-479b-9c40-39d581a7c7e6","execution":{"iopub.status.busy":"2022-01-25T13:17:17.549399Z","iopub.execute_input":"2022-01-25T13:17:17.549634Z","iopub.status.idle":"2022-01-25T13:17:52.688601Z","shell.execute_reply.started":"2022-01-25T13:17:17.549604Z","shell.execute_reply":"2022-01-25T13:17:52.687886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=32\nmax_len=128\nEPOCHS=2","metadata":{"_uuid":"0a70ea60-c055-442d-a17b-92aa31a13d59","_cell_guid":"57e0530c-53c6-4440-8074-c165b041a7c5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-25T13:17:52.689985Z","iopub.execute_input":"2022-01-25T13:17:52.69026Z","iopub.status.idle":"2022-01-25T13:17:52.694831Z","shell.execute_reply.started":"2022-01-25T13:17:52.690221Z","shell.execute_reply":"2022-01-25T13:17:52.693843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"./train.csv\") ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:52.695998Z","iopub.execute_input":"2022-01-25T13:17:52.697207Z","iopub.status.idle":"2022-01-25T13:17:53.482385Z","shell.execute_reply.started":"2022-01-25T13:17:52.697182Z","shell.execute_reply":"2022-01-25T13:17:53.48162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count the individual data\nlabel_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] \npositive_comment=data[data[label_cols].sum(axis=1)==0]\ntoxic_comment=data[data[label_cols].sum(axis=1)>0]\nno_value=data[data[label_cols].sum(axis=1)=='']\n\n# print(len(positive_comment)) #143346\n# print(len(toxic_comment))# 16225\n# print(len(no_value))# 16225\n\n# random samling from positve commnet to tkae out of 14000 comment\n\n# toxic_comment=data[data['toxic']==1]\n# toxic_comment.shape\n## drop some toxic comment for better prediction\n\n# purely toxic\nconditions = [\n    (toxic_comment['toxic']==1)&\n    (toxic_comment['severe_toxic']==0) & (toxic_comment['obscene']==0)&\n    (toxic_comment['threat']==0) & (toxic_comment['insult']==0)&\n    (toxic_comment['identity_hate']==0)\n    ]\npurely_toxic=toxic_comment[conditions[0]]\nprint(purely_toxic.shape)\npurely_toxic.sample(frac = 1)\n# mixed toxic\nmixed_toxic=toxic_comment[toxic_comment['toxic']==1]\nprint(mixed_toxic.shape)\n\n# drop pure toxic\ntoxic_comment.drop(purely_toxic.index,inplace=True)\n\ndata=pd.concat([\n    toxic_comment,\n    purely_toxic.sample(frac=0.6),\n    positive_comment.sample(n=20_000)\n])\n\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:53.483738Z","iopub.execute_input":"2022-01-25T13:17:53.48408Z","iopub.status.idle":"2022-01-25T13:17:53.554428Z","shell.execute_reply.started":"2022-01-25T13:17:53.484028Z","shell.execute_reply":"2022-01-25T13:17:53.553604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nconditions = [\n    (data['toxic']==1)&\n    (data['severe_toxic']==0) & (data['obscene']==0)&\n    (data['threat']==0) & (data['insult']==0)&\n    (data['identity_hate']==0)\n    ]\ntoxic_comment=data[conditions[0]]\n\nconditions1 = [\n    (data['toxic']==0)&\n    (data['severe_toxic']==1) & (data['obscene']==0)&\n    (data['threat']==0) & (data['insult']==0)&\n    (data['identity_hate']==0)\n    ]\nsevere_toxic_comment=data[conditions1[0]]\n\nconditions2 = [\n    (data['toxic']==0)&\n    (data['severe_toxic']==0) & (data['obscene']==0)&\n    (data['threat']==0) & (data['insult']==1)&\n    (data['identity_hate']==0)\n    ]\ninsult_comment=data[conditions2[0]]\n\nconditions3 = [\n    (data['toxic']==0)&\n    (data['severe_toxic']==0) & (data['obscene']==1)&\n    (data['threat']==0) & (data['insult']==0)&\n    (data['identity_hate']==0)\n    ]\nobscene_comment=data[conditions3[0]]\n\nconditions4 = [\n    (data['toxic']==0)&\n    (data['severe_toxic']==0) & (data['obscene']==0)&\n    (data['threat']==1) & (data['insult']==0)&\n    (data['identity_hate']==0)\n    ]\nthreat_comment=data[conditions4[0]] \nprint(data.shape)\n\nlabel_bar = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult']\nlabel_data=[toxic_comment.shape[0],severe_toxic_comment.shape[0],obscene_comment.shape[0],threat_comment.shape[0],insult_comment.shape[0]]\n\nfig=plt.figure(figsize=(10,5)) \nplt.bar(label_bar,label_data,color=\"red\",width=0.3)\nplt.xlabel(\"Comment Sense\")\nplt.ylabel(\"count of Sense\")\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:53.555768Z","iopub.execute_input":"2022-01-25T13:17:53.556099Z","iopub.status.idle":"2022-01-25T13:17:53.769414Z","shell.execute_reply.started":"2022-01-25T13:17:53.556063Z","shell.execute_reply":"2022-01-25T13:17:53.768739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threat_comment.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:53.770656Z","iopub.execute_input":"2022-01-25T13:17:53.770888Z","iopub.status.idle":"2022-01-25T13:17:53.776319Z","shell.execute_reply.started":"2022-01-25T13:17:53.770855Z","shell.execute_reply":"2022-01-25T13:17:53.775476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add up a column positive\nconditions = [\n    (data['toxic']==0)&\n    (data['severe_toxic']==0) & (data['obscene']==0)&\n    (data['threat']==0) & (data['insult']==0)&\n    (data['identity_hate']==0)\n    ]\nvalues = [1]\n\n# data['positive']=np.where(data['toxic']==0 and data['severe_toxic']==0 and  and  and  and ,1,0)\ndata['positive']=np.select(conditions,values)\n\nlabel_cols.append(\"positive\")\n\nprint(data.shape)\npositive_comment=data[data[label_cols].sum(axis=1)==1]\nprint(len(positive_comment))","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:53.779791Z","iopub.execute_input":"2022-01-25T13:17:53.780066Z","iopub.status.idle":"2022-01-25T13:17:53.808359Z","shell.execute_reply.started":"2022-01-25T13:17:53.780018Z","shell.execute_reply":"2022-01-25T13:17:53.807702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"comment_text\"]=data[\"comment_text\"].map(lambda x:re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", x))\n# removing stop words\n","metadata":{"_uuid":"172c050f-530a-4cb9-bfeb-644d1d05e6bb","_cell_guid":"4e553d8c-573a-4b6a-a593-d18eed422cd3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-25T13:17:53.810248Z","iopub.execute_input":"2022-01-25T13:17:53.811292Z","iopub.status.idle":"2022-01-25T13:17:57.167663Z","shell.execute_reply.started":"2022-01-25T13:17:53.811256Z","shell.execute_reply":"2022-01-25T13:17:57.166928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sense_count_pd=pd.DataFrame(data[label_cols].value_counts()) \n# sense_count_pd","metadata":{"_uuid":"acb671f4-2798-46d8-a568-1f0fd86dd624","_cell_guid":"f5302b70-2145-4634-87fa-67fff1e297de","execution":{"iopub.status.busy":"2022-01-25T13:17:57.168977Z","iopub.execute_input":"2022-01-25T13:17:57.169242Z","iopub.status.idle":"2022-01-25T13:17:57.173844Z","shell.execute_reply.started":"2022-01-25T13:17:57.1692Z","shell.execute_reply":"2022-01-25T13:17:57.172685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels =  data[label_cols].values","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:57.175038Z","iopub.execute_input":"2022-01-25T13:17:57.175351Z","iopub.status.idle":"2022-01-25T13:17:57.184401Z","shell.execute_reply.started":"2022-01-25T13:17:57.175315Z","shell.execute_reply":"2022-01-25T13:17:57.183726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:57.185618Z","iopub.execute_input":"2022-01-25T13:17:57.185973Z","iopub.status.idle":"2022-01-25T13:17:57.193526Z","shell.execute_reply.started":"2022-01-25T13:17:57.185938Z","shell.execute_reply":"2022-01-25T13:17:57.192644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \ninput_sen=data[\"comment_text\"].values\n# print(input_sen)\ntrain_inputs,validation_inputs,train_labels,validation_labels=train_test_split(input_sen,labels,random_state=0,test_size=0.1) \n\n\nprint(train_inputs.shape)\nprint(train_labels.shape)\n\nprint(validation_inputs.shape)\nprint(validation_labels.shape)\n ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:57.195147Z","iopub.execute_input":"2022-01-25T13:17:57.195501Z","iopub.status.idle":"2022-01-25T13:17:57.933462Z","shell.execute_reply.started":"2022-01-25T13:17:57.195438Z","shell.execute_reply":"2022-01-25T13:17:57.932694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertSemanticDataGenerator(tf.keras.utils.Sequence): \n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n         \n        \n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token. \n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=128,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            pad_to_max_length=True,truncation=True,\n            return_tensors=\"tf\",\n        )   \n\n        bert_output = bert_model(**encoded)\n        \n        sequence_output = bert_output.last_hidden_state \n         \n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return sequence_output, labels\n        else:\n            return sequence_output\n\n    def on_epoch_end(self): \n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:57.934831Z","iopub.execute_input":"2022-01-25T13:17:57.935209Z","iopub.status.idle":"2022-01-25T13:17:57.946519Z","shell.execute_reply.started":"2022-01-25T13:17:57.935173Z","shell.execute_reply":"2022-01-25T13:17:57.945716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset=BertSemanticDataGenerator(train_inputs,train_labels,shuffle=True)\nvalidation_dataset=BertSemanticDataGenerator(validation_inputs,validation_labels,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:57.948187Z","iopub.execute_input":"2022-01-25T13:17:57.94859Z","iopub.status.idle":"2022-01-25T13:18:06.564007Z","shell.execute_reply.started":"2022-01-25T13:17:57.948553Z","shell.execute_reply":"2022-01-25T13:18:06.563259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[18]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:18:06.565333Z","iopub.execute_input":"2022-01-25T13:18:06.565561Z","iopub.status.idle":"2022-01-25T13:18:06.820664Z","shell.execute_reply.started":"2022-01-25T13:18:06.56553Z","shell.execute_reply":"2022-01-25T13:18:06.819953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_layer = tf.keras.layers.Input(shape=(128, 768), name=None)  \n\n\nbi_lstm_layer=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True))(input_layer)\nmax_pooling1=tf.keras.layers.GlobalMaxPooling1D()(bi_lstm_layer)\naverage_pooling1=tf.keras.layers.GlobalAveragePooling1D()(bi_lstm_layer)\nsch_pooling=tf.keras.layers.concatenate([max_pooling1,average_pooling1])\ndropout1=tf.keras.layers.Dropout(0.1)(sch_pooling)\n\n \n\nflat=tf.keras.layers.Flatten()(dropout1) \noutput = tf.keras.layers.Dense(7, activation=\"softmax\")(flat)\nmodel = tf.keras.models.Model(inputs=input_layer, outputs=output)\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:18:06.82195Z","iopub.execute_input":"2022-01-25T13:18:06.822288Z","iopub.status.idle":"2022-01-25T13:18:07.29045Z","shell.execute_reply.started":"2022-01-25T13:18:06.822248Z","shell.execute_reply":"2022-01-25T13:18:07.289725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.compile()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:18:07.291676Z","iopub.execute_input":"2022-01-25T13:18:07.292084Z","iopub.status.idle":"2022-01-25T13:18:07.296866Z","shell.execute_reply.started":"2022-01-25T13:18:07.292025Z","shell.execute_reply":"2022-01-25T13:18:07.295542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = model.fit(\n#     train_dataset,\n#     validation_data=validation_dataset,\n#     epochs=2\n# )","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:18:07.297897Z","iopub.execute_input":"2022-01-25T13:18:07.298891Z","iopub.status.idle":"2022-01-25T13:18:07.305258Z","shell.execute_reply.started":"2022-01-25T13:18:07.298855Z","shell.execute_reply":"2022-01-25T13:18:07.304525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom training loop \n## update at each train step\n## reset at the end of each batch\n\nimport time\n## defining a optimizer \noptimizer= tf.keras.optimizers.Adam(lr=2e-5)\n\n## defining loss function \nloss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n# ## mean loss define\ntrain_loss=tf.keras.metrics.Mean(name=\"train_loss\")\nvalidation_loss=tf.keras.metrics.Mean(name=\"validation_loss\")\n\nbest_validation_loss=tf.keras.metrics.Mean(name=\"best_validation_loss\")\n\n# Metric\n## dfining the accuracy metric to track our model accuracy.Here for 6 class we \n## have to declare 2d darray of row 6\ntrain_acc_metric=[tf.keras.metrics.CategoricalAccuracy() for i in range(len(label_cols))]\n\nval_acc_metric=[tf.keras.metrics.CategoricalAccuracy() for i in range(len(label_cols))]\n\n# actually from logits denoting the probability from our custom model layed for each label.It is being fetched before the softmax layer to calculate loss between actual and predicted\n\nbatch_size=32\nEPOCH=2\ntrain_dataset_size=60000\nvalidation_dataset_size=15000\n\n@tf.function\ndef train_step(model,x_train,label):\n    # Gradiane tape actually records the operation run in forward step\n    with tf.GradientTape() as tape:\n        #caluculate logits for comparison\n        logits_prob=model(x_train,training=True)\n        # calculate loss value \n        loss_value=loss_fn(label,logits_prob)\n    #calculate gradient of trainable variables against the loss\n    gradients=tape.gradient(loss_value,model.trainable_weights)\n    # update the gradient according to gradient descent\n    optimizer.apply_gradients(zip(gradients,model.trainable_weights))\n    # update the mean train ing loss\n    train_loss(loss_value)\n    # update accuracy metric for each of the 6 classes \n    for i,auc in enumerate(train_acc_metric):\n        auc.update_state(label[:,i],logits_prob[:,i])\n    return loss_value\n\n@tf.function\ndef validation_step(model,x_validation,label):\n    with tf.GradientTape() as tape:\n        validation_logit_prob=model(x_validation,training=False)\n        valid_loss=loss_fn(label,validation_logit_prob)\n        validation_loss(valid_loss)\n        for i,auc in enumerate(val_acc_metric):\n            auc.update_state(label[:,i],validation_logit_prob[:,i]) \n\ndef train_model(model,train_dataset,validation_dataset):\n    for epoch in range(EPOCHS):\n        print('\\n Epoch No %d\\n' % (epoch,))\n\n        ### training part ###\n        for step,(x_batch_train,labels) in enumerate(tqdm(train_dataset)):\n            training_loss=train_step(model,x_batch_train,labels)\n            \n            #log result at every 200 batches\n            if step%200==0:\n                print(f'\\nTrain Step: {epoch}, Loss: {train_loss.result()}')\n#                 print(\"Trainng loss at %d batch of data: %.4f\"%(step,float(training_loss)))\n                # training accuracy metric at end\n                for i, label_name in enumerate(label_cols):\n                    print(f\"{label_name} roc_auc {train_acc_metric[i].result()}\")\n                    # reset the accuracy metric after every epoch\n                    train_acc_metric[i].reset_states()\n            \n#         training_accuracy=train_acc_metric.result()\n#         print(\"\\nTraining accuracy after %d epoch : %.4f\"%(epoch,training_accuracy))\n#         train_acc_metric.reset_states()\n        \n        \n        ### validation part ###\n        for step,(x_batch_val,labels) in enumerate(tqdm(validation_dataset)):\n            validation_step(model,x_batch_val,labels)\n        print(f'\\n Validation Step: {epoch}, Loss: {validation_loss.result()}')\n        ## save best validate model weight\n#         if best_validation_loss<validation_loss:\n#             best_validation_loss=validation_loss\n#             model.save_weights('/kaggle/working/my_checkpoint')\n        for i, label_name in enumerate(label_cols):\n            print(f\"{label_name} roc_auc {val_acc_metric[i].result()}\") \n            val_acc_metric[i].reset_states()\n#         validation_acc=val_acc_metric.result()\n#         val_acc_metric.reset_states()\n#         print(\"\\n validation accuracy : %4.f\"%(validation_acc))\n\ntrain_model(model,train_dataset,validation_dataset)\nmodel.save(\"my_custom_train_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:18:07.30837Z","iopub.execute_input":"2022-01-25T13:18:07.308573Z","iopub.status.idle":"2022-01-25T13:26:11.861598Z","shell.execute_reply.started":"2022-01-25T13:18:07.308551Z","shell.execute_reply":"2022-01-25T13:26:11.860922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.models import load_model\n# model=load_model('../input/mymodel/my_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:26:11.862802Z","iopub.execute_input":"2022-01-25T13:26:11.863042Z","iopub.status.idle":"2022-01-25T13:26:11.866603Z","shell.execute_reply.started":"2022-01-25T13:26:11.863008Z","shell.execute_reply":"2022-01-25T13:26:11.86593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s4 = '''Your code looks like it's undecided about whether it's data or elementdata. I've assumed it's simply a typo.'''\ns5=\"In hindsight, I do apologize for my previous statement.\"\ns6=\"mother fucker bitch!!\"\nsentence_pairs = np.array([s6])\ntest_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\npro=model.predict(test_data) \nprint(np.asarray(pro))\nprint(label_cols[np.argmax(pro)])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:26:11.867925Z","iopub.execute_input":"2022-01-25T13:26:11.868384Z","iopub.status.idle":"2022-01-25T13:26:16.067541Z","shell.execute_reply.started":"2022-01-25T13:26:11.86835Z","shell.execute_reply":"2022-01-25T13:26:16.065907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the pre-defined bert model weights\n# bert_model.Trainable=True\n\n# train_dataset=BertSemanticDataGenerator(train_inputs,train_labels,shuffle=True)\n# validation_dataset=BertSemanticDataGenerator(validation_inputs,validation_labels,shuffle=False)\n\n# trained_history = model.fit(\n#     train_dataset,\n#     validation_data=validation_dataset,\n#     epochs=2\n# )","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:26:16.068995Z","iopub.execute_input":"2022-01-25T13:26:16.06926Z","iopub.status.idle":"2022-01-25T13:26:16.075331Z","shell.execute_reply.started":"2022-01-25T13:26:16.069225Z","shell.execute_reply":"2022-01-25T13:26:16.074633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df=pd.read_csv(\"./sample_submission.csv\",index_col='id')\ntest_df=pd.read_csv(\"./test.csv\")\nlabel_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','positive']\nconditions = [\n    (submission_df['toxic']==0.5)&\n    (submission_df['severe_toxic']==0.5) & (submission_df['obscene']==0.5)&\n    (submission_df['threat']==0.5) & (submission_df['insult']==0.5)&\n    (submission_df['identity_hate']==0.5)\n    ]\nvalues = [0.5] \nsubmission_df['positive']=np.select(conditions,values)\n\nprint(submission_df.head())\nprint(test_df.head())\n\n\ntest_bert_op=BertSemanticDataGenerator(test_df['comment_text'],None,include_targets=False,shuffle=True)\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:26:16.076695Z","iopub.execute_input":"2022-01-25T13:26:16.077204Z","iopub.status.idle":"2022-01-25T13:26:20.155717Z","shell.execute_reply.started":"2022-01-25T13:26:16.077002Z","shell.execute_reply":"2022-01-25T13:26:20.154904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,sen in enumerate(tqdm(test_bert_op)):\n    sample_ids = test_df.iloc[i*32:(i+1)*32]['id'] \n    pred=model.predict(sen)\n    submission_df.loc[sample_ids, label_cols] = pred ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:26:20.156813Z","iopub.execute_input":"2022-01-25T13:26:20.15707Z","iopub.status.idle":"2022-01-25T13:26:29.804499Z","shell.execute_reply.started":"2022-01-25T13:26:20.157019Z","shell.execute_reply":"2022-01-25T13:26:29.802887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:26:29.805532Z","iopub.status.idle":"2022-01-25T13:26:29.806424Z","shell.execute_reply.started":"2022-01-25T13:26:29.806182Z","shell.execute_reply":"2022-01-25T13:26:29.806206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_ids = test_df.iloc[0*32:(0+1)*32]['id'] \n# print(sample_ids)\n# submission_df.loc[sample_ids]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:26:29.807491Z","iopub.status.idle":"2022-01-25T13:26:29.808368Z","shell.execute_reply.started":"2022-01-25T13:26:29.808127Z","shell.execute_reply":"2022-01-25T13:26:29.808152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(bert_op)","metadata":{"_uuid":"8959229a-8e2d-4b3b-aac2-0fb6ce9a033d","_cell_guid":"54912915-4e18-4d75-91e8-026f9ce2e866","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-25T13:26:29.809381Z","iopub.status.idle":"2022-01-25T13:26:29.809899Z","shell.execute_reply.started":"2022-01-25T13:26:29.80967Z","shell.execute_reply":"2022-01-25T13:26:29.809693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # creating batched dataset\n# epochs=2\n# def create_batch_dataset(data,epochs=epochs,batch_size=batch_size,buffer_size=1000,train=True):\n#     dataset=tf.data.Dataset.from_tensor_slices(data)\n# #     print(dataset.as_numpy_iterator())\n#     if train:\n#         dataset=dataset.shuffle(buffer_size=buffer_size)\n#         # uses for shuffling the dataset.Select the first buffer_size element from dataset\n#     dataset=dataset.repeat(epochs)\n#     # just repeat the whole dataset\n#     dataset=dataset.batch(batch_size=batch_size)\n#     # devide the whole dataset into batch size and create an array of array\n#     if train:\n#         dataset=dataset.prefetch(1)\n#     #     It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two \n#     # elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2 elements (2 \n#     # batches, of 20 examples each).\n#     return dataset\n# train_dataset=create_batch_dataset((train_inputs,train_masks,train_labels),train=True)\n# validation_dataset=create_batch_dataset((validation_inputs,validation_masks,validation_labels),train=True)","metadata":{"_uuid":"d627f4cb-eafd-4f52-99d1-cacead47c1ca","_cell_guid":"0af7aedf-d747-4590-bec8-37a35f99eadb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-25T13:26:29.811291Z","iopub.status.idle":"2022-01-25T13:26:29.812075Z","shell.execute_reply.started":"2022-01-25T13:26:29.811819Z","shell.execute_reply":"2022-01-25T13:26:29.811844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/nkaenzig/bert-tensorflow-2-huggingface-transformers\n# https://www.kaggle.com/satyamkryadav/bert-model-96-77/notebook\n# https://github.com/tensorflow/models/blob/master/official/nlp/docs/tfhub.md","metadata":{"_uuid":"cf97dec3-abcc-41fb-92d7-bfa598eacba0","_cell_guid":"0d34a9f6-e683-4261-b26e-d5e31d2ef1e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-25T13:26:29.813378Z","iopub.status.idle":"2022-01-25T13:26:29.814264Z","shell.execute_reply.started":"2022-01-25T13:26:29.814011Z","shell.execute_reply":"2022-01-25T13:26:29.814035Z"},"trusted":true},"execution_count":null,"outputs":[]}]}