{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"5c5f4cc8865644748e11336736bbe584adebe7b1","_cell_guid":"8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\n\n\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\",\"r\") as z:\n    z.extractall(\".\")\n    \nwith zipfile.ZipFile(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\",\"r\") as z:\n    z.extractall(\".\")\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load training and test data","metadata":{"_uuid":"4f65d03ddbfd127307d3e415003346eb898b4d6b","_cell_guid":"80d61838-9025-4cba-bb0e-58175586b21b"}},{"cell_type":"code","source":"train_df=pd.read_csv(\"./train.csv\")\n\ntest_df=pd.read_csv(\"./test.csv\")\n","metadata":{"_uuid":"4e35cd5fcae1581dbd6bc51f14728e27fe63fe70","_cell_guid":"094fff47-db10-447c-965e-08056f718bde","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Examine the data (EDA)","metadata":{"_uuid":"89d1c9a4f9598427e8a20d66fa9e56796ad720f6","_cell_guid":"09986b08-eda6-4438-9cbe-52a61d8d57fa"}},{"cell_type":"code","source":"train_df.sample(5)","metadata":{"_uuid":"9e53b7599d707a9420a75c37c7ac6d05bed9df7b","_kg_hide-output":true,"_cell_guid":"c4c7137d-6bc7-4b41-b50a-e511883155e9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the training data, the comments are labelled as one or more of the six categories; toxic, severe toxic, obscene, threat, insult and identity hate. This is essentially a multi-label classification problem.","metadata":{"_uuid":"6c824d91ae1e801e1489e93e1a9932c8c0cb0e0a","_cell_guid":"40597119-1274-4d5b-a054-b4b17dbcbb36"}},{"cell_type":"code","source":"cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']","metadata":{"_uuid":"7f7f2581edb2f42a64812dec31622a011dceff80","_cell_guid":"7e29bebb-d9b7-44ab-a6ba-9f98e6507d5e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check missing values in numeric columns\ntrain_df.describe()","metadata":{"_uuid":"e4a226272e319458391e117e8fb7f16b17c4884f","_cell_guid":"ce00e980-da07-4412-ae4c-5152cc2036e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing numeric values. \nAs the mean values are very small (some way below 0.05), there would be many not labelled as positive in the six categories. From this I guess that there would be many comments which are not labelled in any of the six categories. Let's take a look.","metadata":{"_uuid":"474f4cf26190a2f2011ca0908052973fec1b1520","_cell_guid":"b2be6443-eb4e-4e12-81b3-faf2ea692ca4"}},{"cell_type":"code","source":"unlabelled_in_all = train_df[(train_df['toxic']!=1) & (train_df['severe_toxic']!=1) & (train_df['obscene']!=1) & \n                            (train_df['threat']!=1) & (train_df['insult']!=1) & (train_df['identity_hate']!=1)]\nprint('Percentage of unlabelled comments is ', len(unlabelled_in_all)/len(train_df)*100)","metadata":{"_uuid":"f0a52911f77fade7ece075fdc0d9df9433029eec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for any 'null' comment\nno_comment = train_df[train_df['comment_text'].isnull()]\nlen(no_comment)","metadata":{"_uuid":"99f1db4864c5c538f2a925d7b6733bb4b1c68707","_cell_guid":"997cc605-71a0-4ceb-a64b-e3e44c172aea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"_uuid":"964e57595bb6e87a47aa82557c9d63aae3c24bd0","_kg_hide-output":true,"_cell_guid":"af5ba625-bd5d-4ad3-948b-5641b10d62fb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_comment = test_df[test_df['comment_text'].isnull()]\nno_comment","metadata":{"_uuid":"c616bcb0dcf611679dcb5009def9d71d6727cd0e","_cell_guid":"6bd65c22-9c8a-4756-a7bf-16187a6044d1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All rows in the training and test data contain comments, so there's no need to clean up null fields.","metadata":{"_uuid":"ad7f31ff8035dc60d37c8070f8bbaa9e7afac32f","_cell_guid":"ecce8833-b5b4-40b4-a164-015b7380b8b1"}},{"cell_type":"code","source":"# let's see the total rows in train, test data and the numbers for the various categories\nprint('Total rows in test is {}'.format(len(test_df)))\nprint('Total rows in train is {}'.format(len(train_df)))\nprint(train_df[cols_target].sum())","metadata":{"_uuid":"e497fc6688e16602e3f49a11939f32324d295a8d","_cell_guid":"b3dcbd9f-dbb8-4a5a-96b2-b93882516e27","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned earlier, majority of the comments in the training data are not labelled in one or more of these categories.","metadata":{"_uuid":"2ff9ed83ba328872d446add97695285dc49f4165","_cell_guid":"981dff09-3acf-4014-b38a-22afc02a6654"}},{"cell_type":"code","source":"# Let's look at the character length for the rows in the training data and record these\ntrain_df['char_length'] = train_df['comment_text'].apply(lambda x: len(str(x)))","metadata":{"_uuid":"1e97432af65b6b75b436daabc83bdf57775a59c1","_cell_guid":"b26588a7-9a7f-4183-98b2-fb94a70bedaa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at the histogram plot for text length\nsns.set()\ntrain_df['char_length'].hist()\nplt.show()","metadata":{"_uuid":"448c3492fc2fe24f30bd7b97047d69f16b58ca2f","scrolled":true,"_cell_guid":"d5ac5111-5bba-44c9-a039-7bb01a5bfd59","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the text length are within 500 characters, with some up to 5,000 characters long.","metadata":{"_uuid":"d28a801b4057518f87e20c46a77f9dc8757ab944","_cell_guid":"5b482de7-5fd1-4ef7-b7b4-2abf84ebdc25"}},{"cell_type":"markdown","source":"Next, let's examine the correlations among the target variables.","metadata":{"_uuid":"e1e0ecc1df6989b0ee73874f273f0dbbfc4c9d5e","_cell_guid":"f16a287f-27d4-4afa-82a1-dd441c2fd36c"}},{"cell_type":"code","source":"data = train_df[cols_target]","metadata":{"_uuid":"fab22b3f850c80a10d665ae43ee09b5107a79887","_cell_guid":"64164a2c-770f-469d-9020-e91714a9b2a8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colormap = plt.cm.plasma\nplt.figure(figsize=(7,7))\nplt.title('Correlation of features & targets',y=1.05,size=14)\nsns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n           linecolor='white',annot=True)","metadata":{"_uuid":"58968a44d8fdb3b93ac57f1ca20f81ffb71d164f","scrolled":true,"_kg_hide-output":false,"_cell_guid":"7fc7803b-a7f1-414d-84fa-d1d2201c8bb7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, it looks like some of the labels are higher correlated, e.g. insult-obscene has the highest at 0.74, followed by toxic-obscene and toxic-insult.","metadata":{"_uuid":"4f9da9651b4e007088b19a06165d0727fb4f4e07","_cell_guid":"095fd031-32a0-400b-9357-af0c8f50dd6b"}},{"cell_type":"markdown","source":"What about the character length & distribution of the comment text in the test data?","metadata":{"_uuid":"b677d6a32b7b72e08ba3b46bce572a223db964de","_cell_guid":"caa00f5d-7e26-48cb-92b3-acc1f1de0aca"}},{"cell_type":"code","source":"test_df['char_length'] = test_df['comment_text'].apply(lambda x: len(str(x)))","metadata":{"_uuid":"36e4d00a5afc15e6b04ffa1e79421396d051f614","_cell_guid":"0993d06b-495e-428a-933a-6ffec6bdcef3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.hist(test_df['char_length'])\nplt.show()","metadata":{"_uuid":"1cc05875b88e54b5a1079eafc088207536dea2f3","_cell_guid":"828b9990-d78e-46c7-b011-1c66e6e6be79","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, the shape of character length distribution looks similar between the training data and the train data. For the training data, I guess the train data were clipped to 5,000 characters to facilitate the folks who did the labelling of the comment categories.","metadata":{"_uuid":"db69eb685cd1be44de2224c399cbdeabb5ceaa06","_cell_guid":"b2bbf246-f098-425a-99e3-2eb2126b975c"}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = STOPWORDS,\n                min_font_size = 10).generate(train_df.comment_text[3])\n  \n# plot the WordCloud image                       \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean up the comment text","metadata":{"_uuid":"d88d9cea99dbd77e81a5b3c4b9309df88b04550b","_cell_guid":"fdf9d2f6-d248-452f-8f94-562755e3a3f3"}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re\nlemmatizer = WordNetLemmatizer()\n\n\ndef preprocess(sentence):\n    sentence=str(sentence)\n    sentence = sentence.lower()\n    sentence=sentence.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', sentence)\n    rem_url=re.sub(r'http\\S+', '',cleantext)\n    rem_num = re.sub('[0-9]+', '', rem_url)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(rem_num)  \n    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n    \n    return \" \".join(filtered_words)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.comment_text.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean the comment_text in train_df \ntrain_df['comment_text']=train_df['comment_text'].map(lambda s:preprocess(s)) ","metadata":{"_uuid":"3944cb9d1311fd3729a7dfb13e8ac8c2309962e5","_cell_guid":"7b426494-c856-4cf4-b0d1-4162bc3aef5b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.comment_text[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv(\"cleaned_trained.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag,wordnet.NOUN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import wordnet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['comment_text'] =  train_df['comment_text'].apply(lambda sentence: ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv(\"clean_lemmatized_trained2.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.comment_text[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dfx = pd.read_csv(\"../input/jigsaw-toxic-comment-support/clean_lemmatized_trained2.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dfx.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['comment_text'] = test_df['comment_text'].map(lambda s:preprocess(s)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['comment_text'] = test_df['comment_text'].apply(lambda sentence: ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv(\"clean_lemmatized_test.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dfx = pd.read_csv(\"../input/jigsaw-toxic-comment-support/clean_lemmatized_test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_dfx.comment_text\ntest_X = test_dfx.comment_text","metadata":{"_uuid":"061d59552c6ef83bea8ecf9ffbf203286aeab6f8","_cell_guid":"49547fd7-9633-4f6a-bd46-84d8966f1e8b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape, test_X.shape)","metadata":{"_uuid":"6b1e91df6163a437c5f55e9c4de88dc11a89e5ba","_cell_guid":"083686b8-483a-4fbd-8584-cb9da8357c57","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vectorize the data","metadata":{"_uuid":"cc4c2aeef221f5a97e1bd5ea1154052172175351","_cell_guid":"b2d898ae-79bc-48b8-8544-fb077f876c67"}},{"cell_type":"code","source":"# import and instantiate TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=1000,stop_words='english')\nvect","metadata":{"_uuid":"7b7adf15d16408eb99689884906d0d687c2f8407","_cell_guid":"9be5a4f2-0e85-4f9a-ac00-b8e9916116cb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# learn the vocabulary in the training data, then use it to create a document-term matrix\nX_dtm = vect.fit_transform(X.values.astype('U'))\n# examine the document-term matrix created from X_train\nX_dtm","metadata":{"_uuid":"b755b1d4db58eeb5b0ab668d1aaf4a651d3de441","_cell_guid":"283c1d48-c267-431b-834e-37c8d9222b3c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lendata = []\nfor i in range(159571):\n    lendata.append(X_dtm[i].size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(lendata)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.hist(lendata, bins=10)\nplt.hist(lendata, bins=10,\n         histtype='stepfilled', color='steelblue',\n         edgecolor='none');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nbins = [0,2,4,8,16,32,64,128] # your bins\n\nhist, bin_edges = np.histogram(lendata,bins) # make the histogram\n\nfig,ax = plt.subplots()\n\n# Plot the histogram heights against integers on the x axis\nax.bar(range(len(hist)),hist,width=1) \n\n# Set the ticks to the middle of the bars\n#ax.set_xticks([0.5+i for i,j in enumerate(hist)])\n\n# Set the xticklabels to a string that tells us what the bin edges were\nax.set_xticklabels(['{} - {}'.format(bins[i],bins[i+1]) for i,j in enumerate(hist)])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(X_dtm.toarray(),columns=vect.get_feature_names())\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key in list(vect.vocabulary_)[:10]:\n    print(key, vect.vocabulary_[key])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform the test data using the earlier fitted vocabulary, into a document-term matrix\ntest_X_dtm = vect.transform(test_X.values.astype('U'))\n# examine the document-term matrix from X_test\ntest_X_dtm","metadata":{"_uuid":"408301ccb78e3f4056a6d2ebbd239594d1a59da0","_cell_guid":"54050711-560a-47cf-b4f9-2fbaf59bc2e4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}