{"cells":[{"metadata":{},"cell_type":"markdown","source":"### reference\n* \"Writing code for NLP Research\" Tutorial at EMNLP 2018: https://docs.google.com/presentation/d/17NoJY2SnC2UMbVegaRCWA7Oca7UCZ3vHnMqBV4SUayc/\n\n* “An In-Depth Tutorial to AllenNLP (From Basics to ELMo and BERT)”: http://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/\n\n* \"About my 0.9872 single model\": https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644\n\n* \"Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms\": https://arxiv.org/abs/1805.09843 ACL 2018\n\n* \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\": https://arxiv.org/abs/1810.04805 NAACL 2019"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\n\nfrom allennlp.common.params import Params\nfrom allennlp.common.util import prepare_environment, dump_metrics\nfrom allennlp.data.iterators import BasicIterator, BucketIterator\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.token_indexers import PretrainedBertIndexer\nfrom allennlp.data.tokenizers import WordTokenizer\nfrom allennlp.data.tokenizers.word_splitter import BertBasicWordSplitter\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\nfrom allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.training import Trainer\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert.optimization import BertAdam\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.optim as optim\n\n# from utility script\nfrom swemencoder import SWEMEncoder\nfrom toxiccommentclassificationreader import ToxicCommentClassificationReader\nfrom toxiccommentpredictor import ToxicCommentPredictor\nfrom toxicbaseclassifier import ToxicBaseClassifier\nfrom toxicbertclassifier import ToxicBertClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam.py\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:            \n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://github.com/lonePatient/lookahead_pytorch/blob/master/optimizer.py\nimport itertools as it\nfrom torch.optim import Optimizer\n\nclass Lookahead(Optimizer):\n    def __init__(self, base_optimizer,alpha=0.5, k=6):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        self.optimizer = base_optimizer\n        self.param_groups = self.optimizer.param_groups\n        self.alpha = alpha\n        self.k = k\n        for group in self.param_groups:\n            group[\"step_counter\"] = 0\n        self.slow_weights = [[p.clone().detach() for p in group['params']]\n                                for group in self.param_groups]\n\n        for w in it.chain(*self.slow_weights):\n            w.requires_grad = False\n            \n        self.state = base_optimizer.state\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        loss = self.optimizer.step()\n        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n            group['step_counter'] += 1\n            if group['step_counter'] % self.k != 0:\n                continue\n            for p,q in zip(group['params'],slow_weights):\n                if p.grad is None:\n                    continue\n                q.data.add_(self.alpha,p.data - q.data)\n                p.data.copy_(q.data)\n                self.state = self.optimizer.state\n        return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sets random seeds for reproducible experiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = Params({})\nprepare_environment(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reader = ToxicCommentClassificationReader(token_indexers={\n    \"tokens1\": SingleIdTokenIndexer(),\n    \"tokens2\": SingleIdTokenIndexer(),\n})\nall_dataset = reader.read('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntrain_dataset, validation_dataset = train_test_split(all_dataset, test_size=0.2, random_state=11)\nvocab = Vocabulary.from_instances(train_dataset, min_count={'tokens': 3})\niterator = BucketIterator(\n    batch_size=512,\n    sorting_keys=[(\"tokens\", \"num_tokens\")],\n)\niterator.index_with(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_params = Params({\n    'pretrained_file': '../input/glove-stanford/glove.twitter.27B.200d.txt',\n    'embedding_dim': 200,\n    'trainable': False\n})\nfasttext_params = Params({\n    'pretrained_file': '../input/fatsttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec',\n    'embedding_dim': 300,\n    'trainable': False\n})\nglove_embedding = Embedding.from_params(vocab, glove_params)\nfasttext_embedding = Embedding.from_params(vocab, fasttext_params)\nword_embeddings = BasicTextFieldEmbedder({\"tokens1\": glove_embedding, \"tokens2\": fasttext_embedding})\nseq2vec_encoder = BagOfEmbeddingsEncoder(embedding_dim=word_embeddings.get_output_dim())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try simple bag of embedding model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ToxicBaseClassifier(\n    text_field_embedder=word_embeddings,\n    seq2seq_encoder=None,\n    seq2vec_encoder=seq2vec_encoder,\n    dropout=0.5,\n    num_labels=6,\n    vocab=vocab\n)\nmodel.cuda()\n\ntrainer = Trainer(\n    model=model,\n    optimizer=Lookahead(RAdam(model.parameters())),\n    iterator=iterator,\n    train_dataset=train_dataset,\n    validation_dataset=validation_dataset,\n    cuda_device=0,\n    num_epochs=1000,\n    grad_norm=5.0,\n    grad_clipping=1.0,\n    patience=10\n)\nmetrics = trainer.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('metrics: {}'.format(metrics))\nprint('best_validation_loss: {}'.format(metrics['best_validation_loss']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"seq2vec_encoder: BagOfEmbeddingsEncoder -> SWEMEncoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"seq2vec_encoder = SWEMEncoder(embedding_dim=word_embeddings.get_output_dim())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ToxicBaseClassifier(\n    text_field_embedder=word_embeddings,\n    seq2seq_encoder=None,\n    seq2vec_encoder=seq2vec_encoder,\n    dropout=0.5,\n    num_labels=6,\n    vocab=vocab\n)\nmodel.cuda()\n\ntrainer = Trainer(\n    model=model,\n    optimizer=Lookahead(RAdam(model.parameters())),\n    iterator=iterator,\n    train_dataset=train_dataset,\n    validation_dataset=validation_dataset,\n    cuda_device=0,\n    num_epochs=1000,\n    grad_norm=5.0,\n    grad_clipping=1.0,\n    patience=10\n)\nmetrics = trainer.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('metrics: {}'.format(metrics))\nprint('best_validation_loss: {}'.format(metrics['best_validation_loss']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"seq2seq_encoder: None -> Bidirectional LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm = torch.nn.LSTM(\n    bidirectional=True,\n    input_size=word_embeddings.get_output_dim(),\n    hidden_size=40,\n    num_layers=2,\n    batch_first=True\n)\nseq2seq_encoder = PytorchSeq2SeqWrapper(lstm)\nseq2vec_encoder = SWEMEncoder(embedding_dim=seq2seq_encoder.get_output_dim())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ToxicBaseClassifier(\n    text_field_embedder=word_embeddings,\n    seq2seq_encoder=seq2seq_encoder,\n    seq2vec_encoder=seq2vec_encoder,\n    dropout=0.5,\n    num_labels=6,\n    vocab=vocab\n)\nmodel.cuda()\n\ntrainer = Trainer(\n    model=model,\n    optimizer=Lookahead(RAdam(model.parameters())),\n    iterator=iterator,\n    train_dataset=train_dataset,\n    validation_dataset=validation_dataset,\n    cuda_device=0,\n    num_epochs=1000,\n    grad_norm=5.0,\n    grad_clipping=1.0,\n    patience=10\n)\nmetrics = trainer.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('metrics: {}'.format(metrics))\nprint('best_validation_loss: {}'.format(metrics['best_validation_loss']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try BERT base model"},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_MODEL_PATH = '../input/bertpretrained/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nWORK_DIR = \"../working/\"\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin'\n)\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_indexer = PretrainedBertIndexer(\n    pretrained_model=BERT_MODEL_PATH,\n    max_pieces=128,\n    do_lowercase=True,\n)\n\ntokenizer = WordTokenizer(word_splitter=BertBasicWordSplitter())\n\nreader = ToxicCommentClassificationReader(\n    tokenizer=tokenizer,\n    token_indexers={\"bert\": token_indexer}\n)\n\nall_dataset = reader.read('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntrain_dataset, validation_dataset = train_test_split(all_dataset, test_size=0.2, random_state=11)\n\niterator = BucketIterator(\n    batch_size=64,\n    sorting_keys=[(\"tokens\", \"num_tokens\")],\n)\n\nvocab = Vocabulary()\niterator.index_with(vocab)\n\nmodel = ToxicBertClassifier(\n    vocab=vocab,\n    bert_model=WORK_DIR,\n    num_labels=6\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.cuda()\n\ntrainer = Trainer(model=model,\n                  optimizer=Lookahead(RAdam(model.parameters())),\n                  iterator=iterator,\n                  train_dataset=train_dataset,\n                  validation_dataset=validation_dataset,\n                  cuda_device=0,\n                  num_epochs=1000,\n                  grad_norm=5.0,\n                  grad_clipping=1.0,\n                  patience=3)\nmetrics = trainer.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('metrics: {}'.format(metrics))\nprint('best_validation_loss: {}'.format(metrics['best_validation_loss']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = reader.read('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\nseq_iterator = BasicIterator(batch_size=64)\nseq_iterator.index_with(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor = ToxicCommentPredictor(model, seq_iterator, cuda_device=0)\ntest_preds = predictor.predict(test_dataset)\nsubmission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = test_preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}