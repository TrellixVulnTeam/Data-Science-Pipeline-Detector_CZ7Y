{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\n\nimport pickle \n#import mglearn\nimport time\n\n\nfrom nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\nimport nltk\nfrom nltk import Text\nfrom nltk.tokenize import regexp_tokenize\nfrom nltk.tokenize import word_tokenize  \nfrom nltk.tokenize import sent_tokenize \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-12T05:04:32.204968Z","iopub.execute_input":"2022-03-12T05:04:32.205311Z","iopub.status.idle":"2022-03-12T05:04:33.948591Z","shell.execute_reply.started":"2022-03-12T05:04:32.205222Z","shell.execute_reply":"2022-03-12T05:04:33.947925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt = [\"He is ::having a great Time, at the park time?\",\n       \"She, unlike most women, is a big player on the park's grass.\",\n       \"she can't be going\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:33.949795Z","iopub.execute_input":"2022-03-12T05:04:33.950353Z","iopub.status.idle":"2022-03-12T05:04:33.953421Z","shell.execute_reply.started":"2022-03-12T05:04:33.950321Z","shell.execute_reply":"2022-03-12T05:04:33.952761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize a CountVectorizer object: count_vectorizer\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n\n# Transforms the data into a bag of words\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\n# Print the first 10 features of the count_vec\nprint(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))\nprint(\"\\nEvery 3rd feature:\\n{}\".format(count_vec.get_feature_names_out()[::3]))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:33.954487Z","iopub.execute_input":"2022-03-12T05:04:33.955028Z","iopub.status.idle":"2022-03-12T05:04:33.978208Z","shell.execute_reply.started":"2022-03-12T05:04:33.954995Z","shell.execute_reply":"2022-03-12T05:04:33.977271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:33.98043Z","iopub.execute_input":"2022-03-12T05:04:33.980933Z","iopub.status.idle":"2022-03-12T05:04:33.987093Z","shell.execute_reply.started":"2022-03-12T05:04:33.980886Z","shell.execute_reply":"2022-03-12T05:04:33.986206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"N gram","metadata":{}},{"cell_type":"code","source":"#N=2\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:33.988116Z","iopub.execute_input":"2022-03-12T05:04:33.988433Z","iopub.status.idle":"2022-03-12T05:04:34.00265Z","shell.execute_reply.started":"2022-03-12T05:04:33.988404Z","shell.execute_reply":"2022-03-12T05:04:34.001606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#N=3\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 3), max_df=1.0, min_df=1, max_features=None)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.003943Z","iopub.execute_input":"2022-03-12T05:04:34.004341Z","iopub.status.idle":"2022-03-12T05:04:34.021832Z","shell.execute_reply.started":"2022-03-12T05:04:34.004303Z","shell.execute_reply":"2022-03-12T05:04:34.020552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#N=4\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 4), max_df=1.0, min_df=1, max_features=None)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.02305Z","iopub.execute_input":"2022-03-12T05:04:34.023519Z","iopub.status.idle":"2022-03-12T05:04:34.035037Z","shell.execute_reply.started":"2022-03-12T05:04:34.023481Z","shell.execute_reply":"2022-03-12T05:04:34.034304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Min_df\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=0.6, max_features=None)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.036039Z","iopub.execute_input":"2022-03-12T05:04:34.03662Z","iopub.status.idle":"2022-03-12T05:04:34.046387Z","shell.execute_reply.started":"2022-03-12T05:04:34.03658Z","shell.execute_reply":"2022-03-12T05:04:34.045668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Max_df\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=0.50, min_df=1, max_features=None)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.047636Z","iopub.execute_input":"2022-03-12T05:04:34.048132Z","iopub.status.idle":"2022-03-12T05:04:34.060127Z","shell.execute_reply.started":"2022-03-12T05:04:34.048097Z","shell.execute_reply":"2022-03-12T05:04:34.059295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Max_features\ncount_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=4)\n\ncount_train = count_vec.fit(txt)\nbag_of_words = count_vec.transform(txt)\n\nprint(count_vec.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.063169Z","iopub.execute_input":"2022-03-12T05:04:34.063922Z","iopub.status.idle":"2022-03-12T05:04:34.074797Z","shell.execute_reply.started":"2022-03-12T05:04:34.063886Z","shell.execute_reply":"2022-03-12T05:04:34.073778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tfidf\ntxt1 = ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']\ntf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\ntxt_fitted = tf.fit(txt1)\ntxt_transformed = txt_fitted.transform(txt1)\nprint (\"The text: \", txt1)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.076146Z","iopub.execute_input":"2022-03-12T05:04:34.07856Z","iopub.status.idle":"2022-03-12T05:04:34.089669Z","shell.execute_reply.started":"2022-03-12T05:04:34.078519Z","shell.execute_reply":"2022-03-12T05:04:34.088746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.vocabulary_","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.090938Z","iopub.execute_input":"2022-03-12T05:04:34.091337Z","iopub.status.idle":"2022-03-12T05:04:34.102048Z","shell.execute_reply.started":"2022-03-12T05:04:34.091306Z","shell.execute_reply":"2022-03-12T05:04:34.100957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idf = tf.idf_\nprint(dict(zip(txt_fitted.get_feature_names(), idf)))\nprint(\"\\nWe see that the tokens 'sang','she' have the most idf weight because \\\nthey are the only tokens that appear in one document only.\")\nprint(\"\\nThe token 'not' appears 6 times but it is also in all documents, so its idf is the lowest\")","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.103562Z","iopub.execute_input":"2022-03-12T05:04:34.104439Z","iopub.status.idle":"2022-03-12T05:04:34.115317Z","shell.execute_reply.started":"2022-03-12T05:04:34.1044Z","shell.execute_reply":"2022-03-12T05:04:34.114187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rr = dict(zip(txt_fitted.get_feature_names(), idf))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.117229Z","iopub.execute_input":"2022-03-12T05:04:34.117656Z","iopub.status.idle":"2022-03-12T05:04:34.130635Z","shell.execute_reply.started":"2022-03-12T05:04:34.11761Z","shell.execute_reply":"2022-03-12T05:04:34.129712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\ntoken_weight.columns=('token','weight')\ntoken_weight = token_weight.sort_values(by='weight', ascending=False)\ntoken_weight \n\nsns.barplot(x='token', y='weight', data=token_weight)            \nplt.title(\"Inverse Document Frequency(idf) per token\")\nfig=plt.gcf()\nfig.set_size_inches(10,5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.131794Z","iopub.execute_input":"2022-03-12T05:04:34.132237Z","iopub.status.idle":"2022-03-12T05:04:34.350558Z","shell.execute_reply.started":"2022-03-12T05:04:34.132204Z","shell.execute_reply":"2022-03-12T05:04:34.349549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get feature names\nfeature_names = np.array(tf.get_feature_names_out())\nsorted_by_idf = np.argsort(tf.idf_)\nprint(\"Features with lowest idf:\\n{}\".format(\n       feature_names[sorted_by_idf[:3]]))\nprint(\"\\nFeatures with highest idf:\\n{}\".format(\n       feature_names[sorted_by_idf[-3:]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.352184Z","iopub.execute_input":"2022-03-12T05:04:34.352563Z","iopub.status.idle":"2022-03-12T05:04:34.358294Z","shell.execute_reply.started":"2022-03-12T05:04:34.352528Z","shell.execute_reply":"2022-03-12T05:04:34.357656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1\\\n its weight is 0 because it does not appear there.\")\ntxt_transformed.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.359266Z","iopub.execute_input":"2022-03-12T05:04:34.360089Z","iopub.status.idle":"2022-03-12T05:04:34.374475Z","shell.execute_reply.started":"2022-03-12T05:04:34.36005Z","shell.execute_reply":"2022-03-12T05:04:34.373801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the more times a token appears in a document, the more weight it will have. However, the more documents the token appears in, it is 'penalized' and the weight is diminished. For example, the weight for token 'not' is 4, but if it did not appear in all documents (that is, only in one document) its weight would have been 8.3\nTF-IDF - Maximum token value throughout the whole dataset","metadata":{}},{"cell_type":"code","source":"new1 = tf.transform(txt1)\n\n# find maximum value for each of the features over all of dataset:\nmax_val = new1.max(axis=0).toarray().ravel()\n\n#sort weights from smallest to biggest and extract their indices \nsort_by_tfidf = max_val.argsort()\n\nprint(\"Features with lowest tfidf:\\n{}\".format(\n      feature_names[sort_by_tfidf[:3]]))\n\nprint(\"\\nFeatures with highest tfidf: \\n{}\".format(\n      feature_names[sort_by_tfidf[-3:]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.3756Z","iopub.execute_input":"2022-03-12T05:04:34.376443Z","iopub.status.idle":"2022-03-12T05:04:34.38723Z","shell.execute_reply.started":"2022-03-12T05:04:34.376409Z","shell.execute_reply":"2022-03-12T05:04:34.386334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = pd.read_csv('../input/train.csv')\n# holdout = pd.read_csv('../input/test.csv').fillna(' ') \ntrain  =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\nholdout  =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n#test_target =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:34.388316Z","iopub.execute_input":"2022-03-12T05:04:34.388546Z","iopub.status.idle":"2022-03-12T05:04:38.859032Z","shell.execute_reply.started":"2022-03-12T05:04:34.38852Z","shell.execute_reply":"2022-03-12T05:04:38.858098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Lemmatizing and stemming gives us a lower ROC-AUC score. So we will only clean \\\\n's, Username, IP and http links\"\"\"\n\nstart_time=time.time()\n# remove '\\\\n'\ntrain['comment_text'] = train['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n    \n# remove any text starting with User... \ntrain['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n    \n# remove IP addresses or user IDs\ntrain['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n    \n#remove http links in the text\ntrain['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n\nend_time=time.time()\nprint(\"total time\",end_time-start_time)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:38.86025Z","iopub.execute_input":"2022-03-12T05:04:38.860632Z","iopub.status.idle":"2022-03-12T05:04:43.571804Z","shell.execute_reply.started":"2022-03-12T05:04:38.860601Z","shell.execute_reply":"2022-03-12T05:04:43.570925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove '\\\\n'\nholdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n    \n# remove any text starting with User... \nholdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n    \n# remove IP addresses or user IDs\nholdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n    \n#remove http links in the text\nholdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\nx = train['comment_text']\ny = train.iloc[:, 2:8]  ","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:43.572973Z","iopub.execute_input":"2022-03-12T05:04:43.573201Z","iopub.status.idle":"2022-03-12T05:04:47.826262Z","shell.execute_reply.started":"2022-03-12T05:04:43.573171Z","shell.execute_reply":"2022-03-12T05:04:47.825406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:47.827399Z","iopub.execute_input":"2022-03-12T05:04:47.827697Z","iopub.status.idle":"2022-03-12T05:04:47.883079Z","shell.execute_reply.started":"2022-03-12T05:04:47.827667Z","shell.execute_reply":"2022-03-12T05:04:47.882106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the vectorizer\nword_vectorizer = TfidfVectorizer(\n    stop_words='english',\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n    ngram_range=(1, 1),\n    max_features=30000)\n\n# fit and transform on it the training features\nword_vectorizer.fit(X_train)\nX_train_word_features = word_vectorizer.transform(X_train)\n\n#transform the test features to sparse matrix\ntest_features = word_vectorizer.transform(X_test)\n\n# transform the holdout text for submission at the end\nholdout_text = holdout['comment_text']\nholdout_word_features = word_vectorizer.transform(holdout_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:04:58.226431Z","iopub.execute_input":"2022-03-12T05:04:58.226796Z","iopub.status.idle":"2022-03-12T05:05:25.347026Z","shell.execute_reply.started":"2022-03-12T05:04:58.226761Z","shell.execute_reply":"2022-03-12T05:05:25.346218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']\n\nlosses = []\nauc = []\n\nfor class_name in class_names:\n    #call the labels one column at a time so we can run the classifier on them\n    train_target = y_train[class_name]\n    test_target = y_test[class_name]\n    classifier = LogisticRegression(solver='sag', C=10)\n\n    cv_loss = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='neg_log_loss'))\n    losses.append(cv_loss)\n    print('CV Log_loss score for class {} is {}'.format(class_name, cv_loss))\n\n    cv_score = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='accuracy'))\n    print('CV Accuracy score for class {} is {}'.format(class_name, cv_score))\n    \n    classifier.fit(X_train_word_features, train_target)\n    y_pred = classifier.predict(test_features)\n    y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n    auc_score = metrics.roc_auc_score(test_target, y_pred_prob)\n    auc.append(auc_score)\n    print(\"CV ROC_AUC score {}\\n\".format(auc_score))\n    \n    print(confusion_matrix(test_target, y_pred))\n    print(classification_report(test_target, y_pred))\n\nprint('Total average CV Log_loss score is {}'.format(np.mean(losses)))\nprint('Total average CV ROC_AUC score is {}'.format(np.mean(auc)))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:05:40.697353Z","iopub.execute_input":"2022-03-12T05:05:40.697653Z","iopub.status.idle":"2022-03-12T05:08:51.39778Z","shell.execute_reply.started":"2022-03-12T05:05:40.697623Z","shell.execute_reply":"2022-03-12T05:08:51.396723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vectorize, Classify (with parameter tuning)\nx = train['comment_text']\ny = train.iloc[:, 2:8]  \nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)\nstart_time=time.time()\n\npipe = make_pipeline(TfidfVectorizer(\n                                    stop_words='english',\n                                    strip_accents='unicode',\n                                    token_pattern=r'\\w{1,}', #accept tokens that have 1 or more characters\n                                    analyzer='word',\n                                    ngram_range=(1, 1),\n                                    min_df=5),\n                     OneVsRestClassifier(LogisticRegression()))\nparam_grid = {'tfidfvectorizer__max_features': [10000, 30000],\n              'onevsrestclassifier__estimator__solver': ['liblinear', 'sag'],\n             } \ngrid = GridSearchCV(pipe, param_grid, cv=3, scoring='roc_auc')\n\ngrid3 = grid.fit(X_train, y_train)\n\nend_time=time.time()\nprint(\"total time\",end_time-start_time)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:09:06.024503Z","iopub.execute_input":"2022-03-12T05:09:06.025591Z","iopub.status.idle":"2022-03-12T05:12:19.7084Z","shell.execute_reply.started":"2022-03-12T05:09:06.025548Z","shell.execute_reply":"2022-03-12T05:12:19.707475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid3.best_estimator_.named_steps['onevsrestclassifier'])\nprint(grid3.best_estimator_.named_steps['tfidfvectorizer'])\ngrid3.best_params_\n\n\ngrid3.best_score_\npredicted_y_test = grid3.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:12:56.347032Z","iopub.execute_input":"2022-03-12T05:12:56.347368Z","iopub.status.idle":"2022-03-12T05:12:58.396812Z","shell.execute_reply.started":"2022-03-12T05:12:56.347336Z","shell.execute_reply":"2022-03-12T05:12:58.396065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Toxic Confusion Matrixs: \\n{}\".format(confusion_matrix(y_test['toxic'], predicted_y_test[:,0])))\nprint(\"\\nSevere Toxic: \\n{}\".format(confusion_matrix(y_test['severe_toxic'], predicted_y_test[:,1])))\nprint(\"\\nObscene: \\n{}\".format(confusion_matrix(y_test['obscene'], predicted_y_test[:,2])))\nprint(\"\\nThreat: \\n{}\".format(confusion_matrix(y_test['threat'], predicted_y_test[:,3])))\nprint(\"\\nInsult: \\n{}\".format(confusion_matrix(y_test['insult'], predicted_y_test[:,4])))\nprint(\"\\nIdentity Hate: \\n{}\".format(confusion_matrix(y_test['identity_hate'], predicted_y_test[:,5])))\n\nprint(\"\\nToxic Classification report: \\n{}\".format(classification_report(y_test['toxic'], predicted_y_test[:,0])))\nprint(\"\\nSevere Toxic: \\n{}\".format(classification_report(y_test['severe_toxic'], predicted_y_test[:,1])))\nprint(\"\\nObscene: \\n{}\".format(classification_report(y_test['obscene'], predicted_y_test[:,2])))\nprint(\"\\nThreat: \\n{}\".format(classification_report(y_test['threat'], predicted_y_test[:,3])))\nprint(\"\\nInsult: \\n{}\".format(classification_report(y_test['insult'], predicted_y_test[:,4])))\nprint(\"\\nIdentity Hate: \\n{}\".format(classification_report(y_test['identity_hate'], predicted_y_test[:,5])))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:13:03.389543Z","iopub.execute_input":"2022-03-12T05:13:03.390014Z","iopub.status.idle":"2022-03-12T05:13:03.643178Z","shell.execute_reply.started":"2022-03-12T05:13:03.389967Z","shell.execute_reply":"2022-03-12T05:13:03.642225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = grid3.best_estimator_.named_steps[\"tfidfvectorizer\"]\n# transform the training dataset:\nX_test_set = vectorizer.transform(X_test)\n\n\n# find maximum value for each of the features over dataset:\nmax_value = X_test_set.max(axis=0).toarray().ravel()\nsorted_by_tfidf = max_value.argsort()\n\n# get feature names\nfeature_names = np.array(vectorizer.get_feature_names_out())\n\nprint(\"Features with lowest tfidf:\\n{}\".format(\n      feature_names[sorted_by_tfidf[:20]]))\n\nprint(\"\\nFeatures with highest tfidf: \\n{}\".format(\n      feature_names[sorted_by_tfidf[-20:]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:13:27.509375Z","iopub.execute_input":"2022-03-12T05:13:27.5103Z","iopub.status.idle":"2022-03-12T05:13:29.594535Z","shell.execute_reply.started":"2022-03-12T05:13:27.510258Z","shell.execute_reply":"2022-03-12T05:13:29.593657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_by_idf = np.argsort(vectorizer.idf_)\nprint(\"Features with lowest idf:\\n{}\".format(\n       feature_names[sorted_by_idf[:100]]))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:13:34.359515Z","iopub.execute_input":"2022-03-12T05:13:34.360368Z","iopub.status.idle":"2022-03-12T05:13:34.369594Z","shell.execute_reply.started":"2022-03-12T05:13:34.36033Z","shell.execute_reply":"2022-03-12T05:13:34.368745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holdout_comments = holdout['comment_text']\n# holdoutComments are automatically transformed throguh the grid3 pipeline before prodicting probabilities\ntwod = grid3.predict_proba(holdout_comments)\nholdout_predictions = {}\nholdout_predictions = {'id': holdout['id']}  \n\nholdout_predictions['toxic']=twod[:,0]\nholdout_predictions['severe_toxic']=twod[:,1]\nholdout_predictions['obscene']=twod[:,2]\nholdout_predictions['threat']=twod[:,3]\nholdout_predictions['insult']=twod[:,4]\nholdout_predictions['identity_hate']=twod[:,5]\n    \nsubmission = pd.DataFrame.from_dict(holdout_predictions)\nsubmission = submission[['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']] #rearrange columns\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:13:38.02543Z","iopub.execute_input":"2022-03-12T05:13:38.026041Z","iopub.status.idle":"2022-03-12T05:13:48.653926Z","shell.execute_reply.started":"2022-03-12T05:13:38.026006Z","shell.execute_reply":"2022-03-12T05:13:48.652978Z"},"trusted":true},"execution_count":null,"outputs":[]}]}