{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nfrom keras.callbacks import Callback\nfrom keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom collections import defaultdict\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n#train = pd.read_csv('../input/cleaned-toxic-comments/train_preprocessed.csv')\ntrain= pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\n#test = pd.read_csv('../input/cleaned-toxic-comments/test_preprocessed.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emoji_dict = defaultdict()\nwith open('../input/emoji-unicode-names/emoji_unicode_names_final.txt', 'r', encoding=\"utf8\") as f:\n    lines = f.readlines()\n    for line in lines:\n        tokens = line.strip().split('\\t')\n        emoji_dict[tokens[0]] = tokens[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here is the sample emoji list..\nfor i in emoji_dict:\n    if \"face\" in emoji_dict[i] or \"eyes\" in emoji_dict[i]:\n        print(i , \" : \", emoji_dict[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"repl = {\n    \"&lt;3\": \" good \",\n    \":d\": \" good \",\n    \":dd\": \" good \",\n    \":p\": \" good \",\n    \"8)\": \" good \",\n    \":-)\": \" good \",\n    \":)\": \" good \",\n    \";)\": \" good \",\n    \"(-:\": \" good \",\n    \"(:\": \" good \",\n    \"yay!\": \" good \",\n    \"yay\": \" good \",\n    \"yaay\": \" good \",\n    \"yaaay\": \" good \",\n    \"yaaaay\": \" good \",\n    \"yaaaaay\": \" good \",\n    \":/\": \" bad \",\n    \":&gt;\": \" sad \",\n    \":')\": \" sad \",\n    \":-(\": \" bad \",\n    \":(\": \" bad \",\n    \":s\": \" bad \",\n    \":-s\": \" bad \",\n    \"&lt;3\": \" heart \",\n    \":d\": \" smile \",\n    \":p\": \" smile \",\n    \":dd\": \" smile \",\n    \"8)\": \" smile \",\n    \":-)\": \" smile \",\n    \":)\": \" smile \",\n    \";)\": \" smile \",\n    \"(-:\": \" smile \",\n    \"(:\": \" smile \",\n    \":/\": \" worry \",\n    \":&gt;\": \" angry \",\n    \":')\": \" sad \",\n    \":-(\": \" sad \",\n    \":(\": \" sad \",\n    \":s\": \" sad \",\n    \":-s\": \" sad \",\n    r\"\\br\\b\": \"are\",\n    r\"\\bu\\b\": \"you\",\n    r\"\\bhaha\\b\": \"ha\",\n    r\"\\bhahaha\\b\": \"ha\",\n    r\"\\bdon't\\b\": \"do not\",\n    r\"\\bdoesn't\\b\": \"does not\",\n    r\"\\bdidn't\\b\": \"did not\",\n    r\"\\bhasn't\\b\": \"has not\",\n    r\"\\bhaven't\\b\": \"have not\",\n    r\"\\bhadn't\\b\": \"had not\",\n    r\"\\bwon't\\b\": \"will not\",\n    r\"\\bwouldn't\\b\": \"would not\",\n    r\"\\bcan't\\b\": \"can not\",\n    r\"\\bcannot\\b\": \"can not\",\n    r\"\\bi'm\\b\": \"i am\",\n    \"m\": \"am\",\n    \"r\": \"are\",\n    \"u\": \"you\",\n    \"haha\": \"ha\",\n    \"hahaha\": \"ha\",\n    \"don't\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"didn't\": \"did not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"hadn't\": \"had not\",\n    \"won't\": \"will not\",\n    \"wouldn't\": \"would not\",\n    \"can't\": \"can not\",\n    \"cannot\": \"can not\",\n    \"i'm\": \"i am\",\n    \"m\": \"am\",\n    \"i'll\" : \"i will\",\n    \"its\" : \"it is\",\n    \"it's\" : \"it is\",\n    \"'s\" : \" is\",\n    \"that's\" : \"that is\",\n    \"weren't\" : \"were not\",\n}\nkeys = [i for i in repl.keys()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_data = []\nnew_test_data = []\nc_train = 0\nc_test = 0\nltr = train[\"comment_text\"].tolist()\nlte = test[\"comment_text\"].tolist()\nfor i in ltr:\n    arr = str(i).split()\n    xx = \"\"\n    for j in arr:\n        j = str(j).lower()\n        if j[:4] == 'http' or j[:3] == 'www':\n            # c_train += 1\n            continue\n        if j in keys:\n            # print(\"inn\")\n            c_train += 1\n            j = repl[j]\n        if j in emoji_dict:\n            c_train += 1\n            j = emoji_dict[j]\n        xx += j + \" \"\n    new_train_data.append(xx)\nfor i in lte:\n    arr = str(i).split()\n    xx = \"\"\n    for j in arr:\n        j = str(j).lower()\n        if j[:4] == 'http' or j[:3] == 'www':\n            # c_test += 1\n            continue\n        if j in keys:\n            # print(\"inn\")\n            c_test += 1\n            j = repl[j]\n        if j in emoji_dict:\n            c_test += 1\n            j = emoji_dict[j]\n        xx += j + \" \"\n    new_test_data.append(xx)\ntrain[\"new_comment_text\"] = new_train_data\ntest[\"new_comment_text\"] = new_test_data\n\nprint(\"replacements in train data : \", c_train)\nprint(\"replacements in test data : \", c_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(\"comment_text\", axis=1)\ntest = test.drop(\"comment_text\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf43ac37cbd14d8baa088648c2275123550135d6","_cell_guid":"e8bd3575-f711-4ca6-a653-8ec1c74c0204","trusted":true},"cell_type":"code","source":"train[\"new_comment_text\"].fillna(\"fillna\")\ntest[\"new_comment_text\"].fillna(\"fillna\")\n#X_train = train[\"new_comment_text\"].str.lower()\nX_train = train[\"new_comment_text\"]\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\n#X_test = test[\"new_comment_text\"].str.lower()\nX_test = test[\"new_comment_text\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efad6a0ecd758a759f14287a69bfd9cafa8c8fb2","_cell_guid":"da409613-3688-4d2e-a072-f67dee02617b","trusted":true},"cell_type":"code","source":"max_features=100000\nmaxlen=200\nembed_size=300","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70c576f3b0afd3e779df184f788107fb51233424","_cell_guid":"3b0804b5-d64e-474e-8252-78daa4a4be62","trusted":true},"cell_type":"code","source":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b07e998ccedaf3aaaf4b4e67b207ad5490eb24f7","_cell_guid":"7a665c08-b4a9-4792-b40b-481b3da907e5","trusted":true},"cell_type":"code","source":"tok=text.Tokenizer(num_words=max_features,lower=True)\ntok.fit_on_texts(list(X_train)+list(X_test))\nX_train=tok.texts_to_sequences(X_train)\nX_test=tok.texts_to_sequences(X_test)\nx_train=sequence.pad_sequences(X_train,maxlen=maxlen)\nx_test=sequence.pad_sequences(X_test,maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9488bc9d68dfd1fde1f99d23a9f1ed7b30ceb87f","_cell_guid":"9e57a7cb-c061-4361-bbe2-05c0486a3f18","trusted":true},"cell_type":"code","source":"embeddings_index = {}\nwith open(EMBEDDING_FILE,encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d56ad119931a971b2588355deb726a045764c9ad","_cell_guid":"e2490100-fc9c-4e46-ae84-7dfa65fcddba","trusted":true},"cell_type":"code","source":"word_index = tok.word_index\n#prepare embedding matrix\nnum_words = min(max_features, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"560d3faac051bbb95dae6f1bf7013d52b404533c","_cell_guid":"1a4a1cf3-7faf-4ee4-a72e-a258169778a5","trusted":true},"cell_type":"code","source":"from keras.layers import K, Activation\nfrom keras.engine import Layer\nfrom keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\ngru_len = 128\nRoutings = 5\nNum_capsule = 10\nDim_capsule = 16\ndropout_p = 0.25\nrate_drop_dense = 0.28\n\ndef squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\n\n# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n\n\ndef get_model():\n    input1 = Input(shape=(maxlen,))\n    embed_layer = Embedding(max_features,\n                            embed_size,\n                            input_length=maxlen,\n                            weights=[embedding_matrix],\n                            trainable=False)(input1)\n    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n\n    x = Bidirectional(\n        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n        embed_layer)\n    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n                      share_weights=True)(x)\n    # output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n    capsule = Flatten()(capsule)\n    capsule = Dropout(dropout_p)(capsule)\n    output = Dense(6, activation='sigmoid')(capsule)\n    model = Model(inputs=input1, outputs=output)\n    model.compile(\n        loss='binary_crossentropy',\n        optimizer='adam',\n        metrics=['accuracy'])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19975febdf6a0bd3077d8a92da13bb433085ce80","scrolled":true,"_cell_guid":"46df26aa-adcd-4b2c-8644-76a1e51df2bc","trusted":true},"cell_type":"code","source":"model = get_model()\n\nbatch_size = 256\nepochs = 3\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\nRocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7956b05d34604689b7a10d56a61640091559eda2","scrolled":true,"_cell_guid":"e1962822-5dfb-4249-a714-ce95346150d4","trusted":true},"cell_type":"code","source":"hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val),\n                 callbacks=[RocAuc], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b53a2bd1fc68f70f93f5553624afb34a84dc117","scrolled":false,"_cell_guid":"ae576779-105a-4b55-a214-197932cebf97","trusted":true},"cell_type":"code","source":"hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val),\n                 callbacks=[RocAuc], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7377e50952ffb6cc14bab3b34442788864eed68","scrolled":false,"_cell_guid":"265115a8-296e-4b67-a6fc-02d0a584b501","trusted":true},"cell_type":"code","source":"hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val),\n                 callbacks=[RocAuc], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddc5afff4b22841fb184e32cc4b19a2225dec451","scrolled":true,"_cell_guid":"5c703574-cf76-495f-b800-1fc6c9384ded","trusted":true},"cell_type":"code","source":"y_pred = model.predict(x_test, batch_size=1024, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f28a8748f9726bd1d3ef4dfc63fc97dbf54274d6","_cell_guid":"bffedb6f-f020-4ff8-b2f5-acd9c5957841","trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\nsubmission.to_csv('submission.csv', index=False)\nmodel.save_weights('best.hdf5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}