{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nfrom keras.callbacks import Callback\nfrom keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom collections import defaultdict\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\ntrain_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\n\nMAX_SEQUENCE_LENGTH = 150\nMAX_NB_WORDS = 100000\nEMBEDDING_DIM = 300\nVALIDATION_SPLIT = 0.1\n\nnum_lstm = 300\nnum_dense = 256\nrate_drop_lstm = 0.2\nrate_drop_dense = 0.2\n\nact = 'relu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ncolor = sns.color_palette()\nsns.set_style(\"dark\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train_df.iloc[:,2:].sum()\n#marking comments without any tags as \"clean\"\nrowsums=train_df.iloc[:,2:].sum(axis=1)\ntrain_df['clean']=(rowsums==0)\n#count number of clean entries\ntrain_df['clean'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nx = train_df.iloc[:,2:].sum()\nplt.figure(figsize=(8,4))\nax = sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"No of Comments per Class\")\nplt.ylabel('No of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=rowsums.value_counts()\n\n#plot\nplt.figure(figsize=(8,4))\nax = sns.barplot(x.index, x.values, alpha=0.8,color='g')\nplt.title(\"Multiple tags per comment\")\nplt.ylabel('No of Occurrences', fontsize=12)\nplt.xlabel('No of tags ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanData(text, stemming = False, lemmatize=False):    \n    text = text.lower().split()\n    text = \" \".join(text)\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    if stemming:\n        st = PorterStemmer()\n        txt = \" \".join([st.stem(w) for w in text.split()])\n    if lemmatize:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Indexing word vectors')\n\ncount = 0\nembeddings_index = {}\nf = open(EMBEDDING_FILE)\nfor line in f:\n    values = line.split()\n    word = ' '.join(values[:-300])\n    coefs = np.asarray(values[-300:], dtype='float32')\n    embeddings_index[word] = coefs.reshape(-1)\n    coef = embeddings_index[word]\nf.close()\n\nprint('Found %d word vectors of glove.' % len(embeddings_index))\nemb_mean,emb_std = coef.mean(), coef.std()\nprint(emb_mean,emb_std)\n\nprint('Total %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\nimport csv\nimport codecs\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import *\nfrom keras.layers import concatenate, CuDNNGRU\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nprint('Processing text dataset')\n\ntrain_df['comment_text'] = train_df['comment_text'].map(lambda x: cleanData(x,  stemming=False, lemmatize=False))\ntest_df['comment_text'] = test_df['comment_text'].map(lambda x: cleanData(x,  stemming=False, lemmatize=False))\n\n#Regex to remove all Non-Alpha Numeric and space\nspecial_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n#regex to replace all numerics\nreplace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n\ndef text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n    text = text.lower().split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n    \n    #Remove Special Characters\n    text=special_character_removal.sub('',text)\n    #Replace Numbers\n    text=replace_numbers.sub('n',text)\n\n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    return(text)\n\n\nlist_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train_df[list_classes].values\nlist_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n\n\ncomments = []\nfor text in list_sentences_train:\n    comments.append(text_to_wordlist(text))\n    \ntest_comments=[]\nfor text in list_sentences_test:\n    test_comments.append(text_to_wordlist(text))\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(comments + test_comments)\n\nsequences = tokenizer.texts_to_sequences(comments)\ntest_sequences = tokenizer.texts_to_sequences(test_comments)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', y.shape)\n\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of test_data tensor:', test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_post = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post', truncating='post')\nprint('Shape of data tensor:', data_post.shape)\nprint('Shape of label tensor:', y.shape)\n\ntest_data_post = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\nprint('Shape of test_data tensor:', test_data_post.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Preparing embedding matrix')\nnb_words = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i >= MAX_NB_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\nprint('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efad6a0ecd758a759f14287a69bfd9cafa8c8fb2","_cell_guid":"da409613-3688-4d2e-a072-f67dee02617b","trusted":true},"cell_type":"code","source":"max_features=100000\nmaxlen=150\nembed_size=300","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70c576f3b0afd3e779df184f788107fb51233424","_cell_guid":"3b0804b5-d64e-474e-8252-78daa4a4be62","trusted":true},"cell_type":"code","source":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"560d3faac051bbb95dae6f1bf7013d52b404533c","_cell_guid":"1a4a1cf3-7faf-4ee4-a72e-a258169778a5","trusted":true},"cell_type":"code","source":"from keras.layers import K, Activation\nfrom keras.engine import Layer\nfrom keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\ngru_len = 128\nRoutings = 5\nNum_capsule = 10\nDim_capsule = 16\ndropout_p = 0.25\nrate_drop_dense = 0.28\n\ndef squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\n# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    \n    def __init__(self, num_capsule=10, dim_capsule=16, routings=5, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n#             print(\"W: \"+ str(self.W.shape) +\"\\n\")\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n#             print(\"u_vec: \"+ str(u_vecs) +\"\\n\")\n#             print(\"u_hat_vecs: \"+ str(u_hat_vecs) +\"\\n\")\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n#         print(\"u_hat_vecs: after reshape 1: \"+ str(u_hat_vecs) +\"\\n\")\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n#         print(\"u_hat_vecs: after reshape 2: \"+ str(u_hat_vecs) +\"\\n\")\n#         final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n#             print(\"b: \"+ str(b) +\"\\n\")\n#             print(\"c: \"+ str(c) +\"\\n\")\n#             print(\"outputs: \"+ str(outputs) +\"\\n\")\n            if i < self.routings - 1:\n                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19975febdf6a0bd3077d8a92da13bb433085ce80","scrolled":true,"_cell_guid":"46df26aa-adcd-4b2c-8644-76a1e51df2bc","trusted":true},"cell_type":"code","source":"def get_model():\n    \n    input1_pre = Input(shape=(maxlen,))\n    embed_layer1_pre = Embedding(max_features,\n                            embed_size,\n                            input_length=maxlen,\n                            weights=[embedding_matrix],\n                            trainable=False)(input1_pre)\n    embed_layer1_pre = SpatialDropout1D(0.4)(embed_layer1_pre)\n    \n    x_pre = Bidirectional(CuDNNGRU(128, return_sequences=True))(embed_layer1_pre)\n    capsule_pre = Capsule(num_capsule=10, dim_capsule=16, routings=5,share_weights=True)(x_pre)\n    capsule_pre = GlobalMaxPooling1D()(capsule_pre)\n    #capsule_pre = Flatten()(capsule_pre)\n    capsule_pre = Dropout(0.25)(capsule_pre)\n    \n    input1_post = Input(shape=(maxlen,))\n    embed_layer1_post = Embedding(max_features,\n                            embed_size,\n                            input_length=maxlen,\n                            weights=[embedding_matrix],\n                            trainable=False)(input1_post)\n    embed_layer1_post = SpatialDropout1D(0.4)(embed_layer1_post)\n    \n    x_post = Bidirectional(CuDNNGRU(128, return_sequences=True))(embed_layer1_post)\n    capsule_post = Capsule(num_capsule=10, dim_capsule=16, routings=5,share_weights=True)(x_post)\n    capsule_post = GlobalMaxPooling1D()(capsule_post)\n    #capsule_post = Flatten()(capsule_post)\n    capsule_post = Dropout(0.25)(capsule_post)\n    \n    concat = concatenate([capsule_pre,capsule_post])\n    output = Dense(6, activation='sigmoid')(concat)\n    \n    model = Model(inputs=[input1_pre,input1_post], outputs=output)\n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=Adam(lr=1e-3,decay=0),\n        metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = \"capsule_val0.05.h5\"\nmodel = get_model()\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n\ncallbacks_list = [checkpoint, early] \nhist = model.fit([data, data_post], y, epochs=10, batch_size=128, shuffle=True, validation_split=0.05, callbacks = callbacks_list, \n                 verbose=1)\n\nbest_score = min(hist.history['val_loss'])\nprint (best_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nimport numpy as np\n\ntest_predicts_list = []\n\ndef train_folds(data,data_post, y,fold_count=10):\n    print(\"Starting to train models...\")\n    fold_size = len(data) // fold_count\n    models = []\n    for fold_id in range(0, fold_count):\n        fold_start = fold_size * fold_id\n        fold_end = fold_start + fold_size\n\n        if fold_id == fold_size - 1:\n            fold_end = len(data)\n\n        print(\"Fold {0}\".format(fold_id))\n        \n        train_x = np.concatenate([data[:fold_start], data[fold_end:]])\n        train_xp = np.concatenate([data_post[:fold_start], data_post[fold_end:]])\n        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n\n        val_x = data[fold_start:fold_end]\n        val_xp = data_post[fold_start:fold_end]\n        val_y = y[fold_start:fold_end]\n        \n        file_path=\"capsule_fold{0}.h5\".format(fold_id)\n        model = get_model()\n        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n        RocAuc = RocAucEvaluation(validation_data=([val_x, val_xp], val_y), interval=1)\n        callbacks_list = [checkpoint, early,RocAuc] \n\n        hist = model.fit([train_x, train_xp], train_y, epochs=15, batch_size=128, shuffle=True, \n                         validation_data=([val_x, val_xp], val_y), callbacks = callbacks_list, verbose=1)\n        model.load_weights(file_path)\n        best_score = min(hist.history['val_loss'])\n        \n        print(\"Fold {0} loss {1}\".format(fold_id, best_score))\n        print(\"Predicting validation...\")\n        val_predicts_path = \"capsule_val_predicts{0}.npy\".format(fold_id)\n        val_predicts = model.predict([val_x, val_xp], batch_size=1024, verbose=1)\n        np.save(val_predicts_path, val_predicts)\n        \n        print(\"Predicting results...\")\n        test_predicts_path = \"capsule_test_predicts{0}.npy\".format(fold_id)\n        test_predicts = model.predict([test_data, test_data_post], batch_size=1024, verbose=1)\n        test_predicts_list.append(test_predicts)\n        np.save(test_predicts_path, test_predicts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_folds(data, data_post, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ntest_predicts_am = np.zeros(test_predicts_list[0].shape)\n\nfor fold_predict in test_predicts_list:\n    test_predicts_am += fold_predict\n\ntest_predicts_am = (test_predicts_am / len(test_predicts_list))\n\ntest_ids = test_df[\"id\"].values\ntest_ids = test_ids.reshape((len(test_ids), 1))\n\ntest_predicts_am = pd.DataFrame(data=test_predicts_am, columns=CLASSES)\ntest_predicts_am[\"id\"] = test_ids\ntest_predicts_am = test_predicts_am[[\"id\"] + CLASSES]\ntest_predicts_am.to_csv(\"10fold_capsule_am.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}