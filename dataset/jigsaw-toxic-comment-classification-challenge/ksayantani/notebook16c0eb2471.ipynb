{"nbformat":4,"metadata":{"language_info":{"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.6.4","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":1,"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nimport re\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport sklearn.ensemble\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"f0bb6c539af67469c38bed52f203db8442ac5808","_cell_guid":"9268b28a-2dce-4556-88e6-b8d5dff83578"},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"ENGLISH_STOP_WORDS = set(stopwords.words(\"english\"))\ntargets = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']","metadata":{},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"space_join = \" \".join\n\ndef custom_preprocessor(comment):\n    comment = re.sub(r'(\\w+!)', r'EMOTION', comment)\n    comment = re.sub(r'(\\n\\n*)', '', comment)\n    comment = re.sub(r'\\d+[.]\\d+[.]\\d+[.]\\d+', '', comment)\n    comment = re.sub(r'([.?!;])(\\s+)(\\w+)', r'\\1\\3', comment)\n    return comment\n\ndef custom_tokenizer(comment):\n    if comment == \"\":\n        return []\n    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n    tokens = token_pattern.findall(comment)\n    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n    return list(set(tokens))\n\ndef get_nonalphanumeric_tokens(comment):\n    if comment == \"\":\n        return []\n    return list(set(re.findall(r'[^\\w\\s\\.]', comment)))\n\ndef get_datetime(comment):\n    searches = re.findall(r'[012][0-9][:][0-9][0-9]\\W?\\s+[0-9][0-9]\\s[\\w]+', comment)\n    if len(searches) > 0:\n        return 1\n    return 0\n\ndef get_random_indx(df, type_of_question):\n    return np.random.choice(df.loc[df[type_of_question] == 1].index)\n\ndef get_sum(row):\n    sum_target = 0.0\n    for t in targets:\n        sum_target += row[t]\n    return sum_target","metadata":{"_uuid":"610fbbafe70821627c6b8f4818667540f88736c4","_cell_guid":"0f4d4ef9-b8d0-421f-b36c-a5f7e0a48212","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"train = pd.read_csv('../input/train.csv')\ntrain['unlabeled'] = train.apply(lambda x: get_sum(x), axis=1)","metadata":{},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"train['preprocessed_text'] = train['comment_text'].apply(lambda text: custom_preprocessor(text))\ntrain['tokens'] = train['preprocessed_text'].apply(lambda text: custom_tokenizer(text))\n\ntrain['n_vocab']  = train['preprocessed_text'].apply(lambda row: len(sorted(set(row))))\ntrain['n_tokens'] = train['tokens'].apply(lambda row: len(row))\n\ntrain['non_alphas'] = train['preprocessed_text'].apply(lambda text: get_nonalphanumeric_tokens(text))\ntrain['n_non_alphas'] = train['non_alphas'].map(len)","metadata":{"_uuid":"b55e0f18e0bbb15f273d07a7ea76fcc4ad3ec4a9","_cell_guid":"2bd11961-9ca0-4a49-a113-68c3deca98c6","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"count_vect = CountVectorizer(min_df=1, max_df=3, max_features = 256)\nX_train_counts = count_vect.fit_transform(pd.Series(train['preprocessed_text'].tolist()))\n\nvocabulary = count_vect.get_feature_names()\nbow = pd.DataFrame(X_train_counts.toarray(), columns=vocabulary)\nbow['id'] = train['id']\ntrain_bow = train.merge(bow, how='inner', on='id')","metadata":{"_uuid":"b50bbcad6531bd2506f433cfab964d6bdc8effb2","_cell_guid":"da6c9aa0-ee7b-4976-93e6-80a6b8cc2dee"},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"estimators = {}\nfeatures = vocabulary + ['n_non_alphas']\n\nfor target in targets:\n    X = train_bow[features]\n    y = train_bow[target]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    \n    clf = RandomForestClassifier(random_state=0)\n    clf = clf.fit(X_train, y_train)\n    estimators[target] = clf\n    print(\"{0:<20} {1}\".format(target, log_loss(y_test, clf.predict_proba(X_test))))","metadata":{},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"test = pd.read_csv('../input/test.csv')\ntest.fillna(\"\", inplace=True)\n\ntest['preprocessed_text'] = test['comment_text'].apply(lambda text: custom_preprocessor(text))\ntest['tokens'] = test['preprocessed_text'].apply(lambda text: custom_tokenizer(text))\n\ntest['n_vocab']  = test['preprocessed_text'].apply(lambda row: len(sorted(set(row))))\ntest['n_tokens'] = test['tokens'].apply(lambda row: len(row))\n\ntest['non_alphas'] = test['preprocessed_text'].apply(lambda text: get_nonalphanumeric_tokens(text))\ntest['n_non_alphas'] = test['non_alphas'].map(len)","metadata":{"_uuid":"74f6a4ad42735fe7283a993b7e277a7cd89bcfa4","_cell_guid":"74f17572-e208-448f-9020-d1b56543cd30","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"X_test_counts = count_vect.transform(pd.Series(test['preprocessed_text'].tolist()))\n\nX_test = pd.DataFrame(X_test_counts.toarray(), columns=vocabulary)\nX_test['n_non_alphas'] = test['n_non_alphas']","metadata":{"collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"submission = pd.read_csv('../input/sample_submission.csv')\nfor key, clf in estimators.items():\n    pred = clf.predict_proba(X_test[features])[:, 1]\n    submission[key] = pd.Series(pred)","metadata":{"_uuid":"40909292c3308ab0bf25898a8747e1b7e1364403","_cell_guid":"223a581a-cc72-4b2d-a374-09296ed0b41b"},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"pd.isnull(submission).sum()","metadata":{"_uuid":"ff1c905c354eefb147ba5ae6920020376aabd638","_cell_guid":"3520f6e5-0a32-457e-9180-90cdf870b3fa"},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"submission.to_csv('submission.csv', index=False)","metadata":{"_uuid":"61f2ca7359462831d0dc4b680f69d18f5fca6d93","_cell_guid":"520884ea-d6ab-4e76-b3f7-4b8fba573248"},"execution_count":null,"outputs":[],"cell_type":"code"}]}