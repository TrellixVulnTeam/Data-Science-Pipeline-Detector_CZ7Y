{"cells":[{"metadata":{"_uuid":"bb56331fa1541341295222fdf292593d6ca57960"},"cell_type":"markdown","source":"## [Workbook 1](https://www.kaggle.com/sabasiddiqi/workbook-1-text-pre-processing-for-beginners) - Text Preprocessing for Beginners - Data Cleaning\n<br>\n**Level** : Beginner\n\nThis notebook discusses **Text Data Preprocessing** for **NLP Problems** using Toxic Comment Classification Dataset. Data comprises of large number of Wikipedia comments which have been labeled by human raters for toxic behavior\n\nData is available via following link.\n[Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)\n\nNext Workbook : [Workbook 2 - Text Preprocessing for Beginners - Feature Extraction](https://www.kaggle.com/sabasiddiqi/workbook-2-text-preprocessing-feature-extraction) \n\nTo skip the initial steps (reading data, text extraction from data), Jump to [Text Pre-Processing Steps](#jump)."},{"metadata":{"_uuid":"f187dd217d98c4dcd57c7b806fe0eda25b048721"},"cell_type":"markdown","source":"Starting by importing required libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\nimport string\nfrom string import digits\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b81ba34af6ad302da74b215e71035409883d88d"},"cell_type":"markdown","source":"Reading training and test data from CSV file and saving as Pandas' Dataframe"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint(\"\\nTrain data: \\n\",train.head())\nprint(\"\\nTest data: \\n\",test.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8b5d95fb89e0b6a715d42ed7035b82665130062"},"cell_type":"markdown","source":"Data here comprises of ids, comments, and labels. \n\nRemoving IDs from Train data, keeping Test data IDs for submission. "},{"metadata":{"trusted":true,"_uuid":"0eb0328ebfd691bf5008abc20d55b1f9baa12b6e"},"cell_type":"code","source":"train_data=train.drop(train.columns[0], axis=1) \ntest_data=test\nprint(train_data.head())\nprint(test_data.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28f108017087cdcd4140318eaf26440e9b9763e2"},"cell_type":"markdown","source":"Now extracting comments from train and test data, and storing their index for later use.\nMerging comments for both train and test, so that Preprocessing Steps can be performed on both at same time."},{"metadata":{"trusted":true,"_uuid":"3923723aeef9d8c8c12f8eb087540c766ede376f"},"cell_type":"code","source":"train_comments=train_data.iloc[:,0]\ntest_comments=test_data.iloc[:,1]\n\n#saving index to separate them later\ntrain_comments_index=train_comments.index\ntest_comments_index=test_comments.index\n\nframes = [train_comments, test_comments]\ncomments = pd.concat(frames, ignore_index=True)\n\n\nlabels=train_data.iloc[:,1:]\n\nprint(\"Train Comments Shape: \",train_comments.shape)\nprint(\"Test Comments Shape: \",test_comments.shape)\nprint(\"Comments Shape after Merge: \",comments.shape)\nprint(\"Comments are: \\n\",comments.head())\nprint(\"\\nLabels are: \\n\", labels.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a9173fa5cf23077316c23f581010c7b5e43ca1b"},"cell_type":"markdown","source":"<br>\n###   <a id=\"jump\">Basic Text Preprocessing Steps - Cleaning </a>\n<br>\nNow that we have comments, its time to process them to convert them into a form that can be fed to classifier.\n\nTo do so following basic steps are performed and to get a better idea of what these steps do, an example is added as well. \n\n**“You are annoying!!! goJumpOff4Cliff pleaseeeeeeee”**\n* Step 1 - [Remove punctuation](#1) →** You are annoying goJumpOff4Cliff pleaseeeeeeee**\n* Step 2 - [Remove digits](#2)→ ** You are annoying goJumpOffCliff please**\n* Step 3 - [Split combined words](#3) → **You are annoying go Jump Off Cliff please**\n* Step 4 - [Convert to lowercase](#4) →   ** your are annoying go jump off cliff please**\n* Step 5 - [Split each sentence using delimiter](#5) →   ** your, are, annoying, go, jump, off, cliff, please**\n* Step 6 - [Remove stop words](#6) →       **annoying, jump, cliff **\n* Step 7 - [Convert Word to Base Form](#7) →                      **annoy, jump, cliff** \n\nPlease note that order of steps matter here, if step number 4 is performed before Step 3, we wont be able to split the Combined words like **goJumpOffCliff**."},{"metadata":{"_uuid":"1bccb9dcd1609c1015e5655a1d3510730d06c028"},"cell_type":"markdown","source":"<a id=\"1\">Step 1 - Remove Punctuation</a>"},{"metadata":{"trusted":true,"_uuid":"cc43012244bf477c8aeaa66e13481030a2f0f655"},"cell_type":"code","source":"c=comments.str.translate(str.maketrans(' ', ' ', string.punctuation))\nc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6787905090303a072f5da8bb0d6690f542c4f92"},"cell_type":"markdown","source":"<a id=\"2\">Step 2 - Remove Digits </a>\n\nRemoving \\n and digits"},{"metadata":{"trusted":true,"_uuid":"3e20a6ef51edc389a0d971f844ff6e55e5bc03e1"},"cell_type":"code","source":"c=c.str.translate(str.maketrans(' ', ' ', '\\n'))\nc=c.str.translate(str.maketrans(' ', ' ', digits))\nc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83046568d2d665eb948b2883b7d836ab59b33421"},"cell_type":"markdown","source":"<a id=\"3\"> Step 3 - Split combined words </a>\n\nFor instance, converting **whyAreYou** to **why Are You **"},{"metadata":{"trusted":true,"_uuid":"fa3f07d0489c52923c0d26a0dcd245c2673c898e"},"cell_type":"code","source":"c=c.apply(lambda tweet: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',tweet))\nc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00569c5680aaa623750dd5adf56f74cd9d3a3ea3"},"cell_type":"markdown","source":"<a id=\"4\"> Step 4 - Convert to lowercase </a>\n"},{"metadata":{"trusted":true,"_uuid":"44476b98e5ea4e337fcd58cc013e86bece14b894"},"cell_type":"code","source":"c=c.str.lower()\nc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55798f058bed4976fad95983289fbd5c3190ce16"},"cell_type":"markdown","source":"<a id=\"5\"> Step 5 - Split each sentence using delimiter </a>\n\nConverting each sentence to list of words. We are doing it to keep necessary words in the upcoming steps and descarding the rest."},{"metadata":{"trusted":true,"_uuid":"d33ed62a40c4f5ea689190cef26d819f944639d8"},"cell_type":"code","source":"c=c.str.split()\nc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31402aa2f7c2c697d302861e035adc7afd29ffe0"},"cell_type":"markdown","source":"<a id=\"6\"> Step 6 - Remove Stop Words </a>\n\nStop words are the most common words in a language and mostly filtered in NLP problems."},{"metadata":{"trusted":true,"_uuid":"d730ddbc6ba04053b45b339a4081c43fa67d3ba3"},"cell_type":"code","source":"stop = set(stopwords.words('english'))\nc=c.apply(lambda x: [item for item in x if item not in stop])\nc.head()    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cef1f657ac3b141292aa1c174c64f3c4c6417c68"},"cell_type":"markdown","source":"<a id=\"7\"> Step 7 - Convert Word to Base Form or Lematize </a> \n\nConverting each word to its base form e.g. trying to try, or tried to try for simplification; using **WordNetLemmatizer** function from **NLTK** library."},{"metadata":{"trusted":true,"_uuid":"702e54ae6c5f03d284cc837f51b24401a4e0180a"},"cell_type":"code","source":"from tqdm import tqdm\nlemmatizer = WordNetLemmatizer()\ncom=[]\nfor y in tqdm(c):\n    new=[]\n    for x in y:\n        z=lemmatizer.lemmatize(x)\n        z=lemmatizer.lemmatize(z,'v')\n        new.append(z)\n    y=new\n    com.append(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48813f5e0a8bc5df6c43f10f2e49ede0c5036718"},"cell_type":"markdown","source":"Data obtained after Lemmatization is in array form, and is converted to Dataframe in the next step."},{"metadata":{"trusted":true,"_uuid":"44a940c65a571bcbf4d7e4fc474ca9558190e66f"},"cell_type":"code","source":"clean_data=pd.DataFrame(np.array(com), index=comments.index,columns={'comment_text'})\nclean_data['comment_text']=clean_data['comment_text'].str.join(\" \")\nprint(clean_data.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f07a2152a0da7e0e23f52993a78b2bd29ce76ff2"},"cell_type":"markdown","source":"Separating Train and Test Comments using the index stored earlier."},{"metadata":{"trusted":true,"_uuid":"e823e46e4afe9023f7f02a439e384edef2ab35fe"},"cell_type":"code","source":"train_clean_data=clean_data.loc[train_comments_index]\ntest_clean_data=clean_data.drop(train_comments_index,axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cff6c73cf88b0f335477fbd6f1f421672e6b73d"},"cell_type":"code","source":"print(\"PreProcessed Train Data : \",train_clean_data.head(5))\nprint(\"PreProcessed Test Data : \",test_clean_data.head(5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e87b3ce4ff75c0428cf6c02660fb1ab4a150070"},"cell_type":"markdown","source":"Merging comments and labels for training data set and ids for test data set."},{"metadata":{"trusted":true,"_uuid":"f370ed62a2c4b40c0528572d74fb7824386eb32a"},"cell_type":"code","source":"frames=[train_clean_data,labels]\ntrain_result = pd.concat(frames,axis=1)\nframes=[test.iloc[:,0],test_clean_data]\ntest_result = pd.concat(frames,axis=1)\nprint(train_result.head())\nprint(test_result.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77daf9455c751f8ef69e3de525fabce6d77c396c"},"cell_type":"markdown","source":"Saving data in csv format to use it in different notebook, or you can continue working in the same notebook."},{"metadata":{"trusted":true,"_uuid":"d3849add3d70a89ecb6bc9d2c8b67fe2af9ff913"},"cell_type":"code","source":"train_result.to_csv('train_data.csv', index = False)\ntest_result.to_csv('test_data.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19e0f6b3f322c86e6f4fd0546e107ae0128f163f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}