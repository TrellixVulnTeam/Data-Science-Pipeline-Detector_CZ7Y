{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"946e6cb5-2347-b09e-d017-c18a815fee6e"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb\nfrom sklearn import model_selection, preprocessing, ensemble, metrics\nfrom bayes_opt import BayesianOptimization"},{"cell_type":"markdown","metadata":{"_cell_guid":"85a6e1fa-fabd-74ec-3cd1-857ccbaef9ad"},"source":"**Data Preprocessing:**\n\nLet us load the data and do all the necessary pre-processing steps"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a14c422c-0d10-4495-0544-f380b541bbb6"},"outputs":[],"source":"# read the data into pandas dataframe #\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n# Getting the id column #\ntrain_id = train.id.values\ntest_id = test.id.values\n\n# names of numerical and categorical columns #\nnum_cols = ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul']\ncat_cols = ['color']\n\n# label encode the cat variable #\nlbl = preprocessing.LabelEncoder()\nlbl.fit(list(train['color'])+list(test['color']))\ntrain['color'] = lbl.transform(list(train['color']))\ntest['color'] = lbl.transform(list(test['color']))\ncolor_classes = lbl.classes_\n\n# label encode the target variable #\nlbl = preprocessing.LabelEncoder()\ntrain['type'] = lbl.fit_transform(list(train['type']))\ntrain_y = train.type.values\ntype_classes = lbl.classes_\n\n# Get the train and test (X and y variables) #\ntrain_X = np.array(train[num_cols+cat_cols])\ntest_X = np.array(test[num_cols+cat_cols])"},{"cell_type":"markdown","metadata":{"_cell_guid":"851ac878-6ad7-9be8-9096-f69c8f3cc137"},"source":"**Random Forest:**\n\nLet us start with the random forest classifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6dd5d955-666f-0cf5-58b6-cf44cced1130"},"outputs":[],"source":"# A helper function #\ndef cross_validation_genrator(splits=5, random_state=None):\n    kf = model_selection.KFold(n_splits=splits, shuffle=True, random_state=random_state)\n    for dev_index, val_index in kf.split(train_y):\n        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n        dev_y, val_y = train_y[dev_index], train_y[val_index]\n        yield dev_X, dev_y, val_X, val_y\n        \ndef run_rfcv(max_depth, min_samples_split, max_features):\n    cv_scores = []\n    for dev_X, dev_y, val_X, val_y in cross_validation_genrator(splits=5, random_state=0):\n        model = ensemble.RandomForestClassifier(max_depth=int(max_depth),\n                                                min_samples_split=int(min_samples_split),\n                                                max_features=min(max_features, 0.999),\n                                                n_estimators=100,\n                                                n_jobs=7,\n                                                random_state=1\n                                                )\n        model.fit(dev_X, dev_y)\n        pred_val_y = model.predict(val_X)\n        accuracy = metrics.accuracy_score(val_y, pred_val_y)\n        cv_scores.append(accuracy)\n    return np.mean(cv_scores)\n    \ndef run_rf(max_depth, min_samples_split, max_features):\n    model = ensemble.RandomForestClassifier(max_depth=int(max_depth),\n                                            min_samples_split=int(min_samples_split),\n                                            max_features=min(max_features, 0.999),\n                                            n_estimators=100,\n                                            n_jobs=7,\n                                            random_state=1)\n    model.fit(train_X, train_y)\n    preds = model.predict(test_X)\n    return preds\n    \nrf_params = {'max_depth': (25,40),\n             'min_samples_split': (2, 25),\n             'max_features': (0.1, 0.5)}\nrf_bo = BayesianOptimization(run_rfcv, rf_params)\nrf_bo.maximize(init_points=10, n_iter=25, acq='ei')\n\nprint(\"Final Result : \")\nprint(\"Best score : \",rf_bo.res['max']['max_val'])\nrf_best_params = rf_bo.res['max']['max_params']\nprint(\"Best params : \",rf_best_params)\npred_test_y = run_rf(rf_best_params['max_depth'], rf_best_params['min_samples_split'], rf_best_params['max_features'])\npred_test_y = [type_classes[pred] for pred in pred_test_y]\nout_df = pd.DataFrame({'id':test_id})\nout_df['type'] = pred_test_y\nout_df.to_csv(\"rf_bayesian_opt.csv\", index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"56950949-4d9a-99ec-d298-29e48abf99ad"},"source":"More to come.!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}