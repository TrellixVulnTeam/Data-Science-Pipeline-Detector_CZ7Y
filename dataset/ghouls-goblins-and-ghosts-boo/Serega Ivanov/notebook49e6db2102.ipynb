{"metadata":{"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","file_extension":".py","name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"metadata":{"_kg_hide-output":true,"_uuid":"59c26a788f50a9124b0e11576186fbbd96696177","_kg_hide-input":false,"_cell_guid":"ccc2020d-9a52-4bfa-8b4c-dfded4e9a814"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\n\ntrain = pd.read_csv(\"../input/train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/test.csv\", index_col=0)\n\ntrain.head()\n\n\n# Really simple data preparation\ny_train = pd.get_dummies(train[[\"type\"]], prefix=\"\")\ntrain.drop(\"type\", inplace=True, axis=1)\n\ntrain_test = pd.concat([train, test], axis=0)\n\n# It looks like the color actually is just noise, and does not give any signal to the monster-class.\n# Comment one of these lines.\n#train_test = pd.get_dummies( train_test, columns=[\"color\"], drop_first=False)\ntrain_test.drop(\"color\", inplace=True, axis=1)\n\nX_train = train_test.iloc[:len(y_train)]\nX_test  = train_test.iloc[len(y_train):]\n\n# Clean up\ndel train_test\ndel train\ndel test\n\n\n## A dead simple neural network class in Python+Numpy. Plain SGD, and no regularization.\ndef sigmoid(X):\n    return 1.0 / ( 1.0 + np.exp(-X) )\n\ndef softmax(X):\n    _sum = np.exp(X).sum()\n    return np.exp(X) / _sum\n\nclass neuralnet(object):\n    def __init__(self, num_input, num_hidden, num_output):\n        self._W1 = (np.random.random_sample((num_input, num_hidden)) - 0.5).astype(np.float32)\n        self._b1 = np.zeros((1, num_hidden)).astype(np.float32)\n        self._W2 = (np.random.random_sample((num_hidden, num_output)) - 0.5).astype(np.float32)\n        self._b2 = np.zeros((1, num_output)).astype(np.float32)\n\n    def forward(self,X):\n        net1 = np.matmul( X, self._W1 ) + self._b1\n        y = sigmoid(net1)\n        net2 = np.matmul( y, self._W2 ) + self._b2\n        z = softmax(net2)\n        return z,y\n\n    def backpropagation(self, X, target, eta):\n        z, y = self.forward(X)\n        d2 = (z - target)\n        d1 = y*(1.0-y) * np.matmul(d2, self._W2.T)\n        # The updates are done within this method. This more or less implies\n        # utpdates with Stochastic Gradient Decent. Let's fix that later.\n        # TODO: Support for full batch and mini-batches etc.\n        self._W2 -= eta * np.matmul(y.T,d2)\n        self._W1 -= eta * np.matmul(X.reshape((-1,1)),d1)\n        self._b2 -= eta * d2\n        self._b1 -= eta * d1\n        \n\n# Some hyper-parameters to tune.\nnum_hidden = 8\nn_epochs   = 1500\neta        = 0.01\n# Create the net.\nnn = neuralnet( X_train.shape[1], num_hidden, y_train.shape[1])\n\n# (EDIT: It's much faster to convert the dataframes to numpy arrays and then iterate)\nX = np.array(X_train, dtype=np.float32)\nY = np.array(y_train, dtype=np.float32)\nfor epoch in range(n_epochs):\n    for monster, target in zip(X,Y):\n        nn.backpropagation( monster, target, eta)\n\n        \nwith open('submission-{}-hidden.csv'.format(num_hidden), 'w') as f:\n    f.write(\"id,type\\n\")\n    for index, monster in X_test.iterrows():\n        probs = nn.forward( np.array(monster, dtype=np.float32))[0]\n        f.write(\"{},{}\\n\".format(index, y_train.columns.values[np.argmax(probs)][1:]))","execution_count":null,"cell_type":"code"}],"nbformat":4,"nbformat_minor":1}