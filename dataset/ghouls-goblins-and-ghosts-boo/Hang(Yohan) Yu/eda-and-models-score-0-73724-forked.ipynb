{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d380e2ad-947b-d014-44cd-72cbaeeb2a0e"},"source":"This is my attempt in analyzing this dataset. At first some data exploration with insights. Then trying to predict with various models."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af87edf5-f225-1037-4893-e15589fdaf4e"},"outputs":[],"source":"#Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5dba8f0f-f47a-41ad-8af1-6aadbfd27fb4"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f09d961-e78f-7f5e-43f8-5697ba33621b"},"outputs":[],"source":"train.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d00a3977-5efc-4381-c137-f0ac349affee"},"source":"So there is 1 categorical and 4 numerical variables. And no missing values, which is nice!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6b808a59-d41e-c422-95bc-a07b52433def"},"outputs":[],"source":"train.describe(include='all')"},{"cell_type":"markdown","metadata":{"_cell_guid":"69676445-d7a2-0175-dc8a-115494a8dcfc"},"source":"Numerical columns are either normalized or show a percentage, so no need to scale them."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"714c4080-d68e-193c-8ec3-5531b9d39936"},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7800826-4182-ffb4-bbf4-9cafe313f02d"},"outputs":[],"source":"plt.subplot(1,4,1)\ntrain.groupby('type').mean()['rotting_flesh'].plot(kind='bar',figsize=(7,4), color='r')\nplt.subplot(1,4,2)\ntrain.groupby('type').mean()['bone_length'].plot(kind='bar',figsize=(7,4), color='g')\nplt.subplot(1,4,3)\ntrain.groupby('type').mean()['hair_length'].plot(kind='bar',figsize=(7,4), color='y')\nplt.subplot(1,4,4)\ntrain.groupby('type').mean()['has_soul'].plot(kind='bar',figsize=(7,4), color='teal')"},{"cell_type":"markdown","metadata":{"_cell_guid":"aac1e55a-5548-21f7-0a67-8201c7d64744"},"source":"It seems that all numerical features may be useful."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa607bdb-102a-d2ed-14ec-5bbef10a49d8"},"outputs":[],"source":"sns.factorplot(\"type\", col=\"color\", col_wrap=4, data=train, kind=\"count\", size=2.4, aspect=.8)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ca9c9c9f-1ae8-54e5-6ad5-6dbd78da3f24"},"source":"Funny, but many colors are evenly distributes among the monsters. So they maybe nor very useful for analysis."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"099fb0fe-a593-0857-b672-35da0eea7f56"},"outputs":[],"source":"#test_id will be used later, so save it\ntest_id = test['id']\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5291ac0e-ef49-f524-c4d4-e59ab5995844"},"outputs":[],"source":"#Deal with 'color' column\ncol = 'color'\ndummies = pd.get_dummies(train[col], drop_first=False)\ndummies = dummies.add_prefix(\"{}#\".format(col))\ntrain.drop(col, axis=1, inplace=True)\ntrain = train.join(dummies)\ndummies = pd.get_dummies(test[col], drop_first=False)\ndummies = dummies.add_prefix(\"{}#\".format(col))\ntest.drop(col, axis=1, inplace=True)\ntest = test.join(dummies)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8dd366d9-1ced-1e73-d738-4e45af67f84e"},"outputs":[],"source":"X_train = train.drop('type', axis=1)\nle = LabelEncoder()\nY_train = le.fit_transform(train.type.values)\nX_test = test"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9d2229e-f1b1-9988-20f7-6359d0a8558e"},"outputs":[],"source":"clf = RandomForestClassifier(n_estimators=200)\nclf = clf.fit(X_train, Y_train)\nindices = np.argsort(clf.feature_importances_)[::-1]\n\n# Print the feature ranking\nprint('Feature ranking:')\n\nfor f in range(X_train.shape[1]):\n    print('%d. feature %d %s (%f)' % (f + 1, indices[f], X_train.columns[indices[f]],\n                                      clf.feature_importances_[indices[f]]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ef55766-2301-7a2d-adaa-f8d1c8b12367"},"source":"As graphs showed color has little impact, so I won't use it. In fact I tried using all features, but the result was worse than when I didn't use blood."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc9d1dcc-2a4a-15e2-722a-f174431b5a04"},"outputs":[],"source":"best_features=X_train.columns[indices[0:4]]\nX = X_train[best_features]\nXt = X_test[best_features]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05e2fd94-1f90-41f1-59d0-c38fe16e869f"},"outputs":[],"source":"#Splitting data for validation\nXtrain, Xtest, ytrain, ytest = train_test_split(X, Y_train, test_size=0.20, random_state=36)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c06a887-13da-5c8b-b662-91dc48f61c1b"},"outputs":[],"source":"#At first I try Random Forest.\n#Normally you input all parameters and their potential values and run GridSearchCV.\n#My PC isn't good enough so I divide parameters in two groups and repeatedly run two GridSearchCV until I'm satisfied with the result.\nforest = RandomForestClassifier(max_depth = None,                                \n                                min_samples_split =5,\n                                min_weight_fraction_leaf = 0.0,\n                                max_leaf_nodes = 60)\n\nparameter_grid = {'n_estimators' : [10, 20, 100, 150],\n                  'criterion' : ['gini', 'entropy'],\n                  'max_features' : ['auto', 'sqrt', 'log2', None]\n                 }\n\ngrid_search = GridSearchCV(forest, param_grid=parameter_grid, cv=StratifiedKFold(5))\ngrid_search.fit(X, Y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e960059-9e6e-c8b3-317b-32cb80c1ae76"},"outputs":[],"source":"forest = RandomForestClassifier(n_estimators = 20,\n                                criterion = 'entropy',\n                                max_features = 'auto')\nparameter_grid = {\n                  'max_depth' : [None, 5, 20, 100],\n                  'min_samples_split' : [2, 5, 7],\n                  'min_weight_fraction_leaf' : [0.0, 0.1],\n                  'max_leaf_nodes' : [40, 60, 80],\n                 }\n\ngrid_search = GridSearchCV(forest, param_grid=parameter_grid, cv=StratifiedKFold(5))\ngrid_search.fit(X, Y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"deb314e8-e014-1ded-0a99-279a5a1d4de3"},"outputs":[],"source":"#Optimal parameters\nclf = RandomForestClassifier(n_estimators=20, n_jobs=-1, criterion = 'entropy', max_features = 'auto',\n                             min_samples_split=5, min_weight_fraction_leaf=0.0,\n                             max_leaf_nodes=60, max_depth=100)\n#Calibration improves probability predictions\ncalibrated_clf = CalibratedClassifierCV(clf, method='isotonic', cv=5)\ncalibrated_clf.fit(Xtrain, ytrain)\ny_val = calibrated_clf.predict_proba(Xtest)\n#Prediction 'y_val' shows probabilities of classes, so at first the most probable class is chosen,\n#then it is converted to classes.\nprint(\"Validation accuracy: \", sum(pd.DataFrame(y_val, columns=le.classes_).idxmax(axis=1).values == le.inverse_transform(ytest))/len(ytest))"},{"cell_type":"markdown","metadata":{"_cell_guid":"2e07caf0-1d6e-4679-34b9-de13789e6aee"},"source":"I used the best parameters and validation accuracy is ~70,66-72%. Not bad. But let's try something else."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3863caca-078b-85d2-c4c7-8002ef34e2c8"},"outputs":[],"source":"svc = svm.SVC(kernel='linear')\nsvc.fit(Xtrain, ytrain)\ny_val_s = svc.predict(Xtest)\nprint(\"Validation accuracy: \", sum(le.inverse_transform(y_val_s)\n                                   == le.inverse_transform(ytest))/len(ytest))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0d1f3c90-0f9f-9d13-657e-96ed2f72c27b"},"source":"Much better! Usually RandomForest requires a lot of data for good performance. It seems that in this case there was too little data for it."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"473e57bc-3055-5919-3cff-b156bdbcd72a"},"outputs":[],"source":"#The last model is logistic regression\nlogreg = LogisticRegression()\n\nparameter_grid = {'solver' : ['newton-cg', 'lbfgs'],\n                  'multi_class' : ['ovr', 'multinomial'],\n                  'C' : [0.005, 0.01, 1, 10, 100, 1000],\n                  'tol': [0.0001, 0.001, 0.005]\n                 }\n\ngrid_search = GridSearchCV(logreg, param_grid=parameter_grid, cv=StratifiedKFold(5))\ngrid_search.fit(Xtrain, ytrain)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af1d8915-1556-4bf4-fed2-d9f489758c9f"},"outputs":[],"source":"log_reg = LogisticRegression(C = 1, tol = 0.0001, solver='newton-cg', multi_class='multinomial')\nlog_reg.fit(Xtrain, ytrain)\ny_val_l = log_reg.predict_proba(Xtest)\nprint(\"Validation accuracy: \", sum(pd.DataFrame(y_val_l, columns=le.classes_).idxmax(axis=1).values\n                                   == le.inverse_transform(ytest))/len(ytest))"},{"cell_type":"markdown","metadata":{"_cell_guid":"72769754-988f-4e96-0043-516fda73cad0"},"source":"It seems that regression is better. The reason? As far as I understand, the algorithms are similar, but with different loss function. And most importantly: SVC is a hard classifier and LR gives probabilities."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad95ad48-97fb-9dd5-7f17-11221a727b37"},"outputs":[],"source":"#So this is it. Now fit and model on full dataset\nlog_reg.fit(X, Y_train)\ny_pred = log_reg.predict_proba(Xt)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c549039-a53e-b097-fbfb-f3a6bd6c83f8"},"outputs":[],"source":"submission = pd.DataFrame({'id':test_id,\n                           'type':pd.DataFrame(y_pred, columns=le.classes_).idxmax(axis=1).values})"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e74b0d0-1fd4-0cdb-9cc8-593904fea2c4"},"outputs":[],"source":"submission.to_csv('GGG_submission.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6cb777b7-7fac-50af-2e58-78337498f985"},"source":"This solution gave me score of 0.73724."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}