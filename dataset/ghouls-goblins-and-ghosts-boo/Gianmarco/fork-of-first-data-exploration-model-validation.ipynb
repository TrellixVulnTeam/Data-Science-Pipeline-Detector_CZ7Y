{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"93755911-f308-1365-7e8a-b4a2e7e20691"},"source":"![alt text](https://www.kaggle.com/static/images/site-logo.png \"Kaggle logo\")\n<div align=\"center\">\n    <a href=\"https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo\">\n        <h1>\n            Ghouls, Goblins, and Ghosts... Boo!\n        </h1>\n    </a>\n</div>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ed4ca15-fd12-5ab3-4116-6c27eb3b2e5c"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"04fa7460-566b-0e05-5676-35a96a45af36"},"outputs":[],"source":"# SETUP\n%matplotlib inline\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport zipfile\nimport warnings\nimport copy\nwarnings.filterwarnings(\"ignore\")\n\nimport plotly\nimport plotly.graph_objs as go\nplotly.offline.init_notebook_mode()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3cfda81-5c75-1d51-e1ba-8112d554ffdc"},"outputs":[],"source":"# Define some functions\nimport collections\n\ndef flatten(d, parent_key='', sep='_'):\n    \"\"\" Function that flatten a dict \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\ndef showExecTime(startPoint, initialString = \"\", verbose=True):\n    \"\"\"\n    Compute the execution time from an initial starting point.\n    You can also pass me a string to print out at the end of computation.\n    \n    Parameters\n    ----------\n    startPoint : float, timestamp of the starting point\n    initialString : string to output on the console, before the execution time\n    \n    Returns\n    -------\n    endPoint - startPoint, the difference between the two timestamps\n    \"\"\"\n    eex = time.time()\n    seconds = round(eex - startPoint, 2)\n    minutes = (seconds/60)\n    hours = int(minutes/60)\n    minutes = int(minutes % 60)\n    seconds = round(seconds % 60, 2)\n    if verbose:\n        print(\"\\n- \"+initialString+\" Execution time: %sh %sm %ss -\" % (hours, minutes, seconds))\n    return eex - startPoint\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c9eca4b5-2064-9899-d033-e7fff8c7ed18"},"outputs":[],"source":"# Load the test and the train tables\ntest = pd.read_csv(\"../input/test.csv\")\ntrain = pd.read_csv(\"../input/train.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"db453ac6-7785-7189-5e2c-003486310e01"},"source":"***\n<div>\n    <h1>Data Exploration</h1>\n</div>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a645942-c1af-ab92-0c63-6e3a10862dc3"},"outputs":[],"source":"# Look the train\nprint(\"\\nTrain structure:\\n\\n\", train.head(5))\n# Look the test\nprint(\"\\nTest structure:\\n\\n\", test.head(5))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ef1781d7-8834-a5df-6c52-b6edc1c7561c"},"source":"<p>\nData fields:\n<ul>\n    <li><b>id</b>              - id of the creature</li>\n    <li><b>bone_length</b>     - average length of bone in the creature, normalized between 0 and 1</li>\n    <li><b>rotting_flesh</b>   - percentage of rotting flesh in the creature</li>\n    <li><b>hair_length</b>     - average hair length, normalized between 0 and 1</li>\n    <li><b>has_soul</b>        - percentage of soul in the creature</li>\n    <li><b>color</b>           - dominant color of the creature: 'white', 'black', 'clear', 'blue', 'green', 'blood'</li>\n    <li><b>type</b>            - target variable: 'Ghost', 'Goblin', and 'Ghoul'</li>\n</ul>\n</p>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1bcd8ee1-494d-7075-e273-8496f9311444"},"outputs":[],"source":"print(\"\\nTrain info:\\n\")\ntrain.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bdcbff92-8cdb-017c-d71b-81e5f928146f"},"source":"<p>\nIn the end, we have <b>4 continous variables</b> (bone_length, rotting_flesh, hair_length, has_soul) and <b>1 categorical variable</b> (color).<br>\nWe are able to see some <b>statistics</b> of the numerical variables.\n</p>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e58f9a62-5b3f-4880-ff25-49ed1212b69f"},"outputs":[],"source":"print(\"\\nTrain description:\\n\\n\", train.drop('id', axis=1, inplace=False).describe())\nprint(\"\\nTest description:\\n\\n\", test.drop('id', axis=1, inplace=False).describe())"},{"cell_type":"markdown","metadata":{"_cell_guid":"0f951650-4dfd-70c7-fb56-2742820f19b1"},"source":"<p>\nNow we create a map that translate the type of creature into a color.<br>\nWe can use this one:\n<ul>\n    <li><b>Ghost</b>  - <span style=\"color:#ff4141\">#ff4141 (red)</span></li>\n    <li><b>Ghoul</b>  - <span style=\"color:#995bbe\">#995bbe (violet)</span></li>\n    <li><b>Goblin</b> - <span style=\"color:#16dc88\">#16dc88 (green)</span></li>\n</ul>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c83fb8f-32a8-9896-69d1-cc27a0cf9c59"},"outputs":[],"source":"colors = {\n    \"Ghost\" : \"#ff4141\",\n    \"Ghoul\" : \"#995bbe\",\n    \"Goblin\": \"#16dc88\"\n}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce7f80b4-b6b1-5b4d-60f8-cf0f21c4b549"},"outputs":[],"source":"sns.set(style=\"whitegrid\", context=\"talk\")\nsns.set_color_codes(\"pastel\")\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(7, 4))\n\nx, y = [], []\nfor key in colors:\n    x.append(key)\n    y.append(train['type'].value_counts()[key])\n\n# Plot the different type occurrences\nsns.barplot(x, y, palette=colors)\n\n# Finalize the plot\nfor n, (label, _y) in enumerate(zip(x, y)):\n    ax.annotate( # Attach the counts\n        s='{:.0f}'.format(abs(_y)),\n        xy=(n, _y),\n        ha='center',va='center',\n        xytext=(0,10),\n        textcoords='offset points',\n        weight='bold'\n    )\n    ax.annotate( # Attach the type label\n        s=label,\n        xy=(n, _y),\n        ha='center',va='center',\n        xytext=(0,-15),\n        textcoords='offset points',\n        weight='bold'\n    )\n\n# Add a legend and informative axis label\nax.set(ylabel=\"Number of occurrences\", xlabel=\"Creature type\")\nax.set_xticks([])\nplt.setp(f.axes, yticks=[])\nplt.title(\"Monsters occurrences\")\nplt.tight_layout(h_pad=3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49ff6ccf-cd69-a7dd-e29a-49f258ae9b30"},"outputs":[],"source":"fig = {\n    'data': [{'labels': x,\n              'values': y,\n              'marker': {'colors': [colors[m] for m in x]},\n              'type': 'pie'}],\n    'layout': {'title': 'Monsters occurences'}\n}\n\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode()\n\niplot(fig, filename='monsters_in_pie')"},{"cell_type":"markdown","metadata":{"_cell_guid":"f7c20c16-fbf6-3e7d-1742-2d40703cbb39"},"source":"<p>\n<h2>Numerical variables</h2><br>\nWe can have a look at the different creature with respect to different features.<br>\nWe have 4 different continous variable, then we can produce 6 different plot with distribution from each creature type.\n</p>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a59cc49f-d650-43a9-3627-67eb60967c48"},"outputs":[],"source":"colormap = {\n    \"Ghost\" : \"Reds\",\n    \"Ghoul\" : \"BuPu\",\n    \"Goblin\": \"Greens\"\n}\n\n# Subset the dataset by creature\nghost = train.query(\"type == 'Ghost'\")\nghoul = train.query(\"type == 'Ghoul'\")\ngoblin = train.query(\"type == 'Goblin'\")\n\nfeatures = list(train.describe().columns[1:])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bd8a07f-a854-55b8-d420-32794b792fc4"},"outputs":[],"source":"# Set up the matplotlib figure\nf, axes = plt.subplots(2, 3, figsize=(7, 7), sharex=False, sharey=False)\n\nax_ind = 0\nfor i in range(len(features)-1):\n    for j in range(i+1, len(features)):\n        # Get the features\n        feat1 = features[i]\n        feat2 = features[j]\n        \n        # Set up the figure\n        ax = axes.flat[ax_ind]\n        ax_ind += 1\n        ax.set_aspect(\"equal\")\n\n        # Draw the three density plots\n        sns.kdeplot(ghost[feat1], ghost[feat2], ax=ax,\n                         cmap=colormap['Ghost'], shade=True, shade_lowest=False, alpha=.6)\n        sns.kdeplot(ghoul[feat1], ghoul[feat2], ax=ax,\n                         cmap=colormap['Ghoul'], shade=True, shade_lowest=False, alpha=.6)\n        sns.kdeplot(goblin[feat1], goblin[feat2], ax=ax,\n                         cmap=colormap['Goblin'], shade=True, shade_lowest=False, alpha=.6)\n\n# Conclude\nplt.suptitle(\"Bivariate kernel densities\")\nf.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"adbf8b77-0052-c84a-a90c-37bf2f90e8e2"},"source":"<p>\nMaybe, also a scatterplot matrix could be useful.\n</p>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1a9e9d6a-d252-8451-536d-232f21fbeb6f"},"outputs":[],"source":"# Set up the matplotlib figure\nsns.pairplot(train.drop('id', axis=1, inplace=False),\n             palette=colors, hue=\"type\",\n             diag_kind=\"kde\", diag_kws=dict(shade=True))\nplt.suptitle(\"Pairwise relationships in the dataset\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"5334ad09-b1a6-52c3-55aa-19927876f3e0"},"source":"<p>\nWe can notice how <i>has_soul</i> and <i>hair_length</i> seems to be the most discrimant features, in the same \"direction\".<br>\nIn the same way, <i>bone_length</i> seems a little less strong.<br>\nThen, we can create some <i>new variables</i> that are the combination of the original ones and analyze them.\n</p>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ebb5922-cbc8-8f40-f770-9756cf4b2509"},"outputs":[],"source":"train['new_var'] = train['hair_length'] + train['has_soul'] + train['bone_length'] - train['rotting_flesh']\ntrain['new_var'] = train['new_var'] - min(train['new_var'])\ntrain['new_var'] = train['new_var'] / max(train['new_var'])\ntrain.describe()['new_var']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12ae646a-0dc6-ae1e-05e5-7a856086819f"},"outputs":[],"source":"# Insert other features\ntrain['bone_hair'] = train['hair_length'] + train['bone_length']\ntrain['bone_hair'] = train['bone_hair'] - min(train['bone_hair'])\ntrain['bone_hair'] = train['bone_hair'] / max(train['bone_hair'])\n\ntrain['hair_soul'] = train['hair_length'] + train['has_soul']\ntrain['hair_soul'] = train['hair_soul'] - min(train['hair_soul'])\ntrain['hair_soul'] = train['hair_soul'] / max(train['hair_soul'])\n\ntrain['bone_soul'] = train['bone_length'] + train['has_soul']\ntrain['bone_soul'] = train['bone_soul'] - min(train['bone_soul'])\ntrain['bone_soul'] = train['bone_soul'] / max(train['bone_soul'])\n\ntrain['flesh_soul'] = train['rotting_flesh'] + train['has_soul']\ntrain['flesh_soul'] = train['flesh_soul'] - min(train['flesh_soul'])\ntrain['flesh_soul'] = train['flesh_soul'] / max(train['flesh_soul'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0bfd9b7d-bbfa-46b0-7bc3-de7c56d0b389"},"outputs":[],"source":"# Insert other very strange features\ntrain['new_var_2'] = (train['hair_length'] * train['has_soul'] * train['bone_length'] * (1-train['rotting_flesh'])) ** (1/4)\n\ntrain['bone_hair_2'] = (train['hair_length'] * train['bone_length']) ** (1/2)\n\ntrain['flesh_soul_2'] = (train['rotting_flesh'] * train['has_soul']) ** (1/2)\n\ntrain['hair_soul_2'] = (train['hair_length'] * train['has_soul']) ** (1/2)\n\ntrain['bone_soul_2'] = (train['bone_length'] * train['has_soul']) ** (1/2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9bc3373-a05d-d567-11da-6c8c91016e90"},"outputs":[],"source":"# Group together similar features names\nold_numerical = ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul']\nnew_ones = ['new_var', 'bone_hair', 'flesh_soul', 'hair_soul', 'bone_soul']\nnew_ones_2 = [x+'_2' for x in new_ones]\nnew_ones, new_ones_2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"958ac700-0863-07c3-053e-6b1ffeebe991"},"outputs":[],"source":"# Subset the dataset by creature\nghost = train.query(\"type == 'Ghost'\")\nghoul = train.query(\"type == 'Ghoul'\")\ngoblin = train.query(\"type == 'Goblin'\")\n\nsns.set(style=\"darkgrid\")\n\n# Draw the three density plots\nplt.figure(figsize=(8, 4))\nfeature_you_want = 'new_var_2' # -> you can modify it, pick what you want!\nsns.kdeplot(ghost[feature_you_want], color=colors['Ghost'], shade=True, shade_lowest=False, alpha=.6)\nsns.kdeplot(goblin[feature_you_want], color=colors['Goblin'], shade=True, shade_lowest=False, alpha=.6)\nsns.kdeplot(ghoul[feature_you_want], color=colors['Ghoul'], shade=True, shade_lowest=False, alpha=.6)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07a9d507-f4ea-d4a8-c150-1b90bceb26d5"},"outputs":[],"source":"# Boxplots for new variables - Grouped Horizontal Box Plot\ndata = [\n    # One dictionary for each monster type\n    {\n        # Ghost\n        'name': 'Ghost',\n        'x': sum([list(ghost[var]) for var in new_ones + new_ones_2], []),\n        'y': sum([[var]*len(ghost) for var in new_ones + new_ones_2], []),\n        'marker': {'color': colors['Ghost']},\n        'boxmean': False,\n        'orientation': 'h',\n        'type': 'box',\n    },\n    {\n        # Goblin\n        'name': 'Goblin',\n        'x': sum([list(goblin[var]) for var in new_ones + new_ones_2], []),\n        'y': sum([[var]*len(goblin) for var in new_ones + new_ones_2], []),\n        'marker': {'color': colors['Goblin']},\n        'boxmean': False,\n        'orientation': 'h',\n        'type': 'box',\n    },\n    {\n        # Ghoul\n        'name': 'Ghoul',\n        'x': sum([list(ghoul[var]) for var in new_ones + new_ones_2], []),\n        'y': sum([[var]*len(ghoul) for var in new_ones + new_ones_2], []),\n        'marker': {'color': colors['Ghoul']},\n        'boxmean': False,\n        'orientation': 'h',\n        'type': 'box',\n    }\n]\n\nlayout = {\n    'title': 'Analysis on New Features Distributions',\n    'xaxis': {\n        'title': 'normalized moisture',\n        'zeroline': True,\n    },\n    'boxmode': 'group',\n    'height': 1600,\n}\n\nfig = go.Figure(data=data, layout=layout)\n\nplotly.offline.iplot(fig)"},{"cell_type":"markdown","metadata":{"_cell_guid":"be9f5128-115f-2985-c2f1-45044c76df3a"},"source":"<p>\nWe can notice the relevant new features and the not-so-meaningful ones.<br>\nIt seems that <i>flesh_soul</i> and <i>flesh_soul_2</i> are a bit confusing, while the remaining ones are more or less useful in the same way.\n</p>"},{"cell_type":"markdown","metadata":{"_cell_guid":"e984c7fc-ef93-65ed-c2e5-5d3b1f05473c"},"source":"<p>\n<h2>Categorical variables</h2><br>\n\nWe should also explore the categorical variable referring to the color.\n\n</p>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b12a3828-45a5-43be-75b9-d30671ce02d7"},"outputs":[],"source":"plt.figure(figsize=(9,4))\nsns.countplot(x='color', hue='type', palette=colors, data=train)\nplt.suptitle(\"Distribution of the 'color' class\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e1470e49-077b-3514-32a2-5785926a7994"},"source":"<p>\nAll alone, the <i>green</i>, the <i>black</i> and the <i>blue</i> monsters are not so distinguishable.\n</p>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49906d2e-dddf-5a03-0de4-2c74286532ea"},"outputs":[],"source":"all_colors = list(set(train['color'].values))\nuseful_colors = ['clear', 'white', 'blood']\ncolor =  pd.get_dummies(train['color'])\ntrain_data = pd.concat([train, color[useful_colors]], axis = 1)\ntrain_data.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61d942b1-6c1f-aab8-da5a-0a27aa9f7291"},"outputs":[],"source":"sns.pairplot(train_data[useful_colors + ['type', 'new_var', 'hair_soul_2']], hue=\"type\",\n             diag_kind=\"kde\", diag_kws=dict(shade=True))\nplt.suptitle(\"Pairwise relationships in the dataset\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c2d156fd-f54f-4200-9f46-6e180069c45c"},"source":"<p>\nWe can see how the <i>new variable</i> together with the <i>useful colors</i> could provide nice insights.<br>\nWe should exploit this fact in order to reach better results in the classification.\n</p>"},{"cell_type":"markdown","metadata":{"_cell_guid":"e24524b8-787e-e8fa-256b-195159d3fcd8"},"source":"***\n<p>\n<h1>Data Modeling</h1><br>\nIn this part, we try to <b>create some models</b> from our data, in order to characterize our creatures.<br>\nWe fit the data in <u>different models</u>, feeding them with <u>different attributes</u>, with the objective to <b>compare them</b> and, in the end, pick the best one.\n</p>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b5f35a9-22fa-46a9-1195-534d17bce3c3"},"outputs":[],"source":"from sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import naive_bayes\nfrom sklearn import ensemble\nfrom sklearn import tree\nfrom sklearn import dummy\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn import model_selection\nfrom sklearn import linear_model\nimport time\n\ntry:\n    train.drop(['flesh_soul', 'flesh_soul_2'], axis=1, inplace=True)\n    new_ones.remove('flesh_soul')\n    new_ones_2.remove('flesh_soul_2')\nexcept:\n    # already dropped\n    pass\n\n# Insert other features in the test set\ntest['new_var'] = test['hair_length'] + test['has_soul'] + test['bone_length'] - test['rotting_flesh']\ntest['new_var'] = test['new_var'] - min(test['new_var'])\ntest['new_var'] = test['new_var'] / max(test['new_var'])\n\ntest['bone_hair'] = test['hair_length'] + test['bone_length']\ntest['bone_hair'] = test['bone_hair'] - min(test['bone_hair'])\ntest['bone_hair'] = test['bone_hair'] / max(test['bone_hair'])\n\ntest['hair_soul'] = test['hair_length'] + test['has_soul']\ntest['hair_soul'] = test['hair_soul'] - min(test['hair_soul'])\ntest['hair_soul'] = test['hair_soul'] / max(test['hair_soul'])\n\ntest['bone_soul'] = test['bone_length'] + test['has_soul']\ntest['bone_soul'] = test['bone_soul'] - min(test['bone_soul'])\ntest['bone_soul'] = test['bone_soul'] / max(test['bone_soul'])\n\n# Insert vother ery strange features\ntest['new_var_2'] = (test['hair_length'] * test['has_soul'] * test['bone_length'] * (1-test['rotting_flesh'])) ** (1/4)\ntest['bone_hair_2'] = (test['hair_length'] * test['bone_length']) ** (1/2)\ntest['hair_soul_2'] = (test['hair_length'] * test['has_soul']) ** (1/2)\ntest['bone_soul_2'] = (test['bone_length'] * test['has_soul']) ** (1/2)\n\ncolor =  pd.get_dummies(test['color'])\ntest_data = pd.concat([test, color[all_colors]], axis = 1)\n\ncolor =  pd.get_dummies(train['color'])\ntrain_data = pd.concat([train, color[all_colors]], axis = 1)\n\ntrain_data = train_data.drop('id', axis=1, inplace=False)\ntest_data = test_data.drop('id', axis=1, inplace=False)\n\n# Asert that test set has only the 'type' column left\ntrain_data.columns - test_data.columns == ['type']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"213df748-4389-a224-8d6e-d010edc592c1"},"outputs":[],"source":"# MODELS\n\n# Naive Bayes\nnb = {'name': 'Bernoulli NaiveBayes'}\nnb['model'] = naive_bayes.BernoulliNB()\n\n# Logistic Regression\nlr = {'name': 'Logistic Regression'}\nlr['model'] = linear_model.LogisticRegression(solver='lbfgs')\n\n# Logistic Regression with CV\nlrcv = {'name': 'Cross-Validated Logistic Regression'}\nlrcv['model'] = linear_model.LogisticRegressionCV(Cs=100, solver='lbfgs', n_jobs=-1)\n\n# SVC\nsvc = {'name': 'Support Vector Machine'}\nsvc['model'] = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True))\n\n# Decision Tree\ndtree = {'name': 'Decision Tree'}\ndtree['model'] = tree.DecisionTreeClassifier(max_depth=20, min_samples_leaf=3)\n\n# Random Forest\nrf = {'name': 'Random Forest'}\nrf['model'] = ensemble.RandomForestClassifier(n_estimators=1000, max_depth=20, min_samples_leaf=3, n_jobs=-2)\n\n# K-Nearest Neighbors\nk, w = 7, ['uniform', 'distance'][1]\nknn = {'name': str(k)+'-Nearest Neighbors '+w}\nknn['model'] = KNeighborsClassifier(n_neighbors=k, weights=w, algorithm='auto', n_jobs=-2)\n\n# Dummy most_frequent - baseline\ndummy_uni = {'name': 'Dummy MostFrequent'}\ndummy_uni['model'] = dummy.DummyClassifier(strategy='most_frequent')\n\n# Dummy Stratified - baseline\ndummy_str = {'name': 'Dummy Stratified'}\ndummy_str['model'] = dummy.DummyClassifier(strategy='stratified')"},{"cell_type":"markdown","metadata":{"_cell_guid":"8d61a75b-e9f7-38ea-1feb-008c624513e1"},"source":"***\n<p>\n<h1>Model Validation</h1><br>\nIn this part, we validate our models.\n</p>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d6fd70b-4b06-11c0-a491-509065311a83"},"outputs":[],"source":"# VALIDATION\n# Keep all the models together\nmodels = [nb, lr, lrcv, svc, dtree, rf, knn, dummy_uni, dummy_str]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"477ecee9-53e5-3f6d-4766-9c4129238bb4"},"outputs":[],"source":"# Evaluate each model\ndef evaluate_models(models, features, verbose=True, n_splits=10):\n    for model in models:\n        clf = model['model']\n        if verbose: print(\"\\n\"+model['name'])\n        \n        # Fitting\n        begin = time.time()\n        clf.fit(train_data[features], train_data['type'])\n        model['fit_time'] = showExecTime(begin, model['name']+\" fitted.\", verbose)\n\n        # Prediction on the entire train set, where they are fitted\n        begin = time.time()\n        predicted_train = np.array(clf.predict_proba(train_data[features]))\n        model['prediction_time'] = showExecTime(begin, model['name']+\" prediction complete.\", verbose)\n        logloss = log_loss(train_data['type'].values, predicted_train)\n        model['train_log_loss'] = logloss    \n        if verbose: print (\"\\n\\tLog_loss on the train:\", logloss)\n\n        # Accuracy performance\n        predicted_train = np.array(clf.predict(train_data[features]))\n        accuracy_score(train_data['type'], predicted_train)\n        accuracy = accuracy_score(train_data['type'], predicted_train)\n        model['train_accuracy'] = accuracy\n        if verbose: print(\"\\tTrain accuracy:\", accuracy)    \n\n        # Strified K-Fold\n        skf = model_selection.StratifiedKFold(n_splits=n_splits)\n        logloss_train, logloss_test = [], []\n        accuracies = []\n        if verbose: print(\"\\n\\tStratified K-Fold\")\n        for train_index, test_index in skf.split(train_data[features], train_data['type']):\n            #print(\"\\t\\tTEST FOLD: [%d: %d]\" % (test_index[0], test_index[-1]))\n            x_train, x_test = train_data[features].iloc[train_index], train_data[features].iloc[test_index]\n            y_train, y_test = train_data['type'][train_index], train_data['type'][test_index]\n\n            # Fit the model with the X at each iteration\n            clf.fit(x_train, y_train)\n\n            # Probabilities estimation\n            predict_proba_train = np.array(clf.predict_proba(x_train))\n            predict_proba_test = np.array(clf.predict_proba(x_test))\n            # Log loss\n            logloss_train.append(log_loss(y_train, predict_proba_train))\n            logloss_test.append(log_loss(y_test, predict_proba_test))\n\n            # Classification\n            predict_test = np.array(clf.predict(x_test))\n            # Accuracy\n            accuracy = accuracy_score(y_test, predict_test)\n            accuracies.append(accuracy)\n\n        model['skf_validation'] = {\n            'logloss_train': {\n                'mean': np.mean(logloss_train),\n                'std': np.std(logloss_train)\n            },\n            'logloss_test': {\n                'mean': np.mean(logloss_test),\n                'std': np.std(logloss_test)\n            },\n            'accuracy': {\n                'mean': np.mean(accuracies),\n                'std': np.std(accuracies),\n            }\n        }\n        if verbose:\n            print (\"\\tLog_loss on the train: %0.2f (+/- %0.2f)\" % (np.mean(logloss_train), np.std(logloss_train) * 2))\n            print (\"\\tLog_loss on the test: %0.2f (+/- %0.2f)\" % (np.mean(logloss_test), np.std(logloss_test) * 2))\n            print (\"\\tClassification accuracy: %0.2f (+/- %0.2f)\" % (np.mean(accuracies), np.std(accuracies) * 2))\n    return models"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22d46567-cfef-5f96-a2a4-5c0c5831496e"},"outputs":[],"source":"# Select the features\nall_features = all_colors + new_ones + new_ones_2 + old_numerical\nprint(all_features)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55fb4c1f-b2d2-8911-ea3b-fe560c5bf2f2"},"outputs":[],"source":"print(\"Validating group 1\")\nmodels1 = {'data': evaluate_models(copy.deepcopy(models), all_colors, False),\n           'attr': 'only colors'}\nprint(\"Validating group 2\")\nmodels2 = {'data': evaluate_models(copy.deepcopy(models), old_numerical, False),\n           'attr': 'old numerical'}\nprint(\"Validating group 3\")\nmodels3 = {'data': evaluate_models(copy.deepcopy(models), old_numerical + new_ones + new_ones_2, False),\n           'attr': 'all numerical'}\nprint(\"Validating group 4\")\nmodels4 = {'data': evaluate_models(copy.deepcopy(models), all_features, False),\n           'attr': 'all features'}\nprint(\"Validating group 5\")\nmodels5 = {'data': evaluate_models(copy.deepcopy(models), new_ones, False),\n           'attr': 'new ones'}\nprint(\"Validating group 6\")\nmodels6 = {'data': evaluate_models(copy.deepcopy(models), new_ones_2, False),\n           'attr': 'new ones squared'}\nprint(\"Validating group 7\")\nmodels7 = {'data': evaluate_models(copy.deepcopy(models), new_ones + new_ones_2, False),\n           'attr': 'all new ones'}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d375b23-3f30-1b17-6559-d36d66820de6"},"outputs":[],"source":"def bar_and_scatter(models_dict):\n    df = pd.DataFrame([flatten(d) for d in models_dict['data']]).drop(['model', 'fit_time'], axis=1, inplace=False)\n    names = df.name\n    skf_accuracy_mean, skf_accuracy_std = df.skf_validation_accuracy_mean, df.skf_validation_accuracy_std\n    skf_logloss_mean1, skf_logloss_std1 = df.skf_validation_logloss_test_mean, df.skf_validation_logloss_test_std\n    bar = go.Bar(\n        x=names,\n        y=skf_accuracy_mean,\n        name=models_dict['attr'],\n        error_y=dict(\n            type='data',\n            array=skf_accuracy_std\n        ),\n        opacity=0.7\n    )\n    scatter = go.Scatter(\n        x=names,\n        y=skf_logloss_mean1,\n        name=models_dict['attr'],\n        error_y=dict(\n            type='data',\n            array=skf_logloss_std1\n        ),\n        opacity=0.7\n    )\n    return (bar, scatter)\n\n# Get all the traces\n(bar1, scatter1) = bar_and_scatter(models1)\n(bar2, scatter2) = bar_and_scatter(models2)\n(bar3, scatter3) = bar_and_scatter(models3)\n(bar4, scatter4) = bar_and_scatter(models4)\n(bar5, scatter5) = bar_and_scatter(models5)\n(bar6, scatter6) = bar_and_scatter(models6)\n(bar7, scatter7) = bar_and_scatter(models7)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ebf74a71-18d3-1952-7574-39bfb9010726"},"outputs":[],"source":"# Bar chart for the accuracy\nlayout = go.Layout(\n    title='Accuracy'\n)\ndata = [bar1, bar2, bar3, bar4, bar5, bar6, bar7]\nfig = go.Figure(data=data, layout=layout)\nplotly.offline.iplot(fig, filename='error-bar-bar')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07ad0d89-7577-46aa-76b0-3f49662fc6dc"},"outputs":[],"source":"layout = go.Layout(\n    title='Log loss'\n)\ndata = [scatter1, scatter2, scatter3, scatter4, scatter5, scatter6, scatter7]\nfig = go.Figure(data=data, layout=layout)\nplotly.offline.iplot(fig, filename='basic-error-bar')"},{"cell_type":"markdown","metadata":{"_cell_guid":"225d96fe-6b3b-88d6-39e0-a0cae93bf210"},"source":"***\n<p>\n<h1>Final Classification</h1><br>\nWe are now able to make the final classification.\n</p>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58d66315-2d7f-539b-416d-e3adf2ccbe5a"},"outputs":[],"source":"# Choose the model \nmodel = lrcv['model'] # or baseModel, choose the model you want\n\nfeatures = new_ones_2\n\nevaluate_models(copy.deepcopy([lrcv]), features)\n\n# Fitting\nmodel.fit(train_data[features], train_data['type'])\n# Predicting\npredicted_test = model.predict(test_data[features])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77142f82-7a7d-b44b-9f93-2d5a750d9f90"},"outputs":[],"source":"# SUBMISSION \nsubmission = pd.concat([test['id'], pd.DataFrame(predicted_test)], axis=1)\nsubmission.columns = ['id', 'type']\n\nsubmission.to_csv(\"new_sub.csv\", header=True, index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}