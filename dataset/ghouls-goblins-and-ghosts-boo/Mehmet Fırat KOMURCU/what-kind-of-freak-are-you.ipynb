{"cells":[{"metadata":{"_uuid":"ceeae512506997b1dee990fc98ff30ee8bd96c69"},"cell_type":"markdown","source":" <b> What Kind of Freak Are You!?</b>\n![](http://www.sideshowtoy.com/wp-content/uploads/2013/06/400101-product-feature.jpg)"},{"metadata":{"_uuid":"efd94b32187a5f3bb4705b1bc2e59db81517722c"},"cell_type":"markdown","source":"Hello everyone, you saw a monster and curious about this monster's kind? Then welcome to the \"Freak Type Predictor\". \nIn this kernel we'll analyze some dead freak's bodies."},{"metadata":{"_uuid":"587487dc0ce9ea31468709e9ac4dd9898749d3f7"},"cell_type":"markdown","source":"First thing first, let's import libraries."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d285a3c30bb7b3191947af7a057da8d6092aef83"},"cell_type":"code","source":"#importing libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nsns.set_style(\"darkgrid\")\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"05f95368082e01abe47dbb8664a7e99a7555ebb6"},"cell_type":"markdown","source":"Now we'll gel our dataset and start to analyze cadavers."},{"metadata":{"trusted":true,"_uuid":"318b1f88993d51306a6f4e1af922eddb94029416","collapsed":true},"cell_type":"code","source":"#reading dataset\ndataset = pd.read_csv('../input/train.csv', index_col='id')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4c29acfe2e79971483fba8ee6d8471a5d2b90746"},"cell_type":"code","source":"#Data Analysis","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"1da23752a0953d8916edaffa6f13272ec5210ee0"},"cell_type":"markdown","source":"let's see what we've got here"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"bb5ee159f3cad2543886f48b8c34db4302f155a7","collapsed":true},"cell_type":"code","source":"dataset.describe(include = 'all')","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"517f89a79a7649dd96e98e440de0e3980509ce90"},"cell_type":"markdown","source":"These createures have 6 color skins, very diverse.So, are there any correlations between these features? ."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"fe0968b0894c7b672ebd6c9e70689a1d89655a2c","collapsed":true},"cell_type":"code","source":"sns.heatmap(dataset.corr(), annot = True, fmt = \".2f\")","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"586fb1c8c20c2aa579117b502512ebe6010e0f7f"},"cell_type":"markdown","source":"hairlength-has_soul couple has the highest correlation with 0.47 and has_soul-bone_length couple are following them with 0.38 .\nAlright, now we're going to seperate these \"things\" by their colors."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"65fef9b2f7c431581ad8534b8643c96c3b7975a8","collapsed":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize= (15, 8))\nsns.countplot(x='color', hue='type', data=dataset, ax= ax)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"42315719b8a56230542b657cb50ae317819ab8f0"},"cell_type":"markdown","source":"Looks like they are spreading uniformly, but Ghouls have more white colors then others, that can be seen from first look.We can see this graphic with different type too :"},{"metadata":{"trusted":true,"_uuid":"265022c8f50e729710deb28fdddce815fbf3cbaf","collapsed":true},"cell_type":"code","source":"#g = sns.FacetGrid(data = dataset, col=\"type\")\n#g = g.map(plt.hist, \"color\", color=\"m\")\ng = sns.factorplot(x = \"color\", col=\"type\", kind=\"count\", data=dataset)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d7c232eae9567e0f85bf80e53013ab26846f413","collapsed":true},"cell_type":"code","source":"sns.pairplot(dataset, hue=\"type\")","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"b3d2af215395a3da1ce3d0e7438ae4db2af0c980"},"cell_type":"markdown","source":"when we look at has_soul-hair_length plot, we can see that they look well-seperated, let's look closer."},{"metadata":{"trusted":true,"_uuid":"5e56867159f73fb799118b91572f573fde65c58a","collapsed":true},"cell_type":"code","source":"sns.lmplot(x = \"hair_length\", y = \"has_soul\", hue=\"type\", data=dataset,\n           fit_reg=False)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"8a73fb908a9b7e2fd805d01010b7baa977ce4407"},"cell_type":"markdown","source":"looks like Ghouls have more hair_length and soul_percent(I guess) than other, that's a good hint for you!\n\nWe can see these creatures' soul percents here too : "},{"metadata":{"trusted":true,"_uuid":"500218e2ccde87808d2511d54c35bdfc9e718b2b","collapsed":true},"cell_type":"code","source":"df_goblin = dataset[dataset['type'] == 'Goblin']\ndf_ghoul = dataset[dataset['type'] == 'Ghoul']\ndf_ghost = dataset[dataset['type'] == 'Ghost']\n\n#PDF of types\nax = sns.kdeplot(df_goblin['has_soul'], shade=True, label = \"Goblin\")\nax = sns.kdeplot(df_ghoul['has_soul'], shade=True, label = \"Ghoul\")\nax = sns.kdeplot(df_ghost['has_soul'], shade=True, label = \"Ghost\")","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"c68e8c382820320ed9682bd102d8644f08e4a2aa"},"cell_type":"markdown","source":"Now, data Preprocessing time!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9c87608d221b51a2fa66f619afa2998d68ba82aa"},"cell_type":"code","source":"#Data Preprocessing","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3add5272c73f58217a577880c41f7ebbc36d7405"},"cell_type":"code","source":"X = dataset.iloc[:, :5].values\ny = dataset.iloc[:, 5:].values","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d7512408d049884ab3d761df138a42e8e401baa","collapsed":true},"cell_type":"code","source":"#Encoding Categorical Data(Independent)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nLabelEncoder_X = LabelEncoder()\nX[:, 4] = LabelEncoder_X.fit_transform(X[:, 4])\nOneHotEncoder_X = OneHotEncoder(categorical_features = [4])\nX = OneHotEncoder_X.fit_transform(X).toarray()\n\n#Encoding Categorical Data(Dependent)\nLabelEncoder_y = LabelEncoder()\ny = LabelEncoder_y.fit_transform(y)\n\n#Drop one columns of X for dummy variable trap\nX = X[:, 1:]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"731fecf26a988152581b2cb4fec9c061cccde693"},"cell_type":"code","source":"#Splitting data to test and traning sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,\n                                                    random_state = 0)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"c0af7aaaf741f62aeec143315e54d975004ef3ce"},"cell_type":"markdown","source":"and now it's time for data modelling. We'll try different models: Logistic Regression, Kernel SVM, Random Forest Classifier and XGBoost. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6765e40ed1928eb11769066741bf6ca755b82755"},"cell_type":"code","source":"#Data Modelling ","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"168e7356ab6cea11e4491399864b6d63d086d13f","collapsed":true},"cell_type":"code","source":"#Applying Logistic Regression to training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier_LR = LogisticRegression()\nclassifier_LR.fit(X_train, y_train)\n\n#Predicting the test set results\ny_pred = classifier_LR.predict(X_test)\n\n#Getting accuracy score\nfrom sklearn.metrics import accuracy_score\nLR_Score = accuracy_score(y_test, y_pred)\nprint(\"Logistic Reg. Score: \" + str(LR_Score))","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35ee661a45b02b94e4f84ccc3b14788510f39e8a","collapsed":true},"cell_type":"code","source":"#applying kernel SVM to training set\nfrom sklearn.svm import SVC\nclassifier_SVC = SVC(kernel = 'rbf', random_state = 0)\nclassifier_SVC.fit(X_train, y_train)\n\n#Predicting the test set results\ny_pred = classifier_SVC.predict(X_test)\n\n#Getting accuracy score\nSVM_Score = accuracy_score(y_test, y_pred)\nprint(\"Kernel SVM Score: \" + str(SVM_Score))","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f57820ac0ad6a9c1efe99dc731bb45560307f739","collapsed":true},"cell_type":"code","source":"#applying random forest to training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_RF = RandomForestClassifier(n_estimators = 100, criterion = \"entropy\",\n                                       random_state = 0)\nclassifier_RF.fit(X_train, y_train)\n\n#Predicting the test set results\ny_pred = classifier_RF.predict(X_test)\n\n#Getting accuracy score\nRF_Score = accuracy_score(y_test, y_pred)\nprint(\"Random Forest Score : \" + str(RF_Score))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a8ca2522e78880825065a7725a89f2e4a78a349","collapsed":true},"cell_type":"code","source":"#applying xgboost to training set\nfrom xgboost import XGBClassifier\nclassifier_XGB = XGBClassifier()\nclassifier_XGB.fit(X_train, y_train)\n\n#Predicting the test set results\ny_pred = classifier_XGB.predict(X_test)\n\n#Getting accuracy score\nXG_Score = accuracy_score(y_test, y_pred)\nprint(\"XGBoost Score : \" + str(XG_Score))","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"9a02270857dd679850430868824a291c672c01d4"},"cell_type":"markdown","source":"Now we should compare this model's scores:"},{"metadata":{"trusted":true,"_uuid":"999f0257a1794702fc420d247eec7e507d693ab8","collapsed":true},"cell_type":"code","source":"Scores_dict = {'Logistic Reg.': LR_Score, 'Kernel SVM': SVM_Score, \n               'Random Forest': RF_Score, 'XGBoost': XG_Score}\ndf_scores = pd.DataFrame.from_dict(data = Scores_dict, orient = \"index\")\ndf_scores.plot.bar(legend = False)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"5f76c82815099a08fc4e76c2cdb995feb24cd45f"},"cell_type":"markdown","source":"Random Forest is the winner!"},{"metadata":{"trusted":true,"_uuid":"900c247a2006c4049f74bf37d6f05763d3b2769d","collapsed":true},"cell_type":"code","source":"importances = classifier_RF.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in classifier_RF.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49d006f1eafa018ff2f11cb1425b5818622fdd47","collapsed":true},"cell_type":"code","source":"# Plot the feature importances of the forest\nplt.figure(figsize = (15, 5))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"ea96473bf037b5e89f591aa692dfcba98e01e2ce"},"cell_type":"markdown","source":"if we look at feature's importances, we realize that colors don't have that much impact on accuracy, maybe we should try to drop these and try to predict again."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8cf86c631c151b755deacb59b9a8e6faab18591f"},"cell_type":"code","source":"#dropping color columns\nX_train = X_train[:, 5:]\nX_test = X_test[:, 5:]","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"106112835df924ad1978eaae732bd92909da079c","collapsed":true},"cell_type":"code","source":"#fit again random forest\nclassifier_RF = RandomForestClassifier(n_estimators = 1000, criterion = \"entropy\",\n                                       max_depth=7,random_state = 0)\nclassifier_RF.fit(X_train, y_train)\n#Predicting the test set results\ny_pred = classifier_RF.predict(X_test)\n#Getting accuracy score\nRF_Score = accuracy_score(y_test, y_pred)\nprint(\"accuracy score without color : \" + str(RF_Score))","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"28324bbd359cd925f283f36b8247c051aca0df01"},"cell_type":"markdown","source":"That didn't change anything actually, but now we have less feature.\nNow, time to model selection, we have to make sure about our model's score is reliable."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"287485e2564f9884f984f4e50d7429cc291ae987"},"cell_type":"code","source":"#model selection","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2e7926ec470747aadf1b6e8701404238210a6ad","collapsed":true},"cell_type":"code","source":"# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier_RF, X = X_train, y = y_train, cv = 10)\nprint(\"accurasies mean: \" + str(accuracies.mean()))\nprint(\"accurasies std : \" + str(accuracies.std()))","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80e513774383595929369fd4c18af300b66859b8","collapsed":true},"cell_type":"code","source":"# Applying Grid Search to find the best model and the best parameters\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\"n_estimators\" : [1, 10, 100, 256, 512, 1024],\n              \"min_samples_leaf\": [1,2,3,4,5],\n              \"min_samples_split\" : [2,3,4,5],\n              \"max_depth\" : [1, 2, 4, 8, 16]}\ngrid_search = GridSearchCV(estimator = classifier_RF, param_grid = parameters,\n                           scoring = \"accuracy\", n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint(\"best score : \" + str(best_accuracy))\nprint(\"best parameters : \" + str(best_parameters))","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"beb160f3926d03598336373fa81b40b3a5fa8b3b"},"cell_type":"markdown","source":"that's for it for today everyone, thanks for reading. If you have any advice, feel free to comment."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}