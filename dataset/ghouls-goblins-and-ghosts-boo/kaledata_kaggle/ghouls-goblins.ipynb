{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # Matlab-style plotting\n# plotly\n\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['id']\ntest_ID = test['id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"id\", axis = 1, inplace = True)\ntest.drop(\"id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True, cbar=True, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"missing = train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummy = pd.get_dummies(pd.read_csv('../input/train.csv'))\ntrain_dummy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = train_dummy.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs.tail(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ghost_num = {\"type\":     {\"Ghoul\": 1, \"Goblin\": 2, \"Ghost\": 3} }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.replace(ghost_num, inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny = train.type.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['type'], axis=1, inplace=True)\n\n\nprint(\"all_data size is : {}\".format(all_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.drop(['color'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[\"bone_soul\"] = all_data[\"bone_length\"]*all_data[\"has_soul\"]\nall_data[\"hair_soul\"] = all_data[\"hair_length\"]*all_data[\"has_soul\"]\nall_data[\"flesh_soul\"] = all_data[\"rotting_flesh\"]*all_data[\"has_soul\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[\"bone_hair\"] = all_data[\"bone_length\"]*all_data[\"hair_length\"]\nall_data[\"flesh_hair\"] = all_data[\"rotting_flesh\"]*all_data[\"hair_length\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data_simple = pd.DataFrame()\nall_data_simple[\"bone_hair\"] = all_data[\"bone_hair\"]\nall_data_simple[\"rotting_flesh\"] = all_data[\"rotting_flesh\"]\nall_data_simple[\"bone_soul\"] = all_data[\"bone_soul\"]\nall_data_simple[\"hair_soul\"] = all_data[\"hair_soul\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data_simple.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['bone_length']>0.8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_data = pd.get_dummies(all_data)\nX = all_data_simple[:ntrain]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\n\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    NuSVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]\n\n# Logging for Visual Comparison\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns=log_cols)\n\nfor clf in classifiers:\n    clf.fit(train_X, train_y)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    \n    score = clf.score(val_X, val_y)\n    print(\"Score: {:.4%}\".format(score))\n    \n    \n    \nprint(\"=\"*30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\naccuracy_scorer = metrics.make_scorer(metrics.accuracy_score)\n\n\nparams = {'n_estimators':[10, 20, 50, 100], 'criterion':['gini', 'entropy'], 'max_depth':[None, 5, 10, 25, 50]}\nrf = RandomForestClassifier(random_state = 0)\nclf = GridSearchCV(rf, param_grid = params, scoring = accuracy_scorer, cv = 5, n_jobs = -1)\nclf.fit(train_X, train_y)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best = RandomForestClassifier(criterion= 'entropy', max_depth= 5, n_estimators= 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_estimators':[10, 25, 50, 100], 'max_samples':[1, 3, 5, 10]}\nbag = BaggingClassifier(random_state = 0)\nclf = GridSearchCV(bag, param_grid = params, scoring = accuracy_scorer, cv = 5, n_jobs = -1)\nclf.fit(train_X, train_y)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_best = BaggingClassifier(max_samples = 5, n_estimators = 100, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap \nexplainer = shap.TreeExplainer(classifiers[1], train_X)\nshap_values = explainer.shap_values(train_X)\n\nshap.summary_plot(shap_values, train_X)\n# For example for feature 33 low values have a negative impact on model predictions (zero is more likely), \n#and high values have a positive impace (ones are more likely). Feature 217 has an opposite effect: \n#low values have a positive impact and high values have a negative impact.\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], train_X.iloc[:,1:10])\n##\ntop_cols = train_X.columns[np.argsort(shap_values.std(0))[::-1]][:10]\nfor col in top_cols:\n    shap.dependence_plot(col, shap_values, train_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble=VotingClassifier(estimators=[('4', classifiers[4]), ('3', classifiers[3]), ('5', classifiers[5])],\n                       voting='soft', weights=[1,1,1]).fit(train_X,train_y)\nprint('The accuracy for DecisionTree and Random Forest is:',ensemble.score(val_X,val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_clf = VotingClassifier(estimators=[('rf', rf_best), ('bag', bag_best)]\n                              , voting='hard')\nvoting_clf.fit(train_X, train_y)\nprint('The accuracy for DecisionTree and Random Forest is:',voting_clf.score(val_X,val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ghost_cat = {\"type\":     {1: \"Ghoul\", 2: \"Goblin\", 3: \"Ghost\"} }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = all_data_simple[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['id'] = test_ID\nsub['type'] = voting_clf.predict(test)\nsub.replace(ghost_cat, inplace=True)\nsub.to_csv('subvoting_clf.csv',index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}