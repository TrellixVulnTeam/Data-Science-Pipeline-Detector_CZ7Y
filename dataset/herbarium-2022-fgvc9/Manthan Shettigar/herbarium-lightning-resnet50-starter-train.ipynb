{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"Table\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> Table of Contents\n</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":" | S.No       |                   Heading                |\n | :------------- | :-------------------:                |                     \n |  01 |  [**Libraries**](#library)                        |  \n |  02 |  [**JSON to Pandas CSV**](#json)                |\n |  03 |  [**Sample Images**](#sample-images)      |\n |  04 |  [**Config**](#config)                |\n |  05 |  [**Custom Dataset**](#dataset)  |\n |  06 |  [**Image Augmentation : Albumentations**](#Transform)   |\n |  07 |  [**Create Dataloader**](#dataloader)   |\n |  08 |  [**Model**](#model) |\n |  09 |  [**Trainerâš¡**](#Trainer) |\n |  10 |  [**Plot Metrics**](#metrics) |","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:#648c54;\">Identify plant species of the Americas from herbarium specimens</span>**\n\nThe FGVC9 2022 Herbarium Challenge is to identify melastome species from herbarium specimens provided by the New York Botanical Garden (NYBG).\nBackground\n\nThere are more than 400,000 known plant species with an estimated 80,000 still to be discovered. In flowering plants, it takes approximately 35 years from plant collection to species description while less than 16% of new species are described in less than 5 years. It has also been suggested that â€˜herbaria are a major frontier for speciesâ€™ with more than 50% of unknown species already in herbarium collections.\n\nThe dataset encompasses over 90% of all plant taxa in North America, spanning 1.05M images for 15501 plant taxa, collected from 60 different institutions around the world.\n\nReferences : \n* https://www.kaggle.com/c/herbarium-2022-fgvc9/discussion/307792\n* https://www.kaggle.com/c/herbarium-2019-fgvc6","metadata":{}},{"cell_type":"markdown","source":"<a id=\"library\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> Libraries\n</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport json\nimport cv2\nimport seaborn as sns\nfrom skimage import io\n\n!pip install albumentations\n!pip install pytorch-lightning\n!pip install timm\n!pip install torchmetrics\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.functional as F\nfrom torchvision import datasets, transforms,models\nfrom torch.utils.data import Dataset,DataLoader\n\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nimport torchmetrics\n\n\nimport albumentations as A\nfrom albumentations.core.composition import Compose\nfrom albumentations.pytorch import ToTensorV2\n\nimport pytorch_lightning as pl\n%matplotlib inline","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-12T07:34:01.400738Z","iopub.execute_input":"2022-03-12T07:34:01.401397Z","iopub.status.idle":"2022-03-12T07:34:39.331707Z","shell.execute_reply.started":"2022-03-12T07:34:01.401363Z","shell.execute_reply":"2022-03-12T07:34:39.330947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"json\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> JSON to Pandas CSV\n</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"**Reference/Credits** :  [JSON -> PANDASðŸ“Š\n@Sanskar Hasija](https://www.kaggle.com/odins0n/json-pandas-herbarium-2022/)","metadata":{}},{"cell_type":"code","source":"\n\nTRAIN_DIR = \"../input/herbarium-2022-fgvc9/train_images/\"\nTEST_DIR = \"../input/herbarium-2022-fgvc9/test_images/\"\n\nwith open(\"../input/herbarium-2022-fgvc9/train_metadata.json\") as json_file:\n    train_meta = json.load(json_file)\nwith open(\"../input/herbarium-2022-fgvc9/test_metadata.json\") as json_file:\n    test_meta = json.load(json_file)\n    \nimage_ids = [image[\"image_id\"] for image in train_meta[\"images\"]]\nimage_dirs = [TRAIN_DIR + image[\"file_name\"] for image in train_meta[\"images\"]]\ncategory_ids = [annot[\"category_id\"] for annot in train_meta[\"annotations\"]]\ngenus_ids = [annot[\"genus_id\"] for annot in train_meta[\"annotations\"] ]\ntest_ids = [image[\"image_id\"] for image in test_meta]\ntest_dirs = [TEST_DIR + image[\"file_name\"] for image in test_meta ]\n\ntrain = pd.DataFrame(data =np.array([image_ids , image_dirs, genus_ids, category_ids ]).T, \n                     columns = [\"image_id\", \"directory\",\"genus_id\", \"category\",])\ntest = pd.DataFrame(data =np.array([test_ids  , test_dirs ]).T, \n                    columns = [\"image_id\", \"directory\",])\n\ntrain.to_csv(\"train.csv\", index = False)\ntest.to_csv(\"test.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:34:39.333763Z","iopub.execute_input":"2022-03-12T07:34:39.334013Z","iopub.status.idle":"2022-03-12T07:34:59.608437Z","shell.execute_reply.started":"2022-03-12T07:34:39.333976Z","shell.execute_reply":"2022-03-12T07:34:59.607654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('train.csv')\ngenera = pd.DataFrame(train_meta['genera'])\ngenera  = df_train.merge(genera,on='genus_id')\ngenera.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:34:59.609911Z","iopub.execute_input":"2022-03-12T07:34:59.610164Z","iopub.status.idle":"2022-03-12T07:35:01.18725Z","shell.execute_reply.started":"2022-03-12T07:34:59.610131Z","shell.execute_reply":"2022-03-12T07:35:01.186547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.category.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:01.188498Z","iopub.execute_input":"2022-03-12T07:35:01.188929Z","iopub.status.idle":"2022-03-12T07:35:01.201348Z","shell.execute_reply.started":"2022-03-12T07:35:01.188891Z","shell.execute_reply":"2022-03-12T07:35:01.20068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total No. Of Unique Classes/Labels : **15501**","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:#648c54;\">Top 10 Genera Distribution</span>**","metadata":{}},{"cell_type":"code","source":"genera['genus'].value_counts().head(10).plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:01.203585Z","iopub.execute_input":"2022-03-12T07:35:01.204021Z","iopub.status.idle":"2022-03-12T07:35:01.52937Z","shell.execute_reply.started":"2022-03-12T07:35:01.203983Z","shell.execute_reply":"2022-03-12T07:35:01.528676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sample-images\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> Sample Images \n</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"from PIL import Image\ndef display_images(genus):\n    images = df_train.loc[genera['genus'] == genus]['directory'][:12]\n    i = 1\n    fig = plt.figure(figsize = (10, 10))\n    plt.suptitle(genus, fontsize = '35')\n    for image in images:\n        ax = fig.add_subplot(3, 4, i)\n        image = Image.open(image).convert('RGB')\n        ax.imshow(image)\n        i+=1\n    fig.tight_layout()\n    plt.show()\n\ndisplay_images('Carex')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:01.624244Z","iopub.execute_input":"2022-03-12T07:35:01.624626Z","iopub.status.idle":"2022-03-12T07:35:03.791981Z","shell.execute_reply.started":"2022-03-12T07:35:01.624567Z","shell.execute_reply":"2022-03-12T07:35:03.790999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_images('Astragalus')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:03.792879Z","iopub.execute_input":"2022-03-12T07:35:03.793085Z","iopub.status.idle":"2022-03-12T07:35:06.47579Z","shell.execute_reply.started":"2022-03-12T07:35:03.793057Z","shell.execute_reply":"2022-03-12T07:35:06.47508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:#648c54;\">Encode Class Labels</span>**","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nle.fit(df_train['category'])\ndf_train['category'] = le.transform(df_train['category'])\ndf_train.category.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:06.477012Z","iopub.execute_input":"2022-03-12T07:35:06.477363Z","iopub.status.idle":"2022-03-12T07:35:06.546353Z","shell.execute_reply.started":"2022-03-12T07:35:06.477332Z","shell.execute_reply":"2022-03-12T07:35:06.545526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the Training Data is large approx 119 GB will take a lot time in training processs,We will work with a smaller sample of first 5000 images.","metadata":{}},{"cell_type":"code","source":"df_train = df_train[:5000]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:06.547809Z","iopub.execute_input":"2022-03-12T07:35:06.548067Z","iopub.status.idle":"2022-03-12T07:35:06.551739Z","shell.execute_reply.started":"2022-03-12T07:35:06.548034Z","shell.execute_reply":"2022-03-12T07:35:06.551077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"config\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> Config \n</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"\nclass CFG:\n    seed = 42\n    model_name = 'resnet50'\n    pretrained = True\n    img_size = 224\n    num_classes = df_train.category.nunique() # 86 - since the train set is reduced to 5000 images \n    lr = 5e-4\n    min_lr = 1e-6\n    t_max = 20\n    num_epochs = 20\n    batch_size = 32\n    accum = 1\n    precision = 16\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:36:49.961295Z","iopub.execute_input":"2022-03-12T07:36:49.96195Z","iopub.status.idle":"2022-03-12T07:36:50.010953Z","shell.execute_reply.started":"2022-03-12T07:36:49.961899Z","shell.execute_reply":"2022-03-12T07:36:50.010233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:36:50.103947Z","iopub.execute_input":"2022-03-12T07:36:50.105583Z","iopub.status.idle":"2022-03-12T07:36:50.116266Z","shell.execute_reply.started":"2022-03-12T07:36:50.105553Z","shell.execute_reply":"2022-03-12T07:36:50.11566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:36:50.252596Z","iopub.execute_input":"2022-03-12T07:36:50.253061Z","iopub.status.idle":"2022-03-12T07:36:50.262328Z","shell.execute_reply.started":"2022-03-12T07:36:50.25303Z","shell.execute_reply":"2022-03-12T07:36:50.261686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"dataset\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> Custom Dataset \n</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"class HerbariumDataset(Dataset):\n    def __init__(self,data,transform=None):\n        self.annotations = data\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self,index):\n        \n        img_path = self.annotations.iloc[index,1]\n        image = io.imread(img_path)\n        y_label = torch.tensor(int(self.annotations.iloc[index,3]))\n        \n        if self.transform:\n            image = self.transform(image=np.array(image))\n        \n        return(image,y_label) ","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:36:50.396847Z","iopub.execute_input":"2022-03-12T07:36:50.397404Z","iopub.status.idle":"2022-03-12T07:36:50.409112Z","shell.execute_reply.started":"2022-03-12T07:36:50.39736Z","shell.execute_reply":"2022-03-12T07:36:50.408426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Transform\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> Image Augmentation : Albumentations \n</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"**Image augmentation** is a process of creating new training examples from the existing ones. To make a new sample, you slightly change the original image. For instance, you could make a new image a little brighter; you could cut a piece from the original image; you could make a new image by mirroring the original one, etc.\n![](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Falbumentations.ai%2Fdocs%2Fimages%2Fintroduction%2Fimage_augmentation%2Faugmentation.jpg&f=1&nofb=1)\n\n**Albumentations** is a Python library for fast and flexible image augmentations. Albumentations efficiently implements a rich variety of image transform operations that are optimized for performance, and does so while providing a concise, yet powerful image augmentation interface for different computer vision tasks, including object classification, segmentation, and detection.\n\nTo learn more [**Albumentations**](https://albumentations.ai/docs/)","metadata":{}},{"cell_type":"code","source":"def Transform(phase: str):\n    if phase == 'train':\n        return Compose([\n            A.RandomResizedCrop(height=CFG.img_size, width=CFG.img_size),\n            A.HorizontalFlip(p=0.5),\n            A.ShiftScaleRotate(p=0.5),\n            A.RandomBrightnessContrast(p=0.5),\n            A.Normalize(),\n            ToTensorV2(),\n        ])\n    else:\n        return A.Compose([\n            A.Resize(height=CFG.img_size, width=CFG.img_size),\n            A.Normalize(),\n            ToTensorV2(),\n        ])","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:36:50.526233Z","iopub.execute_input":"2022-03-12T07:36:50.526882Z","iopub.status.idle":"2022-03-12T07:36:50.534172Z","shell.execute_reply.started":"2022-03-12T07:36:50.526837Z","shell.execute_reply":"2022-03-12T07:36:50.533475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"dataloader\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> Create Dataloader \n</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"train_dataset = HerbariumDataset(df_train,transform=Transform('train'))\nvalid_dataset = HerbariumDataset(df_train,transform=Transform('valid'))\n\ntrain_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:36:50.69526Z","iopub.execute_input":"2022-03-12T07:36:50.695476Z","iopub.status.idle":"2022-03-12T07:36:50.700999Z","shell.execute_reply.started":"2022-03-12T07:36:50.695449Z","shell.execute_reply":"2022-03-12T07:36:50.700091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:#648c54;\">Transformed Image</span>**","metadata":{}},{"cell_type":"code","source":"dataiter = iter(train_loader)\nimages, labels = dataiter.next()\nprint(images['image'][1].shape)\nprint(labels[1].shape)\nplt.imshow(images['image'][3].squeeze().permute(1,2,0), cmap='Greys_r')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:36:50.806236Z","iopub.execute_input":"2022-03-12T07:36:50.806544Z","iopub.status.idle":"2022-03-12T07:36:55.536423Z","shell.execute_reply.started":"2022-03-12T07:36:50.806516Z","shell.execute_reply":"2022-03-12T07:36:55.535654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"model\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> Model\n</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"class CustomResNet(nn.Module):\n    def __init__(self, model_name='resnet18', pretrained=False):\n        super().__init__()\n        # timm contains collection of pretrained image models like torchvision.models\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.model.get_classifier().in_features\n        self.model.fc = nn.Linear(in_features, CFG.num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:36:55.541329Z","iopub.execute_input":"2022-03-12T07:36:55.543187Z","iopub.status.idle":"2022-03-12T07:36:55.551966Z","shell.execute_reply.started":"2022-03-12T07:36:55.543138Z","shell.execute_reply":"2022-03-12T07:36:55.551237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitHerbarium(pl.LightningModule):\n    def __init__(self, model):\n        super(LitHerbarium, self).__init__()\n        self.model = model # ResNet50 \n        self.metric = torchmetrics.F1(num_classes=CFG.num_classes) # F1 Score\n        self.criterion = nn.CrossEntropyLoss() # Cross Entropy loss\n        self.lr = CFG.lr # Learning Rate\n\n    def forward(self, x, *args, **kwargs):\n        return self.model(x)\n\n    def configure_optimizers(self):\n        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization\"\"\"\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=CFG.t_max, eta_min=CFG.min_lr)\n\n        return {'optimizer': self.optimizer, 'lr_scheduler': self.scheduler}\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger\"\"\"\n        image,target = batch\n        image = image['image']\n        output = self.model(image)\n        loss = self.criterion(output, target)\n        score = self.metric(output.argmax(1), target)\n        logs = {'train_loss': loss, 'train_f1': score, 'lr': self.optimizer.param_groups[0]['lr']}\n        self.log_dict(\n            logs,\n            on_step=False, on_epoch=True, prog_bar=True, logger=True\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Operates on a single batch of data from the validation set .calculate anything of interest like accuracy.\"\"\"\n        image,target = batch\n        image = image['image']\n        output = self.model(image)\n        loss = self.criterion(output, target)\n        score = self.metric(output.argmax(1), target)\n        logs = {'valid_loss': loss, 'valid_f1': score}\n        self.log_dict(\n            logs,\n            on_step=False, on_epoch=True, prog_bar=True, logger=True\n        )\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:36:55.555338Z","iopub.execute_input":"2022-03-12T07:36:55.555573Z","iopub.status.idle":"2022-03-12T07:36:55.573728Z","shell.execute_reply.started":"2022-03-12T07:36:55.555551Z","shell.execute_reply":"2022-03-12T07:36:55.572873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **To Learn more about** [LightningModule](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html?highlight=training_step#)","metadata":{}},{"cell_type":"code","source":"model = CustomResNet(model_name=CFG.model_name, pretrained=CFG.pretrained)\nlit_model = LitHerbarium(model)\nlit_model","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:36:55.579298Z","iopub.execute_input":"2022-03-12T07:36:55.580174Z","iopub.status.idle":"2022-03-12T07:37:00.568138Z","shell.execute_reply.started":"2022-03-12T07:36:55.580139Z","shell.execute_reply":"2022-03-12T07:37:00.567456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pytorch computational Graph of ResNet50 \n\n# # !pip install torchviz\n# from torchviz import make_dot\n# batch = next(iter(train_loader))\n# images , labels = batch\n# label = model(images['image'])\n# make_dot(label, params=dict(list(lit_model.named_parameters())))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:37:00.569632Z","iopub.execute_input":"2022-03-12T07:37:00.57009Z","iopub.status.idle":"2022-03-12T07:37:00.573642Z","shell.execute_reply.started":"2022-03-12T07:37:00.570054Z","shell.execute_reply":"2022-03-12T07:37:00.572971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Trainer\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> Trainerâš¡\n</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"* **Max_epochs** : Stop training once this number of epochs is reached\n* **gpus** : Number of GPUs to train on (int)\n* **accumulate_grad_batches** : Accumulates grads every k batches or as set up in the dict. Trainer also calls optimizer.step() for the last indivisible step number.\n* **precision** : 16 bit floating points to reduce memory footprint during model training. This can result in improved performance.\n* **callbacks** : A callback is a self-contained program that can be reused across projects.\n* **checkpoint_callback / enable_checkpointing** : If True, enable checkpointing Else None.\n* **logger** :  PyTorch TensorBoard logging under the hood, and stores the logs to a directory \n\nTo learn more about [**Trainer**](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer)","metadata":{}},{"cell_type":"code","source":"logger = CSVLogger(save_dir='logs/', name=CFG.model_name)\nlogger.log_hyperparams(CFG.__dict__)\ncheckpoint_callback = ModelCheckpoint(monitor='valid_loss',\n                                      save_top_k=1,\n                                      save_last=True,\n                                      save_weights_only=True,\n                                      filename='checkpoint/{epoch:02d}-{valid_loss:.4f}-{valid_f1:.4f}',\n                                      verbose=False,\n                                      mode='min')\n\ntrainer = Trainer(\n    max_epochs=CFG.num_epochs,\n    gpus=1,\n    accumulate_grad_batches=CFG.accum,\n    precision=CFG.precision,\n    callbacks=[EarlyStopping(monitor='valid_loss', patience=3, mode='min')],\n    checkpoint_callback=checkpoint_callback,\n    enable_checkpointing=True,\n    logger=logger,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:37:00.575299Z","iopub.execute_input":"2022-03-12T07:37:00.575558Z","iopub.status.idle":"2022-03-12T07:37:00.599736Z","shell.execute_reply.started":"2022-03-12T07:37:00.575524Z","shell.execute_reply":"2022-03-12T07:37:00.599021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(lit_model, train_dataloader=train_loader, val_dataloaders=valid_loader)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:37:00.600916Z","iopub.execute_input":"2022-03-12T07:37:00.601291Z","iopub.status.idle":"2022-03-12T08:18:33.092814Z","shell.execute_reply.started":"2022-03-12T07:37:00.601257Z","shell.execute_reply":"2022-03-12T08:18:33.092062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"metrics\"></a>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#648c54;\n           font-size:110%;\n           font-family:Verdana;\n           text-align: center;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\"> Plot Metrics\n</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')\n\ntrain_acc = metrics['train_f1'].dropna().reset_index(drop=True)\nvalid_acc = metrics['valid_f1'].dropna().reset_index(drop=True)\n    \nfig = plt.figure(figsize=(7, 6))\n\nplt.plot(train_acc, color=\"r\", label='train/f1')\nplt.plot(valid_acc, color=\"b\", label='valid/f1')\nplt.ylabel('F1', fontsize=24)\nplt.xlabel('Epoch', fontsize=24)\nplt.legend(loc='lower right', fontsize=18)\nplt.savefig(f'{trainer.logger.log_dir}/f1.png')\n\ntrain_loss = metrics['train_loss'].dropna().reset_index(drop=True)\nvalid_loss = metrics['valid_loss'].dropna().reset_index(drop=True)\n\nfig = plt.figure(figsize=(7, 6))\nplt.plot(train_loss, color=\"r\", label='train/loss')\nplt.plot(valid_loss, color=\"b\", label='valid/loss')\nplt.ylabel('Loss', fontsize=24)\nplt.xlabel('Epoch', fontsize=24)\nplt.legend(loc='upper right', fontsize=18)\nplt.savefig(f'{trainer.logger.log_dir}/loss.png')\\\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-12T08:18:33.094636Z","iopub.execute_input":"2022-03-12T08:18:33.094922Z","iopub.status.idle":"2022-03-12T08:18:33.633929Z","shell.execute_reply.started":"2022-03-12T08:18:33.094882Z","shell.execute_reply":"2022-03-12T08:18:33.633281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}