{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## SVM with minimal data preprocessing ##"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport math\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load training and testing data\ntrainx = pd.read_csv('../input/X_train.csv')\ntrainy = pd.read_csv('../input/y_train.csv')\ntestx = pd.read_csv('../input/X_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What's in the data? ###\n\n3,810 series:\nIt looks like we have 3,810 different series id's, where a series is a set of measurements from sensors that describes a floor surface, such as \"concrete\", \"carpet\", \"wood\", etc.\n\n128 measurements from 10 sensors per series:\nEach series has 128 measurements from 10 different sensors, such as \"orientation_X\", \"orientation_Y\", \"angular_velocity_X\", etc.\n\n9 surfaces with variable number of examples per surface:\nThere are 9 different surfaces. hard_tiles has only 21 examples, and concrete has 779 examples! So the dataset is biased - we'll have to account for this or the predictions will be biased too."},{"metadata":{},"cell_type":"markdown","source":"### Simple model ###\n\nOne simple approach is to take the means of measurements and use them as features.\nSo, instead of 128 rows, each example (i.e. series) will have just one row of data - \narithmetic averages of the sensor measurements.\n\nLet's create this new dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataset with averages\n\n# list of unique series_id's\nseries_ids = trainx.series_id.unique()\n\n# create an empty dataframe\ncols = trainx.columns.tolist()\ncols.remove('row_id')\ncols.remove('measurement_number')\nshape = (len(series_ids),11)\ndata = np.empty(shape=shape)\ntrainx_base = pd.DataFrame(data=data,columns=cols)\n\n# list of sensors\ncols.remove('series_id')\nsensors = cols\n\nfor id in series_ids:\n    means = []\n    for sensor in sensors:\n        means.append(trainx[trainx.series_id==id][sensor].mean())\n    means.insert(0, id)\n    trainx_base.iloc[id] = means\n\n# default data type of float64 is fine for all columns except for series_id - it needs to be int64\ntrainx_base = trainx_base.astype({'series_id':np.int64})\n\ntrainx_base.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And another one for the test set\n\n# list of unique series_id's\nseries_ids = testx.series_id.unique()\n\n# create am empty dataframe\ncols = testx.columns.tolist()\ncols.remove('row_id')\ncols.remove('measurement_number')\nshape = (len(series_ids),11)\ndata = np.empty(shape=shape)\ntestx_base = pd.DataFrame(data=data,columns=cols)\n\n# list of sensors\ncols.remove('series_id')\nsensors = cols\n\nfor id in series_ids:\n    means = []\n    for sensor in sensors:\n        means.append(testx[testx.series_id==id][sensor].mean())\n    means.insert(0, id)\n    testx_base.iloc[id] = means\n\n# default data type of float64 is fine for all columns except for series_id - it needs to be int64\ntestx_base = testx_base.astype({'series_id':np.int64})\n\ntestx_base.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Min-max normalize all data, i.e. convert to number from 0 to 1.\n\nscaler = MinMaxScaler(feature_range=(0,1))\nsensors = trainx_base.columns.tolist()\nsensors.remove('series_id')\n\ntrainx_base_normalized = pd.DataFrame(data=trainx_base)\ntrainx_base_normalized[sensors] = scaler.fit_transform(trainx_base[sensors])\ntrainx_base_normalized.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And again for the test set\n\nscaler = MinMaxScaler(feature_range=(0,1))\nsensors = testx_base.columns.tolist()\nsensors.remove('series_id')\n\ntestx_base_normalized = pd.DataFrame(data=testx_base)\ntestx_base_normalized[sensors] = scaler.fit_transform(testx_base[sensors])\ntestx_base_normalized.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split training data to training and validation sets ###\n\nIn order to evaluate how a classifier is doing without submitting things to Kaggle, I will reserve a part (~10%) of the training set for validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, lets flag about 10% of the data for testing, working through each surface individually\n\n# percent of sample to reserve for testing/validation:\ntest_part = 0.1\n\n# for reproducability\nrandom.seed(27)\n\ntrainx_base_normalized['test'] = 0\n\nsurfaces = trainy.surface.unique().tolist()\n\nfor surface in surfaces:\n    #print('working on surface \"{}\"'.format(surface))\n    surface_series_id = trainy[trainy.surface==surface].series_id.tolist()\n    #print('  found {} ids in total'.format(len(surface_series_id)))\n    test_part_cnt = math.floor(len(surface_series_id) * test_part)\n    #print('  picked {} ids for testing'.format(test_part_cnt))\n    test_series_id = random.sample(surface_series_id, test_part_cnt)\n    trainx_base_normalized.loc[test_series_id, 'test'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(trainx_base_normalized[trainx_base_normalized.test==0]), '<- for training')\nprint(len(trainx_base_normalized[trainx_base_normalized.test==1]), '<- for testing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets bring in the labels\n\nbasemodel = trainx_base_normalized.copy()\nbasemodel['surface'] = trainy.surface\nbasemodel.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, separate out the 10% of the data that we flagged earlier for testing/validation\n\nbasemodel_train_X = basemodel.copy()\nbasemodel_train_X.drop(basemodel_train_X[basemodel_train_X['test']==1].index.tolist(), inplace=True)\nbasemodel_train_X.reset_index(drop=True, inplace=True)\n\nbasemodel_valid_X = basemodel.copy()\nbasemodel_valid_X.drop(basemodel_valid_X[basemodel_valid_X['test']==0].index.tolist(), inplace=True)\nbasemodel_valid_X.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finally, create labels for each of the two datasets above\n\nbasemodel_train_y = basemodel_train_X.copy()\nbasemodel_train_X.drop(['series_id','test','surface'], axis=1, inplace=True)\nbasemodel_train_y.drop(['series_id','orientation_X','orientation_Y','orientation_Z','orientation_W',\n                        'angular_velocity_X','angular_velocity_Y','angular_velocity_Z','linear_acceleration_X',\n                        'linear_acceleration_Y','linear_acceleration_Z','test'], axis=1, inplace=True)\n\nbasemodel_valid_y = basemodel_valid_X.copy()\nbasemodel_valid_X.drop(['series_id','test','surface'], axis=1, inplace=True)\nbasemodel_valid_y.drop(['series_id','orientation_X','orientation_Y','orientation_Z','orientation_W',\n                        'angular_velocity_X','angular_velocity_Y','angular_velocity_Z','linear_acceleration_X',\n                        'linear_acceleration_Y','linear_acceleration_Z','test'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use SVM classifier and fit the training data\n\nfrom sklearn.svm import SVC\n\nclf = SVC(kernel='rbf', gamma='scale')\nclf.fit(basemodel_train_X, basemodel_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions and compute accuracy score (on the validation set)\nfrom sklearn.metrics import accuracy_score\n\ny_true = basemodel_valid_y.surface.tolist()\ny_pred = clf.predict(basemodel_valid_X)\nacc_score = accuracy_score(y_true, y_pred)\nprint(acc_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on the test set\n\nbasemodel_test = testx_base_normalized.copy()\n# we don't need the \"series_id\" columns because it is the same as the index\nbasemodel_test.drop(['series_id'], axis=1, inplace=True)\n\ntest_preds = clf.predict(basemodel_test)\n\n# into a dataframe\ntest_preds_df = pd.DataFrame(data={'series_id':range(0,3816), 'surface':test_preds})\n\ntest_preds_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write to file (for submission)\n# test_preds_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The file above gets a submission score of 0.26. Can we improve?\n\nOne way to improve is by using all the data for training, without reserving any for validation. So let's try that."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use all training data (no validation)\n\nbasemodel_full_X = basemodel.copy()\nbasemodel_full_X.drop(['series_id','test'], axis=1, inplace=True)\nbasemodel_full_y = basemodel_full_X.copy()\nbasemodel_full_X.drop(['surface'], axis=1, inplace=True)\nbasemodel_full_y.drop(['orientation_X','orientation_Y','orientation_Z','orientation_W','angular_velocity_X','angular_velocity_Y','angular_velocity_Z','linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z'], axis=1, inplace=True)\n\n# print(basemodel_full_X.shape)\n# print(basemodel_full_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model\nclf_full = SVC(kernel='rbf', gamma='scale')\nclf_full.fit(basemodel_full_X, basemodel_full_y)\n\n# And just because we can, let's predict on the validation set we still have from before\ny_true = basemodel_valid_y.surface.tolist()\ny_pred = clf_full.predict(basemodel_valid_X)\nacc_score = accuracy_score(y_true, y_pred)\nprint(acc_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on the test set\ntest_preds_full = clf.predict(basemodel_test)\n# into a dataframe\ntest_preds_full_df = pd.DataFrame(data={'series_id':range(0,3816), 'surface':test_preds_full})\ntest_preds_full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write to file (for submission)\ntest_preds_full_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}