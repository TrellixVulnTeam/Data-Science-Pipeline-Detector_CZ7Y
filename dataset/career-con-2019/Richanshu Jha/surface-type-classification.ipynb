{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# **AI Project 1**\n## **Surface Type Classification**\n\n---\n\n### Richanshu Jha - rj1469"},{"metadata":{},"cell_type":"markdown","source":"## Importing Modules\n* `pandas` and `numpy`: Pandas Dataframes are used here to store the datasets. Numpy is used along with this, numpy arrays are used which are very handy, allow useful array operations and work well with pandas dataframes\n* `matplotlib.pyplot`: Used here for plotting graphs and plots.\n* `sklearn.preprocessing`: `LabelEncoder` and `StandardScalar` are used for the preprocessing of data. Their use has been documented in detail at their respective cells. \n* `sklearn.metrics`: Its `accuracy_score` is used to generate accuracy. While it could easily be found using a custom accuracy function, I thought it would be better to use a universal function to do it.\n* `seaborn` and `sklearn.metrics.confusion_matrix`: These are used twice in this notbook to create confusion matrices to better understand testing results.\n* `sklearn.neural_network.MLPClassifier` and `sklearn.ensemble.RandomForestClassifier`: These are the two classifier that I have worked on in this notebook.\n* `sklearn.model_selection.StratifiedKFold`: Used for cross-validation of the final model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import metrics\n\nimport seaborn as sn\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Importing the data from kaggle\n* The dataset is loaded from Kaggle. X_train and y_train have been split into training and testing data according to the instructions provided in Slack."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"X_train = pd.read_csv('/kaggle/input/career-con-2019/X_train.csv')\ny_train = pd.read_csv('/kaggle/input/career-con-2019/y_train.csv')\n\n# split X_train\nsamples = 20\ntime_series = 128\n\nstart_x = X_train.shape[0] - samples*time_series\nX_train_new, X_test_new = X_train.iloc[:start_x], X_train.iloc[start_x:]\n\n# split y_train\nstart_y = y_train.shape[0] - samples\ny_train_new, y_test_new = y_train.iloc[:start_y], y_train.iloc[start_y:]\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Approach\nThere are two ways of approaching this ML problem. We have time 128 series data points for each series. One thing that can be done is to disregard the time series entirely and perform model training without altering the size of the dataset. Another approach is that since data of time series is fairly similar, they can all be grouped together using aggregations. This allows for generation of a large number of relevant features if the correct aggregations are used. I have gone for this approach to solve the problem.\n\n## Feature Engineering and Cleaning Data\nThe following has been done for engineering features for the model:\n 1. Angular velocity and acceleration are given in X, Y ,Z. New features `velocityMagnitude` and `accelerationMagnitude` are created from these by getting the magnitude of their resultant vector.\n 2. In order to create single values for each time series, the features have been grouped by the `series_id` and aggregates of the measurements has been considered. \n 3. Various statistical aggregates have been taken: \n     * The mean, median, min, max values are a good representation of the features.\n     * Standard deviation and a 'max-min' (named `variation`) is a useful metric that should be able to separate surfaces based on their roughness.\n     * Since this is a time series data, it is essential that we take the differences between pairs of subsequent measurements. To do this, the differences of the mean has been taken (`AbsMeanDelta`)\n 4. After the training data is engineered, it has the number of rows equal to the number of `series_id` values. \n 5. Since the training data rows have effectivly been internally merged into the number of time series present, there is no need to expand the test set by 128. If the first discussed approach was followed, then this would have to be done \n 6. Features that were engineered that had very low importance scores have been removed. The final feature engineering results in 110 total features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain = X_train_new\nyTrain = y_train_new\n\nxTest = X_test_new\nyTest = y_test_new\n    \ndef engineerFeats(inputDf):\n    df = pd.DataFrame()\n    \n    featureCols = ['orientation_X', 'orientation_Y', 'orientation_Z', \\\n                   'angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z',   \\\n                   'linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']\n    \n    #If we are not considering orientation features\n    #featureCols = ['angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z', \\\n    #               'linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']\n    \n    inputDf['velocityMagnitude'] = ((inputDf['angular_velocity_X']**2) + (inputDf['angular_velocity_Y']**2) + (inputDf['angular_velocity_Z']**2)) ** 0.5\n    featureCols.append('velocityMagnitude')\n    \n    inputDf['accelerationMagnitude'] = ((inputDf['linear_acceleration_X']**2) + (inputDf['linear_acceleration_Y']**2) + (inputDf['linear_acceleration_Z']**2)) ** 0.5\n    featureCols.append('accelerationMagnitude')\n        \n    \n    for col in featureCols:\n        df[col + 'Min'] = inputDf.groupby('series_id')[col].min()\n        df[col + 'Max'] = inputDf.groupby('series_id')[col].max()\n        df[col + 'Mean'] = inputDf.groupby('series_id')[col].mean()\n        df[col + 'Median'] = inputDf.groupby('series_id')[col].median()\n        df[col + 'StdDev'] = inputDf.groupby('series_id')[col].std()\n        df[col + 'q33'] = inputDf.groupby('series_id')[col].quantile(0.33)\n        df[col + 'q66'] = inputDf.groupby('series_id')[col].quantile(0.66)\n        df[col + 'q99'] = inputDf.groupby('series_id')[col].quantile(0.99)\n        df[col + 'variation'] = inputDf.groupby('series_id')[col].max() \\\n                                - inputDf.groupby('series_id')[col].min()\n        df[col + 'AbsMeanDelta'] = inputDf.groupby(['series_id'])[col] \\\n                                .apply(lambda x: np.mean(np.abs(np.diff(x))))\n        \n    #Cleaning the data\n    df.fillna(0, inplace = True)\n    df.replace(-np.inf, 0, inplace = True)\n    df.replace(np.inf, 0, inplace = True)\n    \n    return(df)\n\nprint('Engineering features for Training dataset')\nxTrain = engineerFeats(xTrain)\n\nprint('Engineering features for Testing dataset')\nxTest = engineerFeats(xTest)\n\nprint('xTrain shape -> ',xTrain.shape)\n\nprint('xTest shape -> ',xTest.shape)\n#print(xTest.head())\n\nxTrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding Labels\n* Here, `sklearn.preprocessing.LabelEncoder` is being used to label the categorical surface outputs. It essentially creates a hash mapping of each unique surface with an integer from 0 to n-1 (n being the number of unique surfaces. \n* Label Encoding is required so that the classifier gets discrete integers from 0 to 9 in order to classify properly."},{"metadata":{"trusted":true},"cell_type":"code","source":"labelEncoder = LabelEncoder()\n\nlabelEncoder.fit(yTrain['surface'])\n\nyTrain['label'] = labelEncoder.transform(yTrain['surface'])\nyTest['label'] = labelEncoder.transform(yTest['surface'])\n\nyTrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reviewing the Train and Test datasets\n* This cell is primarily used for debugging.\n* Note: The shape of the output dataframe is `(n,4)` because it stores `series_id`, `group_id`, `surface`, and the actual label: `label`. `DataFrame.label` will be passed as a numpy array to the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('TRAINING FEATURES')\nprint('SHAPE: ',xTrain.shape)\n#print(xTrain.head(), end='\\n---------------\\n')\n\nprint('\\nTRAINING LABELS')\nprint('SHAPE: ',yTrain.shape)\n#print(yTrain.head(), end='\\n---------------\\n')\n\nprint('\\nTESTING FEATURES')\nprint('SHAPE: ',xTest.shape)\n#print(xTest.head(), end='\\n---------------\\n')\n\nprint('\\nTESTING LABELS')\nprint('SHAPE: ',yTest.shape)\n#print(yTest.head(), end='\\n---------------\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models\nModels that were considered were `Logistic Regression`, `Support Vector Machines`, `Decision Trees`, `Random Forests`, `Deep Neural Networks` and `Convolutional Neural Networks`. Models that were implemented were optimized and tested, Logistic and Support vector machines were elimiated first. I tried building a convolutional neural network using tensorflow's `tflearn` module but kaggle experiences import errors when `import tflearn` is called, possibly due to version issues. \n# Neural Network\nScikit learn's Multi Layer Perceptron was used for the Neural network Implemenation. Initially, many models were made and tested on the same data. The parameters were left more or less constant and only the shape of the network was altered to find one that was giving the relatively best results. The `max_iterations` was initially set to 100, this was done to have it be feasible to test many different and complex networks. The final  2 to 3 layers, starting with 200 neurons in the first hidden layer, reducing by around half each layer. The `max_iterations` was gradually increased in subsequent tests while tuning the hyperparameters with small variations."},{"metadata":{},"cell_type":"markdown","source":"## Scaling\nFor a neural network, the data must be properly standardized. `sklearn.preprocessing.StandardScaler` is being used to scale it."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\n# FEATURE SCALING\nscaler.fit(xTrain)\nxTrainScaled = scaler.transform(xTrain)\nxTestScaled = scaler.transform(xTest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"numLabels = len(yTrain.groupby('label').count())\nprint('Number of Labels: ', numLabels)\n\nbSize = int(xTrain.shape[0]/6)\nprint('Batch Size: ', bSize)\n\nhiddenLayersShape = (int(xTrain.shape[1]*2), int(xTrain.shape[1]*1), int(xTrain.shape[1]*0.5))\n#hiddenLayersShape = (int(numLabels*numLabels), int(numLabels*numLabels/2))\nprint('Training Model with NN Hidden Layer Shape: ',hiddenLayersShape)\n\nmodel = MLPClassifier(solver='adam', n_iter_no_change = 35, batch_size = bSize, max_iter = 1000 , hidden_layer_sizes=(hiddenLayersShape), activation = 'tanh', verbose = True)\nmodel.fit(xTrainScaled, yTrain['label'])\nprint('Model Trained')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trueArr = np.array(yTrain['label'])\npredArr = model.predict(xTrainScaled)\n\ntrainingAccuracy = metrics.accuracy_score(trueArr, predArr)\nprint('Training Accuracy: ',trainingAccuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing Accuracy \nThis model was tested with the 20 sample `y_test_new` and had a high variance in scores. The score ranged from `0.85` to `0.95`. The corrosponding confusion matrix has been plotted."},{"metadata":{"trusted":true},"cell_type":"code","source":"trueArr = np.array(yTest['label'])\npredArr = model.predict(xTestScaled)\n\ntestingAccuracy = metrics.accuracy_score(trueArr, predArr)\n'''\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(predArr,trueArr))\nprint('\\n')\nprint('Number of test samples: ',len(trueArr))\n'''\nprint('Testing Acuracy', testingAccuracy)\n\ndfConfusionMat = pd.DataFrame(confusion_matrix(predArr, trueArr))\nplt.figure(figsize = (10,7))\nsn.heatmap(dfConfusionMat, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier\nScikit learn's `RandomForestClassifier` has been used for getting a more reliable classification. It has been consitently giving a low variance in the training and testing accuracies, with a lower difference in training and testing accuracy than the Neural Network."},{"metadata":{},"cell_type":"markdown","source":"## Training the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = RandomForestClassifier(n_estimators = 100, verbose = 2, max_depth = 25)\nmodel = RandomForestClassifier(n_estimators = 600, max_depth = 25 ,n_jobs = -1, verbose = 2)\n\nmodel.fit(xTrain, yTrain['label'])\nprint('Model Trained')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting important features\nFeature importance from this has been the main metric of selecting and engineering features at the start of this notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"### PLOTTING\ndf = pd.DataFrame()\ndf['x'] = [col for col in xTrain]\ndf['y'] = model.feature_importances_\ndf = df.sort_values(by = ['y'])\n\nfig=plt.figure(figsize=(18, df.shape[0]/5))\nplt.xticks(fontsize = 10)\nplt.yticks(fontsize = 10)\nplt.barh(df['x'],df['y'],0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing Accuracy\nThis model was tested with the 20 sample `y_test_new` and is consitent with the scores. This model generally scores `0.95` on the given test data. While comparing this with the neural network, larger data samples were tested, The neural network sometimes gets a better testing accuracy than this; however it has unreliable amount of varation in accuracies even with the same hyperparameters. Due to this, the Random Forest model has been selected for this project. \n\nThe confusion matrix corrosponding to the 20 sample test has been plotted."},{"metadata":{"trusted":true},"cell_type":"code","source":"trueArr = np.array(yTest['label'])\npredArr = model.predict(xTest)\n\ntestingAccuracy = metrics.accuracy_score(trueArr, predArr)\n\n'''\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(predArr, trueArr))\nprint('\\n')\nprint('Number of test samples: ',len(trueArr))\n'''\nprint('Testing Acuracy', testingAccuracy)\n\ndfConfusionMat = pd.DataFrame(confusion_matrix(predArr, trueArr))\nplt.figure(figsize = (10,7))\nsn.heatmap(dfConfusionMat, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stratified K Fold Cross validation\nK-Fold crossvalidation iteratively takes k sections of the dataset to test and train the model. Stratified K-Fold tries to ensure that the ratio of classes in the dataset and subsets of the datasets remains even. This method gives a much less biased value for the accuracy of the model.\n\n#### **PLEASE NOTE** : This Algorithm for Stratified K-Fold Cross validation, while present in some or complete part in most notebooks, has been been directly referred to from https://www.kaggle.com/gpreda/robots-need-help"},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=1234)\n\nsub_preds_rf = np.zeros((xTest.shape[0], 9))\noof_preds_rf = np.zeros((xTrain.shape[0]))\nscore = 0\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(xTrain, yTrain['label'])):\n    #clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n    clf = RandomForestClassifier(n_estimators = 200 ,n_jobs = -1, verbose = 0)\n    clf.fit(xTrain.iloc[trn_idx], yTrain['label'][trn_idx])\n    oof_preds_rf[val_idx] = clf.predict(xTrain.iloc[val_idx])\n    sub_preds_rf += clf.predict_proba(xTest) / folds.n_splits\n    score += clf.score(xTrain.iloc[val_idx], yTrain['label'][val_idx])\n    print('Fold: {} score: {}'.format(fold_,clf.score(xTrain.iloc[val_idx], yTrain['label'][val_idx])))\n    \nprint('\\nAvg Accuracy', score / folds.n_splits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nTo conclude, two models have been developed for this project. The neural network model could be optimized furhter but as it currently stands, the Random Forest is providing a better classification and thus its scores are going to be reported\n\n## Final Scores\n\n* With K-Fold CrossValidation: ~88%\n* Accuracy of sample `y_test_new`: 95% (19/20 samples)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}