{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"<h1><center><font size=\"6\">Robots need help!</font></center></h1>\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/df/RobotsMODO.jpg\" width=\"400\"></img>\n\n<br>\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Prepare the data analysis</a>  \n- <a href='#3'>Data exploration</a>   \n - <a href='#31'>Check the data</a>   \n - <a href='#32'>Distribution of target feature - surface</a>   \n - <a href='#33'>Distribution of group_id</a>    \n - <a href='#34'>Density plots of features</a>   \n - <a href='#35'>Target feature - surface and group_id distribution</a>   \n - <a href='#36'>Features correlation</a>   \n- <a href='#4'>Feature engineering</a>\n- <a href='#5'>Model</a>\n- <a href='#6'>Submission</a>  \n- <a href='#7'>References</a>"},{"metadata":{"_uuid":"9784cc8ed4bceb3bb0ee60778bab3b3355518f37"},"cell_type":"markdown","source":"# <a id='1'>Introduction</a>  \n\n## Competition\nIn this competition, we willl help robots recognize the floor surface theyâ€™re standing on. The floor could be of various types, like carpet, tiles, concrete.\n\n## Data\nThe data provided by the organizers  is collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises.  \n\n## Kernel\nIn this Kernel we perform EDA on the data, explore with feature engineering and build two predictive models.\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/en/3/39/BB-8%2C_Star_Wars_The_Force_Awakens.jpg)"},{"metadata":{"_uuid":"abdb16570ed4a120ab7d9822c12fd3cf5c2339da"},"cell_type":"markdown","source":"# <a id='2'>Prepare for data analysis</a>  \n\n\n## Load packages\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport itertools\nfrom lightgbm import LGBMClassifier\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1baf9418913d861c75106802ec96ec409ed9c7d"},"cell_type":"markdown","source":"## Load data   \n\nLet's check what data files are available."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"660a0cee494565f5cabec59168e932d43ca037f1","scrolled":true},"cell_type":"code","source":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"../input/careercon/\"\nelse:\n    PATH=\"../input/\"\nos.listdir(PATH)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d87a4413ad47c4f1eebba4275f99a731e1ae191"},"cell_type":"markdown","source":"Let's load the data."},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false,"_uuid":"ddbbd73634939a6633428bed86290aece4f2b04c"},"cell_type":"code","source":"%%time\nX_train = pd.read_csv(os.path.join(PATH, 'X_train.csv'))\nX_test = pd.read_csv(os.path.join(PATH, 'X_test.csv'))\ny_train = pd.read_csv(os.path.join(PATH, 'y_train.csv'))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"70f03694af7770fc55246d4fbf133b824369ce50"},"cell_type":"code","source":"print(\"Train X: {}\\nTrain y: {}\\nTest X: {}\".format(X_train.shape, y_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b37dc2ee7a3054c9f7a9d6d120a087208027532"},"cell_type":"markdown","source":"We can observe that train data and labels have different number of rows."},{"metadata":{"_uuid":"d8b1e19622d059e03ff7250d4f76198c254936ec"},"cell_type":"markdown","source":"# <a id='3'>Data exploration</a>  \n\n## <a id='31'>Check the data</a>  \n\nLet's check the train and test set.\n\nWe start with the train."},{"metadata":{},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/en/3/39/R2-D2_Droid.png)"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"aebb41b2152f76491b80b70e2c838d433993d242"},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66c5e3028a2a8160529a165a8908901920d504e9","_kg_hide-input":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We follow with the test."},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"10a2f9b4e0034866ec9eb1fe5f8f74cf3d6fe6f4","_kg_hide-input":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ad951cc65c7cafcf1a5cb987775c691116e5354","_kg_hide-input":false},"cell_type":"markdown","source":"X_train and X_test datasets have the following entries:  \n\n* series and measurements identifiers: **row_id**, **series_id**, **measurement_number**: these identify uniquely a series and measurement; there are 3809 series, each with max 127 measurements;  \n* measurement orientations: **orientation_X**, **orientation_Y**, **orientation_Z**, **orientation_W**;   \n* angular velocities: **angular_velocity_X**, **angular_velocity_Y**, **angular_velocity_Z**;\n* linear accelerations: **linear_acceleration_X**, **linear_acceleration_Y**, **linear_acceleration_Z**.\n\ny_train has the following columns:  \n\n* **series_id** - this corresponds to the series in train data;  \n* **group_id**;  \n* **surface** - this is the surface type that need to be predicted.  \n\n\nLet's check now for missing data.\n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"10f26f3760a7d5b15b7587edb452a5d71539e3aa"},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing data in the train set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5db2dadb11bfa6f0e130ae566099c7ca8a2baf74"},"cell_type":"code","source":"missing_data(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing data in the test set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f73a4fe3ded19c5096a0d532d96e48dd73917b62"},"cell_type":"code","source":"missing_data(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c4f949ea6d589c07ae2ffc46f5f16e77a4cd493"},"cell_type":"markdown","source":"There are no missing values in train and test data.  \nLet's check also train labels."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d352ded04b435b79969f654835e53cc843949f30"},"cell_type":"code","source":"missing_data(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06ae1576a757f0e80397df975c5a246140faaed5","_kg_hide-input":true},"cell_type":"markdown","source":"Also, train labels has no missing data.\n\nLet's check now the data distribution using *describe*."},{"metadata":{"trusted":true,"_uuid":"f415baee23e3f48ec22811f0d9a5266f3693c664","_kg_hide-input":true},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b49d454bf0edba43d2618091260d37101a90857","_kg_hide-input":true},"cell_type":"code","source":"X_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78747516fd066a70f0f4db78f9543d08b93334d1","_kg_hide-input":true},"cell_type":"code","source":"y_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f62c1341e7d69716f0a599c80ead90d41e0f7473"},"cell_type":"markdown","source":"There is the same number of series in X_train and y_train, numbered from 0 to 3809 (total 3810). Each series have 128 measurements.   \nEach series in train dataset is part of a group (numbered from 0 to 72).  \nThe number of rows in X_train and X_test differs with 6 x 128, 128 being the number of measurements for each group.  "},{"metadata":{"_uuid":"a449515fe43d68dbbcf1d70cec8d663ddf7d5dfb"},"cell_type":"markdown","source":"## <a id='32'>Distribution of target feature - surface</a> \n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8d0db47428faaa50c9a7a6002dd7e08ca953ca57"},"cell_type":"code","source":"f, ax = plt.subplots(1,1, figsize=(16,4))\ntotal = float(len(y_train))\ng = sns.countplot(y_train['surface'], order = y_train['surface'].value_counts().index, palette='Set3')\ng.set_title(\"Number and percentage of labels for each class\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(100*height/total),\n            ha=\"center\") \nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8ee735c665db1764766aa75654db3971ccc74bf"},"cell_type":"markdown","source":"## <a id='33'>Distribution of group_id</a>  "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d1bceb6846adbd3ee3bc900afa68fecda8d4d2fe"},"cell_type":"code","source":"f, ax = plt.subplots(1,1, figsize=(18,8))\ntotal = float(len(y_train))\ng = sns.countplot(y_train['group_id'], order = y_train['group_id'].value_counts().index, palette='Set3')\ng.set_title(\"Number and percentage of group_id\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.1f}%'.format(100*height/total),\n            ha=\"center\", rotation='90') \nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42ed8502739e99aaa472c3c36ba70ef2fcb95ebc"},"cell_type":"markdown","source":"## <a id='34'>Density plots of features</a>  \n\nLet's show now the density plot of variables in train and test dataset. \n\nWe represent with different colors the distribution for values with different values of **surface**.\n\nWe introduce two utility functions for plotting."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2e0f6a8ca0e6f1c64b24f5d4b9e033516e59d8ec"},"cell_type":"code","source":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,5,figsize=(16,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,5,i)\n        sns.distplot(df1[feature], hist=False, label=label1)\n        sns.distplot(df2[feature], hist=False, label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f1260bd87f03984fe5573323837a9f9961a8d25","_kg_hide-input":true},"cell_type":"code","source":"features = X_train.columns.values[3:13]\nplot_feature_distribution(X_train, X_test, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"85d698f6a2898d01f51717be90e4d467eed6ce64"},"cell_type":"code","source":"def plot_feature_class_distribution(classes,tt, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,2,figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5,2,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.distplot(ttc[feature], hist=False,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"479d3b7b152a5dae5a07fba203381aa43d973a97"},"cell_type":"code","source":"classes = (y_train['surface'].value_counts()).index\ntt = X_train.merge(y_train, on='series_id', how='inner')\nplot_feature_class_distribution(classes, tt, features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc192afa20abe8b5c80fa119470ae85f78f932ef"},"cell_type":"markdown","source":"## <a id='35'>Target feature - surface and group_id distribution</a>  \n\nLet's show now the distribution of target feature - surface and group_id."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"905a36932960c979882b51404f94168e02a7f506"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(24,6))\ntmp = pd.DataFrame(y_train.groupby(['group_id', 'surface'])['series_id'].count().reset_index())\nm = tmp.pivot(index='surface', columns='group_id', values='series_id')\ns = sns.heatmap(m, linewidths=.1, linecolor='black', annot=True, cmap=\"YlGnBu\")\ns.set_title('Number of surface category per group_id', size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"851b183833c421fc26b5d8db44514db10d7bb154"},"cell_type":"markdown","source":"# <a id='36'>Features correlation</a>  \n\nLet's check the features correlation for train set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"479d88091087a77849b89a6bd73588e046ad993a"},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(6,6))\nm = X_train.iloc[:,3:].corr()\nsns.heatmap(m, annot=True, linecolor='darkblue', linewidths=.1, cmap=\"YlGnBu\", fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffb1c05457ee7f6d18af52e0c04685ef236187b7"},"cell_type":"markdown","source":"Very strong correlation (1.0) is between **orientation_X** and **orientation_W** and between **orientation_Z** and **orientation_Y**.   \nThere is a strong inverse correlation (-0.8) between **angular_velocity_Z** and **angular_velocity_Y**.    \nAlso, there is a medium positive correlation (0.4) between **linear_acceleration_Y** and **linear_acceleration_Z**.  \n\nLet's also check the features correlation for test set.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"949da09898831b94872c38eba1b59210e8fd0edf"},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(6,6))\nm = X_test.iloc[:,3:].corr()\nsns.heatmap(m, annot=True, linecolor='darkblue', linewidths=.1, cmap=\"YlGnBu\", fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a849ec5f058341fdfc267c84bcf4aff48cec2b79"},"cell_type":"markdown","source":"Very strong correlation (1.0) is between **orientation_X** and **orientation_W** and between **orientation_Z** and **orientation_Y**.   \nThere is a strong inverse correlation (-0.8) between **angular_velocity_Z** and **angular_velocity_Y**.    \nAlso, there is a medium positive correlation (0.4) between **linear_acceleration_Y** and **linear_acceleration_Z**.  "},{"metadata":{"_uuid":"394b783f6bbfcaf18a43c41faf11f7de121ba2eb"},"cell_type":"markdown","source":"# <a id='4'>Features engineering</a>  \n"},{"metadata":{"_uuid":"a9ab8347dd0fc263d6a81e2316b0939bfa25f2da"},"cell_type":"markdown","source":"This section is heavily borrowing from: https://www.kaggle.com/vanshjatana/help-humanity-by-helping-robots Kernel. \nThe quaternion_to_euler transformation procedure is also credited in the original Kernel, and I kept this reference as well.\nI also corrected few issues and added some more engineered features. Thanks for @timmmmmms for pointing them out."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"6735b8e29eeeb48a635a907355fca4f91b34d95d"},"cell_type":"code","source":"# https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We calculate euler factors and several addtional features starting from the original features."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def perform_euler_factors_calculation(df):\n    df['total_angular_velocity'] = np.sqrt(np.square(df['angular_velocity_X']) + np.square(df['angular_velocity_Y']) + np.square(df['angular_velocity_Z']))\n    df['total_linear_acceleration'] = np.sqrt(np.square(df['linear_acceleration_X']) + np.square(df['linear_acceleration_Y']) + np.square(df['linear_acceleration_Z']))\n    df['total_xyz'] = np.sqrt(np.square(df['orientation_X']) + np.square(df['orientation_Y']) +\n                              np.square(df['orientation_Z']))\n    df['acc_vs_vel'] = df['total_linear_acceleration'] / df['total_angular_velocity']\n    \n    x, y, z, w = df['orientation_X'].tolist(), df['orientation_Y'].tolist(), df['orientation_Z'].tolist(), df['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    df['euler_x'] = nx\n    df['euler_y'] = ny\n    df['euler_z'] = nz\n    \n    df['total_angle'] = np.sqrt(np.square(df['euler_x']) + np.square(df['euler_y']) + np.square(df['euler_z']))\n    df['angle_vs_acc'] = df['total_angle'] / df['total_linear_acceleration']\n    df['angle_vs_vel'] = df['total_angle'] / df['total_angular_velocity']\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We define the routine for feature engineering."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b05e1a29dc7cc012384c02643f329be539d3908a"},"cell_type":"code","source":"def perform_feature_engineering(df):\n    df_out = pd.DataFrame()\n    \n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n\n    def mean_abs_change(x):\n        return np.mean(np.abs(np.diff(x)))\n    \n    for col in df.columns:\n        if col in ['row_id', 'series_id', 'measurement_number']:\n            continue\n        df_out[col + '_mean'] = df.groupby(['series_id'])[col].mean()\n        df_out[col + '_min'] = df.groupby(['series_id'])[col].min()\n        df_out[col + '_max'] = df.groupby(['series_id'])[col].max()\n        df_out[col + '_std'] = df.groupby(['series_id'])[col].std()\n        df_out[col + '_mad'] = df.groupby(['series_id'])[col].mad()\n        df_out[col + '_med'] = df.groupby(['series_id'])[col].median()\n        df_out[col + '_skew'] = df.groupby(['series_id'])[col].skew()\n        df_out[col + '_range'] = df_out[col + '_max'] - df_out[col + '_min']\n        df_out[col + '_max_to_min'] = df_out[col + '_max'] / df_out[col + '_min']\n        df_out[col + '_mean_abs_change'] = df.groupby('series_id')[col].apply(mean_abs_change)\n        df_out[col + '_mean_change_of_abs_change'] = df.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df_out[col + '_abs_max'] = df.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n        df_out[col + '_abs_min'] = df.groupby('series_id')[col].apply(lambda x: np.min(np.abs(x)))\n        df_out[col + '_abs_mean'] = df.groupby('series_id')[col].apply(lambda x: np.mean(np.abs(x)))\n        df_out[col + '_abs_std'] = df.groupby('series_id')[col].apply(lambda x: np.std(np.abs(x)))\n        df_out[col + '_abs_avg'] = (df_out[col + '_abs_min'] + df_out[col + '_abs_max'])/2\n        df_out[col + '_abs_range'] = df_out[col + '_abs_max'] - df_out[col + '_abs_min']\n\n    return df_out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Euler factors and additional features\n\nWe calculate the Euler factors and few additional features. First we calculate for train set."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%time\nX_train = perform_euler_factors_calculation(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we calculate the same factors for test set."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%time\nX_test = perform_euler_factors_calculation(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"features = X_train.columns.values[13:23]\nplot_feature_distribution(X_train, X_test, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"classes = (y_train['surface'].value_counts()).index\ntt = X_train.merge(y_train, on='series_id', how='inner')\nplot_feature_class_distribution(classes, tt, features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregated feature engineering\n\nWe apply now the feature engineering procedure for train and test. \nThe resulted features are calculated by aggregation of original features (and the features calculated in the previous step - optionally). "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"USE_ALL_FEATURES = False\nif(USE_ALL_FEATURES):\n    features = X_train.columns.values\nelse:\n    features = X_train.columns.values[:13]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"310409206394429b14bf60e7bd759234bd2d81c5"},"cell_type":"code","source":"%%time\nX_train = perform_feature_engineering(X_train[features])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9d4aafa4271c524651b68f860d310bb5a64d1e65"},"cell_type":"code","source":"%%time\nX_test = perform_feature_engineering(X_test[features])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f585f9545bdb6265e93e3529d2e77f3b9732d40a"},"cell_type":"markdown","source":"After feature engineering, the new shapes are:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"fc5eb9245984978a2e3e310b23e905a66ff871fd"},"cell_type":"code","source":"print(\"Train X: {}\\nTrain y: {}\\nTest X: {}\".format(X_train.shape, y_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"eef14b6a9b05bdeca2e8dd2f6bca98e7915ecfdd"},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"0103613591cb8146a2c20fc0471c8da5d37cd82b"},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"## Features correlation\n\n\nLet's look now to the new features correlation for train set."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%time\ncorrelations = X_train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the least correlated features."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"correlations.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And let's see now the most correlated features. We show only the first 10, then we print the total number of them."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"correlations.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"n_top_corr = correlations[correlations[0]==1.0].shape[0]\nprint(\"There are {} different features pairs with correlation factor 1.0.\".format(n_top_corr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We eliminate the features that have a correlation factor 1.0 with other features.   \n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"drop_features = list(correlations.head(n_top_corr)['level_0'].unique())\nX_train = X_train.drop(drop_features,axis=1)\nX_test = X_test.drop(drop_features,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nLet's show again the shape of train and test."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Train X: {}\\nTrain y: {}\\nTest X: {}\".format(X_train.shape, y_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We visualize the correlation matrix."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"corr = X_train.corr()\nfig, ax = plt.subplots(1,1,figsize=(16,16))\nsns.heatmap(corr,  xticklabels=False, yticklabels=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1de959929ee5a7f643ba02c00390cf8428a0ad90"},"cell_type":"markdown","source":"# <a id='5'>Model</a>  \n\nWe use LabelEncoder for the target feature."},{"metadata":{"trusted":true,"_uuid":"3c0a07575a08df25acf3c2d648f2c15dcbc1de5f","_kg_hide-input":true},"cell_type":"code","source":"le = LabelEncoder()\ny_train['surface'] = le.fit_transform(y_train['surface'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d8bf8326b07d7a813b914e0605ae24c4508a90d"},"cell_type":"markdown","source":"We replace with 0 NAs and $\\infty$."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"67b8bfb18f66307f15270b8fa7fcf0827f3ebe17"},"cell_type":"code","source":"X_train.fillna(0, inplace = True)\nX_train.replace(-np.inf, 0, inplace = True)\nX_train.replace(np.inf, 0, inplace = True)\nX_test.fillna(0, inplace = True)\nX_test.replace(-np.inf, 0, inplace = True)\nX_test.replace(np.inf, 0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We scale the train and test data."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\nX_test_scaled = pd.DataFrame(scaler.transform(X_test))\nprint (\"Scaled !\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4096b3516249d5c54d1b2c909e17a7906fafc923"},"cell_type":"markdown","source":"## Prepare for cross-validation"},{"metadata":{"trusted":true,"_uuid":"613d0b00834f0ed248556b858b4a4d793b8e8180","_kg_hide-input":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=49, shuffle=True, random_state=2018)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3b31725e5e353783a5625542936c00175059405"},"cell_type":"markdown","source":"## Random Forest classifier\n\nWe use first a Random Forest Classifier model."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"fedc8cba976a3640613080a5e379033090c6d4b3"},"cell_type":"code","source":"sub_preds_rf = np.zeros((X_test_scaled.shape[0], 9))\noof_preds_rf = np.zeros((X_train_scaled.shape[0]))\nscore = 0\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_scaled, y_train['surface'])):\n    clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n    clf.fit(X_train_scaled.iloc[trn_idx], y_train['surface'][trn_idx])\n    oof_preds_rf[val_idx] = clf.predict(X_train_scaled.iloc[val_idx])\n    sub_preds_rf += clf.predict_proba(X_test_scaled) / folds.n_splits\n    score += clf.score(X_train_scaled.iloc[val_idx], y_train['surface'][val_idx])\n    print('Fold: {} score: {}'.format(fold_,clf.score(X_train_scaled.iloc[val_idx], y_train['surface'][val_idx])))\nprint('Avg Accuracy', score / folds.n_splits)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3578b1854543f46b61915922e30d8400bf995098"},"cell_type":"markdown","source":"Let's check the confusion matrix.\n\nWe will use a simplifed version of the plot function defined here: https://www.kaggle.com/artgor/where-do-the-robots-drive"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d762daf19c18974b70521c2f603d6918e26825b2"},"cell_type":"code","source":"def plot_confusion_matrix(actual, predicted, classes, title='Confusion Matrix'):\n    conf_matrix = confusion_matrix(actual, predicted)\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Greens)\n    plt.title(title, size=12)\n    plt.colorbar(fraction=0.05, pad=0.05)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    thresh = conf_matrix.max() / 2.\n    for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n        plt.text(j, i, format(conf_matrix[i, j], 'd'),\n        horizontalalignment=\"center\", color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"10516615f053bcff2969af4db3a6d42f75284406"},"cell_type":"code","source":"plot_confusion_matrix(y_train['surface'], oof_preds_rf, le.classes_, title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e7b27f046b9e7b3e72665bd9208db30b006ccd9"},"cell_type":"markdown","source":"\n## LightGBM Classifier\n\nWe also use a LightGBM Classifier model."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a5c46248d71fff6666db8605ba541939e04a4eac"},"cell_type":"code","source":"USE_LGB = True\nif(USE_LGB):\n    sub_preds_lgb = np.zeros((X_test.shape[0], 9))\n    oof_preds_lgb = np.zeros((X_train.shape[0]))\n    score = 0\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train['surface'])):\n        train_x, train_y = X_train.iloc[trn_idx], y_train['surface'][trn_idx]\n        valid_x, valid_y = X_train.iloc[val_idx], y_train['surface'][val_idx]\n        clf =  LGBMClassifier(\n                      nthread=-1,\n                      n_estimators=2000,\n                      learning_rate=0.01,\n                      boosting_type='gbdt',\n                      is_unbalance=True,\n                      objective='multiclass',\n                      numclass=9,\n                      silent=-1,\n                      verbose=-1,\n                      feval=None)\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n                     verbose= 1000, early_stopping_rounds= 200)\n\n        oof_preds_lgb[val_idx] = clf.predict(valid_x)\n        sub_preds_lgb += clf.predict_proba(X_test) / folds.n_splits\n        score += clf.score(valid_x, valid_y)\n        print('Fold: {} score: {}'.format(fold_,clf.score(valid_x, valid_y)))\n    print('Avg Accuracy', score / folds.n_splits)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6849fb0f877d73e91820a46b732c35c4d81f8b63"},"cell_type":"markdown","source":"# <a id='6'>Submission</a>  \n\nWe submit the solution for both the RF and LGB."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"410056706ec800a78050d6850c2620a96ae3a70c"},"cell_type":"code","source":"submission = pd.read_csv(os.path.join(PATH,'sample_submission.csv'))\nsubmission['surface'] = le.inverse_transform(sub_preds_rf.argmax(axis=1))\nsubmission.to_csv('submission_rf.csv', index=False)\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29370efad06e957166e5e0b3e7d36e810d421860","_kg_hide-input":true},"cell_type":"code","source":"USE_LGB = True\nif(USE_LGB):\n    submission['surface'] = le.inverse_transform(sub_preds_lgb.argmax(axis=1))\n    submission.to_csv('submission_lgb.csv', index=False)\n    submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7664ec331050ab5aefd2cf26c898f7c9c6c7e352"},"cell_type":"markdown","source":"# <a id='7'>References</a>    \n\n[1] https://www.kaggle.com/vanshjatana/help-humanity-by-helping-robots-4e306b  \n[2] https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\n[3] https://www.kaggle.com/artgor/where-do-the-robots-drive  \n[4] https://www.kaggle.com/hsinwenchang/randomforestclassifier  \n[5] https://en.wikipedia.org/wiki/Quaternion\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}