{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Help Robots Navigate - Trying with Spectral Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Revisions:\n- Rev 01: Trying to learnd from freq domain using LGBM\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Imports\n# General imports\nimport time\nimport math\n\nfrom pathlib import Path\n\n# FFT\nfrom scipy import fftpack\n\n# Standard DS imports\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import mode\n\n# ML Related Imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgb\n\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Constanta\nHOME_DIR = Path('../input')\nTRAIN_VAL_SPLIT = 0.2\nRANDOM_SEED = 42\nF_S = 400    # Sampling frequency (need to investigate this more . . . )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Read in all data\nX_train_df = pd.read_csv(HOME_DIR/'X_train.csv')\ny_train_df = pd.read_csv(HOME_DIR/'y_train.csv')\nX_test_df = pd.read_csv(HOME_DIR/'X_test.csv')\nsample_submission_df = pd.read_csv(HOME_DIR/'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'X_train shape: {X_train_df.shape}\\ny_train shape: {y_train_df.shape}\\n'\n      f'X_test shape: {X_test_df.shape}\\nsample submission shape: {sample_submission_df.shape}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now take a quick look at the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_df.surface.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prep Data for Training"},{"metadata":{"trusted":false},"cell_type":"code","source":"def group_by_series_id(df):\n\n    columns_to_use = [ #'orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W',\n                      'angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z',\n                      'linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']\n    # columns_to_use = ['orientation_X', 'orientation_Y']\n\n    def get_col_values(grouped_df):  \n        tmp_df = pd.DataFrame()\n        for col in columns_to_use:\n            # Abbreviate 'orientation_X' to 'oX', etc.\n            col_abbr = ''.join([col[0], col[-1]])\n            \n            tmp_df[col_abbr] = grouped_df[col].values\n        return tmp_df\n        \n    return df.groupby('series_id').apply(get_col_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def group_by_series_id2(df):\n    # This version is faster. I think because its not being forced into tmp_df over and over again\n\n    columns_to_use = [ 'orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W',\n                      'angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z',\n                      'linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']\n    # columns_to_use = ['orientation_X', 'orientation_Y']\n\n    def get_col_values(grouped_df):\n        # Make a dict of col_abbr:full_column_name. This is used both to make col names short and create a new DF\n        # with the values of the columns\n        tmp_dict = {}\n        for col in columns_to_use:\n            # Abbreviate 'orientation_X' to 'oX', etc.\n            col_abbr = ''.join([col[0], col[-1]])\n            tmp_dict[col_abbr] = col\n        # Now use a dict comprehension and return only the values in each of the columns\n        return pd.DataFrame({col_abbr:grouped_df[col].values for (col_abbr, col) in tmp_dict.items()})\n        \n    return df.groupby('series_id').apply(get_col_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# Get the data into a DF grouped by series_id\nX_train_grouped = group_by_series_id2(X_train_df)\nX_test_grouped = group_by_series_id2(X_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Some sanity checking\nX_train_df[X_train_df.series_id == 247].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_grouped.loc[247].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def convert_td_to_fd(td):\n    ''' Return frequency domain data from time domain data'''\n    fd = fftpack.fft(td)\n    return np.abs(fd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def add_fd_columns(df):\n    ''' This will add freq domain columns (appending_f) to the td columns of df '''\n    # Apply FFT to each column of the df and add the '_f' suffix to resulting dataframe columns to indicate freq domain\n    df_f = df.apply(convert_td_to_fd).add_suffix('_fd')\n    # Now merge the two df and return it\n    return pd.merge(df, df_f, how='outer', left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# Now add the freq domain data to both train and test dfs (suffix _wfd = with freq domain)\nX_train_wfd = X_train_grouped.groupby('series_id').apply(add_fd_columns)\nX_test_wfd = X_test_grouped.groupby('series_id').apply(add_fd_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_wfd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Extract values from the FD columns of X_train_wfd and Y_train_wfd to prep for training\nx_train_fd_cols = [x for x in X_train_wfd if x.endswith('_fd') ]\nX_train_values=X_train_wfd[x_train_fd_cols].groupby('series_id').apply(lambda x: x.values.T)\nx_test_fd_cols = [x for x in X_test_wfd if x.endswith('_fd') ]\nX_test_values=X_test_wfd[x_train_fd_cols].groupby('series_id').apply(lambda x: x.values.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Convert the y_train catogories (concrete, etc) to numbers\ny_labels, y_categoricals = pd.factorize(y_train_df.surface)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Split the training set to train and valid sets. Doing spimple splitting for now. More sophisticated splits will be done\n# in later experiments\ntrain_X, val_X, train_y, val_y = train_test_split(X_train_values, y_labels, test_size=TRAIN_VAL_SPLIT, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_X.shape, train_y.shape, val_X.shape, val_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Now the train and val X need to be \"stacked\" to make them a 2 dim NP array so it can be fed into the models\ntrain_X_np = np.vstack(train_X)\nval_X_np = np.vstack(val_X)\n# Similarly with the test set\ntest_X_np = np.vstack(X_test_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# After the train and val are stacket, the y labels must be repeated by len(x_train_fd_cols) times.\ntrain_y_rep = np.repeat(train_y, len(x_train_fd_cols))\nval_y_rep = np.repeat(val_y, len(x_train_fd_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_X_np.shape, train_y_rep.shape, val_X_np.shape, val_y_rep.shape, test_X_np.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now Train, validate, etc"},{"metadata":{"trusted":false},"cell_type":"code","source":"# LightGBM Parameters\nparams = {'application': 'multiclass',\n          'num_class': 9,\n          'boosting': 'gbdt',\n          # 'metric': 'rmse',\n          'num_leaves': 600,    # Orig: 90\n          'max_depth': 100,      # Orig: 9\n          'min_data_in_leaf': 80,\n          'learning_rate': 0.25, # Orig: 0.01. 0.25 gives best result so far\n          # 'n_estimators': 100,\n          'bagging_fraction': 0.85,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.02,    # Orig: 0.02\n          'min_child_samples': 150,   # Orig: 150\n          'min_child_weight': 0.002,    # Orig: 0.02\n          'lambda_l2': 0.0475,             # Orig: 0.0475\n          # 'lambda_l1': 0.1,\n          'verbosity': -1,\n          # 'n_jobs': 8,\n          'data_random_seed': RANDOM_SEED}\n\n# Additional parameters:\nearly_stop = 1000\nverbose_eval = 100\nn_splits = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nlgb_clf = lgb.LGBMClassifier(**params)\nlgb_clf.fit(train_X_np, train_y_rep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predict on the validation set\npred_val_y = lgb_clf.predict(val_X_np)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predict multiclass accuracy\naccuracy_score(val_y_rep, pred_val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Retrain on the full set\ntrain_X_full_np = np.vstack(X_train_values)\ntrain_y_full_rep = np.repeat(y_labels, len(x_train_fd_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_X_full_np.shape, train_y_full_rep.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nlgb_clf_full = lgb.LGBMClassifier(**params)\nlgb_clf_full.fit(train_X_full_np, train_y_full_rep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predict for test set\npred_test_y = lgb_clf_full.predict(test_X_np)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_X_np.shape, pred_test_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Reshape the array such that predictions for one series is in one row\npred_test_y_reshape = pred_test_y.reshape((X_test_values.shape[0], len(x_test_fd_cols)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_test_y_reshape.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# For now, simply take the mode of each row\nsubmission_preds = sp.stats.mode(pred_test_y_reshape, axis=1)[0].reshape(-1)\nsubmission_preds_counts = sp.stats.mode(pred_test_y_reshape, axis=1)[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get the equivalent of \"value_counts\" (or table() in R) for the prediction counts.\n# This will give us a feel for the number of predictions that were same for each feature\nunique, counts = np.unique(submission_preds_counts, return_counts=True)\nnp.asarray((unique, counts)).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission_df.surface = list(map(lambda x: y_categoricals[int(x)], submission_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do a quick sanity check\nsample_submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do a quick sanity check\nsample_submission_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Now write it out\nsample_submission_df.to_csv('submission_fft_lgbm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":1}