{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Jonathan Glaser**\n\n**jmg764**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"X_test_new = pd.read_csv(\"../input/new-datasets/X_test_new.csv\")\nX_train_new = pd.read_csv(\"../input/new-datasets/X_train_new.csv\")\ny_test_new = pd.read_csv(\"../input/new-datasets/y_test_new.csv\")\ny_train_new = pd.read_csv(\"../input/new-datasets/y_train_new.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"**Distribution of surface types in y_train_new**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_new.surface.value_counts().plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Distributions of feature values in series_id = 0**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(26, 16))\nfor i, col in enumerate(X_train_new.columns[4:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(X_train_new.loc[X_train_new['series_id'] == 0, col])\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"In X_train_new, each series_id corresponds to a collection of measurements obtained from a robot driving over a given floor surface. For example, rows 1-128 contain the orientation_X, orientation_Y, orientation_Z, angular_velocity_X, etc of various robots driving on fine concrete. It is therefore more useful to create a selection of features that summarize the measurements obtained on each surface. For example, we can obtain orientation_X_mean to represent the average orientation_X values for a particular series_id. "},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=['orientation_X','orientation_Y','orientation_Z','orientation_W','angular_velocity_X','angular_velocity_Y','angular_velocity_Z','linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']\ndef feature_data(X):\n    new_data=pd.DataFrame()\n    for col in columns:\n        new_data[col+'_mean'] = X.groupby(['series_id'])[col].mean()\n        new_data[col+'_median'] = X.groupby(['series_id'])[col].median()\n        new_data[col+'_max'] = X.groupby(['series_id'])[col].max()\n        new_data[col+'_min'] = X.groupby(['series_id'])[col].min()\n        new_data[col + '_abs_max'] = X.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        new_data[col + '_abs_min'] = X.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        new_data[col + '_abs_avg'] = (new_data[col + '_abs_min'] + new_data[col + '_abs_max'])/2\n        new_data[col+'_var'] = X.groupby(['series_id'])[col].var()\n        new_data[col+'_std'] = X.groupby(['series_id'])[col].std()\n        new_data[col + '_maxtoMin'] = new_data[col + '_max'] / new_data[col + '_min']\n        new_data[col + '_range'] = new_data[col + '_max'] - new_data[col + '_min']\n        new_data[col + '_mean_abs_chg'] = X.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        new_data[col + '_abs_median_chg'] = X.groupby(['series_id'])[col].apply(lambda x: np.median(np.abs(np.diff(x))))\n        new_data[col + '_abs_std_chg'] = X.groupby(['series_id'])[col].apply(lambda x: np.std(np.abs(x)))\n        \n    return new_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new_2 = feature_data(X_train_new)\nX_train_new_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_new_2 = feature_data(X_test_new)\nX_test_new_2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n* Here we evaluate several supervised learning models including multinomial logistic regression, decision tree, random forest, support vector machine, and k-nearest neighbors on their ability to accurately classify a particular surface given the information provided in X_train_new.\n* Each model evaluation explores two scenarios: \n    1. Training the model using X_train_new and y_train_new and determining accuracy by testing on X_test_new and comparing results with y_test_new.\n    2. Combining X_train_new with X_test_new and y_train_new with y_test_new in order to evaluate the accuracy of the given model using k-fold cross validation. "},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"**Part 1**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport warnings  \nwarnings.filterwarnings('ignore')\n\nlr = LogisticRegression(multi_class='multinomial')\nlr.fit(X_train_new_2, y_train_new['surface'])\ny_pred = lr.predict(X_test_new_2)\naccuracy = metrics.accuracy_score(y_test_new['surface'], y_pred)\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Part 2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine X_train_new_2 and X_test_new_2, as well as y_train_new and y_test_new \n# in order to create X and y dataframes for performing k-fold cross validation\n\nX_data = [X_train_new_2, X_test_new_2]\ny_data = [y_train_new, y_test_new]\n\nX = np.asarray(pd.concat(X_data))\ny = np.asarray(pd.concat(y_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nimport statistics\n\ndef k_fold_cross_validation_logistic(k, X, y):\n    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n    avg_accuracy = 0\n    accuracies = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # scaling the data matrix:\n        X_train = preprocessing.scale(X_train)\n        X_test = preprocessing.scale(X_test)\n        \n        # Make prediction and determine accuracy for this fold:\n        lr = LogisticRegression(multi_class='multinomial')\n        lr.fit(X_train, y_train)\n        y_pred = lr.predict(X_test)\n        accuracy = metrics.accuracy_score(y_test, y_pred)\n        avg_accuracy += accuracy\n        accuracies.append(accuracy)\n\n    avg_accuracy = avg_accuracy / k\n    stdev = statistics.stdev(accuracies)\n    return  avg_accuracy, stdev\n\navg_accuracy, stdev = k_fold_cross_validation_logistic(10, X, y[:,3])\nprint(\"Average 10-fold CV accuracy = {:10.2f}, standard deviation = {:10.2f}\".format(avg_accuracy, stdev))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The low standard deviation obtained suggests that the logistic regression model tends to have low variance which indicates that it is reliable. Additionally, the higher accuracy average obtained using 10-fold cross validation may be due to the fact that it uses a larger dataset (X_train_new and X_test_new combined) thereby allowing for better label predictions. "},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"**Part 1**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier()\n\ntree.fit(X_train_new_2, y_train_new['surface'])\ny_pred_test_tree = tree.predict(X_test_new_2) \naccuracy = metrics.accuracy_score(y_test_new['surface'], y_pred_test_tree)\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Part 2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_fold_cross_validation_decisiontree(k, X, y):\n    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n    avg_accuracy = 0\n    accuracies = []\n    \n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # scaling the data matrix:\n        X_train = preprocessing.scale(X_train)\n        X_test = preprocessing.scale(X_test)\n        \n        # Make prediction and determine accuracy for this fold:\n        tree = DecisionTreeClassifier()\n        tree.fit(X_train, y_train)\n        y_pred = tree.predict(X_test) \n        accuracy = metrics.accuracy_score(y_test, y_pred)\n        avg_accuracy += accuracy\n        accuracies.append(accuracy)\n        \n    avg_accuracy = avg_accuracy / k\n    stdev = statistics.stdev(accuracies)\n    return  avg_accuracy, stdev\n\n\navg_accuracy, stdev = k_fold_cross_validation_decisiontree(10, X, y[:,3])\nprint(\"Average 10-fold CV accuracy = {:10.2f}, standard deviation = {:10.2f}\".format(avg_accuracy, stdev))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above results support the generality that decision trees work well with the data used to create them, but they are not flexible when it comes to classifying new samples. \n\nThe higher accuracy in part 1 compared to part 2 shows that the decision tree classifier has a tendency to overfit. The standard deviation of 10-fold cross validation accuracies is slightly higher than that of logistic regression. Taken together, these indicate low bias and high variance which is generally not favorable when selecting a model."},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"**Part 1**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=600)\n\nforest.fit(X_train_new_2, y_train_new['surface'])\ny_pred_test_forest = forest.predict(X_test_new_2) \naccuracy = metrics.accuracy_score(y_test_new['surface'], y_pred_test_forest)\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Part 2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_fold_cross_validation_randomforest(k, X, y):\n    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n    avg_accuracy = 0\n    accuracies = []\n    \n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # scaling the data matrix:\n        X_train = preprocessing.scale(X_train)\n        X_test = preprocessing.scale(X_test)\n        \n        # Make prediction and determine accuracy for this fold:\n        forest = RandomForestClassifier(n_estimators=600)\n        forest.fit(X_train, y_train)\n        y_pred = forest.predict(X_test) \n        accuracy = metrics.accuracy_score(y_test, y_pred)\n        avg_accuracy += accuracy\n        accuracies.append(accuracy)\n        \n    avg_accuracy = avg_accuracy / k\n    stdev = statistics.stdev(accuracies)\n    return  avg_accuracy, stdev\n\n\navg_accuracy, stdev = k_fold_cross_validation_randomforest(10, X, y[:,3])\nprint(\"Average 10-fold CV accuracy = {:10.2f}, standard deviation = {:10.2f}\".format(avg_accuracy, stdev))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general, random forests are more flexible than decision trees when it comes to classifying new samples. The above results support this claim since it resulted in higher accuracy and less variance than decision tree classification."},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{},"cell_type":"markdown","source":"**Part 1**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm = SVC(decision_function_shape='ovo')\nsvm.fit(X_train_new_2, y_train_new['surface'])\ny_pred_test_svm = svm.predict(X_test_new_2) \naccuracy = metrics.accuracy_score(y_test_new['surface'], y_pred_test_svm)\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Part 2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_fold_cross_validation_svm(k, X, y):\n    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n    avg_accuracy = 0\n    accuracies = []\n    \n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # scaling the data matrix:\n        X_train = preprocessing.scale(X_train)\n        X_test = preprocessing.scale(X_test)\n        \n        # Make prediction and determine accuracy for this fold:\n        svm = SVC(decision_function_shape='ovo')\n        svm.fit(X_train, y_train)\n        y_pred = svm.predict(X_test)  \n        accuracy = metrics.accuracy_score(y_test, y_pred)\n        avg_accuracy += accuracy\n        accuracies.append(accuracy)\n        \n    avg_accuracy = avg_accuracy / k\n    stdev = statistics.stdev(accuracies)\n    return  avg_accuracy, stdev\n\n\navg_accuracy, stdev = k_fold_cross_validation_svm(10, X, y[:,3])\nprint(\"Average 10-fold CV accuracy = {:10.2f}, standard deviation = {:10.2f}\".format(avg_accuracy, stdev))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As with logistic regression, average 10-fold cross validation accuracy is higher than when simply training on X_train_new and testing on X_test_new, and standard deviation is relatively low. Here, however, the disparity between accuracies is greater which suggests that SVM may have a higher dependence on dataset size than logistic regression. "},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbors"},{"metadata":{},"cell_type":"markdown","source":"**Part 1**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_accuracies = []\nfor i in range(1, 100):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train_new_2, y_train_new['surface'])\n    y_pred_test_knn = knn.predict(X_test_new_2) \n    accuracy = metrics.accuracy_score(y_test_new['surface'], y_pred_test_knn)\n    knn_accuracies.append(accuracy)\n\nprint(\"Maximum accuracy:\", max(knn_accuracies), \"obtained at k =\", knn_accuracies.index(max(knn_accuracies)))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Part 2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_fold_cross_validation_knn(k, X, y):\n    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n    avg_accuracy = 0\n    accuracies = []\n    \n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # scaling the data matrix:\n        X_train = preprocessing.scale(X_train)\n        X_test = preprocessing.scale(X_test)\n        \n        # Make prediction and determine accuracy for this fold:\n        knn = KNeighborsClassifier(n_neighbors=20)\n        knn.fit(X_train, y_train['surface'])\n        y_pred = knn.predict(X_test) \n        accuracy = metrics.accuracy_score(y_test['surface'], y_pred)        \n        avg_accuracy += accuracy\n        accuracies.append(accuracy)\n        \n    avg_accuracy = avg_accuracy / k\n    stdev = statistics.stdev(accuracies)\n    return  avg_accuracy, stdev\n\n\navg_accuracy, stdev = k_fold_cross_validation_svm(10, X, y[:,3])\nprint(\"Average 10-fold CV accuracy = {:10.2f}, standard deviation = {:10.2f}\".format(avg_accuracy, stdev))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy is reasonably high in both cases and low variance is achieved, but the random forest classifier seems to achieve an even higher accuracy."},{"metadata":{},"cell_type":"markdown","source":"## Final Prediction using Random Forest"},{"metadata":{},"cell_type":"markdown","source":"In conclusion, the random forest classifier's high accuracy and relatively low variance proves that it is the best model for surface type classification out of the classifiers explored here."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = pd.DataFrame(y_pred_test_tree)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = metrics.accuracy_score(y_test_new['surface'], y_pred_test_forest)\nprint(\"Accuracy = \", accuracy)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}