{"cells":[{"metadata":{"_uuid":"51abbcef133f4539a585ee4bb9dc4c4d75946ade"},"cell_type":"markdown","source":"# My Little EDA with Random Forest on the Top (or at the Bottom)\n\nThis kernel compares some of the overall statistics of robots on different surfaces, builds some features and runs a Random Forest classifier on the results."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef99b75ad054392bf5cf2126b5e0d77eb9e055e8"},"cell_type":"markdown","source":"## Load the Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/X_train.csv\")\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43c8787902c73716dcf7d74f5e5fffc29f25e90c"},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/X_test.csv\")\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06984e736bf375e9352e88a10bbdbecce855902f"},"cell_type":"markdown","source":"## Quick Check on Data"},{"metadata":{"_uuid":"d2062716b4de9fee040170f9086fd5e2b7dcda4b"},"cell_type":"markdown","source":"First, check to see that the data contains all series in order, so first is all series 1 data, followed by 2,3,4, ..."},{"metadata":{"trusted":true,"_uuid":"158a7bbd4ce6e5b96fdcd7b4539497a8d7da050e"},"cell_type":"code","source":"df_train[\"series_id\"].is_monotonic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4d1a0d9e1357bcfa5f2b75600a041b2ca6cc18e"},"cell_type":"code","source":"df_train.groupby(\"series_id\").count().head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53ef000a5dfbbf3a15aa43c0ed76a30020ca6c41"},"cell_type":"markdown","source":"The above says the data is all in sequential order (monotonic) and each series has 128 values as expected."},{"metadata":{"_uuid":"36bd3b3035771cf1f568bd74170121668fca2ccb"},"cell_type":"markdown","source":"## Target Distributions"},{"metadata":{"_uuid":"7b9b6f4893d2dd07112f7408b4c68dc436ab3563"},"cell_type":"markdown","source":"Now to check the target distributions etc.:"},{"metadata":{"trusted":true,"_uuid":"768816bde6e4c63b802360c80c87bffe096d1183"},"cell_type":"code","source":"df_y = pd.read_csv(\"../input/y_train.csv\")\ndf_y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"663709c5347ffca500bbea6d3835a88516136909"},"cell_type":"code","source":"df_y.nunique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3925606a82dc10e11011f84c18ba52d717d19067"},"cell_type":"markdown","source":"This distribution has been discussed and plotted many times already, but is it very uneven:"},{"metadata":{"trusted":true,"_uuid":"2b4b486983261a747961d86066e3b87e619a6e81"},"cell_type":"code","source":"df_y[\"surface\"].value_counts().plot(kind=\"barh\", figsize=(14,8), fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9b792446881187743754c01b1fc4e25b5c3ee1a"},"cell_type":"markdown","source":"## Encode the target (surface values) for training and predictions:"},{"metadata":{"trusted":true,"_uuid":"12393bc0b237007d98f7328b749925ac2141d01b"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# encode class values as integers so they work as targets for the prediction algorithm\nencoder = LabelEncoder()\ny = encoder.fit_transform(df_y[\"surface\"])\ny_count = len(list(encoder.classes_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1e39c10e4e8af111891f71c06488dcb872cd45c"},"cell_type":"markdown","source":"Create a mapping to look up names from numerical id values:"},{"metadata":{"trusted":true,"_uuid":"ed24f6d4e2b98b33bc4c9c25f60c279a39b66511"},"cell_type":"code","source":"label_mapping = {i: l for i, l in enumerate(encoder.classes_)}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b492422a854634abc9baeaaacc2a31c869fb1db1"},"cell_type":"markdown","source":"See how the target (y) values stack against the training data:"},{"metadata":{"trusted":true,"_uuid":"ed7f7757025ddf62e80b3c248797715787c71c9b"},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05926785ee55f3820f8233a37a6f19f890705cae"},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c1117e89722fba37034c50bd520323800800214"},"cell_type":"code","source":"3810*128 #this should match the two values above","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5604861a25f6d88ee20039c77e1028e49a8979d5"},"cell_type":"markdown","source":"Above shows target is per measurement session. To be able to play with the data in more detail, lets map target to each row in training data:"},{"metadata":{"trusted":true,"_uuid":"973e5393ea4d50e25f45bdb79f2779d4f752e7d2"},"cell_type":"code","source":"df_train[\"target\"] = y.repeat(128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01a7a5bd325c9665a12fdd181ff65a9041201e28"},"cell_type":"code","source":"#a look at just a few columns to see target is there\ndf_train[[\"series_id\", \"orientation_X\", \"orientation_Y\", \"orientation_Z\", \"target\"]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d16ab8e76bf0be4f72b385ee6c2ab86e99563a8"},"cell_type":"markdown","source":"## Visualizing the data"},{"metadata":{"_uuid":"596f75a5ee0d7c9db753a0ebafcab7c17eb4a9ba"},"cell_type":"markdown","source":"Plot an example of each surface type robot series / training data to see what the data is like:"},{"metadata":{"trusted":true,"_uuid":"131f6713e4ed9725e6fe8c58af9fe7f5f6f3eeff"},"cell_type":"code","source":"def plot_robot_series(series_id):\n    robot_series_data = df_train[df_train[\"series_id\"] == series_id]\n    orientation_data = robot_series_data[[\"orientation_X\", \"orientation_Y\", \"orientation_Z\"]]\n    angular_data = robot_series_data[[\"angular_velocity_X\", \"angular_velocity_Y\", \"angular_velocity_Z\"]]\n    linear_data = robot_series_data[[\"linear_acceleration_X\", \"linear_acceleration_Y\", \"linear_acceleration_Z\"]]\n    surface = robot_series_data[\"target\"].iloc[0]\n    surface = label_mapping[surface]\n\n    fig, axs = plt.subplots(figsize=(15,3), nrows=1, ncols=3)\n    axs[0].plot(orientation_data)\n    axs[0].set_title(surface+\": orientation XYZ\")\n    axs[0].legend((\"X\", \"Y\", \"Z\"), loc=\"upper left\")\n    axs[1].plot(angular_data)\n    axs[1].set_title(surface+\": angular velocity\")\n    axs[1].legend((\"X\", \"Y\", \"Z\"), loc=\"upper left\")\n    axs[2].plot(linear_data)\n    axs[2].set_title(surface+\": linear acceleration\")\n    axs[2].legend((\"X\", \"Y\", \"Z\"), loc=\"upper left\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"736c34a63d56f9726996adf5cfb8e214a3fb7b55"},"cell_type":"code","source":"for key in label_mapping:\n    rows = df_train[df_train[\"target\"] == key]\n    #find the first row with this surface type\n    row = df_train.index.get_loc(rows.iloc[0].name)\n    sid = df_train.iloc[row][\"series_id\"]\n    plot_robot_series(sid)\n    #print(row)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7ef0565e815ef6648b7b83741a404051612c7b1"},"cell_type":"markdown","source":"There are a few points that catch the eye above:\n\n- The data description does not tell how the robots are moving during the experiment. So are they going straight, in circles, or something in between? The orientation graphs show it seems to be mostly straight but little variation. Maybe just minor noise because the robot is affected by something (maybe surface? or any other of many choices..). There will be more on this later in this kernel.\n\n- Soft tiles and hard tiles on large space show this strange sine curve shape of Z velocity going down. Hard tiles alone does not. These plots are for single values only, so maybe this does not generalize? Will look into this more later.\n"},{"metadata":{"_uuid":"037c82bd61c13af966b62201b6b49199692aa41a"},"cell_type":"markdown","source":"## Look for Outliers\n\nFirst plot some the data for each surface type to see if there are any seemingly outliers to be found:"},{"metadata":{"trusted":true,"_uuid":"2c739cb696e6ef8897e0f3d4a42972d26703f8b3"},"cell_type":"code","source":"grouped = df_train.groupby(\"target\")[\"linear_acceleration_Y\"]\nrowlength = int(grouped.ngroups/3)\nfig, axs = plt.subplots(figsize=(15,15), \n                        nrows=3, ncols=rowlength)\n\ntargets = zip(grouped.groups.keys(), axs.flatten())\nfor i, (key, ax) in enumerate(targets):\n    ax.plot(grouped.get_group(key))\n    ax.set_title('a='+label_mapping[key])\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fb8b0fd83079e59fb3abf1895ee2455148d6c4b"},"cell_type":"markdown","source":"There are some spikes in this data that seem like outliers, so I smooth them all to remove outliers (remove data outside the usual 3\\*STD range):"},{"metadata":{"trusted":true,"_uuid":"f903e18b6492934b08c21b2078ba7872693cec23"},"cell_type":"code","source":"from tqdm import tqdm\n\ndef process_outliers(df, outlier_cols):\n    def fz_raw(x):\n        #TODO: negative outliers?\n        zscore = (x - x.mean())/x.std()\n        z_upper = x.mean()+3*x.std()\n        z_upper = np.repeat(z_upper, 128)\n        z_lower = x.mean()-3*x.std()\n        z_lower = np.repeat(z_lower, 128)\n\n        return np.array((zscore, z_upper, z_lower))\n\n    cols_to_drop = []\n    for col in tqdm(outlier_cols):\n        a = df[col].values\n        \n        series_groups = df.groupby(\"series_id\")\n        xyz = series_groups[col].apply(fz_raw)\n        xyz_c = np.concatenate(xyz, axis=1)\n        \n        z = xyz_c[0]\n        mask = z >= 3\n#        print(sum(mask))\n        a[mask] = xyz_c[1][mask]\n        mask = z <= -3\n#        print(sum(mask))\n        a[mask] = xyz_c[2][mask]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f524d4ab42f07ed8b883e27ad01dd6dd24c41c1"},"cell_type":"markdown","source":"I leave out the orientation as it does not seem to make sense to fix that. I would expect velocity and acceleration to have some bumbs but orientation more smooth. Graphing it from above also gives some confirmation for this, but I skip making this too long. \n\nRemove orientations and target from outlier tuning:"},{"metadata":{"trusted":true,"_uuid":"ed10dbb57793a9767c9030018bf0ea9c37ce3f0b"},"cell_type":"code","source":"outlier_cols = [col_name for col_name in df_train.columns if col_name not in [\"row_id\", \"series_id\", \"measurement_number\"]]\noutlier_cols.remove(\"orientation_X\")\noutlier_cols.remove(\"orientation_Y\")\noutlier_cols.remove(\"orientation_Z\")\noutlier_cols.remove(\"orientation_W\")\noutlier_cols.remove(\"target\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce35e45123a85d694d35961c8c72f739f0ad1fba"},"cell_type":"code","source":"process_outliers(df_train, outlier_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37e404d3ab49ab374ffaa42c8a02e52d12cde1e3"},"cell_type":"code","source":"process_outliers(df_test, outlier_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be2d8951e441f7bb67359e79e29f0825cf36e02e"},"cell_type":"markdown","source":"Now lets see if outlier removal had any effect on the data vs plots from before:"},{"metadata":{"trusted":true,"_uuid":"c3b2ea8417a94bf2f0ab7db21dbae21ae9d9bc64"},"cell_type":"code","source":"grouped = df_train.groupby(\"target\")[\"linear_acceleration_Y\"]\nrowlength = int(grouped.ngroups/3)   # fix up if odd number of groups\nfig, axs = plt.subplots(figsize=(12,12), \n                        nrows=3, ncols=rowlength)\n#                        gridspec_kw=dict(hspace=0.4)) # Much control of gridspec\n\ntargets = zip(grouped.groups.keys(), axs.flatten())\nfor i, (key, ax) in enumerate(targets):\n    ax.plot(grouped.get_group(key))\n    ax.set_title('a='+label_mapping[key])\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cff3879bee7ab7eb089698e85a4c626cb7ef199b"},"cell_type":"markdown","source":"Comparing the plot above to the one before removing the outliers, the biggest spikes have gotten smaller, and each dataset seems a bit more \"balanced\". Not sure if this has effect on the resulting predictions but there you go.. :)"},{"metadata":{"_uuid":"fd92d4e982f1a33a986831835b2bdf9ebe08548d"},"cell_type":"markdown","source":"# Derived Features"},{"metadata":{"trusted":true,"_uuid":"0e2dc5d2092d3c92afbf4488931f0566ab2c02f3"},"cell_type":"markdown","source":"Now create a set of derived features. Many of these are same as in other kernels, others I made myself. Some reasoning for the features I added:\n\n- sum of diffs: If there is more vibration (or similar flakiness) on specific surface, maybe this will reflect that. If the robot has some shakiness on a surface in left/right or up/down directions, but otherwise goes straight on, they should cancel each other out. So the scale of this might also indicate if the robots go straight or not.\n- sum of absolute diffs: total amound of \"vibration\". As opposed to \"sum of diffs\", this is on absolute values so they will not cancel each other out. Maybe the classifier can use relations between the two to figure out real amount of \"vibration\".\n\nBesides these, I use many others collected from other kernels. They seem to be copied to many kernels, so not sure who provided the original code. At least I used https://www.kaggle.com/prashantkikani/help-humanity-by-helping-robots. In any case, thanks to everyone for the base code :)"},{"metadata":{"trusted":true,"_uuid":"2792b09054893230090b4cd48fe634ac6ba7ac1a"},"cell_type":"code","source":"# https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\n\ndef fe(actual):\n    new = pd.DataFrame()\n    actual['total_angular_velocity'] = (actual['angular_velocity_X'] ** 2 + actual['angular_velocity_Y'] ** 2 + actual['angular_velocity_Z'] ** 2) ** 0.5\n    actual['total_linear_acceleration'] = (actual['linear_acceleration_X'] ** 2 + actual['linear_acceleration_Y'] ** 2 + actual['linear_acceleration_Z'] ** 2) ** 0.5\n    \n    actual['acc_vs_vel'] = actual['total_linear_acceleration'] / actual['total_angular_velocity']\n    \n    df = actual\n    xyz = np.vectorize(quaternion_to_euler)(df['orientation_X'], df['orientation_Y'], df['orientation_Z'], df['orientation_W'])\n    actual['euler_x'] = xyz[0]\n    actual['euler_y'] = xyz[1]\n    actual['euler_z'] = xyz[2]\n    \n    actual['total_angle'] = (actual['euler_x'] ** 2 + actual['euler_y'] ** 2 + actual['euler_z'] ** 2) ** 5\n    actual['angle_vs_acc'] = actual['total_angle'] / actual['total_linear_acceleration']\n    actual['angle_vs_vel'] = actual['total_angle'] / actual['total_angular_velocity']\n    \n    series_groups = df.groupby(\"series_id\")\n    \n    def f1(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    def f2(x):\n        return np.mean(np.abs(np.diff(x)))\n    \n    def fx_raw(x):\n        diff = np.diff(x)\n        diff = np.concatenate([[np.nan], diff])\n        abs_diff = np.abs(diff)\n        abs_diff_diff = np.abs(np.diff(abs_diff))\n        abs_diff_diff = np.concatenate([[np.nan], abs_diff_diff])\n        \n        raw_array = np.array((abs_diff, abs_diff_diff, diff))\n        return raw_array\n    \n    def fx_sum(x):\n        abs_max = np.max(np.abs(x))\n        abs_min = np.min(np.abs(x))\n        abs_diff = np.abs(np.diff(x))\n        abs_diff_avg = np.mean(abs_diff)\n        abs_diff_diff = np.abs(np.diff(abs_diff))\n        abs_diff_diff_avg = np.mean(abs_diff_diff)\n        sum_diff = np.sum(np.diff(x))\n        sum_abs_diff = np.sum(abs_diff)\n        count_diffs = np.count_nonzero(x)\n        \n        sum_array = np.array((abs_max, abs_min, abs_diff_avg, abs_diff_diff_avg, \n                              sum_diff, sum_abs_diff, count_diffs))\n        return sum_array\n    \n    for col in tqdm(actual.columns):\n        if col in ['row_id', 'series_id', 'measurement_number', 'target']:\n            continue\n\n        new[col + '_mean'] = series_groups[col].mean()\n        new[col + '_min'] = series_groups[col].min()\n        new[col + '_max'] = series_groups[col].max()\n        new[col + '_std'] = series_groups[col].std()\n        new[col + '_max_to_min'] = new[col + '_max'] / new[col + '_min']\n\n        xyz = series_groups[col].apply(fx_sum)\n        rows = len(series_groups)\n        col_count = len(xyz[0]) # the count returned by fx_sum(x)\n        xyz_sum = np.concatenate(xyz).reshape(rows, col_count)\n        column_names = [col+\"_abs_max\", col+\"_abs_min\", \n                        col+\"_abs_diff_avg\", col+\"_abs_diff_diff_avg\",\n                        col+\"_sum_diff\", col+\"_sum_abs_diff\",\n                        col+\"_count_diff\"\n                       ]\n        df_xyz_sum = pd.DataFrame(xyz_sum, columns=column_names)\n        new = pd.concat([new, df_xyz_sum], axis=1)\n        \n        xyz = series_groups[col].apply(fx_raw)\n        xyz_c = np.concatenate(xyz, axis=1)\n        df_xyz_raw = pd.DataFrame({col+\"_abs_diff\": xyz_c[0],\n                                   col+\"_abs_diff_diff\": xyz_c[1],\n                                   col+\"_diff\": xyz_c[2]\n                                   })\n        #this could become an infinite loop as this loop is over columns in \"actual\" and this adds more columns to it\n        #but this does not happen, so i guess the column set is only read at start of loop\n        actual = pd.concat([actual, df_xyz_raw], axis=1)\n\n    return new, actual","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7fb08dec5f11010772bc2e5059cc19b1ec0b5db"},"cell_type":"markdown","source":"Create two dataframes from the processed data. One with the raw timeseries data, all 128 steps (rows) in each case/per series id. Another with summary data so only one row per series id. The raw data is useful for some analysis and for classifiers such as LSTM. The summary data for summary analysis and classifiers such as Random Forest."},{"metadata":{"trusted":true,"_uuid":"dd838a874e35acb20737dceb8629e0bb8b240e5b"},"cell_type":"code","source":"%%time\ndf_train_sum, df_train = fe(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75289ef152860e26baa4bc6807aae6ff24a65306"},"cell_type":"code","source":"%%time\ndf_test_sum, df_test = fe(df_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b393fde5553e97ccbe3786cbb1aea9d2d07507b"},"cell_type":"markdown","source":"Brief overall look at the summary data:"},{"metadata":{"trusted":true,"_uuid":"791fe9f8f60df2af3df6761c4519bd36f1f0d07f"},"cell_type":"code","source":"df_train_sum.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fdccb9d1b08f6826b49d7b16172dd4a357f152d"},"cell_type":"markdown","source":"The above shows the robot is sometimes changing its orientation, whatever that means. For example, the *orientation_X_sum_diff* variable has a max diff and max abs_diff of same value. Meaning there must be a session where the robot has constantly changed X orientation, and it is not just being \"flaky\" or \"shaky\". Let's see what that is."},{"metadata":{"trusted":true,"_uuid":"f2dcc1de9add3718f7fc3d3111dc4ca7e71ff478"},"cell_type":"code","source":"max_x_id = df_train_sum[\"orientation_X_sum_diff\"].idxmax()\nmax_x_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a35096bb78e862c6588279fa070575a30d549434"},"cell_type":"code","source":"plot_robot_series(1904)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e04391aff65514409f7ccc33b00cfd6768e09225"},"cell_type":"markdown","source":"Shows a linear increase in the X orientation as expected. Whatever that means :). Actually, if looking closely, the Y also shows a similar trend but much smaller. This is again one of those moments where in a real-world case we could just ask the people doing the measurements, consult the domain experts, etc. Oh well."},{"metadata":{"_uuid":"3cd52b560df09283657fb3eb691b0472507873ee"},"cell_type":"markdown","source":"## Correlations:"},{"metadata":{"_uuid":"db6aa18042a80c4e3768640d17aff7572703cb77"},"cell_type":"markdown","source":"Plot the usual correlation matrix plot:"},{"metadata":{"trusted":true,"_uuid":"fd01dcf2ea05db1424bd3f11e68ebef53d058dae"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmatfig = plt.figure(figsize=(12,12))\nplt.matshow(df_train.corr(), fignum=matfig.number)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edd19ec1a92e63d608650b7db780275bc50b97b5"},"cell_type":"markdown","source":"The plot above has too many variables in too small space to really see much. But there do seem to be some spots with high correlations. Let's see what they are:"},{"metadata":{"trusted":true,"_uuid":"4ce74133e64de2e8fce55f2782e42fe4ede2498e"},"cell_type":"code","source":"#https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n\ncorr_matrix = df_train.corr().abs()\n\n#the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\nsol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n                 .stack()\n                 .sort_values(ascending=False))\ntop_df = pd.DataFrame(sol).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"526c63e5750adfffa3e36da9a30819c6937b54b2"},"cell_type":"markdown","source":"See the top 10 highest correlated pairs and correlation:"},{"metadata":{"trusted":true,"_uuid":"12edd2b18142fa6bbba3b62ee1f94d43997847c0"},"cell_type":"code","source":"top_df.columns = [\"var1\", \"var2\", \"corr\"]\ntop_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7994b94cd1e017b0d977f5b9ef8cb3b890e48196"},"cell_type":"markdown","source":"Orientations seem to be highly correlated with each other. But their diffs are less correlated, so shaking in different ways while mostly maintaining course, I guess?"},{"metadata":{"_uuid":"cac3a05a999ba69c38e384f074d728fc4ce544a9"},"cell_type":"markdown","source":"Lets take the correlation values as are (above was absolute values) to see if it is negative or positive correlations:"},{"metadata":{"trusted":true,"_uuid":"a992fe49f5a94f03f754cfb272e68a61d7d390a3"},"cell_type":"code","source":"cor = df_train.corr()\n#just print one to see it works\ncor[\"orientation_X\"][\"orientation_W\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c2c2e308abb9efe156e77ad4f3fb667f18666a0"},"cell_type":"markdown","source":"Do it for all pairs, \"corr\" will be the absolute correlation, \"corr2\" the \"raw\" version:"},{"metadata":{"trusted":true,"_uuid":"ec923ca0036fb0acfed4db07e8c8e546529ce91c"},"cell_type":"code","source":"for x in range(len(top_df)):\n    var1 = top_df.iloc[x][\"var1\"]\n    var2 = top_df.iloc[x][\"var2\"]\n    corr = cor[var1][var2]\n    top_df.at[x, \"corr2\"] = corr\ntop_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f7d0c0cdd06ff1986dfa7943779cf0058389b72"},"cell_type":"markdown","source":"And since we are predicting the target (surface) variable, is anything correlated with that?"},{"metadata":{"trusted":true,"_uuid":"9e3ff929e018a188b76f216b86e6915e1856bdc6"},"cell_type":"code","source":"#https://stackoverflow.com/questions/21137150/format-suppress-scientific-notation-from-python-pandas-aggregation-results\npd.set_option('display.float_format', lambda x: '%.5f' % x)\ntop_df[top_df[\"var2\"] == \"target\"].sort_values(by=\"corr\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01b8e9ae49226a6fac139c6d66ca86ea5416e7d4"},"cell_type":"markdown","source":"Not much. But anyway.."},{"metadata":{"trusted":true,"_uuid":"11377cccb4da76af51f6d5efd9d784abec83dc1f"},"cell_type":"markdown","source":"# Differences between Surface Types"},{"metadata":{"trusted":true,"_uuid":"2bfcf1740064b056fbe634d896945eeea2ddad00"},"cell_type":"markdown","source":"On average, is there any difference between the surface types? Let's see:"},{"metadata":{"trusted":true,"_uuid":"8fb3700eb3b60a79695e4d27d10975248a6590ca"},"cell_type":"code","source":"df_train.groupby(\"target\").mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dbeb085f219a80693f4d5b7fea20238d6bd0090"},"cell_type":"markdown","source":"Target in the index is the surface Id. Overall, the above table is not very friendly, so let's try some visualization:"},{"metadata":{"trusted":true,"_uuid":"da76780bdbec8733b30769c5f9df0be280613796"},"cell_type":"code","source":"mean_group = df_train.groupby(\"target\").mean()\n\ndef scatterplot_variable(var_x, var_y):\n    ax = mean_group.plot.scatter(x=var_x, y=var_y, figsize=(8, 5))\n    for i in mean_group.index:\n        label = label_mapping[i]\n        row = mean_group.loc[i]\n        ax.annotate(label, (row[var_x], row[var_y]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d38873157619be69324b5af29a46b4ec62ff69a3"},"cell_type":"markdown","source":"First some scatterplots so show the average values of each surface type against each other:"},{"metadata":{"trusted":true,"_uuid":"c3e83eb2dbf5cf9ab6a1bddf11ee6abc9ea3b40b"},"cell_type":"code","source":"scatterplot_variable(\"orientation_X\", \"orientation_Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39242ec11bc7f69e87a18eb3b12c1b1a0db69b03"},"cell_type":"code","source":"scatterplot_variable(\"angular_velocity_X\", \"angular_velocity_Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cccdb1c6765b324ec6c6e7696342933d8ce85211"},"cell_type":"code","source":"scatterplot_variable(\"linear_acceleration_X\", \"linear_acceleration_Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e83377801ff9e0682d427181b4b1f32e4bd7db8"},"cell_type":"code","source":"scatterplot_variable(\"total_angular_velocity\", \"total_linear_acceleration\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fffc6c0727d711eed4bb6ae4edaeb672583cddc"},"cell_type":"code","source":"scatterplot_variable(\"euler_x\", \"euler_y\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38b13a0b72937ba729eb909c00e8a5d2412ae302"},"cell_type":"markdown","source":"The plots above seem to indicate maybe the average values of different surfaces have some defining characteristics. But then again, this just sums them all up and averages, so ..."},{"metadata":{"_uuid":"677c35eedb41d621ab3f07d8c5f7b3010552f1ae"},"cell_type":"markdown","source":"Now to average the all time series per surface for the orientation features:"},{"metadata":{"trusted":true,"_uuid":"ca63aef76d6c99973b21e8bc2e16aea1417fab36"},"cell_type":"code","source":"def plot_average_diff(col_name):\n    labels = []\n    rows = []\n    for x in range(9):\n        label = label_mapping[x]\n        labels.append(label)\n        ser = df_train[df_train[\"target\"] == x]\n        #it is called \"ox\" because I started with orientation_X.\n        ox = ser[col_name+\"_diff\"].values.reshape(-1, 128)\n        # this was something I just used to get a bigger scale\n        #ox *= 100\n        omx = ox.mean(0)\n        rows.append(omx)\n    df = pd.DataFrame(rows)\n    df.index = labels\n    df.T.plot(figsize=(14,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"597b4d09d682994072236d6dcdd4d5dceacaca70"},"cell_type":"markdown","source":"The following plots will now show what the \"average\" series for that type would look like:"},{"metadata":{"trusted":true,"_uuid":"e6c73388c3877a0da2f9563cd590b7cacafcc88a"},"cell_type":"code","source":"plot_average_diff(\"orientation_X\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df48ac0cb33e704a83758e9e53c048c41a2ba441"},"cell_type":"code","source":"plot_average_diff(\"orientation_Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"921e2f84a8e1fa1e83842d9f9fb87bcc79ad6db9"},"cell_type":"code","source":"plot_average_diff(\"orientation_Z\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db007c2647d4254801c2f05dd9b44ff59e3d6e6c"},"cell_type":"markdown","source":"So in the above, Orientation X and Y look like there might be someting, orientation Z is a bit of a mess, although some types have higher variation.\n\nRemember, in the beginning I plotted one example graph for each surface type. There two of the tiled ones showed some downward sinus-like curve for angular velocity, so let's see if that holds more generally:"},{"metadata":{"trusted":true,"_uuid":"a4f2a26e53f35498cb0165f3596365b2736443dd"},"cell_type":"code","source":"plot_average_diff(\"angular_velocity_X\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a23c7712852551a2ede5d748c2328320f23e47eb"},"cell_type":"code","source":"plot_average_diff(\"angular_velocity_Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a54ff8f7282f73fa81688bab466bd19f0aa7dd32"},"cell_type":"code","source":"plot_average_diff(\"angular_velocity_Z\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f62beaba1fad31c662a35cb9cc2c5545fbc6e5aa"},"cell_type":"markdown","source":"Well, the soft tiles for angular velocity X do seem a bit funky :). Not that I know what that means."},{"metadata":{"_uuid":"df7fe95717d4f103254c200c92adfa8c9614db44"},"cell_type":"markdown","source":"## Scale and Save the Processed Data:"},{"metadata":{"_uuid":"ce3fec7aebf0ebc60537a62aefe5ac5c59600bc9"},"cell_type":"markdown","source":"First pick the columns to scale:"},{"metadata":{"trusted":true,"_uuid":"bc5580264aad3945339da7e146f480d955cef2e5"},"cell_type":"code","source":"feature_cols = list(df_train.columns)\nfeature_cols.remove('row_id')\nfeature_cols.remove('series_id')\nfeature_cols.remove('measurement_number')\nfeature_cols.remove('target')\n#feature_cols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"371e146276ffd61ca367056dff65a8965cd79a3b"},"cell_type":"markdown","source":"Before scaling anything, save the unscaled versions. This allows using this kernel as datasource for different data formats in other kernels, without repeating all the processing."},{"metadata":{"trusted":true,"_uuid":"6884c2ae4ff17a2cbee4f570df8e67887b9c9011"},"cell_type":"code","source":"df_train.to_csv(\"features_train_raw.csv\")\ndf_test.to_csv(\"features_test_raw.csv\")\ndf_train_sum.to_csv(\"features_train_sum.csv\")\ndf_test_sum.to_csv(\"features_test_sum.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fa4026b54d6433a3aa2fa45a1d061b2ed089c18"},"cell_type":"code","source":"#function to scale two dataframes. fit and transform the first (e.g., training set), \n#and use the same scaler to transform the seconds one (e.g., test set)\ndef scale_df(df1, df2, feature_cols):\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(df1[feature_cols])\n    #df_X_train = pd.DataFrame(scaled_features, index=df_X_train.index, columns=df_X_train.columns)\n    df1[feature_cols] = scaled_features\n    scaled_test_features = scaler.transform(df2[feature_cols])\n    df2[feature_cols] = scaled_test_features\n    return df1, df2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18ac3151483380def408bb1089415d35531ce7b9"},"cell_type":"markdown","source":"Scale the \"raw\" dataframes first."},{"metadata":{"trusted":true,"_uuid":"b9699f6a506804fc16eebce8d779dd73d4be7dae"},"cell_type":"code","source":"df_train, df_test = scale_df(df_train, df_test, feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99c411efec0bb1ba08f9bbe5152ac8131c5cc9c0"},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48c18f85b4c025207feaa652547973694847b9b4"},"cell_type":"markdown","source":"Now scale the \"summary\" dataframe similarly."},{"metadata":{"trusted":true,"_uuid":"6f8287966312191c1d60c5bea1f4afa1aaba7005"},"cell_type":"code","source":"#appears we stripped the columns in this summary dataframe already, so can just use those columns as is\ndf_train_sum, df_test_sum = scale_df(df_train_sum, df_test_sum, df_train_sum.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a90b61ed49e50226184293ecd050def08fae7513"},"cell_type":"markdown","source":"And save the scaled in the same way as the un-scaled before:"},{"metadata":{"trusted":true,"_uuid":"68f0ed41dd2ffee0ab83eea9925aa07cd233d92b"},"cell_type":"code","source":"df_train.to_csv(\"features_train_scaled_raw.csv\")\ndf_test.to_csv(\"features_test_scaled_raw.csv\")\ndf_train_sum.to_csv(\"features_train_scaled_sum.csv\")\ndf_test_sum.to_csv(\"features_test_scaled_sum.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62a16547548e1c9e2ebc134d8c7e7a19471e6c77"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3447c56587ae648b7f2b326711a73a0e6ddb1a5b"},"cell_type":"code","source":"df_features = df_train[feature_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7377d2e076db0d660ffb274a1ab2d947b91c9053"},"cell_type":"markdown","source":"# Run a classifier:"},{"metadata":{"trusted":true,"_uuid":"dd1f9c702861cda055231fb413b1d3fe6b037d90"},"cell_type":"code","source":"df_train_sum.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa2973475ae2e975651d2300fd5fbe3aeccac16a","scrolled":false},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\nimport itertools\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport collections\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=69)\nsub_preds_rf = np.zeros((df_test_sum.shape[0], 9))\noof_preds_rf = np.zeros((df_train_sum.shape[0]))\nscore = 0\nmisclassified_indices = []\nmisclassified_tuples_all = []\nfor i, (train_index, test_index) in enumerate(folds.split(df_train_sum, y)):\n    print('-'*20, i, '-'*20)\n    \n    clf =  RandomForestClassifier(n_estimators = 200, n_jobs = -1)\n    clf.fit(df_train_sum.iloc[train_index], y[train_index])\n    oof_preds_rf[test_index] = clf.predict(df_train_sum.iloc[test_index])\n    sub_preds_rf += clf.predict_proba(df_test_sum) / folds.n_splits\n    score += clf.score(df_train_sum.iloc[test_index], y[test_index])\n    print('score ', clf.score(df_train_sum.iloc[test_index], y[test_index]))\n    importances = clf.feature_importances_\n    features = df_train_sum.columns\n\n    feat_importances = pd.Series(importances, index=features)\n    feat_importances.nlargest(30).sort_values().plot(kind='barh', color='#86bf91', figsize=(10,8))\n    plt.show()\n    \n    missed = y[test_index] != oof_preds_rf[test_index]\n    misclassified_indices.append(test_index[missed])\n    misclassified_samples1 = y[test_index][missed]\n    misclassified_samples2 = oof_preds_rf[test_index][missed].astype(\"int\")\n    m1 = encoder.inverse_transform(misclassified_samples1)\n    m2 = encoder.inverse_transform(misclassified_samples2)\n    misclassified_tuples = [(a, b) for a, b in zip(m1, m2)]\n    misclassified_tuples_all.append(misclassified_tuples)\n\nprint('Avg Accuracy', score / folds.n_splits)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"476cb3b33585d106e2f39c1141ea9310220990e6"},"cell_type":"markdown","source":"The importances visualized above seem a bit odd, why would orientation have such a big importance? Maybe I am missing something, or is there some issue with the visualization?"},{"metadata":{"_uuid":"a50de78491170e55f38f2d51fa713d9ce91aab0c"},"cell_type":"markdown","source":"In any case, write out the submission file:"},{"metadata":{"trusted":true,"_uuid":"457ccbcbe19cdb512d317ad4d87482d819b4dd15"},"cell_type":"code","source":"ss = pd.read_csv('../input/sample_submission.csv')\nss['surface'] = encoder.inverse_transform(sub_preds_rf.argmax(axis=1))\nss.to_csv('rf.csv', index=False)\nss.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d598b8b4e71002bd0209ab56e76e1e70959457af"},"cell_type":"markdown","source":"## Look at the results in a bit more detail"},{"metadata":{"trusted":true,"_uuid":"7ac4a7fa3bc5d847cd2e6cdc1ecc0f37d1fda097"},"cell_type":"markdown","source":"If we wanted to understand the results and how to optimize better, now would be a good time to look at what did the classifier mis-classify in training. Plotting the confusion matrix is always a good start. Again, I pilfered this plotter code from some other kernel that references this other kernel... You get the point. Thanks for the code anyway!"},{"metadata":{"trusted":true,"_uuid":"52aeb9c31c5dc5c83e6331d6656705110835100d"},"cell_type":"code","source":"# https://www.kaggle.com/artgor/where-do-the-robots-drive\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e3de22972a46c8ed1537eaa5adfd6119c4b1286"},"cell_type":"code","source":"plot_confusion_matrix(y, oof_preds_rf, encoder.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"670b3beec27b49a9ea43297f7cda72db19d61437"},"cell_type":"markdown","source":"A good next step could be to collect the mis-labeled rows and look at how they are different from others. The code below collects those rows and looks at some basic stats.\n\nFirst the plain indices and misclassificatin tuples collected during training:"},{"metadata":{"trusted":true,"_uuid":"763d7286114e2b762310dcf238958a52c8e51022"},"cell_type":"code","source":"#misclassified_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7a159f6762f90ba4adf3d7315aae6608ff60f9b"},"cell_type":"code","source":"#misclassified_tuples_all","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ab26fa3566c3d0000424537a4e2fc575873da9e"},"cell_type":"markdown","source":"The actual feature rows they are from:"},{"metadata":{"trusted":true,"_uuid":"45925145ce310ac69a58aa10978a8ccca73abd38"},"cell_type":"code","source":"df_train_sum.iloc[misclassified_indices[0]].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f53ae7adbd61e48f5e37ad948cd6ac37f69295fa"},"cell_type":"markdown","source":"Now to count summary statistics for all the training misses:"},{"metadata":{"trusted":true,"_uuid":"3e61d77c510a0338006dfb599eff1adfcd7436ff"},"cell_type":"code","source":"df_missed_all = None\ntotal_counter = collections.Counter()\n\ndef counter_to_df(df_missed_all, counter, new_col_name):\n    sorted_freqs = counter.most_common()\n    #single item in sorted_freqs is like this: ('fine_concrete', 'wood'), 7)\n    rows = [[row[0][0], row[0][1], row[1]] for row in sorted_freqs]\n    df_missed = pd.DataFrame(rows, columns=[\"expected\", \"actual\", \"count\"])\n    if df_missed_all is None:\n        df_missed_all = df_missed\n    else:\n        df_missed_all = df_missed_all.merge(df_missed, how='left', on=[\"expected\", \"actual\"])\n    created_col = df_missed_all.columns[-1]\n    print(\"renaming: \"+new_col_name)\n    df_missed_all.rename(columns={created_col: new_col_name}, inplace=True)\n    df_missed_all[new_col_name].fillna(0, inplace=True)\n    df_missed_all[new_col_name] = df_missed_all[new_col_name].astype(int)\n    return df_missed_all\n\ni = 1\nfor misclassified_tuples in misclassified_tuples_all:\n    counter = collections.Counter(misclassified_tuples)\n    total_counter.update(misclassified_tuples)\n    df_missed_all = counter_to_df(df_missed_all, counter, \"count\"+str(i))\n    i += 1\n\ndf_missed_all = counter_to_df(df_missed_all, total_counter, \"all\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ce28125d6eeb6106135f325e65c4aa6b26221d2"},"cell_type":"code","source":"#df_missed_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"406dfe677f08cbe794bd70edcbd9efa5c1c5ce5b"},"cell_type":"code","source":"new_order = [0, 1, 7, 2, 3, 4, 5, 6]\ncolumns = [df_missed_all.columns[i] for i in new_order]\ndf_missed_all = df_missed_all[columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f261fb2ee9af5a9775bca493992267aa9a37ec1a"},"cell_type":"code","source":"df_missed_all.sort_values(by=\"all\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"644aa28dd4e38ab4afb877be17bd208261e441dc"},"cell_type":"markdown","source":"And how many per surface type did we get right and wrong?"},{"metadata":{"trusted":true,"_uuid":"0a686d57b3074fdbd5e9458616c7247e2acfb378"},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"297adda22a139c506fafd5bfc927ff0a6bcebd18"},"cell_type":"code","source":"miss_map = {}\nhit_map = {}\nids, counts = np.unique(y, return_counts=True)\ntotals = dict(zip(ids, counts))\ntotal_misses = 0\ntotal_hits = 0\n\nfor label_id in label_mapping.keys():\n    label_name = label_mapping[label_id]\n    misses = df_missed_all[df_missed_all[\"expected\"] == label_name][\"all\"].sum()\n    total_misses += misses\n    miss_map[label_name] = misses\n    total = totals[label_id]\n    hits = total - misses\n    total_hits += hits\n    hit_map[label_name] = hits\nprint(miss_map)\nprint(hit_map)\nprint(total_misses)\nprint(total_hits)\nprint(total_misses+total_hits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"723b700d07af2507ba616bd9d7fb330a9bd16368"},"cell_type":"code","source":"df_hit_miss = pd.DataFrame.from_dict(hit_map, orient=\"index\")\ndf_miss = pd.DataFrame.from_dict(miss_map, orient=\"index\")\ndf_hit_miss = df_hit_miss.join(df_miss, how='outer', lsuffix='_left', rsuffix='_right')\ndf_hit_miss.columns = [\"hits\", \"misses\"]\ndf_hit_miss.sort_values(by=\"hits\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91e5b01dc5d29919f0166aa5779ceece3acd674d"},"cell_type":"markdown","source":"Concrete and wood at least stand out in the above for further exploration."},{"metadata":{"_uuid":"775d6acfc9555dcd69492c0517376ced47d84fcf"},"cell_type":"markdown","source":"The above collected the misclassified labels and their statistics. In a similar way, the set of most commonly misclassified rows could be collected. I tried this at first but realized since the rows are collected by running a cross-validation strategy, each prediction is for different set of rows. So this will never show the same row multiple times. \n\nWould need to run the classifier multiple times to get better statistics on what rows might be commonly mis-classified. Since RF runs very fast for this small dataset, I see no problem. But this kernel is long enough as it is. Maybe another time.."},{"metadata":{"trusted":true,"_uuid":"9c076cab5353bbbc9db583f82b557cdcebf0e519"},"cell_type":"code","source":"idx_counter = collections.Counter()\nfor missed_idx in misclassified_indices:\n    idx_counter.update(missed_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d56a34273982ab1a73ed15632f35ba38d78339a7"},"cell_type":"code","source":"len(idx_counter.most_common())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e186142ccfe2c4dd14c0960d97495310e9a65d1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}