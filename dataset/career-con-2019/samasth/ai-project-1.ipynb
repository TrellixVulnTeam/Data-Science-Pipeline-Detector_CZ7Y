{"cells":[{"metadata":{},"cell_type":"markdown","source":"name = Samsth Norway Ananda\nnet-id = sna354"},{"metadata":{},"cell_type":"markdown","source":"In this notebook we will help robots recognize the floor surface theyâ€™re standing on using data collected from Inertial Measurement Units (IMU sensors).\nWe are given dataset which is collected by IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. We have to predict the floor surface on which the robot is moving so that we can improve the efficiency of movement of the robot."},{"metadata":{},"cell_type":"markdown","source":"Loading the packages and displaying the given datasets"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/career-con-2019/X_train.csv')\ntarget = pd.read_csv('../input/career-con-2019/y_train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the frequency of group id's. We can notice that some group id's have very less frequency compared to others."},{"metadata":{"trusted":true},"cell_type":"code","source":"from seaborn import countplot\nplt.figure(figsize=(23,5)) \nsns.set(style=\"white\")\ncountplot(x=\"group_id\", data=target, order = target['group_id'].value_counts().index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert the Y values from labels to integer values so that we can train our model on it. For this we use sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n%matplotlib inline\nle = preprocessing.LabelEncoder()\n\ntarget['surface'] = le.fit_transform(target['surface'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split our dataset into train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tra = pd.DataFrame(data[:485120][:])\nX_tes = pd.DataFrame(data[485120:][:])\nY_tra = pd.DataFrame(target[:3790]['surface'])\nY_tes = pd.DataFrame(target[3790:]['surface'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Exploration**\nLet's print see the dataset given to check how are the values for each attribute"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tra.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tra.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Data Preprocessing**"},{"metadata":{},"cell_type":"markdown","source":"We need to classify on which surface the robot is standing on. It is a multiclass classification problem where each class is a floor surface and we have 9 differenrt types of floor surfaces."},{"metadata":{},"cell_type":"markdown","source":"**Converting**\nbefore we dive into building a model we need to have the right data to train the model on. So we start making changes to the given dataset so that it becomes feasible to train on our model"},{"metadata":{},"cell_type":"markdown","source":"The orientation values show the current angles how the robot is oriented as a quaternion. Angular velocity describes the angle and speed of motion, and linear acceleration values show how the speed is changing at different times.\nBelow we convert the quaternion angles to teh euler form"},{"metadata":{"trusted":true},"cell_type":"code","source":"def quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_step0 (actual):\n    \n    actual['norm_quat'] = (actual['orientation_X']**2 + actual['orientation_Y']**2 + actual['orientation_Z']**2 + actual['orientation_W']**2)\n    actual['mod_quat'] = (actual['norm_quat'])**0.5\n    actual['norm_X'] = actual['orientation_X'] / actual['mod_quat']\n    actual['norm_Y'] = actual['orientation_Y'] / actual['mod_quat']\n    actual['norm_Z'] = actual['orientation_Z'] / actual['mod_quat']\n    actual['norm_W'] = actual['orientation_W'] / actual['mod_quat']\n    \n    return actual","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each series we have 128 measurements. So we club the 128 measurements to get a single value so that the dimension of X and Y become same."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tra = fe_step0(X_tra)\nX_tes = fe_step0(X_tes)\nprint(X_tra.shape)\nX_tra.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We compare the norm values of the test and train dataset. as their sizes vary a lot they have less similarity in norm values."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(18, 5))\n\nax1.set_title('quaternion X')\nsns.kdeplot(X_tra['norm_X'], ax=ax1, label=\"train\")\nsns.kdeplot(X_tes['norm_X'], ax=ax1, label=\"test\")\n\nax2.set_title('quaternion Y')\nsns.kdeplot(X_tra['norm_Y'], ax=ax2, label=\"train\")\nsns.kdeplot(X_tes['norm_Y'], ax=ax2, label=\"test\")\n\nax3.set_title('quaternion Z')\nsns.kdeplot(X_tra['norm_Z'], ax=ax3, label=\"train\")\nsns.kdeplot(X_tes['norm_Z'], ax=ax3, label=\"test\")\n\nax4.set_title('quaternion W')\nsns.kdeplot(X_tra['norm_W'], ax=ax4, label=\"train\")\nsns.kdeplot(X_tes['norm_W'], ax=ax4, label=\"test\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting Quaternion to Euler angles. The orientation Z also matters as the robot will be moving on different surfaces and can have changes in the z direction based on the surface it is walking on."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_step1 (actual):\n    \"\"\"Quaternions to Euler Angles\"\"\"\n    \n    x, y, z, w = actual['norm_X'].tolist(), actual['norm_Y'].tolist(), actual['norm_Z'].tolist(), actual['norm_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    actual['euler_x'] = nx\n    actual['euler_y'] = ny\n    actual['euler_z'] = nz\n    return actual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tra = fe_step1(X_tra)\nX_tes = fe_step1(X_tes)\nprint (X_tra.shape)\nX_tra.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(15, 5))\n\nax1.set_title('Roll')\nsns.kdeplot(X_tra['euler_x'], ax=ax1, label=\"train\")\nsns.kdeplot(X_tes['euler_x'], ax=ax1, label=\"test\")\n\nax2.set_title('Pitch')\nsns.kdeplot(X_tra['euler_y'], ax=ax2, label=\"train\")\nsns.kdeplot(X_tes['euler_y'], ax=ax2, label=\"test\")\n\nax3.set_title('Yaw')\nsns.kdeplot(X_tra['euler_z'], ax=ax3, label=\"train\")\nsns.kdeplot(X_tes['euler_z'], ax=ax3, label=\"test\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tra.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feat_eng(data):\n    \n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2)** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2)**0.5\n    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2)**0.5\n    data['acc_vs_vel'] = data['totl_linr_acc'] / data['totl_anglr_vel']\n    \n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] / df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_mean_change_of_abs_change'] = data.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])/2\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_tra = feat_eng(X_tra)\nX_tes = feat_eng(X_tes)\n#print (\"New features: \",data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_t = X_tra[:3600]\nX_v = X_tra[3600:]\nY_t = Y_tra[:3600]\nY_v = Y_tra[3600:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"**3. Selection of an algorithm**"},{"metadata":{},"cell_type":"markdown","source":"In this step we are going to check for the possible ML models we can use for this problem and pick the one which suites the best.\nAt first lets try with the Radom Forest algorithm we use the stratified K fold validation stratigy"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=59)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = np.zeros((X_tes.shape[0],9))\nmeasured= np.zeros((X_tra.shape[0]))\nscore = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.ensemble import RandomForestClassifier\nimport gc\n\nfor times, (trn_idx, val_idx) in enumerate(folds.split(X_tra.values,Y_tra['surface'].values)):\n    model = RandomForestClassifier(n_estimators=500, n_jobs = -1)\n    #print(trn_idx)\n    #print(val_idx)\n    #print(times)\n    #model = RandomForestClassifier(n_estimators=500, max_depth=10, min_samples_split=5, n_jobs=-1)\n    model.fit(X_tra.iloc[trn_idx],Y_tra['surface'][trn_idx])\n    measured[val_idx] = model.predict(X_tra.iloc[val_idx])\n    predicted += model.predict_proba(X_tes)/folds.n_splits\n    #print(predicted.shape)\n    score += model.score(X_tra.iloc[val_idx],Y_tra['surface'][val_idx])\n    print(\"Fold: {} score: {}\".format(times,model.score(X_tra.iloc[val_idx],Y_tra['surface'][val_idx])))\n\n    importances = model.feature_importances_\n    #print(len(importances))\n    indices = np.argsort(importances)\n    #print(indices)\n    features = X_tra.columns\n    #print(features)\n    #print(len(features))\n    \n    if model.score(X_tra.iloc[val_idx],Y_tra['surface'][val_idx]) > 0.92000:\n        hm = 30\n        plt.figure(figsize=(7, 10))\n        plt.title('Feature Importances')\n        plt.barh(range(len(indices[:hm])), importances[indices][:hm], color='b', align='center')\n        plt.yticks(range(len(indices[:hm])), [features[i] for i in indices])\n        plt.xlabel('Relative Importance')\n        plt.show()\n\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/career-con-2019/sample_submission.csv')\nsub = sub[3796:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['surface'] = predicted.argmax(axis=1)\nsub = pd.DataFrame(sub['surface'])\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets calculate the accuracy of the Random forest algorithm by comparing the True y values and the predicted y values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ndi = classification_report(Y_tes, sub, output_dict=True)\nprint(di['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get a good accuracy of around 90% for the Random forest algorithm."},{"metadata":{},"cell_type":"markdown","source":"Now let's try with a different algorithm. I have used Pytorch to implement a deep neural network. Even though deep neural networks require large amount of data to train the model, I have tried to fit a model wih the given data by keeping the number of hidden layers less and less neurons in each neuron. This helps me in achieving a good accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torchvision import datasets\nimport torchvision.transforms as transforms","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am splitting my data into train, validation and test. I am using validation to check if my model is overfitting as the number of test data is very less."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = torch.from_numpy(X_t.values)\ny = torch.from_numpy(Y_t.values)\nX_va = torch.from_numpy(X_v.values)\nY_va = torch.from_numpy(Y_v.values)\nX_te = torch.from_numpy(X_tes.values)\ny_te = torch.from_numpy(Y_tes.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport sklearn.datasets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess the datasets to be ready to train for the neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler() # normalization, x-mean/std\nX=sc.fit_transform(X) # apply to data\nX=torch.tensor(X) # convert numpy to torch tensor\n#y=torch.tensor(y).unsqueeze(1) # add extra dim (768,) to (768,1)\ny=torch.tensor(y) # add extra dim (768,) to (768,1)\n\nX_va = sc.fit_transform(X_va) # apply to data\nX_va=torch.tensor(X_va) # convert numpy to torch tensor\n#y=torch.tensor(y).unsqueeze(1) # add extra dim (768,) to (768,1)\nY_va=torch.tensor(Y_va) # add extra dim (768,) to (768,1)\n\nX_te = sc.fit_transform(X_te)\nX_te = torch.tensor(X_te) # convert numpy to torch tensor\n#y_te = torch.tensor(y_te).unsqueeze(1) # add extra dim (768,) to (768,1)\ny_te = torch.tensor(y_te) # add extra dim (768,) to (768,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape,X.dtype)\nprint(y.shape,y.dtype)\nprint(X_va.shape,X_va.dtype)\nprint(Y_va.shape,Y_va.dtype)\nprint(X_te.shape,X_te.dtype)\nprint(y_te.shape,y_te.dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.autograd import Variable\nX=Variable(X) #Variable initialization\ny=Variable(y)\n\nX_va = Variable(X_va)\nY_va = Variable(Y_va)\n\nX_te = Variable(X_te)\ny_te = Variable(y_te)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\nclass Dataset(Dataset):\n  def __init__(self,x,y):\n    self.x=x\n    self.y=y\n  \n  def __getitem__(self,index):\n    return self.x[index], self.y[index]\n  \n  def __len__(self):\n    return len(self.x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=Dataset(X,y)\nprint(dataset.x.shape)\nprint(dataset.y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a data loader to train the model in batches of 50 datasets each time"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader= torch.utils.data.DataLoader(dataset=dataset, batch_size=50, shuffle=True) # load data\n#test_loader= torch.utils.data.DataLoader(dataset=dataset_test, shuffle=True) # load data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The nural network has 3 hiddden layers with dimensions of 276, 300, 150 respectively. I am using batch normalisation to normalise the data after each layer. I have used softmax at the output layer to get all the values in 0-1 range. I have used sigmoid as the activation fuction for 1st layer and relu for the 2nd layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n  def __init__(self):\n    super(Model,self).__init__()\n    self.fc1 =torch.nn.Linear(X.shape[1],300)\n    self.relu=nn.ReLU() \n    self.batchnorm1 = nn.BatchNorm1d(300)\n    self.sigmoid=torch.nn.Sigmoid()\n    self.fc2 =torch.nn.Linear(300,150) \n    self.batchnorm2 = nn.BatchNorm1d(150)\n    self.fc3 =torch.nn.Linear(150,9)\n    #self.batchnorm3 = nn.BatchNorm1d(100)\n    #self.fc4 =torch.nn.Linear(100,9)\n    #self.sigmoid=torch.nn.Sigmoid()\n    self.softmax = nn.Softmax()\n\n    \n  def forward(self,x):\n    out =self.fc1(x)\n    out = self.batchnorm1(out)\n    out =self.sigmoid(out)\n    #out = self.relu(out)\n    out =self.fc2(out)\n    out = self.batchnorm2(out)\n    #out =self.sigmoid(out)\n    out = self.relu(out)\n    out =self.fc3(out)\n    #out = self.batchnorm3(out)\n    #out = self.relu(out)\n    #out =self.sigmoid(out)\n    #out =self.fc4(out)\n    #out= self.sigmoid(out)\n    out = self.softmax(out)\n    return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we are working on a classification problem like this it is best to use a cross entrophy loss as it punishes the wrong outputs more based on their intensity. I have used Adam optimiser for my backpropagation. After trying a learning rate value from 0.1 to 0.001 I felt 0.1 was a more appropriate for this model. I  tried using Mean Squared error but it is not suggested as MSE is meant for regression problems"},{"metadata":{"trusted":true},"cell_type":"code","source":"net = Model()\n#criterion =torch.nn.MSELoss(size_average=True)\ncriterion = torch.nn.CrossEntropyLoss(size_average=True)\noptimizer =torch.optim.Adam(net.parameters(), lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am training the model for 200 epochs, We can't get the exact same accuracy each time we train the model. I have got my accuracy to this model structure from 70% to 90% for different values of the hyper parameters and fine tuning. I have commented some of the different structures I have tried for the model above."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nnum_epochs = 200\nfor epoch in range(num_epochs):\n  data = 0\n  for inputs, labels in train_loader:\n    inputs = Variable(inputs.float())\n    labels= Variable(labels.float())\n    output = net(inputs)\n    #print(torch.round(output))\n    #print(output.argmax(1))\n    optimizer.zero_grad()\n    #print(output.argmax(1))\n    #print(labels.squeeze())\n    loss = criterion(output,labels.squeeze().long())\n    #output = (output>0.5).float()\n    #data += (output == labels).float().sum()\n    data += classification_report(labels, output.argmax(1),output_dict=True)['accuracy']\n    #data += (output.argmax(1) == labels.argmax(1)).float().sum()\n    # loss = -(labels * torch.log(output)+ (1-labels) * torch.log(1-output)).mean()\n    loss.backward()\n    optimizer.step()\n    #print(output - labels)\n  print(data/76)\n  #output = (output>0.5).float()\n  #print(output)\n  #print(labels)\n  #print(classification_report(labels, output))\n  #data += (output == labels).float().sum()\n  #print(correct)\n  #print(\"Epoch {}/{}, loss:{:.3f}, Accuracy: {:.3f}\".format(epoch+1,num_epochs,loss.item(),data/2800*9))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am calculating the validation accuracy here to make sure that my model is not over fitting. for one structure my model was over fitting with around 99% accuracy during train and just arounf 80% accuracy in test. So having a validation test is important in finetuning the model and choosing the best hyper parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Variable(X_va.float())\nlabels= Variable(Y_va.float())\noutput = net(inputs)\npre_d = output.argmax(1).unsqueeze(1)\ny_tru = labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(output)\ndi = classification_report(y_tru, pre_d, output_dict=True)\nprint(di['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing the model. i have tested the model on 20 dataset values, it is very less to judge a neural network model. but as I have validated the model have a small test dataset doesn't harm much."},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Variable(X_te.float())\nlabels= Variable(y_te.float())\noutput = net(inputs)\npre_d = output.argmax(1).unsqueeze(1)\ny_tru = labels\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(output)\ndi = classification_report(y_tru, pre_d, output_dict=True)\nprint(di['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So by comparision between the Random forest algorithm and Neural network. for this specific dataset and problem random forest seems to do better with around 90% accuracy. It is not that neural netwroks have to always perform better over other algorithms. The type of data and the quantity if data also matters. Probably by more tuning the model and trying different methods the neural network might do better, but for now it is not so good."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}