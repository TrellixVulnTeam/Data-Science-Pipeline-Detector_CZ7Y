{"cells":[{"metadata":{},"cell_type":"markdown","source":"**This solution was born from those elite scientists's works.**\nPlease accept my deepest aplologies if I missed someone.\n[Markus F](https://www.kaggle.com/friedchips), \n[Thomas Rohwer](https://www.kaggle.com/trohwer64), \n[Nanashi](https://www.kaggle.com/jesucristo) \n[The Missing Link](https://www.kaggle.com/friedchips/the-missing-link)\n[\"The Orientation Sensor\" or \"Science vs. Alchemy\" discussion](https://www.kaggle.com/c/career-con-2019/discussion/87239#latest-512162)\n[Smart Robots. Complete Notebook](https://www.kaggle.com/jesucristo/1-smart-robots-complete-notebook-0-73).\n\n\nI'm a true beginner in machine learning, and I'm a self-learner.\nI want to learn, I need to learn, and I really want a job, this competition is 'fit' for me.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom time import time\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib import rcParams\n# %matplotlib inline\nle = preprocessing.LabelEncoder()\nfrom numba import jit\nimport itertools\nfrom seaborn import countplot,lineplot, barplot\nfrom numba import jit\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import kurtosis, skew\n\nimport matplotlib.style as style\nstyle.use('ggplot')\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\ngc.enable()","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Markus F](https://www.kaggle.com/friedchips)'s work \n[The Missing Link](https://www.kaggle.com/friedchips/the-missing-link) show us there are links between train and test data.\nSame run by robot was recored and splited into train/test.\nBy calculate the squared euclidean distance between two samples, we can find series that link to each other."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#\ndef sq_dist(a, b):\n    ''' the squared euclidean distance between two samples '''\n\n    return np.sum((a - b) ** 2, axis=1)\n\n\ndef find_run_edges(data, edge):\n    ''' examine links between samples. left/right run edges are those samples which do not have a link on that side. '''\n\n    if edge == 'left':\n        border1 = 0\n        border2 = -1\n    elif edge == 'right':\n        border1 = -1\n        border2 = 0\n    else:\n        return False\n\n    edge_list = []\n    linked_list = []\n\n    for i in range(len(data)):\n        dist_list = sq_dist(data[i, border1, :4], data[:, border2, :4])  # distances to rest of samples\n        min_dist = np.min(dist_list)\n        closest_i = np.argmin(dist_list)  # this is i's closest neighbor\n        if closest_i == i:  # this might happen and it's definitely wrong\n            print('Sample', i, 'linked with itself. Next closest sample used instead.')\n            closest_i = np.argsort(dist_list)[1]\n        dist_list = sq_dist(data[closest_i, border2, :4], data[:, border1, :4])  # now find closest_i's closest neighbor\n        rev_dist = np.min(dist_list)\n        closest_rev = np.argmin(dist_list)  # here it is\n        if closest_rev == closest_i:  # again a check\n            print('Sample', i, '(back-)linked with itself. Next closest sample used instead.')\n            closest_rev = np.argsort(dist_list)[1]\n        if (i != closest_rev):  # we found an edge\n            edge_list.append(i)\n        else:\n            linked_list.append([i, closest_i, min_dist])\n\n    return edge_list, linked_list\n\n\ndef find_runs(data, left_edges, right_edges):\n    import time\n    ''' go through the list of samples & link the closest neighbors into a single run '''\n\n    data_runs = []\n\n    for start_point in left_edges:\n        i = start_point\n        run_list = [i]\n        while i not in right_edges:\n            tmp = np.argmin(sq_dist(data[i, -1, :4], data[:, 0, :4]))\n            if tmp == i:  # self-linked sample\n                tmp = np.argsort(sq_dist(data[i, -1, :4], data[:, 0, :4]))[1]\n            i = tmp\n            run_list.append(i)\n        data_runs.append(np.array(run_list))\n\n    return data_runs","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we load the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = pd.read_csv('../input/X_train.csv')\ntest_X  = pd.read_csv('../input/X_test.csv' )","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test_X's series_id shoud be considered as continuous number."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_X['series_id'].tail()) # train_X series_id stop in 3809\ntest_X['series_id'] = test_X['series_id'] + 3810 # so test_X's series_id should start from 3810","execution_count":4,"outputs":[{"output_type":"stream","text":"487675    3809\n487676    3809\n487677    3809\n487678    3809\n487679    3809\nName: series_id, dtype: int64\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"combine train and test data, and check how many series_id in total"},{"metadata":{"trusted":true},"cell_type":"code","source":"_total = pd.concat([train_X, test_X], axis=0).reset_index(drop=True)\nprint('total series' , len(_total['series_id'].unique()))\ntotal = _total.iloc[:,3:].values.reshape(-1,128,10)","execution_count":5,"outputs":[{"output_type":"stream","text":"total series 7626\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"we have 7626 series in total, Now we check how many runs we can find, and how many series we used."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_left_edges, all_left_link = find_run_edges(total, edge='left')\nall_right_edges, all_right_link = find_run_edges(total, edge='right')\nprint('Found', len(all_left_edges), 'left edges and', len(all_right_edges), 'right edges.')\n\nall_runs = find_runs(total, all_left_edges, all_right_edges)\n\nflat_list = [series_id for run in all_runs for series_id in run]\nprint(len(flat_list), len(np.unique(flat_list)))","execution_count":6,"outputs":[{"output_type":"stream","text":"Sample 1 (back-)linked with itself. Next closest sample used instead.\nSample 216 linked with itself. Next closest sample used instead.\nSample 748 (back-)linked with itself. Next closest sample used instead.\nSample 799 linked with itself. Next closest sample used instead.\nSample 1913 linked with itself. Next closest sample used instead.\nSample 2555 linked with itself. Next closest sample used instead.\nSample 2612 linked with itself. Next closest sample used instead.\nSample 2917 linked with itself. Next closest sample used instead.\nSample 3312 linked with itself. Next closest sample used instead.\nSample 4165 linked with itself. Next closest sample used instead.\nSample 5212 linked with itself. Next closest sample used instead.\nSample 5946 (back-)linked with itself. Next closest sample used instead.\nSample 6305 (back-)linked with itself. Next closest sample used instead.\nSample 181 linked with itself. Next closest sample used instead.\nSample 464 linked with itself. Next closest sample used instead.\nSample 851 linked with itself. Next closest sample used instead.\nSample 1334 linked with itself. Next closest sample used instead.\nSample 1679 linked with itself. Next closest sample used instead.\nSample 1730 linked with itself. Next closest sample used instead.\nSample 2720 linked with itself. Next closest sample used instead.\nSample 3005 linked with itself. Next closest sample used instead.\nSample 4390 linked with itself. Next closest sample used instead.\nSample 5026 linked with itself. Next closest sample used instead.\nSample 5219 linked with itself. Next closest sample used instead.\nSample 5480 linked with itself. Next closest sample used instead.\nSample 5510 linked with itself. Next closest sample used instead.\nSample 6674 linked with itself. Next closest sample used instead.\nSample 7142 linked with itself. Next closest sample used instead.\nSample 7207 linked with itself. Next closest sample used instead.\nFound 151 left edges and 151 right edges.\n7600 7600\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"we lost 7626-7600=26 series, we find it, and make it into one run, then append it to all_runs"},{"metadata":{"trusted":true},"cell_type":"code","source":"lost_samples = np.array([ i for i in range(len(total)) if i not in np.concatenate(all_runs) ])\nprint(lost_samples)\nprint(len(lost_samples))\n\nlost_run = np.array(lost_samples[find_runs(total[lost_samples], [0], [5])[0]])\nall_runs.append(lost_run)","execution_count":7,"outputs":[{"output_type":"stream","text":"[4074 4171 4339 4430 4543 4764 4958 5058 5242 5344 5380 5548 5549 5900\n 6015 6524 6657 6788 6801 6925 6983 6993 7005 7169 7327 7465]\n26\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"now we name a Dataframe 'final',it shows run_id/run_pos by series_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"final = pd.DataFrame(index=_total['series_id'].unique())\nfor run_id in range(len(all_runs)):\n    for run_pos in range(len(all_runs[run_id])):\n        series_id = all_runs[run_id][run_pos]\n        final.at[ series_id, 'run_id'  ] = run_id\n        final.at[ series_id, 'run_pos' ] = run_pos\n\ntrain_y = pd.read_csv('../input/y_train.csv')\nfinal['surface'] = train_y['surface']","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is what I did wrong, each run may contain more than one surface type.\nBut here I made same run equals same surface.\nFrankly, I do not know what to do with same run with different surface."},{"metadata":{"trusted":true},"cell_type":"code","source":"for id, surface in zip(final[final['run_id'].notnull()]['run_id'], final[final['surface'].notnull()]['surface']):\n    final.loc[final['run_id']==id, 'surface'] = surface\n\n\nprint(final['run_id'].unique())\nprint(final[final['surface'].isnull()]['run_id'].unique())","execution_count":9,"outputs":[{"output_type":"stream","text":"[101.  24.   5.  87. 148.  18.   6.  11. 141.  96. 136.  22.  21.   0.\n 118.  19.   7.   8.  12.   1.  23.   2.  15.  14.  84.  20.  13.  97.\n   4.   3.  17.  16.   9.  10.  72.  76.  35.  63.  54.  62.  38.  99.\n  26.  28.  31.  53.  33.  90.  30.  36.  50.  74. 108.  56.  40.  32.\n  45.  77.  25. 146.  86.  65.  89.  41. 147. 131. 129.  43.  44.  47.\n  46.  88.  55.  64.  52.  27.  61.  69. 144.  83.  51.  73.  70.  42.\n  29.  58. 140. 128.  34.  48.  39.  59.  66.  37.  78. 132.  82.  68.\n  49.  57.  71.  60.  75.  67. 102.  80. 142.  79. 127. 107. 133. 119.\n  85.  92. 116.  93. 111. 138. 105. 134. 139.  81. 135. 145. 143. 104.\n  94. 125. 103. 151. 124. 114. 126. 110. 121.  91. 149.  95. 117. 150.\n 112.  98. 115. 100. 106. 109. 113. 120. 122. 123. 130. 137.]\n[102.  80. 142.  79. 127. 107. 133. 119.  85.  92. 116.  93. 111. 138.\n 105. 134. 139.  81. 135. 145. 143. 104.  94. 125. 103. 151. 124. 114.\n 126. 110. 121.  91. 149.  95. 117. 150. 112.  98. 115. 100. 106. 109.\n 113. 120. 122. 123. 130. 137.]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Now I have new train/test target"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_target = final[final['surface'].notnull()]\nnew_train_series = final[final['surface'].notnull()].index\nnew_test_series = final[final['surface'].isnull()]['run_id'].index\nnew_train = _total[_total['series_id'].isin(new_train_series)]\nnew_test = _total[_total['series_id'].isin(new_test_series)]","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we do some feature egineering.\n[\"The Orientation Sensor\" or \"Science vs. Alchemy\" discussion](https://www.kaggle.com/c/career-con-2019/discussion/87239#latest-512162)\n[Smart Robots. Complete Notebook](https://www.kaggle.com/jesucristo/1-smart-robots-complete-notebook-0-73)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef feat_eng(data):\n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X'] ** 2 + data['angular_velocity_Y'] ** 2 + data[\n        'angular_velocity_Z'] ** 2) ** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X'] ** 2 + data['linear_acceleration_Y'] ** 2 + data[\n        'linear_acceleration_Z'] ** 2) ** 0.5\n    data['acc_vs_vel'] = data['totl_linr_acc'] / data['totl_anglr_vel']\n\n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n\n    for col in data.columns:\n\n        if col in ['row_id', 'series_id', 'measurement_number',\n                   'orientation_X', 'orientation_Y', 'orientation_Z',\n                   'run_id', 'orientation_W']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] / df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_mean_change_of_abs_change'] = data.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max']) / 2\n\n\n        df[col + '_kurtosis'] = data.groupby(['series_id'])[col].apply(lambda x:kurtosis(x))\n\n        df[col + '_skew'] = data.groupby(['series_id'])[col].skew()\n\n    return df","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are leakage in orientation's data.So I'm not using it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = feat_eng(new_train).reset_index(drop=True)\ntest = feat_eng(new_test).reset_index(drop=True)\n\ndata = data.fillna(0)\ntest = test.fillna(0)\ndata = data.replace(-np.inf,0)\ndata = data.replace(np.inf,0)\ntest = test.replace(-np.inf,0)\ntest = test.replace(np.inf,0)\n","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"surface string into class number"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_target['surface'] = le.fit_transform(new_target['surface'])","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I use RandomForest, and I try Xgboost and have same public score as RF without tunning.\nI should stack this 2 result, but it's too late for me, I ran out my submission chance."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('modelling')\nmodel = RandomForestClassifier(n_estimators=500, max_depth=10, min_samples_split=5, n_jobs=-1)\nmodel.fit(data.values, new_target['surface'].values)\nmeasured = model.predict(data.values)\npredicted = model.predict_proba(test)\nscore = model.score(data.values, new_target['surface'].values)\nprint(\"score: {}\".format(model.score(data.values, new_target['surface'].values)))\nimportances = model.feature_importances_\nindices = np.argsort(importances)\nfeatures = data.columns","execution_count":14,"outputs":[{"output_type":"stream","text":"score: 0.809155583049799\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ngc.collect()\n\nprint('Accuracy RF', score )\n\nresult = pd.DataFrame(data={'surface':le.inverse_transform(predicted.argmax(axis=1))},index= new_test['series_id'].unique())\nnew_target['surface'] = le.inverse_transform(new_target['surface'])\n\ndf = pd.concat([result, new_target[['surface']]], axis=0)\nsub = pd.read_csv('../input/sample_submission.csv')\nsub['surface'] = new_target['surface']","execution_count":15,"outputs":[{"output_type":"stream","text":"Accuracy RF 0.809155583049799\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('my_submission.csv', index=0)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sub.head())","execution_count":17,"outputs":[{"output_type":"stream","text":"   series_id        surface\n0          0  fine_concrete\n1          1           wood\n2          2       concrete\n3          3           wood\n4          4     soft_tiles\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}