{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is the notebook I used for EDA and predictions. I'll link to all the kernels I borrowed from or was influenced by. Thanks to [@nanashi](https://www.kaggle.com/jesucristo), [@ilhamfp31](https://www.kaggle.com/ilhamfp31) and [@artgor](https://www.kaggle.com/artgor/where-do-the-robots-drive) among others for their excellent kernels.\n"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\n\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#Preprocessing and train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale, StandardScaler, LabelEncoder, Imputer\n\n#Classifiers\n#from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport catboost as ctb\n\n#Cross validation\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read and examine the data\n\nI will use some visual EDA after examining it"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train = pd.read_csv(\"../input/X_train.csv\")\ntest = pd.read_csv(\"../input/X_test.csv\")\ntarget = pd.read_csv(\"../input/y_train.csv\")\nsub = pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.countplot(y = 'surface', data = target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing data. But it looks like there is a huge variation in the types of surfaces in the target data. We must take that into account and stratify the target data for predictions."},{"metadata":{},"cell_type":"markdown","source":"## Correlation Heatmap:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corr = train.corr()\n\n_ , ax = plt.subplots(figsize =(14, 10))\nhm = sns.heatmap(corr, ax= ax, annot= True,linewidths=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(26, 16))\nfor i, col in enumerate(train.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.hist(train[col], bins=80)\n    plt.hist(test[col], bins=80)\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"Let's explore the data from the 10 channels for one particular series."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plotseries(df,series_id,color='Blue'):\n    plt.figure(figsize=(26, 16))\n    for i, col in enumerate(df.columns[3:]):\n        plt.subplot(3, 4, i + 1)\n        plt.plot(train.loc[train['series_id'] == series_id, col])\n        plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plotseries(train,3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks very noisy. We should probably denoise that before training models."},{"metadata":{},"cell_type":"markdown","source":"# Fast Fourier Transform (FFT)\n\nAs is shown in [this kernel](https://www.kaggle.com/ilhamfp31/fast-fourier-transform-denoising) by [@ilhamfp31](https://www.kaggle.com/ilhamfp31) I will smooth out the data by using fast fourier transform.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from @theoviel at https://www.kaggle.com/theoviel/fast-fourier-transform-denoising\nfrom numpy.fft import rfft, irfft, rfftfreq\n\ndef filter_signal(signal, threshold=1e3):\n    fourier = rfft(signal)\n    frequencies = rfftfreq(signal.size, d=20e-3/signal.size)\n    fourier[frequencies > threshold] = 0\n    return irfft(fourier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# denoise train and test angular_velocity and linear_acceleration data\n\ntrain_denoised = train.copy()\ntest_denoised = test.copy()\n\n# train\nfor col in train.columns:\n    if col[0:3] == 'ang' or col[0:3] == 'lin':\n        # Apply filter_signal function to the data in each series\n        denoised_data = train.groupby(['series_id'])[col].apply(lambda x: filter_signal(x))\n        \n        # Assign the denoised data back to X_train\n        list_denoised_data = []\n        for arr in denoised_data:\n            for val in arr:\n                list_denoised_data.append(val)\n                \n        train_denoised[col] = list_denoised_data\n        \n# test\nfor col in test.columns:\n    if col[0:3] == 'ang' or col[0:3] == 'lin':\n        # Apply filter_signal function to the data in each series\n        denoised_data = test.groupby(['series_id'])[col].apply(lambda x: filter_signal(x))\n        \n        # Assign the denoised data back to X_test\n        list_denoised_data = []\n        for arr in denoised_data:\n            for val in arr:\n                list_denoised_data.append(val)\n                \n        test_denoised[col] = list_denoised_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the denoised data for the 10 channels."},{"metadata":{"scrolled":false,"trusted":false,"_kg_hide-input":false},"cell_type":"code","source":"plotseries(train_denoised,3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly it's hard to tell the difference from that. Let's take a closer look."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nplt.subplot(2,1,1)\nplt.plot(train.angular_velocity_X[120:250], label=\"original\");\nplt.plot(train_denoised.angular_velocity_X[120:250], label=\"denoised\");\nplt.title('linear_acceleration_X')\nplt.subplot(2,1,2)\nplt.plot(train.angular_velocity_Y[120:250], label=\"original\");\nplt.plot(train_denoised.angular_velocity_Y[120:250], label=\"denoised\");\nplt.title('linear_acceleration_Y')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All right, definitely made a difference."},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nI will define some features first. Heavily borrowed from:\n\n1. https://www.kaggle.com/pluceroo/new-features-lgbm-and-simple-rf\n2. https://www.kaggle.com/prashantkikani/help-humanity-by-helping-robots"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    if col[0:3] == 'ang' or col[0:3] == 'lin':\n        train_denoised[col + '_noise'] = np.abs(train[col] - train_denoised[col])\n        \nfor col in test.columns:\n    if col[0:3] == 'ang' or col[0:3] == 'lin':\n        test_denoised[col + '_noise'] = np.abs(test[col] - test_denoised[col])\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"I've used \"https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\nI do a coordinate transformation from quaternion to Euler\" to convert quaternions to euler angles\"\"\" \n\ndef quaternion_to_euler(x, y, z, w):\n\n        import math\n        t0 = +2.0 * (w * x + y * z)\n        t1 = +1.0 - 2.0 * (x * x + y * y)\n        X = math.atan2(t0, t1)\n\n        t2 = +2.0 * (w * y - z * x)\n        t2 = +1.0 if t2 > +1.0 else t2\n        t2 = -1.0 if t2 < -1.0 else t2\n        Y = math.asin(t2)\n\n        t3 = +2.0 * (w * z + x * y)\n        t4 = +1.0 - 2.0 * (y * y + z * z)\n        Z = math.atan2(t3, t4)\n\n        return X, Y, Z\n\ndef _kurtosis(x):\n    return kurtosis(x)\n\ndef CPT5(x):\n    den = len(x)*np.exp(np.std(x))\n    return sum(np.exp(x))/den\n\ndef skewness(x):\n    return skew(x)\n\ndef SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    xn_i1 = x[0:len(x)-2]  # xn-1\n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:]) \n\ndef wave_length(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    return sum(abs(xn_i2-xn))\n    \ndef norm_entropy(x):\n    tresh = 3\n    return sum(np.power(abs(x),tresh))\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA/len(x),2)\n\ndef mean_abs(x):\n    return sum(abs(x))/len(x)\n\ndef zero_crossing(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1\n    return sum(np.heaviside(-xn*xn_i2,0))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feat(df):\n      \n    df['total_angular_velocity'] = (df['angular_velocity_X']**2+df['angular_velocity_Y']**2+df['angular_velocity_Z']**2)**0.5\n    df['total_linear_acceleration'] = (df['linear_acceleration_X']**2+df['linear_acceleration_Y']**2+df['linear_acceleration_Z']**2)**0.5\n    df['acc_vs_vel'] = df['total_linear_acceleration']/df['total_angular_velocity']\n    \n    x, y, z, w = df['orientation_X'].tolist(), df['orientation_Y'].tolist(), df['orientation_Z'].tolist(), df['orientation_W'].tolist()\n    \n    xlist, ylist, zlist = [], [], []\n    \n    for i in range(len(x)):\n        x2, y2, z2 = quaternion_to_euler(x[i],y[i],z[i],w[i])\n        xlist.append(x2)\n        ylist.append(y2)\n        zlist.append(z2)\n    \n    df['euler_X'] = xlist\n    df['euler_Y'] = ylist\n    df['euler_Z'] = zlist\n    \n    df['euler_orientation'] = (df['euler_X']**2 + df['euler_Y']**2 + df['euler_Z']**2)**0.5\n    \n    def mean_diff(x):\n        return np.mean(np.abs(np.diff(x)))\n  \n    def mean_diff_diff(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    df2 = pd.DataFrame()    \n    \n    for col in df.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        if 'noise' in col:\n            df2[col + '_mean'] = df.groupby(['series_id'])[col].mean()\n        \n        else:\n            df2[col + '_mean'] = df.groupby(['series_id'])[col].mean()\n            df2[col + '_min'] = df.groupby(['series_id'])[col].min()\n            df2[col + '_max'] = df.groupby(['series_id'])[col].max()\n            df2[col + '_std'] = df.groupby(['series_id'])[col].std()\n            df2[col + '_range'] = df2[col + '_max'] - df2[col + '_min']\n            df2[col + '_max_min_ratio'] = df.groupby(['series_id'])[col].max()/df.groupby(['series_id'])[col].min()\n        \n            df2[col + '_mean_abs_difference'] =df.groupby(['series_id'])[col].apply(mean_diff)\n            df2[col + '_mean_diff_of_abs_diff'] = df.groupby('series_id')[col].apply(mean_diff_diff)\n        \n        \n            df2[col + '_CPT5'] = df.groupby(['series_id'])[col].apply(CPT5) \n            df2[col + '_SSC'] = df.groupby(['series_id'])[col].apply(SSC) \n            df2[col + '_skewness'] = df.groupby(['series_id'])[col].apply(skewness)\n            df2[col + '_wave_lenght'] = df.groupby(['series_id'])[col].apply(wave_length)\n            df2[col + '_norm_entropy'] = df.groupby(['series_id'])[col].apply(norm_entropy)\n            df2[col + '_SRAV'] = df.groupby(['series_id'])[col].apply(SRAV)\n            df2[col + '_kurtosis'] = df.groupby(['series_id'])[col].apply(_kurtosis) \n            df2[col + '_mean_abs'] = df.groupby(['series_id'])[col].apply(mean_abs) \n            df2[col + '_zero_crossing'] = df.groupby(['series_id'])[col].apply(zero_crossing) \n        \n        \n        \n    return df2\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fe = feat(train_denoised)\ntest_fe = feat(test_denoised)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some NaN values and infinities, so we'll have to get rid of them. I will replace them with zeroes."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fe.fillna(0,inplace=True)\ntest_fe.fillna(0,inplace=True)\ntrain_fe.replace(-np.inf,0,inplace=True)\ntrain_fe.replace(np.inf,0,inplace=True)\ntest_fe.replace(-np.inf,0,inplace=True)\ntest_fe.replace(np.inf,0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing\n\nI'm scaling the data and using a label encoder for the surdfaces.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\n\nX_train = pd.DataFrame(sc.fit_transform(train_fe))\nX_test = pd.DataFrame(sc.transform(test_fe))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ntarget['surface'] = le.fit_transform(target['surface'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot Confusion Matrix:\n\nI used code from [this kernel](https://www.kaggle.com/artgor/where-do-the-robots-drive) which defines a function to plot the confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/artgor/where-do-the-robots-drive\nimport itertools\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title='Confusion Matrix',cmap=plt.cm.Blues):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models\n\nNow we're ready to start building models and making predictions. I will start with Random forest. I have used grid search (in another Kernel) to extract the best parameters for it."},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = 3\neval_list= []\npred_list = []\nmeas_list = []\n\nfor i in range (0,cv):\n\n    folds = StratifiedKFold(n_splits=8, shuffle=True, random_state=20)\n    predicted_rf = np.zeros((X_test.shape[0],9))\n    measured_rf= np.zeros((X_train.shape[0],9))\n    score = 0\n           \n    for fold, (trn_idx, val_idx) in enumerate(folds.split(X_train.values,target['surface'].values)):\n        \n        X_tr = X_train.iloc[trn_idx]\n        y_tr = target['surface'][trn_idx]\n    \n        X_valid = X_train.iloc[val_idx]\n        y_valid = target['surface'][val_idx]\n        \n        rfc = RandomForestClassifier(n_estimators=100, min_samples_leaf = 1,max_depth= None,n_jobs=-1,random_state=20)\n        rfc.fit(X_tr,y_tr)\n        measured_rf[val_idx] = rfc.predict_proba(X_valid)\n        y_pred = rfc.predict_proba(X_test)\n        predicted_rf += y_pred\n        score += rfc.score(X_valid,y_valid)\n        \n        \n        print(\"Fold: {}, RF Score: {}\".format(fold,rfc.score(X_valid,y_valid)))\n        \n    predicted_rf /= folds.n_splits    \n    \n    meas_list.append(measured_rf)\n    pred_list.append(predicted_rf)\n    eval_list.append(score/folds.n_splits) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(target['surface'], measured_rf.argmax(1), le.classes_,title ='Confusion Matrix for Random Forest')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indx = eval_list.index(max(eval_list))\n                      \nprint(indx, max(eval_list))\npred_rf = pred_list[indx]\nmeas_rf = meas_list[indx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_kg_hide-input":false},"cell_type":"code","source":"sub['surface'] = le.inverse_transform(pred_rf.argmax(1))\n#sub.to_csv('submission_rf_fft.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meas_rf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=8, shuffle=True, random_state=20)\npredicted_et = np.zeros((X_test.shape[0],9))\nmeasured_et = np.zeros((X_train.shape[0],9))\nscore_et = 0\n           \nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X_train.values,target['surface'].values)):\n    \n    X_tr = X_train.iloc[trn_idx]\n    y_tr = target['surface'][trn_idx]\n    \n    X_valid = X_train.iloc[val_idx]\n    y_valid = target['surface'][val_idx]\n    \n    \n    etc = ExtraTreesClassifier(n_estimators=200,max_depth=12,min_samples_leaf=2,n_jobs=-1,random_state=20)\n    etc.fit(X_tr,y_tr)\n    measured_et[val_idx] = etc.predict_proba(X_valid)\n    et_pred = etc.predict_proba(X_test)\n    predicted_et += et_pred\n    score_et += etc.score(X_valid,y_valid)/folds.n_splits\n        \n    print(\"Fold: {}, ET Score: {}\".format(fold,etc.score(X_valid,y_valid)))\n        \npredicted_et /= folds.n_splits   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(target['surface'], measured_et.argmax(1), le.classes_,title ='Confusion Matrix for Extra Trees',cmap=plt.cm.YlOrRd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"measured_et.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM\n\nI'll also try LightGBM, the parameters for which I've extracted by using Bayesian Optimization. [This kernel](https://www.kaggle.com/artgor/bayesian-optimization-for-robots) shows how to do that."},{"metadata":{"trusted":true},"cell_type":"code","source":"params_lgb = {'num_leaves': 123,\n          'min_data_in_leaf': 12,\n          'objective': 'multiclass',\n          'max_depth': 24,\n          'learning_rate': 0.0468035094972387,\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": 0.89330183551903,\n          \"bagging_seed\": 11,\n          \"verbosity\": 0,\n          'reg_alpha': 0.9498109326932401,\n          'reg_lambda': 0.805849096054620,\n          \"num_class\": 9,\n          'nthread': -1,\n          'min_split_gain': 0.0099132272405649,\n          'subsample': 0.90273588307031,\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_predict_lgb(X, X_tst, y, param=None):\n    \n    lgb_predicted = np.zeros((X_tst.shape[0],9))\n    lgb_measured= np.zeros((X.shape[0],9))\n    lgb_acc = 0\n    \n    feature_importance = pd.DataFrame()\n    \n    for fold, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n                \n        lgbm = lgb.LGBMClassifier(**param, n_estimators = 20000, verbose = 0, n_jobs = -1,random_state=20,\n                                  early_stopping_rounds=100)\n        lgbm.fit(X.iloc[trn_idx],y[trn_idx], eval_set=[(X.iloc[trn_idx],y[trn_idx]), (X.iloc[val_idx],y[val_idx])], \n                 eval_metric='multi_logloss')\n        lgb_measured[val_idx] = lgbm.predict_proba(X.iloc[val_idx])\n        y_pred = lgbm.predict_proba(X_tst)/folds.n_splits\n        lgb_predicted +=y_pred\n        lgb_acc += accuracy_score(y[val_idx], lgb_measured[val_idx].argmax(1))/folds.n_splits\n    \n        print(\"Fold: {} LGB score: {}\".format(fold,accuracy_score(y[val_idx], lgb_measured[val_idx].argmax(1))))\n        \n       \n    return lgb_measured, lgb_predicted, lgb_acc\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"measured_lgb, predicted_lgb, score_lgb = fit_predict_lgb(X_train,X_test,target['surface'], param=params_lgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(target['surface'], measured_lgb.argmax(1),le.classes_, title ='Confusion Matrix for Training Set with LightGBM',)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVC\n\nSVC is also something to try. I used Grid Search to find the best parameters for SVC, then use that to fit the test data. It won't be as accurate as random forest, but it doesn't hurt to check."},{"metadata":{"trusted":false},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=20)\nsvc_predicted = np.zeros((X_test.shape[0],9))\nsvc_measured= np.zeros((X_train.shape[0]))\nsvc_score = 0\nsvc_acc = []\nsvc = SVC(random_state=314,C=1, decision_function_shape= 'ovo', gamma= 'auto',max_iter= -1, probability=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X_train.values,target['surface'].values)):\n\n    X_tr = X_train.iloc[trn_idx]\n    y_tr = target['surface'][trn_idx]\n        \n    X_valid= X_train.iloc[val_idx]\n    y_valid = target['surface'][val_idx]\n    \n    \n    svc.fit(X_tr,y_tr)\n    svc_measured[val_idx] = svc.predict(X_valid)\n    svc_score += svc.score(X_valid,y_valid)/folds.n_splits\n    \n    svc_pred = svc.predict_proba(X_test)\n    svc_predicted += svc_pred\n    \n    svc_acc.append(svc_score)\n    \n    print(\"Fold: {}, SVC Score: {}\".format(fold,svc.score(X_valid,y_valid)))\n    \nsvc_predicted /= folds.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_confusion_matrix(target['surface'],svc_measured,le.classes_, title ='Confusion Matrix for Training Set with SVC',cmap=plt.cm.YlOrBr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVC isn't doing very well, so I won't use it for stacking."},{"metadata":{},"cell_type":"markdown","source":"# Stacking:\n\nI will use Logistic Regression as the meta classifier built on Random Forest, Extra Trees and LightGBM first. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.concatenate((measured_et, meas_rf, measured_lgb), axis=1)\nx_test = np.concatenate((predicted_et, pred_rf, predicted_lgb), axis=1)\n\nprint(\"{},{}\".format(x_train.shape, x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression = LogisticRegression()\nlogistic_regression.fit(x_train,target['surface'])\n\nlogreg_pred = logistic_regression.predict_proba(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sub['surface'] = le.inverse_transform(logreg_pred.argmax(1))\n#sub.to_csv('submission_lr_stack.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}