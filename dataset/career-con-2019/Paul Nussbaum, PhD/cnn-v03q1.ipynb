{"cells":[{"metadata":{},"cell_type":"markdown","source":"Forked from (https://www.kaggle.com/donkeys/my-little-eda-with-random-forest/log) My Little EDA with Random Forest; Courtesy of \"averagemn\""},{"metadata":{"_uuid":"51abbcef133f4539a585ee4bb9dc4c4d75946ade"},"cell_type":"markdown","source":"# Kernel for CareerCon 2019 \n(see contest for details https://www.kaggle.com/c/career-con-2019)\n## Paul A. Nussbaum\n### Demonstrates Signal Analysis and Pattern Recognition for Machine Learning\n#### Executive Summary\nInertial measurement sensors on a moving robot record signals representing different accelerations as they vary over time. This algorithm seeks to observe these signals of bouncing and bumping, and from that determine which category of floor type the robot is rolling over. To accomplish this, the original signals are analyzed and presented to a machine learning algorithm which seeks to recognize the different patterns.\n#### Details\n* There are eight floor type classifications, numbered in the training data 0 through 7 in the following order: 'carpet', 'concrete', 'fine_concrete', 'hard_tiles', 'hard_tiles_large_space',  'soft_pvc', 'soft_tiles', 'tiled', and 'wood'\n\n#### Revision History\n* v03 - Current revision. Added one-hot encoding, and turned on GPU. Tuned 2D layers a bit.\n* v02 - Converted to Keras CNN using Numpy arrays.\n* v01 - original fork (see top of page)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\n\nfrom keras.layers import * \nfrom keras.models import Model, Sequential, load_model\nfrom keras import backend as K \nfrom keras import optimizers \nfrom keras.callbacks import * \nfrom keras.backend import clear_session\n\nprint(os.listdir(\"../input\"))","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"['y_train.csv', 'sample_submission.csv', 'X_test.csv', 'X_train.csv']\n","name":"stdout"}]},{"metadata":{"_uuid":"ef99b75ad054392bf5cf2126b5e0d77eb9e055e8"},"cell_type":"markdown","source":"## Load the Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/X_train.csv\")\ndf_test = pd.read_csv(\"../input/X_test.csv\")\ndf_y = pd.read_csv(\"../input/y_train.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12393bc0b237007d98f7328b749925ac2141d01b"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# encode class values as integers so they work as targets for the prediction algorithm\nencoder = LabelEncoder()\ny = encoder.fit_transform(df_y[\"surface\"])\ny_count = len(list(encoder.classes_))","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed24f6d4e2b98b33bc4c9c25f60c279a39b66511"},"cell_type":"code","source":"label_mapping = {i: l for i, l in enumerate(encoder.classes_)}","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Data Frame Shape (Train then Test, then correct training labels)\")\nprint(df_train.shape, df_test.shape, y.shape)\nprint(\"Numpy Array Shape (Train then Test then correct training labels)\")\n\n# --- Convert Training, Testing, and Labels into Numpy arrays\n\nnum_train = int(df_train.shape[0] / 128)\n\n# Use this for 1D Convolutions\n# X_train = np.reshape(np.array(df_train), (num_train,128,13))\n# remove potential leakage info \n# X_train = X_train[:,:,3:14]\n\n# Use this for 2D Convolutions\nX_train = np.reshape(np.array(df_train), (num_train,128,13,1))\n# remove potential leakage info \nX_train = X_train[:,:,3:14,:]\n\nnum_test = int(df_test.shape[0] / 128)\n\n# Use this for 1D Convolutions\n# X_test = np.reshape(np.array(df_test), (num_test,128,13))\n# remove potential leakage info \n# X_test = X_test[:,:,3:14]\n\n# Use this for 2D Convolutions\nX_test = np.reshape(np.array(df_test), (num_test,128,13,1))\n# remove potential leakage info \nX_test = X_test[:,:,3:14,:]\n\ny_array = np.array(y)\n# use one hot encoding\ny_one_hot = np.zeros((y_array.shape[0],y_count))\ny_one_hot[np.arange(y_array.shape[0]),y_array] = 1\nprint(X_train.shape, X_test.shape, y_one_hot.shape)\nnum_features = X_train.shape[2]","execution_count":5,"outputs":[{"output_type":"stream","text":"Data Frame Shape (Train then Test, then correct training labels)\n(487680, 13) (488448, 13) (3810,)\nNumpy Array Shape (Train then Test then correct training labels)\n(3810, 128, 10, 1) (3816, 128, 10, 1) (3810, 9)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"           series_id          ...            linear_acceleration_Z\ncount  487680.000000          ...                    487680.000000\nmean     1904.500000          ...                        -9.364886\nstd      1099.853353          ...                         2.845341\nmin         0.000000          ...                       -75.386000\n25%       952.000000          ...                       -10.193000\n50%      1904.500000          ...                        -9.365300\n75%      2857.000000          ...                        -8.522700\nmax      3809.000000          ...                        65.839000\n\n[8 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>series_id</th>\n      <th>measurement_number</th>\n      <th>orientation_X</th>\n      <th>orientation_Y</th>\n      <th>orientation_Z</th>\n      <th>orientation_W</th>\n      <th>angular_velocity_X</th>\n      <th>angular_velocity_Y</th>\n      <th>angular_velocity_Z</th>\n      <th>linear_acceleration_X</th>\n      <th>linear_acceleration_Y</th>\n      <th>linear_acceleration_Z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n      <td>487680.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1904.500000</td>\n      <td>63.500000</td>\n      <td>-0.018050</td>\n      <td>0.075062</td>\n      <td>0.012458</td>\n      <td>-0.003804</td>\n      <td>0.000178</td>\n      <td>0.008338</td>\n      <td>-0.019184</td>\n      <td>0.129281</td>\n      <td>2.886468</td>\n      <td>-9.364886</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1099.853353</td>\n      <td>36.949327</td>\n      <td>0.685696</td>\n      <td>0.708226</td>\n      <td>0.105972</td>\n      <td>0.104299</td>\n      <td>0.117764</td>\n      <td>0.088677</td>\n      <td>0.229153</td>\n      <td>1.870600</td>\n      <td>2.140067</td>\n      <td>2.845341</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.989100</td>\n      <td>-0.989650</td>\n      <td>-0.162830</td>\n      <td>-0.156620</td>\n      <td>-2.371000</td>\n      <td>-0.927860</td>\n      <td>-1.268800</td>\n      <td>-36.067000</td>\n      <td>-121.490000</td>\n      <td>-75.386000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>952.000000</td>\n      <td>31.750000</td>\n      <td>-0.705120</td>\n      <td>-0.688980</td>\n      <td>-0.089466</td>\n      <td>-0.106060</td>\n      <td>-0.040752</td>\n      <td>-0.033191</td>\n      <td>-0.090743</td>\n      <td>-0.530833</td>\n      <td>1.957900</td>\n      <td>-10.193000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1904.500000</td>\n      <td>63.500000</td>\n      <td>-0.105960</td>\n      <td>0.237855</td>\n      <td>0.031949</td>\n      <td>-0.018704</td>\n      <td>0.000084</td>\n      <td>0.005412</td>\n      <td>-0.005335</td>\n      <td>0.124980</td>\n      <td>2.879600</td>\n      <td>-9.365300</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2857.000000</td>\n      <td>95.250000</td>\n      <td>0.651803</td>\n      <td>0.809550</td>\n      <td>0.122870</td>\n      <td>0.097215</td>\n      <td>0.040527</td>\n      <td>0.048068</td>\n      <td>0.064604</td>\n      <td>0.792263</td>\n      <td>3.798800</td>\n      <td>-8.522700</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3809.000000</td>\n      <td>127.000000</td>\n      <td>0.989100</td>\n      <td>0.988980</td>\n      <td>0.155710</td>\n      <td>0.154770</td>\n      <td>2.282200</td>\n      <td>1.079100</td>\n      <td>1.387300</td>\n      <td>36.797000</td>\n      <td>73.008000</td>\n      <td>65.839000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sanity Check of conversion from data frame to numpy\n# average values in TRAINING array conversion below - should be equal to the individual data frame feature \"Mean\" values above\n\nfor i in range (num_features) :\n    print(X_train[:,:,i].mean())","execution_count":7,"outputs":[{"output_type":"stream","text":"-0.018049738652363475\n0.07506199154080025\n0.012457660075753735\n-0.00380373956336353\n0.00017751337388841564\n0.008337578268795921\n-0.019183612282310716\n0.12928096050926643\n2.8864677185264584\n-9.364885745110374\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"7377d2e076db0d660ffb274a1ab2d947b91c9053"},"cell_type":"markdown","source":"# Run CNN classifier:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model() :\n    \n    scale = 15\n    # scale = 100\n\n    # use a simple sequential convolutional neural network model\n    model2 = Sequential()\n    \n    # Start with a droput to slow learning and avoid local minima which in turn will prevent overfitting on the training set \n    model2.add(Dropout(0.1, input_shape=(128,10,1)))\n    \n    # The first colvolutional layer looks for basic patterns (such as slope) within a sensor's time sequenced data\n    # but will also look across all of the sensors for patterns where the may correlate\n    # Assume there are three basic kinds of slope (+, 0, and -) and there are 2 groups of 3 sensors, and 1 group of 3 sensors, \n    # so (3^3) * (3^3) * (3^4) = 3^10 = 59 k combinations of slopes across the sensors\n    # We can use this as a rough measure of how big to dimension our first layer - let's assume that only \"scale\" out of 1000 of these are \"important\"\n    model2.add(Conv2D(int(60 * scale), (3,1), strides = 1, activation='relu'))\n    \n    # The following convolutional layer(s) are really 1D, and look for larger and larger time patterns (second order, scale increase, etc.)\n    # The use of the strides > 1 in the convolution layer risks the possibility of phase dependence \n    # This could conceivably cause the same time data shifted by one or two samples to appear different, so instead we use pooling strides\n    # These strides can be reduced at the cost of network size, speed, and possible overfitting \n    # If it is found to be needed, we can compensate for overfitting with larger dropout, or more dropouts between layers, or larger \"batch_size\"\n    model2.add(Dropout(0.1))\n    model2.add(MaxPooling2D(pool_size = (2,1), strides = (2,1)))\n    model2.add(Conv2D(int(10 * scale), (3,1), strides = 1, activation='relu'))\n    model2.add(Dropout(0.1))\n    model2.add(MaxPooling2D(pool_size = (2,1), strides = (2,1)))\n    model2.add(Conv2D(int(5 * scale), (3,1), strides = 1, activation='relu'))\n    model2.add(Dropout(0.1))\n    model2.add(MaxPooling2D(pool_size = (2,1), strides = (2,1)))\n    model2.add(Conv2D(int(5 * scale), (3,1), strides = 1, activation='relu'))\n    model2.add(Dropout(0.1))\n    model2.add(MaxPooling2D(pool_size = (2,1), strides = (2,1)))\n    model2.add(Conv2D(int(2 * scale), (3,1), strides = 1, activation='relu'))\n    model2.add(Dropout(0.1))\n    model2.add(Conv2D(int(2 * scale), (3,1), strides = 1, activation='relu'))\n    \n    # After these convolutional layers, we are covering a large chunk of the entire sensor data set of 128 samples\n    # and are ready to classify these \"wavelet-like\" learned patterns using perceptron-style neural netowrks (\"Dense\")\n    model2.add(Flatten())\n    \n    # Include a couple of dense layers in case the classes are not linearly seperable by this point\n    model2.add(Dropout(0.1))\n    model2.add(Dense(int(2 * scale), activation='relu'))\n    model2.add(Dropout(0.1))\n    model2.add(Dense(int(1 * scale), activation='relu'))\n    \n    # Finally, mirror the \"one-hot\" classification scheme with a softmax output layer\n    model2.add(Dropout(0.1))\n    model2.add(Dense(y_count, activation = 'softmax'))\n    \n    # Binary Cross Entropy (or log loss) is used as the error function\n    model2.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model2","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will be slowing down the learning using \"Dropouts\" (see above) so the patience needed to exit local minima can be large\npatience = 15\n# Probably will never reach this many epochs, but want to use a number larger than what we expect\nepochs = 300\n# Divide the data into 15 different versions of training/validation\nn_fold = 15\n# Using KFold instead of StratifiedKFold becuase there is a low degree of confidence that the test classification distributions \n# or more importantly, the real world classification probabilities, are equal to those found in the training set\nfolds = KFold(n_splits=n_fold, shuffle=False, random_state=1234)\n\nsam = X_train.shape[0]\ncol = X_train.shape[1]\n\nsam_test = X_test.shape[0]\ncol_test = X_test.shape[1]\n\nprediction = np.zeros((sam_test, y_count))\nprediction_train = np.zeros((sam, y_count))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train,y_one_hot)):\n    print('Fold', fold_n)\n    X_train2, X_valid2 = X_train[train_index], X_train[valid_index]\n    y_train2, y_valid2 = y_one_hot[train_index], y_one_hot[valid_index]\n    K.clear_session()\n    model = make_model()\n    print(model.summary())\n    checkpointer = ModelCheckpoint('Net1', verbose=1, save_best_only=True)\n    earlystopper = EarlyStopping(patience = patience, verbose=0) \n    results = model.fit(X_train2, y_train2, epochs = epochs, batch_size = 32,\n                    callbacks=[earlystopper, checkpointer], validation_data=[X_valid2, y_valid2])\n    model = load_model('Net1')\n    # For each fold, we will accummulate our opinion of the final classification\n    prediction_train  += model.predict(X_train)/n_fold\n    # Note, in a real world deployment, we may have \"n_fold\" neural networks, each rendering their opinion - but here\n    # to save memory and disk space, we dispose of each NN when the classification is done\n    prediction += model.predict(X_test)/n_fold \n    print()","execution_count":null,"outputs":[{"output_type":"stream","text":"Fold 0\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndropout_1 (Dropout)          (None, 128, 10, 1)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 126, 10, 1200)     4800      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 126, 10, 1200)     0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 63, 10, 1200)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 61, 10, 200)       720200    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 61, 10, 200)       0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 30, 10, 200)       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 28, 10, 100)       60100     \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 28, 10, 100)       0         \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 14, 10, 100)       0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 12, 10, 100)       30100     \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 12, 10, 100)       0         \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 6, 10, 100)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 4, 10, 40)         12040     \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 4, 10, 40)         0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 2, 10, 40)         4840      \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 800)               0         \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 800)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 40)                32040     \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 40)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 20)                820       \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 20)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 9)                 189       \n=================================================================\nTotal params: 865,129\nTrainable params: 865,129\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 3556 samples, validate on 254 samples\nEpoch 1/300\n3556/3556 [==============================] - 6s 2ms/step - loss: 1.9891 - val_loss: 2.0274\n\nEpoch 00001: val_loss improved from inf to 2.02741, saving model to Net1\nEpoch 2/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.7398 - val_loss: 1.8453\n\nEpoch 00002: val_loss improved from 2.02741 to 1.84526, saving model to Net1\nEpoch 3/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.6208 - val_loss: 1.7675\n\nEpoch 00003: val_loss improved from 1.84526 to 1.76746, saving model to Net1\nEpoch 4/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.5153 - val_loss: 1.7405\n\nEpoch 00004: val_loss improved from 1.76746 to 1.74045, saving model to Net1\nEpoch 5/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.4454 - val_loss: 1.6069\n\nEpoch 00005: val_loss improved from 1.74045 to 1.60689, saving model to Net1\nEpoch 6/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.3792 - val_loss: 1.4989\n\nEpoch 00006: val_loss improved from 1.60689 to 1.49892, saving model to Net1\nEpoch 7/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.3241 - val_loss: 1.4585\n\nEpoch 00007: val_loss improved from 1.49892 to 1.45854, saving model to Net1\nEpoch 8/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.2292 - val_loss: 1.3971\n\nEpoch 00008: val_loss improved from 1.45854 to 1.39711, saving model to Net1\nEpoch 9/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.1449 - val_loss: 1.2169\n\nEpoch 00009: val_loss improved from 1.39711 to 1.21690, saving model to Net1\nEpoch 10/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.1092 - val_loss: 1.1104\n\nEpoch 00010: val_loss improved from 1.21690 to 1.11041, saving model to Net1\nEpoch 11/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.0472 - val_loss: 1.1077\n\nEpoch 00011: val_loss improved from 1.11041 to 1.10768, saving model to Net1\nEpoch 12/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.0100 - val_loss: 1.0654\n\nEpoch 00012: val_loss improved from 1.10768 to 1.06538, saving model to Net1\nEpoch 13/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.9701 - val_loss: 1.1537\n\nEpoch 00013: val_loss did not improve from 1.06538\nEpoch 14/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.9642 - val_loss: 1.0273\n\nEpoch 00014: val_loss improved from 1.06538 to 1.02731, saving model to Net1\nEpoch 15/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.9434 - val_loss: 1.0153\n\nEpoch 00015: val_loss improved from 1.02731 to 1.01526, saving model to Net1\nEpoch 16/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.9018 - val_loss: 1.0104\n\nEpoch 00016: val_loss improved from 1.01526 to 1.01037, saving model to Net1\nEpoch 17/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.8917 - val_loss: 0.9620\n\nEpoch 00017: val_loss improved from 1.01037 to 0.96199, saving model to Net1\nEpoch 18/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.8646 - val_loss: 0.8972\n\nEpoch 00018: val_loss improved from 0.96199 to 0.89724, saving model to Net1\nEpoch 19/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.8055 - val_loss: 0.8970\n\nEpoch 00019: val_loss improved from 0.89724 to 0.89698, saving model to Net1\nEpoch 20/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.7949 - val_loss: 0.9051\n\nEpoch 00020: val_loss did not improve from 0.89698\nEpoch 21/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.8113 - val_loss: 0.8516\n\nEpoch 00021: val_loss improved from 0.89698 to 0.85163, saving model to Net1\nEpoch 22/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.7540 - val_loss: 0.8541\n\nEpoch 00022: val_loss did not improve from 0.85163\nEpoch 23/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.7551 - val_loss: 0.8879\n\nEpoch 00023: val_loss did not improve from 0.85163\nEpoch 24/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.7580 - val_loss: 0.8236\n\nEpoch 00024: val_loss improved from 0.85163 to 0.82360, saving model to Net1\nEpoch 25/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.7100 - val_loss: 0.7676\n\nEpoch 00025: val_loss improved from 0.82360 to 0.76761, saving model to Net1\nEpoch 26/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.6904 - val_loss: 0.7764\n\nEpoch 00026: val_loss did not improve from 0.76761\nEpoch 27/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.6881 - val_loss: 0.7790\n\nEpoch 00027: val_loss did not improve from 0.76761\nEpoch 28/300\n","name":"stdout"},{"output_type":"stream","text":"3556/3556 [==============================] - 5s 1ms/step - loss: 0.6353 - val_loss: 0.7326\n\nEpoch 00028: val_loss improved from 0.76761 to 0.73260, saving model to Net1\nEpoch 29/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.6771 - val_loss: 0.7437\n\nEpoch 00029: val_loss did not improve from 0.73260\nEpoch 30/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.6432 - val_loss: 0.7553\n\nEpoch 00030: val_loss did not improve from 0.73260\nEpoch 31/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.6324 - val_loss: 0.7806\n\nEpoch 00031: val_loss did not improve from 0.73260\nEpoch 32/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.6282 - val_loss: 0.7508\n\nEpoch 00032: val_loss did not improve from 0.73260\nEpoch 33/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.6222 - val_loss: 0.7731\n\nEpoch 00033: val_loss did not improve from 0.73260\nEpoch 34/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.6114 - val_loss: 0.7576\n\nEpoch 00034: val_loss did not improve from 0.73260\nEpoch 35/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5900 - val_loss: 0.7270\n\nEpoch 00035: val_loss improved from 0.73260 to 0.72702, saving model to Net1\nEpoch 36/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5680 - val_loss: 0.7395\n\nEpoch 00036: val_loss did not improve from 0.72702\nEpoch 37/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5660 - val_loss: 0.8330\n\nEpoch 00037: val_loss did not improve from 0.72702\nEpoch 38/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5744 - val_loss: 0.7429\n\nEpoch 00038: val_loss did not improve from 0.72702\nEpoch 39/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5513 - val_loss: 0.7979\n\nEpoch 00039: val_loss did not improve from 0.72702\nEpoch 40/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5393 - val_loss: 0.7756\n\nEpoch 00040: val_loss did not improve from 0.72702\nEpoch 41/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5512 - val_loss: 0.8234\n\nEpoch 00041: val_loss did not improve from 0.72702\nEpoch 42/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5369 - val_loss: 0.7631\n\nEpoch 00042: val_loss did not improve from 0.72702\nEpoch 43/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4987 - val_loss: 0.9169\n\nEpoch 00043: val_loss did not improve from 0.72702\nEpoch 44/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5208 - val_loss: 0.7698\n\nEpoch 00044: val_loss did not improve from 0.72702\nEpoch 45/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5522 - val_loss: 0.7349\n\nEpoch 00045: val_loss did not improve from 0.72702\nEpoch 46/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4888 - val_loss: 0.7810\n\nEpoch 00046: val_loss did not improve from 0.72702\nEpoch 47/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5103 - val_loss: 0.6479\n\nEpoch 00047: val_loss improved from 0.72702 to 0.64791, saving model to Net1\nEpoch 48/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5020 - val_loss: 0.7569\n\nEpoch 00048: val_loss did not improve from 0.64791\nEpoch 49/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4767 - val_loss: 0.6720\n\nEpoch 00049: val_loss did not improve from 0.64791\nEpoch 50/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4833 - val_loss: 0.7149\n\nEpoch 00050: val_loss did not improve from 0.64791\nEpoch 51/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4861 - val_loss: 0.8203\n\nEpoch 00051: val_loss did not improve from 0.64791\nEpoch 52/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4588 - val_loss: 0.7979\n\nEpoch 00052: val_loss did not improve from 0.64791\nEpoch 53/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4679 - val_loss: 0.7177\n\nEpoch 00053: val_loss did not improve from 0.64791\nEpoch 54/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.5090 - val_loss: 0.7096\n\nEpoch 00054: val_loss did not improve from 0.64791\nEpoch 55/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4466 - val_loss: 0.6691\n\nEpoch 00055: val_loss did not improve from 0.64791\nEpoch 56/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4501 - val_loss: 0.9028\n\nEpoch 00056: val_loss did not improve from 0.64791\nEpoch 57/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4841 - val_loss: 0.7103\n\nEpoch 00057: val_loss did not improve from 0.64791\nEpoch 58/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4423 - val_loss: 0.7836\n\nEpoch 00058: val_loss did not improve from 0.64791\nEpoch 59/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4208 - val_loss: 0.7002\n\nEpoch 00059: val_loss did not improve from 0.64791\nEpoch 60/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4187 - val_loss: 0.7713\n\nEpoch 00060: val_loss did not improve from 0.64791\nEpoch 61/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.3966 - val_loss: 0.7944\n\nEpoch 00061: val_loss did not improve from 0.64791\nEpoch 62/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 0.4322 - val_loss: 0.7584\n\nEpoch 00062: val_loss did not improve from 0.64791\n\nFold 1\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndropout_1 (Dropout)          (None, 128, 10, 1)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 126, 10, 1200)     4800      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 126, 10, 1200)     0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 63, 10, 1200)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 61, 10, 200)       720200    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 61, 10, 200)       0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 30, 10, 200)       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 28, 10, 100)       60100     \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 28, 10, 100)       0         \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 14, 10, 100)       0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 12, 10, 100)       30100     \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 12, 10, 100)       0         \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 6, 10, 100)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 4, 10, 40)         12040     \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 4, 10, 40)         0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 2, 10, 40)         4840      \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 800)               0         \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 800)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 40)                32040     \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 40)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 20)                820       \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 20)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 9)                 189       \n=================================================================\nTotal params: 865,129\nTrainable params: 865,129\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"},{"output_type":"stream","text":"Train on 3556 samples, validate on 254 samples\nEpoch 1/300\n3556/3556 [==============================] - 6s 2ms/step - loss: 2.0457 - val_loss: 1.9462\n\nEpoch 00001: val_loss improved from inf to 1.94618, saving model to Net1\nEpoch 2/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.7870 - val_loss: 1.7034\n\nEpoch 00002: val_loss improved from 1.94618 to 1.70342, saving model to Net1\nEpoch 3/300\n3556/3556 [==============================] - 5s 1ms/step - loss: 1.6667 - val_loss: 1.6131\n\nEpoch 00003: val_loss improved from 1.70342 to 1.61305, saving model to Net1\nEpoch 4/300\n2848/3556 [=======================>......] - ETA: 0s - loss: 1.5909","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = prediction_train.argmax(axis = 1)\nnum_correct = 0\nfor i in range(y_array.shape[0]) :\n    if pred_y[i] == y_array[i] :\n        num_correct += 1\n        \nprint(\"Score on Training Data =\", num_correct / y_array.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"457ccbcbe19cdb512d317ad4d87482d819b4dd15"},"cell_type":"code","source":"ss = pd.read_csv('../input/sample_submission.csv')\n# ss['surface'] = encoder.inverse_transform(prediction.astype(int))\nss['surface'] = encoder.inverse_transform(prediction.argmax(axis = 1))\nss.to_csv('rf.csv', index=False)\nss.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}