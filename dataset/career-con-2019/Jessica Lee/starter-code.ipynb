{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Starter Code For CareerCon Challenge\n\"To fully understand and properly navigate a task, however, they need input about their environment. In this competition, you’ll help robots recognize the floor surface they’re standing on using data collected from Inertial Measurement Units (IMU sensors).\""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Data\nWe will now load the train, test, and true labels. Notice that we are going to predict for each series_id, a surface label. We have 487680 X training rows and 3810 rows for our labels. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/X_train.csv')\ntest = pd.read_csv('../input/X_test.csv')\ny = pd.read_csv('../input/y_train.csv')\nprint(train.head())\nprint(train.columns)\nprint(y.head())\nprint(\"Length of Train\", len(train))\nprint(\"Length of Y Labels\", len(y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardizing Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardize all Columns that are not ID's or measurement numbers\ncol = train.columns[3:]\nscaler = StandardScaler()\n# scale the columns that contain the data\nnew_df = scaler.fit_transform(train[col])\nnew_df = pd.DataFrame(new_df, columns=col)\n# Add back index\nnew_df[\"series_id\"] = train['series_id']\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding Y Labels\nWe see that we have 9 different surface types, and we should encode them so that our models can predict the label. "},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(y['surface'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.read_csv('../input/y_train.csv')\ny['surface'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ny = le.fit_transform(y['surface'])\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\nWe have 3810 rows to predict, so we aggregate our time-series data using the groupby function, to make 3810 rows. \n\nTaken from: https://www.kaggle.com/jsaguiar/surface-recognition-baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"def change1(x):\n    return np.mean(np.abs(np.diff(x)))\n\ndef change2(x):\n    return np.mean(np.diff(np.abs(np.diff(x))))\n\ndef feature_extraction(raw_frame):\n    frame = pd.DataFrame()\n    raw_frame['angular_velocity'] = raw_frame['angular_velocity_X'] + raw_frame['angular_velocity_Y'] + raw_frame['angular_velocity_Z']\n    raw_frame['linear_acceleration'] = raw_frame['linear_acceleration_X'] + raw_frame['linear_acceleration_Y'] + raw_frame['linear_acceleration_Z']\n    raw_frame['velocity_to_acceleration'] = raw_frame['angular_velocity'] / raw_frame['linear_acceleration']\n    #raw_frame['acceleration_cumsum'] = raw_frame['linear_acceleration'].cumsum()\n    \n    for col in raw_frame.columns[3:]:\n        frame[col + '_mean'] = raw_frame.groupby(['series_id'])[col].mean()\n        frame[col + '_std'] = raw_frame.groupby(['series_id'])[col].std()\n        frame[col + '_max'] = raw_frame.groupby(['series_id'])[col].max()\n        frame[col + '_min'] = raw_frame.groupby(['series_id'])[col].min()\n        frame[col + '_max_to_min'] = frame[col + '_max'] / frame[col + '_min']\n        \n        # Change 1st order\n        frame[col + '_mean_abs_change'] = raw_frame.groupby('series_id')[col].apply(change1)\n        # Change 2nd order\n        #frame[col + '_mean_abs_change2'] = raw_frame.groupby('series_id')[col].apply(change2)\n        frame[col + '_abs_max'] = raw_frame.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n    return frame\n\ntrain_df = feature_extraction(new_df)\nlen(train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Light Gradient Boosting\nWe will now try to classify using Light Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nimport time\nnum_folds = 10\ntarget = y\n\nparams = {\n    'num_leaves': 18,\n    'min_data_in_leaf': 40,\n    'objective': 'multiclass',\n    'metric': 'multi_error',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    \"boosting\": \"gbdt\",\n    \"bagging_freq\": 5,\n    \"bagging_fraction\": 0.812667,\n    \"bagging_seed\": 11,\n    \"verbosity\": -1,\n    'reg_alpha': 0.2,\n    'reg_lambda': 0,\n    \"num_class\": 9,\n    'nthread': -1\n}\n\nt0 = time.time()\ntrain_set = lgb.Dataset(train_df, label=target)\neval_hist = lgb.cv(params, train_set, nfold=10, num_boost_round=9999,\n                   early_stopping_rounds=100, seed=19)\nnum_rounds = len(eval_hist['multi_error-mean'])\n# retrain the model and make predictions for test set\nclf = lgb.train(params, train_set, num_boost_round=num_rounds)\n\nprint(\"Timer: {:.1f}s\".format(time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = clf.predict(train_df, parameters = None)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.argmax(predictions, axis = 1)\nle.inverse_transform(y_pred)","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"array(['fine_concrete', 'concrete', 'concrete', ..., 'fine_concrete',\n       'tiled', 'soft_pvc'], dtype=object)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Extra Links for Further Exploring!\nPyTorch LSTM: https://www.kaggle.com/artgor/basic-pytorch-lstm\nComplete EDA w/Model Analysis: https://www.kaggle.com/artgor/where-do-the-robots-drive\nCurrent Best Score: https://www.kaggle.com/jesucristo/1-robots-eda-rf-cv-predictions-0-73"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}