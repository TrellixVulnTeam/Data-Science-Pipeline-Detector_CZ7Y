{"cells":[{"metadata":{"_uuid":"3c4c51a8e00661de4e966a7e0cbd3319636a4327"},"cell_type":"markdown","source":"# Intro\nThis kernel has used contributes/snippets from many others:\n\n* Forked from https://www.kaggle.com/prashantkikani/help-humanity-by-helping-robots\n* Feature Engineering from https://www.kaggle.com/jesucristo/1-smart-robots-complete-notebook-0-73\n* Group_id insight from https://www.kaggle.com/c/career-con-2019/discussion/87239#latest-507715\n* Additional Features from https://www.kaggle.com/taigokuriyama/random-forest-baseline\n* ... (other cites are in the code, I apologise if I forgot someone)\n\nAs for me, I tried some solutions which, however, didn't work as expected. For example:\n\n* XGB (the validation error plot is fantastic, but the LB doesn't agree)\n* Groups/No Groups Training of RF/XGB Classifier (ok, a better esteem of validation error, and then?)\n* Group Prediction (with a simple SVM, the accuracy is similar to the one of leakage, but...)\n* PCA (Useless, but the plots are nice)\n* ...\n\nAs a submission, I finally used the output of the basic classifier (\"RF No Groups\") which scored 0.71 in the public LB and 0.61 in the private one.\n\nProbably my best error has been relying too much on the \"distribution hack\" discussion. Until the end, I expected that the real distribution had few surfaces with id=7,8.\n\nAnyway, a great experience. Thanks to Kaggle and congratulations to the Winners!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom seaborn import countplot,lineplot, barplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as sp\nfrom functools import partial\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"tr = pd.read_csv('../input/X_train.csv')\nte = pd.read_csv('../input/X_test.csv')\ntarget = pd.read_csv('../input/y_train.csv')\nss = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr0=tr\nte0=te","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now one important thing that I initially haven't noted."},{"metadata":{"trusted":true,"_uuid":"00b011ff104b7c36ebe96ded9ef6bc22497ea18a"},"cell_type":"code","source":"print(tr.shape[0])\nprint(target.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(te.shape[0])\nprint(ss.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target=pd.read_csv('../input/y_train.csv')\ndf_target.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The label to predict is related to series_id. The measurement_id it's a detail present in the dataset called train and test but is actually we have to make our predictions on ss learning from target. In other words measurement_id is a detail that we have to delete (by grouping)."},{"metadata":{"trusted":true},"cell_type":"code","source":"le=LabelEncoder()\ndf_target['surface_id']=le.fit_transform(df_target['surface'])\ndf_target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target_s=df_target[['surface','surface_id']].groupby(by='surface').max().reset_index()\ndf_target_s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target_cnt=target.groupby(by='surface').count()['series_id']\ndf_target_cnt=df_target_cnt.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target_s=df_target_s.merge(df_target_cnt,on='surface')\ndf_target_s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cfdecf95e8c62486dfc8692324b23563df4a3db"},"cell_type":"code","source":"barplot(x='surface_id',y='series_id',data=df_target_s)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/jesucristo/1-smart-robots-complete-notebook-0-73/notebook\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(tr.iloc[:,3:].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is a group? From the data description: \"ID number for all of the measurements taken in a recording session. Provided for the training set only, to enable more cross validation strategies.\" We'll merge it to the training set. And for the test set we'll invent something ..."},{"metadata":{},"cell_type":"markdown","source":"## Groups ...?"},{"metadata":{},"cell_type":"markdown","source":"OK, now we can study a little better what is a group. Let's take in consideration for example, this one:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target[df_target['group_id']==5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target_g=df_target[['group_id','surface_id']].groupby(by='group_id').count().reset_index()\ndf_target_g.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yg=df_target['group_id'].copy()\nfig,ax = plt.subplots(1,1,figsize=(18,4))\ncountplot(yg, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yg.value_counts(ascending=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yg.value_counts(ascending=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Groups Nr Max Min',yg.nunique(),yg.max(),yg.min())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5aa24ed8d55ca5141a28684fc33186c2068f2303"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"target['surface'] = le.fit_transform(target['surface'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"578d6a588c9305a6c66c2db0c932005f83851994"},"cell_type":"code","source":"def fe_step0 (actual):\n    \n    # https://www.mathworks.com/help/aeroblks/quaternionnorm.html\n    # https://www.mathworks.com/help/aeroblks/quaternionmodulus.html\n    # https://www.mathworks.com/help/aeroblks/quaternionnormalize.html\n        \n    actual['norm_quat'] = (actual['orientation_X']**2 + actual['orientation_Y']**2 + actual['orientation_Z']**2 + actual['orientation_W']**2)\n    actual['mod_quat'] = (actual['norm_quat'])**0.5\n    actual['norm_X'] = actual['orientation_X'] / actual['mod_quat']\n    actual['norm_Y'] = actual['orientation_Y'] / actual['mod_quat']\n    actual['norm_Z'] = actual['orientation_Z'] / actual['mod_quat']\n    actual['norm_W'] = actual['orientation_W'] / actual['mod_quat']\n    \n    return actual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = fe_step0(tr)\nte = fe_step0(te)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_step1 (actual):\n    \"\"\"Quaternions to Euler Angles\"\"\"\n    \n    x, y, z, w = actual['norm_X'].tolist(), actual['norm_Y'].tolist(), actual['norm_Z'].tolist(), actual['norm_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    actual['euler_x'] = nx\n    actual['euler_y'] = ny\n    actual['euler_z'] = nz\n    return actual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feat_eng(data):\n    \n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2)** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2)**0.5\n    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2)**0.5\n    data['totl_angle'] = np.sqrt(data['euler_x'] ** 2 + data['euler_y'] ** 2 + data['euler_z'] ** 2)\n    \n    # absolute values\n    data['linear_acceleration_X_abs'] = data['linear_acceleration_X'].where(data['linear_acceleration_X']>=0, - data['linear_acceleration_X'])\n    data['linear_acceleration_Y_abs'] = data['linear_acceleration_Y'].where(data['linear_acceleration_Y']>=0, - data['linear_acceleration_Y'])\n    data['linear_acceleration_Z_abs'] = data['linear_acceleration_Z'].where(data['linear_acceleration_Z']>=0, - data['linear_acceleration_Z'])\n\n    # vs features\n    data['acc_vs_vel'] = data['totl_linr_acc'] / data['totl_anglr_vel']\n    data['angle_vs_acc'] = data['totl_angle'] / data['totl_linr_acc']\n    data['angle_vs_vel'] = data['totl_angle'] / data['totl_anglr_vel']\n    \n    # interaction features\n    data['acc_int_vel'] = data['totl_linr_acc'] * data['totl_anglr_vel']\n    data['angle_int_acc'] = data['totl_angle'] * data['totl_linr_acc']\n    data['angle_int_vel'] = data['totl_angle'] * data['totl_anglr_vel']\n    data['angle_ints_vel_acc'] = data['totl_angle'] * data['totl_anglr_vel'] * data['totl_linr_acc']\n    \n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    df = pd.DataFrame() \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] / df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_mean_change_of_abs_change'] = data.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])/2\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = fe_step1(tr)\nte = fe_step1(te)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dca271ce733f8c41b9ee32d7079d70edec1dda3"},"cell_type":"code","source":"%%time\ntr = feat_eng(tr)\nte = feat_eng(te)\ntr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point we trashed measurement_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tr.shape)\nprint(te.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling & Fillna"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/donkeys/my-little-eda-with-random-forest\n#function to scale two dataframes. fit and transform the first (e.g., training set), \n#and use the same scaler to transform the seconds one (e.g., test set)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\ndef scale_df(df1, df2, feature_cols):\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(df1[feature_cols])\n    df1[feature_cols] = scaled_features\n    scaled_test_features = scaler.transform(df2[feature_cols])\n    df2[feature_cols] = scaled_test_features\n    return df1, df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr,te=scale_df(tr,te,te.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be25ddf75d90f3f5c58fa5ca6969cbfd8951c59d"},"cell_type":"code","source":"tr.fillna(0, inplace = True)\nte.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1610037f237ff3f03800c77a2992bf08f1d304b8"},"cell_type":"code","source":"tr.replace(-np.inf, 0, inplace = True)\ntr.replace(np.inf, 0, inplace = True)\nte.replace(-np.inf, 0, inplace = True)\nte.replace(np.inf, 0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tr.shape,te.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Group Guessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_cols=[x for x in tr.columns if 'orientation_W' in x]\nsvc_cols=svc_cols+[x for x in tr.columns if 'orientation_Z' in x]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_cols=[x for x in tr.columns if not (x in svc_cols)]+['group_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=tr.join(df_target['group_id'],on='series_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=X['group_id']\nX=X[svc_cols]\nT=te[svc_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape,T.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CV_STEPS=10\nfolds = StratifiedKFold(n_splits=CV_STEPS, shuffle=True, random_state=876)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\n#   SVC Groups Prediction\n#\ni=0\ntst_preds_gr = np.zeros((T.shape[0], CV_STEPS))\noof_preds_gr = np.zeros((X.shape[0]))\nsplitter=folds.split(X,y)\nfor (train_index, valid_index) in (splitter):\n    print('='*20, i, '='*20)\n    X_train = X.iloc[train_index]\n    print('train size:',len(train_index))\n    X_valid = X.iloc[valid_index]\n    print('valid size:',len(valid_index))\n    y_train = y.iloc[train_index]\n    y_valid = y.iloc[valid_index]\n    clf =  SVC(kernel='linear')\n    clf.fit(X_train, y_train)\n    oof_preds_gr[valid_index] = clf.predict(X_valid)\n    tst_preds_gr[:,i] = clf.predict(T)\n    print('score ', accuracy_score(oof_preds_gr[valid_index], y_valid.values))\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extr_test_pred(tst_preds_cv):\n    gr_preds=np.zeros(len(tst_preds_cv),dtype=int)\n    for r in range(0,len(tst_preds_cv)):\n        gr_preds[r]=Counter(tst_preds_cv[r,:]).most_common(1)[0][0]\n    return gr_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr['group_id']=y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te['group_id']=extr_test_pred(tst_preds_gr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tr.shape,te.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=tr[clf_cols]\nT=te[clf_cols]\ny=pd.DataFrame(target['surface'],index=tr.index)\nLABELS=[0,1,2,3,4,5,6,7,8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.to_csv('X.csv', index=False)\nT.to_csv('T.csv', index=False)\ny.to_csv('y.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"CV_STEPS=30\nfolds = StratifiedKFold(n_splits=CV_STEPS, shuffle=True, random_state=1984)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groups=yg.unique()\ndummy=np.zeros(len(groups))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_id_drop(W):\n    try:\n        W.drop('group_id',axis=1,inplace=True)\n    except:\n        print('Info: group_id already dropped.')\n    return(W)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a24eae22ebce03caa80241cd0583b0c937d8aa31"},"cell_type":"code","source":"#\n#   Random Forest Groups Based\n#\ntst_preds_rf = np.zeros((T.shape[0], CV_STEPS))\noof_preds_rf = np.zeros((X.shape[0]))\nimportances = np.zeros((CV_STEPS,X.shape[1]-1)) # group_id will be deleted\ni=0\nsplitter=folds.split(groups,dummy)\nfor (tr_gr_idx, va_gr_idx) in (splitter):\n    print('-'*20, i, '-'*20)\n    tr_groups=groups[tr_gr_idx]\n    va_groups=groups[va_gr_idx]\n    X_train = X[X['group_id'].isin(tr_groups)]\n    X_train.drop('group_id',axis=1,inplace=True)\n    train_index = X_train.index\n    print('train size:',len(train_index))\n    X_valid = X[X['group_id'].isin(va_groups)]\n    X_valid.drop('group_id',axis=1,inplace=True)\n    valid_index = X_valid.index\n    print('valid size:',len(valid_index))\n    y_train = y.iloc[train_index]\n    curr_labels=y_train['surface'].unique()\n    print('used labels:',len(curr_labels),' [',','.join([str(c) for c in curr_labels]),']')\n    y_valid = y.iloc[valid_index]\n    clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n    clf.fit(X_train, y_train)\n    oof_preds_rf[valid_index] = clf.predict(X_valid)\n    TWG=T.drop('group_id',axis=1)\n    tst_preds_rf[:,i] = clf.predict(TWG)\n    ass_labels=np.unique(tst_preds_rf[:,i])\n    print('assigned labels:',len(ass_labels),' [',','.join([str(int(c)) for c in ass_labels]),']')\n    print('score ', accuracy_score(oof_preds_rf[valid_index], y_valid.values))\n    importances[i,0:X.shape[1]] = clf.feature_importances_\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hists(oof,sub,title_oof,title_sub):\n    fig,ax=plt.subplots(1,3,figsize=(12,4))\n    countplot(target['surface'], ax=ax[0], order=LABELS).set_title('Train (Labels)')\n    countplot(oof, ax=ax[1],order=LABELS).set_title(title_oof)\n    countplot(sub, ax=ax[2],order=LABELS).set_title(title_sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/artgor/where-do-the-robots-drive\nimport itertools\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tst_preds_rf0 = tst_preds_rf\noof_preds_rf0 = oof_preds_rf\nimportances0 = importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_rf0=extr_test_pred(tst_preds_rf0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(target['surface'], oof_preds_rf0, le.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hists(oof_preds_rf0,preds_rf0,'OOF Train (RF Groups)','Test (RF Groups)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(target['surface'], oof_preds_rf0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"#\n#   Random Forest No Groups\n#\ni=0\ntst_preds_rf = np.zeros((T.shape[0], CV_STEPS))\noof_preds_rf = np.zeros((X.shape[0]))\nimportances = np.zeros((CV_STEPS,X.shape[1]-1)) # group_id will be deleted\nsplitter=folds.split(X,y)\nfor (train_index, valid_index) in (splitter):\n    print('#'*20, i, '#'*20)\n    X_train = X.iloc[train_index]\n    print('train size:',len(train_index))\n    X_valid = X.iloc[valid_index]\n    print('valid size:',len(valid_index))\n    y_train = y.iloc[train_index]\n    curr_labels=y_train['surface'].unique()\n    X_train=group_id_drop(X_train)\n    X_valid=group_id_drop(X_valid)\n    print('used labels:',len(curr_labels),' [',','.join([str(c) for c in curr_labels]),']')\n    y_valid = y.iloc[valid_index]\n    clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n    clf.fit(X_train, y_train)\n    oof_preds_rf[valid_index] = clf.predict(X_valid)\n    TWG=T.drop('group_id',axis=1)\n    tst_preds_rf[:,i] = clf.predict(TWG)\n    ass_labels=np.unique(tst_preds_rf[:,i])\n    print('assigned labels:',len(ass_labels),' [',','.join([str(int(c)) for c in ass_labels]),']')\n    print('score ', accuracy_score(oof_preds_rf[valid_index], y_valid.values))\n    importances[i,0:X.shape[1]] = clf.feature_importances_\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_rf=extr_test_pred(tst_preds_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tst_preds_rf[0,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_rf[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfi=pd.DataFrame(index=X_train.columns)\ndfi['importance']=importances.mean(axis=0)\ndfi=dfi.sort_values(by='importance',ascending=False)\ndfi.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfi=dfi.head(20)\nfig,ax=plt.subplots(1,1,figsize=(18,10))\nbarplot(y=dfi.index,x=dfi['importance'],ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e76f6c727d9d0a076ba399a7251901081013060"},"cell_type":"code","source":"plot_confusion_matrix(target['surface'], oof_preds_rf, le.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hists(oof_preds_rf,preds_rf,'OOF Train (RF Classifier)','Test (RF Classifier)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(target['surface'], oof_preds_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# It doesn't work, but I want to keep this section anyway\nCV_STEPS=10\nfolds = StratifiedKFold(n_splits=CV_STEPS, shuffle=True, random_state=1984)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NR_EPOCHS=1000\nxgb_params = {\n    'booster' : 'gbtree',\n    'objective' : 'multi:softprob',\n    'num_class' : 9,\n    'seed': 1984,\n    'eta': 0.01,\n    'max_depth': 3,\n    'gamma': 0.1,\n    'lambda': 0.8,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'tree_method': 'gpu_hist',\n    'device': 'gpu',\n    'silent': 1,\n}\nverbose_eval = 100\nearly_stop = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_train_no_groups(params, X, y, T):\n    oof_train_preds = np.zeros(X.shape[0])\n    tst_preds = np.zeros((T.shape[0],CV_STEPS))\n    perf_dict_list=[]\n    i=0\n    splitter=folds.split(X,y)\n    for (train_index, valid_index) in (splitter):\n        print('#'*20, i, '#'*20)\n        X_train = X.iloc[train_index]\n        print('train size:',len(train_index))\n        X_valid = X.iloc[valid_index]\n        print('valid size:',len(valid_index))\n        y_train = y.iloc[train_index]\n        curr_labels=y_train['surface'].unique()\n        X_train=group_id_drop(X_train)\n        X_valid=group_id_drop(X_valid)\n        print('used labels:',len(curr_labels),' [',','.join([str(c) for c in curr_labels]),']')\n        y_valid = y.iloc[valid_index]\n\n        d_train = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n        d_valid = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_valid.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        perf=dict()\n        model = xgb.train(dtrain=d_train, num_boost_round=NR_EPOCHS, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params,\n                         evals_result=perf)\n\n        valid_pred = model.predict(xgb.DMatrix(X_valid, feature_names=X_valid.columns), ntree_limit=model.best_ntree_limit)\n        TWG=T.drop('group_id',axis=1)\n        test_pred = model.predict(xgb.DMatrix(TWG, feature_names=TWG.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train_preds[valid_index] = valid_pred.argmax(axis=1)\n        tst_preds[:,i] = test_pred.argmax(axis=1)\n        perf_dict_list = perf_dict_list + [perf]\n        \n        print('Accuracy =',accuracy_score(target['surface'][valid_index], valid_pred.argmax(axis=1)))\n\n        i += 1\n        \n    return model, oof_train_preds, tst_preds, perf_dict_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_train_groups_based(params, X, y, T):\n    oof_train_preds = np.zeros(X.shape[0])\n    tst_preds = np.zeros((T.shape[0],CV_STEPS))\n    perf_dict_list=[]\n    i=0\n    splitter=folds.split(groups,dummy)\n    for (tr_gr_idx, va_gr_idx) in (splitter):\n        print('-'*20, i, '-'*20)\n        tr_groups=groups[tr_gr_idx]\n        va_groups=groups[va_gr_idx]\n        X_train = X[X['group_id'].isin(tr_groups)]\n        X_train.drop('group_id',axis=1,inplace=True)\n        train_index = X_train.index\n        print('train size:',len(train_index))\n        X_valid = X[X['group_id'].isin(va_groups)]\n        X_valid.drop('group_id',axis=1,inplace=True)\n        valid_index = X_valid.index\n        print('valid size:',len(valid_index))\n        y_train = y.iloc[train_index]\n        curr_labels=y_train['surface'].unique()\n        print('used labels:',len(curr_labels))\n        y_valid = y.iloc[valid_index]\n\n        d_train = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n        d_valid = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_valid.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        perf=dict()\n        model = xgb.train(dtrain=d_train, num_boost_round=NR_EPOCHS, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params,\n                         evals_result=perf)\n\n        valid_pred = model.predict(xgb.DMatrix(X_valid, feature_names=X_valid.columns), ntree_limit=model.best_ntree_limit)\n        TWG=T.drop('group_id',axis=1)\n        test_pred = model.predict(xgb.DMatrix(TWG, feature_names=TWG.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train_preds[valid_index] = valid_pred.argmax(axis=1)\n        tst_preds[:,i] = test_pred.argmax(axis=1)\n        perf_dict_list = perf_dict_list + [perf]\n        \n        print('Accuracy =',accuracy_score(target['surface'][valid_index], valid_pred.argmax(axis=1)))\n\n        i += 1\n        \n    return model, oof_train_preds, tst_preds, perf_dict_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel0, oof_preds_xgb0, tst_preds_xgb0, perf = xgb_train_groups_based(xgb_params, X,y,T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_xgb0=extr_test_pred(tst_preds_xgb0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hists(oof_preds_xgb0,preds_xgb0,'OOF Train (XGB Groups)','Test (XGB Groups)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(target['surface'], oof_preds_xgb0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel, oof_preds_xgb, tst_preds_xgb, perf = xgb_train_no_groups(xgb_params, X,y,T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12,48))\nxgb.plot_importance(model,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12,8))\nfor i in range(0,CV_STEPS):\n    ax.plot(perf[i]['train']['merror'], color='blue')\n    ax.plot(perf[i]['valid']['merror'], color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_xgb=extr_test_pred(tst_preds_xgb)\npreds_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_preds_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hists(oof_preds_xgb,preds_xgb,'OOF Train (XGB)','Test (XGB)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(target['surface'], oof_preds_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction Choice and Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds=preds_rf0\n# preds=preds_rf\n# preds=preds_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss['surface'] = le.inverse_transform(preds)\nss['surface_id'] = preds\nss.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplot(ss['surface_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Attended Distribution in Test Set (https://www.kaggle.com/donkeys/distribution-hack)\nattended_prc=[0.06,0.16,0.09,0.06,0.10,0.17,0.23,0.03,0.06]\nattended_nr=[p * len(ss) for p in attended_prc]\nbarplot(y=attended_nr,x=LABELS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lots of 7,8,1 (in the first plot) have to become 6 and 3. At first, is it possible to understand what are the main difference between these confused features? For this purpose we will use groups (real or predicted)."},{"metadata":{"trusted":true},"cell_type":"code","source":"XC=X.copy()\nTC=T.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=3, svd_solver='full',random_state=1984)\npca.fit(XC)\nxpc= pca.transform(XC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XPC = pd.DataFrame(data = xpc , columns = ['PC1', 'PC2','PC3'])\nXPC['surface_id'] = y\nXPC.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pca_plot_2d(XF):\n    sns.lmplot( x=\"PC1\", y=\"PC2\", data=XF, fit_reg=False, hue='surface_id', legend=True,\n               scatter_kws={\"s\": 80}, height=10, aspect=2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_sufaces=[1,3,5,6,7,8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\ndef pca_plot_3d(XF):\n    fig = plt.figure(figsize=(10,10))\n    ax = Axes3D(fig)\n    ax.set_xlabel('PC1')\n    ax.set_ylabel('PC2')\n    ax.set_zlabel('PC3')\n    ax.scatter(XF['PC1'],XF['PC2'],XF['PC3'],c=XF['surface_id'],cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XF=XPC[XPC['surface_id'].isin(selected_sufaces)]\npca_plot_2d(XF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group_id is practically perfect to predict the output: one group_id = one surface type (and one PC1 value). Below a 3D plot, nice but (at least for me), useless."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_plot_3d(XF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we haven't the group_id for the test set. At first we can try to segment test set (with the predicted group) using the same components of the train set. Here is what we got:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pcat=pca\n#pcat = PCA(n_components=3, svd_solver='full',random_state=1984)\n#pcat.fit(TC)\ntpc=pcat.transform(TC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TPC = pd.DataFrame(data = tpc , columns = ['PC1','PC2','PC3'])\nTPC['surface_id'] = preds\nTPC.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TF=TPC[TPC['surface_id'].isin(selected_sufaces)]\npca_plot_2d(TF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, the segmentation has some problems also because the group_id has accuracy=0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_plot_3d(TF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the coherence between predicted series_id and group_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"coh = pd.DataFrame()\ncoh['prev_surface_id']=preds\ncoh['prev_group_id']=TC['group_id']\ncoh['prev_id']=coh.index\ncoh.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs=df_target[['group_id','surface_id']].groupby(by=['group_id','surface_id']).first().reset_index()\ngs[gs['surface_id']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_merge=coh.merge(gs,left_on=['prev_surface_id','prev_group_id'],right_on=['surface_id','group_id'],how='left')\nprev_merge=prev_merge.groupby('prev_id').first()\nprev_merge.fillna(-1, inplace=True)\nprev_merge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor i in range(0,len(prev_merge)):\n    gr=prev_merge['group_id'].loc[i]\n    prev_gr=prev_merge['prev_group_id'].loc[i]\n    if (gr==-1):\n        prev_merge['surface_id'].loc[i]=gs['surface_id'][gs['group_id']==prev_gr].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_merge=prev_merge.astype(int)\nprev_merge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplot(prev_merge['surface_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_merge['final_surface_id']=prev_merge['prev_surface_id']\nprev_merge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def counter_distance(cnt,ref):\n    vals=cnt.values()\n    tot=sum(vals)\n    cnt=sorted(cnt.items())\n    dist=0\n    for i in range(0,len(cnt)):\n        dist+=np.abs(ref[i]-cnt[i][1]/tot)\n    return dist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_freq=[]\ncorrections=0\nfor i in range(0,len(prev_merge)):\n    gr=prev_merge['group_id'].loc[i]\n    if (gr==-1):\n        hp1=prev_merge['final_surface_id']\n        hp2=hp1.copy()\n        hp2[i]=prev_merge['surface_id'].loc[i]\n        d_hp1=counter_distance(Counter(hp1),attended_prc)\n        d_hp2=counter_distance(Counter(hp2),attended_prc)\n        if (d_hp2<d_hp1):\n            d_freq=d_freq+[d_hp2]\n            #print('Surface subst from={h1} to from={h2}'.format(h1=d_hp1,h2=d_hp2))\n            corrections=corrections+1\n            prev_merge['final_surface_id'] = hp2\n        else:\n            d_freq=d_freq+[d_hp1]\nprint('Corrections made=',corrections)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(y=d_freq,x=range(0,len(d_freq)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_merge['final_surface_id'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(prev_merge['prev_surface_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(prev_merge['final_surface_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplot(prev_merge['prev_surface_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplot(prev_merge['final_surface_id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Actual Submission"},{"metadata":{},"cell_type":"markdown","source":"This is the prediction come out from classifier:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the correction (comment/uncomment to apply/unapply):"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ss['surface'] = le.inverse_transform(prev_merge['final_surface_id'])\n#ss['surface_id'] = prev_merge['final_surface_id']\nss.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.drop('surface_id',axis=1,inplace=True)\nss.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}