{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## Table of content:\n    1. Data processing\n    2. FCN    \n    3. ResNet\n    4. Fit model\n    5. Hyperparameter tunning\n\nSee detailed steps in the comments.\n\nRefer to the competition page for details: https://www.kaggle.com/c/career-con-2019","metadata":{}},{"cell_type":"code","source":"!pip install hyperas\n!pip install hyperopt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Conv1D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, GlobalAveragePooling1D\nfrom keras.models import Model\nfrom sklearn.model_selection import GroupKFold\n\nfrom hyperopt import Trials, STATUS_OK, tpe\nfrom hyperas import optim\nfrom hyperas.distributions import choice, uniform","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data processing","metadata":{}},{"cell_type":"code","source":"# Import data\nX_train = pd.read_csv('../input/X_train.csv').iloc[:,3:].values.reshape(-1,128,10)\nX_test  = pd.read_csv('../input/X_test.csv' ).iloc[:,3:].values.reshape(-1,128,10)\nprint('X_train shape:', X_train.shape, ', X_test shape:', X_test.shape)\n\ndfy= pd.read_csv('../input/y_train.csv')\n# Get groups for CV later\ngroups= dfy.iloc[:,1].values\nY_train=dfy.iloc[:,-1]\n# Convert to one-hot for classes\nnum_classes = len(Y_train.unique())\nY_train = Y_train.replace(Y_train.unique(),range(num_classes))\nY_train = to_categorical(Y_train.values,num_classes)\nprint('Y_train shape:', Y_train.shape)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. FCN\nThe FCN used in this work consists of 3 convolutional blocks, each composed by a 1-dimensional convolution followed by a batch normalization layer and a rectified linear unit (ReLU) activation function. The output of the last convolutional block are fed to the GAP layer, to which a traditional softmax is fully connected for the time series classification.","metadata":{"trusted":true}},{"cell_type":"code","source":"def model_FCN(input_shape=X_train.shape[1:], filters=1, kernel_size=1, s=1, units=num_classes):\n    \n    # Define the input placeholder as a tensor with shape input_shape.\n    X_input = Input(input_shape)\n\n    # Zero-Padding: none\n\n    # CONV -> BN -> RELU Block\n    X = Conv1D(filters, kernel_size, strides=s)(X_input)\n    X = BatchNormalization(axis = 2)(X)\n    X = Activation('relu')(X)\n    \n    # CONV -> BN -> RELU Block\n    X = Conv1D(filters, kernel_size, strides=s)(X)\n    X = BatchNormalization(axis = 2)(X)\n    X = Activation('relu')(X)\n    \n    # CONV -> BN -> RELU Block\n    X = Conv1D(filters, kernel_size, strides=s)(X)\n    X = BatchNormalization(axis = 2)(X)\n    X = Activation('relu')(X)\n\n    # MAXPOOL - none\n    \n    # GAP\n    X = GlobalAveragePooling1D()(X)\n    \n    # FLATTEN - none\n    \n    # FULLYCONNECTED\n    X = Dense(units, activation='softmax',name='d0')(X)\n\n    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n    model = Model(inputs = X_input, outputs = X)\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. ResNet\nThe ResNet here consists of 3 residual blocks, each composed of three 1-dimensional convolutional layers, and their output is added to input of the residual block. The last residual block, as for the FCN, is followed by a GAP layer and a softmax.","metadata":{}},{"cell_type":"code","source":"def model_ResNet(input_shape=X_train.shape[1:], filters=1, kernel_size=1, s=1, units=num_classes):\n    \n    # Define the input placeholder as a tensor with shape input_shape.\n    X_input = Input(input_shape)\n\n    # Zero-Padding: none\n\n    # CONV -> BN -> RELU Block\n    X = Conv1D(filters, kernel_size, strides=s)(X_input)\n    X = Add()([X,X_input])\n    \n    # CONV -> BN -> RELU Block\n    X = Conv1D(filters, kernel_size, strides=s)(X)\n    X = Add()([X,X_input])\n    \n    # CONV -> BN -> RELU Block\n    X = Conv1D(filters, kernel_size, strides=s)(X)\n    X = Add()([X,X_input])\n\n    # MAXPOOL - none\n    \n    # GAP\n    X = GlobalAveragePooling1D()(X)\n    \n    # FLATTEN - none\n    \n    # FULLYCONNECTED\n    X = Dense(units, activation='softmax',name='d0')(X)\n\n    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n    model = Model(inputs = X_input, outputs = X)\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Fit model","metadata":{}},{"cell_type":"code","source":"# Define model parameters\ninput_shape = X_train.shape[1:]\nfilters = 1\nkernel_size = 1\ns = 1\nepochs = 1\nbatch_size = 32\nfolds=2\nmodel_Name = \"model_FCN\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define 10-fold cross validation test harness\ncvloss = []\ncvloss_val = []\ncvacc = []\ncvacc_val = []\ngkf = GroupKFold(n_splits=folds)\n\nfor train_idx,valid_idx in gkf.split(X_train,Y_train,groups=groups):\n    # Create and compile the FCN model\n    model = model_FCN(input_shape, filters, kernel_size,s,num_classes) if model_Name is \"model_FCN\" else\\\n                    model_ResNet(input_shape, filters, kernel_size,s,num_classes)\n    model.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])   \n    # Fit and evaluate the model\n    history = model.fit(x=X_train[train_idx],y=Y_train[train_idx],epochs=epochs,\\\n                            validation_data=(X_train[valid_idx],Y_train[valid_idx]),shuffle=True,verbose=2)\n    # Update score\n    cvloss.append(history.history['loss'][-1])\n    cvloss_val.append(history.history['val_loss'][-1])\n    cvacc.append(history.history['acc'][-1])\n    cvacc_val.append(history.history['val_acc'][-1])\n    \n    '''\n    # Plot loss during training\n    plt.subplot(121)\n    plt.title('Loss in Fold '+str(f))\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='valid')\n    plt.legend()\n    \n    # Plot accuracy during training\n    plt.subplot(122)\n    plt.title('Accuracy in Fold '+str(f))\n    plt.plot(history.history['acc'], label='train')\n    plt.plot(history.history['val_acc'], label='valid')\n    plt.legend()\n    plt.show()   \n    '''\n    \nprint(\"Avg loss: \", np.mean(cvloss), \"Avg acc: \", np.mean(cvacc))\nprint(\"Avg val_loss: \", np.mean(cvloss_val), \"Avg val_acc: \", np.mean(cvacc_val))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Hyperparameter tunning\nWe are using hyperas to tune epochs, filters, batch_size","metadata":{}},{"cell_type":"code","source":"def data():\n    global X_train\n    global Y_train   \n    return X_train,Y_train,X_train, Y_train,","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(x_train, y_train, x_test, y_test):\n\n    model = model_FCN(filters={{choice([1,2,3,4,5])}})\n    model.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])   \n    result = model.fit(x=x_train,y=y_train,\\\n                        batch_size={{choice([64, 128])}},\\\n                        epochs={{choice([64, 128])}},\\\n                        validation_split=0.1,shuffle=True,verbose=2)\n\n    #get the highest validation accuracy of the training epochs\n    validation_acc = np.amax(result.history['val_acc']) \n    print('Best validation acc of epoch:', validation_acc)\n    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_run, best_model = optim.minimize(model=create_model,\n                                      data=data,\n                                      algo=tpe.suggest,\n                                      max_evals=5,\n                                      trials=Trials(),\n                                      notebook_name='2019CareerCon_Help_Navigate_Robot')\nprint(\"Evalutation of best performing model:\")\nprint(best_model.evaluate(X_train, Y_train))\nprint(\"Best performing model chosen hyper-parameters:\")\nprint(best_run)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}