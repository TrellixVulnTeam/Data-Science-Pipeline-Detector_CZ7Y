{"cells":[{"metadata":{"_uuid":"98a1321feef1784d3e0db54b7f4f3ef64e56af4b"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"### This is simple exploratory data analysis coupled with naive approcah for classification of the data points given"},{"metadata":{"_uuid":"199204e50bb8b13853b712c23c10a8b5448276dd"},"cell_type":"markdown","source":"Importing the required packages "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c02ce63001c2203176f80ab9e91e88e316bec7e"},"cell_type":"code","source":"print(\"Files in the folder:\")\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9ef509b9d35da7429def3d1610014bd12e9abed"},"cell_type":"code","source":"train = pd.read_csv('../input/X_train.csv')\ntest = pd.read_csv('../input/X_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e858bfa33bb505c3c16ae7b002db95f6df4d234"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19cc609d0a51313cc518467a2298c680928d264c"},"cell_type":"markdown","source":"Each series has 128 entries as mentioned in the competetion page and row_id and measurement number are only identfires withing series\nFour orientations, angular velocities in 3 directions, linear accelerations in 3 direction is given for each measurement. Lets see the distribution of variables and class wise distributions as well\nBelow functions are copied as is from kernel [#1 Robots EDA+RF+predictions ðŸ¤–](https://www.kaggle.com/jesucristo/1-robots-eda-rf-predictions-0-72)"},{"metadata":{"trusted":true,"_uuid":"7a64571c741972d8e3cb421c61a045aa7cbffefa"},"cell_type":"code","source":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,5,figsize=(16,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,5,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();\n\ndef plot_feature_class_distribution(classes,tt, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,2,figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5,2,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.kdeplot(ttc[feature], bw=0.5,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3309ea3e5c6b5d77fa114dd2821071258d8a6067"},"cell_type":"code","source":"features = train.columns.values[3:]\nplot_feature_distribution(train, test, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df420b893942516d5be849ccc763703ee40006d9"},"cell_type":"markdown","source":"More or less distribution in trainng and test data are similar, now lets look at individual classses in training dataset."},{"metadata":{"trusted":true,"_uuid":"f79de9052dfdb5402a8d717cf3d98ffe1874c359"},"cell_type":"code","source":"labels = pd.read_csv('../input/y_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32f4c3a5861eb773da5ec5a1767d1f45ac57812c"},"cell_type":"code","source":"classes = (labels['surface'].value_counts()).index\naux = train.merge(labels, on='series_id', how='inner')\nplot_feature_class_distribution(classes, aux, features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"657a5f63584d7bcf1aa18489cba154c77e539d65"},"cell_type":"markdown","source":"Difference among different classes are visible in orientation and linear acceleration, but not much in angular velocities."},{"metadata":{"_uuid":"c9fdc0a6ec8d9a85a11ee8f267f16eaaceda10a7"},"cell_type":"markdown","source":"## Lets take simple features and see how model works out using them, I will be taking min, max, mean, median and var of the varibles for one series of measurements and will use them and features."},{"metadata":{"trusted":true,"_uuid":"d75128f582da3a6e54b11ba4e03bac51ae8c2796"},"cell_type":"code","source":"# first drop columns such as measurement and row_id in training data\ntrain_d = train.drop(['row_id', 'measurement_number'], axis=1)\ntest_d = test.drop(['row_id','measurement_number'], axis=1)\ntrain_f = train_d.groupby('series_id').agg(['min', 'max', 'mean', 'median', 'var'])\ntest_f = test_d.groupby('series_id').agg(['min', 'max', 'mean', 'median', 'var'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6799a49de13c3f7c2b4b8890f35c4f88950582ce"},"cell_type":"code","source":"# lets see what we got\ntrain_f.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"69912dad18129bbefb33bd9015a136eb1a0632cd"},"cell_type":"code","source":"test_f.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16c9b8b0dbc4b7cc0edf86ca713692b02ba71771"},"cell_type":"code","source":"# I am going to write to temporary files and then use them for model building\n# add surface to the training points\n#aux = train_f.merge(labels, on='series_id', how='inner')\n#aux.to_csv('training.csv', index=False, header=None)\n#test_f.to_csv(input/testing.csv', index=False, header=None)\ntrain_f['surface'] = labels['surface']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14fc328b2238e18720311cdd5dee2c20de4e20b3"},"cell_type":"code","source":"train_f.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ca5474d262ff056c934209c32d344e173c802c8"},"cell_type":"code","source":"train_f.to_csv('training.csv', index=False, header=None)\ntest_f.to_csv('testing.csv', index=False, header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb3e3da82d869f923c0a8137d48f4a0263c99abf"},"cell_type":"code","source":"# load the traing data \ndata = pd.read_csv('training.csv', header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4377303158f208a4fee2dec815c3dd64d88b509c"},"cell_type":"code","source":"test_data = pd.read_csv('testing.csv', header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23d6314f953ac2b262627e77e0ebe742b6c7a69d"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n# lets use GBM and AdaBoost, see how these work out\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca4d64511949c6eb16cd86b5236486920db5b2f9"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15f803ce5aa28b6cd7c353432dfa189d07d1ff20"},"cell_type":"code","source":"# First lets see AdaBoost  \nmodel_ab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=7), random_state=41)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6964b86c101b4bd1f2bee80fdd30bba1f141274c"},"cell_type":"code","source":"train, valid = train_test_split(data, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1574034e78198d6d12e48f86fe0067115d9fe4f8"},"cell_type":"code","source":"model_ab.fit(train.loc[:, 0:49], train[50])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"060a8d55f6c47bdf2d4b864214834347736a50ad"},"cell_type":"code","source":"predict_train = model_ab.predict(train.loc[:, 0:49])\nprint(classification_report(train[50], predict_train))\nconfusion_matrix(train[50], predict_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e19a10aba2c468865805c91fcd3ed4f23ce5d18e"},"cell_type":"markdown","source":"Overall training results are looking resonable, lets see how it works on validation set"},{"metadata":{"trusted":true,"_uuid":"a3fc8f076223cb6c1387a1ac4026f1437a1a3328"},"cell_type":"code","source":"predict_valid = model_ab.predict(valid.loc[:, 0:49])\nprint(classification_report(valid[50], predict_valid))\nconfusion_matrix(valid[50], predict_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47cf0ef5dea3cd3b7da882d8603e6660a2dadb3e"},"cell_type":"markdown","source":"## Lets submit this to see score, I have already submitted once with this approach using random forest which gave score of 0.65, I think this would be lesser than 0.65 "},{"metadata":{"trusted":true,"_uuid":"a4d921ecf31c3ece90f726a35fab1724fdbd3a5a"},"cell_type":"code","source":"test_predict = model_ab.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"971f0bcf7e50a42d566e6931535798ee8d4ee905"},"cell_type":"code","source":"# read sample submission file\nsubmit = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8685df66ee9fb0c28ddd4a501985ed243849accf"},"cell_type":"code","source":"submit['surface'] = test_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f71bcf1fd796496040bd584fa3421dad0463278"},"cell_type":"code","source":"submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e484c192b7b86fd6d1a3c4d12a047501513bb58"},"cell_type":"code","source":"submit.to_csv('naive_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c2ba4ab354be48b86a1d0c4f4e2938da1850adf"},"cell_type":"code","source":"more naive_submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68f5384fac9a4902953cb5deedb54cbd0f4847a4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}