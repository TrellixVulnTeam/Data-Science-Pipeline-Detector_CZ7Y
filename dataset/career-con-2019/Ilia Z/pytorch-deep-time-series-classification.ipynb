{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Deep Time Series Classification\n\nThe time series classification problem seems to be a great choice to apply Deep Learning models. However, even deep models cannot magically give you good results if the data wasn't propertly prepared. \n\nThe [CareerCon 2019 competition](https://www.kaggle.com/c/career-con-2019) was all about time series classification. In [one of my previous kernels](https://www.kaggle.com/purplejester/a-simple-lstm-based-time-series-classifier), I've tried to apply LSTM model to the dataset and didn't get too impressive accuracy. Also, I was experimenting with 1-d convolutions but again without any luck. So finally, I decided to go with [a more simple apporach](https://www.kaggle.com/purplejester/the-best-friend-of-an-alchemist). Nevertheless, when the competition was ended, [one of the best solutions](https://www.kaggle.com/prith189/starter-code-for-3rd-place-solution) implemented by [prith189](https://www.kaggle.com/prith189) uses Deep Learning to achieve a decent result on both public and private leaderboard.\n\nIn this notebook, we're going to use PyTorch to create a clone of the mentioned solution and see if we can improve it a bit using modern training techniques.\n\n<img src=\"https://i.imgur.com/XWrG9ys.png\" width=\"500\"></img>\n\n## Imports\n\nIn addition to PyTorch, we use \"standard\" Python's data science stack. Also, there are couple of additional functions from the standard library used in utils and snippets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom functools import partial\nfrom multiprocessing import cpu_count\nfrom pathlib import Path\nfrom textwrap import dedent","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path to sample submission\nsample = Path.cwd().parent/'input'/'career-con-2019'/'sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading The Preprocessed Dataset\n\nAs it was already mentioned above, we use a preprocessed data from [this kernel](https://www.kaggle.com/prith189/starter-code-for-3rd-place-solution) to train our models. The preprocessed data was added as a custom dataset. The major differences from the \"raw\" data:\n1. the `orientaion_[XYZW]` columns were converted into Euler angles (original columns were dropped);\n2. each measurment sequence was shifted to have zero mean;\n3. the whole dataset was normalized into `[-1, +1]` range;\n4. the linear and angular accelerations and velocities were processed with `np.fft.rfft` call and saved as an _additional_ dataset to extend the original data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"ROOT = Path.cwd().parent/'input'/'career-con-2019-preprocessed-data'\nenc = joblib.load(ROOT/'encoder.model')\nraw_arr = np.load(ROOT/'feat.npy').transpose(0, 2, 1)\nfft_arr = np.load(ROOT/'feat_fft.npy').transpose(0, 2, 1)\ntarget = np.load(ROOT/'target.npy')\nprint(dedent(f'''\nDataset shapes:\n    raw: {raw_arr.shape}\n    fft: {fft_arr.shape}\n    target: {target.shape}\n'''))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please note that the testing and training data are concatenated into a single Numpy array. The first `3810` rows are the training samples. The rest of rows represent the testing data with dummy labels.\n\n## PyTorch Datasets and Data Loaders\n\nWe create three datasets and data loaders for them to make the data ready for model's training. The process is straightforward. We split the labelled data into two subsets, and keep testing data as is. Also, we convert Numpy arrays into `torch.tensor` objects of proper type (float for samples, and long - for targets)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_datasets(data, target, train_size, valid_pct=0.1, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw, fft = data\n    assert len(raw) == len(fft)\n    sz = train_size\n    idx = np.arange(sz)\n    trn_idx, val_idx = train_test_split(\n        idx, test_size=valid_pct, random_state=seed)\n    trn_ds = TensorDataset(\n        torch.tensor(raw[:sz][trn_idx]).float(), \n        torch.tensor(fft[:sz][trn_idx]).float(), \n        torch.tensor(target[:sz][trn_idx]).long())\n    val_ds = TensorDataset(\n        torch.tensor(raw[:sz][val_idx]).float(), \n        torch.tensor(fft[:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    tst_ds = TensorDataset(\n        torch.tensor(raw[sz:]).float(), \n        torch.tensor(fft[sz:]).float(), \n        torch.tensor(target[sz:]).long())\n    return trn_ds, val_ds, tst_ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_loaders(data, bs=128, jobs=0):\n    \"\"\"Wraps the datasets returned by create_datasets function with data loaders.\"\"\"\n    \n    trn_ds, val_ds, tst_ds = data\n    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=jobs)\n    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    tst_dl = DataLoader(tst_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    return trn_dl, val_dl, tst_dl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we're going to create a couple of helper classes to build a classifier. The `torch` framework doesn't have a dedicated `SeparableConv` layers. But we easily can replicate them with the following class. (Which was taken from one of the PyTorch forum's threads). As the code below shows, the separable convolution is a pretty simple thing: two convolutions following one another. The major purpose of this type of layer is to [reduce the number of model's parameters](https://arxiv.org/pdf/1704.04861.pdf)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class _SepConv1d(nn.Module):\n    \"\"\"A simple separable convolution implementation.\n    \n    The separable convlution is a method to reduce number of the parameters \n    in the deep learning network for slight decrease in predictions quality.\n    \"\"\"\n    def __init__(self, ni, no, kernel, stride, pad):\n        super().__init__()\n        self.depthwise = nn.Conv1d(ni, ni, kernel, stride, padding=pad, groups=ni)\n        self.pointwise = nn.Conv1d(ni, no, kernel_size=1)\n\n    def forward(self, x):\n        return self.pointwise(self.depthwise(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, we extend a bit the separable convolution layer with activation layer and dropout to simplify the layers initialization process and reduce the number of code lines in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"class SepConv1d(nn.Module):\n    \"\"\"Implementes a 1-d convolution with 'batteries included'.\n    \n    The module adds (optionally) activation function and dropout layers right after\n    a separable convolution layer.\n    \"\"\"\n    def __init__(self, ni, no, kernel, stride, pad, drop=None,\n                 activ=lambda: nn.ReLU(inplace=True)):\n    \n        super().__init__()\n        assert drop is None or (0.0 < drop < 1.0)\n        layers = [_SepConv1d(ni, no, kernel, stride, pad)]\n        if activ:\n            layers.append(activ())\n        if drop is not None:\n            layers.append(nn.Dropout(drop))\n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x): \n        return self.layers(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `Flatten` layer replicates `Reshape` layer from `Keras` and makes the convolution layer output ready to pass them into `nn.Linear` layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Flatten(nn.Module):\n    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n\n    def __init__(self, keep_batch_dim=True):\n        super().__init__()\n        self.keep_batch_dim = keep_batch_dim\n\n    def forward(self, x):\n        if self.keep_batch_dim:\n            return x.view(x.size(0), -1)\n        return x.view(-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we create our classifier using the layers created above. As you see, the model consists of two separate \"paths\" which output is concatenated and then passed into the top layers. One path uses the \"raw\" data, and the second one - the data processed with FFT. Therefore, we don't drop the original data but use it in addition to the processed observations."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, raw_ni, fft_ni, no, drop=.5):\n        super().__init__()\n        \n        self.raw = nn.Sequential(\n            SepConv1d(raw_ni,  32, 8, 2, 3, drop=drop),\n            SepConv1d(    32,  64, 8, 4, 2, drop=drop),\n            SepConv1d(    64, 128, 8, 4, 2, drop=drop),\n            SepConv1d(   128, 256, 8, 4, 2),\n            Flatten(),\n            nn.Dropout(drop), nn.Linear(256, 64), nn.ReLU(inplace=True),\n            nn.Dropout(drop), nn.Linear( 64, 64), nn.ReLU(inplace=True))\n        \n        self.fft = nn.Sequential(\n            SepConv1d(fft_ni,  32, 8, 2, 4, drop=drop),\n            SepConv1d(    32,  64, 8, 2, 4, drop=drop),\n            SepConv1d(    64, 128, 8, 4, 4, drop=drop),\n            SepConv1d(   128, 128, 8, 4, 4, drop=drop),\n            SepConv1d(   128, 256, 8, 2, 3),\n            Flatten(),\n            nn.Dropout(drop), nn.Linear(256, 64), nn.ReLU(inplace=True),\n            nn.Dropout(drop), nn.Linear( 64, 64), nn.ReLU(inplace=True))\n        \n        self.out = nn.Sequential(\n            nn.Linear(128, 64), nn.ReLU(inplace=True), nn.Linear(64, no))\n        \n    def forward(self, t_raw, t_fft):\n        raw_out = self.raw(t_raw)\n        fft_out = self.fft(t_fft)\n        t_in = torch.cat([raw_out, fft_out], dim=1)\n        out = self.out(t_in)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_sz = 3810  # only the first `trn_sz` rows in each array include labelled data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = create_datasets((raw_arr, fft_arr), target, trn_sz, seed=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make sure that we run on a proper device (not relevant for Kaggle kernels but helpful in Jupyter sessions)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now everything is ready to create a training loop and see if our model works. For each training epoch, the loop performs the following actions:\n1. train model on the `trn_ds` dataset;\n2. verify quality on the `val_ds` dataset;\n3. check if the quality improved since previous epoch, and if so, save the model's weights onto disk;\n4. in case if the model's quality isn't impoving for `patience` epochs, the training is stopped.\n\nAlso, the code tracks loss and accuracy history, and prints current scores with exponentially increasing logging frequency, i.e., only at 1, 2, 4, 8... epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_feat = raw_arr.shape[1]\nfft_feat = fft_arr.shape[1]\n\ntrn_dl, val_dl, tst_dl = create_loaders(datasets, bs=256)\n\nlr = 0.001\nn_epochs = 3000\niterations_per_epoch = len(trn_dl)\nnum_classes = 9\nbest_acc = 0\npatience, trials = 500, 0\nbase = 1\nstep = 2\nloss_history = []\nacc_history = []\n\nmodel = Classifier(raw_feat, fft_feat, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss(reduction='sum')\nopt = optim.Adam(model.parameters(), lr=lr)\n\nprint('Start model training')\n\nfor epoch in range(1, n_epochs + 1):\n    \n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(trn_dl):\n        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n        opt.zero_grad()\n        out = model(x_raw, x_fft)\n        loss = criterion(out, y_batch)\n        epoch_loss += loss.item()\n        loss.backward()\n        opt.step()\n        \n    epoch_loss /= trn_sz\n    loss_history.append(epoch_loss)\n    \n    model.eval()\n    correct, total = 0, 0\n    for batch in val_dl:\n        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n        out = model(x_raw, x_fft)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \n    acc = correct / total\n    acc_history.append(acc)\n\n    if epoch % base == 0:\n        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n        base *= step\n\n    if acc > best_acc:\n        trials = 0\n        best_acc = acc\n        torch.save(model.state_dict(), 'best.pth')\n        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n    else:\n        trials += 1\n        if trials >= patience:\n            print(f'Early stopping on epoch {epoch}')\n            break\n            \nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can plot the training history and run the model on the testing dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"def smooth(y, box_pts):\n    box = np.ones(box_pts)/box_pts\n    y_smooth = np.convolve(y, box, mode='same')\n    return y_smooth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(12, 4))\n\nax[0].plot(loss_history, label='loss')\nax[0].set_title('Validation Loss History')\nax[0].set_xlabel('Epoch no.')\nax[0].set_ylabel('Loss')\n\nax[1].plot(smooth(acc_history, 5)[:-2], label='acc')\nax[1].set_title('Validation Accuracy History')\nax[1].set_xlabel('Epoch no.')\nax[1].set_ylabel('Accuracy');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_results = []\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.eval()\nfor x_raw, x_fft, _ in tst_dl:\n    batches = [t.to(device) for t in (x_raw, x_fft)]\n    out = model(*batches)\n    y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n    test_results.extend(y_hat.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.read_csv(sample)\nsubmit['surface'] = enc.inverse_transform(test_results)\nsubmit.to_csv('submit_base.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Is It Possible to Improve?\n\nThe model shown above shows pretty good quality of predictions, especially when compared to the naïve solutions. But can we do better? Let's see if we can improve the quality of predictions using a couple of additional tricks.\n\nFirst of all, we can try to use dynamic learning rate scheduling. Let's use the [One-Cycle Policy schedule](https://docs.fast.ai/training.html) inspired by L. Smith's papers."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine(epoch, t_max, ampl):\n    \"\"\"Shifted and scaled cosine function.\"\"\"\n    \n    t = epoch % t_max\n    return (1 + np.cos(np.pi*t/t_max))*ampl/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inv_cosine(epoch, t_max, ampl):\n    \"\"\"A cosine function reflected on X-axis.\"\"\"\n    \n    return 1 - cosine(epoch, t_max, ampl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_cycle(epoch, t_max, a1=0.6, a2=1.0, pivot=0.3):\n    \"\"\"A combined schedule with two cosine half-waves.\"\"\"\n    \n    pct = epoch / t_max\n    if pct < pivot:\n        return inv_cosine(epoch, pivot*t_max, a1)\n    return cosine(epoch - pivot*t_max, (1-pivot)*t_max, a2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 1000\nsched = partial(one_cycle, t_max=n, pivot=0.2)\nitems = [sched(t) for t in range(n)]\nplt.plot(items);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"As the picture above shows, the learning rate slowly increases up to some point, and then starts to decrese down to zero. Also note that our scheduling function returns values in range `[0, 1]`. Then we'll multiply the base learning rate set in the optimizer by these values returned from the scheduling function. Therefore, our function instead of returning absolute learning rates works as a _scaler_ that modifies the base value as training iterations increase."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Scheduler:\n    \"\"\"Updates optimizer's learning rates using provided scheduling function.\"\"\"\n    \n    def __init__(self, opt, schedule):\n        self.opt = opt\n        self.schedule = schedule\n        self.history = defaultdict(list)\n    \n    def step(self, t):\n        for i, group in enumerate(self.opt.param_groups):\n            lr = opt.defaults['lr'] * self.schedule(t)\n            group['lr'] = lr\n            self.history[i].append(lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, we can try to add `nn.BatchNorm1d` layers to our convolutions and use a different activation function that is [less prone to the problem of \"dead neurons\" which could be very prominent in case of `ReLU`](http://cs231n.github.io/neural-networks-1/#actfun). (See the section on activation functions)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class SepConv1d(nn.Module):\n    \"\"\"Implementes a 1-d convolution with 'batteries included'.\n    \n    The module adds (optionally) activation function and dropout \n    layers right after a separable convolution layer.\n    \"\"\"\n    def __init__(self, ni, no, kernel, stride, pad, \n                 drop=None, bn=True,\n                 activ=lambda: nn.PReLU()):\n    \n        super().__init__()\n        assert drop is None or (0.0 < drop < 1.0)\n        layers = [_SepConv1d(ni, no, kernel, stride, pad)]\n        if activ:\n            layers.append(activ())\n        if bn:\n            layers.append(nn.BatchNorm1d(no))\n        if drop is not None:\n            layers.append(nn.Dropout(drop))\n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x): \n        return self.layers(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we could try to increase the depth of network as soon as we included batch normalization layers and `PReLU` activations which could reduce the negative effects of increased depth."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, raw_ni, fft_ni, no, drop=.5):\n        super().__init__()\n        \n        self.raw = nn.Sequential(\n            SepConv1d(raw_ni,  32, 8, 2, 3, drop=drop),\n            SepConv1d(    32,  32, 3, 1, 1, drop=drop),\n            SepConv1d(    32,  64, 8, 4, 2, drop=drop),\n            SepConv1d(    64,  64, 3, 1, 1, drop=drop),\n            SepConv1d(    64, 128, 8, 4, 2, drop=drop),\n            SepConv1d(   128, 128, 3, 1, 1, drop=drop),\n            SepConv1d(   128, 256, 8, 4, 2),\n            Flatten(),\n            nn.Dropout(drop), nn.Linear(256, 64), nn.PReLU(), nn.BatchNorm1d(64),\n            nn.Dropout(drop), nn.Linear( 64, 64), nn.PReLU(), nn.BatchNorm1d(64))\n        \n        self.fft = nn.Sequential(\n            SepConv1d(fft_ni,  32, 8, 2, 4, drop=drop),\n            SepConv1d(    32,  32, 3, 1, 1, drop=drop),\n            SepConv1d(    32,  64, 8, 2, 4, drop=drop),\n            SepConv1d(    64,  64, 3, 1, 1, drop=drop),\n            SepConv1d(    64, 128, 8, 4, 4, drop=drop),\n            SepConv1d(   128, 128, 8, 4, 4, drop=drop),\n            SepConv1d(   128, 256, 8, 2, 3),\n            Flatten(),\n            nn.Dropout(drop), nn.Linear(256, 64), nn.PReLU(), nn.BatchNorm1d(64),\n            nn.Dropout(drop), nn.Linear( 64, 64), nn.PReLU(), nn.BatchNorm1d(64))\n        \n        self.out = nn.Sequential(\n            nn.Linear(128, 64), nn.ReLU(inplace=True), nn.Linear(64, no))\n        \n        self.init_weights(nn.init.kaiming_normal_)\n        \n    def init_weights(self, init_fn):\n        def init(m): \n            for child in m.children():\n                if isinstance(child, nn.Conv1d):\n                    init_fn(child.weights)\n        init(self)\n        \n    def forward(self, t_raw, t_fft):\n        raw_out = self.raw(t_raw)\n        fft_out = self.fft(t_fft)\n        t_in = torch.cat([raw_out, fft_out], dim=1)\n        out = self.out(t_in)\n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training loop is almost exactly copied-and-pasted from the previous cell. The only change is one-cycle scheduler and learning rate updates."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_feat = raw_arr.shape[1]\nfft_feat = fft_arr.shape[1]\n\ntrn_dl, val_dl, tst_dl = create_loaders(datasets, bs=256)\n\nlr = 0.001\nn_epochs = 3000\niterations_per_epoch = len(trn_dl)\nperiod = n_epochs*iterations_per_epoch\nnum_classes = 9\nbest_acc = 0\npatience, trials = 500, 0\nbase = 1\nstep = 2\niteration = 0\nloss_history = []\nacc_history = []\n\nmodel = Classifier(raw_feat, fft_feat, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss(reduction='sum')\nopt = optim.Adam(model.parameters(), lr=lr)\n\n# one-cycle learning rate scheduling\nsched = Scheduler(opt, partial(one_cycle, t_max=period, pivot=0.3))\n\nprint('Start model training')\n\nfor epoch in range(1, n_epochs + 1):\n    \n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(trn_dl):\n        iteration += 1\n        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n        sched.step(iteration)  # update the learning rate\n        opt.zero_grad()\n        out = model(x_raw, x_fft)\n        loss = criterion(out, y_batch)\n        epoch_loss += loss.item()\n        loss.backward()\n        opt.step()\n        \n    epoch_loss /= trn_sz\n    loss_history.append(epoch_loss)\n    \n    model.eval()\n    correct, total = 0, 0\n    for batch in val_dl:\n        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n        out = model(x_raw, x_fft)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \n    acc = correct / total\n    acc_history.append(acc)\n\n    if epoch % base == 0:\n        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n        base *= step\n\n    if acc > best_acc:\n        trials = 0\n        best_acc = acc\n        torch.save(model.state_dict(), 'best.pth')\n        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n    else:\n        trials += 1\n        if trials >= patience:\n            print(f'Early stopping on epoch {epoch}')\n            break\n            \nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's plot the training history and run the new model on the testing dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(12, 4))\n\nax[0].plot(loss_history, label='loss')\nax[0].set_title('Validation Loss History')\nax[0].set_xlabel('Epoch no.')\nax[0].set_ylabel('Loss')\n\nax[1].plot(smooth(acc_history, 5)[:-2], label='acc')\nax[1].set_title('Validation Accuracy History')\nax[1].set_xlabel('Epoch no.')\nax[1].set_ylabel('Accuracy');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_results = []\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.eval()\nfor x_raw, x_fft, _ in tst_dl:\n    batches = [t.to(device) for t in (x_raw, x_fft)]\n    out = model(*batches)\n    y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n    test_results.extend(y_hat.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.read_csv(sample)\nsubmit['surface'] = enc.inverse_transform(test_results)\nsubmit.to_csv('submit_one_cycle.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The new model usually performs a bit better on validation subset than the original one but the result is not too stable. It has changed a couple of times from one kernel run to another. If you try to submit two files to the leaderboard, you usually should see a better score for the later model as a screenshot below shows. So probably it is a good idea to train several models and average their predictions. (As the author of the original kernel did).\n\n![](https://i.imgur.com/94AuMAO.png)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Conclusion\n\nThe Deep Learning models are very powerful solutions to a wide range of Data Science projects. However, even these powerful solutions can't show good results if used naively and without additional efforts to make a proper preparation of the data. The more work we spend to help our models, the better results they will show."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}