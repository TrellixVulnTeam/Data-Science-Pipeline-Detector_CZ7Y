{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The Best Friend Of An Alchemist\n\nWhile working on this competition, I've encountered an intersting library called [`tsfresh`](https://github.com/blue-yonder/tsfresh) that helps one to automate feature engeneering process a bit. Though it didn't show too impressive result, it does a pretty decent job in generating features without too many efforts.\n\nIn this kernel, a simple approach is applied to generate a bunch of features from the \"raw\" data and see how they work. \n\n> **Tip:** Before I discovered how to override the default list of computed metrics, I had wrapped the library's functions with some custom code to compute specific features only, and speed up the process a bit. You probably could achieve the same result using [different settings](https://github.com/blue-yonder/tsfresh/blob/master/tsfresh/feature_extraction/settings.py) instead, or writing your own."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import ChainMap\nfrom multiprocessing import cpu_count\nfrom pathlib import Path","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom tqdm import tqdm_notebook as tqdm\nfrom tsfresh.feature_extraction.feature_calculators import *\nfrom tsfresh.feature_selection.relevance import calculate_relevance_table","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1\nnp.random.seed(seed)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Dataset And Features\n\nFirst of all, we read the files and drop the irrelavant columns."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"ROOT = Path.cwd().parent/'input'\nSAMPLE = ROOT/'sample_submission.csv'\nTRAIN = ROOT/'X_train.csv'\nTARGET = ROOT/'y_train.csv'\nTEST = ROOT/'X_test.csv'\n\nID_COLS = ['series_id', 'measurement_number']\n\nx_cols = {\n    'series_id': np.uint32,\n    'measurement_number': np.uint32,\n    'orientation_X': np.float32,\n    'orientation_Y': np.float32,\n    'orientation_Z': np.float32,\n    'orientation_W': np.float32,\n    'angular_velocity_X': np.float32,\n    'angular_velocity_Y': np.float32,\n    'angular_velocity_Z': np.float32,\n    'linear_acceleration_X': np.float32,\n    'linear_acceleration_Y': np.float32,\n    'linear_acceleration_Z': np.float32\n}\n\ny_cols = {\n    'series_id': np.uint32,\n    'group_id': np.uint32,\n    'surface': str\n}","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"x_trn = pd.read_csv(TRAIN, usecols=x_cols.keys(), dtype=x_cols)\nx_tst = pd.read_csv(TEST, usecols=x_cols.keys(), dtype=x_cols)\ny_trn = pd.read_csv(TARGET, usecols=y_cols.keys(), dtype=y_cols)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features Extraction\n\nNext we prepare a couple of helping utilities to apply feature extraction functions that come with `tsfresh`. We're going to convert sequences of measurements into scalar features and use them to train a classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"def part(f, **params):\n    \"\"\"Partially applies the function's keyword parameters.\"\"\"\n    def wrapper(x): return f(x, **params)\n    wrapper.__name__ = f.__name__\n    return wrapper","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class StatsFeatures:\n    \"\"\"Applies list of functions to a single instance of measurements \n    and returns dictionary with computed features.\n    \"\"\"\n    def __init__(self, funcs):\n        self.funcs = funcs\n    \n    def __call__(self, data):\n        features = {}\n        for col in data.columns:\n            for func in self.funcs:\n                result = func(data[col].values) \n                if hasattr(result, '__len__'):\n                    for key, value in result:\n                        features[f'{col}__{func.__name__}__{key}'] = value\n                else:\n                    features[f'{col}__{func.__name__}'] = result\n        return features","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SliceFeatures:\n    \"\"\"Takes a slice of values from the original sequence of \n    observations.\n    \n    There types of slicing are supported:\n        * first: take N observations from the beginning of the sequence.\n        * middle: take N observations from the middle of the sequence.\n        * last: take last N observations from the sequence.\n        \n    \"\"\"\n    def __init__(self, mode='first', n=5):\n        if mode not in {'first', 'middle', 'last'}:\n            raise ValueError('unexpected mode')\n        self.mode = mode\n        self.n = n\n        \n    def __call__(self, data):\n        if self.mode == 'first':\n            start, end = 0, self.n\n        elif self.mode == 'last':\n            start, end = -self.n, len(data)\n        elif self.mode == 'middle':\n            mid = len(data) // 2\n            div, mod = divmod(self.n, 2)\n            start, end = mid-div, mid+div+mod\n        cols = data.columns\n        vec = data.iloc[start:end].values.T.ravel()\n        new_cols = [\n            f'{col}_{self.mode}{i}' \n            for i in range(self.n) for col in cols]\n        return dict(zip(new_cols, vec))","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a custom code used to compute features in parallel mode. As it was mentioned previously, you can also use `extract_features` or `extract_relevant_features` functions described in the library's [quick start guide](https://tsfresh.readthedocs.io/en/latest/text/quick_start.html). You only need to override the default settings class with custom implementation. Also, you can use [`calculate_relevance_table`](https://tsfresh.readthedocs.io/en/latest/text/feature_filtering.html) function on top of the dataset with extracted features to keep only relevant onces."},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_features(data, features, ignore=None):\n    \"\"\"Extracts tsfresh features from the dataset.\"\"\"\n    \n    with Parallel(n_jobs=cpu_count()) as parallel:\n        extracted = parallel(delayed(generate_features_for_group)(\n            group=group.drop(columns=ignore or []),\n            features=features\n        ) for _, group in tqdm(data.groupby('series_id')))\n    return pd.DataFrame(extracted)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_features_for_group(group, features):\n    \"\"\"Extract tsfresh features from a single measurements group.\"\"\"\n    \n    return dict(ChainMap(*[feat(group) for feat in features]))","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And here is a list of functions we use to extract the features. The list includes statistical features only. Probably one should add some signal processing here as well to get a better quality."},{"metadata":{"trusted":true},"cell_type":"code","source":"funcs = (\n    mean, median, standard_deviation, variance, \n    skewness, kurtosis, maximum, minimum,\n    mean_change, mean_abs_change, count_above_mean, count_below_mean,\n    mean_second_derivative_central, sum_of_reoccurring_data_points, \n    abs_energy, sum_values, sample_entropy,\n    longest_strike_above_mean, longest_strike_below_mean,\n    first_location_of_minimum, first_location_of_maximum,\n    *[part(large_standard_deviation, r=r*0.05) for r in range(1, 20)],\n    *[part(autocorrelation, lag=lag) for lag in range(1, 25)], \n    *[part(number_peaks, n=n) for n in (1, 2, 3, 5, 7, 10, 25, 50)],\n    *[part(c3, lag=lag) for lag in range(1, 5)],\n    *[part(quantile, q=q) for q in (.1, .2, .3, .4, .5, .6, .7, .8, .9)],\n    part(partial_autocorrelation, param=[\n        {'lag': lag} for lag in range(25)\n    ]),\n    part(agg_autocorrelation, param=[\n        {'f_agg': s, 'maxlag': 40} for s in ('mean', 'median', 'var')\n    ]),\n    part(linear_trend, param=[\n        {'attr': a} for a in \n        ('pvalue', 'rvalue', 'intercept', 'slope', 'stderr')\n    ])\n)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\n    StatsFeatures(funcs),\n    SliceFeatures('first'),\n    SliceFeatures('middle'),\n    SliceFeatures('last')\n]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ignore = ['series_id', 'measurement_number']","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Feature extraction on train dataset')\nx_trn = generate_features(x_trn, features, ignore=ignore)","execution_count":15,"outputs":[{"output_type":"stream","text":"Feature extraction on train dataset\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=3810), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b7798a6548544c49914248a069e78aa"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Feature extraction on test dataset')\nx_tst = generate_features(x_tst, features, ignore=ignore)","execution_count":16,"outputs":[{"output_type":"stream","text":"Feature extraction on test dataset\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=3816), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86f1ac1432a648f5a6809868e9434b41"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = LabelEncoder()\ny_trn = pd.Series(enc.fit_transform(y_trn['surface']))","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Model\n\nWe're going to train a LightGBM classifier using the features shown above. The parameters of the classifier are chosen a bit arbitrarily. One could use a more educated approach and use some kind of parameters search and validation techniques to build a more robust model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(y_true, y_pred):\n    n = len(y_true)\n    y_hat = y_pred.reshape(9, n).argmax(axis=0)\n    value = (y_true == y_hat).mean()\n    return 'accuracy', value, True","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMClassifier(n_estimators=3000, learning_rate=0.005,\n                           colsample_bytree=0.4, objective='multiclass',\n                           num_leaves=500, num_class=9)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(x_trn, y_trn, test_size=0.1, random_state=seed)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train, y_train,\n          eval_set=[(x_valid, y_valid)],\n          eval_metric=accuracy,\n          early_stopping_rounds=300,\n          verbose=150)","execution_count":21,"outputs":[{"output_type":"stream","text":"Training until validation scores don't improve for 300 rounds.\n[150]\tvalid_0's multi_logloss: 1.15655\tvalid_0's accuracy: 0.813648\n[300]\tvalid_0's multi_logloss: 0.831254\tvalid_0's accuracy: 0.826772\n[450]\tvalid_0's multi_logloss: 0.656715\tvalid_0's accuracy: 0.839895\n[600]\tvalid_0's multi_logloss: 0.555689\tvalid_0's accuracy: 0.847769\n[750]\tvalid_0's multi_logloss: 0.495266\tvalid_0's accuracy: 0.855643\n[900]\tvalid_0's multi_logloss: 0.457566\tvalid_0's accuracy: 0.860892\n[1050]\tvalid_0's multi_logloss: 0.433278\tvalid_0's accuracy: 0.860892\nEarly stopping, best iteration is:\n[823]\tvalid_0's multi_logloss: 0.474895\tvalid_0's accuracy: 0.863517\n","name":"stdout"},{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.4,\n        importance_type='split', learning_rate=0.005, max_depth=-1,\n        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n        n_estimators=3000, n_jobs=-1, num_class=9, num_leaves=500,\n        objective='multiclass', random_state=None, reg_alpha=0.0,\n        reg_lambda=0.0, silent=True, subsample=1.0,\n        subsample_for_bin=200000, subsample_freq=0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You could also try to run K-folded validation instead.\n#\n# k = 5\n# test = np.zeros((len(x_tst_rich), 9), dtype=np.float32)\n# kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n# for i, (trn_idx, val_idx) in enumerate(kfold.split(x_trn_rich.index, y_trn)):\n#     x_train = x_trn[x_trn.isin(trn_idx)]\n#     x_valid = x_trn[x_trn.isin(val_idx)]\n#     y_train = y_trn[y_trn.isin(trn_idx)]\n#     y_valid = y_trn[y_trn.isin(val_idx)]\n#     model = lgb.LGBMClassifier(n_estimators=3000, learning_rate=0.005,\n#                                colsample_bytree=0.4, objective='multiclass',\n#                                num_leaves=500, num_class=9)\n#     model.fit(x_train, y_train,\n#               eval_set=[(x_valid, y_valid)],\n#               eval_metric=accuracy,\n#               early_stopping_rounds=300,\n#               verbose=150)\n#     test += model.predict_proba(x_tst_rich)\n# test /= k","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = enc.inverse_transform(model.predict(x_tst))","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.read_csv(SAMPLE)\nsubmit['surface'] = test\nsubmit.to_csv('submit.csv', index=None)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}