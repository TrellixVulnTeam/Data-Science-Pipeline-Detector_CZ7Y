{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install cython\n# !pip install TA-Lib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !curl https://drive.google.com/uc?authuser=0&id=1pdEeL05T6-KyEao1kTCwCD4VXIAp3XsG&export=download\n# ! echo \"--LS HERE--\"\n# !ls\n# !tar -xvf ta-lib-0.4.0-src.tar.gz\n# !cd ta-lib/\n# !./configure --prefix=/usr\n# !make\n# !make install","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# local\n# xtrainPath = './X_train.csv'\n# xtestPath = './X_test.csv'\n# samplePath = './sample_submission.csv'\n# ytrainPath = './y_train.csv'\n# kaggle\nxtrainPath = '../input/X_train.csv'\nxtestPath = '../input/X_test.csv'\nsamplePath = '../input/sample_submission.csv'\nytrainPath = '../input/y_train.csv'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"X_train = pd.read_csv(xtrainPath)\nX_test = pd.read_csv(xtestPath)\n#X_test = X_test.set_index('series_id')\n#X_test['series_id'] = X_test.index\nsample_submission = pd.read_csv(samplePath)\ny_train = pd.read_csv(ytrainPath)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Basic info and stats**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Merge target into training set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.join(y_train.set_index('series_id'), on='series_id')\nX_train = X_train.drop(['row_id', 'measurement_number', 'group_id'], axis=1)\nX_test = X_test.drop(['row_id', 'measurement_number'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"surfaces = list(y_train['surface'].unique())\nsurfaces","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classes distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(x='surface', data=X_train, order = X_train['surface'].value_counts().index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Features distrbutions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(2, 5, figsize=(17,9))\nfor column in X_train.columns[1:-1]:\n    i += 1\n    plt.subplot(2, 5, i)\n    sns.distplot(X_train[column])\n    sns.distplot(X_test[column])\n    plt.legend(title=column, loc='upper left', labels=['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature distribution train X test"},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(2, 5, figsize=(17,9))\nfor column in X_train.columns[1:-1]:\n    i += 1\n    plt.subplot(2, 5, i)\n    sns.kdeplot(X_train[column], bw=0.5)\n    sns.kdeplot(X_test[column], bw=0.5)\n    plt.legend(title=column, loc='upper left', labels=['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features distribution per surface"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotDistributionPerSurface(data, num_rows, num_columns, size=(16,24)):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(num_rows, num_columns, figsize=size)\n    for column in data.columns:\n        if column == 'surface':\n            continue\n        i += 1\n        plt.subplot(num_rows, num_columns, i)\n        for surface in surfaces:\n            sns.kdeplot(data[data['surface'] == surface][column], bw=0.5)\n        plt.legend(title=column, loc='upper left', labels=surfaces)\n    plt.show()\n\nplotDistributionPerSurface(X_train[X_train.columns[1:]], 5, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation Matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,8))\nsns.heatmap(X_train[X_train.columns[1:-1]].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform 4D to 3D orientation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_step0 (actual):\n    \n    # https://www.mathworks.com/help/aeroblks/quaternionnorm.html\n    # https://www.mathworks.com/help/aeroblks/quaternionmodulus.html\n    # https://www.mathworks.com/help/aeroblks/quaternionnormalize.html\n        \n    actual['norm_quat'] = (actual['orientation_X']**2 + actual['orientation_Y']**2 + actual['orientation_Z']**2 + actual['orientation_W']**2)\n    actual['mod_quat'] = (actual['norm_quat'])**0.5\n    actual['norm_X'] = actual['orientation_X'] / actual['mod_quat']\n    actual['norm_Y'] = actual['orientation_Y'] / actual['mod_quat']\n    actual['norm_Z'] = actual['orientation_Z'] / actual['mod_quat']\n    actual['norm_W'] = actual['orientation_W'] / actual['mod_quat']\n    \n    return actual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_step1 (actual):\n    \"\"\"Quaternions to Euler Angles\"\"\"\n    \n    x, y, z, w = actual['norm_X'].tolist(), actual['norm_Y'].tolist(), actual['norm_Z'].tolist(), actual['norm_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    actual['euler_x'] = nx\n    actual['euler_y'] = ny\n    actual['euler_z'] = nz\n    return actual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train = fe_step0(X_train)\n#X_test = fe_step0(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train = fe_step1(X_train)\n#X_test = fe_step1(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotDistributionPerSurface(X_train, 11, 2, (16,80))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prepare data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove_features = ['orientation_Z', 'orientation_W', 'angular_velocity_Y', 'angular_velocity_X', 'angular_velocity_Z']\n#remove_features = ['orientation_Z', 'orientation_W', 'norm_quat',\n#                   'mod_quat', 'norm_Z', 'norm_W', 'euler_x', 'euler_y']\n# X_train = X_train.drop(remove_features, axis=1)\n# X_test = X_test.drop(remove_features, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transformData(data, is_train=True):\n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2)** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2)**0.5\n    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2)**0.5\n    data['acc_vs_vel'] = data['totl_linr_acc'] / data['totl_anglr_vel']\n    #data['totl_norm_XY'] = (data['norm_X']**2 + data['norm_Y']**2)** 0.5\n    # PCA\n#     pca = PCA(n_components=1)\n#     data['velocity_pca'] = pca.fit_transform(data[['angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z']])\n#     data['linear_acceleration_pca'] = pca.fit_transform(data[['linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']])\n#     data['norm_pca'] = pca.fit_transform(data[['norm_X', 'norm_Y']])\n#     data['norm_euler_pca'] = pca.fit_transform(data[['norm_X', 'norm_Y', 'euler_z']])\n#     data['orientation_XY_pca'] = pca.fit_transform(data[['orientation_X', 'orientation_Y']])\n    \n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    for column in data.columns:\n        if column in ['row_id','series_id','measurement_number', 'surface']:\n            continue\n        df[column + '_mean'] = data.groupby(['series_id'])[column].mean()\n        df[column + '_median'] = data.groupby(['series_id'])[column].median()\n        df[column + '_std'] = data.groupby(['series_id'])[column].std()\n        df[column + '_min'] = data.groupby(['series_id'])[column].min()\n        df[column + '_max'] = data.groupby(['series_id'])[column].max()\n        #df[column + '_mad'] = data.groupby(['series_id'])[column].mad()\n        #df[column + '_kurt'] = data.groupby(['series_id'])[column].apply(pd.DataFrame.kurt)\n        #df[column + '_skew'] = data.groupby(['series_id'])[column].skew()\n        #df[column + '_ma_10_mean'] = data.groupby(['series_id'])[column].rolling(window=10).mean().mean(skipna=True)\n        #df[column + '_ma_16_mean'] = data.groupby(['series_id'])[column].rolling(window=16).mean().mean(skipna=True)\n        #df[column + '_ma_10_std'] = data.groupby(['series_id'])[column].rolling(window=10).std().mean(skipna=True)\n        #df[column + '_ma_16_std'] = data.groupby(['series_id'])[column].rolling(window=16).std().mean(skipna=True)\n        #df[column + '_qtl25'] = data.groupby(['series_id'])[column].quantile(.25)\n        #df[column + '_qtl50'] = data.groupby(['series_id'])[column].quantile(.50)\n        #df[column + '_qtl75'] = data.groupby(['series_id'])[column].quantile(.75)\n        #df[column + '_pct_change_1'] = data.groupby(['series_id'])[column].pct_change(periods=1)\n        #df[column + '_pct_change_2'] = data.groupby(['series_id'])[column].pct_change(periods=2)\n        #df[column + '_ma_5'] = talib.SMA(data.groupby(['series_id'])[column].values, timeperiod=5) / data.groupby(['series_id'])[column]  \n        #df[column + '_RSI_5'] = talib.RSI(data.groupby(['series_id'])[column].values, timeperiod=n) / data.groupby(['series_id'])[column]\n        #df[column + '_EWN_5'] = data.groupby(['series_id'])[column].ewm(span=5).mean()\n        df[column + '_range'] = df[column + '_max'] - df[column + '_min']\n        df[column + '_maxtoMin'] = df[column + '_max'] / df[column + '_min']\n        df[column + '_mean_abs_chg'] = data.groupby(['series_id'])[column].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        #df[column + '_mean_change_of_abs_change'] = data.groupby('series_id')[column].apply(mean_change_of_abs_change)\n        #df[column + '_mean_abs_chg_pct_1'] = df[column + '_mean_abs_chg'].pct_change(periods=1)\n        #df[column + '_mean_abs_chg_pct_1'] = df[column + '_mean_change_of_abs_change'].pct_change(periods=1)\n        df[column + '_abs_max'] = data.groupby(['series_id'])[column].apply(lambda x: np.max(np.abs(x)))\n        df[column + '_abs_min'] = data.groupby(['series_id'])[column].apply(lambda x: np.min(np.abs(x)))\n        df[column + '_abs_avg'] = (df[column + '_abs_min'] + df[column + '_abs_max'])/2\n        # create a PCA that joins everything\n        #df[column + '_pca'] = pca.fit_transform(df.filter(like=column, axis=1))\n        if is_train:\n            surface_groupby = data.groupby(['series_id']).surface.unique().apply(lambda x: x[0])\n            df.loc[:,'surface'] = surface_groupby.values\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe(data):\n    \n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 +\n                             data['angular_velocity_Z']**2)** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 +\n                             data['linear_acceleration_Z'])**0.5\n    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 +\n                             data['orientation_Z'])**0.5\n   \n    data['acc_vs_vel'] = data['totl_linr_acc'] / data['totl_anglr_vel']\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number', 'surface']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] / df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])/2\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = fe(X_train)\ntest = fe(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.fillna(0, inplace = True)\ntrain.replace(-np.inf,0,inplace=True)\ntrain.replace(np.inf,0,inplace=True)\ntest.fillna(0, inplace = True)\ntest.replace(-np.inf,0,inplace=True)\ntest.replace(np.inf,0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,10))\nsns.heatmap(train.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotDistributionPerSurface(train, 113, 2, (16,260))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# most_important_features = ['orientation_X_max',\n#  'norm_X_max',\n#  'orientation_X_median',\n#  'norm_X_min',\n#  'orientation_X_mean',\n#  'norm_X_median',\n#  'norm_X_mean',\n#  'orientation_X_min',\n#  'linear_acceleration_Y_mean_abs_chg',\n#  'euler_z_max',\n#  'euler_z_mean',\n#  'euler_z_min',\n#  'euler_z_median',\n#  'linear_acceleration_Z_mean_abs_chg',\n#  'norm_Y_min',\n#  'totl_linr_acc_mean_abs_chg',\n#  'norm_Y_mean',\n#  'orientation_Y_min']\n\n# pca = PCA(n_components=1)\n# pca.fit(train[most_important_features])\n# print(pca.explained_variance_ratio_) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['pca_best_1'] = pca.fit_transform(train[most_important_features])\n# pca_best_features = pd.DataFrame(pca.fit_transform(train[most_important_features]),\n#                                  columns=['pca_best_1', 'pca_best_2', 'pca_best_3'])\n# train = train.join(pca_best_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n    \"\"\"\n    Generate matrix plot of confusion matrix with pretty annotations.\n    The plot image is saved to disk.\n    args: \n      y_true:    true label of the data, with shape (nsamples,)\n      y_pred:    prediction of the data, with shape (nsamples,)\n      filename:  filename of figure file to save\n      labels:    string array, name the order of class labels in the confusion matrix.\n                 use `clf.classes_` if using scikit-learn models.\n                 with shape (nclass,).\n      ymap:      dict: any -> string, length == nclass.\n                 if not None, map the labels & ys to more understandable strings.\n                 Caution: original y_true, y_pred and labels must align.\n      figsize:   the size of the figure plotted.\n    \"\"\"\n    if ymap is not None:\n        y_pred = [ymap[yi] for yi in y_pred]\n        y_true = [ymap[yi] for yi in y_true]\n        labels = [ymap[yi] for yi in labels]\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=labels, columns=labels)\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n    #plt.savefig(filename)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotFeatureImportancesRF(forest, X):\n    importances = forest.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n                 axis=0)\n    indices = np.argsort(importances)[::-1]\n\n    # Print the feature ranking\n    print(\"Feature ranking:\")\n\n    for f in range(X.shape[1]):\n        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n    # Plot the feature importances of the forest\n    plt.figure()\n    plt.title(\"Feature importances\")\n    plt.bar(range(X.shape[1]), importances[indices],\n           color=\"r\", yerr=std[indices], align=\"center\")\n    plt.xticks(range(X.shape[1]), indices)\n    plt.xlim([-1, X.shape[1]])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ny = le.fit_transform(y_train['surface'])\nX_train = train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train, X_validation, y_train, y_validation = train_test_split(X_train, y, test_size=0.30, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RandomForest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_folds(X, y, X_test, k):\n    folds = StratifiedKFold(n_splits = k, shuffle=True, random_state=2019)\n    y_test = np.zeros((X_test.shape[0], 9))\n    y_oof = np.zeros((X.shape[0]))\n    score = 0\n    for i, (train_idx, val_idx) in  enumerate(folds.split(X, y)):\n        clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n        clf.fit(X.iloc[train_idx], y[train_idx])\n        y_oof[val_idx] = clf.predict(X.iloc[val_idx])\n        y_test += clf.predict_proba(X_test) / folds.n_splits\n        score += clf.score(X.iloc[val_idx], y[val_idx])\n        print('Fold: {} score: {}'.format(i,clf.score(X.iloc[val_idx], y[val_idx])))\n    print('Avg Accuracy', score / folds.n_splits) \n        \n    return y_oof, y_test ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_oof, y_test = k_folds(X_train, y, test, k= 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_oof,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rfc = RandomForestClassifier(\n#     bootstrap=False,\n#     max_depth=50,\n#     max_features='auto',\n#     min_samples_leaf=1,\n#     min_samples_split=2,\n#     random_state=42,\n#     n_estimators=800,\n#     n_jobs = -1)\n\n# cv = StratifiedKFold(n_splits=3, shuffle=False, random_state=42)\n\n# scores = cross_val_score(rfc, X_train, y, cv=cv, scoring='accuracy')\n# print(\"Auc: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rfc = rfc.fit(X_train, y)\n# feature_importances = pd.DataFrame(rfc.feature_importances_, index = X_train.columns, columns=['importance']).sort_values('importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#f1_score(y_validation, y_pred, average='macro') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features = X_train.columns\n# importances = rfc.feature_importances_\n# indices = np.argsort(importances)\n\n# fig, ax = plt.subplots(figsize=(10, 85))\n# plt.title('Feature Importances')\n# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n# plt.yticks(range(len(indices)), [features[i] for i in indices])\n# plt.xlabel('Relative Importance')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cm_analysis(y_validation, y_pred, rfc.classes_, ymap=None, figsize=(20,20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train = train['surface']\n# X_train = train.drop(['surface'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.argmax(y_test, axis=1)\n\nsubmission = pd.DataFrame(\n    {'surface': le.inverse_transform(y_test)},\n    index=test.index\n)\n\nsubmission.to_csv('submission.csv', index=True)\n\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parameter Tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\n# bootstrap = [True, False]\n# Create the random grid\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n# print(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n# rf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\n# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\n#rf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"{'n_estimators': 1000,\n 'min_samples_split': 2,\n 'min_samples_leaf': 1,\n 'max_features': 'auto',\n 'max_depth': 50,\n 'bootstrap': False}"},{"metadata":{"trusted":true},"cell_type":"code","source":"# best_random = rf_random.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid based on the results of random search \n# param_grid = {\n#     'bootstrap': [False],\n#     'max_depth': [35, 40, 50, 80, 120],\n#     'max_features': ['auto'],\n#     'min_samples_leaf': [1, 3, 5, 8],\n#     'min_samples_split': [2, 5, 8, 12],\n#     'n_estimators': [800, 1000, 2000, 3000]\n# }\n# Create a based model\n# rf = RandomForestClassifier()\n# Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the grid search to the data\n#grid_search.fit(X_train, y_train)\n#grid_search.best_params_\n#best_grid = grid_search.best_estimator_\n# grid_accuracy = evaluate(best_grid, test_features, test_labels)\n# print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"{'bootstrap': False,\n 'max_depth': 50,\n 'max_features': 'auto',\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'n_estimators': 800}"},{"metadata":{"trusted":true},"cell_type":"code","source":"# et = ExtraTreesClassifier(\n#     bootstrap=False,\n#     max_depth=50,\n#     max_features='auto',\n#     min_samples_leaf=1,\n#     min_samples_split=2,\n#     random_state=42,\n#     n_estimators=800,\n#     n_jobs = -1).fit(X_train, y_train)\n# y_pred = et.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.DataFrame({'surface': y_pred}, index=test.index).to_csv('submission.csv', index=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}