{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy.stats as st\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":9,"outputs":[{"output_type":"stream","text":"['y_train.csv', 'sample_submission.csv', 'X_train.csv', 'X_test.csv']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"* Credit: the model is based on : https://gist.github.com/kastnerkyle/75483d51641a0c03bf7c\n* This method is super slow and has to be optimized later (Both features extraction and models fitting)"},{"metadata":{},"cell_type":"markdown","source":"# Features Extraction"},{"metadata":{},"cell_type":"markdown","source":"## Load the files"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nx_train = pd.read_csv('../input/X_train.csv')\ny_train = pd.read_csv('../input/y_train.csv')\nLabelEncoder_x = LabelEncoder()\nlabel = LabelEncoder_x.fit_transform(y_train.surface)\nx_test = pd.read_csv('../input/X_test.csv')","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features: STFT & Peak Finding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.lib.stride_tricks import as_strided\ndef stft(x, fftsize=64, overlap_pct=.5):   \n    hop = int(fftsize * (1 - overlap_pct))\n    w = scipy.hanning(fftsize + 1)[:-1]    \n    raw = np.array([np.fft.rfft(w * x[i:i + fftsize]) for i in range(0, len(x) - fftsize, hop)])\n    return raw[:, :(fftsize // 2)]\ndef peakfind(x, n_peaks, l_size=3, r_size=3, c_size=3, f=np.mean):\n    win_size = l_size + r_size + c_size\n    shape = x.shape[:-1] + (x.shape[-1] - win_size + 1, win_size)\n    strides = x.strides + (x.strides[-1],)\n    xs = as_strided(x, shape=shape, strides=strides)\n    def is_peak(x):\n        centered = (np.argmax(x) == l_size + int(c_size/2))\n        l = x[:l_size]\n        c = x[l_size:l_size + c_size]\n        r = x[-r_size:]\n        passes = np.max(c) > np.max([f(l), f(r)])\n        if centered and passes:\n            return np.max(c)\n        else:\n            return -1\n    r = np.apply_along_axis(is_peak, 1, xs)\n    top = np.argsort(r, None)[::-1]\n    heights = r[top[:n_peaks]]\n\n    top[top > -1] = top[top > -1] + l_size + int(c_size / 2.)\n    return heights, top[:n_peaks]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training data features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\ncolumns = ['orientation_X', 'orientation_Y','orientation_Z','orientation_W', 'angular_velocity_X','angular_velocity_Y','angular_velocity_Z', 'linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']\nall_obs = []\nfor ide in x_train.series_id.unique():\n    d = np.abs(stft(x_train[x_train['series_id']==ide][columns[0]]))\n    for column in columns[1:]:\n        pic = np.abs(stft(x_train[x_train['series_id']==ide][column]))\n        d = np.concatenate((d, pic), axis=0)\n    n_dim = 6\n    obs = np.zeros((n_dim, d.shape[0]))\n    for r in range(d.shape[0]):\n        _, t = peakfind(d[r, :], n_peaks=n_dim)\n        obs[:, r] = t.copy()\n\n    all_obs.append(obs)\n    \nall_obs = np.atleast_3d(all_obs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing data features"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_obs = []\nfor ide in x_test.series_id.unique():\n    d = np.abs(stft(x_test[x_test['series_id']==ide][columns[0]]))\n    for column in columns[1:]:\n        pic = np.abs(stft(x_test[x_test['series_id']==ide][column]))\n        d = np.concatenate((d, pic), axis=0)\n    n_dim = 6\n    obs = np.zeros((n_dim, d.shape[0]))\n    for r in range(d.shape[0]):\n        _, t = peakfind(d[r, :], n_peaks=n_dim)\n        obs[:, r] = t.copy()\n\n    x_test_obs.append(obs)\n    \nx_test_obs = np.atleast_3d(x_test_obs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GMMHMM model"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class gmmhmm:\n    # This class converted with modifications from:\n    # https://code.google.com/p/hmm-speech-recognition/source/browse/Word.m\n    def __init__(self, n_states):\n        self.n_states = n_states\n        self.random_state = np.random.RandomState(0)\n        \n        # Normalize random initial state\n        self.prior = self._normalize(self.random_state.rand(self.n_states, 1))\n        self.A = self._stochasticize(self.random_state.rand(self.n_states, self.n_states))\n        \n        self.mu = None\n        self.covs = None\n        self.n_dims = None\n           \n    def _forward(self, B):\n        log_likelihood = 0.\n        T = B.shape[1] # Number of observation\n        alpha = np.zeros(B.shape)\n        for t in range(T):\n            if t == 0:\n                alpha[:, t] = B[:, t] * self.prior.ravel()\n            else:\n                alpha[:, t] = B[:, t] * np.dot(self.A.T, alpha[:, t - 1])\n         \n            alpha_sum = np.sum(alpha[:, t])\n            alpha[:, t] /= alpha_sum\n            log_likelihood = log_likelihood + np.log(alpha_sum)\n        return log_likelihood, alpha\n    \n    def _backward(self, B):\n        T = B.shape[1]\n        beta = np.zeros(B.shape);\n           \n        beta[:, -1] = np.ones(B.shape[0])\n            \n        for t in range(T - 1)[::-1]:\n            beta[:, t] = np.dot(self.A, (B[:, t + 1] * beta[:, t + 1]))\n            beta[:, t] /= np.sum(beta[:, t])\n        return beta\n    \n    def _state_likelihood(self, obs):\n        obs = np.atleast_2d(obs)\n        B = np.zeros((self.n_states, obs.shape[1]))\n        for s in range(self.n_states):\n            np.random.seed(self.random_state.randint(1))\n            B[s, :] = st.multivariate_normal.pdf(\n                obs.T, mean=self.mu[:, s].T, cov=self.covs[:, :, s].T) \n            # This is a statstical density func rather than a pdf\n        return B\n    \n    def _normalize(self, x):\n        return (x + (x == 0)) / np.sum(x)\n    \n    def _stochasticize(self, x):\n        return (x + (x == 0)) / np.sum(x, axis=1)\n    \n    def _em_init(self, obs):\n        # Using this _em_init function allows for less required constructor args\n        if self.n_dims is None:\n            self.n_dims = obs.shape[0]\n        if self.mu is None:\n            subset = self.random_state.choice(np.arange(self.n_dims), size=self.n_states, replace=True)\n            self.mu = obs[:, subset]\n        if self.covs is None:\n            self.covs = np.zeros((self.n_dims, self.n_dims, self.n_states))\n            self.covs += np.diag(np.diag(np.cov(obs)))[:, :, None]\n        return self\n    \n    def _em_step(self, obs): \n        obs = np.atleast_2d(obs)\n        B = self._state_likelihood(obs)\n        T = obs.shape[1]\n        \n        log_likelihood, alpha = self._forward(B)\n        beta = self._backward(B)\n        \n        xi_sum = np.zeros((self.n_states, self.n_states))\n        gamma = np.zeros((self.n_states, T))\n        \n        for t in range(T - 1):\n            partial_sum = self.A * np.dot(alpha[:, t], (beta[:, t] * B[:, t + 1]).T)\n            xi_sum += self._normalize(partial_sum)\n            partial_g = alpha[:, t] * beta[:, t]\n            gamma[:, t] = self._normalize(partial_g)\n              \n        partial_g = alpha[:, -1] * beta[:, -1]\n        gamma[:, -1] = self._normalize(partial_g)\n        \n        expected_prior = gamma[:, 0]\n        expected_A = self._stochasticize(xi_sum)\n        \n        expected_mu = np.zeros((self.n_dims, self.n_states))\n        expected_covs = np.zeros((self.n_dims, self.n_dims, self.n_states))\n        \n        gamma_state_sum = np.sum(gamma, axis=1)\n        # Set zeros to 1 before dividing\n        gamma_state_sum = gamma_state_sum + (gamma_state_sum == 0)\n        \n        for s in range(self.n_states):\n            gamma_obs = obs * gamma[s, :]\n            expected_mu[:, s] = np.sum(gamma_obs, axis=1) / gamma_state_sum[s]\n            partial_covs = np.dot(gamma_obs, obs.T) / gamma_state_sum[s] - np.dot(expected_mu[:, s], expected_mu[:, s].T)\n            #Symmetrize\n            partial_covs = np.triu(partial_covs) + np.triu(partial_covs).T - np.diag(partial_covs)\n        \n        # Ensure positive semidefinite by adding diagonal loading\n        expected_covs += .01 * np.eye(self.n_dims)[:, :, None]\n        \n        self.prior = expected_prior\n        self.mu = expected_mu\n        self.covs = expected_covs\n        self.A = expected_A\n        return log_likelihood\n    \n    def fit(self, obs, n_iter=15):\n        count = obs.shape[0]\n        for n in range(count):\n            for i in range(n_iter):\n                self._em_init(obs[n, :, :])\n                log_likelihood = self._em_step(obs[n, :, :])\n        return self\n    \n    def transform(self, obs):\n        # Support for 2D and 3D arrays\n        # 2D: (n_features, n_dims)\n        # 3D: (n_examples, n_features, n_dims)\n        if len(obs.shape) == 2:\n            B = self._state_likelihood(obs)\n            log_likelihood, _ = self._forward(B)\n            return log_likelihood\n        elif len(obs.shape) == 3:\n            count = obs.shape[0]\n            out = np.zeros((count,))\n            for n in range(count):\n                B = self._state_likelihood(obs[n, :, :])\n                log_likelihood, _ = self._forward(B)\n                out[n] = log_likelihood\n            return out","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training & Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"ys = set(label)\nms = [gmmhmm(9) for y in ys]\n_ = [m.fit(all_obs) for m in ms]\nps = [m.transform(x_test_obs) for m in ms]\nres = np.vstack(ps)\npredicted_labels = np.argmax(res, axis=0)","execution_count":18,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in true_divide\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_labels = LabelEncoder_x.inverse_transform(predicted_labels)","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom datetime import datetime\nver = 'GMMHMM'\nfilename = 'subm_{}_{}_'.format(ver, datetime.now().strftime('%Y-%m-%d'))\npd.DataFrame({\n    'series_id': x_test.series_id.unique(),\n    'surface': predicted_labels\n}).to_csv(filename+'1'+'.csv', index=False)","execution_count":21,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"arrays must all be same length","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-d0fabf788d6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m pd.DataFrame({\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'series_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseries_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m'surface'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m }).to_csv(filename+'1'+'.csv', index=False)\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   7354\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7356\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7358\u001b[0m     \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   7400\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7402\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays must all be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: arrays must all be same length"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}