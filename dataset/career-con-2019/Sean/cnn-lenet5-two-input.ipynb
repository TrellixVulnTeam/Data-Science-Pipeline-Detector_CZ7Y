{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[{"output_type":"stream","text":"['X_train.csv', 'sample_submission.csv', 'X_test.csv', 'y_train.csv']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Features Extraction"},{"metadata":{},"cell_type":"markdown","source":"## Load the files"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nx_train = pd.read_csv('../input/X_train.csv')\ny_train = pd.read_csv('../input/y_train.csv')\nLabelEncoder_x = LabelEncoder()\nlabel = LabelEncoder_x.fit_transform(y_train.surface)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Short Time Fourier Transfer with Hanning Window"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\ndef stft(x, fftsize=24, overlap_pct=.5):   \n    hop = int(fftsize * (1 - overlap_pct))\n    w = scipy.hanning(fftsize + 1)[:-1]    \n    raw1 = np.array([20 *np.log(np.abs(np.fft.rfft(w * x[i:i + fftsize]))+1) for i in range(0, len(x) - fftsize, hop)])\n    raw2 = np.array([np.angle(np.fft.rfft(w * x[i:i + fftsize])) for i in range(0, len(x) - fftsize, hop)])\n    return [raw1[:, :(fftsize // 2)], raw2[:, :(fftsize // 2)]]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 as cv2\ndef features_extraction(df, columns):\n    ids = df.series_id.unique()\n    features1 = []\n    features2 = []\n    for ide in ids:\n        id_features1 = []\n        id_features2 = []\n        for column in columns:\n            img1, img2 = stft(df[df['series_id']==ide][column])\n            id_features1.append(cv2.resize(img1,(12,12)))\n            id_features2.append(cv2.resize(img2,(12,12)))\n        features1.append(np.array(id_features1))\n        features2.append(np.array(id_features2))\n    return np.transpose(np.array(features1), (0, 3, 2, 1)), np.transpose(np.array(features2), (0, 3, 2, 1))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['orientation_X', 'orientation_Y','orientation_Z','orientation_W', 'angular_velocity_X','angular_velocity_Y','angular_velocity_Z', 'linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']\nfeatures1, features2 = features_extraction(x_train, columns)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-Trained CNN and Classifiers"},{"metadata":{},"cell_type":"markdown","source":"## CNN Architecture:"},{"metadata":{},"cell_type":"markdown","source":"* Model modified from: https://www.tensorflow.org/tutorials/estimators/cnn#training_and_evaluating_the_cnn_mnist_classifier\n* Muti-input images: x1-abs of stft, x2-angle of stft"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ndef cnn_model_fn(features, labels, mode):\n    \"\"\"Model function for CNN.\"\"\"\n    # Input Layer\n    input_layer1 = tf.reshape(features[\"x1\"], [-1, 12, 12, 10])\n    input_layer2 = tf.reshape(features[\"x2\"], [-1, 12, 12, 10])\n\n    # Convolutional Layer #1\n    conv11 = tf.layers.conv2d(\n      inputs=input_layer1,\n      filters=32,\n      kernel_size=[3, 3],\n      padding=\"same\",\n      activation=tf.nn.relu)\n    \n    conv12 = tf.layers.conv2d(\n      inputs=input_layer2,\n      filters=32,\n      kernel_size=[3, 3],\n      padding=\"same\",\n      activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool11 = tf.layers.max_pooling2d(inputs=conv11, pool_size=[2, 2], strides=2)\n    pool12 = tf.layers.max_pooling2d(inputs=conv12, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2\n    conv21 = tf.layers.conv2d(\n      inputs=pool11,\n      filters=64,\n      kernel_size=[3, 3],\n      padding=\"same\",\n      activation=tf.nn.relu)\n    \n    conv22 = tf.layers.conv2d(\n      inputs=pool12,\n      filters=64,\n      kernel_size=[3, 3],\n      padding=\"same\",\n      activation=tf.nn.relu)\n    \n    # Normalization\n    batch_mean, batch_var = tf.nn.moments(conv21, list(range(len(conv21.get_shape()) - 1)))\n    conv21 = tf.nn.batch_normalization(conv21, batch_mean, batch_var,\n                                       offset=None, scale=None,\n                                       variance_epsilon=1e-3)\n    \n    batch_mean, batch_var = tf.nn.moments(conv22, list(range(len(conv22.get_shape()) - 1)))\n    conv22 = tf.nn.batch_normalization(conv22, batch_mean, batch_var,\n                                       offset=None, scale=None,\n                                       variance_epsilon=1e-3)\n\n    conv2 = tf.concat([conv21, conv22], 3)\n    \n    # Convolutional Layer #3\n    conv3 = tf.layers.conv2d(\n      inputs=conv2,\n      filters=64,\n      kernel_size=[3, 3],\n      padding=\"same\",\n      activation=tf.nn.relu)\n    \n    # Pooling Layer #2\n    pool2 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n\n    # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 3 * 3 * 64 ])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n    # Logits Layer\n    logits = tf.layers.dense(inputs=dropout, units=9)\n\n    # Compute predictions.\n    predictions = {\n    # Generate predictions (for PREDICT and EVAL mode)\n    \"features\": dense\n    }\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n    # Calculate Loss (for both TRAIN and EVAL modes): penalized for imbalanced data\n    loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits))\n\n    # Configure the Training Op (for TRAIN mode)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n    # Add evaluation metrics (for EVAL mode)\n    eval_metric_ops = {\n      \"accuracy\": tf.metrics.accuracy(\n          labels=labels, predictions=predictions[\"classes\"])}\n    return tf.estimator.EstimatorSpec(\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-trained CNN for features"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = tf.estimator.Estimator(model_fn=cnn_model_fn)\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x1\": features1.astype(\"float32\"), \"x2\": features2.astype(\"float32\")},\n    y=label,\n    batch_size=1600,\n    num_epochs=None,\n    shuffle=True)\nclassifier.train(\n    input_fn=train_input_fn,\n    steps=2000)","execution_count":8,"outputs":[{"output_type":"stream","text":"INFO:tensorflow:Using default config.\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpv5ljnj_g\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpv5ljnj_g', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6bb4a15a90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:From <ipython-input-7-1becd04ba119>:14: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.conv2d instead.\nWARNING:tensorflow:From <ipython-input-7-1becd04ba119>:24: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.max_pooling2d instead.\nWARNING:tensorflow:From <ipython-input-7-1becd04ba119>:68: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dense instead.\nWARNING:tensorflow:From <ipython-input-7-1becd04ba119>:70: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dropout instead.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:809: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpv5ljnj_g/model.ckpt.\nINFO:tensorflow:loss = 2.9124007, step = 0\nINFO:tensorflow:global_step/sec: 12.7132\nINFO:tensorflow:loss = 0.20050955, step = 100 (7.869 sec)\nINFO:tensorflow:global_step/sec: 12.9301\nINFO:tensorflow:loss = 0.010819537, step = 200 (7.734 sec)\nINFO:tensorflow:global_step/sec: 12.9465\nINFO:tensorflow:loss = 0.0031165737, step = 300 (7.724 sec)\nINFO:tensorflow:global_step/sec: 12.6001\nINFO:tensorflow:loss = 0.0015295641, step = 400 (7.937 sec)\nINFO:tensorflow:global_step/sec: 12.9893\nINFO:tensorflow:loss = 0.0012379321, step = 500 (7.697 sec)\nINFO:tensorflow:global_step/sec: 12.9744\nINFO:tensorflow:loss = 0.00068658486, step = 600 (7.709 sec)\nINFO:tensorflow:global_step/sec: 13.008\nINFO:tensorflow:loss = 0.00085589255, step = 700 (7.687 sec)\nINFO:tensorflow:global_step/sec: 12.9822\nINFO:tensorflow:loss = 0.00044205054, step = 800 (7.703 sec)\nINFO:tensorflow:global_step/sec: 12.9372\nINFO:tensorflow:loss = 0.00022746682, step = 900 (7.728 sec)\nINFO:tensorflow:global_step/sec: 12.9938\nINFO:tensorflow:loss = 0.00035851143, step = 1000 (7.698 sec)\nINFO:tensorflow:global_step/sec: 12.9651\nINFO:tensorflow:loss = 0.0002446471, step = 1100 (7.713 sec)\nINFO:tensorflow:global_step/sec: 12.7333\nINFO:tensorflow:loss = 0.00019835982, step = 1200 (7.853 sec)\nINFO:tensorflow:global_step/sec: 12.8508\nINFO:tensorflow:loss = 0.00019461194, step = 1300 (7.781 sec)\nINFO:tensorflow:global_step/sec: 12.7397\nINFO:tensorflow:loss = 0.00016994198, step = 1400 (7.850 sec)\nINFO:tensorflow:global_step/sec: 12.6639\nINFO:tensorflow:loss = 9.819783e-05, step = 1500 (7.896 sec)\nINFO:tensorflow:global_step/sec: 12.9391\nINFO:tensorflow:loss = 0.000116071205, step = 1600 (7.728 sec)\nINFO:tensorflow:global_step/sec: 12.9695\nINFO:tensorflow:loss = 7.849918e-05, step = 1700 (7.709 sec)\nINFO:tensorflow:global_step/sec: 12.9046\nINFO:tensorflow:loss = 6.9699156e-05, step = 1800 (7.751 sec)\nINFO:tensorflow:global_step/sec: 12.8978\nINFO:tensorflow:loss = 0.0001277464, step = 1900 (7.753 sec)\nINFO:tensorflow:Saving checkpoints for 2000 into /tmp/tmpv5ljnj_g/model.ckpt.\nINFO:tensorflow:Loss for final step: 3.5159803e-05.\n","name":"stdout"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f6bb4a15860>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={\"x1\": features1.astype(\"float32\"),\"x2\": features2.astype(\"float32\")},\n        num_epochs=1,\n        shuffle=False)\npredictions = classifier.predict(input_fn=pre_input_fn);\nres = []\nfor pred_dict in predictions:\n    pretrained = pred_dict['features']\n    res.append(pretrained)","execution_count":34,"outputs":[{"output_type":"stream","text":"INFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmp/tmpv5ljnj_g/model.ckpt-2000\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Training Classifiers"},{"metadata":{},"cell_type":"markdown","source":"### Muti-Class SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedShuffleSplit\ncv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=0)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_svc = svm.SVC(gamma='scale', decision_function_shape='ovo',class_weight=\"balanced\")\nclf_svc.fit(np.array(res), label)\nscores = cross_val_score(clf_svc, np.array(res), label, cv=cv)\nscores","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"array([1., 1., 1., 1., 1.])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf_rf = RandomForestClassifier(n_estimators=120, random_state=0,class_weight=\"balanced\")\nclf_rf.fit(np.array(res), label)\nscores = cross_val_score(clf_rf, np.array(res), label, cv=cv)\nscores","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"array([1., 1., 1., 1., 1.])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Light gbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb  \nimport pickle  \nfrom sklearn.metrics import roc_auc_score  \nfrom sklearn.model_selection import train_test_split  \n  \nX, val_X, y, val_y = train_test_split(np.array(res),label,test_size=0.05,random_state=1,stratify=label)  \n\nlgb_train = lgb.Dataset(X, y)  \nlgb_eval = lgb.Dataset(val_X, val_y, reference=lgb_train)  \n\nparams = {  \n    'boosting_type': 'gbdt',  \n    'objective': 'multiclass',  \n    'num_class': 9,  \n    'metric': 'multi_error',  \n    'num_leaves': 60,  \n    'min_data_in_leaf': 50,  \n    'learning_rate': 0.01,  \n    'feature_fraction': 0.8,  \n    'bagging_fraction': 0.8,  \n    'bagging_freq': 5,  \n    'lambda_l1': 0.4,  \n    'lambda_l2': 0.5,  \n    'min_gain_to_split': 0.2,  \n    'verbose': 5,  \n    'is_unbalance': True  \n}  \n  \n# train  \ngbm = lgb.train(params,  \n                lgb_train,  \n                num_boost_round=3000,  \n                valid_sets=lgb_eval,  \n                early_stopping_rounds=500)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test data"},{"metadata":{},"cell_type":"markdown","source":"## Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = pd.read_csv('../input/X_test.csv')\ntest_features1, test_features2 = features_extraction(x_test, columns)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={\"x1\": test_features1.astype(\"float32\"),\"x2\": test_features2.astype(\"float32\")},\n        num_epochs=1,\n        shuffle=False)\npredictions = classifier.predict(input_fn=pre_input_fn);\nres = []\nfor pred_dict in predictions:\n    pretrained = pred_dict['features']\n    res.append(pretrained)\n# SVM Prediction\nres_svc = clf_svc.predict(np.array(res))\n# RF Prediction\nres_rf = clf_rf.predict(np.array(res))\n# light gbm Prediction\npreds = gbm.predict(np.array(res), num_iteration=gbm.best_iteration)\nres_lgb = []\nfor pred in preds:  \n    res_lgb.append(int(np.argmax(pred)))","execution_count":32,"outputs":[{"output_type":"stream","text":"INFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmp/tmpv5ljnj_g/model.ckpt-2000\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'gbm' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-a25cbe69497d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mres_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# light gbm Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mres_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gbm' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"## Predictions and Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"res_svc = LabelEncoder_x.inverse_transform(res_svc)\nres_rf = LabelEncoder_x.inverse_transform(res_rf)\nres_lgb = LabelEncoder_x.inverse_transform(res_lgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom datetime import datetime\nver = 'CNN_svm'\nfilename = 'subm_{}_{}_'.format(ver, datetime.now().strftime('%Y-%m-%d'))\npd.DataFrame({\n    'series_id': x_test.series_id.unique(),\n    'surface': res_svc\n}).to_csv(filename+'1'+'.csv', index=False)\nver = 'CNN_rf'\nfilename = 'subm_{}_{}_'.format(ver, datetime.now().strftime('%Y-%m-%d'))\npd.DataFrame({\n    'series_id': x_test.series_id.unique(),\n    'surface': res_rf\n}).to_csv(filename+'1'+'.csv', index=False)\nver = 'CNN_lgb'\nfilename = 'subm_{}_{}_'.format(ver, datetime.now().strftime('%Y-%m-%d'))\npd.DataFrame({\n    'series_id': x_test.series_id.unique(),\n    'surface': res_lgb\n}).to_csv(filename+'1'+'.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I only want to show that processing the data with the physical knowlege of data such as conventional signal processing skills still has the ability to retrieve the fair good results. Never like the pure eye-based processing though."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}