{"cells":[{"metadata":{},"cell_type":"markdown","source":"For this kernel I descided not to repeat what others did, so there are no visualizations of the data, and class analysis, you can go to other notebooks for that. I descided that I will go straight to the point :)"},{"metadata":{},"cell_type":"markdown","source":"So this will be our network:\n<img src=\"https://imgur.com/C5YptiK.png\">"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"x_train_data = pd.read_csv('../input/X_train.csv')\ny_train_data = pd.read_csv('../input/y_train.csv')\nx_test_data = pd.read_csv('../input/X_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_data.drop(['series_id', 'measurement_number'], axis=1).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets add some features :)"},{"metadata":{},"cell_type":"markdown","source":"As we see not all data columns have zero mean and variance of 1. To help the training process we going to normalize the data with standard scaller."},{"metadata":{"trusted":true},"cell_type":"code","source":"def quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.degrees(math.atan2(t0, t1))\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.degrees(math.asin(t2))\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.degrees(math.atan2(t3, t4))\n\n    return X, Y, Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(data):\n    eulers = np.array([quaternion_to_euler(*x) for x in data[['orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W']].values])\n    data['euler_orientation_x'] = eulers[:, 0]\n    data['euler_orientation_y'] = eulers[:, 1]\n    data['euler_orientation_z'] = eulers[:, 2]\n\n    accCols = ['linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']\n    gyroCols = ['angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z']\n    quatCols = ['orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W']\n    gyroEulerCols = ['euler_orientation_x', 'euler_orientation_y', 'euler_orientation_z']\n\n    data['angular_velocity_norm'] = np.sqrt(np.sum(np.square(data[gyroCols]),axis=1))\n    data['orientation_norm'] = np.sqrt(np.sum(np.square(data[quatCols]),axis=1))\n    data['linear_acceleration_norm'] = np.sqrt(np.sum(np.square(data[accCols]),axis=1))\n    data['euler_orientation_norm'] = np.sqrt(np.sum(np.square(data[gyroEulerCols]),axis=1))\n    data['acc_vs_vel'] = data['linear_acceleration_norm'] / data['angular_velocity_norm']\n    \n    return data\n\nx_train_data = feature_engineering(x_train_data)\nx_test_data = feature_engineering(x_test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\noriginal_series_id = x_train_data['series_id']\ndf_x_train = pd.DataFrame(sc.fit_transform(x_train_data.drop(['row_id', 'series_id', 'measurement_number'], axis=1)),\n                       columns=x_train_data.drop(['row_id', 'series_id', 'measurement_number'], axis=1).columns)\ndf_x_train['series_id'] = original_series_id\ndf_x_train.describe()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"       orientation_X  orientation_Y      ...          acc_vs_vel      series_id\ncount   4.876800e+05   4.876800e+05      ...        4.876800e+05  487680.000000\nmean   -6.313893e-17   4.491886e-15      ...       -4.406076e-16    1904.500000\nstd     1.000001e+00   1.000001e+00      ...        1.000001e+00    1099.853353\nmin    -1.416154e+00  -1.503351e+00      ...       -3.950798e-01       0.000000\n25%    -1.002005e+00  -1.078812e+00      ...       -2.879285e-01     952.000000\n50%    -1.282060e-01   2.298604e-01      ...       -1.940077e-01    1904.500000\n75%     9.768948e-01   1.037082e+00      ...       -3.418381e-02    2857.000000\nmax     1.468801e+00   1.290433e+00      ...        7.235175e+01    3809.000000\n\n[8 rows x 19 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>orientation_X</th>\n      <th>orientation_Y</th>\n      <th>orientation_Z</th>\n      <th>orientation_W</th>\n      <th>angular_velocity_X</th>\n      <th>angular_velocity_Y</th>\n      <th>angular_velocity_Z</th>\n      <th>linear_acceleration_X</th>\n      <th>linear_acceleration_Y</th>\n      <th>linear_acceleration_Z</th>\n      <th>euler_orientation_x</th>\n      <th>euler_orientation_y</th>\n      <th>euler_orientation_z</th>\n      <th>angular_velocity_norm</th>\n      <th>orientation_norm</th>\n      <th>linear_acceleration_norm</th>\n      <th>euler_orientation_norm</th>\n      <th>acc_vs_vel</th>\n      <th>series_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>4.876800e+05</td>\n      <td>487680.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-6.313893e-17</td>\n      <td>4.491886e-15</td>\n      <td>1.938412e-15</td>\n      <td>-2.961005e-16</td>\n      <td>-2.600537e-17</td>\n      <td>-3.140226e-16</td>\n      <td>-1.569629e-16</td>\n      <td>-1.333096e-17</td>\n      <td>1.054999e-16</td>\n      <td>-9.197266e-16</td>\n      <td>-5.275495e-15</td>\n      <td>-2.077760e-15</td>\n      <td>-1.300913e-16</td>\n      <td>-3.532297e-16</td>\n      <td>1.028696e-11</td>\n      <td>6.801249e-16</td>\n      <td>3.774655e-15</td>\n      <td>-4.406076e-16</td>\n      <td>1904.500000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1099.853353</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.416154e+00</td>\n      <td>-1.503351e+00</td>\n      <td>-1.654092e+00</td>\n      <td>-1.465171e+00</td>\n      <td>-2.013508e+01</td>\n      <td>-1.055745e+01</td>\n      <td>-5.453192e+00</td>\n      <td>-1.935011e+01</td>\n      <td>-5.811808e+01</td>\n      <td>-2.320326e+01</td>\n      <td>-5.064095e+00</td>\n      <td>-4.670922e+00</td>\n      <td>-1.588886e+00</td>\n      <td>-1.065295e+00</td>\n      <td>-2.674867e+00</td>\n      <td>-3.962085e+00</td>\n      <td>-1.298703e+00</td>\n      <td>-3.950798e-01</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-1.002005e+00</td>\n      <td>-1.078812e+00</td>\n      <td>-9.617994e-01</td>\n      <td>-9.804119e-01</td>\n      <td>-3.475569e-01</td>\n      <td>-4.683154e-01</td>\n      <td>-3.122764e-01</td>\n      <td>-3.528890e-01</td>\n      <td>-4.338970e-01</td>\n      <td>-2.910425e-01</td>\n      <td>-3.680604e-01</td>\n      <td>-5.908340e-01</td>\n      <td>-8.179249e-01</td>\n      <td>-6.621412e-01</td>\n      <td>-7.837094e-01</td>\n      <td>-3.973346e-01</td>\n      <td>-8.922462e-01</td>\n      <td>-2.879285e-01</td>\n      <td>952.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-1.282060e-01</td>\n      <td>2.298604e-01</td>\n      <td>1.839337e-01</td>\n      <td>-1.428654e-01</td>\n      <td>-7.924474e-04</td>\n      <td>-3.299045e-02</td>\n      <td>6.043561e-02</td>\n      <td>-2.299244e-03</td>\n      <td>-3.209117e-03</td>\n      <td>-1.455907e-04</td>\n      <td>-1.190265e-01</td>\n      <td>7.473167e-03</td>\n      <td>-2.885605e-01</td>\n      <td>-3.409779e-01</td>\n      <td>6.687445e-04</td>\n      <td>-1.505976e-01</td>\n      <td>-1.809789e-01</td>\n      <td>-1.940077e-01</td>\n      <td>1904.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>9.768948e-01</td>\n      <td>1.037082e+00</td>\n      <td>1.041900e+00</td>\n      <td>9.685492e-01</td>\n      <td>3.426337e-01</td>\n      <td>4.480377e-01</td>\n      <td>3.656402e-01</td>\n      <td>3.544222e-01</td>\n      <td>4.263105e-01</td>\n      <td>2.959879e-01</td>\n      <td>1.144805e-01</td>\n      <td>6.676946e-01</td>\n      <td>9.443379e-01</td>\n      <td>2.595756e-01</td>\n      <td>7.802151e-01</td>\n      <td>2.270091e-01</td>\n      <td>7.763260e-01</td>\n      <td>-3.418381e-02</td>\n      <td>2857.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.468801e+00</td>\n      <td>1.290433e+00</td>\n      <td>1.351793e+00</td>\n      <td>1.520372e+00</td>\n      <td>1.937801e+01</td>\n      <td>1.207493e+01</td>\n      <td>6.137744e+00</td>\n      <td>1.960214e+01</td>\n      <td>3.276607e+01</td>\n      <td>2.643056e+01</td>\n      <td>5.914152e+00</td>\n      <td>6.313439e+00</td>\n      <td>1.897833e+00</td>\n      <td>1.203852e+01</td>\n      <td>2.617213e+00</td>\n      <td>5.227411e+01</td>\n      <td>2.216526e+00</td>\n      <td>7.235175e+01</td>\n      <td>3809.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We need to use the same scaller parameters for the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"original_series_id = x_test_data['series_id']\ndf_x_test = pd.DataFrame(sc.transform(x_test_data.drop(['row_id', 'series_id', 'measurement_number'], axis=1)),\n                       columns=x_test_data.drop(['row_id', 'series_id', 'measurement_number'], axis=1).columns)\ndf_x_test['series_id'] = original_series_id","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_id = x_train_data.series_id.unique()\nset([len(df_x_train[df_x_train['series_id'] == row_id]) for row_id in series_id])","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"{128}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We need to get as much data as we can, so we add sliding window."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sliding_window(x, y, seq_length, shift):\n    data_grouped_by_classes = {key: [] for key in set(y['surface'].values)}\n    for class_name in set(y['surface'].values):\n        for y_row in y.iterrows():\n            y_row = y_row[1]\n            if y_row['surface'] == class_name:\n                data_grouped_by_classes[class_name].append([x[x['series_id'] == y_row['series_id']], y_row['surface']])       \n    \n    x_ret = []\n    y_ret = []\n    for class_name in data_grouped_by_classes.keys():\n        for arr in data_grouped_by_classes[class_name]:\n            arr_y = arr[1]\n            arr_x = arr[0].values\n            batch_size_total = seq_length\n            n_batches = len(arr_x)//batch_size_total\n            arr_x = arr_x[:n_batches * batch_size_total]\n            for n in range(0, arr_x.shape[0], shift):  \n                if n + seq_length > arr_x.shape[0]:\n                    break\n                x_batch = arr_x[n:n+seq_length]\n                y_batch = arr_y\n                x_ret.append(x_batch)\n                y_ret.append(y_batch)\n    return x_ret, y_ret\n\ndef get_data(x, y, batch_size, seq_length, shift):\n    import random\n    data_x = []\n    data_y = []\n    for x_, y_ in zip(*get_sliding_window(x, y, seq_length, shift)):\n        data_x.append(x_)\n        data_y.append(y_)\n    \n    c = list(zip(data_x, data_y))\n    random.shuffle(c)\n    data_x, data_y = zip(*c)\n    ret_x = []\n    ret_y = []\n    \n    for i in range(0, len(data_x), batch_size):\n        if len(data_x) <= i+batch_size:\n            break\n        ret_x.append(np.array(data_x[i:i+batch_size])[0])\n        ret_y.append(np.array(data_y[i:i+batch_size])[0])\n    return np.array(ret_x), np.array(ret_y)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, y_train = get_data(df_x_train, y_train_data, 1, 16, 2)\nx_train = x_train[:, :, :-1]\nx_train = x_train.reshape(-1, 4, 4, 18)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nenc_l = LabelEncoder()\nenc = OneHotEncoder(handle_unknown='ignore')\nnr_y_train = enc_l.fit_transform(np.array(y_train))\none_hot_y_train = enc.fit_transform(nr_y_train.reshape(-1, 1))","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deep Learning Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, LSTM, GRU, SimpleRNN, Conv1D, TimeDistributed, MaxPooling1D, Flatten, Dropout\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_model():\n    model = Sequential()\n    model.add(TimeDistributed(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same'), batch_input_shape=(None, None, 4, 18)))\n    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(16))\n    model.add(Dense(9, activation='softmax'))\n    \n    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n\n    model.compile(optimizer=adam,\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = init_model()\ncallbacks = [EarlyStopping('val_loss', patience=3)]\nmodel.fit(x_train, one_hot_y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=callbacks)","execution_count":25,"outputs":[{"output_type":"stream","text":"Train on 173735 samples, validate on 43434 samples\nEpoch 1/10\n173735/173735 [==============================] - 15s 85us/sample - loss: 1.2104 - acc: 0.5703 - val_loss: 0.8577 - val_acc: 0.7044\nEpoch 2/10\n173735/173735 [==============================] - 13s 76us/sample - loss: 0.7153 - acc: 0.7535 - val_loss: 0.6142 - val_acc: 0.7880\nEpoch 3/10\n173735/173735 [==============================] - 13s 76us/sample - loss: 0.5622 - acc: 0.8082 - val_loss: 0.5185 - val_acc: 0.8188\nEpoch 4/10\n173735/173735 [==============================] - 13s 78us/sample - loss: 0.4858 - acc: 0.8313 - val_loss: 0.4606 - val_acc: 0.8388\nEpoch 5/10\n173735/173735 [==============================] - 13s 77us/sample - loss: 0.4343 - acc: 0.8484 - val_loss: 0.4131 - val_acc: 0.8558\nEpoch 6/10\n173735/173735 [==============================] - 13s 77us/sample - loss: 0.3967 - acc: 0.8600 - val_loss: 0.3801 - val_acc: 0.8670\nEpoch 7/10\n173735/173735 [==============================] - 16s 89us/sample - loss: 0.3682 - acc: 0.8691 - val_loss: 0.3644 - val_acc: 0.8681\nEpoch 8/10\n173735/173735 [==============================] - 14s 80us/sample - loss: 0.3458 - acc: 0.8767 - val_loss: 0.3397 - val_acc: 0.8787\nEpoch 9/10\n173735/173735 [==============================] - 13s 77us/sample - loss: 0.3261 - acc: 0.8843 - val_loss: 0.3221 - val_acc: 0.8850\nEpoch 10/10\n173735/173735 [==============================] - 13s 76us/sample - loss: 0.3097 - acc: 0.8902 - val_loss: 0.3090 - val_acc: 0.8915\n","name":"stdout"},{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f02a7ca2e10>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_id = df_x_test.series_id.unique()\nour_predictions = []\nfor row_id in series_id:\n    batch_data = df_x_test[df_x_test['series_id'] == row_id].drop(['series_id'], axis=1).values[:128, :].reshape(8, 16, 18).reshape(8, 4, 4, 18)\n    predictions = model.predict(batch_data)\n    final_predictions = list(sum(predictions)/len(predictions))\n    our_predictions.append(final_predictions)\n\nlabel_rez = enc_l.inverse_transform(enc.inverse_transform(our_predictions))\n\ndf = pd.DataFrame({'series_id': series_id.tolist(),'surface': label_rez.tolist()})\ndf.to_csv('submission.csv', index=False)","execution_count":27,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-e89a2b84f906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mour_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseries_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_x_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_x_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'series_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrow_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'series_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfinal_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 raise TypeError('Could not compare {typ} type with Series'\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid type comparison\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}