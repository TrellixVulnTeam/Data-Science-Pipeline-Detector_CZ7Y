{"cells":[{"metadata":{},"cell_type":"markdown","source":"Name: Zete Dai\n\nNetID: zd790"},{"metadata":{},"cell_type":"markdown","source":"This solution is based on the PyTorch course: [Intro to Deep Learning with PyTorch](https://classroom.udacity.com/courses/ud188)"},{"metadata":{},"cell_type":"markdown","source":"# Importing the Package"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport torch # PyTorch package\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Data"},{"metadata":{},"cell_type":"markdown","source":"### Split the Data\n\nBecause the kaggle submission judgement is wrong for some unknown reason, we have to split the data to test the result for ourselves"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read Kaggle datasets\nX_train_old = pd.read_csv('/kaggle/input/career-con-2019/X_train.csv')\ny_train_old = pd.read_csv('/kaggle/input/career-con-2019/y_train.csv')\n# split X_train\nsamples = 20\ntime_series = 128\nstart_x = X_train_old.shape[0] - samples*time_series\nX_train, X_test = X_train_old.iloc[:start_x], X_train_old.iloc[start_x:]\n# split y_train\nstart_y = y_train_old.shape[0] - samples\ny_train, y_test = y_train_old.iloc[:start_y], y_train_old.iloc[start_y:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inspect the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspect the data\ndisplay(X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge the Data\n\nAfter inspecting the data, we found that the \"series_id\" is the primary key in y, and it's also the foreign key to y in X.\n\nUsually in deep learning, it's easier for training that each example in X to have a y label, so in order to do that, we merge the X dataframe and y dataframe according to the key \"series_id\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge X and y so we have the y label for each row in X\n# Because there is no ommitted data (perfect), we don't need to specify how to merge\nXy_train = X_train.merge(y_train, on='series_id')\nXy_test = X_test.merge(y_test, on='series_id')\ndisplay(Xy_train, Xy_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extract the Features\n\nAfter looking at each column of the Xy dataframe, we can find that there are only 9 actual features for predicting the surface type:'orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W', 'angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z', 'linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z'. The other features like 'row_id', 'series_id' and 'measurement_number' are just for indexing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features for predict the surface type\nX_columns = ['orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W', 'angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z', 'linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's easier to use the integers to indicate the class so we use dictionaries to encode surface type to integers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use dictionarys to map different surface types to int for easier calculation\nencode_dic = {'fine_concrete': 0, \n              'concrete': 1, \n              'soft_tiles': 2, \n              'tiled': 3, \n              'soft_pvc': 4,\n              'hard_tiles_large_space': 5, \n              'carpet': 6, \n              'hard_tiles': 7, \n              'wood': 8}\n\ndecode_dic = {0: 'fine_concrete',\n              1: 'concrete',\n              2: 'soft_tiles',\n              3: 'tiled',\n              4: 'soft_pvc',\n              5: 'hard_tiles_large_space',\n              6: 'carpet',\n              7: 'hard_tiles',\n              8: 'wood'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert into PyTorch tensors\n\nIn order to work with PyTorch, we have to convert the Pandas dataframes into PyTorch tensors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert pandas dataframes into PyTorch tensors\nX_train = torch.tensor(Xy_train[X_columns].values).float()\ny_train = torch.tensor(Xy_train['surface'].map(encode_dic).values)\nX_test = torch.tensor(Xy_test[X_columns].values).float()\ny_test = torch.tensor(Xy_test['surface'].map(encode_dic).values)\ndisplay(X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if the shapes of X and y are aligned"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build and Train the Neural Network\n\nWe first have to define our Neural Network, then at each training step (epoch) in deep learning:\n\n1. Do a forward pass (propagation) to get the output of our Neural Network\n2. Use the output of our Neural Network to calculate the cost function\n3. Use the cost function to do a back propagation to calculate the gradient of the cost function with regard to each weight\n4. Use a certain optimization algoritm (gradient descent) to update the weights\n\nTherefore, while defining our Neural Network, we need to:\n1. Define the structure of Neural Network\n2. Define cost funtion and optimization algorithm (gradient descent)"},{"metadata":{},"cell_type":"markdown","source":"### Define the Structure of Neural Network\n\n\nEach measurement has 10 features, and there are 9 classes to predict, so the input layer has 10 neurons and the output layer has 9 output. The number of hidden layers and the number of how many units in each hidden layer is mostly experimental. Except the output layer, we all use the ReLU as the activation function funcion, and we don't use ReLU in output layer is mainly because the cost function we choose (which will be explained in the next section).\n\nHere are the image preview of the structure of our Neural Network:\n![NN Structure](https://i.imgur.com/G2k28an.png)\nThere are 7 layers, the number of units in each layer is 10, 63, 54, 45, 36, 27, 9."},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn\n\nmodel = nn.Sequential(nn.Linear(10, 63),\n                      nn.ReLU(),\n                      nn.Linear(63, 54),\n                      nn.ReLU(),\n                      nn.Linear(54, 45),\n                      nn.ReLU(),\n                      nn.Linear(45, 36),\n                      nn.ReLU(),\n                      nn.Linear(36, 27),\n                      nn.ReLU(),\n                      nn.Linear(27, 9)\n                     )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define Cost Funtion and Optimization Algorithm (Gradient Descent)\n\nThe cost function we use is Cross Entropy Loss Function, according to the offical document of [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/nn.html?highlight=cross#torch.nn.CrossEntropyLoss), this criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. Therefore, we don't need to use any activation function in the output layer (but if we want to calculate the probabilities of each class, we still have to apply a softmax function to the final output).\n\nThe optimization algorithm (gradient descent) we use is [Adam](https://arxiv.org/abs/1412.6980), which is an alternative for batch/normal gradient descent or stochastic gradient descent. Adam \"is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters\"[1]. The learning rate is mostly experimental.\n\n[1]Kingma, Diederik P., and Jimmy Ba. \"Adam: A method for stochastic optimization.\" arXiv preprint arXiv:1412.6980 (2014)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use GPU to Speed Up\n\nWe can utilize the GPU to significantly increase the speed of training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the computational device to GPU\ndevice = torch.device(\"cuda:0\")\n\n# Move the neural network model to GPU\nmodel.to(device)\n\n# Move all the tensors to GPU\nX_train, y_train, X_test, y_test = X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the Neural Network\n\nAs stated before, at each training step (epoch) in deep learning:\n1. Do a forward pass (propagation) to get the output of our Neural Network\n2. Use the output of our Neural Network to calculate the cost function\n3. Use the cost function to do a back propagation to calculate the gradient of the cost function with regard to each weight\n4. Use a certain optimization algoritm (gradient descent) to update the weights\n\nThe reason we use [optimizer.zero_grad()](https://pytorch.org/docs/stable/optim.html?highlight=zero_grad#torch.optim.Optimizer.zero_grad) is that PyTorch by default will trace every calculation we do to the input and use these traces to later calculate the gradients of weights, and after each epoch, these gradients will not be reset by default, so we need to manually reset the gradients of weights before forward passing in each epoch.\n\nThe training process will take several minutes."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nepochs = 2500\n\nfor e in range(epochs):\n    \n    # Reset the gradients\n    optimizer.zero_grad()\n    \n    # Forward passing\n    output = model(X_train)\n\n    # Calculate the loss/cost function\n    loss = criterion(output, y_train)\n    \n    # Back propagation\n    loss.backward()\n    \n    # Update the weights\n    optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict the Surfaces\n\nWith our Neural Network trained, we can now predict the surface type in the test set and calculate the accuracy.\n\nThe reason we use [torch.no_grad()](https://pytorch.org/docs/stable/autograd.html?highlight=no_grad#torch.autograd.no_grad) is that we don't want PyTorch to trace our calculations in predicting process because we don't need to back propagate here."},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    # Calculate the output of Neural Network\n    network_output = model(X_test)\n    \n    # Use the softmax to calculate the probabilities of each class, \"dim=1\" means across the columns\n    possibilities = torch.softmax(network_output, dim=1)\n    \n    # Use argmax to find the class has the highest probability\n    predict = possibilities.argmax(dim=1)\n    \n    # Compare the predicted result with y_test to find out the accuracy\n    acc = (1.0 * (predict==y_test).sum().item() / y_test.shape[0])\n    \n    print(f\"Accuracy = {acc}\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}