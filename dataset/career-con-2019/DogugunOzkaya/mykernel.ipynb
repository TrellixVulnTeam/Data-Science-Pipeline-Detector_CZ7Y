{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nfor f in os.listdir('../input'):\n    print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"File sizes are pretty fair. So no need to adjust feature types for space utilization."},{"metadata":{"trusted":true,"_uuid":"c5047bb5b87903400d160503ef69e0b28fd28888"},"cell_type":"code","source":"x_df = pd.read_csv('../input/X_train.csv')\nx_test_df = pd.read_csv('../input/X_test.csv')\ny_df = pd.read_csv('../input/y_train.csv')\n\n\nx_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fba8a0db7e6a9bdfd770e72de37050429bc92984"},"cell_type":"code","source":"y_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8e05856aa183ce17063c147a2c4c8b785e7b3ff"},"cell_type":"code","source":"x_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"139804cf91d84d99107e94a81125cd3625e12ec6"},"cell_type":"code","source":"x_test_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a913035e7a096ab4dd94a329ac5b7aa5cd54a81"},"cell_type":"code","source":"y_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16ed306d96e949b588fb613b89fd49416aecc7a6"},"cell_type":"code","source":"len(y_df['series_id'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05c59ba78b3a135e1a127bac95083f55c2ece13c"},"cell_type":"code","source":"len(x_df['series_id'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dbdf259ac97b7953231d8e8ffd198a397bdee02"},"cell_type":"code","source":"len(x_df['measurement_number'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e37a4693f151a13b11e9c30b4e88b31e8bc8b110"},"cell_type":"markdown","source":"Let's see if there is any null values in our files. If not, then merge them."},{"metadata":{"trusted":true,"_uuid":"48931a3399c1a1ef7ecf74182d256c21e92d34be"},"cell_type":"code","source":"print(y_df.head())\n\nprint(x_df.isnull().values.any())\nprint(x_test_df.isnull().values.any())\nprint(y_df.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff8f2d65b31c8543d335df742f984b17245ccf71"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style='darkgrid')\nsns.countplot(y = 'surface',\n              data = y_df,\n              order = y_df['surface'].value_counts().index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c165392a05cea089e080d90f86300365d6e9eaaa"},"cell_type":"markdown","source":"Now let's convert the quaternions to Euler coordinates. I'd suggest [this](https://developerblog.myo.com/quaternions/) short reading to have an introductionary idea about how quaternions work. TL;DR: quaternions represent a formula for the angular rotations of an object. Since they are represented in imaginary numbers, we should convert them to Euler angles in each state. \n\nI will use below formula for this conversion:\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/91bfc471db4c589bb4bbf4f3b138c19769db1bb2)\n"},{"metadata":{"trusted":true,"_uuid":"74fda259a63f33d28bb79e9a38ca21fd04ffe5ab"},"cell_type":"code","source":"import math\ndef phi(x):    \n    return math.atan2(x['orientation_W']*x['orientation_X'] + x['orientation_Y']*x['orientation_Z'], 1-2*(x['orientation_X']**2+x['orientation_Y']**2))\n\ndef theta(x):\n    return math.asin(2*(x['orientation_W']*x['orientation_Y'] - x['orientation_Z']*x['orientation_X']))\n\ndef chi(x):\n    return math.atan2(2*(x['orientation_W']*x['orientation_Z']+x['orientation_X']*x['orientation_Y']), 1-2*(x['orientation_Y']**2+x['orientation_Z']**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c4eee613334fa081e2e1971fde4b96d428b6c96"},"cell_type":"code","source":"x_df['phi'] = x_df.apply(phi, axis=1)\nx_df['theta'] = x_df.apply(theta, axis=1)\nx_df['chi'] = x_df.apply(chi, axis=1)\n\nx_test_df['phi'] = x_test_df.apply(phi, axis=1)\nx_test_df['theta'] = x_test_df.apply(theta, axis=1)\nx_test_df['chi'] = x_test_df.apply(chi, axis=1)\n\nx_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"160df4b956019777f0ebf503542cfef01ae614d3"},"cell_type":"markdown","source":"As our inputs are complete so far, let's see the correlation among them"},{"metadata":{"trusted":true,"_uuid":"4a00a248a588498b2e3d1b860232fb104e9af369"},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(x_df.iloc[:,3:].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2844d9cdff60161f74fd59dd93fc2c99d1aa59a3"},"cell_type":"markdown","source":"Now let's check the features' distributions. If there are features that are not normal, I'll try to transform them into normal/Gaussian distribution."},{"metadata":{"trusted":true,"_uuid":"0f4bcfc5802d62105ee94de0f608b5dbe678b15c"},"cell_type":"code","source":"def plot_feature_distribution(df1, df2, label1, label2, features,a=3,b=6):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(17,9))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5afe4643dbfa6ba4080f21584a87c797ad73a450","trusted":true},"cell_type":"code","source":"features = x_df.columns.values[3:]\nplot_feature_distribution(x_df, x_test_df, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10496051d84330104baf4efb0284761a5e90f6f8"},"cell_type":"markdown","source":"As seene here, orientation X, orientation Y and chi angle in Euler form has a distruption from normal distribution. "},{"metadata":{"trusted":true,"_uuid":"811c874f3ac92a5df0e0a8022cd4d2773bc9b822"},"cell_type":"code","source":"def plot_feature_class_distribution(classes,tt, features,a=5,b=3):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(16,24))\n\n    for feature in features:\n        \n        i += 1\n        plt.subplot(a,b,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.kdeplot(ttc[feature], bw=0.5,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06569ea67082e2db17a5241d6719a9ea137ef3df"},"cell_type":"code","source":"plt.clf()\nplt.cla()\nplt.close()\nclasses = (y_df['surface'].value_counts()).index\nprint(x_df.head())\ndata = x_df.merge(y_df, on='series_id', how='inner')\nplot_feature_class_distribution(classes, data, features)\nx_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21e82a6a41c892b36d664cf74d48c4c4eba230a2"},"cell_type":"code","source":"\nx_df['chi_new'] = x_df['chi']*np.log10(np.absolute(x_df['chi']))\nx_test_df['chi_new'] = x_test_df['chi']*np.log10(np.absolute(x_test_df['chi']))\n\n\n\nsns.kdeplot(x_df['chi_new'], bw=0.5,label='train')\nsns.kdeplot(x_test_df['chi_new'], bw=0.5,label='test')\nplt.xlabel('chi_new', fontsize=9)\nlocs, labels = plt.xticks()\nplt.tick_params(axis='x', which='major', labelsize=8)\nplt.tick_params(axis='y', which='major', labelsize=8)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c5ccaaec20ea983ab0e87eba2deb85b5ba5e9de"},"cell_type":"code","source":"x_df['orientation_X_new'] = x_df['orientation_X']*np.absolute(x_df['orientation_X']) #np.log2(x_df['orientation_X'])\nx_test_df['orientation_X_new'] = x_test_df['orientation_X']*np.absolute(x_test_df['orientation_X'])#np.log2(x_test_df['orientation_X'])\n\nsns.kdeplot(x_df['orientation_X_new'], bw=0.5,label='train')\nsns.kdeplot(x_test_df['orientation_X_new'], bw=0.5,label='test')\nplt.xlabel('orientation_X_new', fontsize=9)\nlocs, labels = plt.xticks()\nplt.tick_params(axis='x', which='major', labelsize=8)\nplt.tick_params(axis='y', which='major', labelsize=8)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e879fa82fe9685997b0337244beeec9b5577d97c"},"cell_type":"code","source":"x_df['orientation_Y_new'] = x_df['orientation_Y']*np.absolute(x_df['orientation_Y'])\nx_test_df['orientation_Y_new'] = x_test_df['orientation_Y']*np.absolute(x_test_df['orientation_Y'])\n\nsns.kdeplot(x_df['orientation_Y_new'], bw=0.5,label='train')\nsns.kdeplot(x_test_df['orientation_Y_new'], bw=0.5,label='test')\nplt.xlabel('orientation_Y_new', fontsize=9)\nlocs, labels = plt.xticks()\nplt.tick_params(axis='x', which='major', labelsize=8)\nplt.tick_params(axis='y', which='major', labelsize=8)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b71016c33183853351e1c78891d2af700af72732"},"cell_type":"code","source":"def normalize_quaternions (data):\n    data['norm_quat'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2 + data['orientation_W']**2)\n    data['mod_quat'] = (data['norm_quat'])**0.5\n    data['norm_X'] = data['orientation_X'] / data['mod_quat']\n    data['norm_Y'] = data['orientation_Y'] / data['mod_quat']\n    data['norm_Z'] = data['orientation_Z'] / data['mod_quat']\n    data['norm_W'] = data['orientation_W'] / data['mod_quat']\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2068b86791489c33361d3075a63a985775a57806","_kg_hide-output":false},"cell_type":"code","source":"x_df = normalize_quaternions(x_df)\nx_test_df = normalize_quaternions(x_test_df)\n\nx_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feat_eng(data):\n    \n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2)** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2)**0.5\n    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2)**0.5\n    data['acc_vs_vel'] = data['totl_linr_acc'] / data['totl_anglr_vel']\n    \n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] / df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_mean_change_of_abs_change'] = data.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])/2\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_df = feat_eng(x_df)\nx_test_df = feat_eng(x_test_df)\nx_df=x_df.reset_index()\nx_test_df=x_test_df.reset_index()\n\nprint(x_df.shape)\nx_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = x_df.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.tail(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_df.shape)\nprint(x_test_df.shape)\nx_df = x_df.drop(['acc_vs_vel_max',\\\n           'acc_vs_vel_min',\\\n           'mod_quat_max',\\\n           'mod_quat_min',\\\n           'norm_quat_max',\\\n           'norm_quat_min',\\\n           'phi_max',\\\n           'phi_min',\\\n           'totl_anglr_vel_max',\\\n           'totl_anglr_vel_min',\\\n           'totl_linr_acc_max',\\\n           'totl_linr_acc_min',\\\n           'totl_xyz_max',\\\n           'totl_xyz_min'], axis=1)\n\nx_test_df = x_test_df.drop(['acc_vs_vel_max',\\\n           'acc_vs_vel_min',\\\n           'mod_quat_max',\\\n           'mod_quat_min',\\\n           'norm_quat_max',\\\n           'norm_quat_min',\\\n           'phi_max',\\\n           'phi_min',\\\n           'totl_anglr_vel_max',\\\n           'totl_anglr_vel_min',\\\n           'totl_linr_acc_max',\\\n           'totl_linr_acc_min',\\\n           'totl_xyz_max',\\\n           'totl_xyz_min'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\ny_df['surface'] = le.fit_transform(y_df['surface'])\ny_df['surface'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\n\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=59)\npredicted = np.zeros((x_test_df.shape[0],9))\nmeasured= np.zeros((x_df.shape[0]))\nscore = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom operator import itemgetter\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n\ndef run_gridsearch(X, y, clf, param_grid, cv=5):\n    #Run a grid search for best Decision Tree parameters => X:features, y:target\n    #clf:classifier, param_grid:parameters, cv:k-foldCV\n    #top_params: [dict]\n    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=cv, verbose=2)\n    start = time()\n    grid_search.fit(X, y)\n    print((\"\\nGridSearchCV took {:.2f} seconds for {:d} candidate\"\n           \"parameter settings.\").format(time() - start, len(grid_search.cv_results_)))\n    top_params = report(grid_search.cv_results_, 2)\n    print(grid_search.best_params_)\n    return  top_params\n\ndef report(grid_scores, n_top=2):\n    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n    print(grid_scores['mean_train_score'])#top_scores[0]\n    #for top_score in top_scores:\n    #    print('mean_train_score:' + str(top_score['mean_train_score']) +top_score['params'])\n    return top_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nwarnings.filterwarnings(\"ignore\") # Don't want to see the warnings in the notebook\nprint(\"-- Grid Parameter Search via 10-fold CV\")\n# set of parameters to test\n'''\nparam_grid = {\"criterion\": [\"gini\"],\n              \"n_estimators\": [500],\n              \"min_samples_split\": [5, 10],\n              \"max_depth\": [20, 25],\n              \"min_samples_leaf\": [20,  50],\n              \"max_leaf_nodes\": [20, 40]\n              }\n              '''\nparam_grid = {\"loss\": [\"deviance\"],\n              \"n_estimators\": [500],\n              \"min_samples_split\": [5,  20],\n              \"max_depth\": [10,  50],\n              \"min_samples_leaf\": [10,   50],\n              \"max_leaf_nodes\": [20, 40]\n              }\nseed=43\nclf = GradientBoostingClassifier(random_state=seed)\n#ts_gs = run_gridsearch(x_df, y_df['surface'], clf, param_grid, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.model_selection as ms\nX_train, X_test, y_train, y_test = ms.train_test_split(x_df, y_df['surface'], \n                                                                    test_size=0.25, \n                                                                    random_state=seed,\n                                                                   stratify= y_df['surface'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n#clf = RandomForestClassifier(random_state=seed,criterion='gini', max_depth=20, max_leaf_nodes=40, \n#                            min_samples_leaf=20, min_samples_split=5, n_estimators=500)\nclf=RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n#clf = GradientBoostingClassifier(random_state=seed,loss= 'deviance', \n#                                 max_depth= 10, max_leaf_nodes= 20, min_samples_leaf= 50, \n#                                 min_samples_split= 5, n_estimators= 100)\n\n\nclf.fit(x_df, y_df['surface'])\ntrn_acc = accuracy_score(y_train, clf.predict(X_train))\ntst_acc = accuracy_score(y_test, clf.predict(X_test))\n\nprint('training accuracy:', trn_acc)\nprint('test accuracy    :', tst_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(0.2, 1.0, 50)):\n    plt.figure(figsize=(10,6))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, '-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, '-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    plt.grid(\"on\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.model_selection import learning_curve\n\n#kfold = model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n#plot_learning_curve(clf, 'Learning Curve', x_df, y_df['surface'], cv=kfold)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(x_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_test_df['surface'] = le.inverse_transform(y_pred)\nsub_df = x_test_df.filter(['series_id','surface'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission_last_5.csv', index=False)\nprint(sub_df.shape)\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Avg Accuracy RF', score / folds.n_splits)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}