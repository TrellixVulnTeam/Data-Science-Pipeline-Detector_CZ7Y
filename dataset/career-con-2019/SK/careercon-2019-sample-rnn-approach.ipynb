{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nimport seaborn as sns\nimport random \nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['X_train.csv', 'sample_submission.csv', 'X_test.csv', 'y_train.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_X_train = pd.read_csv('../input/X_train.csv')\ndf_y_train = pd.read_csv('../input/y_train.csv')\ndf_X_test = pd.read_csv('../input/X_test.csv')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_X_train.shape,df_X_test.shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"(487680, 13) (488448, 13)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\n\ndef fe_step0 (actual):\n    \n    # https://www.mathworks.com/help/aeroblks/quaternionnorm.html\n    # https://www.mathworks.com/help/aeroblks/quaternionmodulus.html\n    # https://www.mathworks.com/help/aeroblks/quaternionnormalize.html\n        \n    actual['norm_quat'] = (actual['orientation_X']**2 + actual['orientation_Y']**2 + actual['orientation_Z']**2 + actual['orientation_W']**2)\n    actual['mod_quat'] = (actual['norm_quat'])**0.5\n    actual['norm_X'] = actual['orientation_X'] / actual['mod_quat']\n    actual['norm_Y'] = actual['orientation_Y'] / actual['mod_quat']\n    actual['norm_Z'] = actual['orientation_Z'] / actual['mod_quat']\n    actual['norm_W'] = actual['orientation_W'] / actual['mod_quat']\n    \n    return actual\n\ndef fe_step1 (actual):\n    \"\"\"Quaternions to Euler Angles\"\"\"\n    \n    x, y, z, w = actual['norm_X'].tolist(), actual['norm_Y'].tolist(), actual['norm_Z'].tolist(), actual['norm_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    actual['euler_x'] = nx\n    actual['euler_y'] = ny\n    actual['euler_z'] = nz\n    return actual\n\n\nprint('Before Step 0:', df_X_train.shape,df_X_test.shape)\n\ndf_X_train = fe_step0(df_X_train)\ndf_X_test = fe_step0(df_X_test)\n\nprint('Before Step 1:',df_X_train.shape,df_X_test.shape)\n\ndf_X_train = fe_step1(df_X_train)\ndf_X_test = fe_step1(df_X_test)\n\nprint('Before Step 2:',df_X_train.shape,df_X_test.shape)\n","execution_count":10,"outputs":[{"output_type":"stream","text":"Before Step 0: (487680, 13) (488448, 13)\nBefore Step 1: (487680, 19) (488448, 19)\nBefore Step 2: (487680, 22) (488448, 22)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_X_train.shape[1]","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"22"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"2514586f60b25a7a8941022f68a9fecac5ecb4dd"},"cell_type":"code","source":"train_dataset_all = np.array(df_X_train.drop(columns=['row_id', 'series_id','measurement_number'])).reshape(-1,128,df_X_train.shape[1]-3)\ntrain_labels_all = pd.get_dummies(df_y_train['surface'])\nlabel_names = [col for col in train_labels_all.columns]\ntrain_labels_all = np.array(train_labels_all)\n\ntrain_dataset, val_dataset, train_labels, val_labels = train_test_split(train_dataset_all, train_labels_all, test_size=0.1, random_state=0)\ntest_dataset_all = np.array(df_X_test.drop(columns=['row_id', 'series_id','measurement_number'])).reshape(-1,128,df_X_train.shape[1]-3)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.shape,val_dataset.shape","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"((3429, 128, 19), (381, 128, 19))"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"236867539f7268f09dfb1d014ce119fb6cc6ba96"},"cell_type":"code","source":"#http://ataspinar.com/2018/07/05/building-recurrent-neural-networks-in-tensorflow/\nsignal_length = 128\nnum_components = train_dataset.shape[2]\nnum_labels = 9\n\nnum_hidden = 128\nlearning_rate = 0.001\nlambda_loss = 0.010\ntotal_steps = 1000\ndisplay_step = 100\nbatch_size = 1000\n\ntf.reset_default_graph()\n\ndef lstm_rnn_model(data, num_hidden, num_labels):\n    splitted_data = tf.unstack(data, axis=1)\n    \n    cell = tf.keras.layers.LSTMCell(num_hidden)\n\n    outputs, current_state = tf.nn.static_rnn(cell, splitted_data, dtype=tf.float32)\n    output = outputs[-1]\n    \n    w_softmax = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n    b_softmax = tf.Variable(tf.random_normal([num_labels]))\n    logit = tf.matmul(output, w_softmax) + b_softmax\n    return logit\n\ndef bidirectional_rnn_model(data, num_hidden, num_labels):\n    splitted_data = tf.unstack(data, axis=1)\n\n    lstm_cell1 = tf.keras.layers.LSTMCell(num_hidden) #, forget_bias=1.0, state_is_tuple=True\n    lstm_cell2 = tf.keras.layers.LSTMCell(num_hidden) # , forget_bias=1.0, state_is_tuple=True\n    outputs, _, _ = tf.nn.static_bidirectional_rnn(lstm_cell1, lstm_cell2, splitted_data, dtype=tf.float32)\n    #outputs, _, _ = keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))\n    output = outputs[-1]\n    \n    w_softmax = tf.Variable(tf.truncated_normal([num_hidden*2, num_labels]))\n    b_softmax = tf.Variable(tf.random_normal([num_labels]))\n    logit = tf.matmul(output, w_softmax) + b_softmax\n    return logit\n\ndef accuracy(y_predicted, y):\n    return (100.0 * np.sum(np.argmax(y_predicted, 1) == np.argmax(y, 1)) / y_predicted.shape[0])\n \n####\n\n#1) First we put the input data in a tensorflow friendly form.    \ntf_dataset = tf.placeholder(tf.float32, shape=(None, signal_length, num_components))\ntf_labels = tf.placeholder(tf.float32, shape = (None, num_labels))\n\n#2) Then we choose the model to calculate the logits (predicted labels)\n# We can choose from several models:\n#logits = rnn_model(tf_dataset, num_hidden, num_labels)\n#logits = lstm_rnn_model(tf_dataset, num_hidden, num_labels)\nlogits = bidirectional_rnn_model(tf_dataset, num_hidden, num_labels)\n#logits = multi_rnn_model(tf_dataset, num_hidden, num_labels)\n#logits = gru_rnn_model(tf_dataset, num_hidden, num_labels)\n\n#3) Then we compute the softmax cross entropy between the logits and the (actual) labels\nl2 = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_labels)) + l2\n\n#4. \n# The optimizer is used to calculate the gradients of the loss function \noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n#optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n#optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n\n# Predictions for the training, validation, and test data.\nprediction = tf.nn.softmax(logits)\n\nsession = tf.Session()\n \ntf.global_variables_initializer().run(session=session)\n\n\n# Add ops to save and restore all the variables.\nsaver = tf.train.Saver()\n\n\nprint(\"\\nInitialized\")\n\nfor step in range(total_steps):\n    #Since we are using stochastic gradient descent, we are selecting  small batches from the training dataset,\n    #and training the convolutional neural network each time with a batch. \n    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n    batch_data = train_dataset[offset:(offset + batch_size), :, :]\n    batch_labels = train_labels[offset:(offset + batch_size), :]\n\n    feed_dict = {tf_dataset : batch_data, tf_labels : batch_labels}\n    _, l, train_predictions = session.run([optimizer, loss, prediction], feed_dict=feed_dict)\n    train_accuracy = accuracy(train_predictions, batch_labels)\n\n    if step % display_step == 0:\n        feed_dict = {tf_dataset : val_dataset, tf_labels : val_labels}\n        #feed_dict = {tf_dataset : train_dataset, tf_labels : train_labels}\n        _, val_predictions = session.run([loss, prediction], feed_dict=feed_dict)\n        test_accuracy = accuracy(val_predictions, val_labels)\n        #test_accuracy = accuracy(train_predictions, train_labels)\n        message = \"step {:04d} : loss is {:06.2f}, accuracy on training set {} %, accuracy on test set {:02.2f} %\".format(step, l, train_accuracy, test_accuracy)\n        print(message)\n        \nsave_path = saver.save(session, \"/tmp/model.ckpt\")\nprint(\"Model saved in path: %s\" % save_path)\n    \nfeed_dict = {tf_dataset : test_dataset_all, tf_labels : val_labels}  \ntest_predictions = session.run(prediction, feed_dict=feed_dict)","execution_count":15,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-15-982647616a41>:33: static_bidirectional_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))`, which is equivalent to this API\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:1565: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From <ipython-input-15-982647616a41>:61: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\n\nInitialized\nstep 0000 : loss is 010.97, accuracy on training set 13.2 %, accuracy on test set 12.60 %\nstep 0100 : loss is 006.06, accuracy on training set 57.0 %, accuracy on test set 53.02 %\nstep 0200 : loss is 004.89, accuracy on training set 71.8 %, accuracy on test set 67.98 %\nstep 0300 : loss is 004.18, accuracy on training set 78.0 %, accuracy on test set 69.55 %\nstep 0400 : loss is 003.62, accuracy on training set 82.0 %, accuracy on test set 75.85 %\nstep 0500 : loss is 003.07, accuracy on training set 87.0 %, accuracy on test set 76.90 %\nstep 0600 : loss is 002.72, accuracy on training set 87.0 %, accuracy on test set 78.48 %\nstep 0700 : loss is 002.25, accuracy on training set 91.8 %, accuracy on test set 77.69 %\nstep 0800 : loss is 001.98, accuracy on training set 93.0 %, accuracy on test set 78.48 %\nstep 0900 : loss is 001.70, accuracy on training set 95.0 %, accuracy on test set 79.27 %\nModel saved in path: /tmp/model.ckpt\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"59446e63b5f62a5b5a2c3bd2854aabe6499ad91c"},"cell_type":"code","source":"submission = pd.read_csv(\"../input/sample_submission.csv\")\nsubmission['surface'] = pd.DataFrame(np.argmax(test_predictions, axis = 1))[0].apply(lambda x : label_names[x]).values.reshape(-1)\nsubmission.to_csv(\"submission_birectional.csv\", index = False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}