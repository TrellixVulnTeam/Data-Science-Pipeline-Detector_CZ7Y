{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nfrom seaborn import countplot,lineplot, barplot\nfrom scipy import stats\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdae1e5cd1e495efa55a10fdb67d05972a56c798","_kg_hide-input":false},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\nfrom numpy import dstack\nfrom pandas import read_csv\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.merge import concatenate\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import AveragePooling1D\nfrom keras.layers import GlobalAveragePooling1D\nfrom keras.models import load_model\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, GroupKFold\n\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.datasets import make_classification\nfrom keras.utils import np_utils\nfrom keras.callbacks import Callback, EarlyStopping\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train = pd.read_csv('../input/career-con-2019/X_train.csv')\ntarget = pd.read_csv('../input/career-con-2019/y_train.csv')\ntest = pd.read_csv('../input/career-con-2019/X_test.csv')\nsub = pd.read_csv('../input/career-con-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17cb8b0faa6203603662fdb6d18881e775ddbd74"},"cell_type":"code","source":"train['series_id'].nunique(), test['series_id'].nunique() #3810 series's of train, 3816 series's of test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"16e6ce1932a3c66c3134c0deb159437307013768"},"cell_type":"code","source":"target['group_id'].nunique() #73 group_id ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"794f5130123eb4f1d7b33b0f39cfcf382fcd5e7e"},"cell_type":"code","source":"target['surface'].value_counts().reset_index().rename(columns={'index': 'target'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4575efa6ec84573dcd45e5b0afc11f0fae15ff9b","scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\ncountplot(y= 'surface', data= target, order= target['surface'].value_counts().index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plt.figure(figsize=(23,5)) \ncountplot(x=\"group_id\", data=target, order = target['group_id'].value_counts().index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# # https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\n# def quaternion_to_euler(x, y, z, w):\n#     import math\n#     t0 = +2.0 * (w * x + y * z)\n#     t1 = +1.0 - 2.0 * (x * x + y * y)\n#     X = math.atan2(t0, t1)\n\n#     t2 = +2.0 * (w * y - z * x)\n#     t2 = +1.0 if t2 > +1.0 else t2\n#     t2 = -1.0 if t2 < -1.0 else t2\n#     Y = math.asin(t2)\n\n#     t3 = +2.0 * (w * z + x * y)\n#     t4 = +1.0 - 2.0 * (y * y + z * z)\n#     Z = math.atan2(t3, t4)\n\n#     return X, Y, Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# def fe_step0 (data):\n    \n#     actual = data.copy()\n#     # https://www.mathworks.com/help/aeroblks/quaternionnorm.html\n#     # https://www.mathworks.com/help/aeroblks/quaternionmodulus.html\n#     # https://www.mathworks.com/help/aeroblks/quaternionnormalize.html\n        \n#     actual['norm_quat'] = (actual['orientation_X']**2 + actual['orientation_Y']**2 + actual['orientation_Z']**2 + actual['orientation_W']**2)\n#     actual['mod_quat'] = (actual['norm_quat'])**0.5\n#     actual['norm_X'] = actual['orientation_X'] / actual['mod_quat']\n#     actual['norm_Y'] = actual['orientation_Y'] / actual['mod_quat']\n#     actual['norm_Z'] = actual['orientation_Z'] / actual['mod_quat']\n#     actual['norm_W'] = actual['orientation_W'] / actual['mod_quat']\n    \n#     return actual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# def fe_step1 (actual):\n#     \"\"\"Quaternions to Euler Angles\"\"\"\n    \n#     x, y, z, w = actual['norm_X'].tolist(), actual['norm_Y'].tolist(), actual['norm_Z'].tolist(), actual['norm_W'].tolist()\n#     nx, ny, nz = [], [], []\n#     for i in range(len(x)):\n#         xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n#         nx.append(xx)\n#         ny.append(yy)\n#         nz.append(zz)\n    \n#     actual['euler_x'] = nx\n#     actual['euler_y'] = ny\n#     actual['euler_z'] = nz\n#     return actual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"# train_df = fe_step0(train)\n# test_df = fe_step0(test)\n# train_df = fe_step1(train_df)\n# test_df = fe_step1(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# from numpy.fft import rfft, rfftfreq, irfft\n\n# # from @theoviel at https://www.kaggle.com/theoviel/fast-fourier-transform-denoising\n# def filter_signal(signal, threshold=1e3):\n#     fourier = rfft(signal)\n#     frequencies = rfftfreq(signal.size, d=20e-3/signal.size)\n#     fourier[frequencies > threshold] = 0\n#     return irfft(fourier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# def denoised_df(X_train):\n#     # denoise train and test angular_velocity and linear_acceleration data\n#     X_train_denoised = X_train.copy()\n\n#     # train\n#     for col in X_train_denoised.columns:\n#         if col[0:3] == 'ang' or col[0:3] == 'lin':\n#             # Apply filter_signal function to the data in each series\n#             denoised_data = X_train_denoised.groupby(['series_id'])[col].apply(lambda x: filter_signal(x))\n\n#             # Assign the denoised data back to X_train\n#             list_denoised_data = []\n#             for arr in denoised_data:\n#                 for val in arr:\n#                     list_denoised_data.append(val)\n\n#             X_train_denoised[col] = list_denoised_data\n#     return X_train_denoised","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train.copy()\ntest_df = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# plt.figure(figsize=(24, 8))\n# plt.title('linear_acceleration_X')\n# plt.plot(train.angular_velocity_Z[128:256], label=\"original\");\n# plt.plot(train_df.angular_velocity_Z[128:256], label=\"denoised\");\n# plt.legend()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def feat_diff(data):\n    for col in data.columns[3:]:\n        data[col+'_diff'] = data.groupby(['series_id'])[col].diff().fillna(0)\n    return data\n\ndef feat_make(data):\n    data = feat_diff(data)\n    \n    X0  = data.loc[:, 'orientation_X':'orientation_W'].values.reshape(-1, 128, 4)\n    X1  = data.loc[:, 'angular_velocity_X': 'angular_velocity_Z'].values.reshape(-1, 128, 3)\n    X2  = data.loc[:, 'linear_acceleration_X': 'linear_acceleration_Z'].values.reshape(-1, 128, 3)     \n#     X3  = data.loc[:, 'norm_quat':'mod_quat'].values.reshape(-1, 128, 2)\n#     X4  = data.loc[:, 'norm_X':'norm_W'].values.reshape(-1, 128, 4)\n#     X5  = data.loc[:, 'euler_x': 'euler_z'].values.reshape(-1, 128, 3) \n#     X6  = data.loc[:, 'totl_anglr_vel':'acc_vs_vel'].values.reshape(-1, 128, 4)\n    \n    X7  = data.loc[:, 'orientation_X_diff':'orientation_W_diff'].values.reshape(-1, 128, 4)\n    X8  = data.loc[:, 'angular_velocity_X_diff': 'angular_velocity_Z_diff'].values.reshape(-1, 128, 3)\n    X9  = data.loc[:, 'linear_acceleration_X_diff': 'linear_acceleration_Z_diff'].values.reshape(-1, 128, 3)\n#     X10 = data.loc[:, 'norm_quat_diff':'mod_quat_diff'].values.reshape(-1, 128, 2)\n#     X11 = data.loc[:, 'norm_X_diff':'norm_W_diff'].values.reshape(-1, 128, 4)\n#     X12 = data.loc[:, 'euler_x_diff': 'euler_z_diff'].values.reshape(-1, 128, 3)\n#     X13 = data.loc[:, 'totl_anglr_vel_diff':'acc_vs_vel_diff'].values.reshape(-1, 128, 4)\n    \n    return  X0, X1, X2, X7, X8, X9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def mfft(x):\n    return [ x/math.sqrt(128.0) for x in np.absolute(np.fft.fft(x)) ][1:65]\n\ndef feat_fft(X_trn):\n    ars=[]\n    for ar in X_trn:\n        ar= ar.T.tolist()\n        ms= [] \n        for line in ar:\n            m= mfft(line)\n            ms.append(m)\n        ms= np.array(ms).T.tolist()\n        ars.append(ms)\n    ars= np.array(ars)\n    return ars","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# import math\n# def prepare_data(t):\n#     def f(d):\n#         d=d.sort_values(by=['measurement_number'])\n#         return pd.DataFrame({\n#          'lx':[ d['linear_acceleration_X'].values ],\n#          'ly':[ d['linear_acceleration_Y'].values ],\n#          'lz':[ d['linear_acceleration_Z'].values ],\n#          'ax':[ d['angular_velocity_X'].values ],\n#          'ay':[ d['angular_velocity_Y'].values ],\n#          'az':[ d['angular_velocity_Z'].values ],\n#         })\n\n#     t= t.groupby('series_id').apply(f)\n\n#     def mfft(x):\n#         return [ x/math.sqrt(128.0) for x in np.absolute(np.fft.fft(x)) ][1:65]\n\n#     t['lx_f']=[ mfft(x) for x in t['lx'].values ]\n#     t['ly_f']=[ mfft(x) for x in t['ly'].values ]\n#     t['lz_f']=[ mfft(x) for x in t['lz'].values ]\n#     t['ax_f']=[ mfft(x) for x in t['ax'].values ]\n#     t['ay_f']=[ mfft(x) for x in t['ay'].values ]\n#     t['az_f']=[ mfft(x) for x in t['az'].values ]\n#     return t\n\n# t=prepare_data(train_df)\n\n# t=pd.merge(t,target[['series_id','surface','group_id']],on='series_id')\n# t=t.rename(columns={\"surface\": \"y\"})\n\n\n# def aggf(d, feature):\n#     va= np.array(d[feature].tolist())\n#     mean= sum(va)/va.shape[0]\n#     var= sum([ (va[i,:]-mean)**2 for i in range(va.shape[0]) ])/va.shape[0]\n#     dev= [ math.sqrt(x) for x in var ]\n#     return pd.DataFrame({\n#         'mean': [ mean ],\n#         'dev' : [ dev ],\n#     })\n\n# display={\n# 'hard_tiles_large_space':'r-.',\n# 'concrete':'g-.',\n# 'tiled':'b-.',\n\n# 'fine_concrete':'r-',\n# 'wood':'g-',\n# 'carpet':'b-',\n# 'soft_pvc':'y-',\n\n# 'hard_tiles':'r--',\n# 'soft_tiles':'g--',\n# }\n\n# import matplotlib.pyplot as plt\n# plt.figure(figsize=(14, 8*7))\n# #plt.margins(x=0.0, y=0.0)\n# #plt.tight_layout()\n# # plt.figure()\n\n# features=['lx_f','ly_f','lz_f','ax_f','ay_f','az_f']\n# count=0\n\n# for feature in features:\n#     stat= t.groupby('y').apply(aggf,feature)\n#     stat.index= stat.index.droplevel(-1)\n#     b=[*range(len(stat.at['carpet','mean']))]\n\n#     count+=1\n#     plt.subplot(len(features)+1,1,count)\n#     for i,(k,v) in enumerate(display.items()):\n#         plt.plot(b, stat.at[k,'mean'], v, label=k)\n#         # plt.errorbar(b, stat.at[k,'mean'], yerr=stat.at[k,'dev'], fmt=v)\n   \n#     leg = plt.legend(loc='best', ncol=3, mode=\"expand\", shadow=True, fancybox=True)\n#     plt.title(\"sensor: \" + feature)\n#     plt.xlabel(\"frequency component\")\n#     plt.ylabel(\"amplitude\")\n\n# count+=1\n# plt.subplot(len(features)+1,1,count)\n# k='concrete'\n# v=display[k]\n# feature='lz_f'\n# stat= t.groupby('y').apply(aggf,feature)\n# stat.index= stat.index.droplevel(-1)\n# b=[*range(len(stat.at['carpet','mean']))]\n\n# plt.errorbar(b, stat.at[k,'mean'], yerr=stat.at[k,'dev'], fmt=v)\n# plt.title(\"sample for error bars (lz_f, surface concrete)\")\n# plt.xlabel(\"frequency component\")\n# plt.ylabel(\"amplitude\")\n\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# def feat_reshape(t):  \n#     m0= []\n#     for rows in t:\n#         m= []\n#         for cols in rows: \n#             m.append(cols)\n#         m= np.array(m).T.tolist()\n\n#         m0.append(m)\n\n#     m0 = np.array(m0)\n#     return m0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nle.fit(target['surface'])\ny_train= le.transform(target['surface'])\nle.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.transform(['carpet', 'concrete', 'fine_concrete', 'hard_tiles',\n       'hard_tiles_large_space', 'soft_pvc', 'soft_tiles', 'tiled',\n       'wood'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.inverse_transform([0, 1, 2, 3, 4, 5, 6, 7, 8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"990a4391c9b44ae699eb67f8368d606ec497b175","_kg_hide-input":false},"cell_type":"code","source":"X_trn_q, X_trn_a, X_trn_l, X_trn_q_d, X_trn_a_d, X_trn_l_d = feat_make(train_df)\n\nX_tst_q, X_tst_a, X_tst_l, X_tst_q_d, X_tst_a_d, X_tst_l_d = feat_make(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"X_trn_q_f = feat_fft(X_trn_q)\nX_trn_a_f = feat_fft(X_trn_a)\nX_trn_l_f = feat_fft(X_trn_l)\nX_trn_q_d_f = feat_fft(X_trn_q_d)\nX_trn_a_d_f = feat_fft(X_trn_a_d)\nX_trn_l_d_f = feat_fft(X_trn_l_d)\n\nX_tst_q_f = feat_fft(X_tst_q)\nX_tst_a_f = feat_fft(X_tst_a)\nX_tst_l_f = feat_fft(X_tst_l)\nX_tst_q_d_f = feat_fft(X_tst_q_d)\nX_tst_a_d_f = feat_fft(X_tst_a_d)\nX_tst_l_d_f = feat_fft(X_tst_l_d)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef run_set(X_trn):\n    for i in run_id:\n        X_run = target[:][target['run_id']==i]\n        idx= X_run.index\n        X= X_trn[idx].tolist()\n        X=[item for sublist in X for item in sublist]\n        end=int(len(X)/2048)*2048     \n        X=X[:end]\n        X=np.array(X).reshape(-1, 2048, X_trn.shape[-1])\n    return X\n\ndef test_run_set(X_trn):\n    X_=[]\n    for i in test_run_id:\n        X_run = test_target[:][test_target['run_id']==i]\n        idx= X_run.index\n        X= X_trn[idx].tolist()\n        X=[item for sublist in X for item in sublist]\n        end=int(len(X)/2048)*2048     \n        X=X[:end]\n        X=np.array(X).reshape(-1, 2048, X_trn.shape[-1])\n    return X","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# from sklearn.neighbors import KernelDensity\n# from sklearn.decomposition import PCA\n# from sklearn.model_selection import GridSearchCV\n\n# def X_gen_kde(X_trn, surface_type, numbers):\n#     st= target[:][(target['surface'] == surface_type)]\n#     X=[]\n#     i=0\n#     for idx in st.index:\n#         X_t= X_trn[idx].tolist()\n#         X.append(X_t)\n#         i+=1\n# #     print(i)\n#     X= np.array(X).reshape(-1,(X_trn.shape[1]*X_trn.shape[2]))\n#     #reshape \n\n#     # project the multi-dimensional data to a lower dimension\n# #     pca = PCA(n_components=21, whiten=False)\n# #     data = pca.fit_transform(X)\n\n#     # use grid search cross-validation to optimize the bandwidth\n#     params = {'bandwidth': np.logspace(-1, 1, 20)}\n#     grid = GridSearchCV(KernelDensity(), params, cv=5)\n# #     grid.fit(data)\n#     grid.fit(X)\n\n#     print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n\n#     # use the best estimator to compute the kernel density estimate\n#     kde = grid.best_estimator_\n\n#     # sample numbers new points from the data\n#     new_data = kde.sample(numbers, random_state=0)\n# #     new_data = pca.inverse_transform(new_data)\n\n#     # turn data back to multi-dimensional\n#     new_data = new_data.reshape((-1, X_trn.shape[1], X_trn.shape[2]))\n#     return new_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# def X_gen(X_trn, surface_type, numbers):\n#     st= target[:][(target['surface'] == surface_type)]\n\n#     X_sum= np.zeros((X_trn.shape[1], X_trn.shape[2]))\n#     i=0\n#     for idx in st.index:\n#         X_sum += X_trn[idx]\n#         i+=1\n# #     print(i)\n#     X_mean = X_sum/ i\n#     #mean\n    \n#     sq_sum= np.zeros_like(X_mean)\n#     j=0\n#     for idx in st.index:    \n#         dev= np.abs(X_trn[idx] - X_mean)\n#         sq_sum += dev**2\n#         j+=1\n# #     print(j)\n#     X_std = np.sqrt(sq_sum/ j)\n#     #std\n    \n#     X_trn_gen = np.random.normal(X_mean, X_std, (numbers, X_trn.shape[1], X_trn.shape[2]))\n#     #gen\n#     return X_trn_gen","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# ingred= {'carpet':300, 'concrete':0, 'fine_concrete':200, 'hard_tiles':500,\n#          'hard_tiles_large_space':200, 'soft_pvc':0, 'soft_tiles':200, 'tiled':0,\n#          'wood':0}\n\n# def X_aug(X_trn):    \n#     i=0\n#     for surface_type, numbers in ingred.items():\n#         X_aug= X_gen(X_trn, surface_type, numbers)\n# #         X_aug= X_gen_kde(X_trn, surface_type, numbers)\n#         if i==0:\n#             X_aug_total= X_aug\n#         else:\n#             X_aug_total= np.append(X_aug_total, X_aug, axis=0)\n#         i+=1\n        \n#     X_aug_total= np.append(X_trn, X_aug_total, axis=0)\n#     return X_aug_total\n\n# def y_aug(y_trn):\n#     i=0\n#     for surface_type, numbers in ingred.items():\n#         y_aug= np.full( (numbers,), le.transform([surface_type]) )\n#         if i==0:\n#             y_aug_total= y_aug\n#         else:\n#             y_aug_total= np.append(y_aug_total, y_aug, axis=0)\n#         i+=1    \n#     y_aug_total= np.append(y_trn, y_aug_total, axis=0)\n#     return y_aug_total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trn_ = [X_trn_q, X_trn_a, X_trn_l, X_trn_q_d, X_trn_a_d, X_trn_l_d,\n        X_trn_a_f, X_trn_l_f, X_trn_a_d_f, X_trn_l_d_f]\n\n# X_train = [X_aug(item) for item in X_trn_]\n# X_train = X_trn_","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"X_train = [run_set(x) for x in X_trn_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# X_trn_2= [X_trn_a_f, X_trn_l_f]\n\n# X_train_2=[X_aug(item) for item in X_trn_2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train = y_aug(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test= [X_tst_q, X_tst_a, X_tst_l, X_tst_q_d, X_tst_a_d, X_tst_l_d,\n         X_tst_a_f, X_tst_l_f, X_tst_a_d_f, X_tst_l_d_f]\n\n# X_test_2= [X_tst_a_f, X_tst_l_f]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = [test_run_set(x) for x in X_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5db168a94adeb3ac5ca4fc9082da09122333b697","_kg_hide-input":false},"cell_type":"code","source":"class Head:\n    def build(n_steps, n_features):\n        inputA = Input(shape=(n_steps, n_features))\n        x = Conv1D(filters=100, kernel_size=10, activation='relu')(inputA)\n        x = Conv1D(filters=100, kernel_size=10, activation='relu')(x)\n        x = MaxPooling1D(pool_size=3)                             (x)\n        x = Conv1D(filters=160, kernel_size=10, activation='relu')(x)\n        x = Conv1D(filters=160, kernel_size=10, activation='relu')(x)\n        x = AveragePooling1D(pool_size=3)                         (x)\n        x = Dropout(0.5)                                          (x)\n        x = Flatten()                                             (x)\n        x = Model(inputs= inputA, outputs= x)\n        return x\n    \n    def build_f(n_steps, n_features):\n        inputA = Input(shape=(n_steps, n_features))\n        x = Conv1D(filters=100, kernel_size=5, activation='relu')(inputA)\n        x = Conv1D(filters=100, kernel_size=5, activation='relu')(x)\n        x = MaxPooling1D(pool_size=3)                            (x)\n        x = Conv1D(filters=160, kernel_size=5, activation='relu')(x)\n        x = Conv1D(filters=160, kernel_size=5, activation='relu')(x)\n        x = AveragePooling1D(pool_size=3)                        (x)\n        x = Dropout(0.5)                                         (x)\n        x = Flatten()                                            (x)\n        x = Model(inputs= inputA, outputs= x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8daa7b941978cd6af06e0c64137cdbb130e6ae8b","_kg_hide-input":false},"cell_type":"code","source":"def sub_model(X_trn, y_trn):     \n# CNN heads\n    #orignal\n    flatA1 = Head.build(2048, 4)   \n    flatA2 = Head.build(2048, 3)\n    flatA3 = Head.build(2048, 3)\n#     flatA4 = Head.build(128, 3)\n    #diff\n    flatB1 = Head.build(2048, 4)   \n    flatB2 = Head.build(2048, 3)\n    flatB3 = Head.build(2048, 3)\n#     flatB4 = Head.build(128, 3)\n    #orignal fft\n#     fftA1 = Head.build_f(64, 4)\n    fftA2 = Head.build_f(2048, 3)\n    fftA3 = Head.build_f(2048, 3)\n#     fftA4 = Head.build_f(64, 3)\n    #diff fft\n#     fftB1 = Head.build_f(64, 4)\n    fftB2 = Head.build_f(2048, 3)\n    fftB3 = Head.build_f(2048, 3)\n#     fftB4 = Head.build_f(64, 3)\n    \n# merge CNN heads\n    \n    x= concatenate([\n        flatA1.output, flatA2.output, flatA3.output, #flatA4.output,\n        flatB1.output, flatB2.output, flatB3.output, #flatB4.output,\n#         fftA1.output,\n        fftA2.output, fftA3.output, \n#         fftA4.output,\n#         fftB1.output, \n        fftB2.output, fftB3.output, \n#         fftB4.output\n    ])\n       \n# interpretation\n     \n    x = Dense(500, activation='relu') (x)\n    x = Dense(100, activation='relu') (x)\n    x = Dense(n_surfaces, activation='softmax')(x)\n    \n    model = Model(inputs=[\n                          flatA1.input, flatA2.input, flatA3.input, #flatA4.input,\n                          flatB1.input, flatB2.input, flatB3.input, #flatB4.input, \n#                           fftA1.input, \n                          fftA2.input, fftA3.input, #fftA4.input,\n#                           fftB1.input, \n                          fftB2.input, fftB3.input, #fftB4.input\n                         ],\n                  outputs= x)\n#--------\n# save a plot of the model\n    plot_model(model, show_shapes=True, to_file='multichannel.png')\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    model.fit(x= X_trn, y= y_trn, epochs= epochs, batch_size= batch_size, verbose= verbose)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load models from file\ndef load_all_models(n_models):\n    all_models = []\n    for i in range(n_models):\n        # define filename for this ensemble\n        filename = 'model_' + str(i) + '.h5'\n        # load model from file\n        model = load_model(filename)\n        # add to list of members\n        all_models.append(model)\n        print('>loaded %s' % filename)\n    return all_models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define stacked model from multiple member input models\ndef define_stacked_model(members):\n    # update all layers in all models to not be trainable\n    for i in range(len(members)):\n        model = members[i]\n        for layer in model.layers:\n            # make not trainable\n            layer.trainable = False\n            # rename to avoid 'unique layer name' issue\n            layer.name = 'ensemble_' + str(i) + '_' + layer.name\n    # define multi-headed input\n    ensemble_visible = [model.input for model in members] #nested list\n    ensemble_visible = [item for sublist in ensemble_visible for item in sublist] ## unnest the nested list\n    # concatenate merge output from each model\n    ensemble_outputs = [model.output for model in members]\n    merge = concatenate(ensemble_outputs)\n    hidden = Dense(45, activation='relu')(merge)\n    output = Dense(9, activation='softmax')(hidden)\n    model = Model(inputs=ensemble_visible, outputs=output)\n    # plot graph of ensemble\n    plot_model(model, show_shapes=True, to_file='model_graph.png')\n    # compile\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n#     print(cm)\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    plt.grid(False)\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93ca071163cdbe84d7d2a5eaabfb636969157648","_kg_hide-output":false},"cell_type":"code","source":"verbose, epochs, batch_size = 2, 15, 32\nn_surfaces = 9\nn_models= 2\n\n\nfolds = StratifiedShuffleSplit(n_splits= 2, test_size= 0.5, random_state=0)\n# folds = StratifiedKFold(n_splits=5, shuffle=False, random_state=0)\n# folds = GroupKFold(n_splits=5)\n\nnp.set_printoptions(precision=2)\nclass_names= le.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bf7e45d12da327e1805a8f4139b2ef09d31746a","_kg_hide-input":false},"cell_type":"code","source":"# def run_cv(X_trn, y_trn):\n    \nX_trn= X_train ##\ny_trn= y_train ##\n\nfor fold_, (trn, val) in enumerate(folds.split(np.zeros(len(y_trn)), y_trn)):\n    print(\"fold {}\".format(fold_))\n    \n    y_oh = to_categorical(y_trn)\n    xtrn= [x[trn] for x in X_trn]\n    \n    for i in range(n_models): #devide to n models for the specific surface\n        \n#         gp_idx = np.where(y_trn==i)[0].tolist()\n#         co_idx= [idx for idx in gp_idx if idx in trn]\n#         xtrn= [x[co_idx] for x in X_trn]\n#         model = sub_model(xtrn, y_oh[co_idx])\n\n        model = sub_model(xtrn, y_oh[trn])\n        filename = 'model_' + str(i) + '.h5'\n        model.save(filename)\n        print('>Saved %s' % filename)\n\n    # load all models\n    members = load_all_models(n_models)\n    print('Loaded %d models' % len(members))\n\n    # define ensemble model\n    stacked_model = define_stacked_model(members)\n\n    # fit stacked model on test dataset\n    X_t = [x[trn] for x in X_trn] \n    X_t = [X_t for _ in range(n_models)] # duplicate X_v for n level_0-models\n    X_t = [item for sublist in X_t for item in sublist] #un-nest the list\n    \n    X_v = [x[val] for x in X_trn]\n    X_v = [X_v for _ in range(n_models)] # duplicate X_v for n level_0-models\n    X_v = [item for sublist in X_v for item in sublist] #un-nest the list\n    \n    stacked_model.fit(X_t, y_oh[trn], validation_data= (X_v, y_oh[val]), epochs=epochs, verbose=verbose)\n    \n    # summarize history for accuracy\n    plt.plot(stacked_model.history.history['acc'])\n    plt.plot(stacked_model.history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(stacked_model.history.history['loss'])\n    plt.plot(stacked_model.history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n       \n    # make predictions and evaluate \n    y_pred_ = stacked_model.predict(X_v, verbose=verbose)\n    y_pred = y_pred_.argmax(axis=-1)\n    acc = accuracy_score(y_trn[val], y_pred)\n    print('Stacked Test Accuracy: %.3f' % acc)\n\n\n    # Plot non-normalized confusion matrix\n    plot_confusion_matrix(y_trn[val], y_pred, classes= class_names, \n                          title='Confusion matrix, without normalization')\n\n    # Plot normalized confusion matrix\n    plot_confusion_matrix(y_trn[val], y_pred, classes= class_names, \n                                normalize=True, title='Normalized confusion matrix')\n     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91369a16a843271fe85fc98544b41d386aed904e","scrolled":false},"cell_type":"code","source":"#predict\nn_folds = 2\nfor f in range(n_folds):\n    print('fold',f) \n    y_oh = to_categorical(y_train)\n    for i in range(n_models): #devide to n models for the specific surface\n        model = sub_model(X_train, y_oh)\n        filename = 'model_' + str(i) + '.h5'\n        model.save(filename)\n        print('>Saved %s' % filename)\n\n    # load all models\n    members = load_all_models(n_models)\n    print('Loaded %d models' % len(members))\n\n    # define ensemble model\n    stacked_model = define_stacked_model(members)\n    \n    # fit stacked model on test dataset\n    X_t = X_train\n    X_t = [X_t for _ in range(n_models)] # duplicate X_v for n level_0-models\n    X_t = [item for sublist in X_t for item in sublist] #un-nest the list\n    stacked_model.fit(X_t, y_oh, epochs=5, verbose=verbose)\n  \n    X_tst = X_test \n    X_tst = [X_tst for _ in range(n_models)] # duplicate X_tst for n level_0-models\n    X_tst = [item for sublist in X_tst for item in sublist] #un-nest the list\n    y_test_ = stacked_model.predict(X_tst, verbose=verbose)\n    \n    # list all data in history\n    #print(model.history.history.keys())\n    # summarize history for accuracy\n    plt.plot(model.history.history['acc'])\n#     plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n#     plt.legend(['test'], loc='upper left')\n#     plt.show()\n    # summarize history for loss\n    plt.plot(model.history.history['loss'])\n#     plt.title('model loss')\n#     plt.ylabel('loss')\n#     plt.xlabel('epoch')\n\n#     plt.legend(['test'], loc='upper left')\n#     plt.show()\n        \n    if f==0:\n        y_test = np.zeros_like(y_test_)\n    y_test += y_test_\n    \ny_test = y_test/ n_folds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#output\ny_test = y_test.argmax(axis=-1) #one-hot prob to classes code","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_run=[]\nfor i in run_id:\n    X_run = target[:][target['run_id']==i]\n    y = le.transform(X_run['surface'])[0]\n    y_run.append(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for run_id, i in enumerate (y_test):\n    if test_target['run_id']==run_id:\n        test_target['surface']= le.inverse_transform(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2490b081fb6b1c660b628458ce58a68e754745ad"},"cell_type":"code","source":"\nsubm = pd.DataFrame()\nsubm['series_id'] = test_target['series_id']\nsubm['surface']= test_target['surface']\nsubm = subm.sort_values('series_id')\n\nsubm.to_csv(\"submission.csv\", index= False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2834db7f4fa2d64917266e1c1e8b87c4e28afcc2"},"cell_type":"code","source":"subm","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}