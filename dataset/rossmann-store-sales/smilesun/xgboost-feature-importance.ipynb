{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#!/usr/bin/python\n\n'''\nBased on https://www.kaggle.com/justdoit/rossmann-store-sales/xgboost-in-python-with-rmspe/code\nPublic Score :  0.11389\nPrivate Validation Score :  0.096959\n'''\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nimport xgboost as xgb\nimport operator\nimport matplotlib\nmatplotlib.use(\"Agg\") #Needed to save figures\nimport matplotlib.pyplot as plt\n\ndef create_feature_map(features):\n    outfile = open('xgb.fmap', 'w')\n    for i, feat in enumerate(features):\n        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n    outfile.close()\n\ndef rmspe(y, yhat):\n    return np.sqrt(np.mean((yhat/y-1) ** 2))\n\ndef rmspe_xg(yhat, y):\n    y = np.expm1(y.get_label())\n    yhat = np.expm1(yhat)\n    return \"rmspe\", rmspe(y,yhat)\n\n# Gather some features\ndef build_features(features, data):\n    # remove NaNs\n    data.fillna(0, inplace=True)\n    data.loc[data.Open.isnull(), 'Open'] = 1\n    # Use some properties directly\n    features.extend(['Store', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n                     'CompetitionOpenSinceYear', 'Promo', 'Promo2', 'Promo2SinceWeek',\n                     'Promo2SinceYear'])\n\n    # add some more with a bit of preprocessing\n    features.append('SchoolHoliday')\n    data['SchoolHoliday'] = data['SchoolHoliday'].astype(float)\n\n    features.extend(['StoreType', 'Assortment', 'StateHoliday'])\n    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n    data.StoreType.replace(mappings, inplace=True)\n    data.Assortment.replace(mappings, inplace=True)\n    data.StateHoliday.replace(mappings, inplace=True)\n\n    features.extend(['DayOfWeek', 'month', 'day', 'year'])\n    data['year'] = data.Date.dt.year\n    data['month'] = data.Date.dt.month\n    data['day'] = data.Date.dt.day\n    data['DayOfWeek'] = data.Date.dt.dayofweek\n\n\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"## Start of main script\n\nprint(\"Load the training, test and store data using pandas\")\ntypes = {'CompetitionOpenSinceYear': np.dtype(int),\n         'CompetitionOpenSinceMonth': np.dtype(int),\n         'StateHoliday': np.dtype(str),\n         'Promo2SinceWeek': np.dtype(int),\n         'SchoolHoliday': np.dtype(float),\n         'PromoInterval': np.dtype(str)}\ntrain = pd.read_csv(\"../input/train.csv\", parse_dates=[2], dtype=types)\ntest = pd.read_csv(\"../input/test.csv\", parse_dates=[3], dtype=types)\nstore = pd.read_csv(\"../input/store.csv\")\n\nprint(\"Assume store open, if not provided\")\ntrain.fillna(1, inplace=True)\ntest.fillna(1, inplace=True)\n\nprint(\"Consider only open stores for training. Closed stores wont count into the score.\")\ntrain = train[train[\"Open\"] != 0]\nprint(\"Use only Sales bigger then zero. Simplifies calculation of rmspe\")\ntrain = train[train[\"Sales\"] > 0]\n\nprint(\"Join with store\")\ntrain = pd.merge(train, store, on='Store')\ntest = pd.merge(test, store, on='Store')\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"features = []\n\nprint(\"augment features\")\nbuild_features(features, train)\nbuild_features([], test)\nprint(features)\n\nprint('training data processed')\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"params = {\"objective\": \"reg:linear\",\n          \"booster\" : \"gbtree\",\n          \"eta\": 0.09,\n          \"max_depth\": 10,\n          \"subsample\": 0.9,\n          \"colsample_bytree\": 0.7,\n          \"silent\": 1,\n          \"seed\": 1301\n          }\nnum_boost_round = 800\n\nprint(\"Train a XGBoost model\")\nX_train, X_valid = train_test_split(train, test_size=0.012, random_state=10)\ny_train = np.log1p(X_train.Sales)\ny_valid = np.log1p(X_valid.Sales)\ndtrain = xgb.DMatrix(X_train[features], y_train)\ndvalid = xgb.DMatrix(X_valid[features], y_valid)\n\nwatchlist = [(dtrain, 'train'), (dvalid, 'eval')]\ngbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)\n\nprint(\"Validating\")\nyhat = gbm.predict(xgb.DMatrix(X_valid[features]))\nerror = rmspe(X_valid.Sales.values, np.expm1(yhat))\nprint('RMSPE: {:.6f}'.format(error))\n\nprint(\"Make predictions on the test set\")\ndtest = xgb.DMatrix(test[features])\ntest_probs = gbm.predict(dtest)\n# Make Submission\nresult = pd.DataFrame({\"Id\": test[\"Id\"], 'Sales': np.expm1(test_probs)})\nresult.to_csv(\"xgboost_10_submission.csv\", index=False)\n\n# XGB feature importances\n# Based on https://www.kaggle.com/mmueller/liberty-mutual-group-property-inspection-prediction/xgb-feature-importance-python/code\n\ncreate_feature_map(features)\nimportance = gbm.get_fscore(fmap='xgb.fmap')\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\n\ndf = pd.DataFrame(importance, columns=['feature', 'fscore'])\ndf['fscore'] = df['fscore'] / df['fscore'].sum()\n\nfeatp = df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\nplt.title('XGBoost Feature Importance')\nplt.xlabel('relative importance')\nfig_featp = featp.get_figure()\nfig_featp.savefig('feature_importance_xgb.png', bbox_inches='tight', pad_inches=1)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}