{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fast ai v3 on Rossemann\n\nCombining two notebook in fastai lesson6, rossmann data clean and rossmann notebook to create an end to end submission.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.basics import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting the data together\n\nThe extra data from rossmann one has to add to ones notebook.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = Config().data_path()/Path('rossmann/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!mkdir {PATH}\n#mkdir: cannot create directory ‘/root/.fastai/data/rossmann’: No such file or directory\n!mkdir /root/.fastai/data/\n!mkdir /root/.fastai/data/rossmann","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp /kaggle/input/rossmann/*.csv {PATH}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"table_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tables = [pd.read_csv(PATH/f'{fname}.csv', low_memory=False) for fname in table_names]\ntrain, store, store_states, state_names, googletrend, weather, test = tables\ngoogletrend.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_states.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train), len(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. turning holidays into booleans to make the more convenient for modelling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.StateHoliday = train.StateHoliday != '0'\ntest.StateHoliday = test.StateHoliday != '0'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. creating a custom join_df function for joining tables on specific fields","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def join_df(left, right, left_on, right_on=None, suffix='_y'):\n    if(right_on is None):\n        right_on = left_on\n    return left.merge(right, how='left', left_on=left_on, right_on=right_on, suffixes=(\"\", suffix))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. joining weather and state_names","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"weather = join_df(weather, state_names, \"file\", \"StateName\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Adding new columns to googletrends , and replace all instance of NI to HB,NI as it is usedelsewhere in all datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]\ngoogletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\n\ngoogletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Getting particular date fields from a complete datetime. We should always consider this stepwhen working with date time. this we will add to every table with a date field","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_datepart(df, fldname, drop=True, time=False):\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: \n        attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr:\n        df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n    if drop :\n        df.drop(fldname, axis=1, inplace=True)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_datepart(weather, \"Date\", drop=False)\nadd_datepart(googletrend, \"Date\", drop=False)\nadd_datepart(train, \"Date\", drop=False)\nadd_datepart(test, \"Date\", drop=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train), len(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. The Google trends data has a special category for the whole of germany - we'll pull that out to use explicitly","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_germany = googletrend[googletrend.file == \"Rossmann_DE\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. Now we will perform a outer join all datasets and then do a null check to see if the all records are consistent. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store = join_df(store, store_states, \"Store\")\nlen(store[store.State.isnull()])\n\n# is isnull is zero it means the rows are consistent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined  = join_df(train, store, \"Store\")\ncombined_test = join_df(test, store, \"Store\")\nlen(combined[combined.StoreType.isnull()]), len(combined_test[combined_test.StoreType.isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(combined), len(combined_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined  = join_df(combined, googletrend, [\"State\", \"Year\", \"Week\"])\ncombined_test = join_df(combined_test, googletrend, [\"State\", \"Year\", \"Week\"])\nlen(combined[combined.trend.isnull()]), len(combined_test[combined_test.trend.isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(combined), len(combined_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = combined.merge(trend_germany, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\ncombined_test = combined_test.merge(trend_germany, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\nlen(combined[combined.trend_DE.isnull()]),len(combined_test[combined_test.trend_DE.isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(combined), len(combined_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = join_df(combined, weather, [\"State\",\"Date\"])\ncombined_test = join_df(combined_test, weather, [\"State\",\"Date\"])\nlen(combined[combined.Mean_TemperatureC.isnull()]),len(combined_test[combined_test.Mean_TemperatureC.isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(combined), len(combined_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (combined, combined_test):\n    for c in df.columns:\n        if c.endswith('_y'):\n            if c in df.columns:\n                df.drop(c, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(combined), len(combined_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"8. We will fill in the missing values to avoid complications with NA's. Na is when missing values invade a dataframe this is how pandas indicates missing values, many models have a problem ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (combined, combined_test):\n    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"9. Next we'll extract features \"CompetitionOpenSince\" and \"CompetitionDaysOpen\". ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(combined), len(combined_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (combined,combined_test):\n    df[\"CompetitionOpenSince\"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, \n                                                     month=df.CompetitionOpenSinceMonth, day=15))\n    df[\"CompetitionDaysOpen\"] = df.Date.subtract(df.CompetitionOpenSince).dt.days","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"10. Replacing some erroneous data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(combined), len(combined_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (combined, combined_test):\n    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"11. We add \"CompetitionMonthsOpen\" field, limiting the maximum to 2 years to limit number of unique categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (combined,combined_test):\n    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]//30\n    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\ncombined.CompetitionMonthsOpen.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"12. Same we will do for promo dates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install isoweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from isoweek import Week\nfor df in (combined,combined_test):\n    df[\"Promo2Since\"] = pd.to_datetime(df.apply(lambda x: Week(\n        x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1))\n    df[\"Promo2Days\"] = df.Date.subtract(df[\"Promo2Since\"]).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (combined,combined_test):\n    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]//7\n    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n    df.Promo2Weeks.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"13. Converting to pickle for future","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(combined), len(combined_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.to_pickle(PATH/'combined')\ncombined_test.to_pickle(PATH/'combined_test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Durations\nIt is common when working with time series data to extract data that explains relationships across rows as opposed to columns, e.g.:\n\nRunning averages\nTime until next event\nTime since last event\n\nWe'll define a function `get_elapsed` for cumulative counting across a sorted dataframe. Given a particular field `fld` to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero.\n\nUpon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_elapsed(fld, pre):\n    day1 = np.timedelta64(1, 'D')\n    last_date = np.datetime64()\n    last_store = 0\n    res = []\n\n    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):\n        if s != last_store:\n            last_date = np.datetime64()\n            last_store = s\n        if v: last_date = d\n        res.append(((d-last_date).astype('timedelta64[D]') / day1))\n    df[pre+fld] = res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"14. applying to a subset of columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [\"Date\", \"Store\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"]\ndf = train[columns].append(test[columns])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An example.\n\nSay we're looking at School Holiday. We'll first sort by Store, then Date, and then call add_elapsed('SchoolHoliday', 'After'): This will apply to each row with School Holiday:\n\nA applied to every row of the dataframe in order of store and date\n\nWill add to the dataframe the days since seeing a School Holiday\n\nIf we sort in the other direction, this will count the days until another holiday.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fld = 'SchoolHoliday'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for 2 more fiellds\n\nfld = 'StateHoliday'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fld = 'Promo'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting active index to DAte\n\ndf = df.set_index(\"Date\")\n\n# setting null values from elapsed field to 0\ncolumns = ['SchoolHoliday', 'StateHoliday', 'Promo']\nfor o in ['Before', 'After']:\n    for p in columns:\n        a = o+p\n        df[a] = df[a].fillna(0).astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we'll demonstrate window functions in pandas to calculate rolling quantities.\n\nHere we're sorting by date (sort_index()) and counting the number of events of interest (sum()) defined in columns in the following week (rolling()), grouped by Store (groupby()). We do the same in the opposite direction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bwd = df[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()\nfwd = df[['Store']+columns].sort_index(ascending=False\n                                      ).groupby(\"Store\").rolling(7, min_periods=1).sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we want to drop the Store indices grouped together in the window function.\n\nOften in pandas, there is an option to do this in place. This is time and memory efficient when working with large datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bwd.drop('Store',1,inplace=True)\nbwd.reset_index(inplace=True)\nfwd.drop('Store',1,inplace=True)\nfwd.reset_index(inplace=True)\ndf.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will merge these values into the dif\ndf = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\ndf = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])\ndf.drop(columns,1,inplace=True)\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's usually a good idea to back up large tables of extracted / wrangled features before you join them onto another one, that way you can go back to it easily if you need to make changes to it.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_pickle(PATH/'df')\ndf[\"Date\"] = pd.to_datetime(df.Date)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joined = pd.read_pickle(PATH/'combined')\njoined_test = pd.read_pickle(PATH/f'combined_test')\njoined = join_df(joined, df, ['Store', 'Date'])\njoined_test = join_df(joined_test, df, ['Store', 'Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(joined), len(joined_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The original authors also removed all instances where the store had zero sale / was closed. We speculate that this may have cost them a higher standing in the competition. One reason this may be the case is that a little exploratory data analysis reveals that there are often periods where stores are closed, typically for refurbishment. Before and after these periods, there are naturally spikes in sales that one might expect. By ommitting this data from their training, the authors gave up the ability to leverage information about these periods to predict this otherwise volatile behavior.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"joined = joined[joined.Sales!=0]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(joined), len(joined_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Backing up this data as well","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"joined.reset_index(inplace=True)\njoined_test.reset_index(inplace=True)\n\njoined.to_pickle(PATH/'train_clean')\njoined_test.to_pickle(PATH/'test_clean')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(joined), len(joined_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### lets look at final training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#PATH\ntrain_df = pd.read_pickle(PATH/'train_clean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(train_df)\nprint(n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Experimenting with a sample","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.tabular import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = np.random.permutation(range(n))[:2000]\nidx.sort()\nsmall_train_df = train_df.iloc[idx[:1000]]\nsmall_test_df = train_df.iloc[idx[1000:]]\nsmall_cont_vars = ['CompetitionDistance', 'Mean_Humidity']\nsmall_cat_vars =  ['Store', 'DayOfWeek', 'PromoInterval']\nsmall_train_df = small_train_df[small_cat_vars + small_cont_vars + ['Sales']]\nsmall_test_df = small_test_df[small_cat_vars + small_cont_vars + ['Sales']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"small_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"small_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorify = Categorify(small_cat_vars, small_cont_vars)\ncategorify(small_train_df)\ncategorify(small_test_df, test=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorify does basically the same thing that .classes thing for image recognition does for a dependent variable. It's going to take these strings, it's going to find all of the possible unique values of it, and it's going to create a list of them, and then it's going to turn the strings into numbers. So if I call it on my training set, that'll create categories there (small_train_df) and then I call it on my test set passing in test=true, that makes sure it's going to use the same categories that I had before. Now when I say .head, it looks exactly the same:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"small_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's because Pandas has turned this into a categorical variable which internally is storing numbers but externally is showing me the strings. But I can look inside promo interval to look at the cat.categories, this is all standard Pandas here, to show me a list of all of what we would call \"classes\" in fast.ai or would be called just \"categories\" in Pandas.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"small_train_df.PromoInterval.cat.categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"small_train_df['PromoInterval'].cat.codes[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So then if I look at the cat.codes, you can see here this list here is the numbers that are actually stored (-1, -1, 1, -1, 1). What are these minus ones? The minus ones represent NaN - they represent \"missing\". So Pandas uses the special code -1 to be mean missing.\n\nAs you know, these are going to end up in an embedding matrix, and we can't look up item -1 in an embedding matrix. So internally in fast.ai, we add one to all of these.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_missing = FillMissing(small_cat_vars,small_cont_vars)\nfill_missing(small_train_df)\nfill_missing(small_test_df, test=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another useful preprocessor is FillMissing. Again, you can call it on the data frame, you can call on the test passing in test=true.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"small_train_df[small_train_df['CompetitionDistance_na'] == True]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This will create, for anything that has a missing value, it'll create an additional column with the column name underscore na (e.g. CompetitionDistance_na) and it will set it for true for any time that was missing. Then what we do is, we replace competition distance with the median for those. Why do we do this? Well, because very commonly the fact that something's missing is of itself interesting (i.e. it turns out the fact that this is missing helps you predict your outcome). So we certainly want to keep that information in a convenient boolean column, so that our deep learning model can use it to predict things.\n\nBut then, we need competition distance to be a continuous variable so we can use it in the continuous variable part of our model. So we can replace it with almost any number because if it turns out that the missingness is important, it can use the interaction of CompetitionDistance_na and CompetitionDistance to make predictions. So that's what FillMissing does.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Preparing full dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_pickle(PATH/'train_clean')\ntest_df = pd.read_pickle(PATH/'test_clean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(train_df), len(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The main thing you have to do if you want to create a data bunch of tabular data is tell it what are your categorical variables and what are your continuous variables. As we discussed last week briefly, your categorical variables are not just strings and things, but also I include things like day of week and month and day of month. Even though they're numbers, I make them categorical variables. Because, for example, day of month, I don't think it's going to have a nice smooth curve. I think that the fifteenth of the month and the first of the month and the 30th of the month are probably going to have different purchasing behavior to other days of the month. Therefore, if I make it a categorical variable, it's going to end up creating an embedding matrix and those different days of the month can get different behaviors.\n\nYou've actually got to think carefully about which things should be categorical variables. On the whole, if in doubt and there are not too many levels in your category (that's called the cardinality), if your cardinality is not too high, I would put it as a categorical variable. You can always try an each and see which works best.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n    'SchoolHoliday_fw', 'SchoolHoliday_bw']\n\ncont_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You don't have to manually call preprocesses yourself. When you call any kind of item list creator, you can pass in a list of pre processes which you can create like this:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"procs = [FillMissing, Categorify, Normalize]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find validation set indexes\n\nOur final data frame that we're going to pass in is going to be a training set with the categorical variables, the continuous variables, the dependent variable, and the date. The date, we're just going to use to create a validation set where we are basically going to say the validation set is going to be the same number of records at the end of the time period that the test set is for Kaggle. That way, we should be able to validate our model nicely.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dep_var = 'Sales'\ndf = train_df[cat_vars + cont_vars + [dep_var,'Date']].copy()\ntest_df['Date'].min(), test_df['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(len(train_df), len(test_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_idx = range(cut)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[dep_var].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# getting data from fastai","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (TabularList.from_df(df, path=PATH, cat_names=cat_vars, cont_names=cont_vars, procs=procs,)\n                .split_by_idx(valid_idx)\n                .label_from_df(cols=dep_var, label_cls=FloatList, log=True)\n                .add_test(TabularList.from_df(test_df, path=PATH, cat_names=cat_vars, cont_names=cont_vars))\n                .databunch())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is saying \"ok, I want to fill missing, I want to categorify, I want to normalize (i.e. for continuous variables, it'll subtract the mean and divide by the standard deviation to help a train more easily).\" So you just say, those are my procs and then you can just pass it in there and that's it.\n\nLater on, you can go data.export and it'll save all the metadata for that data bunch so you can, later on, load it in knowing exactly what your category codes are, exactly what median values used for replacing the missing values, and exactly what means and standard deviations you normalize by.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Creating model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_log_y = np.log(np.max(train_df['Sales']) * 1.2)\ny_range = torch.tensor([0, max_log_y],\n                      device = defaults.device)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = tabular_learner(data, layers= [1000,500], ps=[0.001,0.01], emb_drop=0.04, y_range=y_range, metrics =exp_rmspe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data.train_ds.cont_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-3, wd=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses(skip_start=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 3e-4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 3e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = learn.get_preds(DatasetType.Test)\ntest_df[\"Sales\"] = np.exp(test_preds[0].data).numpy().T[0]\ntest_df[[\"Id\", \"Sales\"]] = test_df[[\"Id\", \"Sales\"]].astype(\"int\")\ntest_df[[\"Id\", \"Sales\"]].to_csv(\"rossmann_submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}