{"cells":[{"metadata":{"_uuid":"bb11bf764f68f5448d43183d3afb16932a687f6a"},"cell_type":"markdown","source":"# Deep learning using the fastai library\n\n \nThis is a notebook for practicing deep learning model using the fastai library. \nThe feature engineering part is in another notebook:\n\nhttps://www.kaggle.com/zongtseng/rossmann-time-series-data-engineering\n\nIt is mainly based on the fastai course notebook but with some additional features added, such as running length encoding...etc.\n\nhttps://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson6-rossmann.ipynb\n\nhttps://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb\n"},{"metadata":{"_uuid":"e93ca51af2ad31cad481ea702a6f7dc9ae9afdd5"},"cell_type":"markdown","source":"# Env Setup"},{"metadata":{"_uuid":"0ec6ce3c1d8576aa9d307de2c38ac16a8bd1c21f","trusted":true},"cell_type":"code","source":"from fastai.tabular import *\nimport os, tarfile\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\nfrom datetime import *\n\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\n\nnp.random.seed(23)\nnp.set_printoptions(threshold=50, edgeitems=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -al /kaggle/input/rossmann-time-series-data-engineering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OUTPUT = '/kaggle/working/'\nPATH='/kaggle/input/rossmann-time-series-data-engineering/'\ndf = pd.read_feather(f'{PATH}df')\ntrain_df = pd.read_feather(f'{PATH}joined2')\ntest_df = pd.read_feather(f'{PATH}joined2_test')\ntrain_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76f41bf7ed078cad9eb307628ed6fbd80691efac"},"cell_type":"markdown","source":"# Setup dataset"},{"metadata":{},"cell_type":"markdown","source":"Construct the dataset for deep learning model using relevant variables"},{"metadata":{"_uuid":"057b998fc43dfe0d9e527df9358a9e9d838f693e","trusted":true},"cell_type":"code","source":"cat_vars = ['Store', 'DayOfWeek', 'Promo',\n       'StateHoliday', 'SchoolHoliday', 'Year', 'Month', 'Week', 'Day',\n       'Is_year_end', 'Is_year_start', 'StoreType', 'Assortment', \n       'Promo2', 'PromoInterval', 'State',   \n       'Events',  'CompetitionMonthsOpen', \n       'Promo2Weeks',\n       'SchoolHoliday_bw','StateHoliday_bw', 'Promo_bw', 'SchoolHoliday_fw', 'StateHoliday_fw','Promo_fw', \n       'SchoolHoliday_DaySum', 'StateHoliday_DaySum', 'Promo_DaySum', \n       'SchoolHoliday_DayCount', 'StateHoliday_DayCount', 'Promo_DayCount']\n\ncont_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n            'Max_Humidity','Mean_Humidity', 'Min_Humidity',\n            'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h','Precipitationmm','CloudCover',\n            'trend', 'trend_DE', 'CompetitionDaysOpen', 'Promo2Days',\n            'AfterSchoolHoliday', 'BeforeSchoolHoliday', 'AfterStateHoliday',\n            'BeforeStateHoliday', 'AfterPromo', 'BeforePromo']\n\ndep_var = 'Sales'\ndf = train_df[cat_vars + cont_vars + [dep_var,'Date']].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine the time frame used for validation. Take the most recent date from the training set and use the same length as the length of the test set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Date'].min(), test_df['Date'].max(), len(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()\nvalid_idx = range(cut) ; valid_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Date'][0], train_df['Date'][cut] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are taking the last 'n' samples (most recent in time) as validation set. While the 'n' has the same length as the test set."},{"metadata":{},"cell_type":"markdown","source":"Now we can construct the dataset using fastai's databunch method. We will first use a larger batch size to have the model converge faster (smaller batch size gives more noise while large batch size risk over fitting? to be confirmed...)"},{"metadata":{"trusted":true},"cell_type":"code","source":"procs=[FillMissing, Categorify, Normalize]\n\ndatalist = (TabularList.from_df(df, path=OUTPUT, cat_names=cat_vars, cont_names=cont_vars, procs=procs,)\n                .split_by_idx(valid_idx=valid_idx)\n                .label_from_df(cols=dep_var, label_cls=FloatList, log=True)\n                .add_test(TabularList.from_df(test_df, path=PATH, cat_names=cat_vars, cont_names=cont_vars)))\ndata = datalist.databunch(bs=512)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the GPU device. (Should be type = 'cuda')"},{"metadata":{"trusted":true},"cell_type":"code","source":"defaults.device","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will define a boundary condition for our neural network output. (y_range)\nAnd build a fastai learner object. The two fully connected dense layer with size 1000, 500, and dropout rate 0.001, 0.01 as well as the dropout rate for the embedded layer 0.04 are taken from the fastai tutorial notebook directly. "},{"metadata":{"trusted":true},"cell_type":"code","source":"max_log_y = np.log(np.max(train_df['Sales'])*1.2)  # whether it is better to have +20% max sales need to be verified\ny_range = torch.tensor([0, max_log_y], device=defaults.device)\nlearn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, \n                        y_range=y_range, metrics=exp_rmspe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert the learner to fp16 to increase the efficiency"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.to_fp16","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the batch size"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.data.batch_size\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef85b3ea741bae09f0f05337c3b94779a0a8c360"},"cell_type":"markdown","source":"# Start training\nUse the learning rate finder to estimate the best learning rate to start"},{"metadata":{"trusted":true},"cell_type":"code","source":"import fastai\nfastai.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find(end_lr=100, wd=0.3)\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The steepest part is around 2e-2. So we will start at the 10 times less (2e-3). The weight decay 0.3 is choosen to be higher than normally used 0.1 or 0.2 because we have stuffed in almost all the variables without feature selection. As a result, a higher wd to avoid overfitting at the beginning. "},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 3e-3, wd=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('bs512_5ep_2e-3_wd0.3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-3, wd=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('bs512_2_5ep_1e-3_wd0.3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 3e-4, wd=0.3)\nlearn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('bs512_3_5ep_3e-4_wd0.3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do not get much improvement at this moment. Reduce the batch size to 128."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = datalist.databunch(bs=128)\nlearn.data = data\nlearn.data.batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-3, wd=0.2)\nlearn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('bs128_5ep_1e-3_wd0.2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-3, wd=0.2)\nlearn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-3, wd=0.1)\nlearn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(20, 1e-3, wd=0.1)\nlearn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.fit_one_cycle(5, 5e-4, wd=0.1)\n# learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlearn.save('last')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06027a212dbe9089765dbbd8f05b57f86153973f"},"cell_type":"markdown","source":"# Predict on test data"},{"metadata":{"_uuid":"499e33209d168ccca1a92db9925ba6948a330881","trusted":true},"cell_type":"code","source":"test_preds=learn.get_preds(DatasetType.Test)\ntest_df[\"Sales\"]=np.exp(test_preds[0].data).numpy().T[0]\ntest_df[[\"Id\",\"Sales\"]]=test_df[[\"Id\",\"Sales\"]].astype(\"int\")\ntest_df[[\"Id\",\"Sales\"]].to_csv(\"rossmann_submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}