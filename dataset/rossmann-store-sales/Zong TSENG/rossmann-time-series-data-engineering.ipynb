{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a notebook of preparing time-series data for further deep-learning applicaiton. \nIt is mainly based on the following notebook:\n\nhttps://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb\n\nWith additional tweaks like bridging the school/state holiday and promo period with weekend, and add running length encoding features. "},{"metadata":{},"cell_type":"markdown","source":"# Env setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from fastai.tabular import *\nimport pandas as pd\nimport os, tarfile\nimport random\nimport matplotlib.pyplot as plt\nimport re\nfrom datetime import *\nfrom isoweek import Week\n\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\n\nnp.set_printoptions(threshold=50, edgeitems=20)\npd.options.mode.chained_assignment = None\nPATH = '../input/multiple-data-source-of-rossmann/'\nOUTPUT = './'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def rmax(n=50):\n  pd.set_option('display.max_rows', n)\n\ndef cmax(n=50):\n  pd.set_option('display.max_columns',n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tarfile.open(f'{PATH}rossmann.tgz').extractall(path = OUTPUT)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"table_names = ['train', 'store', 'store_states', 'state_names', \n               'googletrend', 'weather', 'test']\ntables = [pd.read_csv(f'{OUTPUT}{fname}.csv', low_memory=False) for fname in table_names]\ntrain, store, store_states, state_names, googletrend, weather, test = tables\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quick summary of the data\nIn addition to the provided data, there are some xternal datasets put together by participants in the Kaggle competition.\n* train: Contains store information on a daily basis, tracks things like sales, customers, whether that day was a holdiay, etc.\n* store: general info about the store including competition, etc.\n* store_states: maps store to state it is in\n* state_names: Maps state abbreviations to names\n* googletrend: trend data for particular week/state\n* weather: weather conditions for each state\n* test: Same as training table, w/o sales and customers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML, display\nfor t in tables: display(t.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning / Add Date-related features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.StateHoliday.value_counts(), test.StateHoliday.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there is only one type of state holiday in the test set, we will thus simplify this variable into binary type."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.StateHoliday = (train.StateHoliday!='0').astype('int')\ntest.StateHoliday = (test.StateHoliday!='0').astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the google trend data is a weekly data (given by start-end date), we need to use either start or end of that period as an anchor to associate with the main store sales data (train/test). Here use of the start date of each period is more like leaking future data into the past. (i.e. the trend of 2014/9/30 is actually the google trend result of 9/28-10/4 .If we are going to predict 10/3's sales based on data before 10/3 such trend data won't be available after 10/4)\n\nMaybe we should consider use the google trend data from the previous week (or use the end-date of the period as the week to join df). Using the start-date "},{"metadata":{"trusted":true},"cell_type":"code","source":"googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we used the fastai's 'add_datepart' function to expand the date information into multiple features"},{"metadata":{"trusted":true},"cell_type":"code","source":"add_datepart(weather, \"Date\", drop=False)\nadd_datepart(googletrend, \"Date\", drop=False)\nadd_datepart(train, \"Date\", drop=False)\nadd_datepart(test, \"Date\", drop=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Join multiple tables\njoin_df is a function for joining tables on specific fields. By default, we'll be doing a left outer join of right on the left argument using the given fields for each table."},{"metadata":{"trusted":true},"cell_type":"code","source":"def join_df(left, right, left_on, right_on=None, suffix='_y'):\n    if right_on is None: right_on = left_on\n    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n                      suffixes=(\"\", suffix))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Join tables of weather and state name"},{"metadata":{"trusted":true},"cell_type":"code","source":"weather = join_df(weather, state_names, \"file\", \"StateName\")\nweather.State.unique() , googletrend.file.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The state codes in weather and googletrend are different: NI and HB,NI, and the googletrend file has a distinct category for the whole Germany."},{"metadata":{"trusted":true},"cell_type":"code","source":"googletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\ngoogletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a separate table for the whole germany google trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_de = googletrend[googletrend.file == 'Rossmann_DE']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now join the remaining tables"},{"metadata":{"trusted":true},"cell_type":"code","source":"store = join_df(store, store_states, \"Store\")\nlen(store[store.State.isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joined = join_df(train, store, \"Store\")\njoined_test = join_df(test, store, \"Store\")\nlen(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joined = join_df(joined, googletrend, [\"State\",\"Year\", \"Week\"])\njoined_test = join_df(joined_test, googletrend, [\"State\",\"Year\", \"Week\"])\nlen(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joined = joined.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\njoined_test = joined_test.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\nlen(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joined = join_df(joined, weather, [\"State\",\"Date\"])\njoined_test = join_df(joined_test, weather, [\"State\",\"Date\"])\nlen(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joined.shape, joined_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clean up the duplicated columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (joined, joined_test):\n    for c in df.columns:\n        if c.endswith('_y'):\n            if c in df.columns: df.drop(c, inplace=True, axis=1)\n        if c.endswith('_DE'):\n            if not c.startswith('trend'):\n              if c in df.columns: df.drop(c, inplace=True, axis=1)\njoined.shape, joined_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fill in missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (joined,joined_test):\n    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\nCreate new feature counting how many days a competition store has opened"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (joined,joined_test):\n    df[\"CompetitionOpenSince\"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, \n                                                     month=df.CompetitionOpenSinceMonth, day=15))\n    df[\"CompetitionDaysOpen\"] = df.Date.subtract(df.CompetitionOpenSince).dt.days\njoined.CompetitionDaysOpen.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joined.CompetitionDaysOpen.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll replace some erroneous / outlying data."},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (joined,joined_test):\n    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0\njoined.CompetitionDaysOpen.describe() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We add \"CompetitionMonthsOpen\" field, limiting the maximum to 2 years to limit number of unique categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (joined,joined_test):\n    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]//30\n    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\njoined.CompetitionMonthsOpen.unique()\njoined.CompetitionMonthsOpen.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same process for the promo date"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (joined,joined_test):\n    df[\"Promo2Since\"] = pd.to_datetime(df.apply(lambda x: Week(\n        x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1).astype('datetime64'))\n    df[\"Promo2Days\"] = df.Date.subtract(df[\"Promo2Since\"]).dt.days\nfor df in (joined,joined_test):\n    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]//7\n    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joined.shape, joined_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fill gap between event by weekend (if applicable)\n\nSince the weekend is usually considered as part of school/state holidays, we will fill in the gaps of school/state holidays by weekend. The same applies to Promo period.\n\nFirst we need to create a weekend variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [\"Date\",'Dayofweek', \"Store\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"]\ndf = train[columns].append(test[columns])\ndf['StateHoliday'] = df.StateHoliday.astype('int')\ndf['Weekend'] = 0\ndf.loc[df.Dayofweek>= 5 ,['Weekend']] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make new columns by combining weekend and School/state holidays"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['SchoolHoliday2'] = df['SchoolHoliday']\ndf['StateHoliday2'] = df['StateHoliday']\ndf['Promo2'] = df['Promo']\n\ndf.loc[df.Weekend==1, ['SchoolHoliday2', 'StateHoliday2', 'Promo2']] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate the duration of each event period"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['SchoolHoliday2', 'StateHoliday2', 'Promo2']\nsub = df[['Date','Store']+columns]   #make a smaller dataframe to work with. \nsub.sort_values(by=['Store', 'Date'], inplace=True)\n\ndaysum = sub.copy()\n\nfor c in columns:\n  daysum.loc[:,c] = sub.groupby(['Store', sub[c].diff().ne(0).cumsum()])[c].transform('sum')\n\ndf = df.merge(daysum, how = 'left', on=['Date', 'Store'], suffixes=['', '_DaySum']) ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmax(500)\ndf[(df.Date>datetime(2015,3,1)) & (df.Store==1)].head(500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are discrete holiday and promo periods which could be further linked if weekend was taken into consideration. \n\nIf the event duration is more than 2 days (DaySum > 2), it could be a weekend adjacent to that event. Replace the original data with the weekend-filled data."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['SchoolHoliday2_DaySum'] > 2, 'SchoolHoliday'] = df['SchoolHoliday2']\ndf.loc[df['StateHoliday2_DaySum'] > 2, 'StateHoliday'] = df['StateHoliday2'] \ndf.loc[df['Promo2_DaySum'] > 2, 'Promo'] = df['Promo2'] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the 'SchoolHoliday', 'StateHoliday', 'Promo' are taking weekend into consideration (will be consecutive if gap filled by the weekend). \n\nClean up the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['SchoolHoliday', 'StateHoliday', 'Promo']\ndf = df[['Date','Store', 'Weekend']+columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(Grabbed from fastai course notebook):\nWe'll define a function get_elapsed for cumulative counting across a sorted dataframe. Given a particular field fld to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero. Upon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly.\nLet's walk through an example. Say we're looking at School Holiday. We'll first sort by Store, then Date, and then call add_elapsed('SchoolHoliday', 'After'): This will apply to each row with School Holiday:\nA applied to every row of the dataframe in order of store and date\nWill add to the dataframe the days since seeing a School Holiday\nIf we sort in the other direction, this will count the days until another holiday."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_elapsed(fld, pre):\n    day1 = np.timedelta64(1, 'D')\n    last_date = np.datetime64()\n    last_store = 0\n    res = []\n\n    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):\n        if s != last_store:\n            last_date = np.datetime64()\n            last_store = s\n        if v: last_date = d\n        res.append(((d-last_date).astype('timedelta64[D]') / day1))\n    df[pre+fld] = res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fld = 'SchoolHoliday'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')\nfld = 'StateHoliday'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')\nfld = 'Promo'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're going to set the active index to Date. Then set null values from elapsed field calculations to 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.set_index(\"Date\")\ncolumns = ['SchoolHoliday', 'StateHoliday', 'Promo']\nfor o in ['Before', 'After']:\n    for p in columns:\n        a = o+p\n        df[a] = df[a].fillna(0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features from running length encoding\nCreate new features to count the total duration of a given event, such as how long is the current promo/holiday period. Add the serial count of that event as well (i.e. the first day of a 42days school holiday)\n\nAny gap of weekend was already filled up. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a smaller dataframe to work with"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['SchoolHoliday', 'StateHoliday', 'Promo']\nsub = df[['Store']+columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub.sort_index(inplace=True)\nsub.sort_values(by=['Store', 'Date'], inplace=True)\ndaysum = sub.copy()\ndaycount = sub.copy()\n\nfor c in columns:\n  daysum.loc[:,c] = sub.groupby(['Store', sub[c].diff().ne(0).cumsum()])[c].transform('sum')\n  daycount[c] = sub.groupby(['Store', sub[c].diff().ne(0).cumsum()])[c].transform('cumsum')\n\nsub2 = sub.merge(daysum, how = 'left', on=['Date', 'Store'], suffixes=['', '_DaySum']) ;\nsub2 = sub2.merge(daycount, how = 'left', on=['Date', 'Store'], suffixes=['', '_DayCount']) ; ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply rolling window functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"bwd = df[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()\nfwd = df[['Store']+columns].sort_index(ascending=False\n                                      ).groupby(\"Store\").rolling(7, min_periods=1).sum()\n\nbwd.drop(columns='Store',inplace=True)\nbwd.reset_index(inplace=True)\nfwd.drop(columns='Store',inplace=True)\nfwd.reset_index(inplace=True)\ndf.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merge the engineered features with the main dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\ndf = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])\n\nsub2.reset_index(inplace=True)\nsub2.drop(columns, 1 ,inplace=True)\ndf = df.merge(sub2, how='left', on=['Date','Store'])\ndf.drop(columns,1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_feather(f'{OUTPUT}df')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joined = join_df(joined, df, ['Store', 'Date'])\njoined_test = join_df(joined_test, df, ['Store', 'Date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instances where the store had zero sale (closed) are removed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"joined = joined[joined.Sales!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save the joined master dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"joined.reset_index(drop=True, inplace=True)\njoined_test.reset_index(drop=True, inplace=True)\njoined.to_feather(f'{OUTPUT}joined2')\njoined_test.to_feather(f'{OUTPUT}joined2_test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can proceed to the deep learning part:\n\nhttps://www.kaggle.com/zongtseng/rossmann-time-series-prediction-deep-learning?scriptVersionId=23443126"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}