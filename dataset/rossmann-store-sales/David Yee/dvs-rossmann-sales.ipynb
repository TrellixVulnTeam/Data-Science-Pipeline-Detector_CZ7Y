{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![Step1](https://raw.githubusercontent.com/davyee100/kaggle/master/Step1.png)\n![Step1_1](https://raw.githubusercontent.com/davyee100/kaggle/master/Step1_1.png)\n\nThis step will read the input file and show the basic information of the following:\n1. Number of observations and features\n1. Structure of the DataFrame\n1. Sample Data\n1. What's the Min and Max Date of both Training and Test Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/rossmann-store-sales/train.csv', low_memory=False, parse_dates=['Date'])\ntest_data = pd.read_csv('../input/rossmann-store-sales/test.csv', low_memory=False, parse_dates=['Date'])\nstore_data = pd.read_csv('../input/rossmann-store-sales/store.csv', low_memory=False)\n\n# train_data_copy= train_data.copy(deep = True)\n# test_data_copy= test_data.copy(deep = True)\n\nprint('Observations and Features Summary')\nprint('---------------------------------')\nprint('Training Data: ', train_data.shape)  # Show the number of observations and features\nprint('Test Data: ', test_data.shape)  # Show the number of observations and features\nprint('Store Data: ', store_data.shape)  # Show the number of observations and features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.info())  # Show the information about DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.info())  # Show the information about DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(store_data.info())  # Show the information about DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Display Sample Data')\n# train_data.head()  # Show the head of DataFrame\n# train_data.tail()  # Show the tail of DataFrame\ntrain_data.sample(10)  # Show number of samples data E.g. 10 records","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.sample(10)  # Show number of samples data E.g. 10 records","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data.sample(10)  # Show number of samples data E.g. 10 records","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Min and Max of Date')\nprint('-------------------')\nprint('Training Data: Min Date:', train_data['Date'].min(), 'Max Date:', train_data['Date'].max())\nprint('Test Data    : Min Date:', test_data['Date'].min(), 'Max Date:', test_data['Date'].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![Step1_2](https://raw.githubusercontent.com/davyee100/kaggle/master/Step1_2.png)\n\nThis step will clean-up inconsistencies in data before being used for modeling. This is to avoid data quality that will affect the training process / impact negatively the results generated\n1. Convert Date Field to proper format\n1. Number of Null Values\n1. Replacing Null Values / Dropping Columns\n1. Sample Data\n\n### **_Training Data_**\n![Section](https://raw.githubusercontent.com/davyee100/kaggle/master/Section.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Date'] = pd.to_datetime(train_data['Date'],format = '%Y-%m-%d')\ntrain_data['Day'] = train_data['Date'].dt.day\ntrain_data['Month'] = train_data['Date'].dt.month\ntrain_data['Year'] = train_data['Date'].dt.year\ntrain_data['SalesPerCustomer'] = train_data['Sales'] / train_data['Customers']\ntrain_data['SalesPerCustomer'].fillna(\"0\", inplace=True)\ntrain_data['SalesPerCustomer'] = train_data['SalesPerCustomer'].astype(np.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary:** No missing values for Training DataSet, therefore no cleansing is required\n\n### **_Test Data_**\n![Section](https://raw.githubusercontent.com/davyee100/kaggle/master/Section.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['Date'] = pd.to_datetime(test_data['Date'],format = '%Y-%m-%d')\ntest_data['Day'] = test_data['Date'].dt.day\ntest_data['Month'] = test_data['Date'].dt.month\ntest_data['Year'] = test_data['Date'].dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** From the above observation, we can see there is missing values in Test data on the column \"Open\". We will analyze on the records that contains missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Distinct Values of \"Open\": ', test_data['Open'].unique())\ntest_data.loc[test_data.Open.isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** Noticed that only Store = 622 has some missing values for column 'Open'. Let's check some sample data for Store = 622"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.loc[(test_data['Store'] == 622)].sort_values(by=['Date'], ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** Only the last few records in Test DataSet have those empty values. It seems that DayOfWeek = 7 is consistently populated in that range but not the otherwise. We could either replace all NaN with values of '1' or let it assumed as '0'. But before that let's confirm if data is consistent in Train DataSet for Store = 622 in terms of 'Open', 'DayOfWeek' and 'StateHoliday'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store = 622, it's a Sunday and Store is Open\ntrain_data.loc[(train_data['Store'] == 622) & (train_data['DayOfWeek'] == 7) & (train_data['Open'] == 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store = 622, it's not a Sunday and State Holiday\ntrain_data.loc[(train_data['Store'] == 622) & (train_data['DayOfWeek'] != 7) & (train_data['StateHoliday'].isin(['a','b','c']))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** From the above, it is deduced that all data for Store = 622 is consistent in terms of 'Open', 'StateHoliday' and 'DayOfWeek'. Therefore we will replace all missing values in column 'Open' with value 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['Open'] = test_data['Open'].fillna(\"1\").astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.loc[test_data['Store'] == 622].sort_values(by=['Date'], ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **_Store Data_**\n![Section](https://raw.githubusercontent.com/davyee100/kaggle/master/Section.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(store_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** There are a few missing values of Competition & Promo2 columns. We will start by processing those 'CompetitionDistance' that is null"},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data.loc[store_data['CompetitionDistance'].isnull() == True]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** Since those store that has missing 'CompetitionDistance' does not have any values for both 'CompetitionOpenSinceMonth' and 'CompetitionOpenSinceYear' we will default it with value '0' for both 'CompetitionOpenSinceMonth' and 'CompetitionOpenSinceYear' first, then only followed by 'CompetitionDistance'"},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data.loc[store_data['CompetitionDistance'].isnull(), 'CompetitionOpenSinceMonth'] = store_data.loc[store_data['CompetitionDistance'].isnull(), 'CompetitionOpenSinceMonth'].fillna(0)\nstore_data.loc[store_data['CompetitionDistance'].isnull(), 'CompetitionOpenSinceYear'] = store_data.loc[store_data['CompetitionDistance'].isnull(), 'CompetitionOpenSinceYear'].fillna(0)\nstore_data['CompetitionDistance'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(store_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** Now let's cleanup both 'CompetitionOpenSinceMonth' and 'CompetitionOpenSinceYear'"},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data.loc[(store_data['CompetitionOpenSinceMonth'].isnull() == True) & (store_data['CompetitionOpenSinceYear'].isnull() == True)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary:** Since we are not able to determine the Year and Month since the competition started, therefore we will default it to '0'. If there is a need to explore other option, we will revisit this default value."},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data['CompetitionOpenSinceMonth'].fillna(0, inplace=True)\nstore_data['CompetitionOpenSinceYear'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** Now let's cleanup the Promo2 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(store_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data.loc[store_data['Promo2SinceWeek'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** We will replace all these values with '0' for simplicity"},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data['Promo2SinceWeek'].fillna(0, inplace=True)\nstore_data['Promo2SinceYear'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** Let's take a look at the 'PromoInterval' column's unique values and replace it with a blank space"},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data['PromoInterval'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data['PromoInterval'].fillna('', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(store_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![Step1_3](https://raw.githubusercontent.com/davyee100/kaggle/master/Step1_3.png)\n\nThis step will replace all representation of string data with an apporpriate integer encoding that allows modeling to be able to run on it.\n\n### **_Train Data & Test Data_**\n![Section](https://raw.githubusercontent.com/davyee100/kaggle/master/Section.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get list of categorical variables\ns = ((train_data.dtypes == 'object') & (train_data.columns != 'Date'))\ntrain_object_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(train_object_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** Only 'StateHoliday' need to be cleaned up for both Training and Test Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nfrom sklearn.preprocessing import OneHotEncoder\n\noneHot = OneHotEncoder()\noneHot = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n# Fit the OneHotEncoder with training data then followed by using the same OneHotEncoder to encode the test data\nOH_train_data = oneHot.fit_transform(train_data[train_object_cols])\nOH_test_data = oneHot.transform(test_data[train_object_cols])\n# Read the columns name of OneHotEncoding Process. By default OneHot encoding will return columns with sequential number E.g. 0, 1, 2, 3 as the column in the matrix returned\ncolumn_names = oneHot.get_feature_names(train_object_cols)\n\n# Convert the matrix into dataframe with the column names acquired in the previous step\nOH_train_data = pd.DataFrame(OH_train_data, columns=column_names)\nOH_test_data = pd.DataFrame(OH_test_data, columns=column_names)\n\n# The DataFrame created in previous step will loose it's index, therefore we need to put back the index\nOH_train_data.index = train_data.index\nOH_test_data.index = test_data.index\n\n# Join the 2 dataframes created into 1 single dataframe\ntrain_data = pd.concat([train_data, OH_train_data], axis=1)\ntest_data = pd.concat([test_data, OH_test_data], axis=1)\n\ndel [[OH_train_data,OH_test_data]]\ngc.collect()\n\n# Convert all fields encoded into int32\ndata_cleaner = [train_data, test_data]\n\nfor dataset in data_cleaner:\n    for cols in column_names:\n        dataset[cols] = dataset[cols].astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **_Store Data_**\n![Section](https://raw.githubusercontent.com/davyee100/kaggle/master/Section.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get list of categorical variables\ns = ((store_data.dtypes == 'object') & (store_data.columns != 'PromoInterval'))\nstore_object_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(store_object_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Steps:** There are 2 columns 'StoreType' and 'Assortment' that has string values that we need to encode"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit and transform the OneHotEncoder with store data\nOH_store_data = oneHot.fit_transform(store_data[store_object_cols])\n\n# Read the columns name of OneHotEncoding Process. By default OneHot encoding will return columns with sequential number E.g. 0, 1, 2, 3 as the column in the matrix returned\ncolumn_names = oneHot.get_feature_names(store_object_cols)\n\n# Convert the matrix into dataframe with the column names acquired in the previous step\nOH_store_data = pd.DataFrame(OH_store_data, columns=column_names)\n\n# The DataFrame created in previous step will loose it's index, therefore we need to put back the index\nOH_store_data.index = store_data.index\n\n# Join the 2 dataframes created into 1 single dataframe\nstore_data = pd.concat([store_data, OH_store_data], axis=1)\n\ndel [[OH_store_data]]\ngc.collect()\n\n# Convert all fields encoded into int32\nfor cols in column_names:\n    store_data[cols] = store_data[cols].astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next Step:** Process the column 'PromoInterval' similar to OneHot Encoding but with each values of the month to each columns where it occurred"},{"metadata":{"trusted":true},"cell_type":"code","source":"month_data = np.array(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sept','Oct','Nov','Dec'])\nmonth_series = pd.Series(month_data, index=[0,1,2,3,4,5,6,7,8,9,10,11])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def promo_month(month_index,promointerval):\n    if month_index in promointerval:\n        return 1\n    else:\n        return 0\n\nfor month_index in month_series:\n    store_data['Promo_' + month_index] = store_data.apply(lambda x: promo_month(month_index, x['PromoInterval']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the data type of columns of Competition and Promo2\nstore_data['CompetitionOpenSinceMonth'] = store_data['CompetitionOpenSinceMonth'].astype('int64')\nstore_data['CompetitionOpenSinceYear'] = store_data['CompetitionOpenSinceYear'].astype('int64')\nstore_data['Promo2SinceWeek'] = store_data['Promo2SinceWeek'].astype('int64')\nstore_data['Promo2SinceYear'] = store_data['Promo2SinceYear'].astype('int64')\nstore_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![Step1_4](https://raw.githubusercontent.com/davyee100/kaggle/master/Step1_4.png)\n\nWe will do this with a left outer join based on 'Store' which is the key. Then we will create a new DataFrame that still contains all Original Fields for Visualizations. Once we have completed all Visualizations then we will drop out all fields that are no longer required."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_visual = train_data[['Store','DayOfWeek','Date','Sales','Customers','SalesPerCustomer','Open','Promo','StateHoliday','SchoolHoliday','Year','Month','Day']].merge(store_data[['Store','StoreType','Assortment','CompetitionDistance','CompetitionOpenSinceMonth','CompetitionOpenSinceYear','Promo2','Promo2SinceWeek','Promo2SinceYear','PromoInterval']], on=['Store'], how='left')\n\nobject_cols = train_object_cols + store_object_cols\nobject_cols.insert(0,\"PromoInterval\")\nobject_cols.insert(0,\"StateHoliday\")\n\ntrain_data = train_data.merge(store_data, on=['Store'], how='left')\ntrain_data = train_data.drop(object_cols, axis=1)\n\ntest_data = test_data.merge(store_data, on=['Store'], how='left')\ntest_data = test_data.drop(object_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data_visual.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('New Train Data with Store: ', train_data.shape)\nprint('New Test Data with Store: ', test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Display Structure of DataFrame')\nprint('------------------------------')\nprint(train_data.info())  # Show the information about DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.info())  # Show the information about DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![Step2](https://raw.githubusercontent.com/davyee100/kaggle/master/Step2.png)\n\nDo an explanatory data analysis using matplotlib and seaborn to see anything else we can discover through manual data discovery"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Display Structure of Visual DataFrame')\nprint('-------------------------------------')\nprint(train_data_visual.info())  # Show the information about DataFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **_Analysis 1: \"Sales\", \"Customers\" and Store \"Open\" vs \"Day of Week\"_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nfrom pandas.plotting import register_matplotlib_converters\n%matplotlib inline\n\nregister_matplotlib_converters()\n\ntrain_data_temp = train_data_visual.groupby(['DayOfWeek']).agg({'Sales':'sum', 'Customers':'sum', 'Open':'sum'}).reset_index()\ntrain_data_temp['SalesPerCustomer'] = train_data_temp['Sales'] / train_data_temp['Customers']\nsns.set_palette('Set2')\n\nfig, qaxis = plt.subplots(2, 2, figsize = (22,12))\n\nsns.barplot(x='DayOfWeek', y='Sales', data=train_data_temp, ax=qaxis[0,0])\nqaxis[0,0].set_title('Sales vs Day of Week Comparison')\n\nsns.barplot(x='DayOfWeek', y='Customers', data=train_data_temp, ax=qaxis[0,1])\nqaxis[0,1].set_title('Customers vs Day of Week Comparison')\n\nsns.barplot(x='DayOfWeek', y='SalesPerCustomer', data=train_data_temp, ax=qaxis[1,0])\nqaxis[1,0].set_title('SalesPerCustomer vs Day of Week Comparison')\n\nsns.barplot(x='DayOfWeek', y='Open', data=train_data_temp, ax=qaxis[1,1])\nqaxis[1,1].set_title('Open vs Day of Week Comparison')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**_Remarks:_** Above shows the comparison between 'DayOfWeek' against both 'Customers' and 'Sales' Measures. It shows that most of the stores closes on Sunday = '7' and both 'Customers' and 'Sales' are mainly recorded during the other days which is the weekday except 'Saturday'\n\n\n### **_Analysis 2: \"Sales\" vs \"Store Type\" and \"Assortment\"_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_temp = train_data_visual.groupby(['StoreType']).agg({'Sales':'sum', 'Customers':'sum'}).reset_index()\ntrain_data_temp['SalesPerCustomer'] = train_data_temp['Sales'] / train_data_temp['Customers']\n\ntrain_data_temp2 = train_data_visual.groupby(['Assortment']).agg({'Sales':'sum', 'Customers':'sum'}).reset_index()\ntrain_data_temp2['SalesPerCustomer'] = train_data_temp2['Sales'] / train_data_temp2['Customers']\n\nfig, qaxis = plt.subplots(3, 2, figsize = (22,20))\n\nsns.barplot(x='StoreType', y='Sales', data=train_data_temp, ax=qaxis[0,0])\nqaxis[0,0].set_title('Sales vs Store Type Comparison')\n\nsns.barplot(x='Assortment', y='Sales', data=train_data_temp2, ax=qaxis[0,1])\nqaxis[0,1].set_title('Sales vs Assortment Comparison')\n\nsns.barplot(x='StoreType', y='Customers', data=train_data_temp, ax=qaxis[1,0])\nqaxis[1,0].set_title('Customers vs Store Type Comparison')\n\nsns.barplot(x='Assortment', y='Customers', data=train_data_temp2, ax=qaxis[1,1])\nqaxis[1,1].set_title('Customers vs Assortment Comparison')\n\nsns.barplot(x='StoreType', y='SalesPerCustomer', data=train_data_temp, ax=qaxis[2,0])\nqaxis[2,0].set_title('SalesPerCustomer vs Store Type Comparison')\n\nsns.barplot(x='Assortment', y='SalesPerCustomer', data=train_data_temp2, ax=qaxis[2,1])\nqaxis[2,1].set_title('SalesPerCustomer vs Assortment Comparison')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**_Remarks:_** From this we can see that Store Type = a has both highest values of 'Sales' and 'Customers'. But on the contrary in terms of 'Sales Per Customer' Store Type = d has the highest value / purchasing power per customer. It could be due to the expensive / exclusivity of the Store Type.\n\nAssortment a = basic, b = extra, c = extended\n\n### **_Analysis 3: \"Sales\" vs \"Promo\" and \"Promo 2\"_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_temp = train_data_visual.groupby(['Promo']).agg({'Sales':'sum', 'Customers':'sum'}).reset_index()\ntrain_data_temp['SalesPerCustomer'] = train_data_temp['Sales'] / train_data_temp['Customers']\n\ntrain_data_temp2 = train_data_visual.groupby(['Promo2']).agg({'Sales':'sum', 'Customers':'sum'}).reset_index()\ntrain_data_temp2['SalesPerCustomer'] = train_data_temp2['Sales'] / train_data_temp2['Customers']\n\nfig, qaxis = plt.subplots(3, 2, figsize = (22,20))\n\nsns.barplot(x='Promo', y='Sales', data=train_data_temp, ax=qaxis[0,0])\nqaxis[0,0].set_title('Sales vs Promo Comparison')\n\nsns.barplot(x='Promo2', y='Sales', data=train_data_temp2, ax=qaxis[0,1])\nqaxis[0,1].set_title('Sales vs Promo2 Comparison')\n\nsns.barplot(x='Promo', y='Customers', data=train_data_temp, ax=qaxis[1,0])\nqaxis[1,0].set_title('Customers vs Promo Comparison')\n\nsns.barplot(x='Promo2', y='Customers', data=train_data_temp2, ax=qaxis[1,1])\nqaxis[1,1].set_title('Customers vs Promo2 Comparison')\n\nsns.barplot(x='Promo', y='SalesPerCustomer', data=train_data_temp, ax=qaxis[2,0])\nqaxis[2,0].set_title('Sales Per Customer vs Promo Comparison')\n\nsns.barplot(x='Promo2', y='SalesPerCustomer', data=train_data_temp2, ax=qaxis[2,1])\nqaxis[2,1].set_title('Sales Per Customer vs Promo2 Comparison')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**_Remarks:_** From the observations above, it seems that Promo2 does not seem to further increase the sales subsequently\n\n### **_Analysis 4: \"Sales\" vs \"School Holiday\" and \"State Holiday\"_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_temp = train_data_visual.groupby(['SchoolHoliday']).agg({'Sales':'sum', 'Customers':'sum'}).reset_index()\ntrain_data_temp['SalesPerCustomer'] = train_data_temp['Sales'] / train_data_temp['Customers']\n\ntrain_data_temp2 = train_data_visual.groupby(['StateHoliday']).agg({'Sales':'sum', 'Customers':'sum'}).reset_index()\ntrain_data_temp2['SalesPerCustomer'] = train_data_temp2['Sales'] / train_data_temp2['Customers']\n\nfig, qaxis = plt.subplots(3, 2, figsize = (22,20))\n\nsns.barplot(x='SchoolHoliday', y='Sales', data=train_data_temp, ax=qaxis[0,0])\nqaxis[0,0].set_title('Sales vs School Holiday Comparison')\n\nsns.barplot(x='StateHoliday', y='Sales', data=train_data_temp2, ax=qaxis[0,1])\nqaxis[0,1].set_title('Sales vs State Holiday Comparison')\n\nsns.barplot(x='SchoolHoliday', y='Customers', data=train_data_temp, ax=qaxis[1,0])\nqaxis[1,0].set_title('Customers vs School Holiday Comparison')\n\nsns.barplot(x='StateHoliday', y='Customers', data=train_data_temp2, ax=qaxis[1,1])\nqaxis[1,1].set_title('Customers vs State Holiday Comparison')\n\nsns.barplot(x='SchoolHoliday', y='SalesPerCustomer', data=train_data_temp, ax=qaxis[2,0])\nqaxis[2,0].set_title('Sales Per Customer vs School Holiday Comparison')\n\nsns.barplot(x='StateHoliday', y='SalesPerCustomer', data=train_data_temp2, ax=qaxis[2,1])\nqaxis[2,1].set_title('Sales Per Customer vs State Holiday Comparison')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**_Remarks:_** Obviously \"School Holiday\" and \"State Holiday\" does not impact the \"Sales\" positively as both graphs shown.\n\n### **_Analysis 5: Monthly Sales Trend_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_temp = train_data_visual.groupby(['StoreType',pd.Grouper(key='Date', freq='7D')]).Sales.sum().reset_index()\n\nfig, ax = plt.subplots(figsize= (28,8))\n\n# # Define the date format\ndate_form = DateFormatter(\"%y/%m\")\nax.xaxis.set_major_formatter(date_form)\nax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n\nax = sns.lineplot(data = train_data_temp, x = 'Date', y = 'Sales', hue = 'StoreType')\nax.set_title('Monthly Sales Trend by Store Type')\n\nplt.legend(['Store Type a', 'Store Type b', 'Store Type c', 'Store Type d'], title='Store Type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**_Remarks :_** From the above visualization, we can deduce that most of the Sales has its highest peak during XMAS.\n\n### **_Analysis 6: Weekly Sales Trend_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_temp = train_data_visual[(train_data_visual['Year'] == 2013)]\ntrain_data_temp = train_data_temp.groupby(['StoreType',pd.Grouper(key='Date', freq='7D')]).Sales.sum().reset_index()\n\nfig, ax = plt.subplots(figsize= (28,8))\n\n# # Define the date format\ndate_form = DateFormatter(\"%m/%d\")\nax.xaxis.set_major_formatter(date_form)\nax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=2))\n\nax = sns.lineplot(data = train_data_temp, x = 'Date', y = 'Sales', hue = 'StoreType')\nax.set_title('Weekly Sales Trend by Store Type')\n\nplt.legend(['Store Type a', 'Store Type b', 'Store Type c', 'Store Type d'], title='Store Type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **_Remarks :_** To revisit the weekly trend later to display background\n\n### **_Analysis 7: Correlation of Features_**\n\nLet's take a look at the Correlation of features in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the correlation matrix \ntrain_corr_all = train_data.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(train_corr_all, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize = (14, 12))\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(train_corr_all,\n            mask = mask,\n            square = True,\n            linewidths = .1,\n            vmax = 1.0,\n            linecolor = 'white',\n            cbar_kws = {'shrink':.9},\n            ax = ax,\n            cmap = colormap)\nplt.title('Rossmann Store Sales Correlation of Features', size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean-up non-active Data Frame\ndel [[train_corr_all,train_data_temp,train_data_temp2]]\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![Step3](https://raw.githubusercontent.com/davyee100/kaggle/master/Step3.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Common Model Helpers\nfrom sklearn import feature_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\n# Common Model Algorithms\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nimport xgboost as xgb\n\nData_X = ['Store','DayOfWeek','Open','Promo','SchoolHoliday','Day','Month','Year', \\\n          'StateHoliday_0','StateHoliday_a','StateHoliday_b','StateHoliday_c','CompetitionDistance','CompetitionOpenSinceMonth','CompetitionOpenSinceYear', \\\n          'Promo2','Promo2SinceWeek','Promo2SinceYear','StoreType_a','StoreType_b','StoreType_c','StoreType_d', 'Assortment_a','Assortment_b','Assortment_c', \\\n          'Promo_Jan', 'Promo_Feb','Promo_Mar','Promo_Apr', 'Promo_May', 'Promo_Jun', 'Promo_Jul', 'Promo_Aug', 'Promo_Sept', 'Promo_Oct','Promo_Nov', 'Promo_Dec']\nTarget_Y = ['Sales']\n\ndef rmspe_xg(yhat, y):\n    y = np.expm1(y.get_label())\n    yhat = np.expm1(yhat)\n    return \"rmspe\", rmspe(y,yhat)\n\ndef ToWeight(y):\n    w = np.zeros(y.shape, dtype=float)\n    ind = y != 0\n    w[ind] = 1./(y[ind]**2)\n    return w\n\ndef rmspe(y, yhat):\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n    return rmspe\n\n# A rule of thumb is to transform my target value to log if i see the values are very dispersed which is the case\n# and then of course revert them with np.exp to their real values\n\nX = train_data[Data_X]\ny = np.log1p(train_data['Sales'])\n\nX_train,X_valid,y_train,y_valid = train_test_split(X, y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Model (Default)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dt = DecisionTreeRegressor(max_depth=12, random_state=1).fit(X_train,y_train)\ny_valid_pred = model_dt.predict(X_valid)\n\nprint(\"Validation - R2 Scoring                             : \", r2_score(np.expm1(y_valid),np.expm1(y_valid_pred)))\nprint(\"Validation - Mean Squared Error (RMSPE)             : \", rmspe(np.expm1(y_valid),np.expm1(y_valid_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model_dt.predict(X_train)\n\nprint(\"Train - R2 Scoring                             : \", r2_score(np.expm1(y_train),np.expm1(y_train_pred)))\nprint(\"Train - Mean Squared Error (RMSPE)             : \", rmspe(np.expm1(y_train),np.expm1(y_train_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Model (Default)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel_rfr = RandomForestRegressor(n_estimators=15).fit(X_train,y_train)\ny_valid_pred = model_rfr.predict(X_valid)\n\nprint(\"Validation - R2 Scoring                             : \", r2_score(np.expm1(y_valid),np.expm1(y_valid_pred)))\nprint(\"Validation - Mean Squared Error (RMSPE)             : \", rmspe(np.expm1(y_valid),np.expm1(y_valid_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model_rfr.predict(X_train)\n\nprint(\"Train - R2 Scoring                             : \", r2_score(np.expm1(y_train),np.expm1(y_train_pred)))\nprint(\"Train - Mean Squared Error (RMSPE)             : \", rmspe(np.expm1(y_train),np.expm1(y_train_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Model (Default)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nprint(\"XGBoost Version: {}\". format(xgb.__version__))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\"objective\": \"reg:linear\", # for linear regression\n          \"booster\" : \"gbtree\",   # use tree based models \n          \"eta\": 0.03,   # learning rate\n          \"max_depth\": 10,    # maximum depth of a tree\n          \"subsample\": 0.9,    # Subsample ratio of the training instances\n          \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n          \"silent\": 1,   # silent mode\n          \"seed\": 10   # Random number seed\n          }\nnum_boost_round = 4000\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndvalid = xgb.DMatrix(X_valid, y_valid)\nwatchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n\nmodel_xgb = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_valid_pred = model_xgb.predict(xgb.DMatrix(X_valid))\n\nprint(\"Validation - R2 Scoring                             : \", r2_score(np.expm1(y_valid),np.expm1(y_valid_pred)))\nprint(\"Validation - Mean Squared Error (RMSPE)             : \", rmspe(np.expm1(y_valid),np.expm1(y_valid_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model_xgb.predict(xgb.DMatrix(X_train))\n\nprint(\"Train - R2 Scoring                             : \", r2_score(np.expm1(y_train),np.expm1(y_train_pred)))\nprint(\"Train - Mean Squared Error (RMSPE)             : \", rmspe(np.expm1(y_train),np.expm1(y_train_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSPE correction on the whole\ndef correction():\n    weights = np.arange(0.98, 1.02, 0.005)\n    errors = []\n    for w in weights:\n        error = rmspe(np.expm1(y_valid), np.expm1(y_valid_pred*w))\n        errors.append(error)\n        \n    # make line plot\n    plt.plot(weights, errors)\n    plt.xlabel('weight')\n    plt.ylabel('RMSPE')\n    plt.title('RMSPE Curve')\n\n    # print min error\n    idx = errors.index(min(errors))\n    print('Best weight is {}, RMSPE is {:.4f}'.format(weights[idx], min(errors)))\n    \ncorrection()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain = xgb.DMatrix(X_train, y_train)\ndtest = xgb.DMatrix(test_data[Data_X])\n\n# specify parameters via map\nparams = {\"objective\": \"reg:linear\", # for linear regression\n          \"booster\" : \"gbtree\",   # use tree based models \n          \"eta\": 0.03,   # learning rate\n          \"max_depth\": 10,    # maximum depth of a tree\n          \"subsample\": 0.9,    # Subsample ratio of the training instances\n          \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n          \"silent\": 1,   # silent mode\n          \"seed\": 10   # Random number seed\n          }\nnum_round = 3000\nmodel_xgb = xgb.train(params, dtrain, num_round)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make prediction\npreds = model_xgb.predict(dtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make Submission using Best Weight\nresult = pd.DataFrame({\"Id\": test_data[\"Id\"],'Sales': np.expm1(preds*1)})\nresult.to_csv(\"submission_xgb.csv\", index=False)\n\n# plot feature importance, show top 10 features\nfig, ax = plt.subplots(figsize=(8,8))\nxgb.plot_importance(model_xgb, max_num_features=10, height=0.5, ax=ax)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}