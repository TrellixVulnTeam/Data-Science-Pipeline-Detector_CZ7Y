{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"outputs":[],"source":"%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"'''\nBased on https://www.kaggle.com/justdoit/rossmann-store-sales/xgboost-in-python-with-rmspe/code\n'''\n\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport uuid\nfrom sklearn import cross_validation\nfrom datetime import date, timedelta\nfrom sklearn.cross_validation import KFold, train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom scipy.stats import pearsonr\nimport numpy as np\nfrom sklearn.cluster import AgglomerativeClustering\n\n\n\ndef pearson_affinity(M):\n    return 1 - np.array([[pearsonr(a,b)[0] for a in M] for b in M])\n\ndef factor(series):\n    #input should be a pandas series object\n    dic = {}\n    for i,val in enumerate(series.value_counts().index):\n        dic[val] = i\n    return [ dic[val] for val in series.values ]  \n\n\n\n\n# Thanks to Chenglong Chen for providing this in the forum\ndef ToWeight(y):\n    w = np.zeros(y.shape, dtype=float)\n    ind = y != 0\n    w[ind] = 1./(y[ind]**2)\n    return w\n\n\ndef rmspe(yhat, y):\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n    return rmspe\n\n\ndef rmspe_xg(yhat, y):\n    # y = y.values\n    y = y.get_label()\n    y = np.exp(y) - 1\n    yhat = np.exp(yhat) - 1\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n    return \"rmspe\", rmspe\n\ndef XGBoost(X_train,X_valid,params,verbose=False):\n    dtrain = xgb.DMatrix(X_train[features], np.log(X_train[\"Sales\"] + 1))\n    dvalid = xgb.DMatrix(X_valid[features], np.log(X_valid[\"Sales\"] + 1))\n    num_trees = params['num_trees']\n    \n    watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n    #watchlist = [(dvalid, 'eval'),(dtrain, 'train')]\n\n    gbm = xgb.train(params,\n                    dtrain,\n                    num_trees,\n                    evals=watchlist,\n                    early_stopping_rounds=10,\n                    feval=rmspe_xg,\n                    verbose_eval=verbose,\n                    )\n    \n    train_probs = gbm.predict(xgb.DMatrix(X_train[features]),ntree_limit=gbm.best_iteration)\n\n    train_error = rmspe(np.exp(train_probs) - 1, X_train['Sales'].values)\n    \n    valid_probs = gbm.predict(xgb.DMatrix(X_valid[features]),ntree_limit=gbm.best_iteration)\n    indices = valid_probs < 0\n    valid_probs[indices] = 0\n    valid_error = rmspe(np.exp(valid_probs) - 1, X_valid['Sales'].values)\n    return gbm, valid_error, train_error\n\n\n\n# Gather some features\ndef build_features(features, data, dates):\n    # remove NaNs\n    data.fillna(0, inplace=True)\n    data.loc[data.Open.isnull(), 'Open'] = 1\n        \n    # Use some properties directly\n    features.extend(['Store', 'CustomerCluster', 'SalesCluster','CompetitionOpenSinceMonth',\n                     'CompetitionOpenSinceYear', 'Promo', 'Promo2SinceWeek', 'Promo2SinceYear'])\n    \n    #log of CompetitionDistance\n    features.append('logDist')\n    data['logDist'] = np.log(1+data.CompetitionDistance)\n\n    # add some more with a bit of preprocessing\n    features.append('SchoolHoliday')\n    data['SchoolHoliday'] = data['SchoolHoliday'].astype(float)\n\n    #features.append('StateHoliday')\n    StateHolidayDict= { 0:0 , '0' : 0 , 'a': 1 , 'b' : 2 , 'c' : 3}\n    data['StateHoliday'] =  [ StateHolidayDict[i]  for i in data.StateHoliday.values ]\n\n    features.append('DayOfWeek')\n    features.append('month')\n    features.append('day')\n    features.append('year')\n    data['Date'] = pd.to_datetime(data.Date)\n    data['Date'] = pd.DatetimeIndex(data.Date)\n    data = data.join(dates,on='Date')\n\n    #features.append('StoreType')\n    StoreTypeDict  = { 'a' : 0 ,'b' : 1 , 'c' : 2 , 'd':3 }\n    data['StoreType']  = [ StoreTypeDict[i]  for i in data.StoreType.values ]\n\n    #features.append('Assortment')\n    AssortmentDict = { 'a' : 0 ,'b' : 1 , 'c' : 2 }\n    data['Assortment'] = [ AssortmentDict[i] for i in data.Assortment.values]\n    \n    features.append('AssortStoreType')\n    data['AssortStoreType'] = data['Assortment'] + 10*data['StoreType']\n    \n    return data\n\n\nprint(\"Load the training, test and store data using pandas\")\ntrain = pd.read_csv(\"../input/train.csv\")\ntest  = pd.read_csv(\"../input/test.csv\")\nstore = pd.read_csv(\"../input/store.csv\")\n\nprint(\"Generate Dates Table\")\ndates = pd.DataFrame(pd.date_range(train.Date.min(),test.Date.max()),columns=['Date']).set_index('Date')\ndates['day']   = dates.index.day.astype(int)\ndates['month'] = dates.index.month.astype(int)\ndates['year']  = dates.index.year.astype(int)\n\nprint(\"Assume store open, if not provided\")\ntest.fillna(1, inplace=True)\n\nprint('Cluster stores by sales correlation.')\nsales_pivot  = pd.pivot_table(train,values='Sales',index='Date', columns=['Store'],aggfunc='mean').dropna()\nprint(sales_pivot.head())\nsales_corr   = sales_pivot.corr()\ncluster = AgglomerativeClustering(n_clusters=50, linkage='average',affinity=pearson_affinity).fit(sales_corr)\nstore['SalesCluster'] = cluster.labels_\n\nprint('Cluster stores by customer correlation.')\ncust_pivot  = pd.pivot_table(train,values='Customers',index='Date', columns=['Store'],aggfunc='mean').dropna()\nprint(cust_pivot.head())\ncust_corr   = sales_pivot.corr()\ncluster = AgglomerativeClustering(n_clusters=50, linkage='average',affinity=pearson_affinity).fit(cust_corr)\nstore['CustomerCluster'] = cluster.labels_\n\n\nprint(\"Join with store\")\ntrain = pd.merge(train, store, on='Store')\ntest = pd.merge(test, store, on='Store')\n\nfeatures = []\n\nprint(\"Augment features\")\ntrain = build_features(features, train, dates)\ntest  = build_features([], test, dates)\nprint(features)\n\nprint('Reduce data for optimization.')\n#train = train[train.Store < 200] \ntrain.index = range(len(train))\ntrain.head()\n\nprint(\"Consider only open stores for training. Closed stores wont count into the score.\")\ntrain = train[train[\"Open\"] != 0]\n\nprint(\"Consider only days with sales for training.\")\ntrain = train[train[\"Sales\"] != 0]\n\n#Cross validation\ntrain.sort(['Date','Store'],inplace=True)\nindex = range(len(train))\ntrain.index = index\nprint(train.head())\ntrainSize = 0.99\ntrainIds, validIds =  train.index[:len(train)*trainSize] , train.index[len(train)*trainSize:] \n#validIds, testIds     =  train_test_split(validTestIds,train_size=0.50)\ntestIds = []\n\ntrainIds = list(trainIds)\n\nprint(\"Size Train:\", len(trainIds))\nprint(\"Size Valid:\", len(validIds))\nprint(\"Size  Test:\", len(testIds))\nassert len(trainIds)+len(testIds)+len(validIds) == len(train)\n\nplt.scatter(trainIds,[1]*len(trainIds),marker='+',color='b',label='Train')\nplt.scatter(validIds,[1]*len(validIds),marker='+',color='r',label='Valid')\nplt.scatter( testIds,[1]*len(testIds) ,marker='+',color='g',label='Test')\nplt.legend(loc=2)\n\n# Parameter optimization\n\nparams = {'base_score': 0, \n          'alpha': 0, \n          'booster': 'gbtree', \n          'colsample_bytree': 0.8, \n          'silent': 1,\n          'subsample': 0.9,\n          'eta': 0.2, \n          'num_trees': 10000, \n          'objective': 'reg:linear', \n          'max_depth': 12, \n          'lambda': 1,\n          'nthread':None}\n\n\n\ngbm,error_valid,error_train = XGBoost(train.loc[trainIds],train.loc[validIds],params,verbose=True)\n\nimport operator\nimportance = gbm.get_fscore()\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\n\ndf = pd.DataFrame(importance, columns=['feature', 'fscore'])\ndf['fscore'] = 100. * df['fscore'] / df['fscore'].max()\ndf.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 20))\n\n# Make Prediction\n\nprint(\"Make predictions on the test set\")\ndtest = xgb.DMatrix(test[features])\ntest_probs = gbm.predict(xgb.DMatrix(test[features]),ntree_limit=gbm.best_iteration)\nindices = test_probs < 0\ntest_probs[indices] = 0\nsubmission = pd.DataFrame({\"Id\": test[\"Id\"], \"Sales\": np.exp(test_probs) - 1})\nsubmission_id = uuid.uuid4()\nprint(submission_id)\nsubmission.to_csv(\"xgboost_script_submission_%s.csv\" %(submission_id), index=False)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}