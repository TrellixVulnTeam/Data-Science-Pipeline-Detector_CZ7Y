{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Rossmann Store Sales Prediction\n# By Mohamed Eltayeb","metadata":{"id":"97a3239d"}},{"cell_type":"markdown","source":"# Import libraries","metadata":{"id":"968bc204"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom pandas.plotting import scatter_matrix\n\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.compose import TransformedTargetRegressor\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.options.mode.chained_assignment = None  # default='warn'\nplt.rcParams[\"figure.figsize\"] = (12, 8)\npd.set_option('display.max_columns', None)","metadata":{"id":"ca5139f0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The Evaluation Metric\ndef RMSPE(y_true, y_pred):\n    '''\n    Compute Root Mean Square Percentage Error between two arrays.\n    '''\n    loss = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0))\n\n    return loss","metadata":{"id":"ac57d761","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the LGBM Features Importances\ndef plotImp(model, X , num = 20, fig_size = (40, 20)):\n    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n    plt.figure(figsize=fig_size)\n    sns.set(font_scale = 5)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances-01.png')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read the training and testing data\n","metadata":{"id":"d4023c8a"}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/rossmann-store-sales/train.csv\",dtype={'StateHoliday': object})\ntest_df = pd.read_csv(\"../input/rossmann-store-sales/test.csv\")\nstore_df = pd.read_csv(\"../input/rossmann-store-sales/store.csv\")","metadata":{"id":"1ede6afb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.merge(train_df, store_df, how = 'left', on = 'Store')\ntest_df = pd.merge(test_df, store_df, how = 'left', on = 'Store')","metadata":{"id":"b4d2ec61","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert the Tabular Data to Time Series Data","metadata":{"id":"433c9901"}},{"cell_type":"code","source":"ID = test_df['Id']\ntest_df.drop('Id',inplace=True,axis=1)\n\ntrain_df.sort_values([\"Store\",\"Date\"], ignore_index=True, inplace=True)\ntest_df.sort_values([\"Store\",\"Date\"], ignore_index=True, inplace=True)\n\nfor dataset in (train_df,test_df):\n    dataset['Date'] = pd.to_datetime(dataset['Date'])\n    dataset['Day'] = dataset.Date.dt.day\n    dataset['Month'] = dataset.Date.dt.month\n    dataset['Year'] = dataset.Date.dt.year\n    dataset['DayOfYear'] = dataset.Date.dt.dayofyear\n    dataset['WeekOfYear'] = dataset.Date.dt.weekofyear\n    dataset.set_index('Date', inplace=True)","metadata":{"id":"882341a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory data analysis","metadata":{"id":"b7f152fb"}},{"cell_type":"markdown","source":"# Features:","metadata":{"id":"eebc4b79"}},{"cell_type":"markdown","source":"* Id - an Id that represents a (Store, Date) duple within the test set\n* Store - a unique Id for each store\n* Sales - the turnover for any given day (this is what you are predicting)\n* Customers - the number of customers on a given day\n* Open - an indicator for whether the store was open: 0 = closed, 1 = open\n* StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n* SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n* StoreType - differentiates between 4 different store models: a, b, c, d\n* Assortment - describes an assortment level: a = basic, b = extra, c = extended\n* CompetitionDistance - distance in meters to the nearest competitor store\n* CompetitionOpenSince[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n* Promo - indicates whether a store is running a promo on that day\n* Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n* Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n* PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store","metadata":{"id":"7bf0a4e6"}},{"cell_type":"code","source":"train_df.shape","metadata":{"id":"7e414fa4","outputId":"41b5bdc8-1db9-4137-d4a3-35d78018fd8c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"id":"0eecb98d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"id":"7eb42a7f","outputId":"8c3a2dcc-3120-4df0-eafe-b1ef54083f45","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"id":"3673a464","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"id":"115a4f9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"id":"9abeadb6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"id":"28cdfec7","outputId":"1cbc65b4-5e60-4339-90e2-374f737dc26a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.describe()","metadata":{"id":"fbf8c52b","outputId":"d58f7c58-0d84-4269-eac2-c08dd1c19dc2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plots","metadata":{"id":"070fd51e"}},{"cell_type":"code","source":"#Numerical Features Histograms (Train)\nnum_feats = list(train_df.select_dtypes(include=['int64', 'float64', 'int32']).columns)\ntrain_df[num_feats].hist(figsize=(20,15));","metadata":{"id":"38601abf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Numerical Features Histograms (Test)\nnum_feats = list(test_df.select_dtypes(include=['int64', 'float64', 'int32']).columns)\ntest_df[num_feats].hist(figsize=(20,15));","metadata":{"id":"e02dfe58","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can notice several things. Firstly, the test set does not have the customers feature, so we cannot use it in prediction. But we maybe can utilize it in doing clusters for the stores.\n#### Secondly, for each feature, the distribution in the training set has almost an equal corresponding one in the test set except for the feature 'SchoolHoliday' which has a lot of '1.0's values in the test set.","metadata":{"id":"34625426"}},{"cell_type":"code","source":"#Sales_Stores\nplt.figure(figsize=(12, 8))\nplt.scatter(train_df['Store'],train_df['Sales'], alpha=0.1);\nplt.plot()","metadata":{"id":"935aeca1","outputId":"e4708ddc-d303-444b-c4bc-833b1c5e4ce8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can see that only several stores have sales more than 20000, and there are quite a lot of 0 sales days. This may be an indication to the days when the stores were closed. Deleting these points later would be better for forcasting well.","metadata":{"id":"eb271b03"}},{"cell_type":"code","source":"#Sales_Customers\nattributes = [\"Sales\", \"Customers\"]\nscatter_matrix(train_df[attributes], alpha=0.1);","metadata":{"id":"a3e6d2ff","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Obviously, this feature is highly correlated to the target and may be a powerful predictor, but it is not in the test set, so we cannot use it except in making clusters or aggregations. ","metadata":{"id":"bd2d5cfc"}},{"cell_type":"code","source":"#Sales_CompetitionDistance\nattributes = [\"Sales\", \"CompetitionDistance\"]\nscatter_matrix(train_df[attributes], alpha=0.1);","metadata":{"id":"e35ea16e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Well, it looks like the further stores distant from each other, the lower sales they get. This maybe due to that people tend to go to places where several same stores setting next to each other in order to get best prices.\n#### Also, several similar stores setting to each other may mean that their place is a big market within the city while that distant stores may mean that they are just small retailing stores.\n","metadata":{"id":"7dabecae"}},{"cell_type":"code","source":"#Sales_Open\nsns.set()\nplt.hist(train_df[train_df['Open'] == 1].Sales, color='green', alpha=0.3, label = 'YES')\nplt.hist(train_df[train_df['Open'] == 0].Sales, color='red', alpha=0.3, label = 'NO')\nplt.legend()\nplt.plot()","metadata":{"id":"7a6cd215","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Nothing's strange. As that no red bars appeares, that does mean the store doesn't have any sales when the store is closed which is reasonable.\n#### We can use this piece of information to fill some of the test set predictions manually.","metadata":{"id":"8707997c"}},{"cell_type":"code","source":"#Sales_Promo\nsns.set()\nplt.hist(train_df[train_df['Promo'] == 1].Sales, color='green', alpha=0.3, label = 'YES')\nplt.hist(train_df[train_df['Promo'] == 0].Sales, color='red', alpha=0.3, label = 'NO')\nplt.legend()\nplt.plot()","metadata":{"id":"84454a64","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sales_Promo2\nsns.set()\nplt.hist(train_df[train_df['Promo2'] == 1].Sales, color='green', alpha=0.3, label = 'YES')\nplt.hist(train_df[train_df['Promo2'] == 0].Sales, color='red', alpha=0.3, label = 'NO')\nplt.legend()\nplt.plot()","metadata":{"id":"fdb6cf49","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It looks like stores with continuing and consecutive promotions doesn't get significantly better sales than stores with indiviual promotions. In fact, it looks like the opposite is true.","metadata":{"id":"c2138f2f"}},{"cell_type":"code","source":"#Sales_DayOfWeek\nplt.figure(figsize=(12, 8))\nplt.scatter(train_df[\"DayOfWeek\"],train_df[\"Sales\"])\nplt.plot()","metadata":{"id":"7dc3d19c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sales_Month\nplt.figure(figsize=(12, 8))\nplt.scatter(train_df[\"Month\"],train_df[\"Sales\"])\nplt.plot()","metadata":{"id":"8c14dc07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### High sales in April, May, June and December\n#### Low sales in January and September","metadata":{"id":"d8cadb63"}},{"cell_type":"code","source":"#Sales_StoreType\nsns.set()\nplt.hist(train_df[train_df['StoreType'] == 'a'].Sales, color='green', alpha=0.3, label = 'a')\nplt.hist(train_df[train_df['StoreType'] == 'b'].Sales, color='red', alpha=0.3, label = 'b')\nplt.hist(train_df[train_df['StoreType'] == 'c'].Sales, color='yellow', alpha=0.3, label = 'c')\nplt.hist(train_df[train_df['StoreType'] == 'd'].Sales, color='blue', alpha=0.3, label = 'd')\nplt.legend()\nplt.plot()","metadata":{"id":"3387a3d8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stores with type 'b' look like having greater mean sales than the others. ","metadata":{"id":"121270d1"}},{"cell_type":"code","source":"#Sales_Assortment\nsns.set()\nplt.hist(train_df[train_df['Assortment'] == 'a'].Sales, color='green', alpha=0.3, label = 'a')\nplt.hist(train_df[train_df['Assortment'] == 'b'].Sales, color='red', alpha=0.3, label = 'b')\nplt.hist(train_df[train_df['Assortment'] == 'c'].Sales, color='yellow', alpha=0.3, label = 'c')\nplt.legend()\nplt.plot()","metadata":{"id":"b5e608fa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stores with Assortment 'b' look like having much less maximum sales than the others. ","metadata":{"id":"43512d7f"}},{"cell_type":"code","source":"#Open_DayOfWeek\nsns.countplot( x='DayOfWeek', data=train_df, hue=\"Open\", palette=\"Set1\");","metadata":{"id":"824a6802","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Almost all the stores are closed at the weekend.\n#### As that the data has been taken from European Countries, it is safe to say that number 7 represent Sunday.","metadata":{"id":"f25ee57d"}},{"cell_type":"code","source":"#PromoInterval\ntrain_df['PromoInterval'].hist()","metadata":{"id":"5280f97f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clearly, most of the stores prefer the Jan,Apr,Jul,Oct Promo interval. ","metadata":{"id":"828f8415"}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{"id":"0de8cf90"}},{"cell_type":"markdown","source":"# Adding Aggregations","metadata":{"id":"64991048"}},{"cell_type":"code","source":"# Sales_per_day, Customers_per_day, avg_sales_per_customer and Sales_per_customers_per_day\n\n# Get total sales, customers and open days per store\nstore_data_sales = train_df.groupby([train_df['Store']])['Sales'].sum()\nstore_data_customers = train_df.groupby([train_df['Store']])['Customers'].sum()\nstore_data_avg_sales = train_df.groupby([train_df['Store']])['Sales'].mean()\nstore_data_avg_customers = train_df.groupby([train_df['Store']])['Customers'].mean()\nstore_data_open = train_df.groupby([train_df['Store']])['Open'].count()\n\n# Calculate sales per day, customers per day and sales per customers per day\nstore_data_sales_per_day = store_data_sales / store_data_open\nstore_data_customers_per_day = store_data_customers / store_data_open\nstore_data_avg_sales_per_customer = store_data_avg_sales / store_data_avg_customers\nstore_data_sales_per_customer_per_day = store_data_sales_per_day / store_data_customers_per_day\n\n#Saving the above values in a dictionary so that they can be mapped to the dataframe.\nsales_per_day_dict = dict(store_data_sales_per_day)\ncustomers_per_day_dict = dict(store_data_customers_per_day)\navg_sales_per_customer_dict = dict(store_data_avg_sales_per_customer)\nsales_per_customers_per_day_dict = dict(store_data_sales_per_customer_per_day)\n\n\n\ntrain_df['SalesPerDay'] = train_df['Store'].map(sales_per_day_dict)\ntrain_df['Customers_per_day'] = train_df['Store'].map(customers_per_day_dict)\ntrain_df['Avg_Sales_per_Customer'] = train_df['Store'].map(avg_sales_per_customer_dict)\ntrain_df['Sales_Per_Customers_Per_Day'] = train_df['Store'].map(sales_per_customers_per_day_dict)\n\ntest_df['Sales_per_day'] = test_df['Store'].map(sales_per_day_dict)\ntest_df['Customers_per_day'] = test_df['Store'].map(customers_per_day_dict)\ntest_df['Avg_Sales_per_Customer'] = test_df['Store'].map(avg_sales_per_customer_dict)\ntest_df['Sales_Per_Customers_Per_Day'] = test_df['Store'].map(sales_per_customers_per_day_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Foureier Frequnecies and Amplitudes","metadata":{}},{"cell_type":"code","source":"freq2_dict_no_log = dict()\nfreq3_dict_no_log = dict()\n\namp2_dict_no_log = dict()\namp3_dict_no_log = dict()\n\nfor feat_1 in ('Year','Month'):\n        for i in range(min(train_df[feat_1].unique()), max(train_df[feat_1].unique()) + 1):\n\n            a = train_df.loc[train_df[feat_1]==i]\n            a_sales = a['Sales']\n\n            Y = np.fft.fft(a_sales.values)\n            Y = abs(Y)\n            freq = np.fft.fftfreq(len(Y), 1)\n\n            intercept_index = np.argmax(Y)\n            Y = np.delete(Y, intercept_index)\n            freq = np.delete(freq, intercept_index)\n\n            amplitude_1_index = np.argmax(Y)\n            amplitude_1 = Y[amplitude_1_index]\n            Y = np.delete(Y, amplitude_1_index)\n            freq_1 = freq[amplitude_1_index]\n            freq = np.delete(freq, amplitude_1_index)\n\n            amplitude_2_index = np.argmax(Y)\n            amplitude_2 = Y[amplitude_2_index]\n            Y = np.delete(Y, amplitude_2_index)\n            freq_2 = freq[amplitude_2_index]\n            freq = np.delete(freq, amplitude_2_index)\n\n            amplitude_3_index = np.argmax(Y)\n            amplitude_3 = Y[amplitude_3_index]\n            Y = np.delete(Y, amplitude_3_index)\n            freq_3 = freq[amplitude_3_index]\n            freq = np.delete(freq, amplitude_3_index)\n            \n\n            #Freq_1 is not included because it seems as it is always 0\n            a[f'Frequency_2_{feat_1}_Sales'] = freq_2\n            a[f'Frequency_3_{feat_1}_Sales'] = freq_3\n\n            a[f'Amplitude_2_{feat_1}_Sales'] = amplitude_2\n            a[f'Amplitude_3_{feat_1}_Sales'] = amplitude_3\n\n\n            freq2_dict_no_log[i] = freq_2\n            freq3_dict_no_log[i] = freq_3\n\n            amp2_dict_no_log[i] = amplitude_2\n            amp3_dict_no_log[i] = amplitude_3\n\n\n            if i == min(train_df[feat_1].unique()):\n                k = a\n            else:\n                k = pd.concat([k,a])\n        train_df = k\n        test_df[f'Frequency_2_{feat_1}_Sales'] = test_df[feat_1].map(freq2_dict_no_log)\n        test_df[f'Frequency_3_{feat_1}_Sales'] = test_df[feat_1].map(freq3_dict_no_log)\n        test_df[f'Amplitude_2_{feat_1}_Sales'] = test_df[feat_1].map(amp2_dict_no_log)\n        test_df[f'Amplitude_3_{feat_1}_Sales'] = test_df[feat_1].map(amp3_dict_no_log)\n        freq2_dict_no_log = dict()\n        freq3_dict_no_log = dict()\n        amp2_dict_no_log = dict()\n        amp3_dict_no_log = dict()\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting \"CompetitionOpenSinceYear/Month\" to Milliseconds","metadata":{}},{"cell_type":"code","source":"feats = ['CompetitionOpenSinceMonth','CompetitionOpenSinceYear']\nmodes = train_df[feats].mode()\n\nfor f in feats:\n        train_df[f] = train_df[f].fillna(modes[f][0])\n        test_df[f] = test_df[f].fillna(modes[f][0])\n\n#---------------------------------------------------------------------------------------------------------------- \ndef convertCompetitionOpen(df):\n    try:\n        date = '{}-{}'.format(int(df['CompetitionOpenSinceYear']), int(df['CompetitionOpenSinceMonth']))\n        return pd.to_datetime(date)\n    except:\n        return np.nan\n\ntrain_df['CompetitionOpenInt'] = train_df.apply(lambda df: convertCompetitionOpen(df), axis=1).astype(np.int64)\ntest_df['CompetitionOpenInt'] = test_df.apply(lambda df: convertCompetitionOpen(df), axis=1).astype(np.int64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop 'Customers' and 'StateHoliday' Columns","metadata":{"id":"abe1bf8f"}},{"cell_type":"code","source":"train_df.drop('Customers',inplace = True, axis=1)  #Because it is not in the test set","metadata":{"id":"0791d108","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.drop('StateHoliday',inplace=True,axis=1)  #Because it reduces the performance\ntest_df.drop('StateHoliday',inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Return to the original order","metadata":{}},{"cell_type":"code","source":"train_df.sort_values([\"Store\"], ignore_index=True, inplace=True)\ntest_df.sort_values([\"Store\"], ignore_index=True, inplace=True)\ntrain_df.sort_values([\"Year\",\"Month\",\"Day\"], ascending=False ,ignore_index=True, inplace=True)\ntest_df.sort_values([\"Year\",\"Month\",\"Day\"], ascending=False ,ignore_index=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{"id":"f46e388c"}},{"cell_type":"markdown","source":"## Missing Values","metadata":{"id":"9593a818"}},{"cell_type":"code","source":"#missing data percentage (Training)\ntotal = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data","metadata":{"id":"5de84649","outputId":"f66e777d-cf63-4167-f79e-292c284e74a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#missing data percentage (Testing)\ntotal = test_df.isnull().sum().sort_values(ascending=False)\npercent_1 = test_df.isnull().sum()/test_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data","metadata":{"id":"2f23d16d","outputId":"9971595f-2ece-4a6d-a55d-f64b019ca9c6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feats = ['Promo2SinceYear','Promo2SinceWeek','CompetitionDistance', 'PromoInterval']\nmodes = train_df[feats].mode()\n\nfor f in feats:\n        train_df[f] = train_df[f].fillna(modes[f][0])\n        test_df[f] = test_df[f].fillna(modes[f][0])\nfor dataset in (train_df,test_df):\n    dataset['Open'] = dataset['Open'].fillna(0)","metadata":{"id":"85983e90","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove Outliers","metadata":{"id":"e7fc780c"}},{"cell_type":"code","source":"train_df = train_df[train_df['Sales'] < 25000]  #Drops samples which have sales more than 25000\ntrain_df.reset_index(drop=True)","metadata":{"id":"e2606b45","outputId":"a1a8f921-a982-4c10-b36d-a138034eef3a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding","metadata":{"id":"1cc59d3a"}},{"cell_type":"markdown","source":"## Label Encoder","metadata":{"id":"8cee2f61"}},{"cell_type":"code","source":"attributes = ['StoreType','Assortment','PromoInterval']\nfor dataset in (train_df,test_df):\n    for f in attributes:\n        dataset[attributes] = dataset[attributes].apply(lambda x: pd.factorize(x)[0])","metadata":{"id":"48be25b5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Only Use non-zero Sales Samples For Training","metadata":{}},{"cell_type":"code","source":"train_df = train_df[train_df['Open'] == 1]\ntrain_df = train_df[train_df['Sales'] > 0.0]","metadata":{"id":"e13bb50b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Correlation with The Target ","metadata":{}},{"cell_type":"code","source":"corr_matrix = train_df.corr()\ncorr_matrix[\"Sales\"].sort_values(ascending=False)","metadata":{"id":"54571b20","outputId":"737c392b-d659-4522-e130-94bf89a35988","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML Modeling","metadata":{"id":"3105b273"}},{"cell_type":"markdown","source":"## Initialize The Model ","metadata":{}},{"cell_type":"code","source":"params = {'n_estimators': 1742,\n          'min_child_samples': 89,\n          'n_jobs':-1,\n          'learning_rate': 0.2723,\n          'max_depth': -1,  \n          'subsample': 1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 0.1,\n          'reg_lambda': 1,\n          'verbosity': -1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LightGBM Regressor\nlgbm = LGBMRegressor(**params, random_state=42)","metadata":{"id":"a2d7474d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation","metadata":{"id":"43b8dce2"}},{"cell_type":"code","source":"#Sort the values to use the last days as a validation set\ntemp = train_df.sort_values([\"Year\",\"Month\",\"Day\"], ignore_index=True).copy()\n\n#The same time period as the test set\ntrain = temp[:-47000].copy()       \nvali = temp[-47000:].copy()\n\n#Return to the same order as the original dataset\ntrain.sort_values([\"Store\"], ignore_index=True, inplace=True)\nvali.sort_values([\"Store\"], ignore_index=True, inplace=True)\ntrain.sort_values([\"Year\",\"Month\",\"Day\"], ascending=False ,ignore_index=True, inplace=True)\nvali.sort_values([\"Year\",\"Month\",\"Day\"], ascending=False ,ignore_index=True, inplace=True)\n\n#Get the target\ny_test = vali['Sales'].copy()\nvali.drop('Sales',inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fit the Model\nlgbm.fit(train.drop('Sales',axis=1),train['Sales'])\ny_pred = lgbm.predict(vali)\n\n#Use a Correction Factor Because we transformed the target with log(x+1) then reversed it\ny_pred = y_pred*0.995\nscore = RMSPE(y_test,y_pred)\nscore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Show the Features Importances","metadata":{}},{"cell_type":"code","source":"plotImp(lgbm,train.drop('Sales',axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Final Model ","metadata":{}},{"cell_type":"markdown","source":"## Prepare the Datasets","metadata":{}},{"cell_type":"markdown","source":"### We will fit the models with two datasets:","metadata":{}},{"cell_type":"markdown","source":"### 1- The Full Training Set ","metadata":{}},{"cell_type":"code","source":"X = train_df.drop('Sales',axis=1)\ny = train_df['Sales']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2- A Dataset Consists of Only the Data Between May and September from All The Years","metadata":{}},{"cell_type":"code","source":"X_MaySept = train_df[(train_df['Month'] >= 5) & (train_df['Month'] <= 9)]\nX_MaySept.reset_index(drop=True)\ny_MaySept = X_MaySept['Sales']\nX_MaySept = X_MaySept.drop('Sales',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models: Layer 1","metadata":{}},{"cell_type":"code","source":"Predictions = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1- Averaging 15 Models With The Same Hyperparameters but Changing The Seed. ","metadata":{}},{"cell_type":"markdown","source":"### Feed Them with The Full Dataset and Get the Mean, Harmonic Mean and Geometric Mean of The Predictions.","metadata":{}},{"cell_type":"code","source":"for seed in range(30,46):\n    lgbm = LGBMRegressor(**params, random_state=seed)\n    #Transform the target with log(x+1) to make the model able to optimize the loss function properly\n    llgbm = TransformedTargetRegressor(lgbm, func = np.log1p, inverse_func = np.expm1)\n    llgbm.fit(X, y)\n\n    Predictions[f'Sales_{seed}'] = llgbm.predict(test_df)\n    Predictions[f'Sales_{seed}'] = Predictions[f'Sales_{seed}'] * 0.995 #Multiply by a Correction Factor\n\nPredictions['Mean'] = Predictions.mean(axis=1)\nPredictions['HMean'] = Predictions.apply(stats.hmean, axis=1)\nPredictions['GMean'] = Predictions.apply(stats.gmean, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2- Averaging 15 Models With The Same Hyperparameters but Changing The Seed. ","metadata":{}},{"cell_type":"markdown","source":"### Feed Them with The MaySeptember Dataset and Get the Mean, Harmonic Mean and Geometric Mean of The Predictions.","metadata":{}},{"cell_type":"code","source":"for seed in range(30,46):\n    lgbm = LGBMRegressor(**params, random_state=seed)\n    #Transform the target with log(x+1) to make the model able to optimize the loss function properly\n    llgbm = TransformedTargetRegressor(lgbm, func = np.log1p, inverse_func = np.expm1)\n    llgbm.fit(X_MaySept, y_MaySept)\n\n    Predictions[f'Sales_2_{seed}'] = llgbm.predict(test_df)\n    Predictions[f'Sales_2_{seed}'] = Predictions[f'Sales_2_{seed}'] * 0.995\n\nPredictions['Mean_2'] = Predictions.mean(axis=1)\nPredictions['HMean_2'] = Predictions.apply(stats.hmean, axis=1)\nPredictions['GMean_2'] = Predictions.apply(stats.gmean, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models: Layer 2","metadata":{}},{"cell_type":"markdown","source":"### Get The Harmonic Mean of The Six Predictions ","metadata":{}},{"cell_type":"code","source":"FinalPred = Predictions[['Mean','HMean','GMean','Mean_2','HMean_2','GMean_2']].apply(stats.hmean,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models: Make The Submission File","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\"Id\": ID ,\"Sales\": FinalPred.values})\nsubmission.to_csv('FinalSubmission.csv',index=False) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Public Leaderboard: 0.10448\n# Private Leaderboard: 0.11323","metadata":{}}]}