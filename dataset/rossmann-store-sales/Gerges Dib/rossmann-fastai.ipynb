{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rossmann - Fastai\n\nThis notebook is based on the code from [Fast.ai Machine learning class lecture 12](https://course18.fast.ai/lessonsml1/lesson12.html). The class lecture basically reviews the third place winner solution described [here](https://www.kaggle.com/c/rossmann-store-sales/discussion/17974).[](http://)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport numpy as np\nimport pandas as pd\nfrom os.path import splitext, join\nfrom IPython.display import HTML, display\nimport os\nimport re\nimport time\nfrom isoweek import Week\nfrom functools import partial\n\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Embedding, concatenate, Flatten, Dropout\n\npd.set_option('display.max_columns', 1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data\n\nThe competition data contains information about historical sales from 1115 Rossmann stores. \n- The training data contains historical sales (and number of customers) from January 1, 2013 till July 31, 2015.\n- The test data, we need to estimate sales for the period betwee August 1, 2015 to September 17, 2015. The Promo information is provided for this \"future\" period, and also State/School holiday information. \n\nBefore reading the data, we need to add few more data files which have been contributed by the community (all data can be downloaded from [here](http://files.fast.ai/part2/lesson14/rossmann.tgz). Those include four files, in addition to the ones provided by the competition: \n1. *googletrend.csv*: A google trend metric for a particular week, and particular state in Germany. \n2. *weather.csv*: Various weather information for the period from training data start till test data end.\n3. *store_states.csv*: The state location of each store ID. I would like to know how did they get that!\n4. *state_names.csv*: A mapping from full state names to their abbreviation.\n\nFirst, walk through the data folder and reads all csv files as pandas dataframes and stores them all into a dictionary.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data = {}\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        full_name = os.path.join(dirname, filename)\n        key = splitext(filename)[0]        \n        data[key] = pd.read_csv(full_name, low_memory=False)\n    \ndisplay(f'Loaded file keys: {list(data.keys())}', )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean and merge tables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Define a function which takes a `datetime` column, and extract date parts from it. There is a default of date parts to extract, but we can also pass a subset of those dateparts to extract.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_date_parts(df, date_column, parts=None, prefix=None):\n    \"\"\"\n    Add date information to the dataframe inplace.\n    \"\"\"\n    prefix = prefix or ''\n    if parts is None:\n        parts = [\n            'Year',\n            'Month',\n            'Week',\n            'Day',\n            'Dayofweek',\n            'Dayofyear',\n            'Is_month_end',\n            'Is_month_start',\n            'Is_quarter_end',\n            'Is_quarter_start',\n            'Is_year_end',\n            'Is_year_start',\n            'Elapsed'\n        ]\n    if not np.issubdtype(df[date_column].dtype, np.datetime64):\n        df[date_column] = pd.to_datetime(df[date_column], infer_datetime_format=True)\n    \n    s = df[date_column]\n    for part in parts:\n        if part == 'Week':\n            df[prefix + part] = s.dt.isocalendar()['week']\n        elif part == 'Elapsed':\n            df[prefix + part] = s.astype(np.int64) // 10 ** 9\n        else:\n            df[prefix + part] = getattr(s.dt, part.lower())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The google trend table columns contain information about date and state name, but needs to be extracted from them. In addition, we fix the name of one state to be consistent with the other tables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"googletrend = data['googletrend']\ngoogletrend['Date'] = pd.to_datetime(googletrend.week.str.split(' - ', expand=True)[0])\ngoogletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\ngoogletrend.loc[googletrend.State=='NI', 'State'] = 'HB,NI'\nadd_date_parts(googletrend, 'Date', parts=['Year', 'Week'])\ngoogletrend.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Google trends data has a special category for whole of Germany (instead of state-wise information). We will extract this to join it by date to each row in the train/test datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_de = googletrend[googletrend.file == 'Rossmann_DE'][['trend', 'Year', 'Week']]\ntrend_de.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we join the weather information table with the state name mapping, and also, we add year and week information to be able to join with google trends.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"weather = data['weather'].merge(data['state_names'], how='left', left_on='file', right_on='StateName').drop('file', axis=1)\nadd_date_parts(weather, 'Date', parts=['Year', 'Week'])\nweather.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we merge the `store` table with the `store_states` table to add state information to that table for merging with train/test datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store = data['store'].merge(data['store_states'], how='left', on='Store')\nstore.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add all date parts information to the train/test datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"add_date_parts(data['train'], 'Date')\nadd_date_parts(data['test'], 'Date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can merge all the support tables: `store`, `googletrend`, `trend_de`, and `weather` with train/test into a single table.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_all(df):\n    out = (df\n        .merge(store, how='left', on='Store')\n        .merge(googletrend, how='left', on=['State', 'Year', 'Week'], suffixes=('', '_y'))\n        .merge(trend_de, how='left', on=['Year', 'Week'], suffixes=('', '_DE'))\n        .merge(weather, how='left', on=['State', 'Date'], suffixes=('', '_y'))\n    )\n    # Drop replicated columns for right merged tables and a couple unwanted ones\n    drop_cols = list(out.columns[out.columns.str.endswith('_y')]) + ['week', 'file']\n    out.drop(drop_cols, inplace=True, axis=1)\n    \n    # Check if the merge resulted in any new nulls\n    print('Merge has nulls:', any([\n        any(out.StoreType.isnull()),\n        any(out.trend.isnull()),\n        any(out.trend_DE.isnull()),\n        any(out.Mean_TemperatureC.isnull()),\n    ]))\n    return out\n\ntrain = merge_all(data['train'])\ntest = merge_all(data['test'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\n\nWe will need to do the same processing steps for both train and test data.The following function adds the these features:\n1. `CompetitionMonthsOpen`: The number of months the nearest competitor has opened since the current row's date since. This is clipped between 0 and 24 months.\n1. `Promo2Weeks`: The number of weeks since the last/next continuing promotion.\n1. `{Before, After}{SchoolHoliday, StateHoliday, Promo}`: The number of days since the last event before it or first event after it.\n1. `{SchoolHoliday, StateHoliday, Promo}{_bw, _fw}`: The total number of each event within the last week.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"%%time\n\ndef add_features(df):\n    df = df.copy()\n    # Convert StateHoliday to a flag (0/1) value instead of a categorical one\n    df.StateHoliday = (df.StateHoliday != '0').astype(int)\n    \n    # Add the number of months a competition has been open\n    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n    df['CompetitionOpenSince'] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, month=df.CompetitionOpenSinceMonth, day=15))\n    df['CompetitionDaysOpen'] = df.Date.subtract(df.CompetitionOpenSince).dt.days\n    df.loc[df.CompetitionDaysOpen < 0, \"CompetitionDaysOpen\"] = 0\n    df.loc[df.CompetitionOpenSinceYear < 1990, \"CompetitionDaysOpen\"] = 0\n    df['CompetitionMonthsOpen'] = (df.CompetitionDaysOpen//30).clip(0, 24)\n    \n    # Add the number of weeks since promotion \n    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)\n    df['Promo2Since'] = pd.to_datetime([\n        Week(y[0], int(y[1])).monday() for y in df[['Promo2SinceYear', 'Promo2SinceWeek']].values\n    ])\n    df['Promo2Days'] = df.Date.subtract(df.Promo2Since).dt.days\n    df.loc[df.Promo2Days < 0, 'Promo2Days'] = 0\n    df['Promo2Weeks'] = (df['Promo2Days']//7).clip(0, 25)\n    \n    # Add elapsed time since and to next of the following flags\n    columns = ['SchoolHoliday', 'StateHoliday', 'Promo']\n\n    df.sort_values(['Store', 'Date'], inplace=True)\n    for column in columns:\n        mask = df[column] == 1    \n        for name, method in zip([f'After{column}', f'Before{column}'], ['ffill', 'bfill']):\n            df.loc[mask, name] = df.loc[mask, 'Date']\n            df[name] = df.groupby('Store')[name].fillna(method=method)\n            df[name] = (df.Date - df[name]).dt.days.fillna(0).astype(int)\n        \n    # Set the active index to Date, so we can do rolling sums\n    df.set_index('Date', inplace=True)\n    \n    # We will sum total number of the following in last/next week\n    bw = df.sort_index().groupby('Store')[columns].rolling(7, min_periods=1).sum()    \n    fw = df.sort_index(ascending=False).groupby('Store')[columns].rolling(7, min_periods=1).sum()\n    df = (\n        df\n        .merge(bw, how='left', on=['Store', 'Date'], suffixes=['', '_bw'])\n        .merge(fw, how='left', on=['Store', 'Date'], suffixes=['', '_fw'])\n    )\n    return df.reset_index()\n\ntrain = add_features(train)\ntest = add_features(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is time to checkpoint our data work.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.reset_index().to_feather('/kaggle/working/train')\ntest.to_feather('/kaggle/working/test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train/Val data preparation\n\nRe-read data if not done yet.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_feather('/kaggle/working/train')\ntest = pd.read_feather('/kaggle/working/test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have all the required features, it is time to prepare the data for training and evaluation. We will manually select and distinguish the categorical and continuous features to keep.\n\nAlso, following what the original authors have done, we remove all columns in the training dataset which have Sales equal to 0 (indicating days which store is closed, e.g. for renovation). This probably because metric does not support 0 labels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = [\n    'Store','DayOfWeek', 'Year', 'Month', \n    'Day', 'StateHoliday', 'CompetitionMonthsOpen','Promo2Weeks',\n    'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear',\n    'Promo2SinceYear', 'State', 'Week', 'Events',\n    'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n]\ncontinuous_cols = [\n    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n]\ntarget = 'Sales'\n\nall_cols = categorical_cols + continuous_cols\n\ntrain = train[train.Sales > 0]\n\n# Select those specified columns\ntrain = train[all_cols + [target, 'Date']]\ntest = test[all_cols + ['Date', 'Id']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the categorical columns, convert them first to pandas `categorical` column `dtype` and then use the numerical codes of those categories to get their integer values. Note that since some columns have nulls, that automatically is mapped to -1 in pandas. We add 1 so nulls will always have a 0 value.\n\nFor the continuous columns, fill out nulls with zeros, convert `dtype` to 32-bits float, and then do standard normalization of the columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical columns to numbers\nfor c in categorical_cols:    \n    train[c] = train[c].astype('category').cat.as_ordered()\n    test[c] = test[c].astype('category').cat.as_ordered()\n    test[c].cat.set_categories(train[c].cat.categories, ordered=True, inplace=True)\n    \n    train[c] = train[c].cat.codes + 1\n    test[c] = test[c].cat.codes + 1\n    \nfor c in continuous_cols:\n    train[c] = train[c].fillna(0).astype('float32')\n    test[c] = test[c].fillna(0).astype('float32')\n\nscaler = StandardScaler()\ntrain[continuous_cols] = scaler.fit_transform(train[continuous_cols])\ntest[continuous_cols] = scaler.transform(test[continuous_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling with deep learning\n\nThe following will define functions to generate the required train/valdation data and the model itself.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(has_validation=True):\n    # train here access a global value, copies it and stores it locally (to the function)\n    data = train.copy().set_index('Date')\n    X = data[all_cols]\n    y = np.log(data[target])\n    y_max = y.max()\n    y_min = y.min()\n    \n    if has_validation:        \n        split_date = '2015-06-15'\n        val_split_date = '2015-06-16'\n\n        X_train, X_val = X.loc[:split_date], X.loc[val_split_date:]\n\n        # Now, convert the training data into a list \n        X_train = [X_train[continuous_cols].values] + [X_train[c].values[..., None] for c in categorical_cols]\n        X_val = [X_val[continuous_cols].values] + [X_val[c].values[..., None] for c in categorical_cols]\n\n        # Get the labels\n        y_train, y_val = y.loc[:split_date].values, y.loc[val_split_date:].values\n        \n        y_train = (y_train - y_min)/(y_max - y_min)\n        y_val = (y_val - y_min)/(y_max - y_min)\n        return y_max, y_min, X_train, y_train, X_val, y_val\n    else:\n        X_train = [X[continuous_cols].values] + [X[c].values[..., None] for c in categorical_cols]\n        y_train = y.values\n        y_train = (y_train - y_min)/(y_max - y_min)\n        return y_max, y_min, X_train, y_train\n\n\ndef get_rmspe(y_min, y_max):\n    def rmspe(y_true, y_pred):\n        y_true = tf.math.exp(y_true * (y_max - y_min) + y_min)\n        y_pred = tf.math.exp(y_pred * (y_max - y_min) + y_min)\n        return tf.math.sqrt(tf.reduce_mean(tf.square((y_true - y_pred)/y_true)))\n    return rmspe\n\n\ndef get_model(y_min, y_max, learning_rate=1e-3, dropout=0.1):\n    # get the embedding sizes\n    embedding_map = []\n    cardinalities = list(train[categorical_cols].nunique().values+1)\n    for name, cardinality in zip(categorical_cols, cardinalities):\n        embedding_map.append({'cardinality': cardinality, 'size': min(50, (cardinality+1)//2)})\n\n    # Define the neural network\n    keras.backend.clear_session()\n    inputs = [keras.Input(shape=(X_train[0].shape[1],))] + [keras.Input(shape=(1,)) for _ in categorical_cols]\n    outputs = [inputs[0]]\n    for cat_input, embedding, name in zip(inputs[1:], embedding_map, categorical_cols):\n        out = Embedding(embedding['cardinality'], embedding['size'], input_length=1, name=name)(cat_input)\n        outputs.append(Flatten()(out))   \n    output = concatenate(outputs)\n    output = Dense(1024, activation='relu')(output)\n    output = Dropout(dropout)(output)\n    output = Dense(512, activation='relu')(output)\n    output = Dropout(dropout)(output)\n    output = Dense(1, activation='sigmoid')(output)\n\n    model = keras.Model(inputs=inputs, outputs=output)\n    model.compile(\n        loss='mean_absolute_error', \n        optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n        metrics=[get_rmspe(y_min, y_max)]\n    )\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we train the model using a validation set so that we select the best hyperparameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def schedule(epoch):\n    if epoch < 20:\n        return 1e-3\n    else:\n        return 5e-4\n    \ncallbacks = [keras.callbacks.LearningRateScheduler(schedule, verbose=0)]\n\nepochs = 30\nbatch_size = 256\nlearning_rate = 2e-3\ndropout = 0.05\n\ny_max, y_min, X_train, y_train, X_val, y_val = get_data(has_validation=True)\n\nmodel = get_model(y_min, y_max, learning_rate, dropout)\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we train the model with all the data so we have access to the most recent data, which might be important to get good score using the test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 30\nbatch_size = 256\nlearning_rate = 1e-3\ndropout = 0.05\n\ny_max, y_min, X_train, y_train = get_data(has_validation=False)\nmodel = get_model(y_min, y_max, learning_rate, dropout)\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict and submit\n\nWe use the final trained model to predict the test dataset. Note that since the model only predicts values between 0 and 1, we must rescale it with the maximum value of Sales in the training set (initially normalized to keep values between 0 and 1).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.set_index('Date')\nX = test[all_cols]\nids = test.Id\n\n# Now, convert the data into a list \nX_test = [X[continuous_cols].values] + [X[c].values[..., None] for c in categorical_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.exp(model.predict(X_test) * (y_max - y_min) + y_min)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame({'Id': ids, 'Sales': y_pred.squeeze()}).sort_values('Id')\nsubmit.to_csv('/kaggle/working/submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}