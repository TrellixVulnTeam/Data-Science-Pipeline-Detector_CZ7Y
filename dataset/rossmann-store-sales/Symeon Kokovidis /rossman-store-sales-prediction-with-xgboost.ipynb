{"cells":[{"metadata":{"_uuid":"c4c5aa8ff03d163b87245f9307cb48d2d39027ca"},"cell_type":"markdown","source":"# Rossmann Store Sales\n\n### Test challenge\n##### Goal: Explore data and predict 6 weeks of daily sales for 1,115 stores located across Germany. "},{"metadata":{"_uuid":"a0dab32e807c413fb29f7c3dd717981b48bf0b67"},"cell_type":"markdown","source":"This notebook mainly focuses on the Time Series Analysis, a topic not covered at Rossmann Competition Kernels. We then disscuss advantages and drawbacks of modeling with Seasonal ARIMA and Prophet.\n\nAs it usually goes, we start with the Exploratory Data Analysis of the main metrics revealing present trends and patterns in the data, giving a solid foundation for the further causal analysis. \n\nAlso, alternatively to forecasting with Prophet, we use one of the most robust and sophisticated algorythm Extreme Gradient Boosting for regression."},{"metadata":{"_uuid":"ba96917c8b4f7ef878c9b70352ecbb7616c09473"},"cell_type":"markdown","source":"![\"shop\"](rossmann_banner2.png \"shop\")"},{"metadata":{"_uuid":"c3eef6e818d322fa5436bf16c856ed31f9c610b2"},"cell_type":"markdown","source":"---"},{"metadata":{"trusted":true,"_uuid":"eea161275ee0bc0af0638e5575fea8ef805643f4"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# loading packages\n# basic + dates \nimport numpy as np\nimport pandas as pd\nfrom pandas import datetime\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns # advanced vizs\n%matplotlib inline\n\n# statistics\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\n# time series analysis\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# prophet by Facebook\nfrom fbprophet import Prophet\n\n# machine learning: XGB\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom xgboost.sklearn import XGBRegressor # wrapper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3e1a7d0c3762ed7edf76de23a1b41ace7c6df52"},"cell_type":"code","source":"# importing train data to learn\ntrain = pd.read_csv(\"../input/train.csv\", \n                    parse_dates = True, low_memory = False, index_col = 'Date')\n\n# additional store data\nstore = pd.read_csv(\"../input/store.csv\", \n                    low_memory = False)\n# time series as indexes\ntrain.index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa1c298aa2edf4c5c226d826297e67fd8d65426e"},"cell_type":"markdown","source":"## Exploratory Data Analysis "},{"metadata":{"_uuid":"bb226f3f1429a5251f321211b18fe1a0710d9d84"},"cell_type":"markdown","source":"In this first section we go through the train and store data, handle missing values and create new features for further analysis."},{"metadata":{"trusted":true,"_uuid":"549ef7ffc4a32621065481cc8f230bc184055afe"},"cell_type":"code","source":"# first glance at the train set: head and tail\nprint(\"In total: \", train.shape)\ntrain.head(5).append(train.tail(5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbcdcaed118fb132d74864ff441e9d9d5edb0f6f"},"cell_type":"markdown","source":"Short description:\n- Sales: the turnover for any given day (target variable).\n- Customers: the number of customers on a given day.\n- Open: an indicator for whether the store was open: 0 = closed, 1 = open.\n- Promo: indicates whether a store is running a promo on that day.\n- StateHoliday: indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. \n- SchoolHoliday: indicates if the (Store, Date) was affected by the closure of public schools."},{"metadata":{"_uuid":"0d5185cbc3cf97b296131a2ba3739c07501e70fc"},"cell_type":"markdown","source":"We are dealing with time series data so it will probably serve us to extract dates for further analysis. We also have two likely correlated vaiables in the dataset, which can be combined into a new feature."},{"metadata":{"trusted":true,"_uuid":"fa78ad2d9a63147254069e7264e611dc4fe583fa"},"cell_type":"code","source":"# data extraction\ntrain['Year'] = train.index.year\ntrain['Month'] = train.index.month\ntrain['Day'] = train.index.day\ntrain['WeekOfYear'] = train.index.weekofyear\n\n# adding new variable\ntrain['SalePerCustomer'] = train['Sales']/train['Customers']\ntrain['SalePerCustomer'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d42190a2f8173578da9a9c29c54e187b80e4524"},"cell_type":"markdown","source":"On average customers spend about 9.50$ per day. Though there are days with Sales equal to zero."},{"metadata":{"_uuid":"31e90490c6640fd40101aa8212f0fc2a9cef0430"},"cell_type":"markdown","source":"### ECDF: empirical cumulative distribution function"},{"metadata":{"_uuid":"18cdde9c8eb8af2c8504939331330bd86d6f11ae"},"cell_type":"markdown","source":"To get the first impression about continious variables in the data we can plot ECDF."},{"metadata":{"trusted":true,"_uuid":"ccbf4b678e2051f27666cc4db11bc8e67ad7c862"},"cell_type":"code","source":"sns.set(style = \"ticks\")# to format into seaborn \nc = '#386B7F' # basic color for plots\nplt.figure(figsize = (12, 6))\n\nplt.subplot(311)\ncdf = ECDF(train['Sales'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c);\nplt.xlabel('Sales'); plt.ylabel('ECDF');\n\n# plot second ECDF  \nplt.subplot(312)\ncdf = ECDF(train['Customers'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c);\nplt.xlabel('Customers');\n\n# plot second ECDF  \nplt.subplot(313)\ncdf = ECDF(train['SalePerCustomer'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c);\nplt.xlabel('Sale per Customer');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5dbfb62b88896e2e4cfab14d3e6e375460b118b"},"cell_type":"markdown","source":"About 20% of data has zero amount of sales/customers that we need to deal with and almost 80% of time daily amount of sales was less than 1000. So what about zero sales, is it only due to the fact that the store is closed?"},{"metadata":{"_uuid":"62e7b876fa88d164b277d55ee2c7f625b8604b8c"},"cell_type":"markdown","source":"### Missing values \n#### Closed stores and zero sales stores"},{"metadata":{"trusted":true,"_uuid":"bb7f955b45c67a6a4d302dca252dce86ac36f5a2"},"cell_type":"code","source":"# closed stores\ntrain[(train.Open == 0) & (train.Sales == 0)].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57ca999b903e89aee516e051c9c795240705ed23"},"cell_type":"markdown","source":"There're 172817 closed stores in the data. It is about 10% of the total amount of observations. To avoid any biased forecasts we will drop these values. \n\nWhat about opened stores with zero sales?"},{"metadata":{"trusted":true,"_uuid":"cf2c7f4cfcd48a33faecc661c6b5020dd2bd2b75"},"cell_type":"code","source":"# opened stores with zero sales\nzero_sales = train[(train.Open != 0) & (train.Sales == 0)]\nprint(\"In total: \", zero_sales.shape)\nzero_sales.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7798b0b1d0de09120c0e5a208cb671923a5962ba"},"cell_type":"markdown","source":"Interestingly enough, there are opened store with __no sales on working days__. There're only 54 days in the data, so we can assume that there were external factors involved, for example manifestations."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"624abfbc2846b717100581d02cf5c5977c6be6b9"},"cell_type":"code","source":"print(\"Closed stores and days which didn't have any sales won't be counted into the forecasts.\")\ntrain = train[(train[\"Open\"] != 0) & (train['Sales'] != 0)]\n\nprint(\"In total: \", train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bb7d50c912b1a046785e05ff9cf4151cc1d7e16"},"cell_type":"markdown","source":"What about store information:"},{"metadata":{"trusted":true,"_uuid":"eee971ff47c6f44f8f28a7df4a84fbca1c008805"},"cell_type":"code","source":"# additional information about the stores\nstore.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85c9fc264d3afe30112971eb22c449826e2869b1"},"cell_type":"markdown","source":"- Store: a unique Id for each store\n- StoreType: differentiates between 4 different store models: a, b, c, d\n- Assortment: describes an assortment level: a = basic, b = extra, c = extended\n- CompetitionDistance: distance in meters to the nearest competitor store\n- CompetitionOpenSince[Month/Year]: gives the approximate year and month of the time the nearest competitor was opened\n- Promo2: Promo2 is a continuing a promotion for some stores: 0 = store is not participating, 1 = store is participating\n- Promo2Since[Year/Week]: describes the year and calendar week when the store started participating in Promo2\n- PromoInterval: describes the consecutive intervals Promo2 is started, naming the months the promotion is started. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"},{"metadata":{"trusted":true,"_uuid":"2e3a888b7e87eaaed7570153340fcf9f09a9f6ae"},"cell_type":"code","source":"# missing values?\nstore.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a6960dbdf5666b833e86ca373e9796ffa947fb2"},"cell_type":"markdown","source":"We have few variables with missing values that we need to deal with. Let's start with the `CompetitionDistance`."},{"metadata":{"trusted":true,"_uuid":"e2830b8c35598bf5605e1ce1bfbcc7da54b4b159"},"cell_type":"code","source":"# missing values in CompetitionDistance\nstore[pd.isnull(store.CompetitionDistance)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f00a44f613c7cd32c91a9f7b2ff9a196ecc8c3c1"},"cell_type":"markdown","source":"Apperently this information is simply missing from the data. No particular pattern observed. In this case, it makes a complete sense to replace NaN with the median values (which is twice less that the average)."},{"metadata":{"trusted":true,"_uuid":"079779db69d145b72d38a08cffa473cf76218d06"},"cell_type":"code","source":"# fill NaN with a median value (skewed distribuion)\nstore['CompetitionDistance'].fillna(store['CompetitionDistance'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c20307dc9f9a7c612d09c5db761b6a24dcc8148a"},"cell_type":"markdown","source":"Continuing further with missing data. What about `Promo2SinceWeek`? May it be that we observe unsusual data points?"},{"metadata":{"trusted":true,"_uuid":"7c72497d95960c226513667463dc2fc272f218e3"},"cell_type":"code","source":"# no promo = no information about the promo?\n_ = store[pd.isnull(store.Promo2SinceWeek)]\n_[_.Promo2 != 0].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7144f17f86503114650f267b20e0364a8fcd08d0"},"cell_type":"markdown","source":"No, if there's no `Promo2` then there's no information about it. We can replace these values by zeros. The same goes for tha variables deducted from the competition, `CompetitionOpenSinceMonth` and `CompetitionOpenSinceYear`."},{"metadata":{"trusted":true,"_uuid":"ccc76bb88fb579902315976c0da70d7b484378da"},"cell_type":"code","source":"# replace NA's by 0\nstore.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01b4b322419285278aa156972b3904eb97c7c207"},"cell_type":"code","source":"print(\"Joining train set with an additional store information.\")\n\n# by specifying inner join we make sure that only those observations \n# that are present in both train and store sets are merged together\ntrain_store = pd.merge(train, store, how = 'inner', on = 'Store')\n\nprint(\"In total: \", train_store.shape)\ntrain_store.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"755322572d29ff375fd467ccecf951dda0df4543"},"cell_type":"markdown","source":"### Store types"},{"metadata":{"_uuid":"a34391f3c548da2e5dde59139d8ec5a54bd1115d"},"cell_type":"markdown","source":"In this section we will closely look at different levels of `StoreType` and how the main metric `Sales` is distributed among them.  "},{"metadata":{"trusted":true,"_uuid":"bd47bca3b667a7fe0b103e174b21c92f65e41898"},"cell_type":"code","source":"train_store.groupby('StoreType')['Sales'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a89e0eaa58c9d2626c7cfb7a40f25fa35bc1ed1c"},"cell_type":"markdown","source":"`StoreType` B has the highest average of Sales among all others, however we have much less data for it. So let's print an overall sum of `Sales` and `Customers` to see which `StoreType` is the most selling and crowded one:"},{"metadata":{"trusted":true,"_uuid":"812b6b9b30f5b6ca05faea445a70ebab9e54bc80"},"cell_type":"code","source":"train_store.groupby('StoreType')['Customers', 'Sales'].sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d79e88a391aaff19ea7b81da7ce22efa76a9cc1"},"cell_type":"markdown","source":"Clearly stores of type A. `StoreType` D goes on the second place in both `Sales` and `Customers`.\nWhat about date periods? Seaborn's facet grid is the best tool for this task:"},{"metadata":{"trusted":true,"_uuid":"70b8908dd6540c9357ede60d69f2211aa5d673e5"},"cell_type":"code","source":"# sales trends\nsns.factorplot(data = train_store, x = 'Month', y = \"Sales\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo', # per promo in the store in rows\n               color = c) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f35eb9574fab0851c5d5176a9f7cbb6ad795509"},"cell_type":"code","source":"# sales trends\nsns.factorplot(data = train_store, x = 'Month', y = \"Customers\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo', # per promo in the store in rows\n               color = c) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"029190e3a5b6b9a53f4d4cc58a04d74a8496b844"},"cell_type":"markdown","source":"All store types follow the same trend but at different scales depending on the presence of the (first) promotion `Promo` and `StoreType` itself (case for B).\n\n__Already at this point, we can see that Sales escalate towards Christmas holidays. But we'll talk about seasonalities and trends later in the Time Series Analysis section.__"},{"metadata":{"trusted":true,"_uuid":"6e644a02fbee2ba51448b6cebbd2004b831f5bff"},"cell_type":"code","source":"# sale per customer trends\nsns.factorplot(data = train_store, x = 'Month', y = \"SalePerCustomer\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo', # per promo in the store in rows\n               color = c) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efd049db4b19be15fcc208c95d77b834e89501dc"},"cell_type":"markdown","source":"Aha! Eventhough the plots above showed `StoreType` B as the most selling and performant one, in reality it is not true. The highest `SalePerCustomer` amount is observed at the `StoreType` D, about 12€ with `Promo` and 10€ without. As for `StoreType` A and C it is about 9€. \n\nLow `SalePerCustomer` amount for `StoreType` B describes its Buyer Cart: there are a lot of people who shop essentially for \"small\" things (or in a little quantity). Plus we saw that overall this `StoreType` generated the least amount of sales and customers over the period."},{"metadata":{"trusted":true,"_uuid":"ce8ef895fb261ce43b59b0eef004791329d3b40e"},"cell_type":"code","source":"# customers\nsns.factorplot(data = train_store, x = 'Month', y = \"Sales\", \n               col = 'DayOfWeek', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'StoreType', # per store type in rows\n               color = c) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4482db522452e07940dca20e5870fdc07085a4d9"},"cell_type":"markdown","source":"We see that stores of `StoreType` C are all closed on Sundays, whereas others are most of the time opened. Interestingly enough, stores of `StoreType` D are closed on Sundays only from October to December.\n\nBt the way what are the stores which are opened on Sundays?"},{"metadata":{"trusted":true,"_uuid":"0f5eeb4c78e7e46211ddaac452e85ff5386581bb"},"cell_type":"code","source":"# stores which are opened on Sundays\ntrain_store[(train_store.Open == 1) & (train_store.DayOfWeek == 7)]['Store'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78d9952474bea011300f3f50eed52d83e582ab54"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ceee0972dcf9f5e53897958f194fa4ca7a757ac"},"cell_type":"markdown","source":"To complete our preliminary data analysis, we can add variables describing the period of time during which competition and promotion were opened:"},{"metadata":{"trusted":true,"_uuid":"3fbb896cfaeb272eda8534768dbb5a166dbb2a5e"},"cell_type":"code","source":"# competition open time (in months)\ntrain_store['CompetitionOpen'] = 12 * (train_store.Year - train_store.CompetitionOpenSinceYear) + \\\n        (train_store.Month - train_store.CompetitionOpenSinceMonth)\n    \n# Promo open time\ntrain_store['PromoOpen'] = 12 * (train_store.Year - train_store.Promo2SinceYear) + \\\n        (train_store.WeekOfYear - train_store.Promo2SinceWeek) / 4.0\n\n# replace NA's by 0\ntrain_store.fillna(0, inplace = True)\n\n# average PromoOpen time and CompetitionOpen time per store type\ntrain_store.loc[:, ['StoreType', 'Sales', 'Customers', 'PromoOpen', 'CompetitionOpen']].groupby('StoreType').mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b89634bac50fcf98537cc2e97b311996d1c87e5"},"cell_type":"markdown","source":"The most selling and crowded `StoreType` A doesn't appear to be the one the most exposed to competitors. Instead it's a `StoreType` B, which also has the longest running period of promotion."},{"metadata":{"_uuid":"75cc76ed065dec5281326903b95a716a50bdc5fc"},"cell_type":"markdown","source":"### Correlational Analysis"},{"metadata":{"_uuid":"213852cc53da0448ff80a8ec27acc40ba98cf9ba"},"cell_type":"markdown","source":"We are finished with adding new variables to the data, so now we can check the overall correlations by plotting the `seaborn` heatmap:"},{"metadata":{"trusted":true,"_uuid":"d9032c116f4984bc78be1ebaf85dd721766a3913"},"cell_type":"code","source":"# Compute the correlation matrix \n# exclude 'Open' variable\ncorr_all = train_store.drop('Open', axis = 1).corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr_all, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize = (11, 9))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_all, mask = mask,\n            square = True, linewidths = .5, ax = ax, cmap = \"BuPu\")      \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2eaf9099570f6bb02f1ed72b7fac5228eb0508c8"},"cell_type":"markdown","source":"As mentioned before, we have a strong positive correlation between the amount of Sales and Customers of a store. We can also observe a positive correlation between the fact that the store had a running promotion (`Promo` equal to 1) and amount of `Customers`. \n\nHowever, as soon as the store continues a consecutive promotion (`Promo2` equal to 1) the number of `Customers` and `Sales` seems to stay the same or even decrease, which is described by the pale negative correlation on the heatmap. The same negative correlation is observed between the presence of the promotion in the store and the day of a week."},{"metadata":{"trusted":true,"_uuid":"0fd8dafae71d1f434ea3d6386489dea1b7fc824b"},"cell_type":"code","source":"# sale per customer trends\nsns.factorplot(data = train_store, x = 'DayOfWeek', y = \"Sales\", \n               col = 'Promo', \n               row = 'Promo2',\n               hue = 'Promo2',\n               palette = 'RdPu') ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"897840a27835937eb4f92838de9ae6676e2b3a6b"},"cell_type":"markdown","source":"There are several things here:\n- In case of no promotion, both `Promo` and `Promo2` are equal to 0, `Sales` tend to peak on Sunday (!). Though we should note that `StoreType` C doesn't work on Sundays. So it is mainly data from `StoreType` A, B and D.\n- On the contrary, stores that run the promotion tend to make most of the `Sales` on Monday. This fact could be a good indicator for Rossmann marketing campaigns. The same trend follow the stores which have both promotion at the same time (`Promo` and `Promo2` are equal to 1).\n- `Promo2` alone doesn't seem to be correlated to any significant change in the `Sales` amount. This can be also prooved by the blue pale area on the heatmap above."},{"metadata":{"_uuid":"2904ad4a194a997a3856ca9c2b3f05a239e046a9"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"9a7511356e8325af6c059982745c31b82ace03cc"},"cell_type":"markdown","source":"## Conclusion of EDA"},{"metadata":{"_uuid":"660e93b5a0484b666f476151710c182ae51707c7"},"cell_type":"markdown","source":"- The most selling and crowded `StoreType` is A.\n\n\n- The best \"Sale per Customer\" `StoreType` D indicates to the higher Buyer Cart. We could also assume that the stores of this types are situated in the rural areas, so that customers prefer buying more but less often.\n\n\n- Low `SalePerCustomer` amount for `StoreType` B indicates to the possible fact that people shop there essentially for small things. Which can also indicate to the label of this store type - \"urban\" - as it's more accessible for public, and customers don't mind shopping there from time to time during a week.\n\n\n- Customers tends to buy more on Mondays when there's one promotion running (`Promo`) and on Sundays when there is no promotion at all (both `Promo` and `Promo1` are equal to 0).\n\n\n- Promo2 alone doesn't seem to be correlated to any significant change in the `Sales` amount."},{"metadata":{"_uuid":"ffc16bfd13bca70696450cb1c2e2e87456ebefb1"},"cell_type":"markdown","source":"<br>\n## Time-Series Analysis per Store Type"},{"metadata":{"_uuid":"d74df4b1f415010ca3fbda5bd536ace0361c9f6d"},"cell_type":"markdown","source":"What makes a time series different from a regular regression problem? \n\n- It is time dependent. The basic assumption of a linear regression that the observations are independent doesn’t hold in this case.\n\n\n- Along with an increasing or decreasing trend, most time series have some form of seasonality trends, i.e. variations specific to a particular time frame. For example, for Christmas holidays, which we will see in this dataset."},{"metadata":{"_uuid":"1ec07a9b57adc7b7614c9e3aa936c37f253fa839"},"cell_type":"markdown","source":"<div class = \"alert alert-block alert-info\"> We build a time series analysis on store types instead of individual stores. The main advantage of this approach is its simplicity of presentation and overall account for different trends and seasonaltities in the dataset. </div>"},{"metadata":{"_uuid":"e3d4fa3c7f941a8b64b6cd49b36cb0eaf8f194e2"},"cell_type":"markdown","source":"In this section, we will analyse time series data: its trends, sesonalities and autocorrelation. Usually at the end of the analysis, we are able to develop a seasonal ARIMA (Autoregression Integrated Moving Average) model but it won't be our main focus today. Instead, we try to understand the data, and only later come up with the forecasts using Prophet methodology."},{"metadata":{"_uuid":"36bcaaaee31141aeb46c9bbd20a96eda02bb564c"},"cell_type":"markdown","source":"### Seasonality"},{"metadata":{"_uuid":"570d441bb1b880ac17285ba952a26b52ed3ef6b4"},"cell_type":"markdown","source":"We take four stores from store types to represent their group:\n- Store number 2 for `StoreType` A\n- Store number 85 for `StoreType` B, \n- Store number 1 for `StoreType` C \n- Store number 13 for `StoreType` D. \n\nIt also makes sense to downsample the data from days to weeks using the `resample` method to see the present trends more clearly."},{"metadata":{"trusted":true,"_uuid":"6661e2713560d9b556d51cbd159cb264fbf1037d"},"cell_type":"code","source":"# preparation: input should be float type\ntrain['Sales'] = train['Sales'] * 1.0\n\n# store types\nsales_a = train[train.Store == 2]['Sales']\nsales_b = train[train.Store == 85]['Sales'].sort_index(ascending = True) # solve the reverse order\nsales_c = train[train.Store == 1]['Sales']\nsales_d = train[train.Store == 13]['Sales']\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (12, 13))\n\n# store types\nsales_a.resample('W').sum().plot(color = c, ax = ax1)\nsales_b.resample('W').sum().plot(color = c, ax = ax2)\nsales_c.resample('W').sum().plot(color = c, ax = ax3)\nsales_d.resample('W').sum().plot(color = c, ax = ax4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5d93a7bc7f5a4c2de8255151dec0e5f932add79"},"cell_type":"markdown","source":"Retail sales for `StoreType` A and C tend to peak for the Christmas season and then decline after the holidays. We might have seen the same trend for `StoreType` D (at the bottom) but there is no information from July 2014 to January 2015 about these stores as they were closed."},{"metadata":{"_uuid":"afe907d465afc36ee4140d83f291ddb0201f0306"},"cell_type":"markdown","source":"### Yearly trend"},{"metadata":{"_uuid":"e9774c6ca2c13d526886c32afea861360a151a70"},"cell_type":"markdown","source":"The next thing to check the presence of a trend in series."},{"metadata":{"trusted":true,"_uuid":"608ea7456944e8ffcf9c6ade07a6c9f92320778f"},"cell_type":"code","source":"f, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (12, 13))\n\n# monthly\ndecomposition_a = seasonal_decompose(sales_a, model = 'additive', freq = 365)\ndecomposition_a.trend.plot(color = c, ax = ax1)\n\ndecomposition_b = seasonal_decompose(sales_b, model = 'additive', freq = 365)\ndecomposition_b.trend.plot(color = c, ax = ax2)\n\ndecomposition_c = seasonal_decompose(sales_c, model = 'additive', freq = 365)\ndecomposition_c.trend.plot(color = c, ax = ax3)\n\ndecomposition_d = seasonal_decompose(sales_d, model = 'additive', freq = 365)\ndecomposition_d.trend.plot(color = c, ax = ax4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20115bbcfa94867799c76c0960a5cb58497dffc5"},"cell_type":"markdown","source":"Overall sales seems to increase, however not for the `StoreType` C (a third from the top). Eventhough the `StoreType` A is the most selling store type in the dataset, it seems that it cab follow the same decresing trajectory as `StoreType` C did."},{"metadata":{"_uuid":"3a9e0f082d2f2a104e48a9e63eee0674aeaa9645"},"cell_type":"markdown","source":"### Autocorrelaion"},{"metadata":{"_uuid":"1c913591e8de83a75823a8b49fc2d12ff4185547"},"cell_type":"markdown","source":"The next step in ourtime series analysis is to review Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots. \n\nACF is a measure of the correlation between the timeseries with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant ‘t1’…’tn’ with series at instant ‘t1-5’…’tn-5’ (t1-5 and tn being end points).\n\nPACF, on the other hand, measures the correlation between the timeseries with a lagged version of itself but after eliminating the variations explained by the intervening comparisons. Eg. at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4. "},{"metadata":{"trusted":true,"_uuid":"c644ca15c28f676edeb82bcf93da18cc4429cde1"},"cell_type":"code","source":"# figure for subplots\nplt.figure(figsize = (12, 8))\n\n# acf and pacf for A\nplt.subplot(421); plot_acf(sales_a, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(422); plot_pacf(sales_a, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for B\nplt.subplot(423); plot_acf(sales_b, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(424); plot_pacf(sales_b, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for C\nplt.subplot(425); plot_acf(sales_c, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(426); plot_pacf(sales_c, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for D\nplt.subplot(427); plot_acf(sales_d, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(428); plot_pacf(sales_d, lags = 50, ax = plt.gca(), color = c)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f2df2fc39aa292a05a1ee8fddee3e3c8ff01026"},"cell_type":"markdown","source":"We can read these plots horizontally. Each horizontal pair is for one 'StoreType', from A to D. In general, those plots are showing the correlation of the series with itself, lagged by x time units correlation of the series with itself, lagged by x time units.\n\nThere is at two things common for each pair of plots: non randomnes of the time series and high lag-1 (which will probably need a higher order of differencing d/D).\n\n- Type A and type B:\nBoth types show seasonalities at certain lags. For type A, it is each 12th observation with positives spikes at the 12 (s) and 24(2s) lags and so on. For type B it's a weekly trend with positives spikes at the 7(s), 14(2s), 21(3s) and 28(4s) lags. \n\n\n- Type C and type D:\nPlots of these two types are more complex. It seems like each observation is coorrelated to its adjacent observations. "},{"metadata":{"_uuid":"7caca2a3fe399f5c891b70456a29a7c636e7bb02"},"cell_type":"markdown","source":"## Time Series Analysis and Forecasting with Prophet\n#### Forecasting for the next 6 weeks for the first store"},{"metadata":{"_uuid":"6a42928944b0766e031928177ccd2cf4d361c2ec"},"cell_type":"markdown","source":"The Core Data Science team at Facebook recently published a new procedure for forecasting time series data called [Prophet](https://research.fb.com/prophet-forecasting-at-scale/). It is based on an additive model where non-linear trends are fit with yearly and weekly seasonality, plus holidays. It enables performing [automated forecasting which are already implemented in R](https://www.rdocumentation.org/packages/forecast/versions/7.3/topics/auto.arima) at scale in Python 3."},{"metadata":{"trusted":true,"_uuid":"7addd2bea2d43ebead19f3d7791efbe1d24bb8d6"},"cell_type":"code","source":"# importing data\ndf = pd.read_csv(\"../input/train.csv\",  \n                    low_memory = False)\n\n# remove closed stores and those with no sales\ndf = df[(df[\"Open\"] != 0) & (df['Sales'] != 0)]\n\n# sales for the store number 1 (StoreType C)\nsales = df[df.Store == 1].loc[:, ['Date', 'Sales']]\n\n# reverse to the order: from 2013 to 2015\nsales = sales.sort_index(ascending = False)\n\n# to datetime64\nsales['Date'] = pd.DatetimeIndex(sales['Date'])\nsales.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03bd20ebae62750e727064883cfca9d723d7ae97"},"cell_type":"code","source":"# from the prophet documentation every variables should have specific names\nsales = sales.rename(columns = {'Date': 'ds',\n                                'Sales': 'y'})\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ad68c22fa4548f49563ca7944c074c17fd85023"},"cell_type":"code","source":"# plot daily sales\nax = sales.set_index('ds').plot(figsize = (12, 4), color = c)\nax.set_ylabel('Daily Number of Sales')\nax.set_xlabel('Date')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6ea31abcb8fd1ba0e0575fb7b9b45111c1cce3f"},"cell_type":"markdown","source":"### Modeling Holidays"},{"metadata":{"_uuid":"c1d7acf84db8f85701166a6aea82c2d562bbe114"},"cell_type":"markdown","source":"Prophet also allows to [model for holidays](https://facebookincubator.github.io/prophet/docs/holiday_effects.html), and that's what we do here.\n\nThe StateHoliday variable in the dataset indicates a state holiday, at which all stores are normally closed. There are also school holidays in the dataset at which ceratin stores are also closing their doors."},{"metadata":{"trusted":true,"_uuid":"1754b50efd23514815aeaaad004c986fd5e0ab20"},"cell_type":"code","source":"# create holidays dataframe\nstate_dates = df[(df.StateHoliday == 'a') | (df.StateHoliday == 'b') & (df.StateHoliday == 'c')].loc[:, 'Date'].values\nschool_dates = df[df.SchoolHoliday == 1].loc[:, 'Date'].values\n\nstate = pd.DataFrame({'holiday': 'state_holiday',\n                      'ds': pd.to_datetime(state_dates)})\nschool = pd.DataFrame({'holiday': 'school_holiday',\n                      'ds': pd.to_datetime(school_dates)})\n\nholidays = pd.concat((state, school))      \nholidays.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"780395de5a2511b0cae176ea76f20234a94f0b72"},"cell_type":"code","source":"# set the uncertainty interval to 95% (the Prophet default is 80%)\nmy_model = Prophet(interval_width = 0.95, \n                   holidays = holidays)\nmy_model.fit(sales)\n\n# dataframe that extends into future 6 weeks \nfuture_dates = my_model.make_future_dataframe(periods = 6*7)\n\nprint(\"First week to forecast.\")\nfuture_dates.tail(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"389960da8748795a21462da8e093e844e889d5fb"},"cell_type":"code","source":"# predictions\nforecast = my_model.predict(future_dates)\n\n# preditions for last week\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57a1d32dc3fa9283ee3ca5a6a5adcc159c1e3a92"},"cell_type":"markdown","source":"The forecast object here is a new dataframe that includes a column yhat with the forecast, as well as columns for components and uncertainty intervals."},{"metadata":{"trusted":true,"_uuid":"24aa1212eac9fe159f6bcb99bf5e309eccf917d8"},"cell_type":"code","source":"fc = forecast[['ds', 'yhat']].rename(columns = {'Date': 'ds', 'Forecast': 'yhat'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7122e1e00c3b716558898477f66adcbfe55b1c4d"},"cell_type":"markdown","source":"Prophet plots the observed values of our time series (the black dots), the forecasted values (blue line) and the uncertainty intervals of our forecasts (the blue shaded regions)."},{"metadata":{"trusted":true,"_uuid":"afeb6b57c319e4dc8501fa88249075bc1810c215"},"cell_type":"code","source":"# visualizing predicions\nmy_model.plot(forecast);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13f091c4a70c8c35f0ca246fe8daf305f8bfcf82"},"cell_type":"markdown","source":"As we see Prophet catches the trends and most of the time gets future values right.\n\nOne other particularly strong feature of Prophet is its ability to return the components of our forecasts. This can help reveal how daily, weekly and yearly patterns of the time series plus manyally included holidayes contribute to the overall forecasted values:"},{"metadata":{"trusted":true,"_uuid":"f40e00c717227efe7decbf67a66f1f46597c6e28"},"cell_type":"code","source":"my_model.plot_components(forecast);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61db89149f6e512c280bf759635ebc10725594e8"},"cell_type":"markdown","source":"The first plot shows that the monthly sales of store number 1 has been linearly decreasing  over time and the second shows the holiays gaps included in the model. The third plot highlights the fact that the weekly volume of last week sales peaks towards the Monday of the next week, while the forth plot shows that the most buzy season occurs during the Christmas holidays."},{"metadata":{"_uuid":"f6a0309c6f29ea2becb1014bd24f14062bef7490"},"cell_type":"markdown","source":"### Conclusion of Time Series forecasting"},{"metadata":{"_uuid":"aca583a466e65014ff564e54da5f6c1f9e7bd0fa"},"cell_type":"markdown","source":"During this part we discussed time series analysis with `.seasonal_decompose()`, `ACF` and `PCF` plots and fited forecasting model using a new procedure by Facebook `Prophet`.\n\nWe can now present main advantages and drawbacks of time series forecasting:\n\n__Advantages__\n- Powerfull tool for the time series forecasting as it accounts for time dependencies, seasonalities and holidays (Prophet: manualy).\n- Easily implemented with R `auto.arima()` from `forecast` package, which runs a complex grid search and sophisticated algorythm behind the scene.\n\n__Drawbacks__\n- Doesn't catch interactions between external features, which could improve the forecasting power of a model. In our case, these variables are `Promo` and `CompetitionOpen`. \n- Eventhough Prophet offers an automated solution for ARIMA, this methodology is under development and not completely stable.\n- Fitting seasonal ARIMA model needs 4 to 5 whole seasons in the dataset, which can be the the biggest drawback for new companies.\n- Seasonal ARIMA in Python has 7 hyperparameters which can be tuned only manually affecting significantly the speed of the forecasting process."},{"metadata":{"_uuid":"6ca61642d919dd9d9c6b666d8b66c61f624d5139"},"cell_type":"markdown","source":"<br>\n## Alternative Approach: Regression XGBoost"},{"metadata":{"_uuid":"14d6a201e5f2e674ae134b80720bbc66dae2cab5"},"cell_type":"markdown","source":"[XGBoost](https://github.com/dmlc/xgboost/blob/master/doc/model.md) is an implementation of Gradient Boosted Decision trees designed for speed and performance. Its more suitable name is a as [regularized Gradient Boosting](http://datascience.la/xgboost-workshop-and-meetup-talk-with-tianqi-chen/), as it uses a more regularized model formalization to control over-fitting. \n\nAdditional advantages of this algorythm are:\n\n- Automated missing values handling: XGB uses a \"learned\" default direction for the missing values. \"Learned\" means learned in the tree construction process by choosing the best direction that optimizes the training loss.\n- Interactive feature analysis (yet implemented only in R): plots the structure of decision trees with splits and leaves.\n- Feature importance analysis: a sorted barplot of the most significant variables."},{"metadata":{"_uuid":"28e94426388585a6613c12b02ed7fe2915b183d4"},"cell_type":"markdown","source":"<div class = \"alert alert-block alert-info\"> As we already saw in the previos section our data is higly seasonal and not random (dependent). Therefore, before fitting any models we need to \"smooth\" target variable Sales. The typical preprocessing step is to log transform the data in question. Once we perform the forecasting we will unwind log transformations in reverse order. </div>"},{"metadata":{"_uuid":"22664b37a173f5f87e0404328d8a8e7371bc9b73"},"cell_type":"markdown","source":"### Quick Run through"},{"metadata":{"trusted":true,"_uuid":"8919a0cde3466da3eec427422a5dfd4c39811342"},"cell_type":"code","source":"# to predict to\ntest = pd.read_csv(\"../input/test.csv\", \n                    parse_dates = True, low_memory = False, index_col = 'Date')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b71b0eabfd6cb897395c38c84193829a47feeeb6"},"cell_type":"markdown","source":"The Id variable represents a (Store, Date) duple within the test set."},{"metadata":{"trusted":false,"_uuid":"dc381083a8b2288f25a2d4fad84e40c2be0402ce"},"cell_type":"code","source":"# test: missing values?\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fe24696b48b0a97e609511df0066956731c1222b"},"cell_type":"code","source":"test[pd.isnull(test.Open)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfe794e29489bda1f9797f6aa691910987b325ee"},"cell_type":"markdown","source":"We see that these stores should be normally opened. Let's assume that they are then."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d3008e45321a7cad77fe41ffedc3eeb89a690113"},"cell_type":"code","source":"# replace NA's in Open variable by 1 \ntest.fillna(1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4dedc40b971e2c31e30b9a5ba5a01e3ff9d14ce"},"cell_type":"markdown","source":"### Data Encoding"},{"metadata":{"_uuid":"c8c7a1f19debcb8d920e61766a3714750fa3968d"},"cell_type":"markdown","source":"XGBoost doesn't support anything else than numbers. So prior to modeling we need to encode certain factor variables into numerical plus extract dates as we did before for the train set."},{"metadata":{"trusted":false,"_uuid":"3793e054f2f941d4269e98f1e43d1e7fd3930f51"},"cell_type":"code","source":"# data extraction\ntest['Year'] = test.index.year\ntest['Month'] = test.index.month\ntest['Day'] = test.index.day\ntest['WeekOfYear'] = test.index.weekofyear\n\n# to numerical\nmappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\ntest.StateHoliday.replace(mappings, inplace = True)\n\ntrain_store.Assortment.replace(mappings, inplace = True)\ntrain_store.StoreType.replace(mappings, inplace = True)\ntrain_store.StateHoliday.replace(mappings, inplace = True)\ntrain_store.drop('PromoInterval', axis = 1, inplace = True)\n\nstore.StoreType.replace(mappings, inplace = True)\nstore.Assortment.replace(mappings, inplace = True)\nstore.drop('PromoInterval', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"530a009cc0c14db11ef83c2bc0c4f1d1eb1c2b5d"},"cell_type":"markdown","source":"Returning back to the `train_store` data:"},{"metadata":{"trusted":false,"_uuid":"a01c4c9bbbe41c99ce88dc3df6675a0a8c5ade23"},"cell_type":"code","source":"# take a look on the train and store again\ntrain_store.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac8bd8979a20e2412b5ef8c18743f11984969fa9"},"cell_type":"markdown","source":"Let's merge `test` and `store` data too:"},{"metadata":{"trusted":false,"_uuid":"3ef8a4f2c75e563d1c415901d4afe569484d6e48"},"cell_type":"code","source":"print(\"Joining test set with an additional store information.\")\ntest_store = pd.merge(test, store, how = 'inner', on = 'Store')\n\ntest_store['CompetitionOpen'] = 12 * (test_store.Year - test_store.CompetitionOpenSinceYear) + (test_store.Month - test_store.CompetitionOpenSinceMonth)\ntest_store['PromoOpen'] = 12 * (test_store.Year - test_store.Promo2SinceYear) + (test_store.WeekOfYear - test_store.Promo2SinceWeek) / 4.0\n\nprint(\"In total: \", test_store.shape)\ntest_store.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1ed4d68bc522920fb70d5d3976d81d2d9ad964f"},"cell_type":"markdown","source":"### Model Training"},{"metadata":{"_uuid":"c2c1607cc9cf4584cc48e2755e1dd42b634424a7"},"cell_type":"markdown","source":"#### Approach"},{"metadata":{"_uuid":"2a9e1a5eaee8cb043b8c4d538510b9101642335b"},"cell_type":"markdown","source":"1. Split train data to train and test set to evaluate the model.\n2. Set `eta` to a relatively high value (e.g. 0.05 ~ 0.1), num_round to 300 ~ 500\n3. Use grid search to find the best combination of additional parameters.\n4. Lower `eta` until we reach the optimum.\n5. Use the validation set as watchlist to retrain the model with the best parameters. "},{"metadata":{"trusted":false,"_uuid":"db24f7da1fd95129cde074a319acaa27d807ce76"},"cell_type":"code","source":"# split into training and evaluation sets\n# excluding Sales and Id columns\npredictors = [x for x in train_store.columns if x not in ['Customers', 'Sales', 'SalePerCustomer']]\ny = np.log(train_store.Sales) # log transformation of Sales\nX = train_store\n\n# split the data into train/test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.3, # 30% for the evaluation set\n                                                    random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bf49bed3654a6b11152b73e0c8a49ed75d31e2af"},"cell_type":"code","source":"# predictors\nX.columns","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c1e61d346ce66ca962efcd87c461a928d1ab8192"},"cell_type":"code","source":"# evaluation metric: rmspe\n# Root Mean Square Percentage Error\n# code chunk shared at Kaggle\n\ndef rmspe(y, yhat):\n    return np.sqrt(np.mean((yhat / y-1) ** 2))\n\ndef rmspe_xg(yhat, y):\n    y = np.expm1(y.get_label())\n    yhat = np.expm1(yhat)\n    return \"rmspe\", rmspe(y, yhat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"606397a9abf426d50a19df62798823aab804d37e"},"cell_type":"markdown","source":"#### Tuning Parameters"},{"metadata":{"_uuid":"343241092778ab6364b0a3274f18afd99f80b6a7"},"cell_type":"markdown","source":"-  `eta`: Step size used in updating weights. Lower value means slower training but better convergence. \n- `num_round`: Total number of iterations.\n- `subsample`: The ratio of training data used in each iteration; combat overfitting. Should be configured in the range of 30% to 80% of the training dataset, and compared to a value of 100% for no sampling.\n- `colsample_bytree`: The ratio of features used in each iteration, default 1.\n- `max_depth`: The maximum depth of each tree. If we do not limit max depth, gradient boosting would eventually overfit.\n- `early_stopping_rounds`: If there's no increase in validation score for a given number of iterations, the algorithm will stop early, also combats overfitting."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"83627eb3be62d7744ddef1a996de3b340ce228d3"},"cell_type":"code","source":"# base parameters\nparams = {\n    'booster': 'gbtree', \n    'objective': 'reg:linear', # regression task\n    'subsample': 0.8, # 80% of data to grow trees and prevent overfitting\n    'colsample_bytree': 0.85, # 85% of features used\n    'eta': 0.1, \n    'max_depth': 10, \n    'seed': 42} # for reproducible results","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"72dcce66187d0c407fc3ed3f44014a785731a33a"},"cell_type":"code","source":"# XGB with xgboost library\ndtrain = xgb.DMatrix(X_train[predictors], y_train)\ndtest = xgb.DMatrix(X_test[predictors], y_test)\n\nwatchlist = [(dtrain, 'train'), (dtest, 'test')]\n\nxgb_model = xgb.train(params, dtrain, 300, evals = watchlist,\n                      early_stopping_rounds = 50, feval = rmspe_xg, verbose_eval = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0d248d307e09d1d6ce0a59c8d871ffae92625a2"},"cell_type":"markdown","source":"Last five rows:\n\n`\n[295]\ttrain-rmspe:0.106959\ttest-rmspe:0.111575\n[296]\ttrain-rmspe:0.106855\ttest-rmspe:0.111498\n[297]\ttrain-rmspe:0.106467\ttest-rmspe:0.111439\n[298]\ttrain-rmspe:0.106348\ttest-rmspe:0.111331\n[299]\ttrain-rmspe:0.105759\ttest-rmspe:0.111298\n`"},{"metadata":{"_uuid":"fdff27c0c2e84889eb3fe1f699d7b3fb36e2056a"},"cell_type":"markdown","source":"Essentially, we want the least value. The model with base hyperparameters gives out better resukt on the train set, indicating to the overfitting issue. "},{"metadata":{"_uuid":"08c1a2c43e8e6fba9a2fb0e01767314b24fc46cd"},"cell_type":"markdown","source":"### Grid Search from sklearn"},{"metadata":{"_uuid":"6600aab811e919c4a853c680cb26d81c8a7246f1"},"cell_type":"markdown","source":"Scikit learn wrapper is famous for the `GridSearchCV` and `RandomizedSearchCV`. Between these two, most of the time the [preference leans towards `RandomnizedSearchCV`](https://stats.stackexchange.com/questions/160479/practical-hyperparameter-optimization-random-vs-grid-search), faster version of `GridSearchCV`. \n\nAs an input, `RandomnizedSearchCV` takes only sklearn wrapper of XGboost, so instead of using the first version of a model, we build the analogous model in sklearn with `XGBRegressor`."},{"metadata":{"trusted":false,"_uuid":"214862865d0eef63b1849cf64626acaca5b08fff"},"cell_type":"code","source":"# XGB with sklearn wrapper\n# the same parameters as for xgboost model\nparams_sk = {'max_depth': 10, \n            'n_estimators': 300, # the same as num_rounds in xgboost\n            'objective': 'reg:linear', \n            'subsample': 0.8, \n            'colsample_bytree': 0.85, \n            'learning_rate': 0.1, \n            'seed': 42}     \n\nskrg = XGBRegressor(**params_sk)\n\nskrg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f36dfef0e3b645ad84cbc4181c412af37b8de846"},"cell_type":"markdown","source":"For parameters we will specify the regularization parameter `reg_alpha` which reduce model complexity and enhance performance, as well as `gamma` parameter which represents the minimum loss reduction required to make a split and also `max_depth` used to control over-fitting."},{"metadata":{"trusted":false,"_uuid":"0f4c82536a3b2199d93395332df3696c8cda88c9"},"cell_type":"code","source":"import scipy.stats as st\n\nparams_grid = {  \n    'learning_rate': st.uniform(0.01, 0.3),\n    'max_depth': list(range(10, 20, 2)),\n    'gamma': st.uniform(0, 10),\n    'reg_alpha': st.expon(0, 50)}\n\nsearch_sk = RandomizedSearchCV(skrg, params_grid, cv = 5) # 5 fold cross validation\nsearch_sk.fit(X_train, y_train)\n\n# best parameters\nprint(search_sk.best_params_); print(search_sk.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9444da3df46e28965a0101340a5359928cee94a3"},"cell_type":"code","source":"# with new parameters\nparams_new = {\n    'booster': 'gbtree', \n    'objective': 'reg:linear', \n    'subsample': 0.8, \n    'colsample_bytree': 0.85, \n    'eta': 0.044338624448041611, \n    'max_depth': 16, \n    'gamma': 0.80198330585415034,\n    'reg_alpha': 23.008226565535971,\n    'seed': 42} \n\nmodel_final = xgb.train(params_new, dtrain, 300, evals = watchlist,\n                        early_stopping_rounds = 50, feval = rmspe_xg, verbose_eval = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e06e2517363d3ad89ba3522639073c3ab7a2146"},"cell_type":"markdown","source":"Last five rows:\n    \n`\n[295]\ttrain-rmspe:0.213295\ttest-rmspe:0.147425\n[296]\ttrain-rmspe:0.213214\ttest-rmspe:0.147367\n[297]\ttrain-rmspe:0.213061\ttest-rmspe:0.147199\n[298]\ttrain-rmspe:0.213084\ttest-rmspe:0.14701\n[299]\ttrain-rmspe:0.212913\ttest-rmspe:0.146808\n`"},{"metadata":{"_uuid":"8f9c0cc3008036355154cc163d2c5879d4d05de6"},"cell_type":"markdown","source":"We resolved an issue with overfitting, but due to the decrease of learning rate (`eta`) we got a bit worse overall score on the test set (~0.11 to ~0.14)."},{"metadata":{"trusted":false,"_uuid":"6c9f59d4189dd42f44140a1ed7f45e6f2ce765f3"},"cell_type":"code","source":"yhat = model_final.predict(xgb.DMatrix(X_test[predictors]))\nerror = rmspe(X_test.Sales.values, np.exp(yhat))\n\nprint('First validation yelds RMSPE: {:.6f}'.format(error))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e13d10fb57753eee803d3b65e7f7e024d4e10325"},"cell_type":"markdown","source":"Even though we observed a bit higher RMSPE value, we need to remember that the corresponding `eta` for the first version was 0.1, which is usually considered as high. I think that it's remarkable that we got more or less the same result but with 2x lower `eta` (0.1 to 0.04).\n\n__Next steps towards model's improvement will involve further decrease of `eta`, tuning of correpsonding `gamma` and `max_depth`.__"},{"metadata":{"_uuid":"244b0269fe075be9f03a96b3903ed0a89363ad21"},"cell_type":"markdown","source":"### Model understanding"},{"metadata":{"_uuid":"9833e20b3c783c24c1432d5595fd2e54592304fb"},"cell_type":"markdown","source":"Feature importance scores help us see which variables contributed the most to the score."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"64c7b9e78f5b885b0dc35a017ba2b5a9d7ec3f0a"},"cell_type":"code","source":"xgb.plot_importance(model_final)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40d8b8f99671f915d7152badc048bc3d40e9b808"},"cell_type":"markdown","source":"Variables `Store` and `CompetitionDistance` are both leading. Then go features `CompetitionOpenSinceMonth`,  `CompetitionOpenSinceYear`, `DayOfWeek`, `PromoSinceWeek` and deducted features `PromoOpen`."},{"metadata":{"_uuid":"01f7f3596ce50703d76107f12529470739a71fde"},"cell_type":"markdown","source":"### Prediction to unseen data"},{"metadata":{"trusted":false,"_uuid":"ccd603e8fd4499e0f39ecad8f8fdd49c11a36a82"},"cell_type":"code","source":"# predictions to unseen data\nunseen = xgb.DMatrix(test_store[predictors])\ntest_p = model_final.predict(unseen)\n\nforecasts = pd.DataFrame({'Id': test['Id'], \n                          'Sales': np.exp(test_p)})\n# forecasts\nforecasts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c009af903c2b1f19e722d5caa9170b67ee99544"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"90f4aeb824b63635ed25208b2bd3297dc4da9f62"},"cell_type":"markdown","source":"## Final thoughts"},{"metadata":{"_uuid":"d91d6bd0b69d1f34e21b289a263ebab9f57f4e9e"},"cell_type":"markdown","source":"Time Series Analysis is a must for time series data. It goes much deeper than ad-hoc Exploratory Data Analysis, revealing trends, non randomness of the data and seasonalities. \n\nI was particularly excited to use a new forecasting procedure `Prophet`. Eventhough this tool is still under development, it has everything set for the advanced modeling as it can account for change points in trends and holidays in the data. In the meantime, the most sophisticated tool for the Time Series Analysis stays `auto.arima` from R `forecast` package. \n\nA significant jump in the forecasting performance of the model fitted above, XGboost with xgboost library, can be achieved by increasing the number and range of hyperparameters. Due to the number of observations (800k) and with a laptop like mine, the \"more developped\" grid search would take about 2-3 days to fit. So I left this room for the improvement for later notebooks.\n\nAnother method that I didn't cover here is a regression model Stacking, which works great for small or medium size data sets. We would basically combine XGboost, RandomForest, NN and SVM for regression. And then stack them together by building the final model.\n\nConcerning the XGboost model, recently Microsoft [open sourced LightGBM](https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide), a potentially better library than XGboost. For the next project, I will try it out.\n\n\nThank you for reading!"},{"metadata":{"_uuid":"ad74148d2bbae86737d6ebad2934ed7be329bd2c"},"cell_type":"markdown","source":"#### P.S. Submission "},{"metadata":{"_uuid":"9c5320f6647e5d6eec6693160b4b8aecc5891d2d"},"cell_type":"markdown","source":"As it is a Kaggle competition, I made two submissions to leaderboard: forecast from the base and tuned model. "},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4335cf65b10145ed0d41832b3867052108ed188e"},"cell_type":"code","source":"# first\n# 0.66419\ntest_base = xgb_model.predict(unseen)\n\nforecasts_base = pd.DataFrame({'Id': test['Id'], \n                          'Sales': np.exp(test_base)})\nforecasts_base.to_csv(\"xgboost_2_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"65b0de6ca612fb925c3ff14f816dd6d653ba74cc"},"cell_type":"code","source":"# final\n# 0.60553\nforecasts.to_csv(\"xgboost_submission.csv\", index = False) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33557ddb3868382a92fc97ed396e5591eb46a1a0"},"cell_type":"markdown","source":"The score with four tuned parameters __improved from 0.66 to 0.60 on the Public Board__ and __from 0.62 to 0.68 on the Provate Board (test set reserved by Kaggle itself)__ . \n\nThese scores are still quite low, but it's a good start for further improvement. "},{"metadata":{"_uuid":"6e5386ad2d862837eae9584059a5da44a654c14f"},"cell_type":"markdown","source":"![\"kaggle\"](kaggle.png \"submission\")"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}