{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-22T18:38:58.549915Z","iopub.execute_input":"2022-02-22T18:38:58.550819Z","iopub.status.idle":"2022-02-22T18:38:59.552753Z","shell.execute_reply.started":"2022-02-22T18:38:58.550674Z","shell.execute_reply":"2022-02-22T18:38:59.551745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The plan for the project is the following:\n\n    1 First, we download the dataset.\n    2 Next, we do some preliminary analysis of the data.\n    3 After that, we set up a validation strategy to make sure our model produces correct predictions.\n    4 Then we implement a linear regression model in Python and NumPy.\n    5 Next, we cover feature engineering to extract important features from the data to improve the model.\n    6 Finally, we see how to make our model stable with regularization and use it to predict car prices.","metadata":{}},{"cell_type":"markdown","source":"# Problem Statement\nOur task is to predict the sales for a few identified stores on a given day.\n\nFrom a pure business perspective, the first question you would need to ask is: \n    \n    Who is the end stakeholder for Rossmann Store Sales and how is he going to utilize the solution? ","metadata":{}},{"cell_type":"markdown","source":"## Designing an SCQ\nThe four components can be defined as follows\n* **Desired Future State**\n    \n    * The marketing & promotions team now have the means to accurately forecast sales for the required stores and therefore design promotional campaigns based on expected store performance in increase overall sales.\n\n\n* **Current Situation**\n\n     * The marketing team at Rossman wants to design promotional campaigns for store customers and thereby increase sales.\n     * The lake of visibility into future stores sales makes the campaign design complicated and less effective for stores expected to perform poorly.\n\n\n* **Complication**\n        \n     * The team lacks the tools to study and estimate future stores sales.\n   \n\n* **Question**\n\n     * How can we estimate future sales for a store?\n     * How accurate is the estimate?","metadata":{}},{"cell_type":"markdown","source":"# Designing the Solution\n\nWe are going to develop an ML model that can learn the sales for a store as a function of internal, external, and temporal (time-based) \nattributes and then predict future sales given the attributes available.\n\nwe consider the data in way that it can be represented as \n\n    sales as a function of store + other attributes\n\nInstead of a time-series based model defined as \n\n    Sales as a function of time \n\nIn this way, we can define a model that can learn the patterns from various stores and other external attributes (which we will explore with the data) to predict the expected sales.\n","metadata":{}},{"cell_type":"markdown","source":"# Exploring the Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/rossmann-store-sales/train.csv\", parse_dates=['Date'],\n                 dtype={\n                           'Store': str,\n                           'DayOfWeek': str,\n                           'StateHoliday': str,\n                       })\ndf_Close = df[df.Sales == 0]\ndf_Open = df[df.Sales != 0]\nprint(f\"{df_Close.shape[0]/df.shape[0]*100:.2f}% of stores are close,   {df_Open.shape[0]/df.shape[0]*100:.2f}% of stores are open\")\nprint(\"\\nFor this Analysis I’m focusing only on open Stores\")\n\ndf = df_Open\ndf.drop(labels='Open', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:38:59.554921Z","iopub.execute_input":"2022-02-22T18:38:59.555273Z","iopub.status.idle":"2022-02-22T18:39:00.79803Z","shell.execute_reply.started":"2022-02-22T18:38:59.555218Z","shell.execute_reply":"2022-02-22T18:39:00.797002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Looking at the Data Dictionary\n    \n* **Store:**         a unique ID for each store,\n* **Sales:**         the turnover for a given day *(our target y variable)*,\n* **Customers:**     the number of customers on a given day,\n* **Open:**          an indicator for whether the store was open: 0 = closed, 1 = open,\n* **Promo:**         indicates whether a store is running a promo on that day,\n* **SchoolHoliday:** indicates if the (Store, Date) was affected by the closure of public schools.\n* **StateHoliday:**  indicates a state holiday.\n    \n        Normally all stores, with few exceptions, are closed on state holidays. \n        Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = none","metadata":{}},{"cell_type":"markdown","source":"## Finding Data Types","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(df.dtypes, columns=['Type']).T","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:00.799327Z","iopub.execute_input":"2022-02-22T18:39:00.799723Z","iopub.status.idle":"2022-02-22T18:39:00.822169Z","shell.execute_reply.started":"2022-02-22T18:39:00.799681Z","shell.execute_reply":"2022-02-22T18:39:00.821246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Duplicated row\n\nNo duplicated rows found.\n","metadata":{}},{"cell_type":"code","source":"df[df.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:00.823359Z","iopub.execute_input":"2022-02-22T18:39:00.823568Z","iopub.status.idle":"2022-02-22T18:39:01.11816Z","shell.execute_reply.started":"2022-02-22T18:39:00.823544Z","shell.execute_reply":"2022-02-22T18:39:01.117308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Null values\n\nNo missing data, Sales — our target variable — doesn’t have any missing values.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(df.isnull().sum(), columns=['Null Values']).T","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:01.120585Z","iopub.execute_input":"2022-02-22T18:39:01.120988Z","iopub.status.idle":"2022-02-22T18:39:01.248505Z","shell.execute_reply.started":"2022-02-22T18:39:01.120951Z","shell.execute_reply":"2022-02-22T18:39:01.247846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Value\n\n**Sales:** the turnover for a given day (our target y variable)","metadata":{}},{"cell_type":"code","source":"# distribution of Sales has a very long tail\nplt.title(\"Distribution of sales\")\nplt.xlabel(\"Sales\")\ngraph = sns.kdeplot(x=\"Sales\", data=df)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:01.249648Z","iopub.execute_input":"2022-02-22T18:39:01.250634Z","iopub.status.idle":"2022-02-22T18:39:04.557658Z","shell.execute_reply.started":"2022-02-22T18:39:01.250585Z","shell.execute_reply":"2022-02-22T18:39:04.556585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribution of Sales has a very long tail\nplt.title(\"Distribution of sales\")\nplt.xlabel(\"Sales\")\ngraph = sns.histplot(x=\"Sales\", data=df, kde=True, hue='Promo')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:04.558808Z","iopub.execute_input":"2022-02-22T18:39:04.559053Z","iopub.status.idle":"2022-02-22T18:39:10.803499Z","shell.execute_reply.started":"2022-02-22T18:39:04.559024Z","shell.execute_reply":"2022-02-22T18:39:10.802585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The long tail makes it quite difficult for us to see the distribution, but it has an even stronger effect on a model: \n    \nsuch distribution can greatly confuse the model, so it won’t learn well enough. \n\nOne way to solve this problem is log transformation","metadata":{}},{"cell_type":"code","source":"df['Log_Sales'] = np.log1p(df.Sales)\nplt.title(\"Distribution of sales after log transformation\")\nplt.xlabel(\"$Log(Sales + 1)$\")\nax = sns.histplot(x=\"Log_Sales\", data=df, kde=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:10.804649Z","iopub.execute_input":"2022-02-22T18:39:10.804902Z","iopub.status.idle":"2022-02-22T18:39:15.513267Z","shell.execute_reply.started":"2022-02-22T18:39:10.804847Z","shell.execute_reply":"2022-02-22T18:39:15.51202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Validation framework\n\nLet’s split the DataFrame such that\n\n* 15% of data goes to validation,\n* 15% goes to test,\n* The remaining 70% goes to training.","metadata":{}},{"cell_type":"code","source":"n = len(df)\n\nn_val = int(0.15 * n)\nn_test = int(0.15 * n)\nn_train = n - (n_val + n_test)\n\nnp.random.seed(16)\nidx = np.arange(n)\nnp.random.shuffle(idx)\n\ndf_shuffle = df.iloc[idx]\n\ndf_train = df_shuffle.iloc[:n_train].copy()\ndf_val = df_shuffle.iloc[n_train: n_train + n_val].copy()\ndf_test = df_shuffle.iloc[n_train + n_val:].copy()\n\nprint(n_val, n_test, n_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:15.514749Z","iopub.execute_input":"2022-02-22T18:39:15.514996Z","iopub.status.idle":"2022-02-22T18:39:15.783467Z","shell.execute_reply.started":"2022-02-22T18:39:15.514968Z","shell.execute_reply":"2022-02-22T18:39:15.782566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the DataFrame is split into three parts, and we can continue.","metadata":{}},{"cell_type":"code","source":"y_train = df_train.Log_Sales.values\n\ny_val = df_val.Log_Sales.values\n\ny_test = df_test.Log_Sales.values","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:15.784938Z","iopub.execute_input":"2022-02-22T18:39:15.785624Z","iopub.status.idle":"2022-02-22T18:39:15.792667Z","shell.execute_reply.started":"2022-02-22T18:39:15.785578Z","shell.execute_reply":"2022-02-22T18:39:15.791766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Machine learning for regression\n\nThe problem we are solving is a regression problem: the goal is to predict the sales for a few identified stores on a given day. \n\nFor this project we will use the simplest regression model: linear regression. ","metadata":{}},{"cell_type":"markdown","source":"#### Linear regression\n\n#### Training linear regression model\n\nWe have multiple ways to do that. \n\nWe will use normal equation, which is the simplest method to implement\n\nTo implement the normal equation, we need to do the following:\n\n1. Create a function that takes in a matrix X with features and a vector y with the target.\n1. Add a dummy column (the feature that is always set to 1) to the matrix X.\n1. Train the model: compute the weights w by using the normal equation.\n1. Split this w into the bias w0 and the rest of the weights, and return them.","metadata":{}},{"cell_type":"code","source":"def train_linear_regression(X, y):\n    \n    # adding the dummy column\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X]) \n    \n    # normal equation formula\n    XT_X = X.T @ X \n    XT_X_inv = np.linalg.inv(XT_X) \n    w = XT_X_inv @ X.T @ y \n    \n    return w[0], w[1:]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:15.793947Z","iopub.execute_input":"2022-02-22T18:39:15.794435Z","iopub.status.idle":"2022-02-22T18:39:15.804732Z","shell.execute_reply.started":"2022-02-22T18:39:15.794406Z","shell.execute_reply":"2022-02-22T18:39:15.804064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predicting the sales\n\nWe now have a function for training a linear regression model at our disposal,\n\nso let’s use it to build a simple baseline solution.","metadata":{}},{"cell_type":"markdown","source":"## Simple feature engineering\n\n### Working with 'Store'","metadata":{}},{"cell_type":"code","source":"print(df.Store.nunique())","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:15.80626Z","iopub.execute_input":"2022-02-22T18:39:15.807001Z","iopub.status.idle":"2022-02-22T18:39:15.865578Z","shell.execute_reply.started":"2022-02-22T18:39:15.806959Z","shell.execute_reply":"2022-02-22T18:39:15.864911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**'Store'** is categorical variable with 1115 different values.\n\nInstead of doing On Hot encoding I propose to substitute the Average Sales for each Store","metadata":{}},{"cell_type":"code","source":"tmp = df_train[['Store', 'Sales']].groupby(['Store']).mean()\ndf_train = df_train.merge(tmp, on=[\"Store\"], how=\"inner\")\ndf_train.rename(columns={'Sales_x':'Sales', 'Sales_y':'Sales_Avg'}, inplace=True)\n\ndf_val = df_val.merge(tmp, on=[\"Store\"], how=\"left\")\ndf_val.rename(columns={'Sales_x':'Sales', 'Sales_y':'Sales_Avg'}, inplace=True)\n\ndf_test = df_test.merge(tmp, on=[\"Store\"], how=\"left\")\ndf_test.rename(columns={'Sales_x':'Sales', 'Sales_y':'Sales_Avg'}, inplace=True)\n\ndf_submit = pd.read_csv(\"/kaggle/input/rossmann-store-sales/test.csv\", parse_dates=['Date'],\n                 dtype={\n                           'Store': str,\n                           'DayOfWeek': str,\n                           'StateHoliday': str,\n                       })\ndf_submit = df_submit.merge(tmp, on=[\"Store\"], how=\"left\")\ndf_submit.rename(columns={'Sales':'Sales_Avg'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:15.866978Z","iopub.execute_input":"2022-02-22T18:39:15.867205Z","iopub.status.idle":"2022-02-22T18:39:16.387941Z","shell.execute_reply.started":"2022-02-22T18:39:15.867177Z","shell.execute_reply":"2022-02-22T18:39:16.387311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Working with 'Date'","metadata":{}},{"cell_type":"code","source":"print(df_train.Date.min())","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:16.39051Z","iopub.execute_input":"2022-02-22T18:39:16.390874Z","iopub.status.idle":"2022-02-22T18:39:16.397585Z","shell.execute_reply.started":"2022-02-22T18:39:16.390831Z","shell.execute_reply":"2022-02-22T18:39:16.39676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nLet’s create additional features that will help our model learn patterns better. \n\nWe will create the **week number**, **month**, **day**, **quarter**, and **year** as features from the date variable.\n\nSimilarly, since we are already creating time-related features, we can add a new feature based on climate and seasons.\n\nConsidering that the stores are in Europe, we can refer to the standard season cycles and create a new **season** feature with values of Spring, Summer, Fall, and Winter.\n\nLets’ take ‘2013-01-01’ as referential of time and define Days as our numerical variable","metadata":{}},{"cell_type":"code","source":"df_train['Age_Day'] = (df_train.Date - df_train.Date.min()).dt.days\ndf_val['Age_Day'] = (df_val.Date - df_train.Date.min()).dt.days\ndf_test['Age_Day'] = (df_test.Date - df_train.Date.min()).dt.days\ndf_submit['Age_Day'] = (df_submit.Date - df_train.Date.min()).dt.days","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:16.399016Z","iopub.execute_input":"2022-02-22T18:39:16.399529Z","iopub.status.idle":"2022-02-22T18:39:16.449607Z","shell.execute_reply.started":"2022-02-22T18:39:16.39949Z","shell.execute_reply":"2022-02-22T18:39:16.448622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"Month\"] = df_train[\"Date\"].dt.month\ndf_train[\"Quarter\"] = df_train[\"Date\"].dt.quarter\ndf_train[\"Year\"] = df_train[\"Date\"].dt.year\ndf_train[\"Day\"] = df_train[\"Date\"].dt.day\ndf_train[\"Week\"] = df_train[\"Date\"].dt.isocalendar().week\n\ndf_train[\"Season\"] = np.where(df_train[\"Month\"].isin([3,4,5]),\"Spring\",\n                            np.where(df_train[\"Month\"].isin([6,7,8]),\"Summer\",\n                                     np.where(df_train[\"Month\"].isin([9,10,11]),\"Fall\",\n                                              np.where(df_train[\"Month\"].isin([12,1,2]),\"Winter\",\"None\"))))\n\ndf_train[[\"Date\",\"Year\",\"Month\",\"Day\",\"Week\",\"Quarter\",\"Season\"]].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:16.450941Z","iopub.execute_input":"2022-02-22T18:39:16.451329Z","iopub.status.idle":"2022-02-22T18:39:17.176026Z","shell.execute_reply.started":"2022-02-22T18:39:16.451281Z","shell.execute_reply":"2022-02-22T18:39:17.174787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val[\"Month\"] = df_val[\"Date\"].dt.month\ndf_val[\"Quarter\"] = df_val[\"Date\"].dt.quarter\ndf_val[\"Year\"] = df_val[\"Date\"].dt.year\ndf_val[\"Day\"] = df_val[\"Date\"].dt.day\ndf_val[\"Week\"] = df_val[\"Date\"].dt.isocalendar().week\n\ndf_val[\"Season\"] = np.where(df_val[\"Month\"].isin([3,4,5]),\"Spring\",\n                            np.where(df_val[\"Month\"].isin([6,7,8]),\"Summer\",\n                                     np.where(df_val[\"Month\"].isin([9,10,11]),\"Fall\",\n                                              np.where(df_val[\"Month\"].isin([12,1,2]),\"Winter\",\"None\"))))\n\ndf_val[[\"Date\",\"Year\",\"Month\",\"Day\",\"Week\",\"Quarter\",\"Season\"]].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:17.177451Z","iopub.execute_input":"2022-02-22T18:39:17.177836Z","iopub.status.idle":"2022-02-22T18:39:17.344321Z","shell.execute_reply.started":"2022-02-22T18:39:17.177794Z","shell.execute_reply":"2022-02-22T18:39:17.343399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[\"Month\"] = df_test[\"Date\"].dt.month\ndf_test[\"Quarter\"] = df_test[\"Date\"].dt.quarter\ndf_test[\"Year\"] = df_test[\"Date\"].dt.year\ndf_test[\"Day\"] = df_test[\"Date\"].dt.day\ndf_test[\"Week\"] = df_test[\"Date\"].dt.isocalendar().week\n\ndf_test[\"Season\"] = np.where(df_test[\"Month\"].isin([3,4,5]),\"Spring\",\n                            np.where(df_test[\"Month\"].isin([6,7,8]),\"Summer\",\n                                     np.where(df_test[\"Month\"].isin([9,10,11]),\"Fall\",\n                                              np.where(df_test[\"Month\"].isin([12,1,2]),\"Winter\",\"None\"))))\n\ndf_test[[\"Date\",\"Year\",\"Month\",\"Day\",\"Week\",\"Quarter\",\"Season\"]].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:17.345476Z","iopub.execute_input":"2022-02-22T18:39:17.345691Z","iopub.status.idle":"2022-02-22T18:39:17.509734Z","shell.execute_reply.started":"2022-02-22T18:39:17.345664Z","shell.execute_reply":"2022-02-22T18:39:17.508641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit[\"Month\"] = df_submit[\"Date\"].dt.month\ndf_submit[\"Quarter\"] = df_submit[\"Date\"].dt.quarter\ndf_submit[\"Year\"] = df_submit[\"Date\"].dt.year\ndf_submit[\"Day\"] = df_submit[\"Date\"].dt.day\ndf_submit[\"Week\"] = df_submit[\"Date\"].dt.isocalendar().week\n\ndf_submit[\"Season\"] = np.where(df_submit[\"Month\"].isin([3,4,5]),\"Spring\",\n                            np.where(df_submit[\"Month\"].isin([6,7,8]),\"Summer\",\n                                     np.where(df_submit[\"Month\"].isin([9,10,11]),\"Fall\",\n                                              np.where(df_submit[\"Month\"].isin([12,1,2]),\"Winter\",\"None\"))))\n\ndf_submit[[\"Date\",\"Year\",\"Month\",\"Day\",\"Week\",\"Quarter\",\"Season\"]].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:17.511283Z","iopub.execute_input":"2022-02-22T18:39:17.511536Z","iopub.status.idle":"2022-02-22T18:39:17.573617Z","shell.execute_reply.started":"2022-02-22T18:39:17.511508Z","shell.execute_reply":"2022-02-22T18:39:17.572362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Baseline solution\n\nWe will start with a very naive way of creating features:\n\nselect a few numerical features, and form the matrix X from them\n\nThis time, we include a couple more features and use the following columns:\n\n    * Sales_Avg\n    * Age_Day\n    * SchoolHoliday\n    * Promo","metadata":{}},{"cell_type":"code","source":"base = ['Sales_Avg', 'Age_Day', 'Promo', 'SchoolHoliday']\ndf_num = df_train[base]\n\nX_train = df_num.values \nprint(X_train.shape, y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:17.574897Z","iopub.execute_input":"2022-02-22T18:39:17.575171Z","iopub.status.idle":"2022-02-22T18:39:17.59622Z","shell.execute_reply.started":"2022-02-22T18:39:17.575144Z","shell.execute_reply":"2022-02-22T18:39:17.595213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w_0, w = train_linear_regression(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:17.597293Z","iopub.execute_input":"2022-02-22T18:39:17.597503Z","iopub.status.idle":"2022-02-22T18:39:17.641121Z","shell.execute_reply.started":"2022-02-22T18:39:17.597479Z","shell.execute_reply":"2022-02-22T18:39:17.639917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have just trained the first model! \n\nNow we can apply it to the training data to see how well it predicts","metadata":{}},{"cell_type":"code","source":"y_pred  = w_0 + X_train @ w","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:17.646904Z","iopub.execute_input":"2022-02-22T18:39:17.6477Z","iopub.status.idle":"2022-02-22T18:39:17.660035Z","shell.execute_reply.started":"2022-02-22T18:39:17.647646Z","shell.execute_reply":"2022-02-22T18:39:17.658696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To see how good the predictions are, we can use histplot to plot the predicted values and compare them with the actual prices","metadata":{}},{"cell_type":"code","source":"sns.histplot(y_pred, label='prediction', color='red')\nsns.histplot(y_train, label='target', color= 'green')\nplt.title(\"Training Predictions vs actual distribution\")\nplt.xlabel(\"$Log(Sales + 1)$\")\nplt.xlim(6, 11)\nax = plt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:17.662034Z","iopub.execute_input":"2022-02-22T18:39:17.662566Z","iopub.status.idle":"2022-02-22T18:39:20.167931Z","shell.execute_reply.started":"2022-02-22T18:39:17.662522Z","shell.execute_reply":"2022-02-22T18:39:20.167025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### $RMSE$: Evaluating model quality","metadata":{}},{"cell_type":"code","source":"def rmse(y, y_pred):\n    error = y - y_pred\n    mse = (error ** 2).mean()\n    return np.sqrt(mse)\n\nprint(f\"Now we can use RMSE = {rmse(y_train, y_pred):.3f} to evaluate the quality of the model.\")","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:20.169436Z","iopub.execute_input":"2022-02-22T18:39:20.169713Z","iopub.status.idle":"2022-02-22T18:39:20.178384Z","shell.execute_reply.started":"2022-02-22T18:39:20.169685Z","shell.execute_reply":"2022-02-22T18:39:20.176937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code prints 0.425. This number tells us that on average, the model’s predictions are off by 0.425. \n\nThis result alone may not be very useful, but we can use it to compare this model with other models. \n\nIf one model has a better (lower) RMSE than the other, it indicates that model is better","metadata":{}},{"cell_type":"markdown","source":"#### Validating the model\n\nFirst, we create the X_val matrix, following the same steps as for X_train:","metadata":{}},{"cell_type":"code","source":"df_num = df_val[base]\nX_val = df_num.values","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:20.179905Z","iopub.execute_input":"2022-02-22T18:39:20.180652Z","iopub.status.idle":"2022-02-22T18:39:20.194233Z","shell.execute_reply.started":"2022-02-22T18:39:20.180615Z","shell.execute_reply":"2022-02-22T18:39:20.192843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We’re ready to apply the model to X_val to get predictions:","metadata":{}},{"cell_type":"code","source":"y_pred  = w_0 + X_val @ w\n\nprint(f\"Now we can (on the evalation dataset) use RMSE = {rmse(y_val, y_pred):.3f} to evaluate the quality of the model.\")","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:20.195805Z","iopub.execute_input":"2022-02-22T18:39:20.196139Z","iopub.status.idle":"2022-02-22T18:39:20.215357Z","shell.execute_reply.started":"2022-02-22T18:39:20.196094Z","shell.execute_reply":"2022-02-22T18:39:20.214114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering\n\n### Handling categorical variables\n\nCategorical variables describe characteristics of objects and can take one of a few possible values.\n\nThe day of week, for example, is categorical: it can take only one of the three possible values (1, 2, 3, 4, 5, 6, and 7).\n\nThis method of encoding categorical variables is called **one-hot encoding**\n\nLet’s test if adding more features leads to any improvements:\n* **'DayOfWeek**', \n* **'StateHoliday'**,\n* **'Month'**, \n* **'Quarter'**, \n* **'Year'**, \n* **'Day'**, \n* **'Week'**, \n* **'Season'** ","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(df.DayOfWeek.value_counts()).T","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:20.216953Z","iopub.execute_input":"2022-02-22T18:39:20.218764Z","iopub.status.idle":"2022-02-22T18:39:20.316426Z","shell.execute_reply.started":"2022-02-22T18:39:20.218712Z","shell.execute_reply":"2022-02-22T18:39:20.315503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_X(df):\n    df = df.copy() \n    features = base.copy()\n    \n    for v in [1, 2, 3, 4, 6]:\n        feature = f'day_of_week_{v}'\n        value = (df['DayOfWeek'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in ['Spring', 'Summer', 'Winter']:\n        feature = f'is_{v}'\n        value = (df['Season'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in np.arange(1,31):\n        feature = f'is_Day_{v}'\n        value = (df['Day'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in np.arange(1,52):\n        feature = f'is_Week_{v}'\n        value = (df['Week'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in [1, 2, 3]:\n        feature = f'is_Quarter_{v}'\n        value = (df['Quarter'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in [2013, 2014]:\n        feature = f'is_Year_{v}'\n        value = (df['Year'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in ['0', 'a', 'b']:\n        feature = f'is_StateHoliday_{v}'\n        value = (df['StateHoliday'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in [1, 2, 3, 4, 6, 7, 8, 9, 10, 11]:\n        feature = f'is_Month_{v}'\n        value = (df['Month'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:20.317872Z","iopub.execute_input":"2022-02-22T18:39:20.318567Z","iopub.status.idle":"2022-02-22T18:39:20.336575Z","shell.execute_reply.started":"2022-02-22T18:39:20.318517Z","shell.execute_reply":"2022-02-22T18:39:20.335945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train = prepare_X(df_train)\n# w_0, w = train_linear_regression(X_train, y_train)\n# X_val = prepare_X(df_val)\n# y_pred = w_0 + X_val @ w\n# print(f'validation: {rmse(y_val, y_pred):.3f}.') ","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:20.337666Z","iopub.execute_input":"2022-02-22T18:39:20.337916Z","iopub.status.idle":"2022-02-22T18:39:20.349479Z","shell.execute_reply.started":"2022-02-22T18:39:20.337879Z","shell.execute_reply":"2022-02-22T18:39:20.348576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> X_train = prepare_X(df_train) \n>  \n> w_0, w = train_linear_regression(X_train, y_train) \n>  \n> X_val = prepare_X(df_val) \n>  \n> y_pred = w_0 + X_val @ w \n>  \n> print(f'validation: {rmse(y_val, y_pred):.3f}.') \n\nNow we have **LinAlgError: Singular matrix**","metadata":{"execution":{"iopub.status.busy":"2022-02-21T20:54:18.427052Z","iopub.execute_input":"2022-02-21T20:54:18.427888Z","iopub.status.idle":"2022-02-21T20:54:21.935065Z","shell.execute_reply.started":"2022-02-21T20:54:18.427845Z","shell.execute_reply":"2022-02-21T20:54:21.933868Z"}}},{"cell_type":"markdown","source":"#### Regularization\n\nRegularization is an important concept in machine learning: it means “controlling” \n\n    controlling the weights of the model so that they behave correctly and don’t grow too large, as in our case.","metadata":{}},{"cell_type":"code","source":"w_0, max(w[1:]), min(w[1:])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:20.350945Z","iopub.execute_input":"2022-02-22T18:39:20.35125Z","iopub.status.idle":"2022-02-22T18:39:20.365969Z","shell.execute_reply.started":"2022-02-22T18:39:20.351212Z","shell.execute_reply":"2022-02-22T18:39:20.365291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear regression with regularization\ndef train_linear_regression_reg(X, y, r=0.0): \n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n    XTX = X.T @X\n    \n    reg = r * np.eye(XTX.shape[0]) \n    XTX = XTX + reg \n    XTX_inv = np.linalg.inv(XTX)\n    \n    w = XTX_inv @ X.T @ y\n    return w[0], w[1:]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:20.367168Z","iopub.execute_input":"2022-02-22T18:39:20.367563Z","iopub.status.idle":"2022-02-22T18:39:20.381317Z","shell.execute_reply.started":"2022-02-22T18:39:20.367533Z","shell.execute_reply":"2022-02-22T18:39:20.380015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s check what happens with our weights for different values of r:","metadata":{}},{"cell_type":"code","source":"for r in [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]:\n    w_0, w = train_linear_regression_reg(X_train, y_train, r=r)\n    print('%5s, %.2f, %.2f, %.2f' % (r, w_0, max(w[1:]), min(w[1:])))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:20.383124Z","iopub.execute_input":"2022-02-22T18:39:20.383416Z","iopub.status.idle":"2022-02-22T18:39:20.640401Z","shell.execute_reply.started":"2022-02-22T18:39:20.383382Z","shell.execute_reply":"2022-02-22T18:39:20.639422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the values that we selected become smaller as r grows. \n \nNow let’s check whether regularization helps with our problem and what RMSE we get after that. \n\nLet’s run it with r=1e4:","metadata":{}},{"cell_type":"code","source":"X_train = prepare_X(df_train) \nw_0, w = train_linear_regression_reg(X_train, y_train, r=10000)\n\nX_val = prepare_X(df_val) \ny_pred = w_0 + X_val @ w \nprint(f\"Now we can (on the evalation dataset) use RMSE = {rmse(y_val, y_pred):.3f} to evaluate the quality of the model.\")","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:20.642052Z","iopub.execute_input":"2022-02-22T18:39:20.642576Z","iopub.status.idle":"2022-02-22T18:39:24.054956Z","shell.execute_reply.started":"2022-02-22T18:39:20.642527Z","shell.execute_reply":"2022-02-22T18:39:24.054027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This result is an improvement over the previous score: 0.425. \n\nLet’s try a couple of different ones to select the best parameter r:","metadata":{}},{"cell_type":"code","source":"#np.arange(9000, 31001, 1000): # [0.01, 0.1, 1, 5, 10, 100, 1000, 10000, 11000, 12000, 13000, 14000, 15000]:\nX_train = prepare_X(df_train)\nX_val = prepare_X(df_val)\n\nfor r in np.arange(21000, 23001, 200):\n    w_0, w = train_linear_regression_reg(X_train, y_train, r=r)\n    y_pred = w_0 + X_val @ w\n    print('%6s' %r, rmse(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:24.056658Z","iopub.execute_input":"2022-02-22T18:39:24.0572Z","iopub.status.idle":"2022-02-22T18:39:40.333574Z","shell.execute_reply.started":"2022-02-22T18:39:24.057147Z","shell.execute_reply":"2022-02-22T18:39:40.332237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We also notice that the performance for values below 22400 don’t change much except in the sixth digit, \nwhich we shouldn’t consider to be significant. \n \nLet’s take the model with r=22400 as the final model. \n\nNow we can check it against the test dataset to verify if the model works:","metadata":{}},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression_reg(X_train, y_train, r=22400)\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val @ w\nprint(f'validation: {rmse(y_val, y_pred):.3f}')\n\nX_test = prepare_X(df_test)\ny_pred = w_0 + X_test @ w\nprint(f'test: {rmse(y_test, y_pred):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:40:15.973558Z","iopub.execute_input":"2022-02-22T18:40:15.974074Z","iopub.status.idle":"2022-02-22T18:40:20.066693Z","shell.execute_reply.started":"2022-02-22T18:40:15.974023Z","shell.execute_reply":"2022-02-22T18:40:20.063769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because these two numbers are pretty close,\n\nWe conclude that the model can generalize well to the new unseen data.","metadata":{}},{"cell_type":"markdown","source":"## Using the model","metadata":{}},{"cell_type":"markdown","source":"Submit has 11 NULL value for ‘Open’ feature, let’s fill it with the mode.","metadata":{}},{"cell_type":"code","source":"df_submit.Open.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:41:36.113009Z","iopub.execute_input":"2022-02-22T18:41:36.113319Z","iopub.status.idle":"2022-02-22T18:41:36.121485Z","shell.execute_reply.started":"2022-02-22T18:41:36.113285Z","shell.execute_reply":"2022-02-22T18:41:36.120629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit['Open'] = df_submit[['Open']].fillna(1.0)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:41:46.414572Z","iopub.execute_input":"2022-02-22T18:41:46.41507Z","iopub.status.idle":"2022-02-22T18:41:46.420989Z","shell.execute_reply.started":"2022-02-22T18:41:46.415022Z","shell.execute_reply":"2022-02-22T18:41:46.420136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit.Open.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:41:50.311844Z","iopub.execute_input":"2022-02-22T18:41:50.312639Z","iopub.status.idle":"2022-02-22T18:41:50.319581Z","shell.execute_reply.started":"2022-02-22T18:41:50.312602Z","shell.execute_reply":"2022-02-22T18:41:50.318765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Close_df = df_submit.groupby(['Open']).get_group(0)\nClose_df['Sales_pred'] = 0.0\n\nOpen_df = df_submit.groupby(['Open']).get_group(1)\nX_submit = prepare_X(Open_df)\ny_pred = w_0 + X_submit @ w\nOpen_df['Sales_pred'] = np.expm1(y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:42:12.081334Z","iopub.execute_input":"2022-02-22T18:42:12.081769Z","iopub.status.idle":"2022-02-22T18:42:12.261793Z","shell.execute_reply.started":"2022-02-22T18:42:12.081739Z","shell.execute_reply":"2022-02-22T18:42:12.260683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission = pd.concat([Close_df[['Id', 'Sales_pred']], Open_df[['Id', 'Sales_pred']]]).sort_index()\nmy_submission.rename(columns={'Sales_pred':'Sales'}, inplace=True)\nmy_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:42:17.10864Z","iopub.execute_input":"2022-02-22T18:42:17.108924Z","iopub.status.idle":"2022-02-22T18:42:17.215224Z","shell.execute_reply.started":"2022-02-22T18:42:17.10889Z","shell.execute_reply":"2022-02-22T18:42:17.214147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looking at the Store Data\n\nLet’s now look at the store csv file and if I can improve my model with this new additional features.","metadata":{}},{"cell_type":"code","source":"store_df = pd.read_csv(\"/kaggle/input/rossmann-store-sales/store.csv\",\n                       dtype={\n                           'Store': str\n                       })\nprint(\"Shape of the Dataset:\",store_df.shape)\nstore_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Dictionary\n    \n* **StoreType:** differentiates between four different store models: a, b, c, d\n* **Assortment:** describes an assortment level: a = basic, b = extra, c = extended,\n* **CompetitionDistance:** distance in meters to the nearest competitor store,\n* **CompetitionOpenSince[Month/Year]:** gives the approximate year and month of the time the nearest competitor was opened,\n* **Promo2:** Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n* **Promo2Since[Year/Week]:** describes the year and calendar week when the store started participating in Promo2\n* **PromoInterval:** describes the consecutive intervals at which Promo2 is started, naming the months the promotion is started anew \n\n    (e.g., “Feb, May, Aug, Nov” means each round starts in February, May, August, and November of any given year for that store)","metadata":{}},{"cell_type":"markdown","source":"## Finding Data Types","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(store_df.dtypes, columns=['Type']).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Duplicated Store","metadata":{}},{"cell_type":"code","source":"store_df[store_df.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Null values\n\nOne important aspect: is there any missing data in the dataset? \n\nLet’s have a look at the number of missing data points in each column (if any) in its associated percentage form.","metadata":{}},{"cell_type":"code","source":"tmp = pd.DataFrame(store_df.isnull().sum()/store_df.shape[0] * 100, columns=['Null_Values(%)']).round(2)\ntmp[tmp['Null_Values(%)'] > 0.0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'CompetitionOpenSinceMonth', and 'CompetitionOpenSinceYear' have over 30% null values.\n\nThis is a big loss and there is nothing much we can do to fix this.\n\nAs a rule of thumb, if there is a loss of anything between 0% and 10%, we can make a few attempts to fill the missing points and use the feature. \n\nBut, 30% technically becomes beyond the usable range.\n\nOn the other hand, we can see 'CompetitionDistance' has around 0.27% missing values. This would much easier to handle and fix.\n\nWe will use the mode to fill in the gaps where we have missing values.","metadata":{}},{"cell_type":"code","source":"store_df[\"CompetitionDistance\"].fillna(store_df[\"CompetitionDistance\"].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_df.drop(labels=['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(store_df.dtypes, columns=['Type']).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.merge(store_df, on=[\"Store\"], how=\"left\")\ndf_val = df_val.merge(store_df, on=[\"Store\"], how=\"left\")\ndf_test = df_test.merge(store_df, on=[\"Store\"], how=\"left\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s test if adding more features leads to any improvements:\n\nWe beging by adding **'CompetitionDistance'** and **'Promo2'** feature.","metadata":{}},{"cell_type":"code","source":"base.append('CompetitionDistance')\nbase.append('Promo2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression_reg(X_train, y_train, r=10000)\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val @ w\nprint(f'validation: {rmse(y_val, y_pred):.3f} Same as before')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time the last 2 categorical variable; **'Assortment'** and **'StoreType'**","metadata":{}},{"cell_type":"code","source":"def prepare_X(df):\n    df = df.copy() \n    features = base.copy()\n    \n    for v in ['a', 'b', 'c']:\n        feature = f'is_Assortment_{v}'\n        value = (df['Assortment'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in ['a', 'b', 'c', 'd']:\n        feature = f'is_StoreType_{v}'\n        value = (df['StoreType'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in [1, 2, 3, 4, 6, 7]:\n        feature = f'day_of_week_{v}'\n        value = (df['DayOfWeek'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in ['Spring', 'Summer', 'Winter', 'Fall']:\n        feature = f'is_{v}'\n        value = (df['Season'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in np.arange(1,32):\n        feature = f'is_Day_{v}'\n        value = (df['Day'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in np.arange(1,53):\n        feature = f'is_Week_{v}'\n        value = (df['Week'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in [1, 2, 3, 4]:\n        feature = f'is_Quarter_{v}'\n        value = (df['Quarter'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in [2013, 2014, 2015]:\n        feature = f'is_Year_{v}'\n        value = (df['Year'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in ['0', 'a', 'b', 'c']:\n        feature = f'is_StateHoliday_{v}'\n        value = (df['StateHoliday'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    for v in [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12]:\n        feature = f'is_Month_{v}'\n        value = (df['Month'] == v).astype(int) \n        df[feature] = value\n        features.append(feature)\n    \n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression_reg(X_train, y_train, r=10000)\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val @ w\nprint(f'validation: {rmse(y_val, y_pred):.3f} Same as before')\n\nX_test = prepare_X(df_test)\ny_pred = w_0 + X_test @ w\nprint(f'test: {rmse(y_test, y_pred):.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Because these two Model have RMSE  pretty close, we conclude that using normal equation is not enough for dealing with non-linearity","metadata":{}},{"cell_type":"code","source":"df_submit = df_submit.merge(store_df, on=[\"Store\"], how=\"left\")\n\nClose_df = df_submit.groupby(['Open']).get_group(0)\nClose_df['Sales_pred'] = 0.0\n\nOpen_df = df_submit.groupby(['Open']).get_group(1)\nX_submit = prepare_X(Open_df)\ny_pred = w_0 + X_submit @ w\nOpen_df['Sales_pred'] = np.expm1(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission = pd.concat([Close_df[['Id', 'Sales_pred']], Open_df[['Id', 'Sales_pred']]]).sort_index()\nmy_submission.rename(columns={'Sales_pred':'Sales'}, inplace=True)\nmy_submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}