{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9ba717b-89ae-c45a-4eb0-a25d04450774"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style()  \npd.set_option('display.max_columns', 100)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"92ecac41-e415-9986-cf8a-858fdd0486e6"},"outputs":[],"source":"def read_files(train_fn, test_fn, store_fn):\n    train = pd.read_csv(train_fn, parse_dates = ['Date'], infer_datetime_format = True)\n    test = pd.read_csv(test_fn, parse_dates = ['Date'], infer_datetime_format = True)\n    store = pd.read_csv(store_fn)\n    \n    return train, test, store"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35307f5d-fb77-1d1f-c4a4-b73be8e55ab0"},"outputs":[],"source":"train, test, store = read_files('../input/train.csv', '../input/test.csv', '../input/store.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ccb05ab7-59d2-f4eb-3b5e-48233ef3fa10"},"outputs":[],"source":"train.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4ad7c476-7fba-10f2-d63b-cf235b1c0631"},"source":"**Check the distribution of Sales and Customers.**\n\nFilter only when the store is open"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8e646d0-e43d-9322-d545-37e20032dff5"},"outputs":[],"source":"train.query('Open == 1')[['Sales', 'Customers']].hist(bins=100, figsize=(10,4), xrot=45, sharey=True);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01c11b6e-6bf9-248f-9597-f18b958f66a1"},"outputs":[],"source":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharey=True)\n\nopen_df = train.query('Open == 1')[['Sales','DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday']]\nsns.violinplot('DayOfWeek', 'Sales', data=open_df, ax=ax1)\nsns.violinplot('Promo', 'Sales', data=open_df, ax=ax2)\nsns.violinplot('StateHoliday', 'Sales', data=open_df, ax=ax3)\nsns.violinplot('SchoolHoliday', 'Sales', data=open_df, ax=ax4)\n\nfig.set_size_inches(15,6)\nfig.tight_layout()\nfig.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5cda697f-e3a8-674e-945b-4e5cc685cd53"},"outputs":[],"source":"train.groupby('Date')['Store'].size().plot(kind='line');"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0884c743-a722-689b-e3c7-c28cb3f7566b"},"outputs":[],"source":"**There are days in the training set where some stores are missing. Lets add those missing stores with Open flag as '0' and Customers and Sales values as '0'**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ff5d711-f2a5-35ae-2a75-ce156e20b4cb"},"outputs":[],"source":"def add_missing_dates(train, all_stores):\n    train_m = pd.DataFrame()\n    store_by_date = train.groupby('Date')['Store'].nunique().reset_index()\n    for i in store_by_date.query('Store != 1115')['Date']:\n        diff_stores = all_stores.difference(set(train[train['Date']==i].Store))\n        s = list(diff_stores)\n        missing = pd.DataFrame(data={\n                                 'Date': [i]*len(s), \n                                 'Store': s, \n                                 'Customers': [0]*len(s),\n                                 'Sales': [0]*len(s),\n                                 'Open': [0]*len(s),\n                                 'Promo': [0]*len(s),\n                                 'SchoolHoliday': [0]*len(s),\n                                 'StateHoliday': ['0']*len(s)\n                                }) \n    \n        train_m = train_m.append(missing)\n        train_m['DayOfWeek'] = train_m.Date.dt.dayofweek+1 \n    \n    return train_m[['Store','DayOfWeek','Date','Sales','Customers','Open','Promo','StateHoliday','SchoolHoliday']]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b26eda9-299f-3095-cd06-a2e186dce156"},"outputs":[],"source":"missing_train = add_missing_dates(train, set(store.Store))\ntrain = pd.concat([train, missing_train])\ntrain.groupby('Date')['Store'].size().plot(kind='line');"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3bb9d654-0bc2-8960-378f-9bf780db7e1b"},"outputs":[],"source":"fig = plt.figure(figsize=(8,4))\n\nrand_stores = np.random.randint(0,1115, 20)\nfor i in rand_stores:\n    data = train[(train[\"Store\"] == i)][['Date','Customers']]\n    plt.plot_date(data.Date, data.Customers,'-', alpha=0.5)\n\nfig.autofmt_xdate() "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b08239b-8f54-ad9b-a647-a68556f84fe1"},"outputs":[],"source":"\nsp = train[['Store','Date','Sales']]\nsales_pivot = sp.pivot(index='Store', columns='Date', values='Sales')\ncol_names = ['col_%d' % i for i in range(len(sales_pivot.columns))]\nsales_pivot.columns = col_names\n\n\ncp = train[['Store','Date','Customers']]\ncustomers_pivot = cp.pivot(index='Store', columns='Date', values='Customers')\ncustomers_pivot.columns = col_names\ncustomers_pivot.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be9665ff-7b11-1a4d-495c-a27adfb22044"},"outputs":[],"source":"from sklearn import cluster\n\ncl = cluster.hierarchical.AgglomerativeClustering(n_clusters=5)\nclusters = cl.fit_predict(X=customers_pivot)\n\ncl_df = pd.DataFrame({'Store' : customers_pivot.index, 'cluster': clusters}).set_index('Store')\nchart_df = train.set_index('Store').join(cl_df)[['Date', 'Customers', 'cluster']]\nchart_df.cluster.value_counts().sort_index()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4165d96e-1ff2-8325-6002-5bf1c77a45cf"},"outputs":[],"source":"\ndef plot_clusters(df, clu, xcol='Date', ycol='Customers',cluster_label='cluster'):\n    fig, axes = plt.subplots(1, cl.n_clusters, sharex=True, sharey=True)\n    \n    for ax, l in zip(axes, np.unique(cl.labels_)):\n        tdf = df.where(df[cluster_label] == l).dropna()\n        \n        for i in tdf.index.unique():\n            data = tdf.loc[i].set_index(xcol).resample('m').agg({ycol: 'sum', cluster_label:'max'}).reset_index()\n            \n            ax.plot_date(x=data[xcol], \n                         y=data[ycol], \n                         linestyle='solid', \n                         xdate=True, \n                         ydate=False, \n                         alpha=0.6)\n\n    \n    fig.set_size_inches(10,4)\n    fig.tight_layout()\n    fig.autofmt_xdate()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a06955e4-0abc-be2b-836d-2d0460731bd1"},"outputs":[],"source":"plot_clusters(chart_df, clu=cl)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9a909695-c2e4-fe7f-e0ee-ac9250887613"},"source":"**There is a clear split of clusters depending on the trend of the Customers feature.\nTODO: do some more analysis on each cluster**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a92c5d7-f832-dbc1-32ed-1de9f121085d"},"outputs":[],"source":"test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22d30de2-006e-0d36-a2f1-71e62a486d80"},"outputs":[],"source":"print (\"Train features:\")\nprint (train.isnull().any())\nprint (\"\\n\")\nprint (\"Test features:\")\nprint (test.isnull().any())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fab15ef6-7d36-6a12-a2ff-b8a3a9a74c44"},"outputs":[],"source":"print (\"Train features:\")\nprint (train.StateHoliday.value_counts())\nprint (\"\\n\")\nprint (train.SchoolHoliday.value_counts())\nprint (\"\\n\")\nprint (train.Promo.value_counts())\nprint (\"\\n\")\nprint (\"Test features:\")\nprint (test.StateHoliday.value_counts())\nprint (\"\\n\")\nprint (test.SchoolHoliday.value_counts())\nprint (\"\\n\")\nprint (test.Promo.value_counts())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"989f9699-1509-4784-5b47-cdd3bcb45427"},"outputs":[],"source":"test[test.Open.isnull()].DayOfWeek.value_counts().sort_index()"},{"cell_type":"markdown","metadata":{"_cell_guid":"62eeee70-5a47-e6b5-115c-8e650496e884"},"source":"Open is NULL on all weekdays except Sunday and on dates which are not holiday, so i assume the store is open. Also i will split the date feature in [year, month, day, weekofyear] features. There are two ['0'] in train.StateHoliday so merge that as well"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2b14958-bad0-71e6-ea8e-e91be63dc547"},"outputs":[],"source":"def split_date(df, date_col):\n    n_date_year = df[date_col].dt.year\n    n_date_month = df[date_col].dt.month\n    n_date_weeknum = df[date_col].dt.weekofyear\n    n_date_day = df[date_col].dt.day\n    \n    return df.assign(date_year=n_date_year, date_month=n_date_month, date_weeknum=n_date_weeknum, date_day=n_date_day)\n \ntrain = split_date(train, 'Date')\ntest = split_date(test, 'Date')\n\ntrain.StateHoliday = train.StateHoliday.map({'0':'0', 'a':'1', 'b': '2', 'c':'3'})\ntest.StateHoliday  = test.StateHoliday.map({'0':'0', 'a':'1', 'b': '2', 'c':'3'})\ntest.Open = test.Open.fillna(1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd295b0a-b932-d7bb-8551-1266317c705f"},"outputs":[],"source":"store.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"791a09eb-74d8-14a7-d3b0-97544e9f6c4a"},"outputs":[],"source":"store.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dcc189f5-f2f1-3edc-5598-611896ac9829"},"outputs":[],"source":"def myPinterval(x):\n    if x=='Feb,May,Aug,Nov':  return([0,1,0,0,1,0,0,1,0,0,1,0])\n    elif x=='Jan,Apr,Jul,Oct':  return([1,0,0,1,0,0,1,0,0,1,0,0])\n    elif x== 'Mar,Jun,Sept,Dec': return([0,0,1,0,0,1,0,0,1,0,0,1])\n    else: return(np.repeat(0,12).tolist())\n\n#Convert the Promointerval from a string column to a set of columns with flag [0/1]\nproInt = store.PromoInterval.apply(myPinterval).tolist()\nproInt = pd.DataFrame(proInt, columns = ['ProInt'+ str(i) for i in range(1,13)] , dtype=np.int8)\nstore = store.drop('PromoInterval',1).join(proInt)\n\n#Fill NA with the median CompetitionDistance, TODO: look for a better solution\ndistmean = store.CompetitionDistance.median()\nstore.CompetitionDistance = store.CompetitionDistance.fillna(distmean) \n\n##Convert CompetitionOpenSince to a date field and set NA = 1970/01/01\nstore['CompetitionOpenSinceDay'] = 1\nstore['CompetitionOpenSinceDT'] = pd.to_datetime(dict(year=store.CompetitionOpenSinceYear, month=store.CompetitionOpenSinceMonth, day=store.CompetitionOpenSinceDay))\nstore = store.drop(['CompetitionOpenSinceYear','CompetitionOpenSinceMonth','CompetitionOpenSinceDay'], axis='columns')\nifnulldt = pd.to_datetime('1970-01-01')\nstore.CompetitionOpenSinceDT = store.CompetitionOpenSinceDT.fillna(ifnulldt) \n\n##Convert PromoSince to a date field and set NA = 1970/01/01\nstore['Promo2Mon'] = 1\nstore['Promo2Day'] = 1\nstore['Promo2SinceDT'] = pd.to_datetime(dict(year=store.Promo2SinceYear, month=store.Promo2Mon, day=store.Promo2Day))\nstore = store.drop(['Promo2SinceYear','Promo2Mon','Promo2Day'], axis='columns') \nmask = store.Promo2SinceWeek.isnull() == False\nstore.loc[mask, 'Promo2SinceWeek'] = store[mask].Promo2SinceWeek.apply(lambda x: np.timedelta64(np.int(x), 'W'))\nstore['Promo2SinceDT'] = store['Promo2SinceDT'] + store['Promo2SinceWeek']\nstore = store.drop(['Promo2SinceWeek'], axis='columns')\nstore.Promo2SinceDT = store.Promo2SinceDT.fillna(ifnulldt)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0334bc44-475b-f81d-176f-3e8e3bb94b1b"},"outputs":[],"source":"store.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a8ee792-7aea-013e-af0f-b5f0499bc3b6"},"outputs":[],"source":"train_df = train.set_index('Store').join(store.set_index('Store'), how='inner').reset_index()\ntest_df = test.set_index('Store').join(store.set_index('Store'), how='inner').reset_index()\n\ntrain_df.DayOfWeek = train_df.DayOfWeek.astype(str)\ndummies = pd.get_dummies(train_df[['Assortment', 'StoreType', 'StateHoliday','DayOfWeek']])\ntrain_df = train_df.join(dummies)\ntrain_df = train_df.drop(['Assortment', 'StoreType', 'StateHoliday','DayOfWeek'], axis=1)\n\ntest_df.DayOfWeek = test_df.DayOfWeek.astype(str)\ndummies = pd.get_dummies(test_df[['Assortment', 'StoreType', 'StateHoliday','DayOfWeek']])\ntest_df = test_df.join(dummies)\ntest_df = test_df.drop(['Assortment', 'StoreType', 'StateHoliday','DayOfWeek'], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4df313b7-4f57-901a-e831-5ecebe77901c"},"outputs":[],"source":"train_df = train_df.assign(days_since_comp = train_df['Date'] - train_df['CompetitionOpenSinceDT'])\ntrain_df = train_df.assign(days_since_promo = train_df['Date'] - train_df['Promo2SinceDT'])\n\ntest_df = test_df.assign(days_since_comp = test_df['Date'] - test_df['CompetitionOpenSinceDT'])\ntest_df = test_df.assign(days_since_promo = test_df['Date'] - test_df['Promo2SinceDT'])\n\ntrain_df.days_since_comp = (train_df.days_since_comp / np.timedelta64(1, 'D')).astype(int)\ntrain_df.days_since_promo = (train_df.days_since_promo / np.timedelta64(1, 'D')).astype(int) \n\ntest_df.days_since_comp = (test_df.days_since_comp / np.timedelta64(1, 'D')).astype(int)\ntest_df.days_since_promo = (test_df.days_since_promo / np.timedelta64(1, 'D')).astype(int) \n\ntrain_df.loc[train_df.CompetitionOpenSinceDT.dt.year <= 1970,'days_since_comp']\ntrain_df.loc[train_df.days_since_comp < 0,'days_since_comp'] = 0\ntrain_df.loc[train_df.Promo2SinceDT.dt.year <= 1970,'days_since_promo'] = 0\ntrain_df.loc[train_df.days_since_promo < 0, 'days_since_promo'] = 0\n\ntest_df.loc[test_df.CompetitionOpenSinceDT.dt.year <= 1970,'days_since_comp']\ntest_df.loc[test_df.days_since_comp < 0,'days_since_comp'] = 0\ntest_df.loc[test_df.Promo2SinceDT.dt.year <= 1970,'days_since_promo'] = 0\ntest_df.loc[test_df.days_since_promo < 0, 'days_since_promo'] = 0\n\ntrain_df.drop(['CompetitionOpenSinceDT','Promo2SinceDT'], axis=1, inplace=True)\ntest_df.drop(['CompetitionOpenSinceDT','Promo2SinceDT'], axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44cd53f8-77f6-2ec9-9469-91379eda6395"},"outputs":[],"source":"def remove_promo_interval_flag(df):\n    df = df.assign(is_promo = ((df['date_month'] == 1)  & (df['ProInt1'] == 1))  |\n                              ((df['date_month'] == 2)  & (df['ProInt2'] == 1))  | \n                              ((df['date_month'] == 3)  & (df['ProInt3'] == 1))  | \n                              ((df['date_month'] == 4)  & (df['ProInt4'] == 1))  | \n                              ((df['date_month'] == 5)  & (df['ProInt5'] == 1))  | \n                              ((df['date_month'] == 6)  & (df['ProInt6'] == 1))  | \n                              ((df['date_month'] == 7)  & (df['ProInt7'] == 1))  | \n                              ((df['date_month'] == 8)  & (df['ProInt8'] == 1))  | \n                              ((df['date_month'] == 9)  & (df['ProInt9'] == 1))  | \n                              ((df['date_month'] == 10) & (df['ProInt10'] == 1)) | \n                              ((df['date_month'] == 11) & (df['ProInt11'] == 1)) |\n                              ((df['date_month'] == 12) & (df['ProInt12'] == 1)))\n    \n    df.is_promo = df.is_promo.astype(np.int8)\n    df = df.drop(['ProInt1' ,'ProInt2' ,'ProInt3',\n                   'ProInt4' ,'ProInt5' ,'ProInt6',\n                   'ProInt7' ,'ProInt8' ,'ProInt9',\n                   'ProInt10','ProInt11','ProInt12'], axis=1)\n    \n    return df\n\ntrain_df = remove_promo_interval_flag(train_df)\ntest_df  = remove_promo_interval_flag(test_df)\n\ntrain_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d242d12-29fb-cbf4-470d-64993cf06a7f"},"outputs":[],"source":"train_df = train_df.set_index(['Store', 'Date']).sort_index()\ntest_df = test_df.set_index(['Store', 'Date']).sort_index()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bbed0b6e-7616-9a5e-2938-b715c9a3fe69"},"outputs":[],"source":"def create_model(store_list, clf):\n    customer_score = []\n    customer_pred = []\n    customer_fimp = []\n    sales_score = []\n    sales_pred = []\n    sales_fimp = []\n    clf_dict = {}\n\n    for i in store_list:\n        clf_list = []\n        print(\"Fitting store: \", i)\n        X = train_df.loc[i]\n        y1 = X.pop('Customers')\n        y2 = X.pop('Sales')\n\n        clf.fit(X[:-100], y1[:-100])\n        y1_pred = clf.predict(X[-100:])\n        y1_score = clf.score(X, y1)\n        print(\"----Customer pred score: \", y1_score)\n        \n        customer_score.append(y1_score)\n        customer_pred.append(y1_pred)\n        \n        if hasattr(clf, 'feature_importances_'):\n            customer_fimp.append(clf.feature_importances_)\n        \n        clf_list.append(clf)\n        \n        clf.fit(X[:-100], y2[:-100])\n        y2_pred = clf.predict(X[-100:])\n        y2_score = clf.score(X, y2)\n        print(\"----Sales pred score: \", y2_score)\n\n        sales_score.append(y2_score)\n        sales_pred.append(y2_pred)\n        if hasattr(clf, 'feature_importances_'):\n            sales_fimp.append(clf.feature_importances_)\n        \n        clf_list.append(clf)\n        clf_dict[i] = clf_list\n        \n    return customer_score, customer_pred, customer_fimp, sales_score, sales_pred, sales_fimp, clf_dict"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91e21545-44fa-aa1c-9b22-ad5101453862"},"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\nclf = RandomForestRegressor(n_estimators=200)\n\nstore_ix = store.Store.tolist()[0:500]\n\ny1 = train_df.loc[store_ix, 'Customers']\ny2 = train_df.loc[store_ix, 'Sales']\ncolumns = train_df.drop(['Customers', 'Sales'], axis=1).columns\n\ny1_score, y1_pred, y1_fimp, y2_score, y2_pred, y2_fimp, clf_dict = create_model(store_ix[0:600], clf)\n\n#model_result = pd.DataFrame({'Y1_true':y1[-100:].values.tolist(), 'y1_pred': np.array(y1_pred).flatten()}, index=y1[-100:].index.get_level_values(1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee1ab84a-ca97-f5cd-724a-72779f7021f7"},"outputs":[],"source":"plt.figure(figsize=(8,3))\nplt.plot(y1_score)\nplt.plot(y2_score)\nplt.ylim(0.5,1)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56405030-6ff5-d67d-ec4f-d3037efa7f11"},"outputs":[],"source":"model_fimp = pd.DataFrame(np.array(y1_fimp), index=store_ix[0:600], columns=columns)\nmodel_fimp.mean().sort_values().plot(kind='barh', title='Feature Importance', figsize=(6,6));"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad8bd302-fe53-cc03-0ce0-3bf1a34d357e"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa596277-fced-f5b0-acaa-67aeebddb2f8"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}