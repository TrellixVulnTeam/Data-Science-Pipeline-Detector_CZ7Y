{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport numpy.random as random\nimport pandas as pd\nimport datetime\nimport math\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\nfrom keras.models import Model as KerasModel\nfrom keras.layers import Input, Dense, Activation, Reshape, Dropout\nfrom keras.layers import Concatenate\nfrom keras.layers.embeddings import Embedding\n\npd.set_option('display.max_columns', 200)\n%precision 3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#データの読み取り\ntrain = pd.read_csv(\"/kaggle/input/rossmann-store-sales/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/rossmann-store-sales/test.csv\")\nstore = pd.read_csv(\"/kaggle/input/rossmann-store-sales/store.csv\")\nstate = pd.read_csv(\"/kaggle/input/rossmann-store-extra/store_states.csv\")\nstate_name = pd.read_csv(\"/kaggle/input/rossmann-store-extra/state_names.csv\")\nweathers = pd.read_csv(\"/kaggle/input/rossmann-store-extra/weather.csv\")\nL = len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmspe(y_pred, y_test):\n    return np.sqrt(np.mean(((y_pred - y_test) / y_test)**2))\n\n#Date系の特徴量生成\n#Month, Day等はEmbeddingの為に0始まりにしている\ndef date(data):\n    data['Date'] = pd.to_datetime(data['Date'])\n    data['Year'] = data['Date'].dt.year\n    data['Year'] = data['Year'] - data['Year'].min()\n    data['Month'] = data['Date'].dt.month - 1\n    data['Day'] = data['Date'].dt.day - 1\n    data['DayOfWeek'] = data['Date'].dt.dayofweek\n    data['WeekOfYear'] = data['Date'].dt.weekofyear\n    data['Days'] = (data['Date'] - data['Date'].min()).dt.days\n    data['Days'] = data['Days'] / data['Days'].max()\n    data['DayOfMonth'] = [0 if i<=10 else 1 if i<=20 else 2 for i in data['Day']]\n    data['QuadYear'] = [0 if i<=13 else 1 if i<=26 else 2 if i<=39 else 3 for i in data['WeekOfYear']]\n    return data\n\n#カテゴリカル変数のラベル付け\ndef label(data, column):\n    unique = data[column].unique()\n    k = 0\n    for str in unique:\n        data.loc[data[column] == str, column] = k\n        k += 1\n\n#カテゴリカル変数をEntityEmbeddingで得られた重みに置き換える\ndef replace(data, weights, features, drop=True):\n    for feature in features:\n        data = data.merge(weights[feature], how='left', on=[feature])\n        if drop == True:\n            data = data.drop([feature], axis=1)\n    return data\n\n#storeの前処理\ndef pre_store(store, state):\n    store['CompetitionDistance'].fillna(store['CompetitionDistance'].median(), inplace = True)\n    store.fillna(0, inplace = True)\n    store = store.merge(state, how='left', on=['Store'])\n    return store\n\n#weatherの前処理\ndef pre_weather(data, weather):\n    weather = weather.rename(columns={'file': 'StateName'})\n    weather = weather.merge(state_name, how='left', on=['StateName'])\n    weather['Date'] = pd.to_datetime(weather['Date'])\n    data = data.merge(weather, how='left', on=['State', 'Date'])\n    data['Events'].fillna('NaN', inplace = True)\n    label(data, 'Events')\n    label(data, 'State')\n    data['Max_Gust_SpeedKm_h'].fillna(0, inplace=True)\n    data['CloudCover'].fillna(data['CloudCover'].median(), inplace = True)\n    data['Max_VisibilityKm'].fillna(data['Max_VisibilityKm'].median(), inplace = True)\n    data['Mean_VisibilityKm'].fillna(data['Mean_VisibilityKm'].median(), inplace = True)\n    data['Min_VisibilitykM'].fillna(data['Min_VisibilitykM'].median(), inplace = True)\n    data['CloudCover'].fillna(data['CloudCover'].median(), inplace = True)\n    data = data.drop(['Date', 'StateName'], axis=1)\n    return data\n\n#データの前処理\ndef preprocess(train, test, store):\n    train['Id'] = 0\n    test.fillna(1, inplace=True)\n    data = pd.concat([train, test], sort=False)\n    data = date(data)\n    data = data[(data['Open']!=0) & (data['Sales']!=0)]\n    data = pd.merge(data, store, how = 'inner', on = 'Store')\n    data['Store'] = data['Store'] - 1\n    data['CompetitionOpen'] = 12 * (2015 - data.Year - data.CompetitionOpenSinceYear) + \\\n        (data.Month - data.CompetitionOpenSinceMonth)\n    data['PromoOpen'] = 12 * (2015 - data.Year - data.Promo2SinceYear) + \\\n        (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n    data['SalesLog'] = data['Sales'].map(math.log)\n    data['CustomersLog'] = data['Customers'].map(math.log)\n    data.loc[data['StateHoliday'] == 0, 'StateHoliday'] = '0'\n    label(data, 'StateHoliday')\n    label(data, 'StoreType')\n    label(data, 'Assortment')\n    label(data, 'PromoInterval')\n    data = data.drop(['Customers', 'Open'], axis=1)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EntitiyEmbedding:\n    def __init__(self):\n        self.input_model = []\n        self.output_model = []\n        self.features = []\n        self.embeddings = []\n\n    def add(self, feature, input_shape, output_shape):\n        self.features.append(feature)\n        self.embeddings.append(feature)\n        input_model = Input(shape=(1,), name=(feature + '_input'))\n        output_model = Embedding(input_shape, output_shape, name=(feature + '_embedding'))(input_model)\n        output_model = Reshape(target_shape=(output_shape,))(output_model)\n        self.input_model.append(input_model)\n        self.output_model.append(output_model)\n\n    def dense(self, feature, output_shape):\n        self.features.append(feature)\n        input_model = Input(shape=(1,), name=(feature + '_input'))\n        output_model = Dense(output_shape, name=(feature + '_dense'))(input_model)\n        self.input_model.append(input_model)\n        self.output_model.append(output_model)\n\n    def concatenate(self):\n        output_model = Concatenate()(self.output_model)\n        output_model = Dense(1000, kernel_initializer=\"uniform\")(output_model)\n        output_model = Activation('relu')(output_model)\n        output_model = Dense(500, kernel_initializer=\"uniform\")(output_model)\n        output_model = Activation('relu')(output_model)\n        output_model = Dense(1)(output_model)\n        output_model = Activation('sigmoid')(output_model)\n        self.model = KerasModel(inputs=self.input_model, outputs=output_model)\n        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n\n    def split_features(self, X):\n        X_list = {}\n        for feature in self.features:\n            X_list[feature + '_input'] = X[[feature]]\n        return X_list\n\n    def fit(self, X_train, y_train, X_test, y_test, epochs=12, batch_size=128):\n        self.X_test = X_test\n        self.model.fit(self.split_features(X_train), y_train,\n                       validation_data=(self.split_features(X_test), y_test),\n                       epochs=epochs,\n                       batch_size=batch_size)\n\n    def predict(self, X=None):\n        if X is None:\n            X = self.X_test\n        pred = self.model.predict(self.split_features(X))\n        return pred\n\n    def get_weight(self):\n        weights = {}\n        for feature in self.embeddings:\n            w = self.model.get_layer(feature + '_embedding').get_weights()[0]\n            columns = []\n            for i in range(w.shape[1]):\n                columns.append(feature + '_' + str(i))\n            w = pd.DataFrame(w, columns=columns)\n            w.index.names = [feature]\n            weights[feature] = w\n        return weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#データの前処理を行う\nstore = pre_store(store, state)\ndata = preprocess(train, test, store)\ndata = pre_weather(data, weathers)\n\nsmax = data['SalesLog'].max()\ncmax = data['CustomersLog'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EntitiyEmbeddingクラスを使い重みを学習する。\n#カテゴリカル変数は model.add(カラム名，ラベル数，出力の次元数(任意，チューニング可))で層を追加\n#連続値や0, 1のみの特徴量は model.dense(カラム名，出力の数)で層を追加\n\nmodel = EntitiyEmbedding()\nmodel.add('Store', input_shape=1115, output_shape=10)\nmodel.add('DayOfWeek', input_shape=7, output_shape=6)\nmodel.add('Year', input_shape=3, output_shape=2)\nmodel.add('Day', input_shape=31, output_shape=10)\nmodel.add('Month', input_shape=12, output_shape=6)\nmodel.add('DayOfMonth', input_shape=3, output_shape=2)\nmodel.add('QuadYear', input_shape=4, output_shape=3)\nmodel.add('State', input_shape=12, output_shape=6)\nmodel.add('StateHoliday', input_shape=4, output_shape=3)\nmodel.add('WeekOfYear', input_shape=53, output_shape=10)\nmodel.add('Assortment', input_shape=3, output_shape=2)\nmodel.add('StoreType', input_shape=4, output_shape=3)\nmodel.add('PromoInterval', input_shape=4, output_shape=3)\nmodel.add('Events', input_shape=22, output_shape=10)\nmodel.dense('Promo', output_shape=1)\nmodel.concatenate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trainとtestを分割\ntrain = data[data['Id']==0]\ntrain = train.sample(frac=1, random_state=22)\n\ntest = data[data['Id']!=0]\nId = test['Id']\n33\nX = train.copy()\ny = train[['CustomersLog', 'SalesLog']]\n\n#20万個を重みの学習に使用\n#重みの学習と予測器の学習でデータを分けて過学習を抑える。\nX_train, X_ee, y_train, y_ee = train_test_split(X, y, test_size=200000, random_state=44)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#重みの学習\nmodel.fit(X_ee, y_ee['CustomersLog']/cmax, X_train, y_train['CustomersLog']/cmax, epochs=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#学習した重みを辞書型で取得\nweights = model.get_weight()\n\n#カテゴリカル変数を学習した重みに置き換える\nX_train = replace(X_train, weights, model.embeddings)\ntest = replace(test, weights, model.embeddings)\n\n#使用しないカラムを除去\nX_train = X_train.drop(['Sales', 'Id', 'SalesLog', 'CustomersLog', \n                        'Dew_PointC', 'MeanDew_PointC', 'Min_DewpointC', 'Max_Sea_Level_PressurehPa', 'Mean_Sea_Level_PressurehPa', \n                  'Min_Sea_Level_PressurehPa'], axis=1)\ntest = test.drop(['Sales', 'Id', 'SalesLog', 'CustomersLog', \n                        'Dew_PointC', 'MeanDew_PointC', 'Min_DewpointC', 'Max_Sea_Level_PressurehPa', 'Mean_Sea_Level_PressurehPa', \n                  'Min_Sea_Level_PressurehPa'], axis=1)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LightGBMで予測器の学習\n\nparams = {\"objective\" : \"rmse\",\n          \"boosting\" : \"gbdt\", \n          \"metric\" : \"rmse\",\n          \"num_iterations\" : 7500,\n          \"top_k\" : 30, \n          \"max_depth\" : 8, \n          \"num_leaves\" : 800, \n          \"min_data_in_leaf\" : 20, \n          \"learning_rate\" : 0.02,\n          \"bagging_fraction\" : 0.7, \n          \"bagging_seed\" : 3,\n          \"bagging_freq\" : 5, \n          \"feature_fraction\" : 0.5, \n          \"num_threads\" : 4\n         }\n\ndataset_params = {\"max_bin\" : 200, \n                  \"min_data_in_bin\" : 3 \n                 }\nlgb_train = lgb.Dataset(X_train, label=y_train['SalesLog'], params=dataset_params)\nmodel = lgb.train(params, lgb_train, verbose_eval=50, keep_training_booster=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(test)\npred = np.exp(pred)\npred = pd.DataFrame(pred.T, index=Id, columns=['Sales'])\n\ndf = pd.DataFrame(range(1, L+1), columns=['Id'])\ndf = df.merge(pred, how='left', on=['Id'])\ndf.fillna(0, inplace=True)\ndf.to_csv(\"/kaggle/working/submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}