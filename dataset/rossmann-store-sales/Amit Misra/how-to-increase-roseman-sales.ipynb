{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Roseman Sales Dataset** - This dataset is a live dataset of Roseman Stores. On analsysing this problem we observe that Roseman problem is a regression problem and our primarily goal is to predict the sales figures of Roseman problem. In this Notebook we work on following topics\n1. Analysing the Dataset by using Exploratory Data Analysis.\n2. Using Exponential Moving Averages analyse Trends and Seasonality in Roseman dataset.\n3. Analyse Regression analysis using following prediction analysis,\n   A. Linear Regression Analysis\n   B. Elastic Regression ( Lasso and Ridge Regression).\n   C. Random Forest Regression.\n   D. Extreme Gradiant Boosting Algorithm."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"%reset -f\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#1.2 Calling Standard Scalar        \nfrom sklearn.preprocessing import StandardScaler\n#1.3 Calling Dimentionality reduction Library PCA\nfrom sklearn.decomposition import PCA\n\n#1.4 Splitting Dataset and calling Grid Serarch and Random Search CV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#1.5 Calling Pipeline Libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\n#1.6 Calling XGBoost\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost import plot_importance\n\n#1.7 Calling Evalution Matrics\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import confusion_matrix\n\n# 1.8 Misc\nimport time\nimport os\nimport gc\nimport random\n\n# 1.9 Set option to dislay many rows\npd.set_option('display.max_columns', 100)\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"#2.0 Set Working Directory and import Dataset\n\nroseman = pd.read_csv(\"/kaggle/input/rossmann-store-sales/train.csv\")\nstore = pd.read_csv(\"/kaggle/input/rossmann-store-sales/store.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Cleansing in Roseman Dataset** - We observe that Roseman Data consists of two dataset details are\nA. Store Dataset - This consists of all the information about various stores types. We observe that there are four categories of stores in Roseman Dataset, we will analyse store wise performance of dataset in detail.\n\nB. Roseman Dataset (train.csv) - This is a complete dataset of all the detailed information about sales in various stores.\n\nWe observe that Roseman store dataset has around 10 Lakhs records means it is a huge dataset. We have clean both the datasets i.e. Roseman and Store separately before joining. **One important thing is that we reduce the datatype to int32 instead of int64 in order to improve performance of our dataset**.\n\nSince we are using sklearn and numeric values are prereuqisite and mandatory requirement for different models, so we ensure that all the columns have int32 values."},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"#3.0 Data Cleaning in Both Roseman and Store Dataset\nstore.isna().sum()\nroseman.isna().sum()\nstore.info()\nroseman.info()\n\nstore.StoreType.unique()\nstore.Promo2SinceWeek.unique()\n\n#Our Observations are Store has Null values in a Dataset whereas main data roseman has no null values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"#3.1 Filling the Null Values in Store Dataset\nstore.head()\nstore['Promo2SinceWeek'] = store['Promo2SinceWeek'].fillna(0)\nstore['Promo2SinceYear'] = store['Promo2SinceYear'].fillna(store['Promo2SinceYear'].mode().iloc[0])\nstore['PromoInterval'] = store['PromoInterval'].fillna(store['PromoInterval'].mode().iloc[0])\nstore['CompetitionDistance'] = store['CompetitionDistance'].fillna(store['CompetitionDistance'].max())\nstore['CompetitionOpenSinceMonth'] = store['CompetitionOpenSinceMonth'].fillna(store['CompetitionOpenSinceMonth'].mode().iloc[0])\nstore['CompetitionOpenSinceYear'] = store['CompetitionOpenSinceYear'].fillna(store['CompetitionOpenSinceYear'].mode().iloc[0])\n\nstore.isna().sum()\nstore =store.drop(['CompetitionDistance','CompetitionOpenSinceMonth','CompetitionOpenSinceYear'],axis=1)\n#Now Store dataset has no null values.\nstore['StoreType']    = store['StoreType'].map({'a':1,'b':2,'c':3,'d':4})\nstore['Assortment']   = store['Assortment'].map({'a':0, 'c':1,'b':2})\nstore['PromoInterval']= store['PromoInterval'].map({'Jan,Apr,Jul,Oct':0, 'Feb,May,Aug,Nov':1,'Mar,Jun,Sept,Dec':2})\nstore.info()\nstore.StoreType.unique()\nstore = store.astype('int32')\n\nroseman.info()\nroseman.StateHoliday.unique()\nroseman['StateHoliday'] = roseman['StateHoliday'].map({'0':0, 0:0,'a':1,'b':2,'c':3})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"#3.2 Doing Feature Engineering in Date column\nroseman['year'] = pd.DatetimeIndex(roseman['Date']).year\nroseman['month'] = pd.DatetimeIndex(roseman['Date']).month\nroseman['day'] = pd.DatetimeIndex(roseman['Date']).day\nroseman['week'] = pd.DatetimeIndex(roseman['Date']).week\nroseman = roseman.drop(['Date'],axis=1)\nroseman =roseman.astype('int32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exploratory Data Analysis of Roseman Stores Sales**"},{"metadata":{},"cell_type":"markdown","source":"**Weekly sales Analysis**"},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"rosemangrp = roseman.groupby(['DayOfWeek'],as_index = False)\nrosemangrp.groups\nrosemangp = rosemangrp.agg({'Sales':np.mean})\nrosemangp\nrosemangp['DayOfWeek'] =rosemangp['DayOfWeek'].map({7:'Sunday',1:'Monday',2:'Tuesday',3:'Wednesday',4:'Thursday',5:'Friday',6:'Saturday'}).astype(str)\nrosemangp\nsns.barplot(x='DayOfWeek',y='Sales',data = rosemangp)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Our Observations after anlysing weekdays sales**- On analysing we observe that Roseman store sales drops drastically on weekends especially on sundays mean people not prefer to go to stores on sundays and sales improve drastically on Monday and Tuesday it indicates that people prefer shopping on roseman stores on weekday and not on weekends."},{"metadata":{},"cell_type":"markdown","source":"**Monthly Sales Analysis**"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#4.1 Observing Monthly Sales of Roseman Stores\nrosemangrpms = roseman.groupby(['month'],as_index = False)\nrosemangrpms.groups\nrosemangpms = rosemangrpms.agg({'Sales':np.mean})\nrosemangpms\nrosemangpms['month'] =rosemangpms['month'].map({1:'January',2:'February',3:'March',4:'April',5:'May',6:'June',7:'July',8:'August',9:'September',10:'October',11:'November',12:'December'}).astype(str)\nrosemangpms\nsns.barplot(x='Sales',y='month',data = rosemangpms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Our Observations on Rosemann Monthly Sales-** We observe that Santa Clause have special blessing on Roseman Stores means in month of December sales increases."},{"metadata":{},"cell_type":"markdown","source":"**Yearly Sales Analysis**"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#4.2 Observing Yearly Sales of Roseman Stores\nrosemangrpys = roseman.groupby(['year'],as_index = False)\nrosemangrpys.groups\nrosemangpys = rosemangrpys.agg({'Sales':np.mean})\nrosemangpys\nsns.barplot(x='year',y='Sales',data = rosemangpys)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Our Observations on Roseman Store Yearly Sales Analysis -** While Observing yearly sales of Roseman Stores we observe that sales of Rosemann store stagnates on year on year basis. Roseman team have to work vey hard to increse the yearly sales."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#5 Merging of Roseman data with Store Data\nrosemanall = store.merge(roseman,on=['Store'],how = 'left')\nrosemanall.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feeling of Roseman Problem Using Exponential Moving Averages (EMA**)- Since Roseman problem is a problem of continous sales data. Exponential Moving Averages are (EMA) is a best indicator to analyse \nA. Trend in a Data - This means that sales data is in a uptrend or downtrend.\nB. Seasonality in a data- This means where sales figure increases in some speciific timeframes.\n\nFor ananlysing regression problems Exponential Moving Average (EMA) figures gives complete picure of data. By analysing this data we are able to understand whether doing business is feasible what are the important opportunities when we are able to get optimum benifits.\n"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#6.1 Exponential Moving Average chart of Sales figure\nrosemanall_weekly = rosemanall.groupby(['year','week'],as_index = False)\nrosemanall_weekly.groups\nrosemanall_weekly = rosemanall_weekly.agg({'Sales':np.mean})\nrosemanll_weekly = rosemanall_weekly.sort_values(by = ['year','week'])\nrosemanall_weekly['weekrow']= rosemanall_weekly.reset_index().index\n\nrosemanall_weekly['10weeks_ema'] = rosemanall_weekly.Sales.ewm(span=10).mean()\nrosemanall_weekly['50weeks_ema'] = rosemanall_weekly.Sales.ewm(span=50).mean()\nrosemanall_weekly['200weeks_ema'] = rosemanall_weekly.Sales.ewm(span=200).mean()\nrosemanall_weekly.plot('weekrow', ['10weeks_ema','50weeks_ema','200weeks_ema'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion and Observations - While Analysing Exponential Moving Average chart Roseman sales we analyse following observations,\n1. Trend - EMA chart analyse that Roseman data is in uptrend and it always above the 200 Exponential Moving Average figures but 200 EMA line is not a steep slope informs that Roseman sales figure is in slow uptrend.\n\n2. Seasonlity - 10 days EMA chart figures shows that sales has upward seasonality and for some weeks sales figures improves drastically.\n"},{"metadata":{},"cell_type":"markdown","source":"**Store Type Wise Exponential Moving Average Analysis** - While analysing database we observe that Roseman have four different types of stores. After analysing complete dataset, We now analyse Exponential Moving Average Analysis of Sales figures Storetype wise means how the behaviour of different store types in order to going deeper and understand how to improve sales."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#6.2Moving Average Chart of Store Wise Sales Figure\nrosemanall_a = rosemanall.loc[rosemanall['StoreType'] == 1]\nrosemanall_b = rosemanall.loc[rosemanall['StoreType'] == 2]\nrosemanall_c = rosemanall.loc[rosemanall['StoreType'] == 3]\nrosemanall_d = rosemanall.loc[rosemanall['StoreType'] == 4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**STORETYPE A WEEKLY EMA CHART**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#6.3 EMA Analysis Storewise\nrosemanall_a_weekly = rosemanall_a.groupby(['year','week'],as_index = False)\nrosemanall_a_weekly.groups\nrosemanall_a_weekly = rosemanall_a_weekly.agg({'Sales':np.mean})\nrosemanll_weekly = rosemanall_a_weekly.sort_values(by = ['year','week'])\nrosemanall_a_weekly['weekrow']= rosemanall_a_weekly.reset_index().index\n\nrosemanall_a_weekly['20days_ema'] = rosemanall_a_weekly.Sales.ewm(span=20).mean()\nrosemanall_a_weekly['50days_ema'] = rosemanall_a_weekly.Sales.ewm(span=50).mean()\nrosemanall_a_weekly['200days_ema'] = rosemanall_a_weekly.Sales.ewm(span=200).mean()\n\nrosemanall_a_weekly.plot('weekrow',['20days_ema','50days_ema','200days_ema'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**STORETYPE B WEEKLY CHART**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rosemanall_b_weekly = rosemanall_b.groupby(['year','week'],as_index = False)\nrosemanall_b_weekly.groups\nrosemanall_b_weekly = rosemanall_b_weekly.agg({'Sales':np.mean})\nrosemanll_weekly = rosemanall_b_weekly.sort_values(by = ['year','week'])\nrosemanall_b_weekly['weekrow']= rosemanall_b_weekly.reset_index().index\n\nrosemanall_b_weekly['20days_ema'] = rosemanall_b_weekly.Sales.ewm(span=20).mean()\nrosemanall_b_weekly['50days_ema'] = rosemanall_b_weekly.Sales.ewm(span=50).mean()\nrosemanall_b_weekly['200days_ema'] = rosemanall_b_weekly.Sales.ewm(span=200).mean()\n\nrosemanall_b_weekly.plot('weekrow',['20days_ema','50days_ema','200days_ema'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**STORETYPE C EMA CHART**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rosemanall_c_weekly = rosemanall_c.groupby(['year','week'],as_index = False)\nrosemanall_c_weekly.groups\nrosemanall_c_weekly = rosemanall_c_weekly.agg({'Sales':np.mean})\nrosemanll_weekly = rosemanall_c_weekly.sort_values(by = ['year','week'])\nrosemanall_c_weekly['weekrow']= rosemanall_c_weekly.reset_index().index\nrosemanall_c_weekly['20days_ema'] = rosemanall_c_weekly.Sales.ewm(span=20).mean()\nrosemanall_c_weekly['50days_ema'] = rosemanall_c_weekly.Sales.ewm(span=50).mean()\nrosemanall_c_weekly['200days_ema'] = rosemanall_c_weekly.Sales.ewm(span=200).mean()\n\nrosemanall_c_weekly.plot('weekrow',['20days_ema','50days_ema','200days_ema'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**STORETYPE D WEEKLY EMA CHART**"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"rosemanall_d_weekly = rosemanall_d.groupby(['year','week'],as_index = False)\nrosemanall_d_weekly.groups\nrosemanall_d_weekly = rosemanall_d_weekly.agg({'Sales':np.mean})\nrosemanll_weekly = rosemanall_d_weekly.sort_values(by = ['year','week'])\nrosemanall_d_weekly['weekrow']= rosemanall_d_weekly.reset_index().index\nrosemanall_d_weekly['20days_ema'] = rosemanall_d_weekly.Sales.ewm(span=20).mean()\nrosemanall_d_weekly['50days_ema'] = rosemanall_d_weekly.Sales.ewm(span=50).mean()\nrosemanall_d_weekly['200days_ema'] = rosemanall_d_weekly.Sales.ewm(span=200).mean()\n\nrosemanall_d_weekly.plot('weekrow',['20days_ema','50days_ema','200days_ema'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Our Observations and Conclusions after analysis of Roseman Storetype wise Exponential Moving Average Charts\n\nAfter analysing consolidated data we now analyse Roseman Data Storewise. Following observations found \n\n1. STORETYPE A - While analysing Exponential Moving Average Chart of Store Type A we observe that \n -Store type A sales figure in slight uptrend 20 days EMA never crosses 200 days EMA.\n -We observe seasonality in EMA chart and found sales of Storetype A sharply increase at Week 50 and week 105.\n\n2. STORETYPE B - While analysing Exponential Moving Average Chart of Store Type B we observe that\n  - Storetype B Sales figures are in strong uptrend and 20 days EMA never crosses 50 days EMA sales chart of Storetype B is very strong.\n  - Although this chart is very strong we observe seasonality in upward seasonality at week 50 and week 105.\n\n3. STORETYPE C - While analysing Exponential Moving Average Chart of Store Type C we observe that \n -Store type A sales figure in slight uptrend 20 days EMA never crosses 200 days EMA.\n -We observe seasonality in EMA chart and found sales of Storetype A sharply increase at Week 50 and week 105.\n\n4. STORETYPE D - While analysing Exponential Moving Average Chart of Store Type D we observe that \n - Storetype D sales chart is in uptrend and better chart than Storetype A and C. In this EMA chart we observe that support of 20 days EMA at 50 days EMA\n   and most of time 20 days EMA line not cuts downward 20 days EMA line.\n\nAfter analysing all the EMA charts store wise we observe following out of all the store type Roseman has to focus aggresively on opening new Storetype B and D and \nfocus on closure of Storetype A and C.Exponential Moving Average Analysis is a part of Time Series Analysis and Roseman problem is a time series analysis problem\nour conclusion is that EMA chart analysis gives us very indepth understanding of Roseman sales."},{"metadata":{},"cell_type":"markdown","source":"Checking Target Variable is Balanced or Imbalance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#7.0 Now checking the dataset target column balance\nrosemanall['Sales'].value_counts()\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#7.1 We Observe that Roseman Store problem is a Regression Problem \nuniq =rosemanall.nunique() \nuniq\ny = rosemanall.Sales\ny.count()\nrosemanall.drop(['Sales'],axis =1)\nX= rosemanall\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size =0.3,random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#7.2 Checking Numerical and Categorical Columns of a Dataset\n\ncolunique=X_train.nunique\ncols= (X_train.nunique() < 8 )\ncols\ncat_cols = cols[cols==True].index.tolist()\nnum_cols = cols[cols==False].index.tolist()\ncat_cols\nnum_cols\ncat_cols_new = ['Promo2','Promo2SinceYear','DayOfWeek','Open','Promo','SchoolHoliday','year']\nobj_cols = ['StoreType','Assortment','PromoInterval','StateHoliday']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predicting Roseman Sales using Linear Regression Model** - We observe that Roseman Sales problem is regression problem means predicted variable is in trend. We start with a very simple model i.e. Linear Regression model and we are using Pipeline to preprocess the data apply Linear Regression Model."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#8.1 Since Roseman Store is a regression problem we first build Linear Regresssion Model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import OrdinalEncoder\nSS = StandardScaler()\nOHE = OneHotEncoder()\nOE  = OrdinalEncoder()\nLR = LinearRegression()\n\n\npreprocess = make_column_transformer(\n                                     (SS,num_cols),\n                                     (OHE,obj_cols)\n                                     )\n\npipeline_linreg = make_pipeline(preprocess,LR)\n\n\n#8.2 Fit the Pipeline in Training data\n\npipeline_linreg.fit(X_train,y_train)\n\n#8.3 Predict the Pipeline in test data\n\ny_predict_linreg = pipeline_linreg.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#8.4 Score of Linear Regrssion pipeline \n\nscore_linreg =r2_score(y_predict_linreg,y_test)\nscore_linreg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(y_test,y_predict_linreg)) \nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion of Linear Regression Model - We observe that Root Mean Square Error figures are fine, in this model we are getting excellent score of 1. Our personal opionion on this model we are getting score 1 means some over fitting issues in this model. "},{"metadata":{},"cell_type":"markdown","source":"**Regularization of Linear Regression Model Using Elastic Net. Elastic Net uses both Lasso and Ridge regression** - Since Linear Regression model gives very high scores we now do some regularisation using Elastic Net regularisation which uses both Lasso and Redge regressions. Theory of Lasso and Redge regression is given below."},{"metadata":{},"cell_type":"markdown","source":"Courtsey- Towards Datascience\n**Linear regression (in scikit-learn**) is the most basic form, where the model is not penalized for its choice of weights, at all. That means, during the training stage, if the model feels like one particular feature is particularly important, the model may place a large weight to the feature. This sometimes leads to overfitting in small datasets. Hence, following methods are invented.\n**Lasso** is a modification of linear regression, where the model is penalized for the sum of absolute values of the weights. During training, the objective function become:\nImage for post\nAs you see, Lasso introduced a new hyperparameter, alpha, the coefficient to penalize weights.\n**Ridg**e takes a step further and penalizes the model for the sum of squared value of the weights. Thus, the weights not only tend to have smaller absolute values and more evenly distributed, but also tend to be close to zeros. The objective function becomes:\nImage for post\n**ElasticNet** is a hybrid of Lasso and Ridge, where both the absolute value penalization and squared penalization are included, being regulated with another coefficient l1_ratio:"},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"#4.3 Regressioin Ananlysis using Elastic Net. Elastic net uses both Lasso and Ridge regressions.\n# Regularisastion of Linear Regression Model using L1 and L2 regularization.\n\nfrom sklearn.linear_model import ElasticNet\n\nEN = ElasticNet()\npipeline_elasticnet = make_pipeline(preprocess,EN)\nparameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n\n# Calling Grid Search CV \nGrid_Search_EN = GridSearchCV(pipeline_elasticnet,parameters)\nGrid_Search_EN.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Compute and print the scores\nr2 = Grid_Search_EN.score(X_test,y_test)\nprint(\"Tuned ElasticNet Alpha: {}\".format(Grid_Search_EN.best_params_))\nprint(\"Tuned ElasticNet R squared: {}\".format(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Courtsey- Wikipedia\nEvaluation using Random Forest Regressor Model- Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]:587–588 Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees."},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"#9.1 Roseman Store problem using Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nRF = RandomForestRegressor(max_depth =5)\n\npipeline_rf = make_pipeline(preprocess,RF)\n\npipeline_rf.fit(X_train,y_train)\n \ny_predict_rf = pipeline_rf.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"score_rf = r2_score(y_predict_rf,y_test)\nscore_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Our Observations and Conclusion for Random Forest Model** We observe that Random Forest model gives nice results but at the same time Linear Regression which is a simple model also gives best results."},{"metadata":{},"cell_type":"markdown","source":"**Prediction of Roseman Sales using XGBoost Model** - XG Boost Model is one of best model for hyper parameter tuning we analyse sales figures using XG Boost Model by using cross validation, Grid Search CV and Random Search CV. "},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"#10.1 Prediction of Roseman Problem using XGBoost Algorithm\nfrom xgboost.sklearn import XGBRegressor\nSS = StandardScaler()\nxgb = XGBRegressor()\npipeline_xg = make_pipeline (preprocess,SS,xgb)\n\n#10.2 Doing Grid Search using XG Boost\npipeline_xg.get_params\n\nparameters = {\n               'xg__learning_rate':  [0.03, 0.05],\n                'xg__n_estimators':   [200,  300], \n                'xg__max_depth':      [4,6]\n            }\n\n\nclf = GridSearchCV(estimator = pipeline_xg,param_grid= parameters,scoring=\"neg_mean_squared_error\",cv=4,verbose=1)\n                   \n\n\n#10.3 Start fitting Data into pipeline\n\n\nclf.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"#10.4 CROSS VALIDATION XG BOOST MODEL USING PIPELINE\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost.sklearn import XGBRegressor\nsteps = [(\"pp\",preprocess),(\"SS\",StandardScaler()),\n         (\"xgb_model\", XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n\nxgb_pipeline = Pipeline(steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cross Validataion Scores for Extreme Boost Model**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"cross_val_scores = cross_val_score(xgb_pipeline,X_train,y_train,cv=10,scoring = \"neg_mean_squared_error\")\ncross_val_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"#10.5 Doing Grid Search CV using XG Boost Model\ngbm_param_grid = {\n        'xgb_model__n_estimat ors': [50],\n    'xgb_model__max_depth': [2, 5]\n}\ngrid_mse = GridSearchCV(estimator=xgb_pipeline,param_grid=gbm_param_grid,scoring=\"neg_mean_squared_error\",cv=4,verbose=1)\n\ngrid_mse.fit(X_train,y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Best Parameter Scores for XG Boost Model using Grid Search**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"best_param = grid_mse.best_params_\nbest_param","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best Scores for XG Boost Scores using Grid Search"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"best_score = grid_mse.best_score_\nbest_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lowest RMSE for Grid Search using XGBoost**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"Lowest_RMSE_gs = np.sqrt(np.abs(best_score))\nLowest_RMSE_gs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Final Conclusion** - Roseman problem is an interesting problem to analyse future sales. In this notesheet we analyse both past sales using Exponential Moving Averages, analyse the trend and seasonality in Roseman Sales figures. We observe that oveall sales figures of Roseman store are stuck from last three years it is in very week uptrend.\n\nWe observe that Roseman have four different types of stores, in this sheet we also analyse the performance of different store types using Exponential Moving Average. We observe that performance of store B is outstanding as compared to performance of all other stores. Our recommendation are that Roseman team has to focus more on Store Type B and D and focus for closure of Store A and C.\n\nWe observe that performance of models are also nice to predict future Roseman Sales."},{"metadata":{},"cell_type":"markdown","source":"**Being an initial Learner, i try my best to understand and predict Roseman Sales. Your UPVOTES will definitely encourage to do best. Your suggestions to improve better are highly Appreciable to do better in future.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}