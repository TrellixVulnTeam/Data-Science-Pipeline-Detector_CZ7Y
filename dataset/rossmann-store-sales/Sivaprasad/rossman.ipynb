{"cells":[{"metadata":{"trusted":true,"_uuid":"851336353493a7ec76acf86089beefa269846323"},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pyspark.sql.functions as func\nimport sys\nimport os\nfrom datetime import  datetime, timedelta\nfrom pyspark.sql.types import ArrayType,DateType,StructType\nfrom pyspark.sql.functions import when,col,lit\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as F\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Rossmann').getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2b89d6cec32b3295bf3b7ba9b0268061f6f62d4"},"cell_type":"code","source":"path      = \"../input\"\nfile_name = path + '/train.csv' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c6325542e48aa1263f3b8a4be1cd3dc6f66db9b"},"cell_type":"code","source":"from pyspark.sql.types import (StructField, StringType, StructType, IntegerType,\n                               FloatType, DateType) \n\ndata_schema = [StructField('Store', StringType(), True),\n               StructField('DayOfWeek', StringType(), True),\n               StructField('Date', DateType(), True),\n               StructField('Sales',FloatType(), True),\n               StructField('Customers',StringType(), True),\n               StructField('Open',StringType(), True),\n               StructField('Promo',StringType(), True),\n               StructField('StateHoliday',StringType(), True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d11595a8173e2e2c815d76f03fce58b9b630884"},"cell_type":"code","source":"final_struc = StructType(fields = data_schema)\ndf = spark.read.csv(file_name,schema=final_struc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"706c80eb952f65b27f8be5fa4c0c27dff98911bd"},"cell_type":"code","source":"#Consider only active stores\ndf = df.where(df['Open'] == 1)\ndf.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d66cd24acdb221655d5efba704d5ff560519c913"},"cell_type":"code","source":"df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b45bfdae6b730f0663b493d631890677b5416f2"},"cell_type":"code","source":"# function for getting date range\ndef generate_date_series(start, stop):\n    date_range = []\n    no_of_days = (stop-start).days + 1\n    for x in range(0, no_of_days):\n        date_list = [start + timedelta(days=x)]\n        k = date_list[0]\n        date_range.append([k])\n    return date_range\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"580e7aca569d93e491acc9211672b005ae78f108"},"cell_type":"code","source":"from pyspark.sql.functions import count, isnan\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd77d902f18f1b1a0334f0d06bceb635633c72e2"},"cell_type":"markdown","source":"**Choose Store 1**"},{"metadata":{"trusted":true,"_uuid":"09679c76e6fe7c7a65184bcee886620639dc2468"},"cell_type":"code","source":"df = df.where(df['Store'] == 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a044952b29ac68297e7b52cda095b7f7703d4df9"},"cell_type":"code","source":"df.createOrReplaceTempView('storedata')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7bb5b48bea26d59a93776e4d3cd962fe755378c"},"cell_type":"markdown","source":"**Impute values for the slected store 1**"},{"metadata":{"trusted":true,"_uuid":"8f6b237390ebc1ffeb8c4df24e8e040f24b78d78"},"cell_type":"code","source":"result = spark.sql(\"SELECT MIN(Date) as mindate,MAX(Date) as maxdate from storedata \")\nminim = result.head(5)[0][0] \nmaxim = result.head(5)[0][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d286b5266576fbab59792f235ff9a566a38e3e6"},"cell_type":"code","source":"minim = minim - timedelta(days=1)\ndate_list = generate_date_series(minim,maxim)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"624c52ea95db550e72e4cb25d0a9f4eac81556ec"},"cell_type":"code","source":"\ndf_datelist = spark.createDataFrame(date_list,['Date'])\ndf_datelist.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c1a1e2b0e8f34cf63667bbf6cb35e5db0ed8eb6"},"cell_type":"code","source":"df = df_datelist.join(df,[\"Date\"],\"leftouter\").orderBy(\"Date\")\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85dd25fc667125a4650724be7c318887598a70f8"},"cell_type":"code","source":"\npd_df = df.toPandas()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee2e410c8382ff376cbb237b23332c8d97f05e93"},"cell_type":"markdown","source":"**Impute with forward and backward fill**"},{"metadata":{"trusted":true,"_uuid":"5e1462dcf5d47d78aea3d1f42b65e66dba967dcc"},"cell_type":"code","source":"pd_df = pd_df.fillna(method = 'bfill').fillna(method = 'ffill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7e76ea3c19e4ef2c9ce3c01701b5dd458143175"},"cell_type":"code","source":"#placing back to spark dataframe\ndf = spark.createDataFrame(pd_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73a8ecf565aa88a459727e36778c7226bcade852"},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a517bb92af3323846fdf546e2fb67f6a49ab7ab"},"cell_type":"code","source":"df.filter((df[\"Customers\"] == \"\") | df[\"Customers\"].isNull() | isnan(df[\"Customers\"])).count()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f364e9435822112b2660e8a7af80d0c296c2873"},"cell_type":"code","source":"df.filter((df[\"Store\"] == \"\") | df[\"Store\"].isNull() | isnan(df[\"Store\"])).count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d80134ee3ec52637cd4aa6a85af5b455973bf1d4"},"cell_type":"markdown","source":"**Add week number coulmn to data frame**"},{"metadata":{"trusted":true,"_uuid":"9e1c03b046473afe9c2bf8389f7cfed6efae9f6e"},"cell_type":"code","source":"from pyspark.sql.functions import weekofyear,year\n                                        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90c51d2eee037bee341d863985ba9be57d37564e"},"cell_type":"code","source":"df = df.withColumn(\"Week\",weekofyear(\"Date\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66ab1f4958c2e88ff8e362195ec0d2027e0bc6b5"},"cell_type":"markdown","source":"***Add the Year column***"},{"metadata":{"trusted":true,"_uuid":"e96e80f7a81c9293030e8a1e04484fee65fa2591"},"cell_type":"code","source":"df= df.withColumn('Year', year('Date'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"954eee1837d925971254a9ff38f56ebb1416b1b4"},"cell_type":"code","source":"df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d033ca47d70ede4681efcdbc98d5ea7d48fdd7a"},"cell_type":"markdown","source":"***Aggregating data to week level***"},{"metadata":{"trusted":true,"_uuid":"ce7544b772d9bdacc6be47d0cc9555dd9ef9ec9d"},"cell_type":"code","source":"#Change the Year data type to string\ndf = df.withColumn(\"Year\", df[\"Year\"].cast(StringType()))\ndf = df.withColumn(\"Store\", df[\"Store\"].cast(IntegerType()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cebc3ec00d3ab15ca2100c7b89b46df7c016d318"},"cell_type":"code","source":"df.printSchema()\n\ndf.createOrReplaceTempView('storedata')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31ce13299657acaaa3739657631d4499cd50877d"},"cell_type":"code","source":"result = spark.sql(\"select  Year, Week, sum(Sales) as Tot_Sales from storedata \\\n                   group by Year, Week order by Year desc,Week desc \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d34931903a099ea0ef63cc010cbdcd8d4091ac88"},"cell_type":"code","source":"result.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5f16ec244174c45436fe429ba6688d79372c114"},"cell_type":"markdown","source":"***Cumulative Sales for present, Week1, Week2, Week3***"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5541d6f0c8cd5f520654aa17188221e0d79ad0a3"},"cell_type":"code","source":"\n\nwindowval = (Window.partitionBy('Year').orderBy(('Week'))\n             .rangeBetween(Window.unboundedPreceding, 0))\ndf_w_cumsum = result.withColumn('cum_sum', F.sum('Tot_Sales').over(windowval))\ndf_w_cumsum.orderBy((df_w_cumsum['Year']).desc(),(df_w_cumsum['Week']).desc() ).show(4)\n#df_w_cumsum.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f71fc77a9756f396bb7cfeb908c4bddb6f891363"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5623f354f26dd50d58217abcbf7e8441d7d2f4e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}