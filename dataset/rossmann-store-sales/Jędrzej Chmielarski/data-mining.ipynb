{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Business Understanding"},{"metadata":{},"cell_type":"markdown","source":"Our project concerns the creation of a machine learning model for sales prediction for the Rossmann store brand, basing on data gathered for over a thousand different stores. The Rossmann company utilized sales prediction on local level, tasking their store managers with predicting sales for up to 6 weeks in advance, in order to improve the efficiency of their supply procedures and lower the logistical cost of running the brand. Therefore, the selected case offers practical application, opening an avenue for implementation of the created model as a part of Rossmann official predictions.\n\nAside for the possibility of practical application in business world, the case study has advantages from the point of view of computer science. As the data is provided directly by Dirk Rossmann GmbH, it has an above-average level of completeness and accuracy, due to originating from the primary source rather than an outside observer.\n\nThe project takes data from over 1000 Rossmann stores and aims at predicting sales for them. We implement three different prediction methods: FB prophet, fast.ai and random forest, in order to compare their accuracy in relation to the effort needed for the prediction.\n"},{"metadata":{},"cell_type":"markdown","source":"# Data Description\nThe data analysed in the project comes from 1115 Rossmann stores.\nData is split into three sets: train set containing data for training models; test set containing data for testing model accuracy and store set containing additional data on the stores.\n\n# Data Fields\nNon-selfexplanatory data fields are described below\n\ntrain.csv set\n* Store - unique ID for the store\n* Sales - turnovers for any given day\n* Customers - number of customers for any given day\n* Open - binary value, denotes if the store is open (0 - closed, 1 - open)\n* StateHoliday - denotes days with state holidays: a - public holiday; b - Christmas; c - Easter; 0 - no holiday\n* SchoolHoliday - binary value, shows if the store was affected by school holiday\n* Promo - indicates if the store has a promotion on that day\n\ntest.csv set\n* ID - tuple containing store ID and date\n\nstore.csv set\n* StoreType - shows one of four store types: a, b, c and d\n* Assortment - describes an assortment level: a - basic, b - extra, c - extended\n* CompetitionDistance - distance in meters to the nearest competitor store\n* nCompetitionOpenSince - gives the approximate year and month of the time the nearest competitor was opened\n* Promo - shows whether a store is running a promo on that day\n* Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n* Promo2Since - describes the year and calendar week when the store started participating in Promo2\n* PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport datetime as dt\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nimport fbprophet as fbp\nimport seaborn as sns\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating metric used in competition\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\ndef ToWeight(y):\n    w = np.zeros(y.shape, dtype=float)\n    ind = y != 0\n    w[ind] = 1./(y[ind]**2)\n    return w\n\ndef rmspe(yhat, y):\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n    return rmspe\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/rossmann-store-sales/train.csv\", parse_dates=['Date'], low_memory=False)\nstore = pd.read_csv(\"../input/rossmann-store-sales/store.csv\", low_memory=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clear the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#First we get rid of useless rows where there are no sales and shops are closed\ninitial_len = train.shape[0]\ntrain=train[(train['Sales']!=0) & (train['Open']!=0)]\nnew_len = train.shape[0]\nprint(f\"We removed {(initial_len-new_len)/initial_len*100}% of rows\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Nothing to clear in train dataframe\ntrain.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some missing values in Store\nstore.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing missing values \ncols = ['CompetitionOpenSinceMonth',\n       'CompetitionOpenSinceYear',\n       'Promo2SinceWeek',\n       'Promo2SinceYear']\nfor col in cols:\n    store[col].fillna(0, inplace=True)\nstore['CompetitionDistance'].fillna(0, inplace=True) #Flaot\nstore[cols].isnull().any() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting Promo Interval equal to zero for those who are not continuing Promo and for missing values\nindex=store[(store['Promo2']==0)&(store['PromoInterval'].isnull())].index\nstore.loc[index,'PromoInterval']=0\n\nstore['PromoInterval'].isnull().any() # To check","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting from float into integer type\nstore[cols]=store[cols].astype(int)\nstore[cols].dtypes # To check","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#object type for categorical variable\nstore['Promo2']=store['Promo2'].astype(object)\nstore.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if no more nulls\nstore.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## New features"},{"metadata":{},"cell_type":"markdown","source":"We can extract additional information from date column"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_date_info(df, field_name='Date'):\n    field = df[field_name]\n    attr = ['Year', 'Month', 'Day','Dayofyear', 'Is_month_end', 'Is_month_start',\n                'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    for n in attr: df[n] = getattr(field.dt, n.lower())\n    df['Weekofyear'] = df.Date.dt.isocalendar().week\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_date_info(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can create new features based on sale info that should make training model easier"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SalesPerCustomer'] = train['Sales']/train['Customers']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_1 = pd.pivot_table(data=train, index=['DayOfWeek','Promo'], values=['Sales','Customers', 'SalesPerCustomer'], aggfunc='mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_1.plot(kind='bar',y=['SalesPerCustomer'],title='Average Sales per customer', figsize=(15,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Worth to note:\n- no promo on weekends\n- promos increased but number of customers and items per customer"},{"metadata":{},"cell_type":"markdown","source":"We can also create new features for each store"},{"metadata":{"trusted":true},"cell_type":"code","source":"# avg_store Dataframe containing columns : 'Average Sales','Average Customers','Average Sales Per Customer'\navg_store=train.groupby('Store')[['Sales','Customers','SalesPerCustomer']].mean()\navg_store.rename(columns=lambda x : 'Avg_' + x,inplace=True)\navg_store.reset_index(inplace=True)\n\n# Adding column Max_Customers(containing maximum value of customers) to avg_store Dataframe \nMax_customer=train.groupby('Store')['Customers'].max()\navg_store=pd.merge(avg_store,Max_customer,how='inner',on='Store')\navg_store.rename(columns={'Customers':'Max_Customers'},inplace=True)\n\n# Adding column Min_Customers(containing mimimum value of customers) to avg_store Dataframe \nMin_customer=train.groupby('Store')['Customers'].min()\navg_store=pd.merge(avg_store,Min_customer,how='inner',on='Store')\navg_store.rename(columns={'Customers':'Min_Customers'},inplace=True)\n\n# Adding column Std_Customers(containing Standard Deviation value of customers) to avg_store Dataframe \nStd_customer=train.groupby('Store')['Customers'].std()\navg_store=pd.merge(avg_store,Std_customer,how='inner',on='Store')\navg_store.rename(columns={'Customers':'Std_Customers'},inplace=True)\n\n# Adding column Med_Customers(containing Median value of customers) to avg_store Dataframe \nMed_customer=train.groupby('Store')['Customers'].median()\navg_store=pd.merge(avg_store,Med_customer,how='inner',on='Store')\navg_store.rename(columns={'Customers':'Med_Customers'},inplace=True)\n\n# Adding column Max_Sales(containing maximum value of Sales) to avg_store Dataframe \nMax_Sale=train.groupby('Store')['Sales'].max()\navg_store=pd.merge(avg_store,Max_Sale,how='inner',on='Store')\navg_store.rename(columns={'Sales':'Max_Sales'},inplace=True)\n\n# Adding column Min_Sales(containing mimimum value of Sales) to avg_store Dataframe \nMin_Sale=train.groupby('Store')['Sales'].min()\navg_store=pd.merge(avg_store,Min_Sale,how='inner',on='Store')\navg_store.rename(columns={'Sales':'Min_Sales'},inplace=True)\n\n# Adding column Std_Sales(containing Standard Deviation value of Sales) to avg_store Dataframe \nStd_Sale=train.groupby('Store')['Sales'].std()\navg_store=pd.merge(avg_store,Std_Sale,how='inner',on='Store')\navg_store.rename(columns={'Sales':'Std_Sales'},inplace=True)\n\n# Adding column Med_Sales(containing Median value of Sales) to avg_store Dataframe \nMed_Sale=train.groupby('Store')['Sales'].median()\navg_store=pd.merge(avg_store,Med_Sale,how='inner',on='Store')\navg_store.rename(columns={'Sales':'Med_Sales'},inplace=True)\n\n\navg_store.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store=pd.merge(store,avg_store,how='inner',on='Store')\nstore.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging\nnew_train=pd.merge(train,store,how='left',on='Store')\nprint('New training dataset shape :',new_train.shape)\nnew_train.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making column \"MonthCompetitionOpen\" which contains date information in months since the competition was opened \nnew_train['MonthCompetitionOpen']=12*(new_train['Year']-new_train['CompetitionOpenSinceYear'])+\\\nnew_train['Month']-new_train['CompetitionOpenSinceMonth']\n\nnew_train.loc[(new_train['CompetitionOpenSinceYear']==0),'MonthCompetitionOpen']=0\n# Negative values indcate that the competitor's store was opened after the Rossman's store opening date.\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making column \"WeekPromoOpen\" which contains date information in weeks since the promo is running\nnew_train['WeekPromoOpen']=52.14298*(new_train['Year']-new_train['Promo2SinceYear'])+\\\nnew_train['Weekofyear']-new_train['Promo2SinceWeek']\n\nnew_train.loc[(new_train['Promo2SinceYear']==0),'WeekPromoOpen']=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sales and Customers by Store Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3,figsize=(17,10) )\npalette = itertools.cycle(sns.color_palette(n_colors=4))\nplt.subplots_adjust(hspace = 0.28)\naxes[0].bar(new_train.groupby(by=\"StoreType\").sum().Customers.index,new_train.groupby(by=\"StoreType\").Sales.mean(),color=[next(palette),next(palette),next(palette),next(palette)])\naxes[0].set_title(\"Average Sales per Store Type\")\naxes[1].bar(new_train.groupby(by=\"StoreType\").sum().Customers.index,new_train.groupby(by=\"StoreType\").Customers.mean(),color=[next(palette),next(palette),next(palette),next(palette)])\naxes[1].set_title(\"Average Number of Customers per Store Type\")\naxes[2].bar(new_train.groupby(by=\"StoreType\").sum().SalesPerCustomer.index,new_train.groupby(by=\"StoreType\").SalesPerCustomer.mean(),color=[next(palette),next(palette),next(palette),next(palette)])\naxes[2].set_title(\"Average Sales per Customer per Store Type\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sales and Customers by Week Day"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3,figsize=(17,10) )\npalette = itertools.cycle(sns.color_palette(n_colors=7))\nplt.subplots_adjust(hspace = 0.28)\naxes[0].bar(new_train.groupby(by=\"DayOfWeek\").sum().Customers.index,new_train.groupby(by=\"DayOfWeek\").Sales.mean(),color=[next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette)])\naxes[0].set_title(\"Average Sales per Week Day\")\naxes[1].bar(new_train.groupby(by=\"DayOfWeek\").sum().Customers.index,new_train.groupby(by=\"DayOfWeek\").Customers.mean(),color=[next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette)])\naxes[1].set_title(\"Average Number of Customers per Week Day\")\naxes[2].bar(new_train.groupby(by=\"DayOfWeek\").sum().SalesPerCustomer.index,new_train.groupby(by=\"DayOfWeek\").SalesPerCustomer.mean(),color=[next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette)])\naxes[2].set_title(\"Average Sales Per Customer per Week Day\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The day of week significantly influences the number of sales and customers per store.\nHighest sales are generated on Mondays and Sundays, while highest customer count happens on Sundays, with almost 75% more customers than on Mondays. The difference in sales and customers happens due to most stores being closed on Sundays, thus generating additional sales and, most importantly, additional popularity in the few stores that are open. Increased sales on Monday are also caused by stores being closed on weekends in areas where no nearby open stores are accessible."},{"metadata":{},"cell_type":"markdown","source":"# Sales and Customers by Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3,figsize=(17,10) )\npalette = itertools.cycle(sns.color_palette(n_colors=12))\nplt.subplots_adjust(hspace = 0.28)\naxes[0].bar(new_train.groupby(by=\"Month\").sum().Customers.index,new_train.groupby(by=\"Month\").Sales.mean(),color=[next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette)])\naxes[0].set_title(\"Average Sales per Month\")\naxes[1].bar(new_train.groupby(by=\"Month\").sum().Customers.index,new_train.groupby(by=\"Month\").Customers.mean(),color=[next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette)])\naxes[1].set_title(\"Average Number of Customers per Month\")\naxes[2].bar(new_train.groupby(by=\"Month\").sum().SalesPerCustomer.index,new_train.groupby(by=\"Month\").SalesPerCustomer.mean(),color=[next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette),next(palette)])\naxes[2].set_title(\"Average Sales Per Customer per Month\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a significant growth in sales and customers in December, most probably due to numerous holidays (Christmas, Hannukah etc.) taking place during the month, thus generating additional sales due to the tradition of gift exchange common among these holidays."},{"metadata":{},"cell_type":"markdown","source":"# How do Promos influence Sales?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.title(\"Sales depending on Promos\")\nsns.set(style=\"whitegrid\",palette=\"pastel\",color_codes=True)\nsns.violinplot(x=\"DayOfWeek\",y=\"Sales\",hue=\"Promo\",split=True, data=new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.title(\"Customers depending on Promos\")\nsns.set(style=\"whitegrid\",palette=\"pastel\",color_codes=True)\nsns.violinplot(x=\"DayOfWeek\",y=\"Customers\",hue=\"Promo\",split=True, data=new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.title(\"Customers depending on Promos\")\nsns.set(style=\"whitegrid\",palette=\"pastel\",color_codes=True)\nsns.violinplot(x=\"DayOfWeek\",y=\"SalesPerCustomer\",hue=\"Promo\",split=True, data=new_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Promos seem to have an influence on the Sales, with significantly different results depending on the day of the week. The largest sales difference between stores with and without promos is on Mondays, which is to be expected due to most stores being closed on Sundays.\nNo promos are run on weekends, thus decreasing the sales per customer value, but the increased number of customers on Sundays balances out the loss in sales."},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"Our project goal is to predict time series, therefore simple random split into training and validation would be wrong approach. The base copmetition was about predicting sales several weeks into the future, that is why we decided to set our goal on 30 day predicion.\nTo validate we split the data into test set that we used to assert models performance and training set used for generating them.\nTest set was composed of information about sales from last 30 days available in dataset.\nTraining set had information about sales from the previous 3 years.\n\nModels are verified basing on the comparison of the results to the data given in test set. The verification uses mean square root percentage error and direct sales difference comparison to check the accuracy of each model."},{"metadata":{},"cell_type":"markdown","source":"##  Model Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Random forest is an ensemble decision tree model, used for regression and classification, which implements multiple decision trees. The random forest method uses a modified bootstrap algorithm, also known as bagging. The algorithm creates many random subsets from the dataset with replacement and trains them using modified regression or classification with random subset of features. The average result of all subtrees is selected as the best fitting one."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get dummies for categorical variables\ndf_forest=pd.get_dummies(data=new_train,columns=['StoreType','StateHoliday','Assortment','PromoInterval'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Period for which we will be making prediction, we take last 30 working days which is similar to the goal of the \n#competition\nperiod = 30\nto_drop = ['Customers','Sales','Open','Date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_forest.sort_values('Date',inplace=True)\ndates = df_forest['Date'].unique()\n\nforest_train = df_forest[df_forest['Date'].isin(dates[:-period])]\nY_train = forest_train['Sales']\nX_train = forest_train.drop(to_drop,axis=1)\nforest_test = df_forest[df_forest['Date'].isin(dates[-period:])]\nY_test = forest_test['Sales']\nX_test = forest_test.drop(to_drop,axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_leaves = [4,6,8,10]\nnum_features = ['sqrt',0.5,0.6,0.7,0.8,1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr=RandomForestRegressor(n_estimators=100,\n                          criterion='mse',\n                          oob_score=True,\n                          n_jobs=12,\n                          verbose=1,\n                          random_state=404,max_features=0.8\n                         )\n\nrfr.fit(X_train,Y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\npredict=rfr.predict(X_test)\npredict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict.shape, Y_test.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Out of Bag score\nprint('oob score :',rfr.oob_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Root mean square error\nmse=mean_squared_error(Y_test,predict)\nprint('Root Mean Square Percent Error {}, RMSE = {}'.format(rmspe(predict,Y_test), sqrt(mse)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import attributes according to model\n\npd.options.display.float_format='{:.5f}'.format\nimportant_features=pd.DataFrame(rfr.feature_importances_,index=X_train.columns)\nimportant_features.sort_values(by=0,ascending=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(Y_test-predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model fast-ai"},{"metadata":{},"cell_type":"markdown","source":"Fast-ai is a deep learning library implementing fully connected neural networks, developed to simplify the process of learning. FCNN are a type of neural networks in which every neuron in a layer is connected to all other neurons in other layers. Fully connected neural networks are of general purpose, requiring no special assumptions on input data, which makes them easy to implement but lowers their learning efficiency."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fast = new_train.copy()\ndf_fast.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fast.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"period = 30\nto_drop = ['Customers','Open','Date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"change_dtypes = {'Weekofyear': np.int64, 'Sales': np.float64, 'Is_month_end':np.int64, 'Is_month_start':np.int64,\n                'Is_quarter_end':np.int64, 'Is_quarter_start':np.int64, 'Is_year_end':np.int64, 'Is_year_start':np.int64}\ndf_fast = df_fast.astype(change_dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fast.sort_values('Date',inplace=True)\ndates = df_fast['Date'].unique()\nindex_train = df_fast[df_forest['Date'].isin(dates[:-period])].index\nindex_test = df_fast[df_forest['Date'].isin(dates[-period:])].index\ndf_fast.drop(to_drop,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dep_var = 'Sales'\ncont_nn,cat_nn = cont_cat_split(df_fast, max_card=1000, dep_var=dep_var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_nn, cont_nn.pop(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cont_nn:\n    change_dtypes = {col: np.float64}\n    df_fast = df_fast.astype(change_dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fast['Sales'] = np.log(df_fast['Sales'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_nn.append('Store')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splits = (list(index_train), list(index_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"procs_nn = [Categorify, Normalize]\nto_nn = TabularPandas(df_fast,procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls = to_nn.dataloaders(1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = to_nn.train.y\ny.min(),y.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = tabular_learner(dls, y_range=(y.min(),y.max()), layers=[500,250,100],\n                        n_out=1, loss_func=F.mse_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean square root percent error"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"preds,targs = learn.get_preds()\nrmspe(np.exp(np.array(preds)),np.exp(np.array(targs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.exp(np.array(preds))-np.exp(np.array(targs)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FBProphet"},{"metadata":{},"cell_type":"markdown","source":"FBProphet is an open source library developed by Facebook for time series forecasting. It implements decomposable models, taking into account not only the trend but also seasonal and holiday changes.\n\ny(t)=g(t)+s(t)+h(t)+e\n\nWhere:\n\ng(t) is a piecewise growth curve, linear or logarithmic, for modelling non-periodic changes in the \ntime series\n\ns(t) is a function responsible for modelling seasonal changes\n\nh(t) is a function responsible for modelling holidays or irregular events\n\ne is the error term accounting for changes not included in previous functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/rossmann-store-sales/train.csv\", parse_dates=['Date'], low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns = {'Date': 'ds', \"Sales\": 'y'}, inplace=True)\ndf_prophet = df[['Store','ds','y','StateHoliday','SchoolHoliday']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prophet allows for using information about holiday in prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"state = df_prophet[(df_prophet.StateHoliday == 'a') | (df_prophet.StateHoliday == 'b') & (df_prophet.StateHoliday == 'c')].loc[:,['Store','ds']]\nstate['holiday'] = 'state_holiday'\nschool = df_prophet[df_prophet.SchoolHoliday == 1].loc[:, ['Store','ds']]\nschool['holiday'] = 'school_holiday'\n\n#state = pd.DataFrame({'holiday': 'state_holiday', 'ds': state_dates})\n#school = pd.DataFrame({'holiday': 'school_holiday', 'ds': school_dates})\n\nholidays_all = pd.concat((state, school))      \nholidays_all.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_prediction_store(store_id, df_all, periods, holidays_all):\n    holiday = holidays_all[holidays_all['Store'] == 1][['ds','holiday']]\n    df = df_all[df_all['Store'] == 1][['ds','y']]\n    df = df.sort_values('ds')\n    df_cut = df[:-periods]\n    \n    model = fbp.Prophet(holidays = holiday)\n    model.fit(df_cut)\n    \n    future_df = model.make_future_dataframe(periods=periods)\n    predictions = model.predict(future_df)\n    \n    \n    return predictions[periods:], df['y'][-periods:]\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = holidays_all[holidays_all['Store'] == 1][['ds','holiday']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prophet['Store'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred, ys = get_prediction_store(1, df_prophet, 30, holidays_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred[pred['ds'].isin(df_prophet.loc[ys.index]['ds'])]['yhat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred[pred['ds'].isin(df_prophet.loc[ys.index]['ds'])][['ds','yhat']],df_prophet.loc[ys.index][['ds','y']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hats = pd.DataFrame(pred[pred['ds'].isin(df_prophet.loc[ys.index]['ds'])]['yhat'])\ny = pd.DataFrame(df_prophet.loc[ys.index]['y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hats[y_hats['yhat'] < 0] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sqrt(mean_squared_error(y_hats,y)),rmspe(np.array(y_hats),np.array(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store = 1 \npred, ys = get_prediction_store(1, df_prophet, 30, holidays_all)\nyhats_glob = pd.DataFrame(pred[pred['ds'].isin(df_prophet.loc[ys.index]['ds'])]['yhat'])\nys_glob = pd.DataFrame(df_prophet.loc[ys.index]['y'])\nfor store_id in df_prophet['Store'].unique()[1:]:\n    pred, ys = get_prediction_store(1, df_prophet, 30, holidays_all)\n    y_hats = pd.DataFrame(pred[pred['ds'].isin(df_prophet.loc[ys.index]['ds'])]['yhat'])\n    ys = pd.DataFrame(df_prophet.loc[ys.index]['y'])\n    y_hats[y_hats['yhat'] < 0] = 0\n    \n    pd.concat((yhats_glob,y_hats))\n    pd.concat((ys_glob,ys))\n    \nsqrt(mean_squared_error(yhats_glob,ys_glob)),rmspe(np.array(yhats_glob),np.array(ys_glob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nOut of the three methods, random forest proved to be the most accurate, achieving a mean square root percentage error of 13%. While it has the lowest error of all methods, it requires more work than the two other approaches.\n\nFBProphet provided worst results, with accuracy of approximately 16%, while also requiring the longest learning time. Despite these drawbacks, it proved to be a good baseline approach, as it requires little to none data preparation and feature engineering, thus being appropriate for simple, straight-forward prediction cases.\n\nFast-ai provided worse results than expected, with accuracy of 15%. The accuracy could be further improved if more time is invested into preparation. It is also the fastest learning approach and thus good for predicting basing on large data sets. We theoretize that using a different neural network approach, for example recurrent, might provide better results.\n\nThe results of all predictions may be skewed due to data preprocessing, as the training set contains a large portion (about 16%) of incomplete entries that had to be filled with most fitting values."},{"metadata":{},"cell_type":"markdown","source":"# Bibliography\n\nRandom Forest\nhttps://builtin.com/data-science/random-forest-algorithm\nhttps://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/\nhttps://towardsdatascience.com/understanding-random-forest-58381e0602d2\n\nFast-ai and FCNN\nhttps://docs.fast.ai/\nhttps://medium.com/swlh/fully-connected-vs-convolutional-neural-networks-813ca7bc6ee5\n\nFBProphet\nhttps://www.analyticsvidhya.com/blog/2018/05/generate-accurate-forecasts-facebook-prophet-python-r/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}