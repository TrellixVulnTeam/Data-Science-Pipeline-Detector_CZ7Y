{"nbformat_minor":1,"cells":[{"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","metadata":{"_uuid":"27c47ece401b160c95fec0e626d9443be7a66afa","_cell_guid":"812a429f-04bc-4b6d-9f95-562f285bfd87"},"outputs":[]},{"execution_count":null,"source":"# GBM prediction\n\n# inspirations:\n# https://www.kaggle.com/the1owl/surprise-me/\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\nimport datetime as dt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\ndef RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred) ** 0.5\n\nstart_time = dt.datetime.now()\nprint(\"Started at \", start_time)","cell_type":"code","metadata":{},"outputs":[]},{"execution_count":null,"source":"data = {\n    'tra': pd.read_csv('../input/air_visit_data.csv'),\n    'as': pd.read_csv('../input/air_store_info.csv'),\n    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n    'ar': pd.read_csv('../input/air_reserve.csv'),\n    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n    'id': pd.read_csv('../input/store_id_relation.csv'),\n    'tes': pd.read_csv('../input/sample_submission.csv'),\n    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n    }","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n#datetime transform+date feature\nfor df in ['ar','hr']:\n    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n\ndata['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\ndata['tra']['year'] = data['tra']['visit_date'].dt.year\ndata['tra']['month'] = data['tra']['visit_date'].dt.month\ndata['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n\ndata['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\ndata['tes']['year'] = data['tes']['visit_date'].dt.year\ndata['tes']['month'] = data['tes']['visit_date'].dt.month\ndata['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n\nunique_stores = data['tes']['air_store_id'].unique()\nstores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i] * len(unique_stores)}) for i in range(7)],\n                   axis=0, ignore_index=True).reset_index(drop=True)","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"def find_outliers(series):\n    return (series - series.mean())>2.4*series.std()\n\ndata['tra']['is_outlier']=data['tra'].groupby('air_store_id').apply(lambda g: find_outliers(g['visitors'])).values","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"# mean max min sure it can be compressed... \ntmp = data['tra'].groupby(['air_store_id', 'dow'], as_index=False)['visitors'].min().rename(\n    columns={'visitors': 'min_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\ntmp = data['tra'].groupby(['air_store_id', 'dow'], as_index=False)['visitors'].mean().rename(\n    columns={'visitors': 'mean_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\ntmp = data['tra'].groupby(['air_store_id', 'dow'], as_index=False)['visitors'].median().rename(\n    columns={'visitors': 'median_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\ntmp = data['tra'].groupby(['air_store_id', 'dow'], as_index=False)['visitors'].max().rename(\n    columns={'visitors': 'max_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\ntmp = data['tra'].groupby(['air_store_id', 'dow'], as_index=False)['visitors'].count().rename(\n    columns={'visitors': 'count_observations'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n\nstores = pd.merge(stores, data['as'], how='left', on=['air_store_id'])\n\nprint(\"Store df info:\")\nprint(stores.info())","cell_type":"code","metadata":{},"outputs":[]},{"execution_count":null,"source":"# NEW FEATURES FROM Georgii Vyshnia\nstores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\nstores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\nlbl = preprocessing.LabelEncoder()\nfor i in range(10):\n    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\nstores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\nstores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n\ndata['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\ndata['hol']['visit_date'] = data['hol']['visit_date'].dt.date\ntrain = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \ntest = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n\ntrain = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \ntest = pd.merge(test, stores, how='left', on=['air_store_id','dow'])","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"# day_of_week label encode\nlbl = preprocessing.LabelEncoder()\ndata['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n\ndata['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['visit_date'] = data['hol']['visit_date'].dt.date\ntrain = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date'])\ntest = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date'])\n\ntrain = pd.merge(data['tra'], stores, how='left', on=['air_store_id', 'dow'])\ntest = pd.merge(data['tes'], stores, how='left', on=['air_store_id', 'dow'])\n\nfor df in ['ar', 'hr']:\n    train = pd.merge(train, data[df], how='left', on=['air_store_id', 'visit_date'])\n    test = pd.merge(test, data[df], how='left', on=['air_store_id', 'visit_date'])\n\nprint(train.describe())\nprint(train.head())","cell_type":"code","metadata":{},"outputs":[]},{"execution_count":null,"source":"\ntrain['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\ntrain['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\ntrain['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\ntrain['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n\ntest['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\ntest['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\ntest['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n\n# NEW FEATURES FROM JMBULL\ntrain['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n\n# NEW FEATURES FROM Georgii Vyshnia\ntrain['lon_plus_lat'] = train['longitude'] + train['latitude'] \ntest['lon_plus_lat'] = test['longitude'] + test['latitude']\n\nlbl = preprocessing.LabelEncoder()\ntrain['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\ntest['air_store_id2'] = lbl.transform(test['air_store_id'])\n","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"train['x'] = np.cos(train['latitude']*math.pi/180.0) * np.cos(train['longitude']*math.pi/180.0)\ntrain['y'] = np.cos(train['latitude']*math.pi/180.0) * np.sin(train['longitude']*math.pi/180.0)\ntest['x'] = np.cos(test['latitude']*math.pi/180.0) * np.cos(test['longitude']*math.pi/180.0)\ntest['y'] = np.cos(test['latitude']*math.pi/180.0) * np.sin(test['longitude']*math.pi/180.0)\ntrain['monthday_int']=train['date_int']%10000\ntrain['day_int']=train['date_int']%100\ntest['monthday_int']=test['date_int']%10000\ntest['day_int']=test['date_int']%100\n# def calc_shifted_ewm(series, alpha, adjust=True):\n#     return series.shift().ewm(alpha=alpha, adjust=adjust).mean()\n\n# train['ewm'] = train.set_index('visit_date').groupby(['air_store_id', 'dow'])\\\n# .apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1))\\\n# .sort_index(level=['air_store_id', 'visit_date']).values\n\n# test['ewm'] = test.set_index('visit_date').groupby(['air_store_id', 'dow'])\\\n# .apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1))\\\n# .sort_index(level=['air_store_id', 'visit_date']).values","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date', 'visitors']]","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"X = train[col]\ny = pd.DataFrame()\ny['visitors'] = np.log1p(train['visitors'].values)\n\n# print(X.info())\n\ny_test_pred = 0\n\n# do a hideout split for information leak-free last-minute check\nX, X_hideout, y, y_hideout = model_selection.train_test_split(X, y, test_size=0.13, random_state=42)\n\nprint(\"Finished data pre-processing at \", dt.datetime.now())","cell_type":"code","metadata":{},"outputs":[]},{"execution_count":null,"source":"X2=X[(X.month==4)&(X.year==2017)]\ny2=y[(X.month==4)&(X.year==2017)]\nX1=X[np.logical_not((X.month==4)&(X.year==2017))]\ny1=y[np.logical_not((X.month==4)&(X.year==2017))]","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"print(\"Start tuning at \", dt.datetime.now())\nparam_grid = {\n   # 'max_depth':[5,7]\n    'reg_lambda':[0,1,2]\n               # 'min_child_weight':[1,3]\n    \n              # 'max_features': [1.0, 0.3, 0.1] ## not possible in our example (only 1 fx)\n              }\nest = xgb.XGBRegressor(\n    learning_rate=0.01,\n    objective='reg:linear',\n    n_estimators=1000,\n    max_depth=6,\n    #min_child_weight=1,\n    subsample=0.8,\n    colsample_bytree=1,\n    scale_pos_weight=1,\n    gamma=0.1,\n    min_child_weight=3,\n    seed=1000)\n# this may take some minutes\ngs_cv = GridSearchCV(est, param_grid, cv=3, scoring = 'neg_mean_squared_error').fit(X_small, y_small)\n\nprint(\"End up tuning at \", dt.datetime.now())\n# best hyperparameter setting\ngs_cv.grid_scores_","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"# Set up folds\nK = 3\nkf = model_selection.KFold(n_splits = K, random_state = 1, shuffle = True)\nnp.random.seed(1)\n\n\n# model\n# xgboost\nboost_params = {'eval_metric': 'rmse'}\nmodel = xgb.XGBRegressor(\n    learning_rate=0.01,\n    objective='reg:linear',\n    n_estimators=10000,\n    max_depth=5,\n    min_child_weight=2,\n    subsample=0.8,\n    colsample_bytree=1,\n    scale_pos_weight=1,\n#     gamma=4,\n#     min_child_weight=3,\n    seed=1000)\n#seed=27","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt\nimport math\n\ndef RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5\nn_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train[col].values)\n    rmse= np.sqrt(-cross_val_score(model, train_x, train_y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"model.fit(X1,y1)","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"pred=model.predict(X1)","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"importances=model.feature_importances_\nfig, ax = plt.subplots(figsize=(20, 10))\nplt.bar(range(train[col].head(1000).shape[1]),importances.tolist())\nplt.xticks(range(train[col].head(1000).shape[1]),list(train[col].head(10)),fontsize=16, rotation=90)\nplt.show()","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"RMSLE(y, pred)","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"RMSLE(y1, pred)","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"pred_hideout=model.predict(X_hideout)\nRMSLE(y_hideout,pred_hideout)","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"pred2=model.predict(X2)\nRMSLE(y2,pred2)","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"\nprint(\"Finished setting up CV folds and regressor at \", dt.datetime.now())\n# Run CV\n\nprint(\"Started CV at \", dt.datetime.now())\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    # Create data for this fold\n    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n    X_train, X_valid = X.iloc[train_index, :].copy(), X.iloc[test_index, :].copy()\n    X_test = test[col]\n    print(\"\\nFold \", i)\n\n    fit_model = model.fit(X_train, y_train)\n    pred = model.predict(X_valid)\n    print('RMSLE GBM Regressor, validation set, fold ', i, ': ', RMSLE(y_valid, pred))\n\n    pred_hideout = model.predict(X_hideout)\n    print('RMSLE GBM Regressor, hideout set, fold ', i, ': ', RMSLE(y_hideout, pred_hideout))\n    print('Prediction length on validation set, GBM Regressor, fold ', i, ': ', len(pred))\n    # Accumulate test set predictions\n\n    pred = model.predict(X_test)\n    print('Prediction length on test set, GBM Regressor, fold ', i, ': ', len(pred))\n    y_test_pred += pred\n\n    del X_test, X_train, X_valid, y_train\n\nprint(\"Finished CV at \", dt.datetime.now())\ny_test_pred /= K  # Average test set predictions\nprint(\"Finished average test set predictions at \", dt.datetime.now())","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"test['is_outlier']=np.nan","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"y_test_pred=model.predict(test[col])","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"test.head()","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"# Create submission file\nsub = pd.DataFrame()\nsub['id'] = test['id']\nsub['visitors'] = np.expm1(y_test_pred) # .clip(lower=0.)\nsub.to_csv('C:/Users/Administrator/Desktop/input/xgboost_submit2.csv', float_format='%.6f', index=False)\n\nprint('We are done. That is all, folks!')\nfinish_time = dt.datetime.now()\nprint(\"Started at \", finish_time)\nelapsed = finish_time - start_time\nprint(\"Elapsed time: \", elapsed)","cell_type":"code","metadata":{"collapsed":true},"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.4","file_extension":".py","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4}