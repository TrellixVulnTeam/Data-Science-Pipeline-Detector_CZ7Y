{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile\nimport os\nimport glob, re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# ZIPで保存されているファイルを一通り読み込み\npath_list = []\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path_list.append(os.path.join(dirname, filename))\n        \npath_list.sort()\nprint(path_list)\nair_reserve = pd.read_csv(path_list[0])\nair_store_info = pd.read_csv(path_list[1])\nair_visit_data = pd.read_csv(path_list[2])\ndate_info = pd.read_csv(path_list[3])\nhpg_reserve = pd.read_csv(path_list[4])\nhpg_store_info = pd.read_csv(path_list[5])\nsample_submission = pd.read_csv(path_list[6])\nstore_id_relation = pd.read_csv(path_list[7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"air_reserve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"air_store_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"air_visit_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hpg_reserve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hpg_store_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_id_relation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### この問題でやって欲しいこと\n- 2016年7月1日〜2017年4月22日のデータを元に、2017年4月最終週〜2017年5月末日までの各日の来店数を予測する\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submittionの中身を確認\n# air_ストアID_日付 の情報から、来客数を予測して欲しい\n# 頭から20件\nprint(sample_submission.head(20))\n\n# (行数、列数)\nsample_submission.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 提出データの作成用にサンプル提出データを前処理しておく","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 元々のものは残しておく\ntest_data = sample_submission\n\n# 元々のIDからstore_idとvisit_dateを切り出す\ntest_data['store_id'] = test_data['id'].str[:20]\ntest_data['visit_date'] = test_data['id'].str[21:]\n# 現時点では visitors に意味がないので切り出し\ntest_data.drop(['visitors'], axis=1, inplace=True)\n# 日付の型をobjectからdatetimeに変換\ntest_data['visit_date'] = pd.to_datetime(test_data['visit_date'])\n# 念のためカラム情報の確認\ntest_data.info()\n# ヘッダー情報もみておく\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 特徴量を作っていく\n\n### 説明変数\n店ID、曜日、その曜日の来客数平均、休日かどうか、翌日が休みかどうか、月末かどうか（？）\n\n### 目的変数\n来客数","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# air_visit_data の前処理\n# Airレジの各レストランの日付と実客数のデータ\n# -> 予測すべきレストランの過去の客数データ  (シミュレータの過去のvimpsと似てる？)\n\n# visit_dateを日付方にする\nair_visit_data['visit_date'] = pd.to_datetime(air_visit_data['visit_date'])\nair_visit_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (行数、列数)\nair_visit_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# あるレストラン（air_00a91d42b08b08d9）の実客数を確認\n## 条件に合うものを選択\nstore_sample = air_visit_data[air_visit_data['air_store_id'] == 'air_00a91d42b08b08d9']\n\n## 基本統計量の確認\n## 平均26.08人、最大99人などの情報\nstore_sample.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visit_dateの確認\n# 被りなしそう（解説ではfirst, lastもあるがこちらでは表示されていない） -> date型にしていないだけでした\n# 2016年7月1日〜2017年4月22日のデータを元に、2017年4月最終週〜2017年5月末日までの各日の来店数を予測\nstore_sample.visit_date.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 狙いを定める\ntarget_df = air_visit_data\ntarget_df.info()\ndate_info['calendar_date'] = pd.to_datetime(date_info['calendar_date'])\ndate_info.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_df =  pd.merge(target_df, date_info, how='left', left_on='visit_date', right_on='calendar_date').drop(columns='calendar_date')\n# 日付から曜日を算出 (0-6, 0:mondayとして数値に変換される)\n# one_hot = pd.get_dummies(target_df['day_of_week'])\n# target_df = target_df.join(one_hot).drop(columns='day_of_week')\ntarget_df['visit_date'] = pd.to_datetime(target_df['visit_date'])\ntarget_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas.tseries.offsets as offsets\n\n# 翌日が休日かフラグと、土日含めて休みかフラグに変更\n# 激烈に遅いのでなんとかできそう\nfor index, row in target_df.iterrows():\n    tommorow = row['visit_date'] + offsets.Day()\n    # 土日はそもそも休日\n    if row['day_of_week'] == 'Saturday' or row['day_of_week'] == 'Sunday':\n        target_df.loc[index, 'holiday_flg'] = 0\n    if index + 1 < len(target_df):\n        row_next = target_df.loc[index + 1]\n        if row_next['air_store_id'] == row['air_store_id'] and (row_next['holiday_flg'] == 1 or row['day_of_week'] == 'Saturday'):\n            target_df.loc[index, 'tomorrow_holiday_flg'] = 1\n            continue\n    target_df.loc[index, 'tomorrow_holiday_flg'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_df.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data =  pd.merge(test_data, date_info, how='left', left_on='visit_date', right_on='calendar_date').drop(columns='calendar_date')\n# 日付から曜日を算出 (0-6, 0:mondayとして数値に変換される)\n# one_hot = pd.get_dummies(test_data['day_of_week'])\n# test_data = test_data.join(one_hot).drop(columns='day_of_week')\ntest_data['visit_date'] = pd.to_datetime(test_data['visit_date'])\ntest_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 翌日が休日かフラグと、土日含めて休みかフラグに変更\n# 激烈に遅いのでなんとかできそう\nfor index, row in test_data.iterrows():\n    tommorow = row['visit_date'] + offsets.Day()\n    # 土日だったらフラグを消す\n    if row['day_of_week'] == 'Saturday' or row['day_of_week'] == 'Sunday':\n        test_data.loc[index, 'holiday_flg'] = 0\n    if index + 1 < len(test_data):\n        row_next = test_data.loc[index + 1]\n        if row_next['store_id'] == row['store_id'] and (row_next['holiday_flg'] == 1 or row['day_of_week'] == 'Saturday'):\n            test_data.loc[index, 'tomorrow_holiday_flg'] = 1\n            continue\n    test_data.loc[index, 'tomorrow_holiday_flg'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = target_df.copy()\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 一旦保存\ntrain_data.to_csv('/kaggle/working/train_data.csv')\ntest_data.to_csv('/kaggle/working/test_data.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 読み直し\ntrain_data = pd.read_csv('/kaggle/working/train_data.csv')\ntest_data = pd.read_csv('/kaggle/working/test_data.csv')\ntrain_data['visit_date'] = pd.to_datetime(train_data['visit_date'])\ntest_data['visit_date'] = pd.to_datetime(test_data['visit_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_data = train_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg', 'tomorrow_holiday_flg']).agg({'visitors':'median'}).reset_index()\nmean_data.columns = ['air_store_id', 'day_of_week', 'holiday_flg', 'tomorrow_holiday_flg','median_visitors_all']\nmean_data\n\n# # train_data(学習用データ)からair_store_idとdowをグルーピングしてvisitorsの中央値（median）を算出\n# agg_data = train_data.groupby(['air_store_id', 'dow']).agg({'visitors':'median'}).reset_index()\n\n# # agg_dataのカラム名をつける\n# agg_data.columns = ['air_store_id', 'dow', 'visitors']\n# agg_data['visitors']= agg_data['visitors']\n \n# # agg_dataを確認\n# agg_data.head(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(date_info)\n\n# data_infoの祝日フラグが1（オン）のデータを確認\ndate_info[date_info['holiday_flg'] == 1].head(10)\n\n# 土日でも祝日であれば祝日フラグが立っているが、土日は基本的に休日なので祝日にする必要はない\n# 土日の場合はフラグを0にするという処理を行う (土日かつ祝日の日付を取得し、フラグを0にする)\nweekend_hdays = date_info.apply((lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\ndate_info.loc[weekend_hdays, 'holiday_flg'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 新しい日付により大きい重みを与える\n# date_info.indexの値が小さい＝より昔のデータ\ndate_info['weight'] = (date_info.index + 1) / len(date_info) \n \n#ヘッダーとテイルの情報を出して確認\nprint(date_info.head())\ndate_info.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# air_visit_dataと重みを追加したdate_infoをマージさせてvisit_dataを作成\n# visit_dataから不必要なcalendar_dateを落とす\nvisit_data = train_data.merge(date_info, left_on=['visit_date'], right_on='calendar_date', how='left', suffixes=['','_y'])\nvisit_data.drop('calendar_date', axis=1, inplace=True)\nvisit_data.drop('day_of_week_y', axis=1, inplace=True)\nvisit_data.drop('holiday_flg_y', axis=1, inplace=True)\n# visit_dataの実客数にnp.log1pの対数関数を使って処理\n# なぜ？\nvisit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n# visit_dataの確認\nvisit_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wmean（重み付き平均）の式を格納\nwmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n# グルーピングして重み付き平均を算出\nvisitors = visit_data.groupby(\n['air_store_id', 'day_of_week', 'holiday_flg', 'tomorrow_holiday_flg']).apply(wmean).reset_index()\nvisitors.rename(columns={0:'weighted_visitors_all'}, inplace=True) \n# データを確認\nvisitors\n\n# air_00a91d42b08b08d9 Monday\n# 祝日フラグあり、なしで重みが違うため、微妙に違う計算結果になっている","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_data = pd.merge(mean_data, visitors, on=['air_store_id', 'day_of_week', 'holiday_flg', 'tomorrow_holiday_flg'])\nagg_data['median_visitors_all'] = agg_data.median_visitors_all.map(pd.np.log1p)\nagg_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df = pd.merge(train_data, agg_data, on=['air_store_id','day_of_week','holiday_flg','tomorrow_holiday_flg'], how='left')\nmerged_df = merged_df.drop('Unnamed: 0', axis=1)\nmerged_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.drop('Unnamed: 0', axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.rename(columns={'store_id':'air_store_id'}, inplace=True)\nmerged_test_data = pd.merge(test_data, agg_data, on=['air_store_id','day_of_week','holiday_flg','tomorrow_holiday_flg'], how='left').drop('id',axis=1)\nmerged_test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = merged_df.copy()\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = merged_test_data.copy()\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 日付から曜日を算出 (0-6, 0:mondayとして数値に変換される)\none_hot = pd.get_dummies(train['day_of_week'])\ntrain = train.join(one_hot)\ntrain = train.drop('day_of_week',axis=1)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot = pd.get_dummies(test['day_of_week'])\ntest = test.join(one_hot)\ntest = test.drop('day_of_week',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠損値を平均値で埋める\ntest['median_visitors_all'] = test['median_visitors_all'].fillna(2.772849)\ntest['weighted_visitors_all'] = test['weighted_visitors_all'].fillna(2.727645)\ntest.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['date_year'] = train['visit_date'].dt.year\ntrain['date_month'] = train['visit_date'].dt.month\ntrain['date_day'] = train['visit_date'].dt.day\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_year'] = test['visit_date'].dt.year\ntest['date_month'] = test['visit_date'].dt.month\ntest['date_day'] = test['visit_date'].dt.day\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['date_year'] = train['date_year'] - 2016.5\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_year'] = test['date_year'] - 2016.5\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['date_month'] = train.date_month.map(pd.np.log1p)\ntrain['date_day'] = train.date_day.map(pd.np.log1p)\ntrain['visitors'] = train.visitors.map(pd.np.log1p)\ntest['date_month'] = test.date_month.map(pd.np.log1p)\ntest['date_day'] = test.date_day.map(pd.np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train[train['visit_date'] <= '2017-01-28'].reset_index().drop('index',axis=1).drop('visit_date',axis=1)\nvalid = train[train['visit_date'] > '2017-01-28'].reset_index().drop('index',axis=1).drop('visit_date',axis=1)\nprint(train_df)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_y = train_df['visitors']\nvalid_y = valid['visitors']\ntrain_df_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_X = train_df.copy().drop(['visitors','air_store_id'], axis=1)\nvalid_X = valid.copy().drop(['visitors','air_store_id'], axis=1)\ntest_df = test.copy().drop(['air_store_id', 'visit_date'], axis=1)\nprint(test_df)\nvalid_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\nlgb_train = lgb.Dataset(train_df_X, train_df_y)\nlgb_eval = lgb.Dataset(valid_X, valid_y)\nparams = {'metric': 'rmse','max_depth' : -1}\ngbm = lgb.train(params,\n               lgb_train,\n               valid_sets=(lgb_train, lgb_eval),\n               num_boost_round=10000,\n               early_stopping_rounds=100,\n               verbose_eval=50)\nlgb.plot_importance(gbm, height=0.5, figsize=(8,16))\n\nvalid_y_pred = gbm.predict(valid_X)\n\ny_pred = gbm.predict(test_df)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sample_submission.drop(['store_id','visit_date'],axis=1)\nsubmission = pd.concat([submission, pd.Series(y_pred)],axis=1)\nsubmission = submission.rename(columns={0:'visitors'})\nsubmission['visitors'] = submission.visitors.map(pd.np.expm1)\nsubmission\n \n# # 提出フォーマットの規定に合うように処理してsub_fileへ格納\n# submission = submission[['id', 'visitors']]\n# final['visitors'][final['visitors'] ==0] = submission['visitors'][final['visitors'] ==0]\n# sub_file = final.copy()\n \n# # データの確認\n# sub_file.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('/kaggle/working/submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nfit_params = {\n    'eval_metric': 'rmse',\n    'eval_set': [[train_df_X,train_df_y]]\n    }\n \n#グリッドサーチの範囲\nparams = {\n    'learning_rate': list(np.arange(0.05, 0.41, 0.05)),\n    'max_depth': list(np.arange(3, 11, 1))\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef GSfit(params):\n    regressor = xgb.XGBRegressor(n_estimators=100)\n    grid = GridSearchCV(regressor, params, cv=3, scoring='neg_mean_squared_error',verbose=2)\n    grid.fit(train_df_X,train_df_y)\n    return grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = GSfit(params)\ngrid_best_params = grid.best_params_\ngrid_scores_df = pd.DataFrame(grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#関数の処理で必要なライブラリ\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n#データ可視化ライブラリ\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#予測値と正解値を描写する関数\ndef True_Pred_map(pred_df):\n    RMSE = np.sqrt(mean_squared_error(pred_df['true'], pred_df['pred']))\n    R2 = r2_score(pred_df['true'], pred_df['pred']) \n    plt.figure(figsize=(8,8))\n    ax = plt.subplot(111)\n    ax.scatter('true', 'pred', data=pred_df)\n    ax.set_xlabel('True Value', fontsize=15)\n    ax.set_ylabel('Pred Value', fontsize=15)\n    ax.set_xlim(pred_df.min().min()-0.1 , pred_df.max().max()+0.1)\n    ax.set_ylim(pred_df.min().min()-0.1 , pred_df.max().max()+0.1)\n    x = np.linspace(pred_df.min().min()-0.1, pred_df.max().max()+0.1, 2)\n    y = x\n    ax.plot(x,y,'r-')\n    plt.text(0.1, 0.9, 'RMSE = {}'.format(str(round(RMSE, 5))), transform=ax.transAxes, fontsize=15)\n    plt.text(0.1, 0.8, 'R^2 = {}'.format(str(round(R2, 5))), transform=ax.transAxes, fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pd.concat([valid_y.reset_index(drop=True), pd.Series(valid_y_pred)], axis=1)\npred_df.columns = ['true', 'pred']\n\nTrue_Pred_map(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = grid.best_estimator_ \nvalid_y_pred = model.predict(valid_X)\ny_pred = model.predict(test_df)\n\npred_df = pd.concat([valid_y.reset_index(drop=True), pd.Series(valid_y_pred)], axis=1)\npred_df.columns = ['true', 'pred']\n\nTrue_Pred_map(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_submission = sample_submission.drop(['store_id','visit_date'],axis=1)\nxgb_submission = pd.concat([xgb_submission, pd.Series(y_pred)],axis=1)\nxgb_submission = xgb_submission.rename(columns={0:'visitors'})\nxgb_submission['visitors'] = xgb_submission.visitors.map(pd.np.expm1)\nxgb_submission\n \n# # 提出フォーマットの規定に合うように処理してsub_fileへ格納\n# submission = submission[['id', 'visitors']]\n# final['visitors'][final['visitors'] ==0] = submission['visitors'][final['visitors'] ==0]\n# sub_file = final.copy()\n \n# # データの確認\n# sub_file.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_merge = submission.copy()\nsubmission_merge = pd.merge(submission_merge, xgb_submission,on='id',suffixes=['','_y'])\nsubmission_merge['mean_visitors'] = round((submission_merge['visitors'] + submission_merge['visitors_y']) / 2)\nsubmission_merge = submission_merge.drop(['visitors', 'visitors_y'], axis=1)\nsubmission_merge = submission_merge.rename(columns={'mean_visitors':'visitors'})\nsubmission_merge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_merge.to_csv('/kaggle/working/submission_merge.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submissionのIDをレストランIDや日付に分ける\nsample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\nsample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n \n# 重み付き平均で予測したvisitorsとsample_submissionをマージする\n# 祝日データを同時にマージし、祝日フラグで紐付ける\nsample_submission.drop('visitors', axis=1, inplace=True)\nsample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\nsample_submission = sample_submission.merge(\n    visitors, on=['air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n \n# データセットを確認\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 「air_store_id」と「day_of_week」のみで欠損データに重み平均を入れる\nmissings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), how='left')['visitors_y'].values\n \n# 改めて欠損データの確認\nmissing_values_table(sample_submission)\n\n# まだ余っているので、air_store_idだけで紐付ける","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 「air_store_id」のみの重み付き平均を計算して欠損データへ入れる\nmissings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n    on='air_store_id', how='left')['visitors_y'].values\n \n# 欠損データを確認\nmissing_values_table(sample_submission)\n\n# 全部埋まっている","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visitorsをnp.expm1で処理して実客数へ戻す\nsample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n \n# 提出フォーマットの規定に合うように処理してsub_fileへ格納\nsample_submission = sample_submission[['id', 'visitors']]\nfinal['visitors'][final['visitors'] ==0] = sample_submission['visitors'][final['visitors'] ==0]\nsub_file = final.copy()\n \n# データの確認\nsub_file.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train(学習用データ)からair_store_idとdowをグルーピングしてvisitorsの中央値（median）を算出\nagg_data = train.groupby(['air_store_id', 'dow']).agg({'visitors':'median'}).reset_index()\n\n# agg_dataのカラム名をつける\nagg_data.columns = ['air_store_id', 'dow', 'visitors']\nagg_data['visitors']= agg_data['visitors']\n \n# agg_dataを確認\nagg_data.head(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testデータのair_store_idを保存し、後でtrainに適応させる\nstore_id = []\n\nfor i in test_data['store_id'].unique():\n  store_id += [i]\n\nprint(store_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IDを処理する(test)\nfor index, name in enumerate(store_id):\n  test_data.replace(name, index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['date_year'] = test_data['visit_date'].dt.year\ntest_data['date_month'] = test_data['visit_date'].dt.month\ntest_data['date_day'] = test_data['visit_date'].dt.day\ntest_data = test_data.drop('visit_date',axis=1)\ntest_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.merge(train_data, pd.DataFrame(store_id, columns=[\"air_store_id\"]), \n                                    how='inner', on='air_store_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IDを処理する(train)\nfor index, name in enumerate(store_id):\n  train_data.replace(name, index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['visit_date'] = pd.to_datetime(train_data['visit_date'])\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['date_year'] = train_data['visit_date'].dt.year\ntrain_data['date_month'] = train_data['visit_date'].dt.month\ntrain_data['date_day'] = train_data['visit_date'].dt.day\ntrain_data = train_data.drop('visit_date',axis=1)\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.drop(columns='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 型を直す\ntrain_data = train_data.astype('int64')\ntest_data = test_data.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.to_csv('train_data.csv')\ntest_data.to_csv('test_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('train_data.csv')\ntest_df = pd.read_csv('test_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 目的変数\ny = train_df['visitors']\n# 説明変数\nX = train_df.drop(columns=['visitors', 'Unnamed: 0'])\n\n# いらない列を消す\ntest_df = test_df.drop(columns=['Unnamed: 0'])\ntest_df = test_df.rename(columns={'store_id': 'air_store_id'})\ntest_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\n\nestimator = xgboost.XGBRegressor()\nestimator.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = estimator.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(path_list[6])\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sample_submission.copy()\nsubmission = submission.drop('visitors', axis=1)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"visitors\"] = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 一旦提出データを作成\n# 各曜日の来客数の中央値を予測値として提出する\n\n# test_dfとagg_dataのstoreid_id、dowをすり合わせmergeさせる\nmerged = pd.merge(submission_data, agg_data, how='left', left_on=['store_id', 'dow'], right_on=['air_store_id', 'dow'])\n \n# idとvisitorsだけをfinalへ格納\nfinal = merged[['id', 'visitors']]\n \n# finalのヘッダー情報\nfinal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NaNを探してテーブルにする関数\ndef missing_values_table(df): \n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum()/len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        return mis_val_table_ren_columns \n \n# finalのNaNを確認 (3.5%程度が欠損値)\nmissing_values_table(final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# とりあえず欠損値には0を入れる\nfinal.fillna(0, inplace=True)\n\n# 念のため確認\nmissing_values_table(final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# より良い予測にするため、重み付き平均を算出\n- 祝日と曜日に注目し、ここで重みをつける\n- また、直近のデータを重視させるような重み付けを行う","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path_list = []\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path_list.append(os.path.join(dirname, filename))\n        \npath_list.sort()\nprint(path_list)\nair_reserve = pd.read_csv(path_list[0])\nair_store_info = pd.read_csv(path_list[1])\nair_visit_data = pd.read_csv(path_list[2])\ndate_info = pd.read_csv(path_list[3])\nhpg_reserve = pd.read_csv(path_list[4])\nhpg_store_info = pd.read_csv(path_list[5])\nsample_submission = pd.read_csv(path_list[6])\nstore_id_relation = pd.read_csv(path_list[7])\n\nprint(date_info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(date_info)\n\n# data_infoの祝日フラグが1（オン）のデータを確認\ndate_info[date_info['holiday_flg'] == 1].head(10)\n\n# 土日でも祝日であれば祝日フラグが立っているが、土日は基本的に休日なので祝日にする必要はない\n# 土日の場合はフラグを0にするという処理を行う (土日かつ祝日の日付を取得し、フラグを0にする)\nweekend_hdays = date_info.apply((lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\ndate_info.loc[weekend_hdays, 'holiday_flg'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 新しい日付により大きい重みを与える\n# date_info.indexの値が小さい＝より昔のデータ\ndate_info['weight'] = (date_info.index + 1) / len(date_info) \n \n#ヘッダーとテイルの情報を出して確認\nprint(date_info.head())\ndate_info.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# air_visit_dataと重みを追加したdate_infoをマージさせてvisit_dataを作成\n# visit_dataから不必要なcalendar_dateを落とす\nvisit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\nvisit_data.drop('calendar_date', axis=1, inplace=True)\n# visit_dataの実客数にnp.log1pの対数関数を使って処理\n# なぜ？\nvisit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n# visit_dataの確認\nvisit_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wmean（重み付き平均）の式を格納\nwmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n# グルーピングして重み付き平均を算出\nvisitors = visit_data.groupby(\n['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\nvisitors.rename(columns={0:'visitors'}, inplace=True) \n# データを確認\nvisitors.head(10)\n\n# air_00a91d42b08b08d9 Monday\n# 祝日フラグあり、なしで重みが違うため、微妙に違う計算結果になっている","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submissionのIDをレストランIDや日付に分ける\nsample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\nsample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n \n# 重み付き平均で予測したvisitorsとsample_submissionをマージする\n# 祝日データを同時にマージし、祝日フラグで紐付ける\nsample_submission.drop('visitors', axis=1, inplace=True)\nsample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\nsample_submission = sample_submission.merge(\n    visitors, on=['air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n \n# データセットを確認\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submissionの欠損データを確認\nmissing_values_table(sample_submission)\n\n# 祝日フラグで紐づかなかったものが余っている","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 「air_store_id」と「day_of_week」のみで欠損データに重み平均を入れる\nmissings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), how='left')['visitors_y'].values\n \n# 改めて欠損データの確認\nmissing_values_table(sample_submission)\n\n# まだ余っているので、air_store_idだけで紐付ける","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 「air_store_id」のみの重み付き平均を計算して欠損データへ入れる\nmissings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n    on='air_store_id', how='left')['visitors_y'].values\n \n# 欠損データを確認\nmissing_values_table(sample_submission)\n\n# 全部埋まっている","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 内容を確認\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visitorsをnp.expm1で処理して実客数へ戻す\nsample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n \n# 提出フォーマットの規定に合うように処理してsub_fileへ格納\nsample_submission = sample_submission[['id', 'visitors']]\nfinal['visitors'][final['visitors'] ==0] = sample_submission['visitors'][final['visitors'] ==0]\nsub_file = final.copy()\n \n# データの確認\nsub_file.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 上記で出した中央値、加重平均のさらに平均をとって提出\n- アンサンブルの考え方\n- 複数のモデルを作成し、それらのモデルの予測結果をマージして最終結果とする","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 算術平均をnp.meanで算出\nsub_file['visitors'] = np.mean([final['visitors'], sample_submission['visitors']], axis = 0)\nsub_file.to_csv('sub_math_mean_1.csv', index=False)\n \n# 相乗平均を算出\nsub_file['visitors'] = (final['visitors'] * sample_submission['visitors']) ** (1/2)\nsub_file.to_csv('sub_geo_mean_1.csv', index=False)\n \n# 調和平均を算出\nsub_file['visitors'] = 2/(1/final['visitors'] + 1/sample_submission['visitors'])\nsub_file.to_csv('sub_hrm_mean_1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 時系列系コンペで必要なこと\n- 今回の手法\n    - 機械学習などを使わず、単純な平均のみでやった\n    - 基本的に時系列系だから〜〜を特殊に使用する、ということはなく、kaggle常連のランダムフォレストやLightGBM、XGBoost、ニューラルネットワークなどが使われているようだ。（https://www.hirayuki.com/kaggle-zakki/read-ion-switching-kernel-2 ）\n- 適切な前処理\n    - 曜日や祝日など、時期的なデータを特徴量にうまく加える必要がある（ここが一番だいじそう）\n        - 2日前、1日前の平均値や1週間前のデータなどを特徴量に落とし込み、学習させる必要がある\n    - 機械学習では非定常過程が多い時系列データについての予測は難しい？（https://tjo.hatenablog.com/entry/2019/09/18/190000 ）\n- 適切なcross validation\n    - 時系列データなので、交差検証も時系列を守る必要がある。（通常はランダムにサンプリングして使ったりできるが、それができない）\n    -> 過学習になりがち\n    - うまく時系列を壊さず交差検証ができるデータを作り、過学習にならないようにバリデーションすることも時系列解析では重要\n    \n- 基本的な方針\n    - 線形分析(kNN, ロジスティクス回帰など？）でだいじそうな特徴量を見積もり、それに対してlightGBMなど複数のモデルで学習を行う。最後に結果をマージして提出という流れが基本になっていそう。x","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}