{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Man kan blandt andet bruge disse algoritmer til time series\n\nArtificial Neural Networks (ANN), Autoregressive Integrated Moving Average (ARIMA) or Seasonal ARIMA (SARIMA), Support Vector Machines (SVM), k-Nearest Neighbors-regressor (kNN), XGBoost (xgb), Deep Learning (DL), RandomForestRegressor (RF), lightGbm (lgb),\nFuzzy Logic (FL), Bayesian Neural Networks (BNN), Simple Exponential Smoothing (SES), Wavelet Transform (WT), Holt-Winters (HW) models, and Gaussian Process (GP) \n"},{"metadata":{},"cell_type":"markdown","source":"# Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/metric/Metric.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Den metric som skal bruges er RMSLE, hvilket betyder, at en god score skal så tæt på 0 som muligt, og at underestimering straffes hårdere end overestimering"},{"metadata":{},"cell_type":"markdown","source":"Vi kan finde mean_squared_log_error (MSLE) i scikit learn og SquareRoot findes i numpy (np.sqrt), så vi kan gøre følgende for at beregne scoren"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\n#np.sqrt(mean_squared_log_error( y_test, predictions ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Beskrivelse af datasæt"},{"metadata":{},"cell_type":"markdown","source":"\nIn this competition, you are provided a time-series forecasting problem centered around restaurant visitors. The data comes from two separate sites:\n\nHot Pepper Gourmet (hpg): similar to Yelp, here users can search restaurants and also make a reservation online\nAirREGI / Restaurant Board (air): similar to Square, a reservation control and cash register system\nYou must use the reservations, visits, and other information from these sites to forecast future restaurant visitor totals on a given date. The training data covers the dates from 2016 until April 2017. The test set covers the last week of April and May of 2017. The test set is split based on time (the public fold coming first, the private fold following the public) and covers a chosen subset of the air restaurants. Note that the test set intentionally spans a holiday week in Japan called the \"Golden Week.\"\n\nThere are days in the test set where the restaurant were closed and had no visitors. These are ignored in scoring. The training set omits days where the restaurants were closed.\n\nFile Descriptions\n\nThis is a relational dataset from two systems. Each file is prefaced with the source (either air_ or hpg_) to indicate its origin. Each restaurant has a unique air_store_id and hpg_store_id. Note that not all restaurants are covered by both systems, and that you have been provided data beyond the restaurants for which you must forecast. Latitudes and Longitudes are not exact to discourage de-identification of restaurants.\n\nair_reserve.csv\n\nThis file contains reservations made in the air system. Note that the reserve_datetime indicates the time when the reservation was created, whereas the visit_datetime is the time in the future where the visit will occur.\n\nair_store_id - the restaurant's id in the air system\nvisit_datetime - the time of the reservation\nreserve_datetime - the time the reservation was made\nreserve_visitors - the number of visitors for that reservation\nhpg_reserve.csv\n\nThis file contains reservations made in the hpg system.\n\nhpg_store_id - the restaurant's id in the hpg system\nvisit_datetime - the time of the reservation\nreserve_datetime - the time the reservation was made\nreserve_visitors - the number of visitors for that reservation\nair_store_info.csv\n\nThis file contains information about select air restaurants. Column names and contents are self-explanatory.\n\nair_store_id\nair_genre_name\nair_area_name\nlatitude\nlongitude\nNote: latitude and longitude are the latitude and longitude of the area to which the store belongs\n\nhpg_store_info.csv\n\nThis file contains information about select hpg restaurants. Column names and contents are self-explanatory.\n\nhpg_store_id\nhpg_genre_name\nhpg_area_name\nlatitude\nlongitude\nNote: latitude and longitude are the latitude and longitude of the area to which the store belongs\n\nstore_id_relation.csv\n\nThis file allows you to join select restaurants that have both the air and hpg system.\n\nhpg_store_id\nair_store_id\nair_visit_data.csv\n\nThis file contains historical visit data for the air restaurants.\n\nair_store_id\nvisit_date - the date\nvisitors - the number of visitors to the restaurant on the date\nsample_submission.csv\n\nThis file shows a submission in the correct format, including the days for which you must forecast.\n\nid - the id is formed by concatenating the air_store_id and visit_date with an underscore\nvisitors- the number of visitors forecasted for the store and date combination\ndate_info.csv\n\nThis file gives basic information about the calendar dates in the dataset.\n\ncalendar_date\nday_of_week\nholiday_flg - is the day a holiday in Japan"},{"metadata":{},"cell_type":"markdown","source":"# Læs data ind"},{"metadata":{"trusted":true},"cell_type":"code","source":"air_r = pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/air_reserve.csv.zip')\nair_si = pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/air_store_info.csv.zip')\nair_vd = pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/air_visit_data.csv.zip')\ndate_info = pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/date_info.csv.zip')\nhpg_r = pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/hpg_reserve.csv.zip')\nhpg_si = pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/hpg_store_info.csv.zip')\nstore_id_r = pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/store_id_relation.csv.zip')\nss = pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/sample_submission.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"der er 32019 delt med 39 dage, dvs 821 restauranter"},{"metadata":{},"cell_type":"markdown","source":"# Concatenering\nman samler to dataframes (ved siden af hinanden) ved hjælp af følgende kald "},{"metadata":{"trusted":true},"cell_type":"code","source":"#result = pd.concat([df1, df2], axis=1, sort=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gridsearch"},{"metadata":{},"cell_type":"markdown","source":"Et eksempel på, hvordan man bruger Gridsearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"#parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n#svc = svm.SVC()\n#clf = GridSearchCV(svc, parameters)\n#clf.fit(iris.data, iris.target)\n#GridSearchCV(estimator=SVC(), param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgb1 = XGBRegressor()\n#parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n#              'objective':['reg:linear'],\n#              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n#              'max_depth': [5, 6, 7],\n#              'min_child_weight': [4],\n#              'silent': [1],\n#              'subsample': [0.7],\n#              'colsample_bytree': [0.7],\n#              'n_estimators': [500]}\n\n#xgb_grid = GridSearchCV(xgb1,\n#                        parameters,\n#                        cv = 2,\n#                        n_jobs = 5,\n#                        verbose=True)\n\n#xgb_grid.fit(x_train_full,\n#         y_train_full)\n\n#print(xgb_grid.best_score_)\n#print(xgb_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Aflevering\nMan afleverer til kaggle på følgende måde"},{"metadata":{},"cell_type":"markdown","source":"Det her er den genrelle måde at lave en submission-fil på. En liste af id'er og en liste af predictions, som begge er lige lange gemmes i en dataframe, som gemmes som csv-fil uden index"},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission = pd.DataFrame({'Id':test['Id'],'Prediction':predictions})\n#submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Opgaven"},{"metadata":{},"cell_type":"markdown","source":"Opgaven er at predicte sidste uge af april 2017 + hele maj måned 2017"},{"metadata":{},"cell_type":"markdown","source":"# Spørgsmål der skal besvares"},{"metadata":{},"cell_type":"markdown","source":"Er der en sammenhæng mellem reserve_datetime og visit_datetime?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob, re\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\nfrom datetime import datetime\nfrom xgboost import XGBRegressor\n\ndata = {\n    'ar': pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/air_reserve.csv.zip'),\n    'as': pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/air_store_info.csv.zip'),\n    'tra':  pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/air_visit_data.csv.zip'),\n    'hol': pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/date_info.csv.zip').rename(columns={'calendar_date':'visit_date'}),\n    'hr':  pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/hpg_reserve.csv.zip'),\n    'hs': pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/hpg_store_info.csv.zip'),\n    'id': pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/store_id_relation.csv.zip'),\n    'tes': pd.read_csv('/kaggle/input/recruit-restaurant-visitor-forecasting/sample_submission.csv.zip')\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n\nfor df in ['ar','hr']:\n    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n\ndata['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\ndata['tra']['year'] = data['tra']['visit_date'].dt.year\ndata['tra']['month'] = data['tra']['visit_date'].dt.month\ndata['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n\ndata['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\ndata['tes']['year'] = data['tes']['visit_date'].dt.year\ndata['tes']['month'] = data['tes']['visit_date'].dt.month\ndata['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n\nunique_stores = data['tes']['air_store_id'].unique()\nstores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sure it can be compressed...\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n\nstores = pd.merge(stores, data['as'], how='left', on=['air_store_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NEW FEATURES FROM Georgii Vyshnia\nstores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\nstores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\nlbl = preprocessing.LabelEncoder()\nfor i in range(10):\n    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\nstores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\nstores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n\ndata['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\ndata['hol']['visit_date'] = data['hol']['visit_date'].dt.date\ntrain = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \ntest = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n\ntrain = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \ntest = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n\nfor df in ['ar','hr']:\n    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n\ntrain['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n\ntrain['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\ntrain['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\ntrain['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n\ntest['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\ntest['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\ntest['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NEW FEATURES FROM JMBULL\ntrain['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntrain['var_max_lat'] = train['latitude'].max() - train['latitude']\ntrain['var_max_long'] = train['longitude'].max() - train['longitude']\ntest['var_max_lat'] = test['latitude'].max() - test['latitude']\ntest['var_max_long'] = test['longitude'].max() - test['longitude']\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NEW FEATURES FROM Georgii Vyshnia\ntrain['lon_plus_lat'] = train['longitude'] + train['latitude'] \ntest['lon_plus_lat'] = test['longitude'] + test['latitude']\n\nlbl = preprocessing.LabelEncoder()\ntrain['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\ntest['air_store_id2'] = lbl.transform(test['air_store_id'])\n\ncol = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = ensemble.GradientBoostingRegressor(learning_rate=0.2, random_state=3, n_estimators=200, subsample=0.8, \n                      max_depth =10)\nmodel2 = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=4)\nmodel3 = XGBRegressor(learning_rate=0.2, random_state=3, n_estimators=280, subsample=0.8, \n                      colsample_bytree=0.8, max_depth =12)\n\nmodel1.fit(train[col], np.log1p(train['visitors'].values))\nmodel2.fit(train[col], np.log1p(train['visitors'].values))\nmodel3.fit(train[col], np.log1p(train['visitors'].values))\n\npreds1 = model1.predict(train[col])\npreds2 = model2.predict(train[col])\npreds3 = model3.predict(train[col])\n\nprint('RMSE GradientBoostingRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds1))\nprint('RMSE KNeighborsRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds2))\nprint('RMSE XGBRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds3))\npreds1 = model1.predict(test[col])\npreds2 = model2.predict(test[col])\npreds3 = model3.predict(test[col])\n\ntest['visitors'] = 0.3*preds1+0.3*preds2+0.4*preds3\ntest['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\nsub1 = test[['id','visitors']].copy()\nsub1.to_csv('submission.csv', index=False)\ndel train; del data;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from hklee\n# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\ndfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n    pd.read_csv(fn)for fn in glob.glob('../input/*.csv')}\n\nfor k, v in dfs.items(): locals()[k] = v\n\nwkend_holidays = date_info.apply(\n    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\ndate_info.loc[wkend_holidays, 'holiday_flg'] = 0\ndate_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n\nvisit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\nvisit_data.drop('calendar_date', axis=1, inplace=True)\nvisit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n\nwmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\nvisitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\nvisitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\nsample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\nsample_submission.drop('visitors', axis=1, inplace=True)\nsample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\nsample_submission = sample_submission.merge(visitors, on=[\n    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n\nmissings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n    how='left')['visitors_y'].values\n\nmissings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n    on='air_store_id', how='left')['visitors_y'].values\n\nsample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\nsub2 = sample_submission[['id', 'visitors']].copy()\nsub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n\nsub_merge['visitors'] = 0.7*sub_merge['visitors_x'] + 0.3*sub_merge['visitors_y']* 1.1\nsub_merge[['id', 'visitors']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}