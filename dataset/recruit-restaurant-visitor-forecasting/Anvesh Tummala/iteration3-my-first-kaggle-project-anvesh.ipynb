{"cells":[{"source":"**1. Exploration of data**","metadata":{"_cell_guid":"405475de-e4aa-4af4-bbe9-c05f6bde8885","_uuid":"15343c91ab1de181c1613e3c2f93f3960750b417"},"cell_type":"markdown"},{"source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom IPython.display import display # Allows the use of display() for DataFrames\nimport matplotlib.pyplot as plt\nfrom datetime import date\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# Input data files are available in the \"../input/\" directory.\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/recruit-restaurant-visitor-forecasting-data\"]).decode(\"utf8\"))","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"4b061695-aa4b-4aca-81f3-28b79e45bab9","_uuid":"b15294b73d6c593f26c0de023dc2ae168742e244"},"cell_type":"code"},{"source":"weather_data = pd.read_csv('../input/recruit-restaurant-visitor-forecasting-data/WeatherData.csv', parse_dates=['calendar_date'])\nweather_data.columns = weather_data.columns.str.replace('area_name', 'station_id')\n#weather_data = weather_data[weather_data['station_id'].notnull()]\n\nanm = pd.read_csv('../input/recruit-restaurant-visitor-forecasting-data/area_name_mapping.csv')\ndisplay(weather_data.describe())\nprint(len(weather_data.station_id.unique()), \", total weather records: \", len(weather_data))","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"2381f1fb-e10e-4bf3-8d00-f597292ccdab","collapsed":true,"_uuid":"bfc2b919bcd1b4550b165980bcb66e53d6bf6e20"},"cell_type":"code"},{"source":"display(anm.head())\nprint(len(anm))\nanm_station_ids = anm.station_id.unique()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"c29369b0-0084-417f-9e45-2ec87f302aa9","collapsed":true,"_uuid":"622ff24e7b0ec49f06654a686efd56d56e81a296"},"cell_type":"code"},{"source":"weather_station_data = weather_data[weather_data.station_id.isin(anm_station_ids)]\ndisplay(weather_station_data.station_id.unique())\nprint(len(weather_station_data.station_id.unique()), \"total weather data records: \", len(weather_station_data))","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"86288c68-d4db-4179-9ed9-ba8a848bf97a","collapsed":true,"_uuid":"3fd300884b979904c38ae1dc2aaeaab956eda57a"},"cell_type":"code"},{"source":"weather_station_data.describe()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"c0dfa3d4-203b-498b-9e83-349ffc0e2632","collapsed":true,"_uuid":"a49ce34b30d6e50532bd74bb12e0669f9fb4794b"},"cell_type":"code"},{"source":"weather_station_data_filled = weather_station_data.interpolate()\nweather_station_data_filled.describe()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"b09dbc59-9117-41cd-8e35-3b32b488a70e","collapsed":true,"_uuid":"6f3d24ba5584a430bc3bf34b91026323e3fb30bb"},"cell_type":"code"},{"source":"\nsample_result = pd.read_csv('../input/recruit-restaurant-visitor-forecasting-data/sample_submission.csv')\nfeatures_to_train = sample_result.copy(deep=True)\ndisplay(features_to_train.head())\nfeatures_to_train['air_store_id'] = features_to_train.id.str.slice(0, 20)\nfeatures_to_train['visit_date'] = pd.to_datetime(features_to_train.id.str.slice(21, 31))\nfeatures_to_train.drop(['visitors', 'id'], axis = 1, inplace=True)\nprint(len(features_to_train.air_store_id.unique()))\nprint(len(features_to_train.visit_date.unique()))\ndisplay(features_to_train.head())\n\n\ndi = pd.read_csv('../input/recruit-restaurant-visitor-forecasting-data/date_info.csv', parse_dates=['calendar_date'])\nfeatures_to_train = pd.merge(features_to_train, di, left_on = 'visit_date', right_on = 'calendar_date', how = 'left')\nfeatures_to_train.drop(['calendar_date'], axis = 1, inplace=True)\n\nairstore = pd.read_csv('../input/recruit-restaurant-visitor-forecasting-data/air_store_info.csv')\nfeatures_to_train = pd.merge(features_to_train,airstore,on=['air_store_id'])\n\ndisplay(features_to_train.head())\ndisplay(weather_station_data_filled.head())\n\nfeatures_to_train = pd.merge(features_to_train, anm, on='air_area_name')\nfeatures_to_train = pd.merge(features_to_train, weather_station_data_filled, left_on=['station_id','visit_date'], right_on=['station_id','calendar_date'])\n\nresult_data_stub = features_to_train.copy(deep=True)\nfeatures_to_train['year'] = features_to_train['visit_date'].dt.year\nfeatures_to_train['month'] = features_to_train['visit_date'].dt.month\nfeatures_to_train['day'] = features_to_train['visit_date'].dt.day\n\nfeatures_to_train.drop(['visit_date', 'calendar_date', 'total_snowfall', 'deepest_snowfall',\n                  'avg_vapor_pressure', 'avg_humidity', 'avg_sea_pressure', 'avg_local_pressure',\n                  'solar_radiation', 'cloud_cover', 'high_temperature','low_temperature',\n                        'avg_temperature', 'avg_wind_speed', 'hours_sunlight',\n                        'station_id', 'precipitation'], axis=1, inplace=True)\n\nprint(len(features_to_train))\ndisplay(features_to_train.head())\n\nair_selected_store_ids = features_to_train.air_store_id.unique()\nprint(len(air_selected_store_ids))\n","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"98d66c58-9d29-4b5f-b149-aeb86a3bc63f","collapsed":true,"_uuid":"15458191a40e7bb1a8506eccd1cb5ff2e7aa66c0"},"cell_type":"code"},{"source":"avd = pd.read_csv('../input/recruit-restaurant-visitor-forecasting-data/air_visit_data.csv', parse_dates=['visit_date'])\ndi = pd.read_csv('../input/recruit-restaurant-visitor-forecasting-data/date_info.csv', parse_dates=['calendar_date'])\ndisplay(avd.head(2))\ndisplay(di.head(2))\n\navd_di = pd.merge(avd, di, left_on = 'visit_date', right_on = 'calendar_date', how = 'left') \navd_di.drop(['calendar_date'], axis = 1, inplace=True)\n\ndisplay(avd_di.head(2))\nprint(\"The AIR visitor data count\",len(avd), \". The AIR visitor data with holiday info : \", len(avd_di), \"records.\")\nprint('Total AIR restaurents in AVD', len(avd.air_store_id.value_counts()))","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"5d6c6df8-9634-48d5-9b77-bad6f7b540a6","collapsed":true,"_uuid":"0f9166d6171a906d7f187573d65e47f68f219cb4"},"cell_type":"code"},{"source":"asi = pd.read_csv('../input/recruit-restaurant-visitor-forecasting-data/air_store_info.csv')\ndisplay(asi.head())\n\nprint( 'Total AIR restaurents: ', len(asi))\nprint('Total AIR restaurents in ASI', len(asi.air_store_id.value_counts()))\n\n#air = pd.merge(airres,airstore,on=['air_store_id'])","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"71dd4438-dafd-493b-ac3a-3e25fe7119b9","collapsed":true,"_uuid":"4ab1ce9027f0d79a6de572258b04b3a558295373"},"cell_type":"code"},{"source":"air = pd.merge(avd_di,asi,on=['air_store_id'])\n#air.drop(['air_area_name'], axis = 1, inplace=True)\ndisplay(air.head(2))\nprint('Total AIR restaurents count: ', len(air.air_store_id.value_counts()))\n","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"7e6816a8-3d32-4d6b-9581-a771a8119528","collapsed":true,"_uuid":"c93b0d122bc6060b2cfe80e1ab94c8e84e638dd2"},"cell_type":"code"},{"source":"plt.rcParams['figure.figsize'] = 16, 8\nplt.show()\n","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"74b33ece-695f-4937-9b56-58fc97a083fb","collapsed":true,"_uuid":"3e0462073f1053e68659cad5ccd5ccc6d762d2b3"},"cell_type":"code"},{"source":"airdata = air\nairdata['year'] = air['visit_date'].dt.year\nairdata['month'] = air['visit_date'].dt.month\nairdata['day'] = air['visit_date'].dt.day\n#airdata = airdata.drop(['visit_date'], axis = 1)\nairdata.head()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"29d323bf-4489-43bf-9414-70014c015f5a","collapsed":true,"_uuid":"bde708464ae3656f9d5e268be62f09aaf003301e"},"cell_type":"code"},{"source":"print(len(airdata))\nairdata_with_sid = pd.merge(airdata, anm, on='air_area_name')\nprint(len(airdata_with_sid))\nairdata_with_sid.head()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"a842857a-3f6c-4651-8b03-a911a497cbc7","collapsed":true,"_uuid":"77870b6ff2f2997dc3f05a49e514324bebc73bd1"},"cell_type":"code"},{"source":"display(weather_station_data_filled.head(2))","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"9a9e57d8-a3e4-4838-8954-b5c281120e1b","collapsed":true,"_uuid":"b7f5f30df3e945ce6a04d104b1ef769c700cd84d"},"cell_type":"code"},{"source":"print(len(airdata_with_sid))\nairdata_full = pd.merge(airdata_with_sid, weather_station_data_filled, left_on=['station_id','visit_date'], right_on=['station_id','calendar_date'])\nprint(len(airdata_full))","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"2bde1e05-94b9-470b-a2fb-f650c0c488f9","collapsed":true,"_uuid":"44232ca304d0fdfa2da3d1e8c84e2872275866a3"},"cell_type":"code"},{"source":"display(airdata_full.head())\nairdata_full.isnull().sum()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"69197216-1292-4489-9d33-eabc35b918a2","collapsed":true,"_uuid":"1f3c3473904d407ba04739bff4da8a82fd2b275a"},"cell_type":"code"},{"source":"airdata_full.dtypes","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"fb55c15f-4347-4bc8-a4a2-635efe2c658b","collapsed":true,"_uuid":"c561ca099923bd940dbd56d2355b57718724c1f3"},"cell_type":"code"},{"source":"#airdata_full.drop(['visit_date', 'calendar_date', 'total_snowfall', 'deepest_snowfall'], axis=1, inplace=True)\nairdata_full.drop(['visit_date', 'calendar_date', 'total_snowfall', 'deepest_snowfall',\n                  'avg_vapor_pressure', 'avg_humidity', 'avg_sea_pressure', 'avg_local_pressure',\n                  'solar_radiation', 'cloud_cover', 'high_temperature','low_temperature',\n                   'avg_temperature', 'avg_wind_speed', 'hours_sunlight',\n                   'station_id','precipitation'], axis=1, inplace=True)\n\ndisplay(airdata_full.head())","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"aa6a5b0c-70b1-484c-9457-ba2e2eb38d9b","collapsed":true,"_uuid":"da66797eb1a8f4c2d49241c017ddd310431e6a48"},"cell_type":"code"},{"source":"airdata_full.isnull().sum()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"5b43599d-28b8-4a7d-a0ec-7e9bccfc5736","collapsed":true,"_uuid":"619a050238c1de3077abcc121ea8acc356a41bad"},"cell_type":"code"},{"source":"adff = airdata_full.interpolate()\ndisplay(adff.head())\ndisplay(adff.describe())\ndisplay(airdata_full.describe())","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"d65f770f-8428-409e-a0e2-98e3cafbda6f","collapsed":true,"_uuid":"30d909b09503e62e1a7db287d31ad88bef7a5b17"},"cell_type":"code"},{"source":"# Split the data into features and target label\nprint(len(adff.air_store_id.unique()))\nairdata = adff[adff.air_store_id.isin(air_selected_store_ids)]\nprint(len(airdata.air_store_id.unique()))\n\nvisitors_data = airdata['visitors']\nfeatures_data = airdata.drop('visitors', axis = 1)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"e3a0130d-66cf-4443-92b3-d397bb5b4822","collapsed":true,"_uuid":"c29c867d851f413f7e8f9f5a47713b511f919cb1"},"cell_type":"code"},{"source":"print('Check the training data: ')\ndisplay(visitors_data.head())\ndisplay(features_data.head())","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"4f390621-ff4b-4cba-aaba-0250ea274775","collapsed":true,"_uuid":"b0fe3ddd4ff03a60b593139e5d10bb3cdef53935"},"cell_type":"code"},{"source":"# Import sklearn.preprocessing.StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\n#visitors\tholiday_flg\tlatitude\tlongitude\tyear\tmonth\tday\tprecipitation\t\n#avg_temperature\thours_sunlight\tavg_wind_speed\tavg_vapor_pressure\tavg_humidity\t\n#avg_sea_pressure\tavg_local_pressure\tsolar_radiation\tcloud_cover\thigh_temperature\t\n#low_temperature\n#numerical = ['latitude', 'longitude', 'year', 'month', 'day', 'precipitation','avg_temperature', \n#             'hours_sunlight', 'avg_wind_speed', 'avg_vapor_pressure', 'avg_humidity', 'avg_sea_pressure',\n#            'avg_local_pressure', 'solar_radiation', 'cloud_cover', 'high_temperature', 'low_temperature']\nnumerical = [ 'year', 'month', 'day', 'latitude', 'longitude']\nfeatures_minmax_transform = pd.DataFrame(data = features_data)\nfeatures_minmax_transform[numerical] = scaler.fit_transform(features_data[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(features_minmax_transform.head(n = 5))\n\nair_selected_store_ids = features_minmax_transform['air_store_id'].unique()\nprint(len(air_selected_store_ids))\n#print(air_selected_store_ids)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"9ab1ab4f-defd-4a2c-a9d3-80dafbc27575","collapsed":true,"_uuid":"d863b692847d2f3bbebe7d6a84f935b19a24eca9"},"cell_type":"code"},{"source":"print(len(features_data.air_store_id.unique()))\nprint(len(features_data.day_of_week.unique()))\nprint(len(features_data.air_genre_name.unique()))\nprint(len(features_data.air_area_name.unique()))\nprint(6)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"077fa851-0752-493c-a717-b4314a14ebcd","collapsed":true,"_uuid":"73c6284f4ac9dfc49d5d5e35e77d68e9415c0c34"},"cell_type":"code"},{"source":"# One-hot encode the 'features_minmax_transform' data using pandas.get_dummies()\nfeatures_final = pd.get_dummies(features_minmax_transform)\n\n# Print the number of features after one-hot encoding\nencoded = list(features_final.columns)\nprint(len(encoded),' total features after one-hot encoding.')\n\n# Uncomment the following line to see the encoded feature names\n#print(encoded)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"df241601-184d-4fe8-9a68-8217ae382bd7","collapsed":true,"_uuid":"2350efdfa10fe07b03ff6addef7113cd31704d4f"},"cell_type":"code"},{"source":"# Import train_test_split\nfrom sklearn.cross_validation import train_test_split\n\n# Split the 'features' and 'visitors_data' data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features_final, \n                                                    visitors_data, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has \",X_train.shape[0], \" samples.\")\nprint(\"Testing set has \",X_test.shape[0],\" samples.\")\nprint(X_train.shape, X_test.shape, len(y_train), len(y_train)/ 10, len(y_train)/100 )","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"f2a1e288-0f70-4674-a15f-587caf012866","collapsed":true,"_uuid":"1958380552e662983d02985d3513588013c7ed4e"},"cell_type":"code"},{"source":"import math\n\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    terms_to_sum = [(math.log(float(y_pred[i]) + 1) - math.log(float(y[i]) + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(terms_to_sum) * (1.0/len(y))) ** 0.5","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"ebf63bf9-2d44-492f-a5e4-04acdf7ce6c5","collapsed":true,"_uuid":"c5509b88d69e71e4f222d780c26a2669aa94974e"},"cell_type":"code"},{"source":"# Import two metrics from sklearn - r2_score and explained_variance_score\nfrom sklearn.metrics import r2_score \nfrom sklearn.metrics import explained_variance_score \n\ndef train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - sample_size: the size of samples (number) to be drawn from training set\n       - X_train: features training set\n       - y_train: income training set\n       - X_test: features testing set\n       - y_test: income testing set\n    '''\n    \n    results = {}\n    \n    # Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n    start = time() # Get start time\n    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time() # Get end time\n    \n    # Calculate the training time\n    results['train_time'] = end-start\n        \n    # Get the predictions on the test set(X_test),\n    #       then get predictions on the first 300 training samples(X_train) using .predict()\n    start = time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train[:300])\n    end = time() # Get end time\n    \n    # Calculate the total prediction time\n    results['pred_time'] = end-start\n            \n    # Compute rmsle on the first 300 training samples which is y_train[:300]\n    results['rmsle_train'] = explained_variance_score(y_train[:300], predictions_train)\n        \n    # Compute rmsle on test set using explained_variance_score()\n    results['rmsle_test'] = explained_variance_score(y_test, predictions_test)\n    \n    # Compute F-score on the the first 300 training samples using fbeta_score()\n    results['r2_train'] = r2_score(y_train[:300], predictions_train)\n        \n    # Compute F-score on the test set which is y_test\n    results['r2_test'] = r2_score(y_test, predictions_test)\n       \n    # Success\n    print(learner.__class__.__name__, \" trained on \",sample_size,\" samples.\")\n        \n    # Return the results\n    return results","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"29af28a9-9edd-41fb-8ef5-cdfbf70aeeae","collapsed":true,"_uuid":"07ac3c82227ccad395a8b4257389b18043a6397c"},"cell_type":"code"},{"source":"import matplotlib.pyplot as pl\nimport matplotlib.patches as mpatches\n\ndef evaluate(results, rmsle, r2):\n    \"\"\"\n    Visualization code to display results of various learners.\n    \n    inputs:\n      - learners: a list of supervised learners\n      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n      - rmsle: The score for the naive predictor\n      - r2: The score for the naive predictor\n    \"\"\"\n  \n    # Create figure\n    fig, ax = pl.subplots(2, 3, figsize = (11,7))\n\n    # Constants\n    bar_width = 0.3\n    colors = ['#A00000','#00A0A0','#00A000']\n    \n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(['train_time', 'rmsle_train', 'r2_train', 'pred_time', 'rmsle_test', 'r2_test']):\n            for i in np.arange(3):\n                \n                # Creative plot code\n                ax[int(j/3), int(j%3)].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n                ax[int(j/3), int(j%3)].set_xticks([0.45, 1.45, 2.45])\n                ax[int(j/3), int(j%3)].set_xticklabels([\"1%\", \"10%\", \"100%\"])\n                ax[int(j/3), int(j%3)].set_xlabel(\"Training Set Size\")\n                ax[int(j/3), int(j%3)].set_xlim((-0.1, 3.0))\n    \n    # Add unique y-labels\n    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n    ax[0, 1].set_ylabel(\"rmsle\")\n    ax[0, 2].set_ylabel(\"R2-score\")\n    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n    ax[1, 1].set_ylabel(\"rmsle\")\n    ax[1, 2].set_ylabel(\"R2-score\")\n    \n    # Add titles\n    ax[0, 0].set_title(\"Model Training\")\n    ax[0, 1].set_title(\"rmsle on Training Subset\")\n    ax[0, 2].set_title(\"R2-score on Training Subset\")\n    ax[1, 0].set_title(\"Model Predicting\")\n    ax[1, 1].set_title(\"rmsle on Testing Set\")\n    ax[1, 2].set_title(\"R2-score on Testing Set\")\n    \n    # Add horizontal lines for naive predictors\n    ax[0, 1].axhline(y = rmsle, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    ax[1, 1].axhline(y = rmsle, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    ax[0, 2].axhline(y = r2, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    ax[1, 2].axhline(y = r2, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    \n    # Set y-limits for score panels\n    ax[0, 1].set_ylim((0, 1))\n    ax[0, 2].set_ylim((0, 1))\n    ax[1, 1].set_ylim((0, 1))\n    ax[1, 2].set_ylim((0, 1))\n\n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colors[i], label = learner))\n    pl.legend(handles = patches, bbox_to_anchor = (-.80, 2.53), \\\n               loc = 'upper center', borderaxespad = 0., ncol = 3, fontsize = 'x-large')\n    \n    # Aesthetics\n    pl.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, y = 1.10)\n    pl.tight_layout()\n    pl.show()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"2cb7e6ae-38fe-47ff-b4f0-edc18c0c9c9a","collapsed":true,"_uuid":"3bdcfe19b9c7084ff3b1cfa3b3d12358135339a8"},"cell_type":"code"},{"source":"import math\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom sklearn.metrics import r2_score \n\ndef performance_metric(y_true, y_predict):\n    \"\"\" Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. \"\"\"\n    \n    # TODO: Calculate the performance score between 'y_true' and 'y_predict'\n    score = r2_score(y_true, y_predict)\n    \n    # Return the score\n    return score","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"d71452c9-717e-47f9-ac80-d31db0df8b83","collapsed":true,"_uuid":"cc8bde893f17bdeaefbaa433972ade9c9dadc4e6"},"cell_type":"code"},{"source":"from sklearn import linear_model, tree, svm, ensemble\nfrom time import time\n\n# Initialize the three models\n#clf_A = tree.DecisionTreeRegressor(random_state = 5, max_depth=50, max_features='auto') # 0.41\n#clf_B = AdaBoostRegressor(random_state = 5) #0.\n#clf_C = BayesianRidge(n_iter = 300, tol = 0.005) #0.469\nclf_A = linear_model.LinearRegression()\nclf_B = linear_model.Ridge()\nclf_C = linear_model.Lasso()\nclf_D = linear_model.ElasticNet()\nclf_E = linear_model.BayesianRidge()\nclf_F = linear_model.RANSACRegressor()\nclf_G = svm.SVR()\nclf_H = ensemble.GradientBoostingRegressor()\nclf_I = tree.DecisionTreeRegressor()\nclf_K = ensemble.RandomForestRegressor()\n\n\n# Calculate the number of samples for 1%, 10%, and 100% of the training data\n# HINT: samples_100 is the entire training set i.e. len(y_train)\n# HINT: samples_10 is 10% of samples_100\n# HINT: samples_1 is 1% of samples_100\nsamples_100 = len(y_train)\nsamples_10 = samples_100/10\nsamples_1 = samples_100/100\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_E, clf_K]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([int(samples_1), int(samples_10), int(samples_100)]):\n        results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_test, y_test)\n\n# Run metrics visualization for the three supervised learning models chosen\nevaluate(results, 0.0, 0.0)\nprint(results)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"9578e727-a6c6-465f-ba09-05e7c830c2e9","collapsed":true,"_uuid":"001a235a7a359057269d775eb6031cbdf2f0f44d"},"cell_type":"code"},{"source":"# Import sklearn.preprocessing.StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\nnumerical = [ 'year', 'month', 'day', 'latitude', 'longitude']\n\nprint('length of features_to_train: ', len(features_to_train))\nfeatures_td_minmax_transform = pd.DataFrame(data = features_to_train)\nfeatures_td_minmax_transform[numerical] = scaler.fit_transform(features_to_train[numerical])\n\n#print(features_td_minmax_transform.visit_date.value_counts())\n# Show an example of a record with scaling applied\ndisplay(features_td_minmax_transform.head(n = 5))\n\n#print(features_td_minmax_transform.air_store_id.value_counts())\nprint(len(features_td_minmax_transform))","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"cb4690a6-55a1-4c0d-a899-c4bb684d6c8c","collapsed":true,"_uuid":"cbbe4a29787f59ea214c56d63ff847a62026b014"},"cell_type":"code"},{"source":"print(len(features_to_train.air_store_id.unique()))\nprint(len(features_to_train.day_of_week.unique()))\nprint(len(features_to_train.air_genre_name.unique()))\nprint(len(features_to_train.air_area_name.unique()))\nprint(6)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"1a4355b1-4596-4c21-9de2-0ccb0fa4215e","collapsed":true,"_uuid":"faaa83b4b64958962063a2fe2ee0849729501212"},"cell_type":"code"},{"source":"features_td_final = pd.get_dummies(features_td_minmax_transform)\n\n# Print the number of features after one-hot encoding\nencoded_td = list(features_td_final.columns)\nprint(len(encoded_td),' total features after one-hot encoding.')\n\n# Uncomment the following line to see the encoded feature names\nprint(encoded_td)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"4a70da65-44aa-4392-8c15-4b91b68cd67e","collapsed":true,"_uuid":"a5378274fbe1c232b59a3c8181f08c04921c1493"},"cell_type":"code"},{"source":"print(len(features_td_final))\nresult = clf_K.predict(features_td_final)\nprint(result)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"e2d87b87-1093-4e17-ab27-1005e359f205","collapsed":true,"_uuid":"a3bb76cf0415e9fb14ea3e2500f6abc94c3b9401"},"cell_type":"code"},{"source":"result_data_stub.head()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"56de02cc-8385-4dbb-88af-9d887951bc0f","collapsed":true,"_uuid":"e0fcc4fc07860efdc9a9b4d870f30febc0d7fb4e"},"cell_type":"code"},{"source":"result_data = result_data_stub[result_data_stub.air_store_id.isin(air_selected_store_ids)]\nresult_data['visitors'] = pd.Series(result, index=result_data.index)\nresult_data['id'] = result_data.air_store_id + result_data.visit_date.dt.strftime('_%Y-%m-%d')\n\nprint(len(result_data.id.unique()))\nprint(len(result_data))\nprint(len(result))\ndisplay(result_data)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"360e66bc-510c-4084-bae0-b8b7da60eb45","collapsed":true,"_uuid":"7ea7ba06ecccce1907c77db1520178c0cdbc93d5"},"cell_type":"code"},{"source":"result_to_save = result_data[['id', 'visitors']]\n\ndisplay(result_to_save.head())\n\nprint(result_to_save.dtypes)\nprint(len(result_to_save[result_to_save.visitors < 0]))\nresult_to_save.loc[~(result_to_save['visitors'] > 0), 'visitors'] = 0\nprint(len(result_to_save[result_to_save.visitors < 0]))\n\ndisplay(result_to_save.head())","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"41c2fc71-c34a-49ce-8d0a-c5251f447e9a","collapsed":true,"_uuid":"5b2eb737ac2ad5b29056d339c22d01da19b14ff5"},"cell_type":"code"},{"source":"print(len(result_to_save))","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"1a486480-4d30-45e9-8d76-025a8efa6fe2","collapsed":true,"_uuid":"831a921626f176bcf0eb4bc946741756e5efee59"},"cell_type":"code"},{"source":"result_to_save.to_csv('csv_to_submit.csv', index = False)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"f1fd7c40-feae-42e3-8a52-a9aa2e591af2","collapsed":true,"_uuid":"40f74546a166c590df14f03ddc34c47dea30dbe9"},"cell_type":"code"},{"source":"","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"881df1b2-2d97-482d-883f-8ac588270389","collapsed":true,"_uuid":"c37140e3268a7ccbd74604dd32e58bfd4048f7a8"},"cell_type":"code"}],"nbformat_minor":1,"metadata":{"language_info":{"file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","version":"3.6.3","nbconvert_exporter":"python","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4}