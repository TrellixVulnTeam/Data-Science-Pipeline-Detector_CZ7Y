{"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"9e5bbde9-c23a-476f-9bfe-38461f6fd504","_uuid":"737885ce5287fc7e6e403ce2a0c7a415228c4bc9"},"cell_type":"markdown","source":"Hi all,\n\nI have seen some kernels here that try to estimate the median of the future time series by means of taking the median of the historical data into account. Some kernels in previous competions like [webtraffic prediction](https://www.kaggle.com/c/web-traffic-time-series-forecasting) were also having acceptable results by this approach. But, there, the goal was to minimze an MAE based error function, while here, we should minimze an MSE based error function.\n\nSo, I have a theory. I think for MAE based error functions median estimation is the right approach but for MSE based functions mean estimation is a better approach.\n\nHere I show it with one numerical simulation and then prove it analytically in the next section.\n\nso, let's make a signal with some slow and fast oscillations with some trend and randomness:\n\n"},{"metadata":{"collapsed":true,"_cell_guid":"fa5bf279-26b6-4f54-a316-06897141634b","_uuid":"820ac240ce7b5ffae58f8863322943a02bcf18b4"},"cell_type":"code","source":"import pandas as pd\nimport pylab as pl\nimport seaborn as sns\nfrom scipy.stats import mode\n\ndef analyzer(n, Seed, Res, Plot):\n    pl.seed(Seed)\n    x = pl.arange(n)\n    y = pl.zeros(n)  # let's make up our signal\n    y += pl.sin(pl.pi * x / 100)  # slow oscillation\n    y += pl.sin(pl.pi * x / 5)  # fast oscillation\n    y += .01*x  # some trend\n    y += .5*pl.cumsum(pl.randn(n))  # some randomness by randomwalk\n    y += pl.exp(pl.randn(n))*pl.randn(n)  # some outliers\n\n    Avg = pl.mean(y)\n    Med = pl.median(y)\n    bins = pl.linspace(y.min(), y.max(), 20)\n    m = mode(pl.digitize(y, bins))\n    Mod = bins[m[0]]-(bins[1]-bins[0])/2\n\n    Res = Res.append(pd.DataFrame([[Seed,'Avg',((y-Avg)**2).mean(),abs(y-Avg).mean()]], columns=ResCols))\n    Res = Res.append(pd.DataFrame([[Seed,'Med',((y-Med)**2).mean(),abs(y-Med).mean()]], columns=ResCols))\n    Res = Res.append(pd.DataFrame([[Seed,'Mod',((y-Mod)**2).mean(),abs(y-Mod).mean()]], columns=ResCols))\n\n    if Plot:\n        pl.figure(figsize=(10,10))\n        pl.subplot(2,1,1)\n        pl.hist(y)\n        pl.xlabel('y values')\n        pl.ylabel('histogram')\n\n        pl.subplot(2,1,2)\n        pl.plot(x, y)\n        pl.plot(x[[0,-1]], [Avg, Avg])\n        pl.plot(x[[0,-1]], [Med, Med])\n        pl.plot(x[[0,-1]], [Mod, Mod])\n        pl.legend(('data','Mean','Median','Mode'))\n        pl.xlabel('samples')\n        pl.ylabel('y values')\n\n    return Res\n\n\nn = 500\nSeed = 1\nResCols = ('Seed','Estimate','MSE','MAE')\nRes = pd.DataFrame([],columns=ResCols)\nRes = analyzer(n, Seed, Res, Plot=True)\nprint(Res)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0067d3fe-e772-455d-bdcd-9e5d273eca74","_uuid":"349ea2598d2e6b8a53d65f465a2a07ca1f0f509a"},"cell_type":"markdown","source":"The table shows for MSE average is a better estimate and for MAE median is doing better, and mode is not working good in either case.\n\nLet's run some statistics:"},{"metadata":{"collapsed":true,"_cell_guid":"6082fa00-93ca-4875-8c05-2e099bcf2cfd","_uuid":"7160d007a9a44b0d9c07ff8a33d0296521fe4ee2"},"cell_type":"code","source":"Res = pd.DataFrame([],columns=ResCols)\nfor Seed in range(100):\n    Res = analyzer(n, Seed, Res, Plot=False)\n\nRes2=Res.pivot(columns='Estimate',index='Seed')\nprint(all(Res2[('MAE','Med')]<=Res2[('MAE','Avg')]))\nprint(all(Res2[('MSE','Med')]>=Res2[('MSE','Avg')]))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c9c679c6-ea73-4feb-b994-05a0e05fd6ae","_uuid":"3ecb13254ba999b8a74999d4e386063fbac4afb5"},"cell_type":"markdown","source":"so, it's clear that for MSE mean estimate works the best and for MAE median is the best estimate.\n\nAnd of course, the difference between the median and mean depends on the distribution of the values of our time series.\n\nHere are some figures:"},{"metadata":{"collapsed":true,"_cell_guid":"2ef03de8-8b56-43d5-b190-8e27327532bf","_uuid":"bf4cc9a175cf82291d8a3292a96d04c220adad03"},"cell_type":"code","source":"pl.figure(figsize=(10,10))\nax=pl.subplot(2,2,1)\nsns.violinplot(x='Estimate', y='MSE', data=Res, ax=ax)\nsns.pointplot(x='Estimate', y='MSE', data=Res, ax=ax, color='y', markers=\".\")\n\nax=pl.subplot(2,2,2)\nsns.violinplot(x='Estimate', y='MAE', data=Res, ax=ax)\nsns.pointplot(x='Estimate', y='MAE', data=Res, ax=ax, color='y', markers=\".\")\n\npl.subplot(2,2,3)\npl.plot(Res2[('MSE','Avg')],Res2[('MSE','Med')], '.')\npl.plot([0,Res.MSE.max()],[0,Res.MSE.max()],'r')\npl.xlabel('Median')\npl.ylabel('Average')\npl.title('MSE')\n\npl.subplot(2,2,4)\npl.plot(Res2[('MAE','Avg')],Res2[('MAE','Med')], '.')\npl.plot([0,Res.MAE.max()],[0,Res.MAE.max()],'r')\npl.xlabel('Median')\npl.ylabel('Average')\npl.title('MAE')\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"645fa143-66e4-4588-939c-eebdb8c494a3","_uuid":"5cc6c77f1b50c486605bc4ceeb378649bd439910"},"cell_type":"markdown","source":"The figures show slight differences in the errors for median and mean estimations, and as I said it depends on the distribution of the signal.\n\nand here is the proof that I promissed:\n\nWe are going to determine what is the best estimate of the signal, if we take some error function.Let's first do it for MSE.\n\n$MSE = \\frac{1}{n}\\sum_{i=1}^n {{\\big(y_i - m\\big)}^2} $\n\nby expanding this formula we get:\n\n$MSE = \\frac{\\sum_{i=1}^n y_i^2}{n} + \\frac{nm^2}{n} + -2m\\frac{\\sum_{i=1}^n y_i}{n} $\n\nand in fact it is a function of m that we are going to minimize with respect to m i.e. find the best m that has the lowest MSE:\n\n$MSE(m) = \\mathbb{E}[y^2] + m^2 + -2mAvg $\n\nthe extremum values of this function are where the derivative is zero:\n\n$\\frac{\\mathrm d}{\\mathrm d m}  MSE  = 2m-2Avg $ \n\nso clearly the extremum is where m is the average!\n\nLet's do the same thing for MAE:\n\n$MAE = \\frac{1}{n} \\sum_{i=1}^n {\\big|y_i - m\\big|} $\n\nIt's derivative is:\n\n$\\frac{\\mathrm d}{\\mathrm d m}  MAE  = \\frac{1}{n} \\Big( \\frac{y_1-m}{|y_1-m|} + \\frac{y_2-m}{|y_2-m|} + ...+ \\frac{y_n-m}{|y_n-m|} \\Big) $\n\nand these  $\\frac{y_i-m}{|y_i-m|}$  terms are +1s and -1s. Therefore, this summation is only minimum if we have the same number of +1s and -1s, which implies m should be the median of this series!"}],"nbformat_minor":1}