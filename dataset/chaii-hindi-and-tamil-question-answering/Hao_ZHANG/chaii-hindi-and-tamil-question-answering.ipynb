{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-15T22:55:45.098274Z","iopub.execute_input":"2021-11-15T22:55:45.098676Z","iopub.status.idle":"2021-11-15T22:55:45.158879Z","shell.execute_reply.started":"2021-11-15T22:55:45.098574Z","shell.execute_reply":"2021-11-15T22:55:45.15817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preliminary Preparation\n\n___\n\n## Dependent Libraries Import","metadata":{}},{"cell_type":"code","source":"# data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# text-processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n\nfrom transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:45.161879Z","iopub.execute_input":"2021-11-15T22:55:45.162301Z","iopub.status.idle":"2021-11-15T22:55:53.432531Z","shell.execute_reply.started":"2021-11-15T22:55:45.162273Z","shell.execute_reply":"2021-11-15T22:55:53.431779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accelerator Detection","metadata":{}},{"cell_type":"code","source":"import torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:53.433736Z","iopub.execute_input":"2021-11-15T22:55:53.434222Z","iopub.status.idle":"2021-11-15T22:55:53.492275Z","shell.execute_reply.started":"2021-11-15T22:55:53.434186Z","shell.execute_reply":"2021-11-15T22:55:53.491585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation\n\n---\n\n## Data Extraction","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/train.csv\")\nprint('train set shape: ', train.shape)\n\ntest = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\nprint('Test set shape: ', test.shape)\n\nsample_submission = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv\")\nprint('Sample submission set shape: ', sample_submission.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:53.497276Z","iopub.execute_input":"2021-11-15T22:55:53.49778Z","iopub.status.idle":"2021-11-15T22:55:54.455474Z","shell.execute_reply.started":"2021-11-15T22:55:53.497738Z","shell.execute_reply":"2021-11-15T22:55:54.454261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:54.456837Z","iopub.execute_input":"2021-11-15T22:55:54.457139Z","iopub.status.idle":"2021-11-15T22:55:54.477453Z","shell.execute_reply.started":"2021-11-15T22:55:54.457099Z","shell.execute_reply":"2021-11-15T22:55:54.476712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:54.478877Z","iopub.execute_input":"2021-11-15T22:55:54.479159Z","iopub.status.idle":"2021-11-15T22:55:54.502319Z","shell.execute_reply.started":"2021-11-15T22:55:54.479123Z","shell.execute_reply":"2021-11-15T22:55:54.501591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:54.503526Z","iopub.execute_input":"2021-11-15T22:55:54.504168Z","iopub.status.idle":"2021-11-15T22:55:54.515215Z","shell.execute_reply.started":"2021-11-15T22:55:54.504127Z","shell.execute_reply":"2021-11-15T22:55:54.514323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:54.516864Z","iopub.execute_input":"2021-11-15T22:55:54.517185Z","iopub.status.idle":"2021-11-15T22:55:54.530756Z","shell.execute_reply.started":"2021-11-15T22:55:54.517149Z","shell.execute_reply":"2021-11-15T22:55:54.530047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:54.531785Z","iopub.execute_input":"2021-11-15T22:55:54.532191Z","iopub.status.idle":"2021-11-15T22:55:54.543061Z","shell.execute_reply.started":"2021-11-15T22:55:54.532154Z","shell.execute_reply":"2021-11-15T22:55:54.542152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Regular Text Processing\n\n---\n\n## Text Clean ","metadata":{}},{"cell_type":"code","source":"'''\n    Make text lowercase, remove text in square brackets,\n    remove links, remove punctuation and remove words containing numbers.\n'''\ndef text_cleaner(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:54.546224Z","iopub.execute_input":"2021-11-15T22:55:54.546443Z","iopub.status.idle":"2021-11-15T22:55:54.552372Z","shell.execute_reply.started":"2021-11-15T22:55:54.546413Z","shell.execute_reply":"2021-11-15T22:55:54.551651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Tokenization","metadata":{}},{"cell_type":"code","source":"'''\n    Cleaning and parsing the text.\n'''\ndef word_tokenizer(text):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = text_cleaner(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    # remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:54.553784Z","iopub.execute_input":"2021-11-15T22:55:54.554322Z","iopub.status.idle":"2021-11-15T22:55:54.562313Z","shell.execute_reply.started":"2021-11-15T22:55:54.554278Z","shell.execute_reply":"2021-11-15T22:55:54.561471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Processor","metadata":{"execution":{"iopub.status.busy":"2021-11-10T22:59:32.550938Z","iopub.execute_input":"2021-11-10T22:59:32.552011Z","iopub.status.idle":"2021-11-10T22:59:32.555875Z","shell.execute_reply.started":"2021-11-10T22:59:32.551965Z","shell.execute_reply":"2021-11-10T22:59:32.555044Z"}}},{"cell_type":"code","source":"train['tokenized_text'] = train['context'].apply(str).apply(lambda x: word_tokenizer(x))\ntrain['text_len'] = train['tokenized_text'].astype(str).apply(len)\ntrain['text_word_count'] = train['tokenized_text'].apply(lambda x: len(str(x).split()))\n\ntrain[['context', 'tokenized_text', 'text_len', 'text_word_count']].head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:54.563799Z","iopub.execute_input":"2021-11-15T22:55:54.564124Z","iopub.status.idle":"2021-11-15T22:55:59.315456Z","shell.execute_reply.started":"2021-11-15T22:55:54.56409Z","shell.execute_reply":"2021-11-15T22:55:59.314783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['tokenized_text'] = test['context'].apply(str).apply(lambda x: word_tokenizer(x))\ntest['text_len'] = test['tokenized_text'].astype(str).apply(len)\ntest['text_word_count'] = test['tokenized_text'].apply(lambda x: len(str(x).split()))\n\ntest[['context', 'tokenized_text', 'text_len', 'text_word_count']]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:59.316863Z","iopub.execute_input":"2021-11-15T22:55:59.317133Z","iopub.status.idle":"2021-11-15T22:55:59.352741Z","shell.execute_reply.started":"2021-11-15T22:55:59.317098Z","shell.execute_reply":"2021-11-15T22:55:59.351988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization\n\n---\n\n## Pie Chart","metadata":{}},{"cell_type":"code","source":"train_groupby = train.groupby(by = ['language']).count()\n\ntrain_groupby","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:59.354127Z","iopub.execute_input":"2021-11-15T22:55:59.354396Z","iopub.status.idle":"2021-11-15T22:55:59.367621Z","shell.execute_reply.started":"2021-11-15T22:55:59.354351Z","shell.execute_reply":"2021-11-15T22:55:59.366983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.color_palette('pastel')","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:59.369006Z","iopub.execute_input":"2021-11-15T22:55:59.369443Z","iopub.status.idle":"2021-11-15T22:55:59.37684Z","shell.execute_reply.started":"2021-11-15T22:55:59.369408Z","shell.execute_reply":"2021-11-15T22:55:59.375957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create text props\ntextprops = dict(horizontalalignment = 'center',\n                 verticalalignment = 'top',\n                 rotation = 0,\n                 # rotation_mode = \"default\",\n                 rotation_mode = 'anchor',\n                 size = 14,\n                 # color = \"#81D8D0\",\n                 color = sns.color_palette('pastel')[-5],)\n\n# create pie chart\nplt.figure(figsize=(6, 6))\n\n# configure pie chart\nplt.pie(x = train_groupby.id,\n        labels = train_groupby.index,\n        colors = sns.color_palette('pastel')[2 : 3] + sns.color_palette('pastel')[-1 : ],\n        autopct = '%.2f%%',\n        # explode = (0.02, 0.02),\n        explode = [0.02] * 2,\n        startangle = 90,\n        pctdistance = 0.4,\n        labeldistance = 1.2,\n        textprops = textprops,)\n\n# configure pie chart legend\nlegend = plt.legend(title = 'Distinguish Samples by Language - Pie Chart',\n                    title_fontsize = 'x-large',\n                    #loc = 'lower center',\n                    bbox_to_anchor = (0, -0.15, 0.5, 0.5),\n                    labels = ['HINDI', 'TAMIL'],\n                    labelcolor = sns.color_palette('pastel')[-5],\n                    fontsize = 'large',\n                    facecolor = '#F6F8ED',\n                    edgecolor = sns.color_palette('pastel')[1],)\n\n# change pie chart legend color\nplt.setp(legend.get_title(), color = sns.color_palette('pastel')[3])\n\n# draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc = 'white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n# show pie chart\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:59.378606Z","iopub.execute_input":"2021-11-15T22:55:59.378888Z","iopub.status.idle":"2021-11-15T22:55:59.558028Z","shell.execute_reply.started":"2021-11-15T22:55:59.378856Z","shell.execute_reply":"2021-11-15T22:55:59.557377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Histogram Charts","metadata":{}},{"cell_type":"code","source":"sns.color_palette('Paired')","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:59.558995Z","iopub.execute_input":"2021-11-15T22:55:59.559308Z","iopub.status.idle":"2021-11-15T22:55:59.567187Z","shell.execute_reply.started":"2021-11-15T22:55:59.559271Z","shell.execute_reply":"2021-11-15T22:55:59.566508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 6))\n\nsns.set_style('whitegrid')\n\nax = sns.histplot(x = train.text_len.sort_values(ascending = False),\n                  bins = 100,\n                  hue = 'language',\n                  data = train,\n                  kde = True,\n                  element = 'step',\n                  palette = sns.color_palette('Paired')[9 : 10] + sns.color_palette('Paired')[1 : 2],\n                  legend = True,)\n\nax.set(xlabel = 'text length',\n       ylabel = 'count',\n       title = 'Original Text Length Distribution',)\n\nlegend = plt.legend(fontsize = 10,\n                    loc = 'upper right',\n                    title = 'Distinguish Samples by Language - Histogram Charts',\n                    title_fontsize = 12,\n                    shadow = True,\n                    facecolor = 'white',\n                    labels = ['HINDI', 'TAMIL'],\n                    labelcolor = sns.color_palette('Paired')[0],)\n\nplt.setp(legend.get_title(),\n         color = sns.color_palette('Paired')[-4],)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:59.568397Z","iopub.execute_input":"2021-11-15T22:55:59.56889Z","iopub.status.idle":"2021-11-15T22:55:59.888017Z","shell.execute_reply.started":"2021-11-15T22:55:59.568851Z","shell.execute_reply":"2021-11-15T22:55:59.887268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.figure(figsize = (12, 6))\n\nsns.set_style('darkgrid')\n\nax = sns.histplot(x = train.text_word_count.sort_values(ascending = False),\n                  bins = 100,\n                  hue = 'language',\n                  data = train,\n                  kde = True,\n                  element = 'step',\n                  palette = sns.color_palette('Paired')[5 : 6] + sns.color_palette('Paired')[7 : 8],)\n\nax.set(xlabel = 'tokenized text length',\n       ylabel = 'count',\n       title = 'Tokenize Text Length Distribution',)\n\nlegend = plt.legend(fontsize = 10,\n                    loc = 'upper right',\n                    title = 'Distinguish Samples by Language - Histogram Charts',\n                    title_fontsize = 12,\n                    shadow = True,\n                    facecolor = 'white',\n                    labels = ['HINDI', 'TAMIL'],\n                    labelcolor = sns.color_palette('Paired')[4],)\n\nplt.setp(legend.get_title(),\n         color = sns.color_palette('Paired')[6],)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:55:59.88962Z","iopub.execute_input":"2021-11-15T22:55:59.890124Z","iopub.status.idle":"2021-11-15T22:56:00.219902Z","shell.execute_reply.started":"2021-11-15T22:55:59.890087Z","shell.execute_reply":"2021-11-15T22:56:00.219214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (20, 5), sharey = True)\nfig.suptitle('Text Length Distribution')\n\n# Original Text\nsns.histplot(ax = axes[0],\n             x = train.text_len.sort_values(ascending = False),\n             bins = 100,\n             hue = 'language',\n             data = train,\n             kde = True,\n             element = 'step',\n             palette = sns.color_palette('Paired')[9 : 10] + sns.color_palette('Paired')[1 : 2],\n             legend = True,)\n\naxes[0].set(xlabel = 'text length',\n       ylabel = 'count',\n       title = 'Original Text Length Distribution',)\n\nlegend_1 = axes[0].legend(fontsize = 10,\n                          loc = 'upper right',\n                          title = 'Distinguish Samples by Language - Histogram Charts',\n                          title_fontsize = 12,\n                          shadow = True,\n                          facecolor = 'white',\n                          labels = ['HINDI', 'TAMIL'],\n                          labelcolor = sns.color_palette('Paired')[0],)\n\nplt.setp(legend_1.get_title(),\n         color = sns.color_palette('Paired')[-4],)\n\n\n\n# Tokenize Text\nsns.histplot(ax = axes[1],\n             x = train.text_word_count.sort_values(ascending = False),\n             bins = 100,\n             hue = 'language',\n             data = train,\n             kde = True,\n             element = 'step',\n             palette = sns.color_palette('Paired')[5 : 6] + sns.color_palette('Paired')[7 : 8],)\naxes[1].set(xlabel = 'tokenized text length',\n            ylabel = 'count',\n            title = 'Tokenize Text Length Distribution',)\n\nlegend_2 = axes[1].legend(fontsize = 10,\n                          loc = 'upper right',\n                          title = 'Distinguish Samples by Language - Histogram Charts',\n                          title_fontsize = 12,\n                          shadow = True,\n                          facecolor = 'white',\n                          labels = ['HINDI', 'TAMIL'],\n                          labelcolor = sns.color_palette('Paired')[4],)\n\nplt.setp(legend_2.get_title(),\n         color = sns.color_palette('Paired')[6],)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:56:00.221325Z","iopub.execute_input":"2021-11-15T22:56:00.221588Z","iopub.status.idle":"2021-11-15T22:56:00.757305Z","shell.execute_reply.started":"2021-11-15T22:56:00.221552Z","shell.execute_reply":"2021-11-15T22:56:00.756614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (15, 12))\ntitle = fig.suptitle(\n    'Text Length Distribution',\n    fontsize = 'xx-large',\n    weight = 'heavy',\n    color = sns.color_palette('Paired')[-1],)\n\n\ngs = fig.add_gridspec(2, 1)\n\n# Original Text\nwith sns.axes_style('whitegrid'):\n    ax_0_0 = fig.add_subplot(gs[0, 0])\n    sns.histplot(ax = ax_0_0,\n                 x = train.text_len.sort_values(ascending = False),\n                 bins = 100,\n                 hue = 'language',\n                 data = train,\n                 kde = True,\n                 element = 'step',\n                 palette = sns.color_palette('Paired')[9 : 10] + sns.color_palette('Paired')[1 : 2],\n                 legend = True,)\n    \nax_0_0.set(xlabel = 'text length',\n           ylabel = 'count',\n           title = 'Original Text Length Distribution',)\n\nax_0_0.set_xlabel(ax_0_0.get_xlabel(),\n                  fontweight = 'bold',)\n\nax_0_0.set_ylabel(ax_0_0.get_ylabel(),\n                  fontweight = 'bold',)\n\nax_0_0.set_title(ax_0_0.get_title(),\n                 fontweight = 'bold',\n                 fontsize = 'large')\n\nlegend_0_0 = ax_0_0.legend(fontsize = 10,\n                           loc = 'upper right',\n                           title = 'Distinguish Samples by Language - Histogram Charts',\n                           title_fontsize = 12,\n                           shadow = True,\n                           facecolor = 'white',\n                           labels = ['HINDI', 'TAMIL'],\n                           labelcolor = sns.color_palette('Paired')[0],\n                          )\n\nplt.setp(legend_0_0.get_title(),\n         color = sns.color_palette('Paired')[-4],)\n\n# Tokenize Text\nwith sns.axes_style('darkgrid'):\n    ax_1_0 = fig.add_subplot(gs[1, 0], sharex = ax_0_0)\n    sns.histplot(ax = ax_1_0,\n                 x = train.text_word_count.sort_values(ascending = False),\n                 bins = 100,\n                 hue = 'language',\n                 data = train,\n                 kde = True,\n                 element = 'step',\n                 palette = sns.color_palette('Paired')[5 : 6] + sns.color_palette('Paired')[7 : 8],)\n\nax_1_0.set(xlabel = 'tokenized text length',\n           ylabel = 'count',\n           title = 'Tokenize Text Length Distribution',)\n\nax_1_0.set_xlabel(ax_1_0.get_xlabel(),\n                  fontweight = 'bold',)\n\nax_1_0.set_ylabel(ax_1_0.get_ylabel(),\n                  fontweight = 'bold',)\n\nax_1_0.set_title(ax_1_0.get_title(),\n                 fontweight = 'bold',\n                 fontsize = 'large')\n\nlegend_1_0 = ax_1_0.legend(fontsize = 10,\n                           loc = 'upper right',\n                           title = 'Distinguish Samples by Language - Histogram Charts',\n                           title_fontsize = 12,\n                           shadow = True,\n                           facecolor = 'white',\n                           labels = ['HINDI', 'TAMIL'],\n                           labelcolor = sns.color_palette('Paired')[4],)\n\nplt.setp(legend_1_0.get_title(),\n         color = sns.color_palette('Paired')[6],)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:56:00.758273Z","iopub.execute_input":"2021-11-15T22:56:00.758481Z","iopub.status.idle":"2021-11-15T22:56:01.315768Z","shell.execute_reply.started":"2021-11-15T22:56:00.758456Z","shell.execute_reply":"2021-11-15T22:56:01.315099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Model Usage\n\n---\n\n## The Most Basic Usage","metadata":{}},{"cell_type":"markdown","source":"### Model `bert-base-multilingual-cased-finetuned-squad`","metadata":{}},{"cell_type":"code","source":"model = \"../input/bertbasemultilingualcasedfinetunedsquad/bert-base-multilingual-cased-finetuned-squad\"\nqna = pipeline('question-answering', model = model, tokenizer = model, device = 0)\n\npredictions = []\n\nfor question, context in test[[\"question\", \"context\"]].to_numpy():\n    result = qna(context=context, question=question)\n    predictions.append(result[\"answer\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:56:01.317191Z","iopub.execute_input":"2021-11-15T22:56:01.317425Z","iopub.status.idle":"2021-11-15T22:56:20.414714Z","shell.execute_reply.started":"2021-11-15T22:56:01.31739Z","shell.execute_reply":"2021-11-15T22:56:20.413728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['PredictionString'] = predictions\nsubmission.to_csv(\"submission_1.csv\", index=None)\n\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:56:20.416342Z","iopub.execute_input":"2021-11-15T22:56:20.416593Z","iopub.status.idle":"2021-11-15T22:56:20.433478Z","shell.execute_reply.started":"2021-11-15T22:56:20.416554Z","shell.execute_reply":"2021-11-15T22:56:20.432528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model `xlm-roberta-squad2`","metadata":{}},{"cell_type":"code","source":"model = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2\"\nqna = pipeline('question-answering', model = model, tokenizer = model, device = 0)\n\npredictions = []\n\nfor question, context in test[[\"question\", \"context\"]].to_numpy():\n    result = qna(context=context, question=question)\n    predictions.append(result[\"answer\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:56:20.435153Z","iopub.execute_input":"2021-11-15T22:56:20.435418Z","iopub.status.idle":"2021-11-15T22:56:42.068573Z","shell.execute_reply.started":"2021-11-15T22:56:20.435383Z","shell.execute_reply":"2021-11-15T22:56:42.067751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['PredictionString'] = predictions\nsubmission.to_csv(\"submission_2.csv\", index=None)\n\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:56:42.070276Z","iopub.execute_input":"2021-11-15T22:56:42.070552Z","iopub.status.idle":"2021-11-15T22:56:42.086555Z","shell.execute_reply.started":"2021-11-15T22:56:42.070516Z","shell.execute_reply":"2021-11-15T22:56:42.085695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\nqna = pipeline('question-answering', model = model, tokenizer = model, device = 0)\n\npredictions = []\n\nfor question, context in test[[\"question\", \"context\"]].to_numpy():\n    result = qna(context=context, question=question)\n    predictions.append(result[\"answer\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:56:42.088149Z","iopub.execute_input":"2021-11-15T22:56:42.088412Z","iopub.status.idle":"2021-11-15T22:57:18.260922Z","shell.execute_reply.started":"2021-11-15T22:56:42.088378Z","shell.execute_reply":"2021-11-15T22:57:18.260087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['PredictionString'] = predictions\nsubmission.to_csv(\"submission.csv\", index=None)\n\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T23:02:04.803684Z","iopub.execute_input":"2021-11-15T23:02:04.803983Z","iopub.status.idle":"2021-11-15T23:02:04.820145Z","shell.execute_reply.started":"2021-11-15T23:02:04.803945Z","shell.execute_reply":"2021-11-15T23:02:04.819003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}