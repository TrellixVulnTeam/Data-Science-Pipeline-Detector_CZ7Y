{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:23:07.352674Z","iopub.execute_input":"2022-01-27T09:23:07.353005Z","iopub.status.idle":"2022-01-27T09:23:08.071692Z","shell.execute_reply.started":"2022-01-27T09:23:07.352925Z","shell.execute_reply":"2022-01-27T09:23:08.070882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install -q pytorch-lightning==1.4.9\n! pip install -q \"lightning-flash[text]==0.5.2\"\n! pip install -qU pandas","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:23:11.940093Z","iopub.execute_input":"2022-01-27T09:23:11.940625Z","iopub.status.idle":"2022-01-27T09:24:21.498888Z","shell.execute_reply.started":"2022-01-27T09:23:11.940585Z","shell.execute_reply":"2022-01-27T09:24:21.498051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How do you pick the right set of Hyperparameters for a Machine Learning project ?\n\n## An introduction to Hyperparameter Search using Lightning-Flash with Optuna\n\nMachine Learning involves modelling which is controlled by many parameters. The parameters that control the learning process of the model as known as **Hyperparameters**.\n\nTo obtain a well performing model, we need to select the right set of parameters and this is not a very easy task and it scales as the depth of your modelling idea increases.\n\nThis issue of obtaining the right set of parameters is solved the **Hyperparameter search** or **Hyperparameter optimization**.\n\nIn this post we will be looking at using **Optuna**, a very widely known automatic hyperparameter optimization framework for machine learning, along with **Lightning Flash**, your go to PyTorch AI factory for any machine learning task, to learn how to perform hyperpamater search.","metadata":{}},{"cell_type":"code","source":"import gc\nimport os\n\nimport pandas as pd\nimport optuna\nimport torch\nfrom flash import Trainer\nfrom flash.text import QuestionAnsweringData, QuestionAnsweringTask\nfrom optuna.integration.pytorch_lightning import PyTorchLightningPruningCallback","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:24:26.351487Z","iopub.execute_input":"2022-01-27T09:24:26.351981Z","iopub.status.idle":"2022-01-27T09:24:35.978826Z","shell.execute_reply.started":"2022-01-27T09:24:26.351943Z","shell.execute_reply":"2022-01-27T09:24:35.977877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Loading the Data and generating splits (Will be used later in the tutorial)\nDATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"_datasets\")\n# ../input/chaii-hindi-and-tamil-question-answering/train.csv\nCHAII_DATASET_PATH = \"../input/chaii-hindi-and-tamil-question-answering/\"\nINPUT_DATA_PATH = os.path.join(CHAII_DATASET_PATH, \"train.csv\")\nTRAIN_DATA_PATH = os.path.join(\"./\", \"_train.csv\")\nVAL_DATA_PATH = os.path.join(\"./\", \"_val.csv\")\nPREDICT_DATA_PATH = os.path.join(CHAII_DATASET_PATH, \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:24:39.041825Z","iopub.execute_input":"2022-01-27T09:24:39.042095Z","iopub.status.idle":"2022-01-27T09:24:39.051703Z","shell.execute_reply.started":"2022-01-27T09:24:39.042063Z","shell.execute_reply":"2022-01-27T09:24:39.051038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(INPUT_DATA_PATH)\nfraction = 0.9\n\ntamil_examples = df[df[\"language\"] == \"tamil\"]\ntrain_split_tamil = tamil_examples.sample(frac=fraction, random_state=200)\nval_split_tamil = tamil_examples.drop(train_split_tamil.index)\n\nhindi_examples = df[df[\"language\"] == \"hindi\"]\ntrain_split_hindi = hindi_examples.sample(frac=fraction, random_state=200)\nval_split_hindi = hindi_examples.drop(train_split_hindi.index)\n\ntrain_split = pd.concat([train_split_tamil, train_split_hindi]).reset_index(drop=True)\nval_split = pd.concat([val_split_tamil, val_split_hindi]).reset_index(drop=True)\n\ntrain_split.to_csv(TRAIN_DATA_PATH, index=False)\nval_split.to_csv(VAL_DATA_PATH, index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:24:40.639102Z","iopub.execute_input":"2022-01-27T09:24:40.639647Z","iopub.status.idle":"2022-01-27T09:24:41.995888Z","shell.execute_reply.started":"2022-01-27T09:24:40.639606Z","shell.execute_reply":"2022-01-27T09:24:41.995151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introduction to Optuna\n\n_| NOTE: This introductory example is inspired from `Optuna` documentation._\n\nTo understand how we can use Optuna, let us start with a basic example of trying to find a value that minimizes the value of a quadratic function.\n\nLet us consider the function `f(x) = x²-4x+4` as our complex ML model. We know that its root (optimal parameter) is 2. We can also find this using Optuna.\n\nTo understand how we can use Optuna for this, we should first convert this to an optimization objective problem. This will be: `minimize f(x) over a certain range of real values [-∞, ∞]`.\n\nGiven we have defined these, the next step is to setup an Optuna Study. An Optuna Study is just optimization on a single objective function. This can be achieved with the following line of code: \n\n`>>> study = optuna.create_study()`.\n\nAn Optuna Study conducts many trials in search of the optimal parameters amongst the provided options which could be a set of discrete values or a continuous range. Each Optuna Trial is a single execution of the objective function with a unique selection of all the hyperparameters.\n\n`>>> study.optimize(objective, n_trials=100)`","metadata":{}},{"cell_type":"markdown","source":"We can notice that the optimize method takes an objective function. In our case we have already defined our objective function, the code version would look like this:","metadata":{}},{"cell_type":"code","source":"def objective(trial: optuna.Trial):\n    x = trial.suggest_float(\"x\", -10, 10)\n    return x ** 2 - 4 * x + 4","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:24:45.873131Z","iopub.execute_input":"2022-01-27T09:24:45.873406Z","iopub.status.idle":"2022-01-27T09:24:45.880184Z","shell.execute_reply.started":"2022-01-27T09:24:45.873375Z","shell.execute_reply":"2022-01-27T09:24:45.879407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The complete code will look like this:","metadata":{}},{"cell_type":"code","source":"def objective(trial: optuna.Trial):\n    x = trial.suggest_float(\"x\", -10, 10)\n    return x ** 2 - 4 * x + 4\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=20)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:24:48.392089Z","iopub.execute_input":"2022-01-27T09:24:48.392378Z","iopub.status.idle":"2022-01-27T09:24:48.478526Z","shell.execute_reply.started":"2022-01-27T09:24:48.392325Z","shell.execute_reply":"2022-01-27T09:24:48.477702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have conducted the study we want the optimal parameters and this is as easy as running `study.best_params` which returns a Python dictionary that maps each hyperparameter variable to the optimal value obtained from the search.","metadata":{}},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:24:50.856677Z","iopub.execute_input":"2022-01-27T09:24:50.856949Z","iopub.status.idle":"2022-01-27T09:24:50.865285Z","shell.execute_reply.started":"2022-01-27T09:24:50.856919Z","shell.execute_reply":"2022-01-27T09:24:50.864538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna with Lightning Flash\n\nFlash is a high-level deep learning framework for fast prototyping, baselining, finetuning and solving deep learning problems. It features a set of tasks for you to use for inference and finetuning out of the box, and an easy to implement API to customize every step of the process for full flexibility.\n\nFlash is built for beginners with a simple API that requires very little deep learning background, and for data scientists, Kagglers, applied ML practitioners and deep learning researchers that want a quick way to get a deep learning baseline with advanced features [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) offers.\n\n**[DESCRIBE OPTUNA]**\n\n_| NOTE: Further sections of the notebook follows from the [previous blog post of the series](https://devblog.pytorchlightning.ai/question-answering-for-dravidian-languages-hindi-and-tamil-f8ffb9fc0c1)._","metadata":{}},{"cell_type":"markdown","source":"### Listing out the Hyperparameters for our task\n\nAs mentioned above, each machine learning project requires a careful selection of hyperparameters. Firstly we need to identify and define the set of hyperparameters required for the project. \n\nThe most common and necessary one would be **_Learning Rate_** which defines how fast or slow the model learns from the provided data. \n\nAnother important hyperparameter is the **_Batch Size_** which defines how many samples of the dataset should the model learn from everytime.\n\nThe choice of hyperparameters changes for every project but the aforementioned ones are common across all projects.\n\nFor our current setup, we will be searching for an optimal _Learning Rate_, _Batch Size_, _Transformer Backbone_, _Optimizer_.","metadata":{}},{"cell_type":"markdown","source":"### Defining our search space\n\nOnce we have listed out the hyperparameters for our project, we have to define the possible values. \n\nIt is impossible to search over an infinite search space to find an optimal set of values and it is time consuming as well.\n\nOptuna let's us provide choices categorical choice, or a range of float values etc.\n\nDefining our search space beforehand:\n- Learning Rate: [1e-5, 5e-4]\n- Batch Size: 2 or 4 or 8\n- Transformer Backbone: \"xlm-roberta-base\" or \"bert-base-multilingual-uncased\"\n- Optimizer: \"adam\" or \"adamw\"","metadata":{}},{"cell_type":"markdown","source":"### Creating an objective function\n\nGiven that we have decided on the hyperparameters and the search space for the project, we have to formulate the optimization objective function. We are in luck here because every machine learning problem is an optimization problem in disguise where we want to minimize / maximize something within some well defined constraints.\n\nHere we would like to minimize the loss of the Question Answering model.\n\nLet us first understand the process of Hyperparameter Search using Optuna.\n\nOptuna provides the `Study` class which is responsible for conducting the hyperparameter search by studying the model's performance across all the combinations of the hyperparameters it generates from the search space.\n\nEach such combination of the hyperparameters is generated by a `Trial` class which is instantiated when we run the `Study.optimize()` method.\n\nThe `objective` function that we pass onto the `Study.optimize()` exposes the current `Trial` object in action whose interface we use to define our search space.","metadata":{}},{"cell_type":"code","source":"# The objective function would look something like this\ndef objective(trial: optuna.Trial):\n\n    # A unique set of hyperparameter combination is sampled.\n    learning_rate = trial.suggest_uniform(\"learning_rate\", 1e-5, 5e-4)\n    batch_size = trial.suggest_categorical(\"batch_size\", [2, 4])\n    backbone = trial.suggest_categorical(\n        \"backbone\", \n        [\"xlm-roberta-base\", \"bert-base-multilingual-uncased\"]\n    )\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\"])\n\n    # Setup the machine learning pipeline with the new set\n    # of hyperparameters.\n    datamodule = QuestionAnsweringData.from_csv(\n        train_file=TRAIN_DATA_PATH,\n        val_file=VAL_DATA_PATH,\n        backbone=backbone,\n        batch_size=batch_size,\n    )\n\n    model = QuestionAnsweringTask(\n        backbone=backbone,\n        learning_rate=learning_rate,\n        optimizer=optimizer,\n    )\n\n    trainer = Trainer(\n        max_epochs=2,\n        limit_train_batches=10, # For the tutorial's sake, to provide\n        limit_val_batches=10,   # a quicker demonstration.\n        gpus= torch.cuda.device_count(),\n    )\n\n    # Train the model for Optuna to understand the current\n    # Hyperparameter combination's behaviour.\n    trainer.fit(model, datamodule=datamodule)\n\n    # The extra step to tell Optuna which value to base the\n    # optimization routine on.\n    value = trainer.callback_metrics[\"val_loss\"].item()\n    \n    del datamodule, model, trainer\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return value","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:24:56.424398Z","iopub.execute_input":"2022-01-27T09:24:56.424652Z","iopub.status.idle":"2022-01-27T09:24:56.441228Z","shell.execute_reply.started":"2022-01-27T09:24:56.424623Z","shell.execute_reply":"2022-01-27T09:24:56.440249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once we have the objective function down, running the search is very simple as mentioned before.","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:24:59.314757Z","iopub.execute_input":"2022-01-27T09:24:59.315203Z","iopub.status.idle":"2022-01-27T09:24:59.32138Z","shell.execute_reply.started":"2022-01-27T09:24:59.315163Z","shell.execute_reply":"2022-01-27T09:24:59.320654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.optimize(objective, n_trials=5)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:25:01.823446Z","iopub.execute_input":"2022-01-27T09:25:01.824011Z","iopub.status.idle":"2022-01-27T09:34:36.868303Z","shell.execute_reply.started":"2022-01-27T09:25:01.823972Z","shell.execute_reply":"2022-01-27T09:34:36.86753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And so is obtaining the best combination of hyperparameters:","metadata":{}},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:34:50.724392Z","iopub.execute_input":"2022-01-27T09:34:50.725782Z","iopub.status.idle":"2022-01-27T09:34:50.737277Z","shell.execute_reply.started":"2022-01-27T09:34:50.72572Z","shell.execute_reply":"2022-01-27T09:34:50.736539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add a note on other available inbuilt optuna tools like Sampler, Pruning Callback, etc ?","metadata":{}}]}