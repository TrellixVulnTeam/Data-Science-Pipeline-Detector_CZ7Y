{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hindi and Tamil Question Answering with Lightning Flash","metadata":{"_uuid":"d751a6a6-1397-45f0-b910-ef7ed20cb0e4","_cell_guid":"2dd28f13-79d4-4fa2-8f09-eefc5591a9f6","execution":{"iopub.status.busy":"2021-08-26T14:08:47.666687Z","iopub.execute_input":"2021-08-26T14:08:47.667354Z","iopub.status.idle":"2021-08-26T14:09:25.640973Z","shell.execute_reply.started":"2021-08-26T14:08:47.667235Z","shell.execute_reply":"2021-08-26T14:09:25.639725Z"},"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"# ! pip install -q pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\n! pip install -q torch==1.10.0+cu111 torchvision==0.11.1+cu111 torchaudio==0.10.0+cu111 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n! pip install -qU torchtext \n! pip install -q pytorch-lightning==1.4.9\n! pip install -q \"git+https://github.com/PyTorchLightning/lightning-flash.git#egg=lightning-flash[text]\"\n! pip install -qU torchmetrics>=0.5.0 rich wandb","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:05:52.890333Z","iopub.execute_input":"2021-11-07T16:05:52.890763Z","iopub.status.idle":"2021-11-07T16:10:13.671363Z","shell.execute_reply.started":"2021-11-07T16:05:52.890679Z","shell.execute_reply":"2021-11-07T16:10:13.670472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! mkdir /kaggle/temp\n! pip list | grep torch\n! pip list | grep tokenizers\n! pip list | grep transformers\n! pip list | grep datasets\n\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:10:13.673404Z","iopub.execute_input":"2021-11-07T16:10:13.673858Z","iopub.status.idle":"2021-11-07T16:10:20.780179Z","shell.execute_reply.started":"2021-11-07T16:10:13.673814Z","shell.execute_reply":"2021-11-07T16:10:20.779115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! nvidia-smi\nENABLE_ORT = False\nif ENABLE_ORT:\n    import torch\n    if torch.cuda.is_available():\n        ! pip install -q torch-ort -f https://download.onnxruntime.ai/onnxruntime_stable_cu111.html\n        ! python -m torch_ort.configure","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:10:20.783201Z","iopub.execute_input":"2021-11-07T16:10:20.783587Z","iopub.status.idle":"2021-11-07T16:10:21.497631Z","shell.execute_reply.started":"2021-11-07T16:10:20.783546Z","shell.execute_reply":"2021-11-07T16:10:21.496742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.enable()\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport pytorch_lightning as pl\n\nimport wandb\n\nfrom dataclasses import asdict, dataclass\nfrom typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Type, Union\nfrom torch.optim.lr_scheduler import (\n    _LRScheduler, StepLR, MultiStepLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts\n)\n\nfrom transformers import (\n    AdamW,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers.trainer_pt_utils import get_parameter_names\n\nimport flash\nfrom flash import Trainer\nfrom flash.core.optimizers import LinearWarmupCosineAnnealingLR\nfrom flash.core.finetuning import NoFreeze\nfrom flash.text import QuestionAnsweringData, QuestionAnsweringTask\nfrom flash.text.question_answering.finetuning import QuestionAnsweringFreezeEmbeddings","metadata":{"_uuid":"feeafed0-6e6e-44e8-afc2-7a0fc53d5e9e","_cell_guid":"2982d4b1-7b16-4030-93ca-d5cc7b37a530","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T16:10:21.49955Z","iopub.execute_input":"2021-11-07T16:10:21.499897Z","iopub.status.idle":"2021-11-07T16:10:29.748616Z","shell.execute_reply.started":"2021-11-07T16:10:21.499859Z","shell.execute_reply":"2021-11-07T16:10:29.747709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup and login into wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nWANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n! wandb login $WANDB_API_KEY","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:10:29.74996Z","iopub.execute_input":"2021-11-07T16:10:29.750314Z","iopub.status.idle":"2021-11-07T16:10:32.18633Z","shell.execute_reply.started":"2021-11-07T16:10:29.750279Z","shell.execute_reply":"2021-11-07T16:10:32.185342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR = \"/kaggle/input/chaii-hindi-and-tamil-question-answering\"\nTEMP_PATH = \"./\"\nINPUT_DATA_PATH = os.path.join(INPUT_DIR, \"train.csv\")\nTRAIN_DATA_PATH = os.path.join(TEMP_PATH, \"_train.csv\")\nVAL_DATA_PATH = os.path.join(TEMP_PATH, \"_val.csv\")\nPREDICT_DATA_PATH = os.path.join(INPUT_DIR, \"test.csv\")\n\n@dataclass\nclass HyperParams:\n    seed: int = 42\n    \n    # dataset specific\n    train_val_split: float = 0.1\n    batch_size: int = 4\n    \n    # model specific\n    backbone: str = \"xlm-roberta-base\"\n    pretrained: bool = True\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    ## Optimizer and Scheduler specific\n    # lr=2e-7,\n    # lr=5.531681197617226e-05, ## Adam\n    # lr=0.0001096478196143185, ## AdamW\n    optimizer = 'AdamW'\n    learning_rate = 3e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n    lr_scheduler: str = \"cosine_schedule_with_warmup\"\n    num_warmup_steps: float = 0.1\n    lr_scheduler_config = {\n        \"interval\": \"step\",\n        \"frequency\": 1,\n    }\n    \n    # Training/Finetuning args\n    debug: bool = False\n    num_gpus: int = torch.cuda.device_count()\n    accumulate_grad_batches: int = 2\n    enable_ort: bool = ENABLE_ORT\n    max_epochs: int = 10\n    finetuning_strategy: str= \"no_freeze\"\n    stochastic_weight_avg: bool = False\n\nHYPER_PARAMS = HyperParams()\npl.seed_everything(HYPER_PARAMS.seed)","metadata":{"_uuid":"af07e575-2071-481a-ae1d-db87f76fb8b9","_cell_guid":"797b9f79-2309-4a41-b6fe-91e5734c19ff","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T16:10:32.187997Z","iopub.execute_input":"2021-11-07T16:10:32.188336Z","iopub.status.idle":"2021-11-07T16:10:32.34899Z","shell.execute_reply.started":"2021-11-07T16:10:32.188299Z","shell.execute_reply":"2021-11-07T16:10:32.348096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display a small portion of the dataset\ndf = pd.read_csv(INPUT_DATA_PATH)\ndisplay(df.head())","metadata":{"_uuid":"d2fc1e77-0202-423b-ad23-d1eb32fdf636","_cell_guid":"4420df5a-77a0-4208-9894-555c473727b2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T16:10:32.350324Z","iopub.execute_input":"2021-11-07T16:10:32.350721Z","iopub.status.idle":"2021-11-07T16:10:33.210189Z","shell.execute_reply.started":"2021-11-07T16:10:32.350684Z","shell.execute_reply":"2021-11-07T16:10:33.209302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Create the DataModule","metadata":{}},{"cell_type":"code","source":"fraction = 1 - HYPER_PARAMS.train_val_split\n\n# Splitting data into train and val beforehand since preprocessing will be different for datasets.\ntamil_examples = df[df[\"language\"] == \"tamil\"]\ntrain_split_tamil = tamil_examples.sample(frac=fraction,random_state=200)\nval_split_tamil = tamil_examples.drop(train_split_tamil.index)\n\nhindi_examples = df[df[\"language\"] == \"hindi\"]\ntrain_split_hindi = hindi_examples.sample(frac=fraction,random_state=200)\nval_split_hindi = hindi_examples.drop(train_split_hindi.index)\n\ntrain_split = pd.concat([train_split_tamil, train_split_hindi]).reset_index(drop=True)\nval_split = pd.concat([val_split_tamil, val_split_hindi]).reset_index(drop=True)\n\ntrain_split.to_csv(TRAIN_DATA_PATH, index=False)\nval_split.to_csv(VAL_DATA_PATH, index=False)","metadata":{"_uuid":"4c89e8fa-6d84-41ff-b136-952528f464f9","_cell_guid":"343b989a-0ee2-4eb2-8402-f0a79f2d2d90","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T16:10:33.212975Z","iopub.execute_input":"2021-11-07T16:10:33.213363Z","iopub.status.idle":"2021-11-07T16:10:33.854575Z","shell.execute_reply.started":"2021-11-07T16:10:33.213324Z","shell.execute_reply":"2021-11-07T16:10:33.853681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Create the DataModule\ndatamodule = QuestionAnsweringData.from_csv(\n    train_file=TRAIN_DATA_PATH,\n    val_file=VAL_DATA_PATH,\n    batch_size=HYPER_PARAMS.batch_size,\n    backbone=HYPER_PARAMS.backbone\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:10:33.856101Z","iopub.execute_input":"2021-11-07T16:10:33.856438Z","iopub.status.idle":"2021-11-07T16:11:06.320445Z","shell.execute_reply.started":"2021-11-07T16:10:33.856403Z","shell.execute_reply":"2021-11-07T16:11:06.319594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Build the task","metadata":{}},{"cell_type":"code","source":"class ChaiiQuestionAnswering(QuestionAnsweringTask):\n    def __init__(\n       self,\n        backbone: str = \"distilbert-base-uncased\",\n        optimizer: Type[torch.optim.Optimizer] = torch.optim.Adam,\n        lr_scheduler: Optional[Union[Type[_LRScheduler], str, _LRScheduler]] = None,\n        metrics: Union[Callable, Mapping, Sequence, None] = None,\n        learning_rate: float = 5e-5,\n        enable_ort: bool = False,\n    ):\n        super().__init__(\n            backbone=backbone,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            metrics=metrics,\n            learning_rate=learning_rate,\n            enable_ort=enable_ort,\n        )\n\n    @staticmethod\n    def jaccard(str1, str2): \n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n        \n    def compute_metrics(self, generated_tokens, batch):\n        scores = []\n        for example in batch:\n            predicted_answer = generated_tokens[example[\"example_id\"]]\n            target_answer = example[\"answer\"][\"text\"][0] if len(example[\"answer\"][\"text\"]) > 0 else \"\"\n            scores.append(ChaiiQuestionAnswering.jaccard(predicted_answer, target_answer))\n\n        result = {\"jaccard_score\": torch.mean(torch.tensor(scores))}\n        return result\n    \n#     def configure_optimizers(self):\n#         decay_parameters = get_parameter_names(self.model, [torch.nn.LayerNorm])\n#         decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n#         optimizer_grouped_parameters = [\n#             {\n#                 \"params\": [p for n, p in self.model.named_parameters() if n in decay_parameters],\n#                 \"weight_decay\": HYPER_PARAMS.weight_decay,\n#             },\n#             {\n#                 \"params\": [p for n, p in self.model.named_parameters() if n not in decay_parameters],\n#                 \"weight_decay\": 0.0,\n#             },\n#         ]\n#         optimizer = AdamW(optimizer_grouped_parameters,lr=self.learning_rate,correct_bias=True)\n\n#         if self.lr_scheduler is not None:\n#             return [optimizer], [self._instantiate_lr_scheduler(optimizer)]\n#         return optimizer\n    \n#     def configure_optimizers(self):\n#         opt_parameters = (\n#             []\n#         )  # To be passed to the optimizer (only parameters of the layers you want to update).\n#         named_parameters = list(self.model.named_parameters())\n\n#         # According to AAAMLP book by A. Thakur, we generally do not use any decay\n#         # for bias and LayerNorm.weight layers.\n#         no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n#         set_2 = [\"layer.4\", \"layer.5\", \"layer.6\", \"layer.7\"]\n#         set_3 = [\"layer.8\", \"layer.9\", \"layer.10\", \"layer.11\"]\n#         init_lr = 1e-6\n\n#         for i, (name, params) in enumerate(named_parameters):\n\n#             weight_decay = 0.0 if any(p in name for p in no_decay) else 0.01\n\n#             if name.startswith(\"roberta.embeddings\") or name.startswith(\"roberta.encoder\"):\n#                 # For first set, set lr to 1e-6 (i.e. 0.000001)\n#                 lr = init_lr\n\n#                 # For set_2, increase lr to 0.00000175\n#                 lr = init_lr * 1.75 if any(p in name for p in set_2) else lr\n\n#                 # For set_3, increase lr to 0.0000035\n#                 lr = init_lr * 3.5 if any(p in name for p in set_3) else lr\n\n#                 opt_parameters.append(\n#                     {\"params\": params, \"weight_decay\": weight_decay, \"lr\": lr}\n#                 )\n\n#             # For regressor and pooler, set lr to 0.0000036 (slightly higher than the top layer).\n#             if name.startswith(\"qa_outputs\"):\n#                 lr = init_lr * 3.6\n\n#                 opt_parameters.append(\n#                     {\"params\": params, \"weight_decay\": weight_decay, \"lr\": lr}\n#                 )\n\n#         optimizer = AdamW(opt_parameters, lr=init_lr, correct_bias=True)\n#         if self.lr_scheduler is not None:\n#             return [optimizer], [self._instantiate_lr_scheduler(optimizer)]\n#         return optimizer\n\n    def configure_optimizers(self):\n        # To be passed to the optimizer (only parameters of the layers you want to update).\n        opt_parameters = []\n        weight_decay = HYPER_PARAMS.weight_decay    \n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        named_parameters = list(self.model.named_parameters())\n    \n        def get_lrs(start_lr, last_lr):\n            step = np.abs(np.log(start_lr) - np.log(last_lr)) / 3\n            return [start_lr,  np.exp(np.log(start_lr) + step), np.exp(np.log(start_lr)+2*step), last_lr]\n        \n        def is_decay_param(name: str):\n            return not any(p in name for p in no_decay)\n        \n        ranges = [(0,69), (69, 133), (133, 197), (197, 199)]\n        lrs = get_lrs(1e-8, self.learning_rate)\n        \n        for _range, lr in zip(ranges, lrs):\n            params = named_parameters[_range[0]:_range[1]]\n            decay_parameters = [p for n, p in params if is_decay_param(n)]\n            no_decay_parameters = [p for n, p in params if not is_decay_param(n)]\n\n            opt_parameters.append(\n                {\"params\": decay_parameters, \"weight_decay\": weight_decay, \"lr\": lr}\n            )\n\n            # According to AAAMLP book by A. Thakur, we generally do not use any decay\n            # for bias and LayerNorm.weight layers.\n            opt_parameters.append(\n                {\"params\": no_decay_parameters, \"weight_decay\": 0.0, \"lr\": lr}\n            )\n        \n\n        optimizer = AdamW(opt_parameters, lr=self.learning_rate, correct_bias=True)\n        if self.lr_scheduler is not None:\n            return [optimizer], [self._instantiate_lr_scheduler(optimizer)]\n        return optimizer\n    \n    def configure_optimizers(self):\n            # To be passed to the optimizer (only parameters of the layers you want to update).\n            opt_parameters = []\n            weight_decay = HYPER_PARAMS.weight_decay    \n            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n            for n, p in self.model.named_parameters():\n                if \"qa_outputs\" in n:\n                    if \"bias\" in n:\n                        opt_parameters.append(\n                            {\"params\": p, \"weight_decay\": 0.0, \"lr\": self.learning_rate}\n                        )\n                    else:\n                        opt_parameters.append(\n                            {\"params\": p, \"weight_decay\": weight_decay, \"lr\": self.learning_rate}\n                        )\n            optimizer = AdamW(opt_parameters, lr=self.learning_rate, correct_bias=True)\n            if self.lr_scheduler is not None:\n                return [optimizer], [self._instantiate_lr_scheduler(optimizer)]\n            return optimizer","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:11:06.321835Z","iopub.execute_input":"2021-11-07T16:11:06.322149Z","iopub.status.idle":"2021-11-07T16:11:06.419162Z","shell.execute_reply.started":"2021-11-07T16:11:06.322113Z","shell.execute_reply":"2021-11-07T16:11:06.418329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ChaiiQuestionAnswering(\n    backbone=HYPER_PARAMS.backbone,\n    learning_rate=HYPER_PARAMS.learning_rate,\n    lr_scheduler=(HYPER_PARAMS.lr_scheduler, {\"num_warmup_steps\": HYPER_PARAMS.num_warmup_steps}, HYPER_PARAMS.lr_scheduler_config),\n    enable_ort=HYPER_PARAMS.enable_ort,\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:11:06.421884Z","iopub.execute_input":"2021-11-07T16:11:06.42231Z","iopub.status.idle":"2021-11-07T16:11:46.450588Z","shell.execute_reply.started":"2021-11-07T16:11:06.422277Z","shell.execute_reply":"2021-11-07T16:11:46.44976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Create the trainer and finetune the model","metadata":{}},{"cell_type":"code","source":"lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n\ncheckpoint_callback = pl.callbacks.ModelCheckpoint(\n    monitor='val_loss',\n    save_top_k=1,\n    filename='checkpoint/{epoch:02d}-{val_loss:.4f}',\n    mode='max',\n)\n\nwandb_logger = pl.loggers.WandbLogger(\n    project='chaii-competition',\n    group=f\"{HYPER_PARAMS.backbone}\",\n    job_type=f\"{HYPER_PARAMS.finetuning_strategy}\",\n    name=\"ORT=False_LR=1e-5_NoFreeze_CW=0.1_AG=2_WRate=0.5_4grpLRD\",\n    log_model=True,\n    config=asdict(HYPER_PARAMS),\n)\n\nearlystopping = pl.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min')\n\nswa = pl.callbacks.StochasticWeightAveraging()","metadata":{"_uuid":"b532aeef-38a6-4ef4-8458-e19bbcb09829","_cell_guid":"b85758e8-f778-4bcb-8665-322174200935","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T16:11:46.451928Z","iopub.execute_input":"2021-11-07T16:11:46.452264Z","iopub.status.idle":"2021-11-07T16:11:46.538624Z","shell.execute_reply.started":"2021-11-07T16:11:46.452228Z","shell.execute_reply":"2021-11-07T16:11:46.537841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FIND_LR = False\ncallbacks = [lr_monitor, earlystopping, checkpoint_callback]\ncallbacks_with_swa = callbacks + [swa]\n\nif FIND_LR:\n    callbacks.append(QuestionAnsweringFreezeEmbeddings(model_type=model.model.config.model_type))\n\ntrainer = Trainer(\n    fast_dev_run=HYPER_PARAMS.debug,\n    logger=wandb_logger if not HYPER_PARAMS.debug else True,\n    callbacks=callbacks,\n    gpus=HYPER_PARAMS.num_gpus,\n    log_every_n_steps=10,\n    weights_summary='top',\n    auto_lr_find=FIND_LR,\n    max_epochs=HYPER_PARAMS.max_epochs,\n    accumulate_grad_batches=HYPER_PARAMS.accumulate_grad_batches,\n    num_sanity_val_steps=0,\n    stochastic_weight_avg=HYPER_PARAMS.stochastic_weight_avg,\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:11:46.541832Z","iopub.execute_input":"2021-11-07T16:11:46.542145Z","iopub.status.idle":"2021-11-07T16:11:46.62684Z","shell.execute_reply.started":"2021-11-07T16:11:46.542119Z","shell.execute_reply":"2021-11-07T16:11:46.624748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_num_training_steps(datamodule, trainer) -> int:\n    \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n    dataset_size = len(datamodule.train_dataloader())\n    num_devices = max(1, trainer.num_gpus, trainer.num_processes)\n    effective_batch_size = trainer.accumulate_grad_batches * num_devices\n    max_estimated_steps = (dataset_size // effective_batch_size) * trainer.max_epochs\n    return max_estimated_steps\n\nnum_training_steps_for_lr_find = get_num_training_steps(datamodule, trainer)\nprint(f\"Num of training steps: {num_training_steps_for_lr_find}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:11:46.628146Z","iopub.execute_input":"2021-11-07T16:11:46.628727Z","iopub.status.idle":"2021-11-07T16:11:46.755761Z","shell.execute_reply.started":"2021-11-07T16:11:46.62869Z","shell.execute_reply":"2021-11-07T16:11:46.753697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if FIND_LR:\n    ## Find LR ##\n    # Run learning rate finder\n    lr_finder = trainer.tuner.lr_find(\n        model=model,\n        datamodule=datamodule,\n        min_lr=1e-8, \n        max_lr=1,\n        num_training=num_training_steps_for_lr_find,\n        mode=\"linear\",\n        early_stop_threshold=None,\n        update_attr=False,\n    )\n    fig = lr_finder.plot(suggest=True)\n    fig.show()\n    new_lr = lr_finder.suggestion()\n    print(f\"Suggested Learning Rate: {new_lr}\")\nelse:\n    ## Finetune the model ##\n    if not HYPER_PARAMS.debug:\n        wandb_logger.watch(model)\n    \n    if HYPER_PARAMS.finetuning_strategy == \"freeze\":\n        print(\"Frozen Model\")\n        trainer.finetune(model, datamodule=datamodule)\n    else:\n        print(\"Unfrozen Model\")\n        trainer.fit(model, datamodule=datamodule)\n    \n    if not HYPER_PARAMS.debug:\n        wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:11:46.757725Z","iopub.execute_input":"2021-11-07T16:11:46.761565Z","iopub.status.idle":"2021-11-07T16:11:54.834832Z","shell.execute_reply.started":"2021-11-07T16:11:46.761521Z","shell.execute_reply":"2021-11-07T16:11:54.831454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T16:11:54.8361Z","iopub.status.idle":"2021-11-07T16:11:54.836939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"# Convert the prediction queries to dictionary format.\npredict_data = pd.read_csv(PREDICT_DATA_PATH)\npredict_data = predict_data[predict_data.columns[:3]].to_dict(orient=\"list\")\n\n# Answer some Questions!\npredictions = model.predict(predict_data)\nprint(predictions)\n\n# Create submission.\nsubmission = {\"id\": [], \"PredictionString\": []}\nfor prediction in predictions:\n    submission[\"id\"].extend(prediction.keys())\n    submission[\"PredictionString\"].extend(prediction.values())\nsubmission = pd.DataFrame(submission)\nsubmission.to_csv(\"./submission.csv\", index=False)","metadata":{"_uuid":"4010070a-19d4-45ad-a736-4761f5daa8ed","_cell_guid":"5b704240-15f4-4c70-8e75-24219cb02841","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T16:11:54.838242Z","iopub.status.idle":"2021-11-07T16:11:54.839049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}