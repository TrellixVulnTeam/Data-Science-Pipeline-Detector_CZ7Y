{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n**THIS is adjusted fork of https://www.kaggle.com/hoshi7/chaii-pytorch-lightining-w-b**\n\nThe model building aspect was taken from: https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit\n\nOver the course of the month, I have learnt a lot about transformers and pytorch. During one such lesson, I stumbled across Pytorch-lightning and how it can create a general framework for the pytorch deep learning model that we are building. \nKeeping that in mind, I set across learning about how to structure regular pytorch code into lightning code. This is one such attempt at that, with WanDB to showcase the ML-OPS part of the training. ","metadata":{}},{"cell_type":"markdown","source":"![image.jpg](data:image/png;base64,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)","metadata":{}},{"cell_type":"markdown","source":"Also, over the course of a lot of learning, I realised that most of the tutorials that can be found easily on the web generally use a prepackaged model such as BertSequenceClassification, this made it difficult to figure out where the _forward_ step should go, or where the linear layers should have gone. Overcoming that obstacle was the main challenge of this notebook, along with finding a way to fit the model on Kaggle's less optimal memory provision. \n\nThus, this notebook highlights upon: \n1. Use of custom model (XLM Roberta) with pytorch lightning. \n2. Weights and Biases to showcase the ML-OPS\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Installing and Importing Libraries\n\nInstalling pytorch lightning and torch to a newer version as torch 1.7.0 has an error while training with lightning. ","metadata":{}},{"cell_type":"code","source":"!pip install -qU '../input/pytorch-lightning-with-hugging-face-transformers/datasets-1.15.1-py3-none-any.whl'\n!pip install -qU '../input/pytorch-lightning-with-hugging-face-transformers/huggingface_hub-0.1.2-py3-none-any.whl'\n!pip install -qU '../input/pytorch-lightning-with-hugging-face-transformers/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl'\n!pip install -qU '../input/pytorch-lightning-with-hugging-face-transformers/transformers-4.12.3-py3-none-any.whl'\n!pip install -qU '../input/pytorch-lightning-with-hugging-face-transformers/pytorch_lightning-1.4.9-py3-none-any.whl'","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:16:39.253561Z","iopub.execute_input":"2021-11-10T13:16:39.2539Z","iopub.status.idle":"2021-11-10T13:21:04.690793Z","shell.execute_reply.started":"2021-11-10T13:16:39.253815Z","shell.execute_reply":"2021-11-10T13:21:04.689947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:21:35.873654Z","iopub.execute_input":"2021-11-10T13:21:35.874186Z","iopub.status.idle":"2021-11-10T13:21:35.877931Z","shell.execute_reply.started":"2021-11-10T13:21:35.874149Z","shell.execute_reply":"2021-11-10T13:21:35.877213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper libraries\nimport os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport collections\nimport multiprocessing\nfrom pathlib import Path\nimport warnings\nfrom argparse import Namespace\nfrom typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom sklearn import model_selection\nfrom collections import OrderedDict\n\n#Pytorch, transformers\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_scheduler,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n\n#Import pytorch lightning: \nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:21:37.911166Z","iopub.execute_input":"2021-11-10T13:21:37.91169Z","iopub.status.idle":"2021-11-10T13:21:44.976515Z","shell.execute_reply.started":"2021-11-10T13:21:37.911645Z","shell.execute_reply":"2021-11-10T13:21:44.975677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest_df = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nexternal_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\nexternal_train = pd.concat([external_mlqa, external_xquad], ignore_index=True)\n\ndel external_mlqa, external_xquad\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:21:48.223309Z","iopub.execute_input":"2021-11-10T13:21:48.224161Z","iopub.status.idle":"2021-11-10T13:21:49.572571Z","shell.execute_reply.started":"2021-11-10T13:21:48.224114Z","shell.execute_reply":"2021-11-10T13:21:49.571943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Necessary Functions\n\nThe index of necessary functions in order:\n- **optimal_num_of_loader_workers**: Find the optimal number of workers based on config. Code from: https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit","metadata":{}},{"cell_type":"code","source":"def optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-10T13:21:51.992777Z","iopub.execute_input":"2021-11-10T13:21:51.993332Z","iopub.status.idle":"2021-11-10T13:21:51.997866Z","shell.execute_reply.started":"2021-11-10T13:21:51.993293Z","shell.execute_reply":"2021-11-10T13:21:51.997056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Configuration","metadata":{}},{"cell_type":"code","source":"config = Namespace(\n    seed = 7,\n    \n    trainer = Namespace(\n        precision = 16,\n        accumulate_grad_batches = 2,\n        max_epochs = 3,\n        weights_summary='top',\n        num_sanity_val_steps = 0,\n        gpus = 1,\n        fast_dev_run=True,\n#         stochastic_weight_avg=True,\n    ),\n    \n    model = Namespace(\n        model_name_or_path = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/\",\n        config_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/\",\n        optimizer_type = 'AdamW',\n        learning_rate = 3e-5,\n        weight_decay = 1e-2,\n        epsilon = 1e-8,\n        max_grad_norm = 1.0,\n        lr_scheduler = 'cosine',\n        warmup_ratio = 0.1,\n    ),\n    \n    data = Namespace(\n        train_batch_size = 4,\n        eval_batch_size = 8,\n        tokenizer_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/\",\n        max_seq_length = 384,\n        doc_stride = 128,\n        valid_split = 0.25,\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:21:53.60315Z","iopub.execute_input":"2021-11-10T13:21:53.605277Z","iopub.status.idle":"2021-11-10T13:21:53.615817Z","shell.execute_reply.started":"2021-11-10T13:21:53.605237Z","shell.execute_reply":"2021-11-10T13:21:53.61303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset class","metadata":{}},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        res = {\n            'input_ids': torch.tensor(feature['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(feature['attention_mask'], dtype=torch.long),\n        }\n        if self.mode == 'train':\n            res.update({\n                'start_position': torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position': torch.tensor(feature['end_position'], dtype=torch.long)\n            })\n        else:\n            res.update({\n                'id': feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            })\n            \n        return res","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:21:55.623261Z","iopub.execute_input":"2021-11-10T13:21:55.623546Z","iopub.status.idle":"2021-11-10T13:21:55.635597Z","shell.execute_reply.started":"2021-11-10T13:21:55.623514Z","shell.execute_reply":"2021-11-10T13:21:55.634669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the `LightningDataModule` with Pytorch Lightning\n","metadata":{}},{"cell_type":"code","source":"class DataModuleFit(pl.LightningDataModule):\n    def __init__(self, df=None, **kwargs):\n        super().__init__()\n        self.save_hyperparameters(ignore=['df'])\n        self.df = df\n        \n    def _prepare_features(self, example):\n        example[\"question\"] = example[\"question\"].lstrip()\n        tokenized_example = self._tokenizer(\n            example[\"question\"],\n            example[\"context\"],\n            truncation=\"only_second\",\n            max_length=self.hparams.max_seq_length,\n            stride=self.hparams.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n\n        sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n        offset_mapping = tokenized_example.pop(\"offset_mapping\")\n\n        features = []\n        for i, offsets in enumerate(offset_mapping):\n            feature = {}\n\n            input_ids = tokenized_example[\"input_ids\"][i]\n            attention_mask = tokenized_example[\"attention_mask\"][i]\n\n            feature['input_ids'] = input_ids\n            feature['attention_mask'] = attention_mask\n            feature['offset_mapping'] = offsets\n\n            cls_index = input_ids.index(self._tokenizer.cls_token_id)\n            sequence_ids = tokenized_example.sequence_ids(i)\n\n            sample_index = sample_mapping[i]\n            answers = example[\"answers\"]\n\n            if len(answers[\"answer_start\"]) == 0:\n                feature[\"start_position\"] = cls_index\n                feature[\"end_position\"] = cls_index\n            else:\n                start_char = answers[\"answer_start\"][0]\n                end_char = start_char + len(answers[\"text\"][0])\n\n                token_start_index = 0\n                while sequence_ids[token_start_index] != 1:\n                    token_start_index += 1\n\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != 1:\n                    token_end_index -= 1\n\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    feature[\"start_position\"] = cls_index\n                    feature[\"end_position\"] = cls_index\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    feature[\"start_position\"] = token_start_index - 1\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    feature[\"end_position\"] = token_end_index + 1\n\n            features.append(feature)\n        return features\n        \n    def prepare_data(self):\n        self._tokenizer = AutoTokenizer.from_pretrained(self.hparams.tokenizer_name)\n        \n        self.df = self.df.sample(frac=1.)\n        train_split = int((1 - self.hparams.valid_split) * df.shape[0])\n        train_set = df.iloc[:train_split]\n        valid_set = df.iloc[train_split:]\n\n        train_features, valid_features = [[] for _ in range(2)]\n        for _, row in train_set.iterrows():\n            train_features += self._prepare_features(row)\n        for _, row in valid_set.iterrows():\n            valid_features += self._prepare_features(row)\n\n        self._train_features = train_features\n        self._valid_features = valid_features\n        \n    def setup(self, stage = None):\n        self._train_dset = DatasetRetriever(self._train_features)\n        self._valid_dset = DatasetRetriever(self._valid_features)\n    \n    def train_dataloader(self):\n        return DataLoader(\n            self._train_dset,\n            batch_size=self.hparams.train_batch_size,\n            num_workers=optimal_num_of_loader_workers(),\n            pin_memory=True,\n            drop_last=False,\n            shuffle=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self._valid_dset,\n            batch_size=self.hparams.eval_batch_size,\n            num_workers=optimal_num_of_loader_workers(),\n            pin_memory=True,\n            drop_last=False,\n            shuffle=False,\n    )\n    \nclass DataModulePredict(DataModuleFit):\n    def __init__(self, df, *args, **kwargs):\n        super().__init__(df=df)\n#         self.save_hyperparameters(ignore=['df'])\n#         self.df = df\n        \n    def _prepare_features(self, example):\n        example[\"question\"] = example[\"question\"].lstrip()\n\n        tokenized_example = self._tokenizer(\n            example[\"question\"],\n            example[\"context\"],\n            truncation=\"only_second\",\n            max_length=self.hparams.max_seq_length,\n            stride=self.hparams.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n\n        features = []\n        for i in range(len(tokenized_example[\"input_ids\"])):\n            feature = {}\n            feature[\"example_id\"] = example['id']\n            feature['context'] = example['context']\n            feature['question'] = example['question']\n            feature['input_ids'] = tokenized_example['input_ids'][i]\n            feature['attention_mask'] = tokenized_example['attention_mask'][i]\n            feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n            feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n            cls_index = feature['input_ids'].index(self._tokenizer.cls_token_id)\n            feature['cls_index'] = cls_index\n            features.append(feature)\n\n        return features\n        \n    def prepare_data(self):\n        self._tokenizer = AutoTokenizer.from_pretrained(self.hparams.tokenizer_name)\n        \n        pred_features = []\n\n        for _, row in self.df.iterrows():\n            pred_features += self._prepare_features(row)\n\n        self.pred_features = pred_features\n        \n    def setup(self, stage = None):\n        self._pred_dset = DatasetRetriever(self.pred_features, mode='predict')\n    \n    def predict_dataloader(self):\n        return DataLoader(\n            self._pred_dset,\n            batch_size=self.hparams.eval_batch_size,\n            num_workers=optimal_num_of_loader_workers(),\n            pin_memory=True,\n            drop_last=False,\n            shuffle=False\n        )    ","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:21:57.640289Z","iopub.execute_input":"2021-11-10T13:21:57.641074Z","iopub.status.idle":"2021-11-10T13:21:57.669564Z","shell.execute_reply.started":"2021-11-10T13:21:57.64103Z","shell.execute_reply":"2021-11-10T13:21:57.668798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the `LightningModule` with Pytorch Lightning","metadata":{}},{"cell_type":"code","source":"class Model(pl.LightningModule):\n\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model_config = AutoConfig.from_pretrained(self.hparams.config_name)\n        self.model = AutoModel.from_pretrained(self.hparams.model_name_or_path, config=self.model_config)\n        self.qa_outputs = nn.Linear(self.model_config.hidden_size, 2)\n        self.dropout = nn.Dropout(self.model_config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"The forward step performs the next step for the model while training.\"\"\"\n        sequence_output = self.model(input_ids, attention_mask=attention_mask)[0]\n        qa_logits = self.qa_outputs(sequence_output)\n\n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def get_num_training_steps(self) -> int:\n        \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n        if not getattr(self, \"trainer\", None):\n            raise MisconfigurationException(\"The LightningModule isn't attached to the trainer yet.\")\n        if isinstance(self.trainer.limit_train_batches, int) and self.trainer.limit_train_batches != 0:\n            num_batches = self.trainer.limit_train_batches\n        elif isinstance(self.trainer.limit_train_batches, float):\n            # limit_train_batches is a percentage of batches\n            dataset_size = len(self.train_dataloader())\n            num_batches = int(dataset_size * self.trainer.limit_train_batches)\n        else:\n            num_batches = len(self.train_dataloader())\n\n        num_devices = max(1, self.trainer.num_gpus, self.trainer.num_processes)\n        if self.trainer.tpu_cores:\n            num_devices = max(num_devices, self.trainer.tpu_cores)\n\n        effective_batch_size = self.trainer.accumulate_grad_batches * num_devices\n        max_estimated_steps = (num_batches // effective_batch_size) * self.trainer.max_epochs\n\n        if self.trainer.max_steps and self.trainer.max_steps < max_estimated_steps:\n            return self.trainer.max_steps\n        return max_estimated_steps\n\n    @staticmethod\n    def _compute_warmup(num_training_steps: int, num_warmup_steps: Union[int, float]) -> int:\n        if isinstance(num_warmup_steps, float) and (num_warmup_steps > 1 or num_warmup_steps < 0):\n            raise MisconfigurationException(\"`num_warmup_steps` as float should be provided between 0 and 1.\")\n\n        if isinstance(num_warmup_steps, int):\n            if num_warmup_steps > num_training_steps:\n                raise MisconfigurationException(\"`num_warmup_steps` as int should be less than `num_training_steps`.\")\n            return num_warmup_steps\n            \n\n        if isinstance(num_warmup_steps, float):\n            # Convert float values to percentage of training steps to use as warmup\n            num_warmup_steps *= num_training_steps\n        return round(num_warmup_steps)                \n\n    def configure_optimizers(self):\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                \"weight_decay_rate\": self.hparams.weight_decay\n            },\n            {\n                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                \"weight_decay_rate\": 0.0\n            },\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps = self.hparams.epsilon, correct_bias=True)\n        if self.hparams.lr_scheduler is not None:\n            num_training_steps = self.get_num_training_steps()\n            lr_scheduler = get_scheduler(\n                name=self.hparams.lr_scheduler,\n                optimizer=optimizer,\n                num_warmup_steps=self._compute_warmup(num_training_steps, self.hparams.warmup_ratio),\n                num_training_steps=num_training_steps,\n            )\n            lr_scheduler_config = {\n                \"scheduler\": lr_scheduler,\n                \"interval\": \"step\",\n                \"frequency\": 1,\n            }\n            return [optimizer], [lr_scheduler_config]\n        return optimizer\n\n#     def configure_optimizers(self):\n#         # To be passed to the optimizer (only parameters of the layers you want to update).\n#         opt_parameters = []\n#         weight_decay = self.hparams.weight_decay    \n#         no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n#         named_parameters = list(self.model.named_parameters())\n#         def get_lrs(start_lr, last_lr):\n#             step = np.abs(np.log(start_lr) - np.log(last_lr)) / 3\n#             return [start_lr,  np.exp(np.log(start_lr) + step), np.exp(np.log(start_lr)+2*step), last_lr]\n#         def is_decay_param(name: str):\n#             return not any(p in name for p in no_decay)\n#         ranges = [(0,69), (69, 133), (133, 197), (197, 199)]\n#         lrs = get_lrs(1e-8, self.hparams.learning_rate)\n#         for _range, lr in zip(ranges, lrs):\n#             params = named_parameters[_range[0]:_range[1]]\n#             decay_parameters = [p for n, p in params if is_decay_param(n)]\n#             no_decay_parameters = [p for n, p in params if not is_decay_param(n)]\n#             opt_parameters.append(\n#                 {\"params\": decay_parameters, \"weight_decay\": weight_decay, \"lr\": lr}\n#             )\n#             # According to AAAMLP book by A. Thakur, we generally do not use any decay\n#             # for bias and LayerNorm.weight layers.\n#             opt_parameters.append(\n#                 {\"params\": no_decay_parameters, \"weight_decay\": 0.0, \"lr\": lr}\n#             )\n#         optimizer = AdamW(opt_parameters, lr=self.hparams.learning_rate, correct_bias=True)\n#         if self.hparams.lr_scheduler is not None:\n#             num_training_steps = self.get_num_training_steps()\n#             lr_scheduler = get_scheduler(\n#                 name=self.hparams.lr_scheduler,\n#                 optimizer=optimizer,\n#                 num_warmup_steps=self._compute_warmup(num_training_steps, self.hparams.warmup_ratio),\n#                 num_training_steps=num_training_steps,\n#             )\n#             lr_scheduler_config = {\n#                 \"scheduler\": lr_scheduler,\n#                 \"interval\": \"step\",\n#                 \"frequency\": 1,\n#             }\n#             return [optimizer], [lr_scheduler_config]\n#         return optimizer\n    \n    def _compute_loss(self, preds, labels):\n        start_preds, end_preds = preds\n        start_labels, end_labels = labels\n        start_loss = F.cross_entropy(start_preds, start_labels, ignore_index=-1)\n        end_loss = F.cross_entropy(end_preds, end_labels, ignore_index=-1)\n        total_loss = (start_loss + end_loss) / 2\n        return total_loss\n    \n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets_start = batch[\"start_position\"]\n        targets_end = batch['end_position']\n        \n        outputs_start, outputs_end = self(input_ids, attention_mask=attention_mask)\n        loss = self._compute_loss((outputs_start, outputs_end), (targets_start, targets_end))\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets_start = batch[\"start_position\"]\n        targets_end = batch['end_position']\n        \n        outputs_start, outputs_end = self(input_ids, attention_mask=attention_mask)\n        loss = self._compute_loss((outputs_start, outputs_end), (targets_start, targets_end))\n        self.log('val_loss', loss, prog_bar=True)\n\n    def predict_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        pred_start, pred_end = self(input_ids, attention_mask=attention_mask)\n        return {\n            'pred_start': pred_start,\n            'pred_end': pred_end,\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:21:59.576927Z","iopub.execute_input":"2021-11-10T13:21:59.577372Z","iopub.status.idle":"2021-11-10T13:21:59.607398Z","shell.execute_reply.started":"2021-11-10T13:21:59.577334Z","shell.execute_reply":"2021-11-10T13:21:59.606459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model with Pytorch lightning","metadata":{}},{"cell_type":"code","source":"pl.seed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:22:01.452621Z","iopub.execute_input":"2021-11-10T13:22:01.453179Z","iopub.status.idle":"2021-11-10T13:22:01.463698Z","shell.execute_reply.started":"2021-11-10T13:22:01.45314Z","shell.execute_reply":"2021-11-10T13:22:01.462524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train_df, external_train], ignore_index=True)\ndf['answers'] = df[['answer_start', 'answer_text']].apply(lambda x: {'answer_start': [x[0]], 'text': [x[1]]}, axis=1)\n\ntest_df['context'] = test_df['context'].apply(lambda x: ' '.join(x.split()))\ntest_df['question'] = test_df['question'].apply(lambda x: ' '.join(x.split()))\n\ndel train_df, external_train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:22:04.422743Z","iopub.execute_input":"2021-11-10T13:22:04.42328Z","iopub.status.idle":"2021-11-10T13:22:04.790366Z","shell.execute_reply.started":"2021-11-10T13:22:04.423241Z","shell.execute_reply":"2021-11-10T13:22:04.789579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initiate callbacks\nlr_monitor = LearningRateMonitor(logging_interval='step')\nlogger = CSVLogger(save_dir='logs/')\n# Checkpoint\nckpt = ModelCheckpoint(\n    monitor=f'val_loss',\n    save_top_k=1,\n    save_last=False,\n    save_weights_only=True,\n    dirpath='checkpoints',\n    filename='{epoch:02d}-{val_loss:.4f}',\n    verbose=False,\n    mode='min',\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:22:05.832842Z","iopub.execute_input":"2021-11-10T13:22:05.83337Z","iopub.status.idle":"2021-11-10T13:22:05.85751Z","shell.execute_reply.started":"2021-11-10T13:22:05.833332Z","shell.execute_reply":"2021-11-10T13:22:05.856844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(**vars(config.model))\ndm = DataModuleFit(df, **vars(config.data))\n\ntrainer = pl.Trainer(\n    logger=logger,\n    callbacks=[ckpt, lr_monitor],\n    **vars(config.trainer)\n)\ntrainer.fit(model, datamodule=dm)\n\ntorch.cuda.empty_cache()\ndel trainer, model, dm\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:22:08.375689Z","iopub.execute_input":"2021-11-10T13:22:08.376225Z","iopub.status.idle":"2021-11-10T13:23:25.100356Z","shell.execute_reply.started":"2021-11-10T13:22:08.376179Z","shell.execute_reply":"2021-11-10T13:23:25.099574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict with Pytorch lightning","metadata":{}},{"cell_type":"code","source":"dm = DataModulePredict(test_df, **vars(config.data))\n\nsub_pred_start = None\nsub_pred_end = None\npred_features = None\n\ntrainer = pl.Trainer(\n    enable_checkpointing=False,\n    **vars(config.trainer),\n)\nmodel = Model.load_from_checkpoint(ckpt.best_model_path, **vars(config.model))\npreds = trainer.predict(model, datamodule=dm)\n\nsub_pred_start = np.vstack([x['pred_start'] for x in preds])\nsub_pred_end = np.vstack([x['pred_end'] for x in preds])    \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:32.75944Z","iopub.execute_input":"2021-11-10T13:23:32.760403Z","iopub.status.idle":"2021-11-10T13:23:33.428148Z","shell.execute_reply.started":"2021-11-10T13:23:32.760361Z","shell.execute_reply":"2021-11-10T13:23:33.426071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post process the results\nCredits: https://www.kaggle.com/kishalmandal/5-folds-infer-combined-model-0-792/","metadata":{}},{"cell_type":"code","source":"def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = []\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"cls_index\"]\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        predictions.append((example[\"id\"], best_answer[\"text\"]))\n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:36:15.055245Z","iopub.status.idle":"2021-11-09T07:36:15.05578Z","shell.execute_reply.started":"2021-11-09T07:36:15.055553Z","shell.execute_reply":"2021-11-09T07:36:15.055579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_preds = postprocess_qa_predictions(test_df, dm.pred_features, (sub_pred_start, sub_pred_end))\nsub_df = pd.DataFrame(processed_preds, columns=['id', 'PredictionString'])\nsub_df = sub_df.merge(test_df, how='left', on='id')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:36:15.056894Z","iopub.status.idle":"2021-11-09T07:36:15.05746Z","shell.execute_reply.started":"2021-11-09T07:36:15.057228Z","shell.execute_reply":"2021-11-09T07:36:15.057251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\ntamil_ad = \"கி.பி\"\ntamil_bc = \"கி.மு\"\ntamil_km = \"கி.மீ\"\nhindi_ad = \"ई\"\nhindi_bc = \"ई.पू\"\n\n\ncleaned_preds = []\nfor _, (pred, context) in sub_df[[\"PredictionString\", \"context\"]].iterrows():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    if pred.endswith(\"...\"):\n            pred = pred[:-3]\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n        \n    cleaned_preds.append(pred)\n\nsub_df[\"PredictionString\"] = cleaned_preds\nsub_df = sub_df[['id', 'PredictionString']]","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:36:15.058569Z","iopub.status.idle":"2021-11-09T07:36:15.059126Z","shell.execute_reply.started":"2021-11-09T07:36:15.05889Z","shell.execute_reply":"2021-11-09T07:36:15.058916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv('submission.csv', header=True, index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:36:15.060196Z","iopub.status.idle":"2021-11-09T07:36:15.060763Z","shell.execute_reply.started":"2021-11-09T07:36:15.060525Z","shell.execute_reply":"2021-11-09T07:36:15.060551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:36:15.061841Z","iopub.status.idle":"2021-11-09T07:36:15.062392Z","shell.execute_reply.started":"2021-11-09T07:36:15.062148Z","shell.execute_reply":"2021-11-09T07:36:15.062173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}