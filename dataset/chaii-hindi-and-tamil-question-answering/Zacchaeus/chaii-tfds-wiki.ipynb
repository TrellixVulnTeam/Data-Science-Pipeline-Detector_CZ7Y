{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U -q tensorflow_datasets","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:51:26.89104Z","iopub.execute_input":"2021-09-19T16:51:26.891406Z","iopub.status.idle":"2021-09-19T16:51:40.368609Z","shell.execute_reply.started":"2021-09-19T16:51:26.891316Z","shell.execute_reply":"2021-09-19T16:51:40.367526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nseed = 42\n\ndef get_data(lang, minlen=150, splitjoin=False):\n    ds = tfds.load(f'wikipedia/20201201.{lang}', split='train', shuffle_files=False)\n    texts = []\n    titles = []\n    for item in tqdm(ds.take(len(ds))):\n        text = item['text'].numpy().decode('utf-8')\n        if len(text) < minlen:\n            continue\n        texts.append(text)\n        titles.append(item['title'].numpy().decode('utf-8'))\n    if splitjoin:\n        texts = [' '.join(x.split()) for x in texts]\n    print(lang, len(texts))\n    return texts, titles","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-19T17:37:17.071849Z","iopub.execute_input":"2021-09-19T17:37:17.072129Z","iopub.status.idle":"2021-09-19T17:37:17.081041Z","shell.execute_reply.started":"2021-09-19T17:37:17.072103Z","shell.execute_reply":"2021-09-19T17:37:17.079951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntrain['len'] = train['context'].apply(len)\nminlen = train['len'].min()\nprint('minlen', minlen)\n\ndata = {'text': [], 'title': [], 'language': []}\nfor lang in ['hi', 'ta']:\n    texts, titles = get_data(lang, minlen=minlen)\n    data['text'].extend(texts)\n    data['title'].extend(titles)\n    data['language'].extend([lang]*len(texts))\ndf = pd.DataFrame(data)\ndf = df.sample(frac=1, random_state=seed).reset_index(drop=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:35:19.726323Z","iopub.execute_input":"2021-09-19T17:35:19.726676Z","iopub.status.idle":"2021-09-19T17:36:31.591589Z","shell.execute_reply.started":"2021-09-19T17:35:19.726641Z","shell.execute_reply":"2021-09-19T17:36:31.590723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('hita-wiki-0920.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T17:38:13.772389Z","iopub.execute_input":"2021-09-19T17:38:13.773223Z","iopub.status.idle":"2021-09-19T17:38:39.821067Z","shell.execute_reply.started":"2021-09-19T17:38:13.773183Z","shell.execute_reply":"2021-09-19T17:38:39.819977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}