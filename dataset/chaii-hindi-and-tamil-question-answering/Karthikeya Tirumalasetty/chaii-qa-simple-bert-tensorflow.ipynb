{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport pandas as pd\n\nfrom transformers import AutoTokenizer, AutoConfig\n\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:06:34.668946Z","iopub.execute_input":"2022-02-12T07:06:34.669681Z","iopub.status.idle":"2022-02-12T07:06:44.15857Z","shell.execute_reply.started":"2022-02-12T07:06:34.669593Z","shell.execute_reply":"2022-02-12T07:06:44.157828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4'\ntrainFile = '../input/chaii-hindi-and-tamil-question-answering/train.csv'\ntestFile = '../input/chaii-hindi-and-tamil-question-answering/test.csv'","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:06:44.161959Z","iopub.execute_input":"2022-02-12T07:06:44.162166Z","iopub.status.idle":"2022-02-12T07:06:44.167858Z","shell.execute_reply.started":"2022-02-12T07:06:44.162143Z","shell.execute_reply":"2022-02-12T07:06:44.167115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_layer = hub.KerasLayer(tfhub_handle_encoder,trainable=True)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\nconfig = AutoConfig.from_pretrained('bert-base-multilingual-cased')\n\ntokenizer.save_pretrained('bert-base-multilingual-cased-tokenizer')\nconfig.save_pretrained('bert-base-multilingual-cased-tokenizer')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:06:44.170594Z","iopub.execute_input":"2022-02-12T07:06:44.170775Z","iopub.status.idle":"2022-02-12T07:07:17.024341Z","shell.execute_reply.started":"2022-02-12T07:06:44.170755Z","shell.execute_reply":"2022-02-12T07:07:17.023566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindf = pd.read_csv(trainFile)\ntestdf = pd.read_csv(testFile)\nprint(traindf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.026369Z","iopub.execute_input":"2022-02-12T07:07:17.026625Z","iopub.status.idle":"2022-02-12T07:07:17.794297Z","shell.execute_reply.started":"2022-02-12T07:07:17.026593Z","shell.execute_reply":"2022-02-12T07:07:17.793503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'bert-chaii'\nmax_seq_length = 384","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.795547Z","iopub.execute_input":"2022-02-12T07:07:17.795805Z","iopub.status.idle":"2022-02-12T07:07:17.800147Z","shell.execute_reply.started":"2022-02-12T07:07:17.795757Z","shell.execute_reply":"2022-02-12T07:07:17.79888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.801461Z","iopub.execute_input":"2022-02-12T07:07:17.801963Z","iopub.status.idle":"2022-02-12T07:07:17.829418Z","shell.execute_reply.started":"2022-02-12T07:07:17.801913Z","shell.execute_reply":"2022-02-12T07:07:17.828686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.830405Z","iopub.execute_input":"2022-02-12T07:07:17.831125Z","iopub.status.idle":"2022-02-12T07:07:17.843375Z","shell.execute_reply.started":"2022-02-12T07:07:17.83109Z","shell.execute_reply":"2022-02-12T07:07:17.842532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def context_offsets_index(offsets):\n    last_start = 0\n    idx = 0\n    for x, (ostart, oend) in enumerate(offsets):\n        if ostart >= last_start:\n            idx = x\n            last_start = ostart\n        else:\n            break\n    return idx+1","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.844372Z","iopub.execute_input":"2022-02-12T07:07:17.845139Z","iopub.status.idle":"2022-02-12T07:07:17.850559Z","shell.execute_reply.started":"2022-02-12T07:07:17.845105Z","shell.execute_reply":"2022-02-12T07:07:17.849693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_samples(question, context, start_char_idx=None, answer_text=None):\n\n    encoding = tokenizer.encode_plus(question, context,  return_tensors='np',\n                                     max_length=384, stride=128, return_overflowing_tokens=True,\n                                     padding=\"max_length\", truncation=True,\n                                     return_offsets_mapping=True)\n\n    input_word_ids = encoding.input_ids\n    token_type_ids = encoding.token_type_ids\n    attention_mask = encoding.attention_mask\n    offsets = encoding.offset_mapping\n\n    # check if end character index is in the context\n    end_char_idx = start_char_idx + len(answer_text)\n    if end_char_idx >= len(context):\n        return\n\n    # mark all the character indexes in context that are also in answer\n    is_char_in_ans = [0] * len(context)\n    for idx in range(start_char_idx, end_char_idx):\n        is_char_in_ans[idx] = 1\n    \n    inputs = []\n    for x in range(len(input_word_ids)):\n        ans_token_idx = []\n        context_index = context_offsets_index(offsets[x])\n        # find all the tokens that are in the answers\n        sample_offsets = offsets[x][context_index:]\n        #print('sample_offsets=', sample_offsets.shape)\n        for idx, (start, end) in enumerate(sample_offsets):\n            if sum(is_char_in_ans[start:end]) > 0:\n                ans_token_idx.append(idx)\n\n        if len(ans_token_idx) == 0:\n            continue\n\n        start_token_idx = ans_token_idx[0]\n        end_token_idx = ans_token_idx[-1]\n        \n        print('Question=', question)\n        print('answer text=', answer_text)\n        print('string=', tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_word_ids[x][context_index:][start_token_idx:end_token_idx+1])))\n\n        input = {'input_word_ids': input_word_ids[x], 'input_type_ids': token_type_ids[x],\n                 'input_mask': attention_mask[x], 'start_token_idx': start_token_idx,\n                 'end_token_idx': end_token_idx}\n\n        inputs.append(input)\n\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.851893Z","iopub.execute_input":"2022-02-12T07:07:17.852267Z","iopub.status.idle":"2022-02-12T07:07:17.86539Z","shell.execute_reply.started":"2022-02-12T07:07:17.852231Z","shell.execute_reply":"2022-02-12T07:07:17.863515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chaii_train_data(df):\n    train_data_samples = []\n    for idx, row in df.iterrows():\n        question = row['question']\n        context = row['context']\n        answer_start = row['answer_start']\n        answer_text = row['answer_text']\n        language = row['language']\n\n        samples = make_samples(question, context, answer_start, answer_text)\n\n        for s in samples:\n            train_data_samples.append(s)\n\n    return train_data_samples","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.867328Z","iopub.execute_input":"2022-02-12T07:07:17.867523Z","iopub.status.idle":"2022-02-12T07:07:17.876105Z","shell.execute_reply.started":"2022-02-12T07:07:17.867502Z","shell.execute_reply":"2022-02-12T07:07:17.875411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_bert_inputs(samples):\n    dataset_dict = {\n        \"input_word_ids\": [],\n        \"input_type_ids\": [],\n        \"input_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n\n    for item in samples:\n        for key in dataset_dict:\n            dataset_dict[key].append(item[key])\n\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [dataset_dict[\"input_word_ids\"],\n         dataset_dict[\"input_mask\"],\n         dataset_dict[\"input_type_ids\"]]\n\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.877194Z","iopub.execute_input":"2022-02-12T07:07:17.877496Z","iopub.status.idle":"2022-02-12T07:07:17.886255Z","shell.execute_reply.started":"2022-02-12T07:07:17.877462Z","shell.execute_reply":"2022-02-12T07:07:17.885524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def buildModel():\n    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n    input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n    \n    bert_inputs = {'input_word_ids': input_word_ids, 'input_mask': input_mask, 'input_type_ids': input_type_ids}\n    bert_outputs = bert_layer(bert_inputs)\n\n    pooled_output = bert_outputs[\"pooled_output\"]      # [batch_size, 768].\n    sequence_output = bert_outputs[\"sequence_output\"]  # [batch_size, seq_length, 768].\n\n    start_logits = tf.keras.layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n    start_logits = tf.keras.layers.Flatten()(start_logits)\n    end_logits = tf.keras.layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n    end_logits = tf.keras.layers.Flatten()(end_logits)\n    start_probs = tf.keras.layers.Activation(tf.keras.activations.softmax)(start_logits)\n    end_probs = tf.keras.layers.Activation(tf.keras.activations.softmax)(end_logits)\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.887573Z","iopub.execute_input":"2022-02-12T07:07:17.887864Z","iopub.status.idle":"2022-02-12T07:07:17.898989Z","shell.execute_reply.started":"2022-02-12T07:07:17.887831Z","shell.execute_reply":"2022-02-12T07:07:17.898224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    train_samples = chaii_train_data(traindf)\n    x, y = create_bert_inputs(train_samples)\n\n    print(f\"{len(train_samples)} training points created.\")\n\n    model = buildModel()\n\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = tf.keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    model.summary()\n\n    model.fit(x, y, epochs=17, batch_size=8)\n    model.save(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.900385Z","iopub.execute_input":"2022-02-12T07:07:17.900683Z","iopub.status.idle":"2022-02-12T07:07:17.908018Z","shell.execute_reply.started":"2022-02-12T07:07:17.90065Z","shell.execute_reply":"2022-02-12T07:07:17.907248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model():\n    model = tf.keras.models.load_model(model_name)\n    for idx, row in testdf.iterrows():\n        id = row['id']\n        question = row['question']\n        context = row['context']\n        encoding = tokenizer.encode_plus(question, context, return_tensors='np',\n                                            max_length=384, stride=128, return_overflowing_tokens=True,\n                                            padding=\"max_length\", truncation=True,\n                                            return_offsets_mapping=True)\n\n        input_word_ids = encoding.input_ids\n        token_type_ids = encoding.token_type_ids\n        attention_mask = encoding.attention_mask\n        offsets = encoding.offset_mapping\n\n        input = {'input_word_ids': input_word_ids, 'input_type_ids': token_type_ids, 'input_mask': attention_mask}\n\n        x = [input[\"input_word_ids\"],input[\"input_mask\"],input[\"input_type_ids\"]]\n\n        pred_start, pred_end = model.predict(x)\n\n        max = 0\n        answer = \"unknown\"\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            print('start probability=', start.max(), ' , end probability=', end.max())\n            start = np.argmax(start)\n            end = np.argmax(end)\n            \n            context_index = context_offsets_index(offsets[idx])\n            # find all the tokens that are in the answers\n            sample_offsets = offsets[idx][context_index:]\n\n            if start >= end : continue\n            if end >= len(sample_offsets): continue\n                \n            char_start = sample_offsets[start][0]\n            char_end = sample_offsets[end][1]\n            answer = context[char_start:char_end]\n\n            print(\"id=\", id, \", Q=\",  question, \", A=\", answer)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.909186Z","iopub.execute_input":"2022-02-12T07:07:17.909756Z","iopub.status.idle":"2022-02-12T07:07:17.922711Z","shell.execute_reply.started":"2022-02-12T07:07:17.909723Z","shell.execute_reply":"2022-02-12T07:07:17.921885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model()\ntest_model()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:07:17.923971Z","iopub.execute_input":"2022-02-12T07:07:17.924731Z","iopub.status.idle":"2022-02-12T07:29:27.674085Z","shell.execute_reply.started":"2022-02-12T07:07:17.924694Z","shell.execute_reply":"2022-02-12T07:29:27.673256Z"},"trusted":true},"execution_count":null,"outputs":[]}]}