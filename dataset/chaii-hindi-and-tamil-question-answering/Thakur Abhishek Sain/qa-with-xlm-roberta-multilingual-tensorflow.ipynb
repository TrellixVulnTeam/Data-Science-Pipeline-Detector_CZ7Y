{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Competition Overview:\n\nIn this competition, the goal is to predict answers to real questions about Wikipedia articles. You will use chaii-1, a new question answering dataset with question-answer pairs. The dataset covers Hindi and Tamil, collected without the use of translation. It provides a realistic information-seeking task with questions written by native-speaking expert data annotators. ","metadata":{}},{"cell_type":"markdown","source":"\n# Competition Rules:\n- CPU Notebook <= 5 hours run-time\n-GPU Notebook <= 5 hours run-time\n-Internet access disabled\n- Freely & publicly available external data is allowed, including pre-trained models\n- Submission file must be named submission.csv","metadata":{}},{"cell_type":"markdown","source":"# Competition Metrics:\nThe metric in this competition is the word-level Jaccard score\n\n`def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))`","metadata":{}},{"cell_type":"markdown","source":"### Ref:\n- https://keras.io/examples/nlp/text_extraction_with_bert/\n- https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n- https://www.kaggle.com/wchowdhu/hands-on-nli-w-transformers-m-bert-xlm-roberta#Data-Exploration-and-Analysis","metadata":{}},{"cell_type":"markdown","source":"### External Datasets\n- This Notebook uses @rhtsingh's hindi dataset: [Hindi External](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing/data)\n- For Tamil, the dataset i have created: [Tamil External](https://www.kaggle.com/msafi04/squad-translated-to-tamil-for-chaii)","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nfrom tqdm.auto import tqdm\nimport collections\n\nimport os\n\nfrom pathlib import Path\n\nimport json\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nsns.set_palette('Set3_r')\n\npd.set_option(\"display.max_rows\", 20, \"display.max_columns\", None)\n\nprint(os.listdir('../input/'))\n        \nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport warnings\nwarnings.simplefilter(action = 'ignore', category = Warning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-08T05:17:50.022227Z","iopub.execute_input":"2021-09-08T05:17:50.022623Z","iopub.status.idle":"2021-09-08T05:17:51.205842Z","shell.execute_reply.started":"2021-09-08T05:17:50.022541Z","shell.execute_reply":"2021-09-08T05:17:51.204985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/chaii-hindi-and-tamil-question-answering/train.csv')\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:17:51.209168Z","iopub.execute_input":"2021-09-08T05:17:51.20945Z","iopub.status.idle":"2021-09-08T05:17:52.170607Z","shell.execute_reply.started":"2021-09-08T05:17:51.209424Z","shell.execute_reply":"2021-09-08T05:17:52.169399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/chaii-hindi-and-tamil-question-answering/test.csv')\nprint(test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:17:52.172113Z","iopub.execute_input":"2021-09-08T05:17:52.172384Z","iopub.status.idle":"2021-09-08T05:17:52.19948Z","shell.execute_reply.started":"2021-09-08T05:17:52.172358Z","shell.execute_reply":"2021-09-08T05:17:52.19837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\nprint(sub.shape)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:17:52.200858Z","iopub.execute_input":"2021-09-08T05:17:52.20115Z","iopub.status.idle":"2021-09-08T05:17:52.219494Z","shell.execute_reply.started":"2021-09-08T05:17:52.201125Z","shell.execute_reply":"2021-09-08T05:17:52.218639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"external_hindi1 = pd.read_csv('/kaggle/input/mlqa-hindi-processed/mlqa_hindi.csv')\nprint(external_hindi1.shape)\nexternal_hindi1.head(2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-08T05:17:52.220687Z","iopub.execute_input":"2021-09-08T05:17:52.221003Z","iopub.status.idle":"2021-09-08T05:17:52.628386Z","shell.execute_reply.started":"2021-09-08T05:17:52.220975Z","shell.execute_reply":"2021-09-08T05:17:52.62748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"external_hindi2 = pd.read_csv('/kaggle/input/mlqa-hindi-processed/xquad.csv')\nprint(external_hindi2.shape)\nexternal_hindi2.head(2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-08T05:17:52.629489Z","iopub.execute_input":"2021-09-08T05:17:52.629774Z","iopub.status.idle":"2021-09-08T05:17:52.720666Z","shell.execute_reply.started":"2021-09-08T05:17:52.629746Z","shell.execute_reply":"2021-09-08T05:17:52.719805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('External hindi dataset...')\nexternal_hindi = pd.concat([external_hindi1, external_hindi2])\nprint(external_hindi.shape)\nexternal_hindi.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-08T05:17:52.721959Z","iopub.execute_input":"2021-09-08T05:17:52.722254Z","iopub.status.idle":"2021-09-08T05:17:52.737743Z","shell.execute_reply.started":"2021-09-08T05:17:52.722226Z","shell.execute_reply":"2021-09-08T05:17:52.736999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('External Tamil dataset...')\nexternal_tamil = pd.read_csv('/kaggle/input/squad-translated-to-tamil-for-chaii/squad_translated_tamil.csv')\nexternal_tamil['language'] = 'tamil'\nprint(external_tamil.shape)\nexternal_tamil.head(2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-08T05:17:52.738745Z","iopub.execute_input":"2021-09-08T05:17:52.739248Z","iopub.status.idle":"2021-09-08T05:17:52.978884Z","shell.execute_reply.started":"2021-09-08T05:17:52.739215Z","shell.execute_reply":"2021-09-08T05:17:52.977993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Combined External dataset...')\nexternal_df = pd.concat([external_hindi, external_tamil])\nexternal_df = external_df.sample(frac = 1).reset_index(drop = True)\nprint(external_df.shape)\nexternal_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:17:52.982454Z","iopub.execute_input":"2021-09-08T05:17:52.982751Z","iopub.status.idle":"2021-09-08T05:17:53.009872Z","shell.execute_reply.started":"2021-09-08T05:17:52.982724Z","shell.execute_reply":"2021-09-08T05:17:53.008894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del external_hindi1, external_hindi2, external_hindi, external_tamil\ngc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-08T05:17:53.011601Z","iopub.execute_input":"2021-09-08T05:17:53.011888Z","iopub.status.idle":"2021-09-08T05:17:53.11665Z","shell.execute_reply.started":"2021-09-08T05:17:53.011861Z","shell.execute_reply":"2021-09-08T05:17:53.115606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2)\nsns.countplot(x = 'language', data = external_df, ax = ax1).set_title('External Dataset Language Counts')\nfor p in ax1.patches:\n    ax1.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\nsns.countplot(x = 'language', data = test, ax = ax2).set_title('Test Language Counts')\nfor p in ax2.patches:\n    ax2.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-08T05:17:53.118215Z","iopub.execute_input":"2021-09-08T05:17:53.11862Z","iopub.status.idle":"2021-09-08T05:17:53.453563Z","shell.execute_reply.started":"2021-09-08T05:17:53.118578Z","shell.execute_reply":"2021-09-08T05:17:53.452468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Huggingface TF XLM RoBerta\nXLM-RoBERTa is a scaled cross lingual sentence encoder. It is trained on 2.5T of data across 100 languages data filtered from Common Crawl. XLM-R achieves state-of-the-arts results on multiple cross lingual benchmarks.","metadata":{}},{"cell_type":"code","source":"import yaml\n\nhparams = {\n    'DEVICE': 'TPU',\n    'EPOCHS': 2,\n    'MODEL_2': '../input/jplu-tf-xlm-roberta-large',\n    'N_FOLDS': 2,\n    'SEED': 777,\n    'VERBOSE': 1,\n    'BATCH_SIZE': 32,\n    'MAX_LENGTH': 512,\n    'DOC_STRIDE': 128\n    \n}","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:17:53.455007Z","iopub.execute_input":"2021-09-08T05:17:53.455385Z","iopub.status.idle":"2021-09-08T05:17:53.485576Z","shell.execute_reply.started":"2021-09-08T05:17:53.455341Z","shell.execute_reply":"2021-09-08T05:17:53.484862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\n\nimport transformers\nfrom transformers import AutoTokenizer, TFXLMRobertaForQuestionAnswering, TFXLMRobertaModel\n\nprint(tf.__version__)\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:17:53.4867Z","iopub.execute_input":"2021-09-08T05:17:53.487096Z","iopub.status.idle":"2021-09-08T05:18:00.760782Z","shell.execute_reply.started":"2021-09-08T05:17:53.487067Z","shell.execute_reply":"2021-09-08T05:18:00.760081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = hparams['SEED']\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:00.761997Z","iopub.execute_input":"2021-09-08T05:18:00.762258Z","iopub.status.idle":"2021-09-08T05:18:00.766663Z","shell.execute_reply.started":"2021-09-08T05:18:00.762233Z","shell.execute_reply":"2021-09-08T05:18:00.765681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = hparams['MODEL_2']\nbatch_size = hparams['BATCH_SIZE']","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:00.767773Z","iopub.execute_input":"2021-09-08T05:18:00.768134Z","iopub.status.idle":"2021-09-08T05:18:00.778395Z","shell.execute_reply.started":"2021-09-08T05:18:00.768107Z","shell.execute_reply":"2021-09-08T05:18:00.777507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_special_tokens = True)\nprint(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:00.780828Z","iopub.execute_input":"2021-09-08T05:18:00.7811Z","iopub.status.idle":"2021-09-08T05:18:02.916753Z","shell.execute_reply.started":"2021-09-08T05:18:00.781075Z","shell.execute_reply":"2021-09-08T05:18:02.915779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['context_num_tokens'] = train['context'].apply(lambda x: len(tokenizer(x)['input_ids']))\ntrain['context_num_tokens'].max()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:02.917998Z","iopub.execute_input":"2021-09-08T05:18:02.918287Z","iopub.status.idle":"2021-09-08T05:18:43.305172Z","shell.execute_reply.started":"2021-09-08T05:18:02.918259Z","shell.execute_reply":"2021-09-08T05:18:43.304429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['context_num_tokens'].hist()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:43.306307Z","iopub.execute_input":"2021-09-08T05:18:43.30675Z","iopub.status.idle":"2021-09-08T05:18:43.531144Z","shell.execute_reply.started":"2021-09-08T05:18:43.306721Z","shell.execute_reply":"2021-09-08T05:18:43.530235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The context length is too long, it has to be split into pieces before processing\n- Usually in NLP tasks long documents are truncated but in QA tasks truncating 'context' would lead to loss of answer\n- To avoid this, the long context is split into many input features each of length less than the max_length parameter\n- And if answer is at the split, we use overlapping of split features which is controlled by the parameter doc_stride","metadata":{}},{"cell_type":"code","source":"train = train.sample(frac = 1, random_state = 2021).reset_index(drop = True)\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:43.532435Z","iopub.execute_input":"2021-09-08T05:18:43.532704Z","iopub.status.idle":"2021-09-08T05:18:43.555523Z","shell.execute_reply.started":"2021-09-08T05:18:43.532679Z","shell.execute_reply":"2021-09-08T05:18:43.554487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split data to folds\nn_folds = hparams['N_FOLDS']\ntrain['kfold'] = -1\n\nskf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = SEED)\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X = train, y = train['language'].values)):\n    train.loc[val_idx, 'kfold'] = fold\ntrain.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:43.556846Z","iopub.execute_input":"2021-09-08T05:18:43.557118Z","iopub.status.idle":"2021-09-08T05:18:43.579926Z","shell.execute_reply.started":"2021-09-08T05:18:43.557093Z","shell.execute_reply":"2021-09-08T05:18:43.578864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set TPU","metadata":{}},{"cell_type":"code","source":"DEVICE = 'TPU'\n\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\n\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-08T05:18:43.583131Z","iopub.execute_input":"2021-09-08T05:18:43.583407Z","iopub.status.idle":"2021-09-08T05:18:48.711128Z","shell.execute_reply.started":"2021-09-08T05:18:43.58338Z","shell.execute_reply":"2021-09-08T05:18:48.710166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = hparams['MAX_LENGTH'] #The maximum length of a feature (question and context)\ndoc_stride = hparams['DOC_STRIDE'] #The authorized overlap between two part of the context when splitting it if needed.\n\npad_on_right = tokenizer.padding_side == \"right\"","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:48.712288Z","iopub.execute_input":"2021-09-08T05:18:48.712571Z","iopub.status.idle":"2021-09-08T05:18:48.717586Z","shell.execute_reply.started":"2021-09-08T05:18:48.712543Z","shell.execute_reply":"2021-09-08T05:18:48.716589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_training(examples):\n    examples['question'] = [q.lstrip().rstrip('?') for q in examples['question']] #remove leading white space\n    \n    #Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    #in one example possible giving several features when a context is long, each of those features having a\n    #context that overlaps a bit the context of the previous feature.\n    \n    tokenized_examples = tokenizer(\n                list(examples['question' if pad_on_right else 'context'].values),\n                list(examples['context' if pad_on_right else 'question'].values),\n                truncation = 'only_second' if pad_on_right else 'only_first',\n                max_length = max_length,\n                stride = doc_stride,\n                return_overflowing_tokens = True,\n                return_offsets_mapping = True,\n                padding = 'max_length'\n            )\n    #Since one example might give us several features if it has a long context, we need a map from a feature to\n    #its corresponding example. This key gives us just that.\n    \n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    \n    #The offset mappings will give us a map from token to character position in the original context. This will\n    #help us compute the start_positions and end_positions.\n    \n    offset_mapping = tokenized_examples.pop('offset_mapping')\n    \n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        \n        sequence_ids = tokenized_examples.sequence_ids(i)\n        \n        #One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples.loc[sample_index, 'answer_text']\n        start_char = examples.loc[sample_index, 'answer_start']\n        \n        # If no answers are given, set the cls_index as answer.\n        if start_char is None:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            # Start/end character idx of the answer in the text.\n            end_char = start_char + len(answers)\n            \n             #Start token idx of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n            #Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                #Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                #Note: we could go after the last offset if the answer is the last word (edge case).\n                \n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                \n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:48.718879Z","iopub.execute_input":"2021-09-08T05:18:48.71923Z","iopub.status.idle":"2021-09-08T05:18:48.73489Z","shell.execute_reply.started":"2021-09-08T05:18:48.719201Z","shell.execute_reply":"2021-09-08T05:18:48.733864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_validation(examples):\n    examples['question'] = [q.lstrip() for q in examples['question']]\n    examples['question'] = [q.rstrip('?') for q in examples['question']]\n    \n    tokenized_examples = tokenizer(\n                list(examples['question' if pad_on_right else 'context'].values),\n                list(examples['context' if pad_on_right else 'question'].values),\n                truncation = 'only_second' if pad_on_right else 'only_first',\n                max_length = max_length,\n                stride = doc_stride,\n                return_overflowing_tokens = True,\n                return_offsets_mapping = True,\n                padding = 'max_length'\n            )\n    \n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    \n    #id column from the dataset\n    tokenized_examples['example_id'] = []\n\n    for i in range(len(tokenized_examples['input_ids'])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples.loc[sample_index, 'id'])\n        tokenized_examples['offset_mapping'][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples['offset_mapping'][i])\n        ]\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:48.736276Z","iopub.execute_input":"2021-09-08T05:18:48.736657Z","iopub.status.idle":"2021-09-08T05:18:48.749205Z","shell.execute_reply.started":"2021-09-08T05:18:48.736618Z","shell.execute_reply":"2021-09-08T05:18:48.748253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_tf_dataset(df, batch_size = 4, flag = 'train'):\n    \n    if flag == 'train':\n        features = prepare_training(df)\n    else:\n        features = prepare_validation(df)\n    \n    input_ids = features['input_ids']\n    attn_masks = features['attention_mask']\n    \n    if flag == 'train':\n        start_positions = features['start_positions']\n        end_positions = features['end_positions']\n        train_dataset = tf.data.Dataset.from_tensor_slices((input_ids, attn_masks, start_positions, end_positions))\n        train_dataset = train_dataset.map(lambda x1, x2, y1, y2: ({'input_ids': x1, 'attention_mask': x2}, {'start_positions': y1, 'end_positions': y2}))\n        train_dataset = train_dataset.batch(batch_size)\n        train_dataset = train_dataset.shuffle(1000)\n        train_dataset = train_dataset.prefetch(AUTO)\n        \n        return train_dataset, features\n    \n    elif flag == 'valid':\n        dataset = tf.data.Dataset.from_tensor_slices((input_ids, attn_masks))\n        dataset = dataset.map(lambda x1, x2: ({'input_ids': x1, 'attention_mask': x2}))\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(buffer_size = AUTO)\n        \n        return dataset, features","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:48.750435Z","iopub.execute_input":"2021-09-08T05:18:48.750755Z","iopub.status.idle":"2021-09-08T05:18:48.764921Z","shell.execute_reply.started":"2021-09-08T05:18:48.750716Z","shell.execute_reply":"2021-09-08T05:18:48.764027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF XLM RoBerta Model","metadata":{}},{"cell_type":"code","source":"def build_model():\n    roberta = TFXLMRobertaModel.from_pretrained(model_checkpoint)\n    \n    input_ids = tf.keras.layers.Input(shape = (max_length, ), name = 'input_ids', dtype = tf.int32)\n    attention_mask = tf.keras.layers.Input(shape = (max_length, ), name = 'attention_mask', dtype = tf.int32)\n    \n    embeddings = roberta(input_ids = input_ids, attention_mask = attention_mask)[0]\n    \n    x1 = tf.keras.layers.Dropout(0.1)(embeddings) \n    x1 = tf.keras.layers.Dense(1, use_bias = False)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax', name = 'start_positions')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(embeddings) \n    x2 = tf.keras.layers.Dense(1, use_bias = False)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax', name = 'end_positions')(x2)\n\n    model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = [x1, x2])\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n    \n    model.compile(loss = [loss, loss], optimizer = optimizer)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:48.766112Z","iopub.execute_input":"2021-09-08T05:18:48.766499Z","iopub.status.idle":"2021-09-08T05:18:48.779971Z","shell.execute_reply.started":"2021-09-08T05:18:48.766457Z","shell.execute_reply":"2021-09-08T05:18:48.779013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import logging\n\nlogging.set_verbosity(40)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:48.783418Z","iopub.execute_input":"2021-09-08T05:18:48.783711Z","iopub.status.idle":"2021-09-08T05:18:48.795222Z","shell.execute_reply.started":"2021-09-08T05:18:48.783686Z","shell.execute_reply":"2021-09-08T05:18:48.794378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K.clear_session()\n# with strategy.scope():\n#     model = build_model()\n    \n# print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:48.796551Z","iopub.execute_input":"2021-09-08T05:18:48.796903Z","iopub.status.idle":"2021-09-08T05:18:48.805216Z","shell.execute_reply.started":"2021-09-08T05:18:48.796876Z","shell.execute_reply":"2021-09-08T05:18:48.804327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:48.806145Z","iopub.execute_input":"2021-09-08T05:18:48.806421Z","iopub.status.idle":"2021-09-08T05:18:48.815772Z","shell.execute_reply.started":"2021-09-08T05:18:48.806396Z","shell.execute_reply":"2021-09-08T05:18:48.815081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post Process Predictions","metadata":{}},{"cell_type":"code","source":"def post_process_predictions(examples, features, start, end, n_best_size = 20, max_answer_length = 30):\n    \n    all_start_logits, all_end_logits = start, end\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    \n    for i, feature in enumerate(features['example_id']):\n        features_per_example[example_id_to_index[feature]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features['input_ids'])} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in examples.iterrows():\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n        min_null_score = None # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        context = example['context']\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features['offset_mapping'][feature_index]\n\n            # Update minimum null prediction.\n            cls_index = features['input_ids'][feature_index].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key = lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        #if not squad_v2:\n        #    predictions[example[\"id\"]] = best_answer[\"text\"]\n        #else:\n        answer = best_answer[\"text\"] \n        predictions[example['id']] = answer\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:18:48.817086Z","iopub.execute_input":"2021-09-08T05:18:48.817364Z","iopub.status.idle":"2021-09-08T05:18:48.833838Z","shell.execute_reply.started":"2021-09-08T05:18:48.817339Z","shell.execute_reply":"2021-09-08T05:18:48.83292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tune Model","metadata":{}},{"cell_type":"code","source":"t1= time()\nprint(f\" time starting {time()-t1}\")\nstart_probs, end_probs = [], []\njaccard_scores=[]\nfold =1\nepochs = hparams['EPOCHS']\ntrain_df = train[train['kfold'] != fold]\nvalid_df = train[train['kfold'] == fold]\n#concat external_df to train_df for training, no change in valid_df\ntrain_df = pd.concat([train_df.iloc[:, 1:-1], external_df])\ntrain_df = train_df.reset_index(drop = True)\nvalid_df = valid_df.reset_index(drop = True) \ntrain_dataset, train_enc = build_tf_dataset(train_df, batch_size = batch_size, flag = 'train')\nvalid_dataset, valid_enc = build_tf_dataset(valid_df, batch_size = batch_size, flag = 'valid')\nprint(f\" built all datasets {time()-t1}\")\n\nK.clear_session()\nwith strategy.scope():\n    model = build_model()\n    \nprint(f\"built model {time()-t1} \")\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(f'qa_model_{fold + 1}.h5', verbose = 1, monitor = 'loss', mode = 'min', save_best_only = True, \n                                                save_weights_only = True)\n\nhistory = model.fit(train_dataset, \n                        epochs = 4, \n                        batch_size = batch_size,\n                        callbacks = [checkpoint],\n                        verbose = 1\n                        )\nprint('Predicting valid dataset...')\nstart_pred, end_pred = model.predict(valid_dataset, batch_size = batch_size, verbose = 1)\nprint('Post-process predictions...')\nvalid_preds = post_process_predictions(valid_df, valid_enc, start_pred, end_pred)\n    \nscore = []\nfor idx in range(len(valid_df)):\n    str1 = valid_df['answer_text'].values[idx]\n    str2 = valid_preds[valid_df.loc[idx, 'id']]\n    score.append(jaccard(str1, str2))\nprint(f'Jaccard Score for fold {fold + 1}: {np.mean(score)}')\njaccard_scores.append(np.mean(score))\n    \nstart_probs.append(start_pred)\nend_probs.append(end_pred)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-08T05:27:36.640677Z","iopub.execute_input":"2021-09-08T05:27:36.641261Z","iopub.status.idle":"2021-09-08T05:45:54.78042Z","shell.execute_reply.started":"2021-09-08T05:27:36.641229Z","shell.execute_reply":"2021-09-08T05:45:54.779577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# strart_probs, end_probs = [], []\n# epochs = hparams['EPOCHS']\n# jaccard_scores = []\n\n\n# for i, fold in enumerate(range(n_folds)):\n#     print('=======' * 15)\n#     print(f\"Fold: {fold + 1}\")\n#     print('=======' * 15)\n#     train_df = train[train['kfold'] != fold]\n#     valid_df = train[train['kfold'] == fold]\n    \n#     #concat external_df to train_df for training, no change in valid_df\n#     train_df = pd.concat([train_df.iloc[:, 1:-1], external_df])\n\n#     train_df = train_df.reset_index(drop = True)\n#     valid_df = valid_df.reset_index(drop = True)    \n#     train_dataset, train_enc = build_tf_dataset(train_df, batch_size = batch_size, flag = 'train')\n#     valid_dataset, valid_enc = build_tf_dataset(valid_df, batch_size = batch_size, flag = 'valid')\n    \n    \n#     K.clear_session()\n#     with strategy.scope():\n#         model = build_model()\n#     if i == 0:\n#         print(model.summary())\n    \n#     checkpoint = tf.keras.callbacks.ModelCheckpoint(f'qa_model_{fold + 1}.h5', verbose = 1, monitor = 'loss', mode = 'min', save_best_only = True, \n#                                                 save_weights_only = True)\n\n#     history = model.fit(train_dataset, \n#                         epochs = 3, \n#                         batch_size = batch_size,\n#                         callbacks = [checkpoint],\n#                         verbose = 1\n#                         )\n#     print('Predicting valid dataset...')\n#     start_pred, end_pred = model.predict(valid_dataset, batch_size = batch_size, verbose = 1)\n#     print('Post-process predictions...')\n#     valid_preds = post_process_predictions(valid_df, valid_enc, start_pred, end_pred)\n    \n#     score = []\n#     for idx in range(len(valid_df)):\n#         str1 = valid_df['answer_text'].values[idx]\n#         str2 = valid_preds[valid_df.loc[idx, 'id']]\n#         score.append(jaccard(str1, str2))\n#     print(f'Jaccard Score for fold {fold + 1}: {np.mean(score)}')\n#     jaccard_scores.append(np.mean(score))\n    \n#     strart_probs.append(start_pred)\n#     end_probs.append(end_pred)\n    \n#     del train_dataset, valid_dataset, model\n#     gc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-06T02:58:00.056116Z","iopub.execute_input":"2021-09-06T02:58:00.056868Z","iopub.status.idle":"2021-09-06T02:58:00.063788Z","shell.execute_reply.started":"2021-09-06T02:58:00.056813Z","shell.execute_reply":"2021-09-06T02:58:00.062606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save the hyperparameters and the validation scores for prediction notebook","metadata":{}},{"cell_type":"code","source":"hparams['JAC_SCORES'] = jaccard_scores\n\nwith open(r'hparams.yaml', 'w') as f:\n    yaml.dump(hparams, f)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T02:58:00.065276Z","iopub.execute_input":"2021-09-06T02:58:00.065617Z","iopub.status.idle":"2021-09-06T02:58:00.083135Z","shell.execute_reply.started":"2021-09-06T02:58:00.065585Z","shell.execute_reply":"2021-09-06T02:58:00.081866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on Test Data","metadata":{}},{"cell_type":"code","source":"test_dataset, test_enc = build_tf_dataset(test, batch_size = batch_size, flag = 'valid')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T02:58:00.084538Z","iopub.execute_input":"2021-09-06T02:58:00.084927Z","iopub.status.idle":"2021-09-06T02:58:00.528383Z","shell.execute_reply.started":"2021-09-06T02:58:00.084889Z","shell.execute_reply":"2021-09-06T02:58:00.527576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_probs, end_probs = [], []\nstart_pred, end_pred = model.predict(test_dataset, batch_size = batch_size, verbose = 1)\nprint(start_pred.shape, end_pred.shape)\nstart_probs.append(start_pred)\nend_probs.append(end_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T02:58:00.5294Z","iopub.execute_input":"2021-09-06T02:58:00.529799Z","iopub.status.idle":"2021-09-06T02:58:11.742203Z","shell.execute_reply.started":"2021-09-06T02:58:00.529771Z","shell.execute_reply":"2021-09-06T02:58:11.74114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start_probs, end_probs = [], []\n# for fold in range(n_folds):\n#     with strategy.scope():\n#         model = build_model()\n#     print('Loading trained model weights...')\n#     model.load_weights(f'qa_model_{fold + 1}.h5')\n#     print(f'Predicting testset - Fold: {fold + 1}...')\n#     start_pred, end_pred = model.predict(test_dataset, batch_size = batch_size, verbose = 1)\n#     print(start_pred.shape, end_pred.shape)\n#     start_probs.append(start_pred)\n#     end_probs.append(end_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T02:58:11.743628Z","iopub.execute_input":"2021-09-06T02:58:11.743944Z","iopub.status.idle":"2021-09-06T02:58:11.748274Z","shell.execute_reply.started":"2021-09-06T02:58:11.743909Z","shell.execute_reply":"2021-09-06T02:58:11.747163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_start_probs, test_end_probs = np.mean(start_probs, axis = 0), np.mean(end_probs, axis = 0)\npredictions = post_process_predictions(test, test_enc, test_start_probs, test_end_probs)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T02:58:11.749607Z","iopub.execute_input":"2021-09-06T02:58:11.749919Z","iopub.status.idle":"2021-09-06T02:58:11.782959Z","shell.execute_reply.started":"2021-09-06T02:58:11.749889Z","shell.execute_reply":"2021-09-06T02:58:11.781631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.DataFrame({'id': list(predictions.keys()), 'PredictionString': list(predictions.values())})\nsub_df.to_csv('./submission.csv', index = False)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T02:58:11.784429Z","iopub.execute_input":"2021-09-06T02:58:11.78484Z","iopub.status.idle":"2021-09-06T02:58:11.803578Z","shell.execute_reply.started":"2021-09-06T02:58:11.784807Z","shell.execute_reply":"2021-09-06T02:58:11.802802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks to Kaggle and fellow Kagglers for the all the learnings, nothing beats doing and learning!!!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}