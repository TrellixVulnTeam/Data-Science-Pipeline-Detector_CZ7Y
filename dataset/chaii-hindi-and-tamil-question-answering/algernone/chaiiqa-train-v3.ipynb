{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.magic import register_cell_magic\nimport os\nfrom pathlib import Path\nimport pickle\n\n## define custom magic to save most useful classes and use them in inference notebook \n## instead of copying the code every time you have changes in the classes\n@register_cell_magic\ndef write_and_run(line, cell):\n    argz = line.split()\n    file = argz[-1]\n    mode = 'w'\n    if len(argz) == 2 and argz[0] == '-a':\n        mode = 'a'\n    with open(file, mode) as f:\n        f.write(cell)\n    get_ipython().run_cell(cell)\n    \nPath('/kaggle/working/scripts').mkdir(exist_ok=True)\nmodels_dir = Path('/kaggle/working/models')\nmodels_dir.mkdir(exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T13:51:19.923673Z","iopub.execute_input":"2021-12-13T13:51:19.924226Z","iopub.status.idle":"2021-12-13T13:51:19.937192Z","shell.execute_reply.started":"2021-12-13T13:51:19.924113Z","shell.execute_reply":"2021-12-13T13:51:19.936463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nimport matplotlib.pyplot as plt\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\nimport pytorch_lightning as pl\nimport collections\nfrom time import time\nfrom tqdm import tqdm\ntqdm.pandas()\n\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING)\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{"id":"E4l6PirHET3x","outputId":"eeaea823-bdbc-4bef-a518-e51736877d6e","execution":{"iopub.status.busy":"2021-12-13T13:51:19.938601Z","iopub.execute_input":"2021-12-13T13:51:19.938886Z","iopub.status.idle":"2021-12-13T13:51:23.40471Z","shell.execute_reply.started":"2021-12-13T13:51:19.938847Z","shell.execute_reply":"2021-12-13T13:51:23.403874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('../input/chaiiqa-sampling/xlm-roberta-base-squad2_features_kfold.pkl', 'rb') as f:\n    feats_data = pickle.load(f)\nfeatures_df = pd.DataFrame(data=feats_data[1], columns=feats_data[0])\nfeatures_df['index'] = features_df['index'].apply(str)\n\nraw_kfold = pd.read_csv('../input/chaiiqa-sampling/kfold_raw.csv', converters={'index': str, 'answers': exec})","metadata":{"execution":{"iopub.status.busy":"2021-12-13T13:51:23.406395Z","iopub.execute_input":"2021-12-13T13:51:23.406697Z","iopub.status.idle":"2021-12-13T13:51:28.227064Z","shell.execute_reply.started":"2021-12-13T13:51:23.406656Z","shell.execute_reply":"2021-12-13T13:51:28.226327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/config.py\n\nclass Config:\n    # model\n    seed = 42\n    model_type = \"deepset/xlm-roberta-base-squad2\"\n#     gradient_accumulation_steps = 2\n\n    max_seq_length = 384\n    doc_stride = 128\n\n    # train\n    epochs = 5\n    train_batch_size = 8\n    eval_batch_size = 8\n\n    # optimizer\n    lr = 0.5e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    warmup_ratio = 0.1\n\n    \npl.utilities.seed.seed_everything(Config.seed, workers=True)\n","metadata":{"id":"LUx5XplNET3y","execution":{"iopub.status.busy":"2021-12-13T13:51:28.229401Z","iopub.execute_input":"2021-12-13T13:51:28.229601Z","iopub.status.idle":"2021-12-13T13:51:28.242638Z","shell.execute_reply.started":"2021-12-13T13:51:28.229576Z","shell.execute_reply":"2021-12-13T13:51:28.241645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset ","metadata":{}},{"cell_type":"code","source":"%%write_and_run scripts/dataset.py\n\n\nclass QADataset(Dataset):\n    def __init__(self, features, mode='train'):\n        super().__init__()\n        self.features = features\n        self.mode = mode\n        self.size = len(self.features)\n        \n    def __len__(self):\n        return self.size\n    \n    def __getitem__(self, item):   \n        feature = self.features.iloc[item]\n        if self.mode == 'train':\n            return {\n                'index':feature['index'],\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'index':feature['index'],\n            }","metadata":{"id":"6TuzHdjmET30","execution":{"iopub.status.busy":"2021-12-13T13:51:28.244672Z","iopub.execute_input":"2021-12-13T13:51:28.245046Z","iopub.status.idle":"2021-12-13T13:51:28.253543Z","shell.execute_reply.started":"2021-12-13T13:51:28.245009Z","shell.execute_reply":"2021-12-13T13:51:28.252769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"%%write_and_run scripts/postprocess.py\n\n\ndef postprocess_qa_predictions(samples, all_features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    CONTEXT_INDEX = 1\n    k = 0\n    \n    predictions = collections.OrderedDict()\n    for _, sample in samples.iterrows():\n     \n        min_null_score = None\n        valid_answers = []\n        \n        context = sample[\"context\"]\n        features = all_features[all_features['index']==sample['index']]\n        for feat_i, (_, feature) in enumerate(features.iterrows()):\n\n            start_logits = raw_predictions[sample['index']][feat_i][0]\n            end_logits = raw_predictions[sample['index']][feat_i][1]\n\n            sequence_ids = feature[\"sequence_ids\"]\n\n            offset_mapping = [\n                (o if sequence_ids[k] == CONTEXT_INDEX else None)\n                for k, o in enumerate(feature[\"offset_mapping\"])\n            ]\n\n            cls_index = feature[\"input_ids\"].index(tokenizer.cls_token_id)\n            \n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indices = np.argsort(-start_logits)[:n_best_size].tolist()\n            end_indices = np.argsort(-end_logits)[:n_best_size].tolist()\n\n            for start_index in start_indices:\n                for end_index in end_indices:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                        or end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n\n                    valid_answers.append(\n                        {\n                            'score': start_logits[start_index] + end_logits[end_index],\n                            'text': context[start_char: end_char].strip(),\n                            'token_positions': (start_index, end_index),\n                            'feature_n': feat_i\n                        }\n                    )\n                    \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x['score'], reverse=True)[0]\n        else:\n            best_answer = {'text': '', 'score': 0.0}\n        \n        predictions[sample['index']] = best_answer\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-12-13T13:51:28.255117Z","iopub.execute_input":"2021-12-13T13:51:28.255659Z","iopub.status.idle":"2021-12-13T13:51:28.273428Z","shell.execute_reply.started":"2021-12-13T13:51:28.255604Z","shell.execute_reply":"2021-12-13T13:51:28.272593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/model.py\n\nclass PTModel(nn.Module):\n    def __init__(self, net, config):\n        super().__init__()\n        self.net = net\n        self.config = config\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.qa_outputs.apply(self._init_weights)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.xavier_normal_(module.weight)\n            module.bias.data.zero_()\n                \n                \n    def forward(self, input_ids, attention_mask):\n        outputs = self.net(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n#         print(sequence_output.shape)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        return start_logits, end_logits \n    \nclass Model(pl.LightningModule):\n    def __init__(self, ptmodel, loss_fn=None, params=None):\n        super().__init__()\n        self.ptmodel = ptmodel\n        self.loss_fn = loss_fn\n        self.params = params \n        self.best_score = float('-inf')\n                \n    def forward(self, input_ids, attention_mask):\n        return self.ptmodel(input_ids, attention_mask) \n    \n    def training_step(self, batch, batch_idx):\n        input_ids, attention_mask, start_position, end_position = batch['input_ids'], batch['attention_mask'], batch['start_position'], batch['end_position']\n        y_hat_start, y_hat_end = self(input_ids, attention_mask)\n        loss = self.loss_fn((y_hat_start, y_hat_end), (start_position, end_position))\n        return {\n            'loss': loss,\n        }\n  \n    def validation_step(self, batch, batch_idx):\n        input_ids, attention_mask, start_position, end_position = batch['input_ids'], batch['attention_mask'], batch['start_position'], batch['end_position']\n        y_hat_start, y_hat_end = self(input_ids, attention_mask)\n        loss = self.loss_fn((y_hat_start, y_hat_end), (start_position, end_position))\n        \n        \n        return {\n            'loss': loss,\n            'batch_index': batch['index'],\n            'y_hat_start': y_hat_start.cpu().numpy(),\n            'y_hat_end': y_hat_end.cpu().numpy(),\n        }\n    \n    def predict_step(self, batch, batch_idx, dataloader_idx):\n        input_ids, attention_mask = batch['input_ids'], batch['attention_mask']\n        y_hat_start, y_hat_end = self(input_ids, attention_mask)\n        return batch['index'], y_hat_start.cpu().numpy(), y_hat_end.cpu().numpy()\n  \n    def validation_epoch_end(self, validation_step_outputs):\n        example_id_to_preds = collections.defaultdict(list)\n        losses = []\n        for out in validation_step_outputs:\n#             print(out.keys())\n#             print(out['batch_index'])\n            losses.append(out['loss'].item())\n            \n            batch_indices, batch_start_preds, batch_end_preds = out['batch_index'], out['y_hat_start'], out['y_hat_end']\n            for index, feat_start_preds, feat_end_preds in zip(batch_indices, batch_start_preds, batch_end_preds):\n                example_id_to_preds[index].append((feat_start_preds, feat_end_preds))\n            \n\n        ### calc val_loss and save if val_loss is less than min_loss\n        val_loss = sum(losses) / len(losses)\n        print('Validation loss:', val_loss)\n\n            \n        #### postprocessing\n        \n        final_preds = postprocess_qa_predictions(self.params['val_raw_df'], self.params['val_feats_df'], example_id_to_preds, n_best_size=20, max_answer_length=30)\n        \n        jaccard_scores = []\n        for _, sample in self.params['val_raw_df'].iterrows():\n            y = sample['answer_text']\n            index = sample['index']\n            y_hat = final_preds[index]['text']\n            jaccard_score = jaccard(y, y_hat)\n            jaccard_scores.append(jaccard_score)\n#             print(y, y_hat, '->', jaccard_score)\n            \n        avg_jaccard_score = sum(jaccard_scores) / len(jaccard_scores)\n        print('Validation Jaccard score:', round(avg_jaccard_score, 3))\n        \n        if avg_jaccard_score > self.best_score:\n            if self.params.get('save_best'):\n                if self.best_score != float('-inf'):\n                    (self.params['models_dir'] / f\"model_fold_{self.params['fold']}_score_{self.best_score:.3f}.pth\").unlink()\n                torch.save(ptmodel, self.params['models_dir'] / f\"model_fold_{self.params['fold']}_score_{avg_jaccard_score:.3f}.pth\")\n            self.best_score = avg_jaccard_score\n\n\n\n    def configure_optimizers(self):\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": self.params['weight_decay'],\n            },\n            {\n                \"params\": [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        \n        \n#         no_decay = [\"bias\", \"LayerNorm.weight\"]\n#         group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n#         group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n#         group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n#         group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n#         optimizer_grouped_parameters = [\n#             {'params': [p for n, p in model.ptmodel.net.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': self.params['weight_decay']},\n#             {'params': [p for n, p in model.ptmodel.net.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': self.params['weight_decay'], 'lr': self.params['lr']/2.6},\n#             {'params': [p for n, p in model.ptmodel.net.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': self.params['weight_decay'], 'lr': self.params['lr']},\n#             {'params': [p for n, p in model.ptmodel.net.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': self.params['weight_decay'], 'lr': self.params['lr']*2.6},\n\n#             {'params': [p for n, p in model.ptmodel.net.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n#             {'params': [p for n, p in model.ptmodel.net.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': self.params['lr']/2.6},\n#             {'params': [p for n, p in model.ptmodel.net.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': self.params['lr']},\n#             {'params': [p for n, p in model.ptmodel.net.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': self.params['lr']*2.6},\n#             {'params': [p for n, p in model.named_parameters() if 'net' not in n], 'lr': self.params['lr']*5, \"weight_decay\": 0.0},\n#         ]\n\n\n        optimizer = torch.optim.AdamW(\n            optimizer_grouped_parameters,\n            lr=self.params['lr'],\n            eps=self.params['epsilon'],\n#             correct_bias=True\n        )\n        \n#         scheduler = get_cosine_schedule_with_warmup(\n#             optimizer,\n#             num_warmup_steps=self.params['num_warmup_steps'],\n#             num_training_steps=self.params['num_training_steps']\n#         )    \n\n        return optimizer\n\n    def on_train_epoch_start(self):\n        self.epoch_start = time()\n        \n    def on_train_epoch_end(self):\n        print('Epoch took:', time() - self.epoch_start, 'secs')\n\n    def on_validation_epoch_start(self):\n        self.val_epoch_start = time()\n        \n    def on_validation_epoch_end(self):\n        print('Validation epoch took:', time() - self.val_epoch_start, 'secs')\n","metadata":{"id":"9OxhKqxcET31","execution":{"iopub.status.busy":"2021-12-13T13:51:28.27484Z","iopub.execute_input":"2021-12-13T13:51:28.275494Z","iopub.status.idle":"2021-12-13T13:51:28.305756Z","shell.execute_reply.started":"2021-12-13T13:51:28.275454Z","shell.execute_reply":"2021-12-13T13:51:28.305027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss","metadata":{}},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\n\n\ndef loss_fn(preds, labels):\n    start_preds, end_preds = preds\n    start_labels, end_labels = labels\n    \n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n    total_loss = (start_loss + end_loss) / 2\n    return total_loss\n\ndef get_scheduler_steps(dl_len):\n    num_training_steps = dl_len * Config.epochs\n    if Config.warmup_ratio > 0:\n        num_warmup_steps = int(Config.warmup_ratio * num_training_steps)\n    else:\n        num_warmup_steps = 0\n    \n    return num_training_steps, num_warmup_steps\n\n","metadata":{"id":"SxuNrJqqET32","execution":{"iopub.status.busy":"2021-12-13T13:51:28.307191Z","iopub.execute_input":"2021-12-13T13:51:28.307734Z","iopub.status.idle":"2021-12-13T13:51:28.318943Z","shell.execute_reply.started":"2021-12-13T13:51:28.307677Z","shell.execute_reply":"2021-12-13T13:51:28.318224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run","metadata":{}},{"cell_type":"code","source":"for fold in range(1):\n    print(f'Fold#{fold+1}')\n    train_ds = QADataset(features_df[features_df.kfold!=fold])\n    val_feats_fold_df = features_df[features_df.kfold==fold]\n    val_ds = QADataset(val_feats_fold_df)\n    val_raw_fold_df = raw_kfold[raw_kfold.kfold==fold]\n    \n    train_dl = DataLoader(\n        train_ds,\n        batch_size=Config.train_batch_size,\n        shuffle=True,\n        num_workers=8,\n        pin_memory=True,\n        drop_last=False \n    )\n\n    val_dl = DataLoader(\n        val_ds,\n        batch_size=Config.eval_batch_size, \n        shuffle=False,\n        num_workers=8,\n        pin_memory=True, \n        drop_last=False\n    )\n\n    config = AutoConfig.from_pretrained(Config.model_type)\n    \n    tokenizer = AutoTokenizer.from_pretrained(Config.model_type)\n    torch.save(tokenizer, 'tokenizer.pth')\n    net = AutoModel.from_pretrained(Config.model_type, config)\n    num_training_steps, num_warmup_steps = get_scheduler_steps(len(train_dl))\n    \n    ptmodel = PTModel(net, config)\n    model = Model(\n        ptmodel,\n        loss_fn=loss_fn,\n        params = {\n            'num_training_steps': num_training_steps,\n            'num_warmup_steps': num_warmup_steps,\n            'lr': Config.lr,\n            'epsilon': Config.epsilon,\n            'weight_decay': Config.weight_decay,\n            'models_dir': models_dir,\n            'fold': fold,\n            'save_best': True,\n            'val_raw_df': val_raw_fold_df,\n            'val_feats_df': val_feats_fold_df\n        }\n    )  \n    \n    \n    trainer = pl.Trainer(\n        fast_dev_run=False, max_epochs=Config.epochs,\n        checkpoint_callback=False,\n         gpus=1,precision=16,\n#          auto_lr_find=True,  \n        limit_train_batches=1.0, limit_val_batches=1.0, \n         num_sanity_val_steps=0, val_check_interval=0.33,\n\n     )\n    \n    \n    trainer.fit(model, train_dl, val_dl)\n\n    torch.cuda.empty_cache()\n    del trainer, net\n    del model, config\n    del train_dl, val_dl, train_ds, val_ds\n    gc.collect()\n","metadata":{"id":"39ei5Bm5ET37","execution":{"iopub.status.busy":"2021-12-13T13:51:28.320253Z","iopub.execute_input":"2021-12-13T13:51:28.320529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls models","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}