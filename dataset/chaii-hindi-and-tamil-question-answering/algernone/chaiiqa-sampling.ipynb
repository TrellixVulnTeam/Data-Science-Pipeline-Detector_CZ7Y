{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.magic import register_cell_magic\nimport os\nfrom pathlib import Path\n\n## define custom magic to save most useful classes and use them in inference notebook \n## instead of copying the code every time you have changes in the classes\n@register_cell_magic\ndef write_and_run(line, cell):\n    argz = line.split()\n    file = argz[-1]\n    mode = 'w'\n    if len(argz) == 2 and argz[0] == '-a':\n        mode = 'a'\n    with open(file, mode) as f:\n        f.write(cell)\n    get_ipython().run_cell(cell)\n    \nPath('/kaggle/working/scripts').mkdir(exist_ok=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T13:05:28.371834Z","iopub.execute_input":"2021-12-13T13:05:28.37226Z","iopub.status.idle":"2021-12-13T13:05:28.405303Z","shell.execute_reply.started":"2021-12-13T13:05:28.372165Z","shell.execute_reply":"2021-12-13T13:05:28.40448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\n\nimport matplotlib.pyplot as plt\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AutoConfig,\n    AutoTokenizer,\n\n)\n\nimport pytorch_lightning as pl\nfrom tqdm import tqdm\ntqdm.pandas()\nimport pickle \n\nclass Config:\n    seed = 42\n    model_type = \"deepset/xlm-roberta-base-squad2\"\n    max_seq_length = 384\n    doc_stride = 128\n    \npl.utilities.seed.seed_everything(Config.seed, workers=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-13T13:05:28.406893Z","iopub.execute_input":"2021-12-13T13:05:28.407352Z","iopub.status.idle":"2021-12-13T13:05:39.975922Z","shell.execute_reply.started":"2021-12-13T13:05:28.407321Z","shell.execute_reply":"2021-12-13T13:05:39.97504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=Config.seed)\n    for fold, (_, val_ids) in enumerate(kf.split(X=data, y=data['language'])):\n        data.loc[val_ids, 'kfold'] = fold\n    return data\n\n\nimport re\ndef find_all_substring_positions(string, substring):\n    return [m.start() for m in re.finditer(re.escape(substring), string)]\n\ndef convert_answers(row):\n    return {'answer_start': [row[0]], 'text': [row[1]]}\n\n\n##FINETUNE \ntrain = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nexternal_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\nexternal_train = pd.concat([external_mlqa, external_xquad])\n\ntrain = create_folds(train, num_splits=5)\nexternal_train[\"kfold\"] = -1\ntrain = pd.concat([train, external_train]).reset_index().drop(['id', 'index'], axis=1).reset_index()\ntrain['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T13:05:39.977439Z","iopub.execute_input":"2021-12-13T13:05:39.978164Z","iopub.status.idle":"2021-12-13T13:05:41.577325Z","shell.execute_reply.started":"2021-12-13T13:05:39.978131Z","shell.execute_reply":"2021-12-13T13:05:41.576362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inds_to_remove = []\nfor i, row in train.iterrows():\n    if not isinstance(row['answers']['text'][0], str):\n        inds_to_remove.append(i)\ntrain = train.drop(inds_to_remove)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T13:05:41.579488Z","iopub.execute_input":"2021-12-13T13:05:41.579777Z","iopub.status.idle":"2021-12-13T13:05:41.987614Z","shell.execute_reply.started":"2021-12-13T13:05:41.579735Z","shell.execute_reply":"2021-12-13T13:05:41.986656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/preprocess.py\n\nclass FeatureExtractor:\n    OUT_COLUMNS = ['index', 'input_ids', 'attention_mask', 'offset_mapping', 'sequence_ids']\n    TRAIN_SPECIFIC_COLUMNS = ['start_position', 'end_position', 'kfold']\n   \n    \n    def __init__(self, tokenizer, cls_token, test=False):\n        self.tokenizer = tokenizer\n        self.test = test\n        self.columns = self.OUT_COLUMNS + [self.TRAIN_SPECIFIC_COLUMNS, []][self.test==True]\n#         self.cls_token = '<CLS>'\n        self.cls_token = cls_token\n        self.cls_token_id = self.tokenizer.encode(self.cls_token)[0]\n        \n    def extract_features(self, df):\n        features = []\n        for i in tqdm(range(len(df))):\n            row = df.iloc[i]\n            tokenized_sample = self.tokenizer(\n                row[\"question\"].strip(),\n                row[\"context\"],\n                truncation=\"only_second\",\n                max_length=Config.max_seq_length,\n                stride=Config.doc_stride,\n                return_overflowing_tokens=True,\n                return_offsets_mapping=True,\n                padding=\"max_length\",\n            )\n\n\n            sample_mapping = tokenized_sample.pop(\"overflow_to_sample_mapping\")\n            offset_mapping = tokenized_sample.pop(\"offset_mapping\")\n            \n            answers = row[\"answers\"]\n            if not isinstance(answers[\"text\"][0], str):\n                continue\n\n            for i, offsets in enumerate(offset_mapping):\n                feature = {}\n\n                input_ids = tokenized_sample[\"input_ids\"][i]\n                attention_mask = tokenized_sample[\"attention_mask\"][i]\n                feature['index'] = row['index']\n                feature['input_ids'] = input_ids\n                feature['attention_mask'] = attention_mask\n                feature['offset_mapping'] = offsets\n                feature['sequence_ids'] = [0 if i is None else i for i in tokenized_sample.sequence_ids(i)]\n                if not self.test:\n                    feature['kfold'] = row['kfold']\n                    cls_index = input_ids.index(self.cls_token_id)\n                    sequence_ids = tokenized_sample.sequence_ids(i)\n\n                    sample_index = sample_mapping[i]\n                    answers = row[\"answers\"]\n\n                    if len(answers[\"answer_start\"]) == 0:\n                        feature[\"start_position\"] = cls_index\n                        feature[\"end_position\"] = cls_index\n                    else:\n                        start_char = answers[\"answer_start\"][0]\n\n                        end_char = start_char + len(answers[\"text\"][0])\n\n                        token_start_index = 0\n                        while sequence_ids[token_start_index] != 1:\n                            token_start_index += 1\n\n                        token_end_index = len(input_ids) - 1\n                        while sequence_ids[token_end_index] != 1:\n                            token_end_index -= 1\n\n                        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                            feature[\"start_position\"] = cls_index\n                            feature[\"end_position\"] = cls_index\n                        else:\n                            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                                token_start_index += 1\n                            feature[\"start_position\"] = token_start_index - 1\n                            while offsets[token_end_index][1] >= end_char:\n                                token_end_index -= 1\n                            feature[\"end_position\"] = token_end_index + 1\n\n                features.append(feature)\n\n        \n        features_df = pd.DataFrame(data=features, columns=self.columns)\n        return features_df","metadata":{"execution":{"iopub.status.busy":"2021-12-13T13:05:41.989352Z","iopub.execute_input":"2021-12-13T13:05:41.989666Z","iopub.status.idle":"2021-12-13T13:05:42.009642Z","shell.execute_reply.started":"2021-12-13T13:05:41.989621Z","shell.execute_reply":"2021-12-13T13:05:42.008763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntokenizer = AutoTokenizer.from_pretrained(Config.model_type)\ntorch.save(tokenizer, f\"{Config.model_type.split('/')[-1]}_tokenizer.pt\")\nfeat_extractor = FeatureExtractor(tokenizer, tokenizer.cls_token)\ntrain.to_csv('kfold_raw.csv', index=False)\n\nfeatures_df = feat_extractor.extract_features(train)\ndata_to_save = (features_df.columns, features_df.values)\nwith open(f\"{Config.model_type.split('/')[-1]}_features_kfold.pkl\", 'wb') as f:\n    pickle.dump(data_to_save, f, protocol=4)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T13:05:42.011328Z","iopub.execute_input":"2021-12-13T13:05:42.012298Z","iopub.status.idle":"2021-12-13T13:07:19.746821Z","shell.execute_reply.started":"2021-12-13T13:05:42.012249Z","shell.execute_reply":"2021-12-13T13:07:19.745764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(Config.model_type)\n# tokenizer.add_tokens(['<CLS>'])\n# print(tokenizer.encode('<CLS>'), tokenizer.decode([48044]))\n# print(tokenizer.encode('<CLS> नियम-निष्ठता '))","metadata":{"execution":{"iopub.status.busy":"2021-12-13T13:07:19.748304Z","iopub.execute_input":"2021-12-13T13:07:19.74856Z","iopub.status.idle":"2021-12-13T13:07:19.752722Z","shell.execute_reply.started":"2021-12-13T13:07:19.748529Z","shell.execute_reply":"2021-12-13T13:07:19.75183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pickle\n# import pandas as pd\n# with open('./xlm-roberta-base-squad2_features_kfold.pkl_SHORT', 'rb') as f:\n#     feat_data = pickle.load(f)\n    \n# f_df = pd.DataFrame(data=feat_data[1], columns=feat_data[0])\n# f_df.head(30)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T13:07:19.754091Z","iopub.execute_input":"2021-12-13T13:07:19.754701Z","iopub.status.idle":"2021-12-13T13:07:19.772021Z","shell.execute_reply.started":"2021-12-13T13:07:19.754654Z","shell.execute_reply":"2021-12-13T13:07:19.77078Z"},"trusted":true},"execution_count":null,"outputs":[]}]}