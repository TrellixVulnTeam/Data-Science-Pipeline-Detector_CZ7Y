{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### complete blog link: https://jarvislabs.ai/blogs/chaii","metadata":{}},{"cell_type":"markdown","source":"# Public Baseline Notebooks\n\nI would love to mention some of the amazing notebooks shared by the Kaggle community with which one could start a strong baseline.\n\n### chaii QA - 5 Fold XLMRoberta Torch - By Rohit Singh\n\nThis is a complete package in this competition for anyone looking for a strong baseline to start with. Notebook is based on the Pytorch framework. It includes many latest training strategies. It also includes the usage of external datasets (MLQA, quad).\n\n#### Training Notebook\nhttps://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit\n#### Inference Notebook\nhttps://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer\n### hello friends chaii pi lo - By Abhishek Thakur\n\nAnother amazing notebook was published by none other than Quadrapule Grandmaster Abhishek Thakur. Similar to Rohit's baseline, this one is also based on the Pytorch framework and also uses external datasets.\n\n#### Training Notebook\nhttps://www.kaggle.com/abhishek/hello-friends-tez-se-chaii-train-kar-lo\n#### Inference notebook\nhttps://www.kaggle.com/abhishek/hello-friends-chaii-pi-lo\n\n## EDA Notebooks\n\nI have already briefed a lot about the competition, but if you are willing to know about the competition more, these are some of the great notebooks one could refer to.\n\n### EDA - By Darek K≈Çeczek\nhttps://www.kaggle.com/thedrcat/chaii-eda-baseline\n\n### chaii-QA: Character/Token/Languages EDA üïµÔ∏è - By Nicholas Broad üê¢\n\nhttps://www.kaggle.com/nbroad/chaii-qa-character-token-languages-eda\n### üî•Chaii - EDA and PreprocessingWeights & Biases - By TensorGirl\nhttps://www.kaggle.com/usharengaraju/chaii-eda-and-preprocessing-weights-biases\n\n## Key Suggestions For A Strong Finish\n\n#### Transformers is all you need\n\nMultilingual Transformer models pre-trained on SQUAD data are completely dominating the competition. Some of the models which you could finetune on competition dataset.\n\n- XLM-Roberta-large\n- Muril-Large-Cased\n- bert-base-multilingual-cased-finetuned-squad\n- bert-multi-cased-finedtuned-xquad-tydiqa-goldp\n- bert-multi-cased-finetuned-xquadv1\n- bert-multi-uncased-finetuned-xquadv1\n- mT5-small-finetuned-tydiqa-for-xqa\n- xlm-multi-finetuned-xquadv1\n\n\n\n#### External Dataset\n\nGiven such a small amount of data for training (~1000 samples), having the right external data could be a key thing to improve the learning of models.\n\n#### Post Processing\n\nSeveral post-processing techniques could be used to generate better predictions which include removal of special characters, unnecessarily occurring words in prediction string, having an answer length limit, etc.\n\n#### Noise-free Dataset\n\nThe training dataset provided by the hosts has many samples having targets labeled incorrectly, this made it difficult to train a model without fixing the labels manually or generating additional data.\n\n#### Prevent Overfitting\n\nThere are very few samples in test data as well. So, even a change in a few predictions could vary score a lot. Having the right validation scheme, a better correlation between validation and public leaderboard score is necessary.\n\n#### Seq2Seq approach\n\nCurrently most of the teams are predicting starting and ending indexes of the answers. Using a sequence to sequence approach could be helpful in the competition by using an autoregressive model for generating answers.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}