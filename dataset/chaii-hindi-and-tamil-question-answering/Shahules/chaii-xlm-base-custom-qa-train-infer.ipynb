{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Introduction</p>\n\n![](https://assets.thehansindia.com/h-upload/2020/11/25/1600x960_1014218-indian-languages.jpg)\n\nWith nearly 1.4 billion people, India is the second-most populated country in the world. Yet Indian languages, like Hindi and Tamil, are underrepresented on the web. Popular Natural Language Understanding (NLU) models perform worse with Indian languages compared to English, the effects of which lead to subpar experiences in downstream web applications for Indian users. With more attention from the Kaggle community and your novel machine learning solutions, we can help Indian users make the most of the web.\n\n**In this notebook, I aim to build a pytorch baseline for this competition. I have used to pytorch + custom QA model to train the model. I will update this notebook for better in coming days...**\n\n<p p style = \"font-family: garamond; font-size:20px; font-style: normal;background-color: #f6f5f6; color :#E94421; border-radius: 10px 10px; text-align:center\">your upvotes and comments motivates me to do more..</p>","metadata":{}},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Import everything required</p>","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom torch.optim import Adam,AdamW\nfrom torch.utils.data import SequentialSampler\nfrom torch.utils.data import Dataset,DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import XLMRobertaTokenizer,XLMRobertaModel,AutoTokenizer,XLMRobertaModel,XLMRobertaConfig,AutoModel,AutoConfig\nfrom transformers import logging\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nimport wandb\nimport random\nimport os\nlogging.set_verbosity_error()","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:32.037069Z","iopub.execute_input":"2021-09-05T06:34:32.03757Z","iopub.status.idle":"2021-09-05T06:34:39.388784Z","shell.execute_reply.started":"2021-09-05T06:34:32.037483Z","shell.execute_reply":"2021-09-05T06:34:39.387801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/torchcontrib/contrib-master\")\nimport torchcontrib","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.391256Z","iopub.execute_input":"2021-09-05T06:34:39.391507Z","iopub.status.idle":"2021-09-05T06:34:39.436468Z","shell.execute_reply.started":"2021-09-05T06:34:39.391479Z","shell.execute_reply":"2021-09-05T06:34:39.435748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Configurations</p>\n","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_1 = user_secrets.get_secret(\"WANDB_KEY\")\nos.environ[\"WANDB_API_KEY\"] = secret_value_1\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-05T06:34:39.438189Z","iopub.execute_input":"2021-09-05T06:34:39.438535Z","iopub.status.idle":"2021-09-05T06:34:39.563299Z","shell.execute_reply.started":"2021-09-05T06:34:39.4385Z","shell.execute_reply":"2021-09-05T06:34:39.562523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_type='base'\nconfig = {\n    'base_model':f\"../input/xlm-roberta-squad2/deepset/xlm-roberta-{model_type}-squad2\",\n    'batch_size':16,\n    \"epochs\":3,\n    'folds':7,\n    'device':torch.device('cuda'),\n    'num_reinit_layers':1,\n    'dropout':0.1,\n    'seed':42,\n    'lr':5e-5,\n    'eval_every':1 ##changed later\n}","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.566311Z","iopub.execute_input":"2021-09-05T06:34:39.566556Z","iopub.status.idle":"2021-09-05T06:34:39.574422Z","shell.execute_reply.started":"2021-09-05T06:34:39.566533Z","shell.execute_reply":"2021-09-05T06:34:39.573637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Dataset</p>\n\nSince BERT can only handle maximum token length on 512 and most the context in our dataset exceeds this limit we will process the samples in a way to handle this token length limit of BERT.\n\nTo do that, we will use the `tokenizers` library, for each sample in the dataset we will convert them into more than once example.\n\n```python\ntokenizer(\n           self.df['context'].values.tolist(),\n           self.df['question'].values.tolist(),\n           truncation=\"only_first\",\n           max_length=self.max_len,\n          stride=self.doc_stride,\n          return_overflowing_tokens=True,\n          return_offsets_mapping=True,\n          padding=\"max_length\")\n```\n\nWe use context and question to form each example,the context is `truncated` whenver it exceeds the max token length limit. It also adds a `stride` of 128 tokens, this helps in creating better context for each example.\nThe `overflow_tokens` is used to map these output examples to input dataset indices.\n\nFor each sample from this list,\nif the `answer_text` is fully present in `context` the offset start and end is returned\nelse it is considered as no answer in the context\n\n","metadata":{}},{"cell_type":"code","source":"class ChaiiDataset(Dataset):\n    \n    def __init__(self,df,max_len=400,doc_stride=128):\n        \n        self.df = df\n        self.max_len = max_len \n        self.doc_stride = doc_stride\n        self.labelled = 'answer_text' in df\n        self.tokenizer = AutoTokenizer.from_pretrained(config['base_model'],add_special_tokens=True)        \n        self.tokenized_samples = self.tokenizer(\n                                self.df['context'].values.tolist(),\n                                self.df['question'].values.tolist(),\n                                truncation=\"only_first\",\n                                max_length=self.max_len,\n                                stride=self.doc_stride,\n                                return_overflowing_tokens=True,\n                                return_offsets_mapping=True,\n                                padding=\"max_length\")\n        \n    \n        \n    def __getitem__(self,idx):\n        \n        data = {}\n        ids,mask,offset = self.tokenized_samples['input_ids'][idx],\\\n                        self.tokenized_samples['attention_mask'][idx],\\\n                        self.tokenized_samples['offset_mapping'][idx]\n        \n        data['index'] = idx\n        data['ids'] = torch.tensor(ids)\n        data['mask'] = torch.tensor(mask)\n        data['offset'] = offset\n        if self.labelled:\n            \n            answer_text,start,end = self.get_targets(idx)\n            data['answer_text'] = answer_text\n            data['start'] = torch.tensor(start)\n            data['end'] = torch.tensor(end)\n            \n        \n        return data\n        \n    \n    def get_targets(self,idx):\n        \n        df_index = self.tokenized_samples['overflow_to_sample_mapping'][idx]\n        start_char = (self.df.iloc[df_index]['answer_start'])\n        end_char = start_char + len(self.df.iloc[df_index]['answer_text'])\n        offset = self.tokenized_samples['offset_mapping'][idx]\n        sequence_ids = self.tokenized_samples.sequence_ids(idx)\n        end_offset = len(self.tokenized_samples['input_ids'][idx])-1\n        start_offset = 1\n        while sequence_ids[end_offset] != 0:\n            end_offset -= 1\n            \n            \n        start_idx = 0;end_idx=0\n        ## answer not in context\n        if (start_char > offset[end_offset][0] or end_char < offset[start_offset][0]):\n            #print(\"In first loop\")\n            start_idx = 0;end_idx=0\n            answer_text=\"\"\n            \n        ## answer partially in context\n        elif ((start_char <= offset[end_offset][0]) and (end_char >  offset[end_offset][0])):\n            #print(\"in second loop\")\n            start_idx = 0;end_idx=0\n            answer_text = \"\"\n        \n        ## answer fully inside context\n        else:\n            #print(\"In third loop\")\n            i=0\n            while (start_idx < len(offset) and offset[i][0]<=start_char and offset[i][1]<start_char):\n                start_idx+=1\n                i+=1\n            end_idx = i\n            while (end_idx < len(offset) and offset[i][1]<end_char):\n                end_idx+=1\n                i+=1\n            answer_text = self.df.iloc[df_index]['answer_text'].strip()\n            \n        \n        return answer_text,start_idx, end_idx \n    \n    \n    def post_process(self,batch,pred_start,pred_end):\n        batch_pred,indices = [],[]\n        for idx,start,end in zip(batch['index'],pred_start,pred_end):\n            a,b = self.tokenized_samples['offset_mapping'][idx][start][0],self.tokenized_samples['offset_mapping'][idx][end][1]\n            df_index = self.tokenized_samples['overflow_to_sample_mapping'][idx]\n\n            if a>b:\n                batch_pred.append(\"\")\n                indices.append(df_index)\n            else: \n                pred_string = self.df.iloc[df_index]['context'][a:b].strip()   \n                batch_pred.append(pred_string.strip())\n                indices.append(df_index)\n\n        return batch_pred,indices\n\n    \n    \n    \n    def __len__(self):\n        return len(self.tokenized_samples['overflow_to_sample_mapping'])\n                \n        \n                \n                \n        \n            \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.577699Z","iopub.execute_input":"2021-09-05T06:34:39.578021Z","iopub.status.idle":"2021-09-05T06:34:39.598745Z","shell.execute_reply.started":"2021-09-05T06:34:39.57798Z","shell.execute_reply":"2021-09-05T06:34:39.597834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Model</p>\nI will use the roberta last layer hidden state output to model the target","metadata":{}},{"cell_type":"code","source":"class ChaiiModel(nn.Module):\n    \n    def __init__(self):\n        super(ChaiiModel,self).__init__()\n        \n        self.model_config = AutoConfig.from_pretrained(config['base_model'])\n        self.model_config.return_dict=True\n        self.model_config.hidden_dropout_prob = config['dropout']\n        self.model_config.attention_probs_dropout_prob = config['dropout']\n        self.model_config.output_hidden_states=True\n        self.model = AutoModel.from_pretrained(config['base_model'],config=self.model_config)\n        self.dropout = nn.Dropout(config['dropout'])\n        self.fc = nn.Linear(self.model_config.hidden_size,2)\n        self.__init_weights(self.fc)\n        \n    def __init_weights(self,module):\n        if isinstance(module,nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        \n    def forward(self,input_ids,attention_mask):\n        \n        output = self.model(input_ids,attention_mask)\n        hidden_states = output['hidden_states'][-1]\n        #x = torch.stack([hidden_states[-1],hidden_states[-2],hidden_states[-3],hidden_states[-4]])\n        #x = torch.mean(x,0)\n        x = self.dropout(hidden_states)\n        x = self.fc(x)\n        start_logits,end_logits = x.split(1,dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n                \n        return start_logits, end_logits\n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.599976Z","iopub.execute_input":"2021-09-05T06:34:39.600531Z","iopub.status.idle":"2021-09-05T06:34:39.613207Z","shell.execute_reply.started":"2021-09-05T06:34:39.600409Z","shell.execute_reply":"2021-09-05T06:34:39.612436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Differential LR</p>\nDifferent learning rate for encoder and head.","metadata":{}},{"cell_type":"code","source":"def get_params(mode,model,lr):\n    \n    if mode == 'i':\n            return [\n                {\n                    \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                    \"weight_decay\": 0.01,'lr':lr,\n                },\n                {\n                    \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                    \"weight_decay\": 0.0,'lr':lr,\n                },\n            ]\n            \n    elif mode =='k':\n    \n        non_bert = [(n,p) for n,p in model.named_parameters() if 'encoder' not in n]\n        no_decay = ['bias']\n\n        return [{'params':[p for n,p in non_bert if any(k not in n for k in no_decay)],\n         'lr':lr,'weight_decay':0.01,'name':'non_bert_weights'},\n         {'params':[p for n,p in non_bert if any(k in n for k in no_decay)],\n         'lr':lr,'weight_decay':0.00,'name':'non_bert_bias'},\n         {'params':[p for (n,p) in model.named_parameters() if (n not in [i for i,m in non_bert]) and any(k not in n for k in no_decay)],\n         'lr':lr*2.6,'weight_decay':0.01},\n         {'params':[p for (n,p) in model.named_parameters() if (n not in [i for i,m in non_bert]) and any(k in n for k in no_decay)],\n         'lr':lr*2.6,'weight_decay':0.00}\n\n        ]\n    else:\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.615519Z","iopub.execute_input":"2021-09-05T06:34:39.615779Z","iopub.status.idle":"2021-09-05T06:34:39.629247Z","shell.execute_reply.started":"2021-09-05T06:34:39.615757Z","shell.execute_reply":"2021-09-05T06:34:39.628525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Reinitializing pretrained tranformer blocks</p>\nThe idea is to reinitialize some final layers of the model. The idea is motivated by computer vision transfer learning results where we know that lower pre-trained layers learn more general features while higher layers closer to the output specialize more to the pre-training tasks. \nExisting methods using Transformer show that using the complete network is not always the most effective choice and usually slows down training and hurts performance.\n\nthanks to this great [notebook](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning) by @rhtsingh","metadata":{}},{"cell_type":"code","source":"def reinit_layers(model):\n    \n    print(\"Reinitilizing layers\")\n    for layer in model.model.encoder.layer[-config['num_reinit_layers']:]:\n\n            for module in layer.modules():\n\n                if isinstance(module,nn.Linear):\n                    module.weight.data.normal_(mean=0.0,std=model.model_config.initializer_range)\n                    if module.bias is not None:\n                            module.bias.data.zero_()\n                elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n                        \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.631944Z","iopub.execute_input":"2021-09-05T06:34:39.632355Z","iopub.status.idle":"2021-09-05T06:34:39.641838Z","shell.execute_reply.started":"2021-09-05T06:34:39.632317Z","shell.execute_reply":"2021-09-05T06:34:39.641034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Loss function and score</p>\n\nI have used `crossentropy` as the loss function and `jaccard` score used to track the metric","metadata":{}},{"cell_type":"code","source":"def safe_div(x,y):\n    if y == 0:\n        return 1\n    return x / y\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return safe_div(float(len(c)) , (len(a) + len(b) - len(c)))\n\ndef get_jaccard_score(y_true,y_pred):\n    assert len(y_true)==len(y_pred)\n    scores=[]\n    for i in range(len(y_true)):\n        if len(y_true[i])>0:\n            scores.append(jaccard(y_true[i], y_pred[i]))\n            \n        \n    return np.mean(scores) if len(scores)>0 else 0.0\n\ndef chaii_loss(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss(reduction='sum')\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = (start_loss + end_loss)/2\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.643513Z","iopub.execute_input":"2021-09-05T06:34:39.643869Z","iopub.status.idle":"2021-09-05T06:34:39.654974Z","shell.execute_reply.started":"2021-09-05T06:34:39.643836Z","shell.execute_reply":"2021-09-05T06:34:39.65418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Train function</p>","metadata":{}},{"cell_type":"code","source":"def train_one_batch(dataloaders,data,model,criterion,optimizer,scheduler,phase):\n    \n                if phase=='train':\n                    model.train()\n\n                else:\n                    model.eval()\n                \n                input_ids = data['ids'].cuda()\n                masks = data['mask'].cuda()\n                start,end = data['start'].cuda(),data['end'].cuda()\n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'):\n\n                    start_logits, end_logits = model(input_ids, masks)\n                    loss = criterion(start_logits,end_logits,start,end)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        if scheduler!=None:\n                            scheduler.step()\n\n                    epoch_loss = loss.item()\n                    \n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    pred_start,pred_end = start_logits.argmax(axis=1),end_logits.argmax(axis=1)\n                    prediction_strings,_ = dataloaders[phase].dataset.post_process(data,pred_start, pred_end)\n                    \n                    epoch_jaccard = get_jaccard_score(data['answer_text'],prediction_strings)\n                return epoch_loss,epoch_jaccard\n                    \n            \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.656295Z","iopub.execute_input":"2021-09-05T06:34:39.656733Z","iopub.status.idle":"2021-09-05T06:34:39.667825Z","shell.execute_reply.started":"2021-09-05T06:34:39.656697Z","shell.execute_reply":"2021-09-05T06:34:39.666756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model,dataloaders,criterion,optimizer,scheduler=None,epochs=3,filename='saved.pth'):\n    \n    \n    model.cuda()\n    best_loss = np.inf\n    step = 0\n    for epoch in range(epochs):\n        \n        train_loss = 0.0\n        train_score = 0.0\n        for i,batch in enumerate(dataloaders['train']):\n            loss,score = train_one_batch(dataloaders,batch,model,criterion,optimizer,scheduler,'train')\n            train_loss += loss\n            train_score += score\n            \n            if (i>0.75*len(dataloaders['train']) and i%config['swa_freq']==0):\n                print(f\"taking swa snapshot @{i}\")\n                optimizer.update_swa()\n            \n            if (step>0 and step%config['eval_every']==0) or (i == len(dataloaders['train'])-1):\n                valid_loss=0.0;valid_score = 0.0\n                \n                if (i==len(dataloaders['train'])-1):\n                    optimizer.swap_swa_sgd()\n                        \n                for k,batch in enumerate(dataloaders['valid']):\n                    loss,score = train_one_batch(dataloaders,batch,model,criterion,optimizer,scheduler,'valid')\n                    valid_loss += loss\n                    valid_score += score\n                \n                if (valid_loss/len(dataloaders['valid'].dataset)) < best_loss:\n                    torch.save(model.state_dict(),filename)\n                    best_loss = valid_loss/len(dataloaders['valid'].dataset)\n                    \n                if (i==len(dataloaders['train'])-1):\n                    optimizer.swap_swa_sgd()\n\n                print('Valid step {} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                    step, valid_loss/len(dataloaders['valid'].dataset), valid_score/len(dataloaders['valid'])))\n            \n                wandb.log({'validation loss':valid_loss/len(dataloaders['valid'].dataset)},step=step)\n                wandb.log({'validation score':valid_score/len(dataloaders['valid'])},step=step)\n\n            step += 1\n            \n        print('Train Epoch {}/{} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, epochs, train_loss/len(dataloaders['train'].dataset), train_score/len(dataloaders['train'])))\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.669233Z","iopub.execute_input":"2021-09-05T06:34:39.669599Z","iopub.status.idle":"2021-09-05T06:34:39.684139Z","shell.execute_reply.started":"2021-09-05T06:34:39.669563Z","shell.execute_reply":"2021-09-05T06:34:39.683297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Train and eval function</p>","metadata":{}},{"cell_type":"code","source":"def train_and_eval(train,valid,fold):\n    \n    config['run_name']=f'fold{fold}_dropout_{config[\"dropout\"]}_reinit_{config[\"num_reinit_layers\"]}_lr_{config[\"lr\"]}'\n    seed_everything(config['seed'])\n    run = wandb.init(reinit=True, project=\"google-chaii-baseline\", config=config)\n    wandb.run.name = config['run_name']\n    \n    with run:\n\n    \n        train = ChaiiDataset(train)\n        train_loader = DataLoader(train,batch_size=config['batch_size'],shuffle=True)\n        config['eval_every'] = int(len(train_loader)//3) ##validate 3 times inside an epoch\n        \n        valid = ChaiiDataset(valid)\n        valid_loader = DataLoader(valid,batch_size=config['batch_size'],shuffle=True)\n\n        model = ChaiiModel()\n        if config['num_reinit_layers']:\n            model = reinit_layers(model)\n\n        criterion = chaii_loss\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n\n        optimizer_grouped_parameters = get_params(\"k\",model,config['lr'])\n        optimizer = AdamW(optimizer_grouped_parameters)\n        optimizer = torchcontrib.optim.SWA(optimizer)\n    \n        steps = (len(train)*config['epochs'])//config['batch_size']\n        scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=int(0*steps),num_training_steps=steps)\n        dataloaders = {'train':train_loader,'valid':valid_loader}\n        filename = f\"{fold}_chaii_model.pth\"\n        train_model(model,dataloaders,criterion,optimizer,scheduler,config['epochs'],filename)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.685706Z","iopub.execute_input":"2021-09-05T06:34:39.686166Z","iopub.status.idle":"2021-09-05T06:34:39.697912Z","shell.execute_reply.started":"2021-09-05T06:34:39.686121Z","shell.execute_reply":"2021-09-05T06:34:39.696963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">K Fold training</p>","metadata":{}},{"cell_type":"code","source":"def run_k_fold(folds=5):\n    \n    \n    \n    train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n    external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv').sample(354)\n    external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv').sample(400)\n    external_train = pd.concat([external_mlqa, external_xquad])\n    external_train['id'] = list(np.arange(1, len(external_train)+1))\n    df = pd.concat([train, external_train]).reset_index(drop=True)\n    print(f\"Number of samples in train data is {df.shape[0]}\")\n\n    \n    df[\"kfold\"] = -1\n\n    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n    for f, (t_, v_) in enumerate(kf.split(X=df, y=df.language.values)):\n        df.loc[v_, 'kfold'] = f\n        \n    for fold in range(folds):\n        \n        \n            print(f\"Training fold {fold}\")\n\n            train = df[df['kfold']!=fold]\n            valid = df[df['kfold']==fold]\n\n            train_and_eval(train,valid,fold)\n\n            print(\"----------------------\")\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.700581Z","iopub.execute_input":"2021-09-05T06:34:39.700884Z","iopub.status.idle":"2021-09-05T06:34:39.711322Z","shell.execute_reply.started":"2021-09-05T06:34:39.700858Z","shell.execute_reply":"2021-09-05T06:34:39.710439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_k_fold(config['folds'])","metadata":{"execution":{"iopub.status.busy":"2021-09-05T06:34:39.714379Z","iopub.execute_input":"2021-09-05T06:34:39.714661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Inference</p>\n\nAfter making predictions using each folds, the pred start and end is used to post-process and get back the original answer strings. The `get_best_prediction` function is used after this step to select the best prediction string from predictions of different folds.","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_process(prediction_strings,indices):\n    \n    df = pd.DataFrame()\n    df['index'] = indices\n    df['answer'] = prediction_strings\n    \n    def best_answer(x):\n        x = [k for k in x if len(k)>0]\n        if len(x)>0:\n            return min(x,key=len)\n        return \"\"\n    \n    answer = df.groupby(['index'])['answer'].apply(lambda x : best_answer(x))\n    \n    return answer\n\n    \n    \n    \n\ndef inference_fn(test,fold):\n    \n    prediction_strings,indices = [],[]\n    test = ChaiiDataset(test)\n    test_loader = DataLoader(test,batch_size=config['batch_size'],shuffle=False)\n\n    model = ChaiiModel()\n    filename = f\"../input/chaiibaseline/{fold}_chaii_model.pth\"\n    \n    model.load_state_dict(torch.load(filename))\n    model.to(config['device'])\n    model.eval()\n    \n    for i,data in enumerate(test_loader):\n        \n        input_ids = data['ids'].cuda()\n        masks = data['mask'].cuda()\n        \n        start_logits,end_logits = model(input_ids,masks)\n        \n        start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n        end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n        pred_start,pred_end = start_logits.argmax(axis=1),end_logits.argmax(axis=1)\n        preds,ind = test_loader.dataset.post_process(data,pred_start, pred_end)\n        prediction_strings.extend(preds);indices.extend(ind)\n        \n    \n    return  post_process(prediction_strings,indices)\n\n\n##credit goes to https://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765\ndef rule_process(test):\n    \n    bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\n    bad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\n    tamil_ad = \"கி.பி\"\n    tamil_bc = \"கி.மு\"\n    tamil_km = \"கி.மீ\"\n    hindi_ad = \"ई\"\n    hindi_bc = \"ई.पू\"\n\n    cleaned_preds = []\n    for pred, context in test[[\"PredictionString\", \"context\"]].to_numpy():\n        if pred == \"\":\n            cleaned_preds.append(pred)\n            continue\n        while any([pred.startswith(y) for y in bad_starts]):\n            pred = pred[1:]\n        while any([pred.endswith(y) for y in bad_endings]):\n            if pred.endswith(\"...\"):\n                pred = pred[:-3]\n            else:\n                pred = pred[:-1]\n\n        if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n            pred = pred+\".\"\n\n        cleaned_preds.append(pred)\n    \n    return cleaned_preds\n        \n\n\ndef get_best_prediction(df):\n    \n    all_answers=[]\n    for i,row in df.iterrows():\n    \n        candidates = [k.strip() for k in row.values.tolist() if len(k)<100 and len(k)>0]\n        if len(candidates)>0:\n            counter = Counter(candidates)\n            answer = counter.most_common(1)[0][0]\n        else:\n            answer = \"\"\n\n        all_answers.append(answer)\n    \n    return all_answers\n\n    \n    \n    \n    \n    \n    \n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf = pd.DataFrame()\ntest_df = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\nfor fold in tqdm(range(config['folds'])):\n    \n    preds = inference_fn(test_df,fold)\n    test_df['PredictionString'] = preds\n    preds = rule_process(test_df)\n    df[f'fold_{fold}'] = preds\n    \n    \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Submission</p>","metadata":{}},{"cell_type":"code","source":"answers =  get_best_prediction(df)   \nsubmission = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv\")\nsubmission['PredictionString'] = answers\n\nsubmission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f6; color :#E94421; border-radius: 10px 10px; text-align:center\">Upvote if you liked it</p>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}