{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq\n!pip install accelerate","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport regex as re\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom datasets import Dataset\nfrom accelerate import Accelerator\nfrom transformers import (AutoTokenizer,AutoModelForQuestionAnswering,AutoModel,\n                          AutoConfig,AdamW,get_linear_schedule_with_warmup,\n                          get_cosine_schedule_with_warmup)\n\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {'model_path':'../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2',\n          \n          'max_length':384,\n          'doc_stride':128,\n          'max_answer_length':30,\n          \n          'lr':1e-5,\n          'wd':1e-2,\n    \n          'epochs':1,\n          'nfolds':5,\n          'batch_size':4,\n          'num_workers':4,\n          'seed':1000}\n\nfor i in range(config['nfolds']):\n    os.makedirs(f'model{i}',exist_ok=True)\n    \ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_data(df,train=True):\n    df['orignal_context_len'] = df['context'].apply(lambda x:len(x.split(\" \")))\n    \n    def remove_stuff(x):\n        x = x.replace('\\n','')\n        x = x.replace(\"(\",\" (\").replace(\")\",\") \")\n        x = re.sub(r'https?\\S+', '', x)\n        x = re.sub(r'\\[[0-9]]','',x)\n        return x\n    \n    df['context'] = df['context'].apply(lambda x: remove_stuff(x))\n    df['question'] = df['question'].apply(lambda x: remove_stuff(x))\n            \n    df['new_context_len'] = df['context'].apply(lambda x:len(x.split(\" \")))\n    \n    print(\"Average original context len: \",df.orignal_context_len.mean())\n    print(\"Average New context len: \",df.new_context_len.mean())\n\n    #define new context and answer_start    \n    def convert_answers(r):\n        return {'answer_start': [r[0]], 'text': [r[1]]}\n    \n    if train:\n        df['answer_text'] = df.answer_text.astype(str)\n        df['answer_start'] = df[['context','answer_text']].apply(lambda x: x[0].find(x[1]),axis=1)\n        df['answers'] = df[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\n        print(\"Answers not found in new context: \",(df.answer_start == -1).sum())\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest_data = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nsample = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\n\ntrain_data['Fold'] = -1\nkfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=train_data['language'])):\n    train_data.loc[valid_idx,'Fold'] = k\n    \ntrain_data = pd.concat([train_data.query(\"language == 'tamil'\"),\n                        train_data.query(\"language == 'hindi'\").sample(n=368)])\n\ntrain_data = clean_data(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape,train_data.language.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_train_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n    examples['question'] = [q.lstrip() for q in examples['question']]\n    \n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\")\n    \n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    \n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n    \n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,model_name):\n        super(Model,self).__init__()\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"layer_norm_eps\": 1e-7,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.roberta = AutoModel.from_pretrained(model_name,config=self.config)\n        self.roberta.pooler = nn.Identity()\n        self.linear = nn.Linear(self.config.hidden_size,2)\n        \n    def loss_fn(self,start_logits,end_logits,start_positions,end_positions):\n        if len(start_positions.size()) > 1:\n            start_positions = start_positions.squeeze(-1)\n        if len(end_positions.size()) > 1:\n            end_positions = end_positions.squeeze(-1)\n\n        ignored_index = start_logits.size(1)\n        start_positions = start_positions.clamp(0, ignored_index)\n        end_positions = end_positions.clamp(0, ignored_index)\n        loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n        start_loss = loss_fct(start_logits, start_positions)\n        end_loss = loss_fct(end_logits, end_positions)\n        total_loss = 0.75 * start_loss + 0.25 * end_loss \n        return total_loss\n    \n    def forward(self,**xb):\n        x = self.roberta(input_ids=xb['input_ids'],attention_mask=xb['attention_mask'])[2]\n        x = (x[-1] + x[-2])/2\n        x = self.linear(x)\n        start_logits,end_logits = x.split(1,dim=-1)\n        start_logits,end_logits = start_logits.squeeze(-1).contiguous(),end_logits.squeeze(-1).contiguous()\n        start_positions,end_positions = xb['start_positions'],xb['end_positions']\n        \n        loss = None\n        if start_positions is not None and end_positions is not None:\n            loss = self.loss_fn(start_logits, end_logits, start_positions, end_positions)\n            \n        return (start_logits,end_logits),loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChaiiDataset:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):            \n        return {\"input_ids\": torch.tensor(self.data[idx][\"input_ids\"], dtype=torch.long),\n                \"attention_mask\": torch.tensor(self.data[idx][\"attention_mask\"], dtype=torch.long),\n                \"start_positions\":torch.tensor(self.data[idx][\"start_positions\"],dtype=torch.long),\n                \"end_positions\":torch.tensor(self.data[idx][\"end_positions\"],dtype=torch.long) }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(fold):\n    \n    def evaluate(model,valid_loader):\n        model.eval()\n        valid_loss = 0\n        with torch.no_grad():\n            for i, inputs in enumerate(tqdm(valid_loader)):\n                inputs = {key:val.reshape(val.shape[0],-1) for key,val in inputs.items()}\n                outputs = model(**inputs)\n                loss = outputs[1]\n                valid_loss += loss.item()\n\n        valid_loss /= len(valid_loader)\n        return valid_loss\n        \n    def train_and_evaluate_loop(train_loader,valid_loader,model,optimizer,\n                                epoch,fold,best_loss,lr_scheduler=None):\n        train_loss = 0\n        for i, inputs in enumerate(tqdm(train_loader)):\n            optimizer.zero_grad()\n            model.train()\n            inputs = {key:val.reshape(val.shape[0],-1) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            loss = outputs[1]\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n        \n        train_loss /= len(train_loader)\n        valid_loss = evaluate(model,valid_loader) \n\n        if valid_loss <= best_loss:\n            print(f\"Epoch:{epoch} |Train Loss:{train_loss}|Valid Loss:{valid_loss}\")\n            print(f\"{g_}Loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n\n            best_loss = valid_loss\n            torch.save(model.state_dict(),f'./model{fold}/model{fold}.bin')\n            tokenizer.save_pretrained(f'./model{fold}')\n                    \n        return best_loss\n        \n    accelerator = Accelerator()\n    print(f\"{accelerator.device} is used\")\n    \n    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n        \n    model = Model(config['model_path'])\n    tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\n    pad_on_right = tokenizer.padding_side == 'right'\n    \n    train_dataset = Dataset.from_pandas(x_train)\n    train_features = train_dataset.map(\n                    partial(\n                        prepare_train_features, \n                        tokenizer=tokenizer,\n                        pad_on_right=pad_on_right, \n                        max_length=config['max_length'],\n                        doc_stride=config['doc_stride']\n                    ),\n                    batched=True,\n                    remove_columns=train_dataset.column_names)\n        \n    train_ds = ChaiiDataset(train_features)\n    train_dl = DataLoader(train_ds,\n                        batch_size = config[\"batch_size\"],\n                        num_workers = config['num_workers'],\n                        shuffle=True,\n                        pin_memory=True,\n                        drop_last=True)\n    \n    valid_dataset = Dataset.from_pandas(x_valid)\n    valid_features = valid_dataset.map(\n                    partial(\n                        prepare_train_features, \n                        tokenizer=tokenizer,\n                        pad_on_right=pad_on_right, \n                        max_length=config['max_length'],\n                        doc_stride=config['doc_stride']\n                    ),\n                    batched=True,\n                    remove_columns=train_dataset.column_names)\n        \n    valid_ds = ChaiiDataset(valid_features)\n    valid_dl = DataLoader(valid_ds,\n                        batch_size = config[\"batch_size\"],\n                        num_workers = config['num_workers'],\n                        shuffle=False,\n                        pin_memory=True,\n                        drop_last=False)\n\n    optimizer = AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])    \n    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                                   num_warmup_steps=0,\n                                                   num_training_steps= config['epochs'] * len(train_dl))\n\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    print(f\"Fold: {fold}\")\n    best_loss = 9999\n    start_time = time.time()\n    for epoch in range(config[\"epochs\"]):\n        print(f\"Epoch Started:{epoch}\")\n        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,optimizer,epoch,fold,best_loss,lr_scheduler)\n        \n        end_time = time.time()\n        print(f\"{m_}Time taken by epoch {epoch} is {end_time-start_time:.2f}s{sr_}\")\n        start_time = end_time\n        \n    return best_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_loss_per_fold = [run(f) for f in range(config['nfolds'])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_loss_per_fold)\nprint(np.mean(best_loss_per_fold))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0.252042833133833","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}