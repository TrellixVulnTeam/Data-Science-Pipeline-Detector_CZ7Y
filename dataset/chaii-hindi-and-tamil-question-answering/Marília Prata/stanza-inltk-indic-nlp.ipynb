{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T17:56:43.769334Z","iopub.execute_input":"2021-08-15T17:56:43.769883Z","iopub.status.idle":"2021-08-15T17:56:43.782129Z","shell.execute_reply.started":"2021-08-15T17:56:43.769848Z","shell.execute_reply":"2021-08-15T17:56:43.781254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSMcqi3PiHDWsNk5YrwALttBllQQdpvfgsOxg&usqp=CAU)amazon.in","metadata":{}},{"cell_type":"markdown","source":"#Three Important NLP Libraries for Indian Languages\n\nAuthor: MOHD SANAD ZAKI RIZVI, JANUARY 23, 2020 \n\nText Processing for Indian Languages using Python:\n\niNLTK\n\nIndic NLP Library\n\nStanza - https://stanfordnlp.github.io/stanza/index.html\n\nhttps://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/","metadata":{}},{"cell_type":"code","source":"!pip install spacy ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T17:56:51.398618Z","iopub.execute_input":"2021-08-15T17:56:51.399174Z","iopub.status.idle":"2021-08-15T17:56:59.344375Z","shell.execute_reply.started":"2021-08-15T17:56:51.399141Z","shell.execute_reply":"2021-08-15T17:56:59.342748Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/train.csv\")\ntrain.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-15T17:57:02.007453Z","iopub.execute_input":"2021-08-15T17:57:02.00789Z","iopub.status.idle":"2021-08-15T17:57:02.504846Z","shell.execute_reply.started":"2021-08-15T17:57:02.007855Z","shell.execute_reply":"2021-08-15T17:57:02.503971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.iloc[9,1]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T17:57:11.90921Z","iopub.execute_input":"2021-08-15T17:57:11.909602Z","iopub.status.idle":"2021-08-15T17:57:11.916636Z","shell.execute_reply.started":"2021-08-15T17:57:11.909571Z","shell.execute_reply":"2021-08-15T17:57:11.915806Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paragraphs = [\"நெல்சன் மண்டேலா (Nelson Rolihlahla Mandela, 18 சூலை 1918 – 5 திசம்பர் 2013)\", \"தென்னாப்பிரிக்காவின் மக்களாட்சி முறையில் தேர்ந்தெடுக்கப்பட்ட முதல் குடியரசுத் தலைவர் ஆவார்.\", \"அதற்கு\",\"முன்னர் நிறவெறிக்கு எதிராகப் போராடிய முக்கிய தலைவர்களுள் ஒருவராக இருந்தார்.\",\"MPWOLKE\"\"தொடக்கத்தில்\",\"அறப்போர் (வன்முறையற்ற) வழியில் நம்பிக்கை கொண்டிருந்த இவர், பிறகு  ஆப்பிரிக்க தேசிய காங்கிரஸின் இராணுவப்\",\"MPWOLKE\",\" பிரிவுக்கு தலைமை தாங்கினார்.\",\"இவர்கள்\", \"மரபுசாரா கொரில்லாப் போர்முறைத் தாக்குதலை நிறவெறி\",\"MPWOLKE\",\" அரசுக்கு எதிராக நடத்தினர்.\", \"மண்டேலாவின் 27\", \"ஆண்டு சிறைவாசம், நிறவெறிக் கொடுமையின் பரவலாக அறியப்பட்ட சாட்சியமாக விளங்குகிறது.\", \"சிறையின்\" , \"பெரும்பாலான காலத்தை இவர் ராபன் தீவில் சிறிய சிறை அறையில் கழித்தார். \",\"1990 இல்\", \"அவரது விடுதலைக்கு பிறகு அமைதியான முறையில் புதிய தென்னாப்பிரிக்கக் குடியரசு மலர்ந்தது.\", \"மண்டேலா\", \"உலகில் அதிகம் மதிக்கப்படும் தலைவர்களில் ஒருவராக விளங்கினார்.\\nமண்டேலா, இனவெறி ஆட்சியில் ஊறிக்கிடந்த தென்னாபிரிக்காவை மக்களாட்சியின் மிளிர்வுக்கு இட்டுச் சென்றவர்.\", \"அமைதிவழிப்\",\" போராளியாக, ஆயுதப் போராட்டத் தலைவனாக, தேசத்துரோகக் குற்றம் சுமத்தப்பட்ட குற்றவாளியாக, 27 ஆண்டுகள் சிறையில் வாடி பின்னர் விடுதலையாகி குடியரசு தலைவராக, அமைதிக்கான நோபல் பரிசு பெற்றவராக இவரின் அரசியல் பயணம் தொடர்ந்தது.\", \"சூன் 2008ல்\" \"பொது வாழ்க்கையிலிருந்து விலகுவதாக அறிவித்தார்.\\n இளமை \\nநெல்சன்\" \"மண்டேலா 1918\" \"ஆம் ஆண்டு சூலை மாதம் 18 ஆம்\" \"தியதி தென்னாப்பிரிக்காவில் உள்ள குலு கிராமத்தில் பிறந்தார்.\" \"இவரது தந்தை சோசா பழங்குடி இன மக்கள் தலைவர் ஆவார்[1].\"\" இவரின் தந்தைக்கு நான்கு மனைவிகள்.\"\" 4 ஆண்களும் 9 பெண்களுமாக 13 பிள்ளைகள். மூன்றாவது மனைவிக்கு மகனாகப் பிறந்தவர் தான் மண்டேலா. இவரின் முழுப்பெயர் \\நெல்சன் ரோபிசலா மண்டேலா\\. நெல்சன் மண்டேலா என்றே பொதுவாக அழைப்பார்கள். இவரின் சிறுவயதில் குத்துச் சண்டை வீரராகவே அறியப் பெற்றார்.\\n\\nஅந்தக் குடும்பத்திலிருந்து  முதன் முதலில் பள்ளி சென்ற மண்டேலா, இளம் வயதில் ஆடு, மாடு மேய்த்துக்கொண்டே பள்ளிக்கூடத்தில் படித்தார். போர் புரியும் கலைகளையும் பயின்றார். இவரின் பெயரின் முன்னால் உள்ள \"\"நெல்சன்\" \"இவர் கல்வி கற்ற முதல் பள்ளியின் ஆசிரியரினால் சூட்டப்பட்டது என்பது குறிப்பிடத்தக்கது. கல்வியறிவைப் பெறுவதில் பெரிதும் நாட்டம் கொண்ட மண்டேலா, லண்டன் மற்றும் தென்னாபிரிக்கா பல்கலைக்கழகங்களிலும் பட்டப்படிப்பை மேற்கொண்டார். 1941 ஆம் ஆண்டு ஜோகானஸ்பேர்க் சென்று பகுதி நேரத்தில் சட்டக்கல்வி படித்தார். ஒரு தங்கச் சுரங்க பாதுகாப்பு அதிகாரியாகவும், தோட்ட முகவராகவும் பணியாற்றி வந்தார்.\\nஅப்போது \\'நோமதாம் சங்கர்\\' என்ற செவிலியரைத்  திருமணம் செய்து கொண்டார். மண்டேலா ஆப்பிரிக்க தேசிய காங்கிரஸ் இயக்கத்தில் தீவிரமாக ஈடுபட்டு இருந்ததால் மனைவிக்கும் அவருக்கும் இடையே கருத்து வேறுபாடு ஏற்பட்டது. பின்னர் தென்னாப்பிரிக்க அரசு, ஆப்பிரிக்க தேசிய காங்கிரஸ் கட்சியைத் தடை செய்தது. மண்டேலா மீது வழக்கு தொடரப்பட்டது. ஐந்தாண்டுகளாக அந்த வழக்கு விசாரணை நடந்து கொண்டு இருந்தபோது 1958 ஆம் ஆண்டு வின்னி மடிகி லேனா என்பவரை மணந்தார். வின்னி தனது கணவரின் கொள்கைகளுக்காகப் போராடி வ (Truth and Reconciliation Commission)\"]  ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T17:57:29.584211Z","iopub.execute_input":"2021-08-15T17:57:29.584746Z","iopub.status.idle":"2021-08-15T17:57:29.593329Z","shell.execute_reply.started":"2021-08-15T17:57:29.584712Z","shell.execute_reply":"2021-08-15T17:57:29.592135Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \" \".join(paragraphs)\nwords = text.split(\" \")","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:40.579982Z","iopub.execute_input":"2021-08-15T17:57:40.580562Z","iopub.status.idle":"2021-08-15T17:57:40.584755Z","shell.execute_reply.started":"2021-08-15T17:57:40.580468Z","shell.execute_reply":"2021-08-15T17:57:40.58387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter \ncnt = Counter(words)\n\ncnt.most_common(10)\n# print ","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:46.634939Z","iopub.execute_input":"2021-08-15T17:57:46.635336Z","iopub.status.idle":"2021-08-15T17:57:46.642927Z","shell.execute_reply.started":"2021-08-15T17:57:46.635301Z","shell.execute_reply":"2021-08-15T17:57:46.641929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spacy.lang.ta import STOP_WORDS as STOP_WORDS_TA\n\n#from spacy.lang.hi import STOP_WORDS as hindi_stopwords\n#from spacy.lang.ta import STOP_WORDS as tamil_stopwords","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:52.010696Z","iopub.execute_input":"2021-08-15T17:57:52.011111Z","iopub.status.idle":"2021-08-15T17:57:52.015667Z","shell.execute_reply.started":"2021-08-15T17:57:52.011077Z","shell.execute_reply":"2021-08-15T17:57:52.014798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://colab.research.google.com/github/rahul1990gupta/indic-nlp-datasets/blob/master/examples/Getting_started_with_processing_hindi_text.ipynb#scrollTo=ghnpMGzngcOn\n\n# Let's remove the stop words before printing most common words \n\nfrom spacy.lang.ta import Tamil\n\nnlp = Tamil()\n\ndoc = nlp(text)\n\nnot_stop_words = []\nfor token in doc:\n  if token.is_stop:\n    continue\n  if token.is_punct or token.text ==\"|\":\n    continue \n  not_stop_words.append(token.text)\n\n\nnot_stop_cnt = Counter(not_stop_words)\n\nnot_stop_cnt.most_common(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:56.706247Z","iopub.execute_input":"2021-08-15T17:57:56.706969Z","iopub.status.idle":"2021-08-15T17:57:56.742801Z","shell.execute_reply.started":"2021-08-15T17:57:56.706912Z","shell.execute_reply":"2021-08-15T17:57:56.741618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cancel Wordcloud Go to plan B \n# Stanza, iNLTK and Indic NLP now are plan A  \nfrom wordcloud import WordCloud\nfrom spacy.lang.ta import STOP_WORDS as STOP_WORS_TA\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:58:04.298723Z","iopub.execute_input":"2021-08-15T17:58:04.299196Z","iopub.status.idle":"2021-08-15T17:58:04.304908Z","shell.execute_reply.started":"2021-08-15T17:58:04.299157Z","shell.execute_reply":"2021-08-15T17:58:04.303405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">iNLTK Toolkit for Indic Languages</span></h1><br>\n\niNLTK (Natural Language Toolkit for Indic Languages)\n\niNLTK has a dependency on PyTorch 1.3.1, to use iNLTK is necessary to install that below:\n\nhttps://inltk.readthedocs.io/en/latest/","metadata":{}},{"cell_type":"code","source":"!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T17:58:10.65942Z","iopub.execute_input":"2021-08-15T17:58:10.65985Z","iopub.status.idle":"2021-08-15T17:58:18.36797Z","shell.execute_reply.started":"2021-08-15T17:58:10.659799Z","shell.execute_reply":"2021-08-15T17:58:18.366323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Install inltk","metadata":{}},{"cell_type":"code","source":"!pip install inltk","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T17:58:24.56696Z","iopub.execute_input":"2021-08-15T17:58:24.567359Z","iopub.status.idle":"2021-08-15T17:58:32.616273Z","shell.execute_reply.started":"2021-08-15T17:58:24.567323Z","shell.execute_reply":"2021-08-15T17:58:32.615118Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Hi means hindi language. Not hello.","metadata":{}},{"cell_type":"code","source":"from inltk.inltk import setup\nsetup('hi')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T17:58:56.904232Z","iopub.execute_input":"2021-08-15T17:58:56.904666Z","iopub.status.idle":"2021-08-15T17:58:56.941064Z","shell.execute_reply.started":"2021-08-15T17:58:56.904628Z","shell.execute_reply":"2021-08-15T17:58:56.939522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Tokenization with iNLTK\n\nThe first step we do to solve any NLP task is to break down the text into its smallest units or tokens. iNLTK supports tokenization of all the 12 languages I showed earlier:\n\nhttps://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/","metadata":{}},{"cell_type":"code","source":"from inltk.inltk import tokenize\n\nhindi_text = \"\"\"अमेरिकी क्रन्तिकारी युद्ध (1775–1783), जिसे संयुक्त राज्य में अमेरिकी स्वतन्त्रता युद्ध या क्रन्तिकारी युद्ध भी कहा जाता है, ग्रेट ब्रिटेन और उसके तेरह उत्तर अमेरिकी उपनिवेशों के बीच एक सैन्य संघर्ष था, जिससे वे उपनिवेश स्वतन्त्र संयुक्त राज्य अमेरिका बने। शुरूआती लड़ाई उत्तर अमेरिकी महाद्वीप पर हुई। सप्तवर्षीय युद्ध में पराजय के बाद, बदले के लिए आतुर फ़्रान्स ने 1778 में इस नए\"\"\"\n\n# tokenize(input text, language code)\ntokenize(hindi_text, \"hi\") #Hi is Hindi language","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T17:59:17.770398Z","iopub.execute_input":"2021-08-15T17:59:17.77098Z","iopub.status.idle":"2021-08-15T17:59:17.905992Z","shell.execute_reply.started":"2021-08-15T17:59:17.770945Z","shell.execute_reply":"2021-08-15T17:59:17.90475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Generate similar sentences from a given text input\n\nSince iNLTK is internally based on a Language Model for each of the languages it supports, we can do interesting stuff like generate similar sentences given a piece of text!\n\nhttps://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/","metadata":{}},{"cell_type":"code","source":"from inltk.inltk import get_similar_sentences\n\n# get similar sentences to the one given in hindi\noutput = get_similar_sentences('मैं आज बहुत खुश हूं', 5, 'hi')\n\nprint(output)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T17:59:26.763879Z","iopub.execute_input":"2021-08-15T17:59:26.764262Z","iopub.status.idle":"2021-08-15T17:59:32.782835Z","shell.execute_reply.started":"2021-08-15T17:59:26.764231Z","shell.execute_reply":"2021-08-15T17:59:32.781663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Extract embedding vectors\n\n\"When we are training machine learning or deep learning-based models for NLP tasks, we usually represent the text data by an embedding like TF-IDF, Word2vec, GloVe, etc. These embedding vectors capture the semantic information of the text input and are easier to work with for the models (as they expect numerical input).\"\n\n\"iNLTK under the hood utilizes the ULMFiT method of training language models and hence it can generate vector embeddings for a given input text. Here’s an example:\"\n\nhttps://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/","metadata":{}},{"cell_type":"code","source":"from inltk.inltk import get_embedding_vectors\n\n# get embedding for input words\nvectors = get_embedding_vectors(\"अमेरिकी उपनिवेशों केया\", \"hi\") #hi is Hindi language\n\nprint(vectors)\n# print shape of the first word\nprint(\"shape:\", vectors[0].shape)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T17:59:38.739223Z","iopub.execute_input":"2021-08-15T17:59:38.739656Z","iopub.status.idle":"2021-08-15T17:59:39.07276Z","shell.execute_reply.started":"2021-08-15T17:59:38.739618Z","shell.execute_reply":"2021-08-15T17:59:39.07178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that each word is denoted by an embedding of 400 dimensions.","metadata":{}},{"cell_type":"markdown","source":"#Finding similarity between two sentences\n\niNLTK provides an API to find semantic similarities between two pieces of text. This is a really useful feature! We can use the similarity score for feature engineering and even building sentiment analysis systems. Here’s how it works:\n\nhttps://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/","metadata":{}},{"cell_type":"code","source":"from inltk.inltk import get_sentence_similarity\n\n# similarity of encodings is calculated by using cmp function whose default is cosine similarity\nget_sentence_similarity('मुझे भोजन पसंद है।', 'मैं ऐसे भोजन की सराहना करता हूं जिसका स्वाद अच्छा हो।', 'hi')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:59:48.370556Z","iopub.execute_input":"2021-08-15T17:59:48.370957Z","iopub.status.idle":"2021-08-15T17:59:49.012418Z","shell.execute_reply.started":"2021-08-15T17:59:48.370924Z","shell.execute_reply":"2021-08-15T17:59:49.011272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#The model gives out a cosine similarity of 0.67 which means that the sentences are pretty close, and that’s correct.","metadata":{}},{"cell_type":"code","source":"get_sentence_similarity(\"शुरूआती लड़ाई उत्तर अमेरिकी\", \"महाद्वीप पर हुई। सप्तवर्षीय युद्\", 'hi')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:59:54.782123Z","iopub.execute_input":"2021-08-15T17:59:54.782542Z","iopub.status.idle":"2021-08-15T17:59:55.440544Z","shell.execute_reply.started":"2021-08-15T17:59:54.782506Z","shell.execute_reply":"2021-08-15T17:59:55.439219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Above the similarity is 0.09326018","metadata":{}},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Indic NLP Library</span></h1><br>","metadata":{}},{"cell_type":"code","source":"!pip install indic-nlp-library","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T18:00:01.84376Z","iopub.execute_input":"2021-08-15T18:00:01.844149Z","iopub.status.idle":"2021-08-15T18:00:09.816206Z","shell.execute_reply.started":"2021-08-15T18:00:01.844117Z","shell.execute_reply":"2021-08-15T18:00:09.815259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download the resource\n!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T18:00:26.542644Z","iopub.execute_input":"2021-08-15T18:00:26.543175Z","iopub.status.idle":"2021-08-15T18:00:27.303008Z","shell.execute_reply.started":"2021-08-15T18:00:26.54313Z","shell.execute_reply":"2021-08-15T18:00:27.301447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download the repo\n!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T18:00:37.947378Z","iopub.execute_input":"2021-08-15T18:00:37.947784Z","iopub.status.idle":"2021-08-15T18:00:38.704782Z","shell.execute_reply.started":"2021-08-15T18:00:37.947747Z","shell.execute_reply":"2021-08-15T18:00:38.702777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Set the path so that Python knows where to find these on your computer:","metadata":{}},{"cell_type":"code","source":"import sys\nfrom indicnlp import common\n\n# The path to the local git repo for Indic NLP library\nINDIC_NLP_LIB_HOME=r\"indic_nlp_library\"\n\n# The path to the local git repo for Indic NLP Resources\nINDIC_NLP_RESOURCES=r\"indic_nlp_resources\"\n\n# Add library to Python path\nsys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))\n\n# Set environment variable for resources folder\ncommon.set_resources_path(INDIC_NLP_RESOURCES)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:00:52.754589Z","iopub.execute_input":"2021-08-15T18:00:52.755003Z","iopub.status.idle":"2021-08-15T18:00:52.762878Z","shell.execute_reply.started":"2021-08-15T18:00:52.754968Z","shell.execute_reply":"2021-08-15T18:00:52.760962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Splitting input text into sentences\n\n\"Indic NLP Library supports many basic text processing tasks like normalization, tokenization at the word level, etc. But sentence level tokenization is what I find interesting because this is something that different Indian languages follow different rules for.\"\n\n\"Here is an example of how to use this sentence splitter:\"\n\nhttps://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/","metadata":{}},{"cell_type":"code","source":"from indicnlp.tokenize import sentence_tokenize\n\nindic_string=\"\"\"பெண்கள் இறப்பதும், பிறந்தபின் குழந்தைகள் இறப்பதும் சர்வ சாதாரணம். லேசான சிராய்ப்புகளும் கீறல்களும் கூட மரணத்திற்கு இட்டுச் சென்றன. ஒரு நுண்ணுயிரை வைத்து இன்னொன்றைக் கொல்லமுடிகிற பெனிஸிலின் போன்ற நச்சுமுறி மருந்துகள் \"\"\"\n# Split the sentence, language code \"hi\" is passed for hingi\nsentences=sentence_tokenize.sentence_split(indic_string, lang='hi')\n\n# print the sentences\nfor t in sentences:\n    print(t)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:01:01.347454Z","iopub.execute_input":"2021-08-15T18:01:01.347949Z","iopub.status.idle":"2021-08-15T18:01:01.355239Z","shell.execute_reply.started":"2021-08-15T18:01:01.347902Z","shell.execute_reply":"2021-08-15T18:01:01.353986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#perform transliteration using the Indic NLP Library: Transliterating from Hindi to Tamil","metadata":{}},{"cell_type":"code","source":"from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n\n# Input text \"\ninput_text='பெண்கள் இறப்பதும், பிறந்தபின் குழந்தைகள் இறப்பதும் சர்வ சாதாரணம்.'\n\n# Transliterate from Hindi to Tamil\nprint(UnicodeIndicTransliterator.transliterate(input_text,\"hi\",\"ta\"))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:01:07.692123Z","iopub.execute_input":"2021-08-15T18:01:07.692558Z","iopub.status.idle":"2021-08-15T18:01:07.699749Z","shell.execute_reply.started":"2021-08-15T18:01:07.692525Z","shell.execute_reply":"2021-08-15T18:01:07.698288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Phonetics for Indian Sub-Continent languages Alphabet\n\n\"The Indian Sub-Continent languages have strong phonetics for their alphabet and that’s why in the Indic NLP Library, each character has a phonetic vector associated with it that defines its properties.\"\n\n\"An example where we take the simple Hindi character ‘उ’ :\n\nhttps://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/","metadata":{}},{"cell_type":"code","source":"from indicnlp.langinfo import *\n\n# Input character \nc='उ'\n# Language is Hindi or 'hi'\nlang='hi'\n\nprint('Is vowel?:  {}'.format(is_vowel(c,lang)))\nprint('Is consonant?:  {}'.format(is_consonant(c,lang)))\nprint('Is velar?:  {}'.format(is_velar(c,lang)))\nprint('Is palatal?:  {}'.format(is_palatal(c,lang)))\nprint('Is aspirated?:  {}'.format(is_aspirated(c,lang)))\nprint('Is unvoiced?:  {}'.format(is_unvoiced(c,lang)))\nprint('Is nasal?:  {}'.format(is_nasal(c,lang)))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:01:13.420715Z","iopub.execute_input":"2021-08-15T18:01:13.421164Z","iopub.status.idle":"2021-08-15T18:01:13.431627Z","shell.execute_reply.started":"2021-08-15T18:01:13.421131Z","shell.execute_reply":"2021-08-15T18:01:13.430291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Stanza NLP Tookit</span></h1><br>","metadata":{}},{"cell_type":"markdown","source":"![](https://i.ytimg.com/vi/Mtkktl0kHV0/mqdefault.jpg)youtube.com","metadata":{}},{"cell_type":"markdown","source":"#Installing Stanza","metadata":{}},{"cell_type":"markdown","source":"Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.\n\nCitation: Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton and Christopher D. Manning. 2020. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. In Association for Computational Linguistics (ACL) System Demonstrations. 2020.\n\nhttps://stanfordnlp.github.io/stanza/","metadata":{}},{"cell_type":"code","source":"!pip install stanza","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T18:01:20.611211Z","iopub.execute_input":"2021-08-15T18:01:20.611655Z","iopub.status.idle":"2021-08-15T18:01:28.347707Z","shell.execute_reply.started":"2021-08-15T18:01:20.61162Z","shell.execute_reply":"2021-08-15T18:01:28.346415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import stanza\nstanza.download('hi')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:01:43.40425Z","iopub.execute_input":"2021-08-15T18:01:43.404632Z","iopub.status.idle":"2021-08-15T18:01:46.584033Z","shell.execute_reply.started":"2021-08-15T18:01:43.404598Z","shell.execute_reply":"2021-08-15T18:01:46.582352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = stanza.Pipeline('hi')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:01:56.147106Z","iopub.execute_input":"2021-08-15T18:01:56.147516Z","iopub.status.idle":"2021-08-15T18:01:57.331369Z","shell.execute_reply.started":"2021-08-15T18:01:56.147482Z","shell.execute_reply":"2021-08-15T18:01:57.330179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = nlp(\"जिसे संयुक्त राज्य में अमेरिकी स्वतन्त्रता युद्ध या क्रन्तिकारी युद्ध भी कहा जाता\")\nprint(doc)\nprint(doc.entities)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-08-15T18:02:03.541522Z","iopub.execute_input":"2021-08-15T18:02:03.542041Z","iopub.status.idle":"2021-08-15T18:02:03.696973Z","shell.execute_reply.started":"2021-08-15T18:02:03.541997Z","shell.execute_reply":"2021-08-15T18:02:03.693472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Document to Python Object","metadata":{}},{"cell_type":"code","source":"nlp = stanza.Pipeline('hi', processors='tokenize,pos')\ndoc = nlp('जिसे संयुक्त राज्य में अमेरिकी स्वतन्त्रता युद्ध या क्रन्तिकारी युद्ध भ') # doc is class Document\ndicts = doc.to_dict() # dicts is List[List[Dict]], representing each token / word in each sentence in the document","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:02:17.591554Z","iopub.execute_input":"2021-08-15T18:02:17.592006Z","iopub.status.idle":"2021-08-15T18:02:18.050677Z","shell.execute_reply.started":"2021-08-15T18:02:17.591966Z","shell.execute_reply":"2021-08-15T18:02:18.049489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Python Object to Document","metadata":{}},{"cell_type":"code","source":"from stanza.models.common.doc import Document\n\ndicts = [[{'id': 1, 'text': 'Test', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'misc': 'start_char=0|end_char=4'}, {'id': 2, 'text': 'sentence', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'misc': 'start_char=5|end_char=13'}, {'id': 3, 'text': '.', 'upos': 'PUNCT', 'xpos': '.', 'misc': 'start_char=13|end_char=14'}]] # dicts is List[List[Dict]], representing each token / word in each sentence in the document\ndoc = Document(dicts) # doc is class Document","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T18:02:24.746706Z","iopub.execute_input":"2021-08-15T18:02:24.747127Z","iopub.status.idle":"2021-08-15T18:02:24.754427Z","shell.execute_reply.started":"2021-08-15T18:02:24.747093Z","shell.execute_reply":"2021-08-15T18:02:24.753107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Tokenization and Sentence Segmentation","metadata":{}},{"cell_type":"code","source":"import stanza\n\nnlp = stanza.Pipeline(lang='hi', processors='tokenize')\ndoc = nlp('केंद्र की मोदी सरकार ने शुक्रवार को अपना अंतरिम बजट पेश किया. कार्यवाहक वित्त')\nfor i, sentence in enumerate(doc.sentences):\n    print(f'====== Sentence {i+1} tokens =======')\n    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:02:30.348925Z","iopub.execute_input":"2021-08-15T18:02:30.34929Z","iopub.status.idle":"2021-08-15T18:02:30.401447Z","shell.execute_reply.started":"2021-08-15T18:02:30.34926Z","shell.execute_reply":"2021-08-15T18:02:30.400363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Use the tokenizer just for sentence segmentation. To access segmented sentences, simply use","metadata":{}},{"cell_type":"code","source":"print([sentence.text for sentence in doc.sentences])","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:02:36.825387Z","iopub.execute_input":"2021-08-15T18:02:36.825767Z","iopub.status.idle":"2021-08-15T18:02:36.832516Z","shell.execute_reply.started":"2021-08-15T18:02:36.82573Z","shell.execute_reply":"2021-08-15T18:02:36.830883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Tokenization without Sentence Segmentation\n\n\"Sometimes you might want to tokenize your text given existing sentences (e.g., in machine translation). You can perform tokenization without sentence segmentation, as long as the sentences are split by two continuous newlines (\\n\\n) in the raw text. Just set tokenize_no_ssplit as True to disable sentence segmentation. Here is an example\"\n\nhttps://stanfordnlp.github.io/stanza/tokenize.html","metadata":{}},{"cell_type":"markdown","source":"#I included my account name in the doc below:","metadata":{}},{"cell_type":"code","source":"import stanza\n\nnlp = stanza.Pipeline(lang='hi', processors='tokenize', tokenize_no_ssplit=True)\ndoc = nlp('வழியில் நம்பிக்கை கொண்டிருந்த இவர்,MPWOLKE, பிறக')\nfor i, sentence in enumerate(doc.sentences):\n    print(f'====== Sentence {i+1} tokens =======')\n    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:06:17.385709Z","iopub.execute_input":"2021-08-15T18:06:17.386119Z","iopub.status.idle":"2021-08-15T18:06:17.436281Z","shell.execute_reply.started":"2021-08-15T18:06:17.386088Z","shell.execute_reply":"2021-08-15T18:06:17.435409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Start with Pretokenized Text\n\n\"In some cases, you might have already tokenized your text, and just want to use Stanza for downstream processing. In these cases, you can feed in pretokenized (and sentence split) text to the pipeline, as newline (\\n) separated sentences, where each sentence is space separated tokens. Just set tokenize_pretokenized as True to bypass the neural tokenizer.\"\n\n\"The code below shows an example of bypassing the neural tokenizer:\"\n\nhttps://stanfordnlp.github.io/stanza/tokenize.html","metadata":{}},{"cell_type":"code","source":"import stanza\n\nnlp = stanza.Pipeline(lang='hi', processors='tokenize', tokenize_pretokenized=True)\ndoc = nlp('केंद्र की मोदी सरकार ने शुक्रवार को अपना अंतरिम बजट पेश किया. कार्यवाहक वित्त')\nfor i, sentence in enumerate(doc.sentences):\n    print(f'====== Sentence {i+1} tokens =======')\n    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:03:40.967808Z","iopub.execute_input":"2021-08-15T18:03:40.968232Z","iopub.status.idle":"2021-08-15T18:03:41.002578Z","shell.execute_reply.started":"2021-08-15T18:03:40.968196Z","shell.execute_reply":"2021-08-15T18:03:41.000951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Exception: spaCy tokenizer is currently only allowed in English pipeline.","metadata":{}},{"cell_type":"code","source":"import stanza\n\nnlp = stanza.Pipeline(lang='hi', processors={'tokenize': 'spacy'}) # spaCy tokenizer is currently only allowed in English pipeline.\ndoc = nlp('केंद्र की मोदी सरकार ने शुक्रवार को अपना अंतरिम बजट पेश किया. कार्यवाहक वित्त')\nfor i, sentence in enumerate(doc.sentences):\n    print(f'====== Sentence {i+1} tokens =======')\n    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-15T18:06:41.168697Z","iopub.execute_input":"2021-08-15T18:06:41.169264Z","iopub.status.idle":"2021-08-15T18:06:41.207244Z","shell.execute_reply.started":"2021-08-15T18:06:41.169212Z","shell.execute_reply":"2021-08-15T18:06:41.205595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#After the snippet above I couldn't make anything more with Stanza since Hindi is not supported yet.\n\nException: spaCy tokenizer is currently only allowed in English pipeline. ","metadata":{"execution":{"iopub.status.busy":"2021-08-14T23:29:25.176406Z","iopub.execute_input":"2021-08-14T23:29:25.176817Z","iopub.status.idle":"2021-08-14T23:29:25.215951Z","shell.execute_reply.started":"2021-08-14T23:29:25.176787Z","shell.execute_reply":"2021-08-14T23:29:25.213652Z"}}},{"cell_type":"markdown","source":"Sources:\n    \n Stanza Package: A Python NLP Package for Many Human Languages\n https://stanfordnlp.github.io/stanza/sentiment.html   \n \n \n Text Processing for Indian Languages using Python\n https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/","metadata":{"execution":{"iopub.status.busy":"2021-08-14T22:28:18.079479Z","iopub.execute_input":"2021-08-14T22:28:18.079864Z","iopub.status.idle":"2021-08-14T22:28:18.087336Z","shell.execute_reply.started":"2021-08-14T22:28:18.079834Z","shell.execute_reply":"2021-08-14T22:28:18.086554Z"}}},{"cell_type":"markdown","source":"#Journeys of a thousand miles begin with a single step!  BioBERT wait for me with Stanza!","metadata":{}}]}