{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/RFR6UZX.jpg\" width=\"100%\"/>\n\n\n# 7. Public Models Revisited\n### [chaii - Hindi and Tamil Question Answering](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering) - A quick overview for QA noobs\n\nHi and welcome! This is the seventh kernel of the series `chaii - Hindi and Tamil Question Answering - A quick overview for QA noobs`.\n\n**In this short kernel, we will go over the current public models in the leaderboard.**\n\n**We have [already gone through public models](https://www.kaggle.com/julian3833/4-exploring-public-models-qa-for-qa-noobs/), but it was much earlier during the competition, and a lot of excellent public work has appeared since. Here we will catch-up with it.**\n\n\n---\n\nThe full series consists of the following notebooks:\n1. [The competition](https://www.kaggle.com/julian3833/1-the-competition-qa-for-qa-noobs)\n2. [The dataset](https://www.kaggle.com/julian3833/2-the-dataset-qa-for-qa-noobs)\n3. [The metric (Jaccard)](https://www.kaggle.com/julian3833/3-the-metric-jaccard-qa-for-qa-noobs)\n4. [Exploring Public Models](https://www.kaggle.com/julian3833/4-exploring-public-models-qa-for-qa-noobs/) \n5. [ü•á XLM-Roberta + Torch's extra data [LB: 0.749]](https://www.kaggle.com/julian3833/5-xlm-roberta-torch-s-extra-data-lb-0-749)\n6. [ü§ó Pre & post processing](https://www.kaggle.com/julian3833/6-pre-post-processing-qa-for-qa-noobs/)\n7. _[Public Models Revisited ](https://www.kaggle.com/julian3833/7-public-models-revisited-qa-for-qa-noobs/) (This notebook)_\n\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# 1. Searching for public models\n\nWe can access the most relevant models sorting by best score in the [Code's tab](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/code?competitionId=30060&sortBy=scoreDescending) of the competition.\n\n<p style=\"text-align:center\">\n<img src=\"https://i.imgur.com/8IHSmkJ.png\" width=\"50%\" height=\"50%\" />\n</p>\n\nRecall from the kernel - [3 - The metric (Jaccard) [QA for QA noobs]](https://www.kaggle.com/julian3833/3-the-metric-jaccard-qa-for-qa-noobs) - that the Jaccard coefficient saturates at 1.\n\nOn the other hand, private solutions in the leaderboard achieve up to `0.827` as of today.\n\n&nbsp;\n&nbsp;\n\n# 2. The best models as of today \n\nHere is a summary of the six best models with a brief description of each of them. We will go into more detail below.\n\n|#| Score | Kernel | Author| Lineage | Comments  | \n|:---| ----- | ----- | :----| :----|: ---|\n|1| 0.792 | [5 folds \\| INFER \\| combined model (0.792)](https://www.kaggle.com/kishalmandal/5-folds-infer-combined-model-0-792)   | [‚≠êÔ∏èKishal Mandal‚≠êÔ∏è](https://www.kaggle.com/kishalmandal) | Torch |  Torch + Nicholas + Different training |\n|2| 0.792 | [Reproduction of 0.792 notebook](https://www.kaggle.com/tkm2261/reproduction-of-0-792-notebook)    | [tkm2261](https://www.kaggle.com/tkm2261) | Torch | Almost equal to #1|\n|3| 0.782 | [how to QA with xlmr5](https://www.kaggle.com/adldotori/how-to-qa-with-xlmr5)    | [adldotori](https://www.kaggle.com/adldotori) | Torch |Almost equal to #1|\n|4| 0.778 | [chaii ensemble](https://www.kaggle.com/mihtw1/chaii-ensemble)    | [mihtw](https://www.kaggle.com/mihtw1)  | Torch + Abhishek | Ensemble of the 2 original public models|\n|5| 0.774 | [Chaii_QnA_Pytorch_MLQA_XQUAD_10Folds_Trained](https://www.kaggle.com/rickykonwar/chaii-qna-pytorch-mlqa-xquad-10folds-trained)    | [Manash J Konwar](https://www.kaggle.com/rickykonwar) | -- | Model with no visible public predecessor|\n|6| 0.772 | [chaii with postprocess](https://www.kaggle.com/mihtw1/chaii-with-postprocess)    |[mihtw](https://www.kaggle.com/mihtw1)  | Abhishek | Adds Nicholas' postprocessing to Abhishek's base model|\n\n\n__Note: I started this research on October 10. New models appear since then, but they as well fit in the categories outlines below.__\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\n\n__Torch's lineage__\n\nThe top 3 models are slight variations of the exact same code, which comes from torch's [**chaii QA - 5 Fold XLMRoberta Torch \\| Infer**](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer). We will study this line of succession in detail shortly, since is one of the most prominent kernels in the public domain right now:\n\n&nbsp;\n&nbsp;\n\n<p style=\"text-align:center\">\n<img src=\"https://i.imgur.com/1XZ2OQI.png\" width=\"50%\" height=\"50%\" />\n</p>\n\n&nbsp;\n&nbsp;\n\n__Abhishek's lineage__\n\nThe other prominent line of succession is one that comes from Abhishek's [**hello friends chaii pi lo**](https://www.kaggle.com/abhishek/hello-friends-chaii-pi-lo), reaching the 4th and 6th positions in the public models leaderboard.\n\nFor completeness, below you can see the top models till the 14th, reaching the two main public kernels from which most of the rest are derivatives. These two are highlighted in **bold**:\n\n&nbsp;\n&nbsp;\n\n\n|#| Score | Kernel | Author| Lineage | Description  | \n|:---| ----- | ----- | :----| :----|: ---|\n|7| 0.771 | [~chaii notes~](https://www.kaggle.com/victorsullivan/chaii-notes)    |[~Saravana Kumar~](https://www.kaggle.com/victorsullivan)  | Abhishek |Exact copy of Abhishek's |\n|8| 0.771 | [~hello friends chaii pi lo~](https://www.kaggle.com/swaana/hello-friends-chaii-pi-lo)    |[~Swati Anand~](https://www.kaggle.com/swaana)  | Abhishek | Exact copy of Abhishek's|\n|9| 0.771 | [**hello friends chaii pi lo**](https://www.kaggle.com/abhishek/hello-friends-chaii-pi-lo)    |[**Abhishek Thakur**](https://www.kaggle.com/abhishek)  | -- | |\n|10| 0.768 | [Chaii: Pytorch Inference [XLMRoberta-Large]](https://www.kaggle.com/maunish/chaii-pytorch-inference-xlmroberta-large)    |[Maunih dave](https://www.kaggle.com/maunish)  | -- | Model with no visible public predecessor |\n|11| 0.765 | [CHAIII QA Starter](https://www.kaggle.com/firefliesqn/chaiii-qa-starter)    |[fireflies](https://www.kaggle.com/firefliesqn)  | thedrcat's baseline | Almost equal to my notebook|\n|12| 0.765 | [chaii QA-Torch 5 fold with post-processing (.765)](https://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765)    |[Nicholas Broad üê¢](https://www.kaggle.com/nbroad)  | Torch | Adds a post-processing that is widely used afterwards|\n|13| 0.762 | [chaii-qa: rembert inference   1 fold üê∏](https://www.kaggle.com/nbroad/chaii-qa-rembert-inference-1-fold)    |[Nicholas Broad üê¢](https://www.kaggle.com/nbroad)  | Torch | Uses RemBERT instead of XLM-R|\n|14| 0.755 | [**chaii QA - 5 Fold XLMRoberta Torch \\| Infer**](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer)    |[**torch**](https://www.kaggle.com/rhtsingh)  | -- | |\n\n\n&nbsp;\n&nbsp;\n\nThe line of succession that comes from Abhishek's kernel is strongly mixed with Torch's one:\n\n\n\n<p style=\"text-align:center\">\n<img src=\"https://i.imgur.com/F1iLhxw.png\" width=\"50%\" height=\"50%\" />\n</p>\n\n\n__Miscellaneous__\n\nFinally, there are 3 models that don't belong to these two systems and might be very useful in an ensemble, since they might introduce variance:\n\n* [Chaii_QnA_Pytorch_MLQA_XQUAD_10Folds_Trained](https://www.kaggle.com/rickykonwar/chaii-qna-pytorch-mlqa-xquad-10folds-trained)\n* [Chaii: Pytorch Inference [XLMRoberta-Large]](https://www.kaggle.com/maunish/chaii-pytorch-inference-xlmroberta-large)\n* [CHAIII QA Starter](https://www.kaggle.com/firefliesqn/chaiii-qa-starter)\n\n&nbsp;\n&nbsp;\n\n__A crucial observation__\n\nWithout taking into account Nicholas' experiment with RemBERT, the following sentence is an important fact regarding the top tier public models:\n\n<h2 style=\"text-align: center; background-color:#C8FF33;padding:40px;border-radius: 30px;\">\n    All the top performing models are <b>XLM Roberta Large</b> coming from the checkpoint <b><a style='font-family:courier;' href='https://huggingface.co/deepset/xlm-roberta-large-squad2'>deepset/xlm-roberta-large-squad2</a></b>\n</h2>\n\n\n\nIn the rest of this notebook we will cover there three groups. First, we will describe the line of succession that comes from Torch's original code in detail. Second, we will mention, in a less detailed manner, Abhishek's lineage and the miscelaneous models.\n\n\nBefore jumping in, you might want to check the previous notebook exploring the early-days leaderboard [4 - Exploring Public Models [QA for QA noobs üáÆüá≥]](https://www.kaggle.com/julian3833/4-exploring-public-models-qa-for-qa-noobs). I also recommend having some knowledge of the Transformers library, because various concept with appear over and over again (like `Tokenizer`, `Model`, `Trainer`, `Dataset`). The [Quick tour](https://huggingface.co/transformers/quicktour.html) might be a good starting point for this.\n\n\n<p>\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n</p>\n\n# 3. Torch's lineage\n\nFirst of all, the first important insight is the following one:\n\n<h2 style=\"text-align: center; background-color:#C8FF33;padding:40px;border-radius: 30px;\">\n    The 3 top notebooks are all the same one\n</h2>\n\n[5 folds \\| INFER \\| combined model (0.792)](https://www.kaggle.com/kishalmandal/5-folds-infer-combined-model-0-792), [Reproduction of 0.792 notebook](https://www.kaggle.com/tkm2261/reproduction-of-0-792-notebook) and [how to QA with xlmr5](https://www.kaggle.com/adldotori/how-to-qa-with-xlmr5) have **exactly the same code**, with very slight differences of configuration.\n\nHow do I know? I downloaded the notebooks, turned them into py files, and compared them with `diff`.\n\n\nI am not sure who was the original author. There was a controversy, and a notebook being deleted (Please, if you followed the discussion on that topic, let me know in the comments and I will reference the original author here).\n\n**Update: The original author of this amazing work, as pointed out in this [comment thread](https://www.kaggle.com/julian3833/7-public-models-revisited-qa-for-qa-noobs/comments#1562967), was [‚≠êÔ∏èKishal Mandal‚≠êÔ∏è](https://www.kaggle.com/kishalmandal). If you happen to have forked his work through one of the other notebooks, give him an uptove to [5 folds \\| INFER \\| combined model (0.792)](https://www.kaggle.com/kishalmandal/5-folds-infer-combined-model-0-792) as well ;)**\n\nFirst we will go over the differences and then we will talk about the code itself.\n\n## Notebook #1 and #2 comparison\n\nThe only difference is that one does an average weighting while the other one does a manual weighting of the logits of the 5 folds:\n\nAverage weighting ([Reproduction of 0.792 notebook](https://www.kaggle.com/tkm2261/reproduction-of-0-792-notebook)):\n\n```python\nstart_logits = (start_logits1 + start_logits2 + start_logits3 + start_logits4+ start_logits5)/5\nend_logits = (end_logits1 + end_logits2 + end_logits3 + end_logits4 + end_logits5 )/5\n```\n\nManual weighting ([5 folds \\| INFER \\| combined model (0.792)](https://www.kaggle.com/kishalmandal/5-folds-infer-combined-model-0-792)):\n```python\nstart_logits = (0.135*start_logits1 + 0.105*start_logits2 + 0.135*start_logits3 + 0.60*start_logits4+ 0.105*start_logits5 )\nend_logits = (0.135*end_logits1 + 0.105*end_logits2 + 0.135*end_logits3 + 0.60*end_logits4 + 0.105*end_logits5 )\n```\n\nBoth use the same 5-fold checkpoints: [5-folds-roberta](https://www.kaggle.com/kishalmandal/5foldsroberta) by [kishalmandal](https://www.kaggle.com/kishalmandal/).\n\nMore on this dataset shorlty.\n\n## Notebooks #1 & #2 vs. #3 comparison\n\nNotebooks 1 and 2 use this configuration:\n\n```python\nmax_seq_length = 400\ndoc_stride = 135\n```\n\nAnd this dataset: [5-folds-roberta](https://www.kaggle.com/kishalmandal/5foldsroberta)\n\nWhile notebook 3 ([how to QA with xlmr5](https://www.kaggle.com/adldotori/how-to-qa-with-xlmr5)) uses this configuration:\n\n```python\nmax_seq_length = 384\ndoc_stride = 128\n```\n\nAnd this dataset: [chaii-xlmr-5-fold](https://www.kaggle.com/nguyenduongthanh/chaii-xlmr-5-fold)\n\n## The two 5-fold XLM-R datasets\n\nThe 2 datasets (**[5-folds-roberta](https://www.kaggle.com/kishalmandal/5foldsroberta)\n and [chaii-xlmr-5-fold](https://www.kaggle.com/nguyenduongthanh/chaii-xlmr-5-fold)**) used in the 3 top notebooks are the same, uploaded twice. I downloaded both datasets and ran a diff and they show **no difference at all**. Everything is exactly the same. \n\nThis command printed no output.\n`diff -r 5fold-roberta chaii-xlmr-5-fold`\n\nSo the difference between `0.782` and the `0.792` is those two configuration parameters, `max_seq_length` and `doc_stride`. I explain those two parameters in the previous notebook of the series, [6- ü§ó Pre & post-processing [QA for QA noobs üáÆüá≥]](https://www.kaggle.com/julian3833/6-pre-post-processing-qa-for-qa-noobs). You can also check the [QA notebook example](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb) by huggingface.\n\n\n## The code and the model\n\nAs it is common these days, the code is split into 2 notebooks: the one that trains and the one that infers.\nThe main reason for this split is the time limit for the submission. The train is performed separatedly because it takes much more time. A second, weaker argument, is that the inferences notebook cannot have access to internet in order to comply with the competition rules, while the training one can.\n\n\nThe original code for this comes from Torch's public kernels:\n<ul> \n    <li>For training: <a href=\"https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit\">chaii QA - 5 Fold XLMRoberta Torch | FIT</a> by <a href=\"https://www.kaggle.com/rhtsingh\">torch</a></li>\n    <li>For Inference: <a href=\"https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer\">chaii QA - 5 Fold XLMRoberta Torch | Infer</a> by <a href=\"https://www.kaggle.com/rhtsingh\">torch</a></li>\n</ul>    \n\n### Inference code\n\n#### Changes from the original code\nThere are just few changes from the original [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer).\n\nFirst of all, the post-processing block proposed by [Nicholas Broad](https://www.kaggle.com/nbroad) in his [notebook](https://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765):\n\n```python\nbad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"‚Äì\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"‚Äì\", \",\", \";\"]\n\ncleaned_preds = []\nfor pred, context in test[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    cleaned_preds.append(pred)\n\ntest[\"PredictionString\"] = cleaned_preds\n```\n\nSome other extra pre and post-processing is done too. I'm not sure about their origin:\n\n```python\ntest['context'] = test['context'].apply(lambda x: ' '.join(x.split()))\ntest['question'] = test['question'].apply(lambda x: ' '.join(x.split()))\n```\n\nAnd:\n\n```python\nsubmission = []\nfor p1, p2 in fin_preds.items():\n    p2 = \" \".join(p2.split())\n    p2 = p2.strip(punctuation)\n    submission.append((p1, p2))\n``` \n\nThe `eval_batch_size` (`32` $\\rightarrow$ `128`) and the `learning_rate` (`1.5e-5` $\\rightarrow$ `1e-5`) are changed from torch's original code too. \n\nAnd those are all the changes.\n\nThe `learning_rate` is not relevant in this inference notebook, but the `eval_batch_size` increase is probably accelerating the prediction time, which is definitely useful for ensembling. ","metadata":{}},{"cell_type":"markdown","source":"\n#### The [original code](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer)\n\nThe original code is a pure pytorch inference code, defining a custom `Dataset`, `DataLoader` and a `Model`.\nThese structures are the same as in the train code (because they should be consistent).\nWe will go into the training code soon. The inference code is getting predictions of 5 different models (that come from 5 different folds over the data) and averaging their logits to obtain a final prediction.\n\nYou probably need to understand the main concepts of pytorch to graps this notebook, but with a simple tutorial you might start getting it. [This video](https://www.youtube.com/watch?v=IC0_FRiX-sw) is a very short introduction to pytorch that might get you up and running to at least read the notebook.\n\nYou can also check these beginner tutorials hosted in the pytorch's homepage: \n* [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n* [Build Model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n\nIt goes like this:\n* Imports\n* A `Config` object. This is a common practice in kaggle to have all the configuration parameter on top of the kernel (not pytorch-specific)\n* `DatasetRetriever` is the custom `Dataset` object which is a standard pytorch structure. It should implement the methods `__len__` and `__getitem__`. See the tutorial [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for more information about this.\n* The `Model` is the neural network, subclassing `nn.Module`. You can see that, at its core, there is a `xlm-roberta-large-squad2` wrapped with a simple linear layer There is an extra dropout layer added, but it's commented out. See the tutorial \n[Build Model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) for more information about this.\n\n```python\nself.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\nself.qa_outputs = nn.Linear(config.hidden_size, 2)\n```\n\n* The pre and post-processing functions used are variations of the ones that come from the  [ü§ó QA fine-tuning example](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb) and that we covered in detail in the notebook [6- ü§ó Pre & post-processing [QA for QA noobs üáÆüá≥]](https://www.kaggle.com/julian3833/6-pre-post-processing-qa-for-qa-noobs).\n\n* The cell with the title `Data Factory` is building a `DataLoader` (See the tutorial [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for more information about this.)\n* The function `get_predictions` is getting the logits from the checkpoints saved during the training step.\n* Finally, the logits from the 5 different models are obtained and averaged to generate the final answers, using the `postprocess_qa_predictions` function that we covered in the notebook notebook [6- ü§ó Pre & post-processing [QA for QA noobs üáÆüá≥]](https://www.kaggle.com/julian3833/6-pre-post-processing-qa-for-qa-noobs).\n\n\n### The training code\n\nThe only available reference to the training code for the 3 top models is done in [5 folds | INFER | combined model (0.792)](https://www.kaggle.com/kishalmandal/5-folds-infer-combined-model-0-792).\n\nHe mentiones that the [5-fold checkpoints dataset](https://www.kaggle.com/kishalmandal/5foldsroberta) that is used for the predictions is not only one model but the sum of two:\n\nIf you go into the `result_dict.json` of each checkpoint in the dataset you will see that the first 2 checkpoints have 7 epochs, while the last 3 have only two.\nThis is because it is the mixture of two different train processes, as stated in the inference notebook.\n\nFolds 0 and fold-1 come from a training notebook with 7 epochs and folds 2, 3, and 4 come from another one, with 2 epochs:\n* Training for 2 epochs : <a href=\"https://www.kaggle.com/kishalmandal/chaii-fit-2-epochs-mlqa-xquad-chaii/\">chaii | FIT - 2 epochs | mlqa, xquad, chaii</a>\n* Training for 7 epochs with tamil_xquad: <a href=\"https://www.kaggle.com/kishalmandal/chaii-fit-7-epochs-extra-tamil-data/\">chaii | FIT - 7 epochs | Extra Tamil Data</a>\n\nThe train code is based in the original train code from torch as well: [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit).\n\nThe first one (with 2 epochs) changes nothing but the random seed and the amount of epochs, while the second one (with 7 epochs) does some more changes along with those:\n\n1. Uses more extra data (`squad_translated_tamil.csv`)\n2. Tweaks the following configuration parameters:\n  * `epochs` 1 $\\rightarrow$ 7\n  * `learning_rate` 1.5e-5 $\\rightarrow$ 3e-5\n  * `warmup_ratio` 0.1 $\\rightarrow$ 0.2\n  * `seed` 2021 $\\rightarrow$ 69\n  \n\nIt also modifies the function `get_optimizer_grouped_parameters()`, but it is actually not used in the code anywhere.\n\nWe took note about this extra dataset and we will cover it, among with the data provided by torch, in the following notebook of the series, where we plan to discuss all the relevant dataset for this competition.\n\n\nThe original train code provided by torch in [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit) overlaps strongly with the inference code, since the data structures involved are the same. Besides those data structures, the code defines a `Loss` (averaged cross entropy over both the start and end logits), a simple logger for the metric that takes averages, maximums and minimums, an various other utility functions and classes to define a stratified K-folds training-validation loop, storing the model of the epoch with the best validation loss for each fold.\n\n\nThe original model is a XLM-roberta large from the `deepset/xlm-roberta-large-squad2` checkpoint, which is then finetuned for the task at hand.\n","metadata":{"execution":{"iopub.status.busy":"2021-10-12T15:21:26.928182Z","iopub.execute_input":"2021-10-12T15:21:26.931388Z","iopub.status.idle":"2021-10-12T15:21:27.385818Z","shell.execute_reply.started":"2021-10-12T15:21:26.930794Z","shell.execute_reply":"2021-10-12T15:21:27.384112Z"}}},{"cell_type":"markdown","source":"# 4. Abhishek's lineage\n\n\nThe line of succession that comes from Abhishek's kernel is strongly mixed with Torch's one:\n\n<p style=\"text-align:center\">\n<img src=\"https://i.imgur.com/F1iLhxw.png\" width=\"50%\" height=\"50%\" />\n</p>\n\n\n|#| Score | Kernel | Author| Lineage | Comments  | \n|:---| ----- | ----- | :----| :----|: ---|\n|4| 0.778 | [chaii ensemble](https://www.kaggle.com/mihtw1/chaii-ensemble)    | [mihtw](https://www.kaggle.com/mihtw1)  | Torch + Abhishek | Ensemble of the 2 original public models|\n|6| 0.772 | [chaii with postprocess](https://www.kaggle.com/mihtw1/chaii-with-postprocess)    |[mihtw](https://www.kaggle.com/mihtw1)  | Abhishek | Adds Nicholas' postprocessing to Abhishek's base model|\n|9| 0.771 | [**hello friends chaii pi lo**](https://www.kaggle.com/abhishek/hello-friends-chaii-pi-lo)    |[**Abhishek Thakur**](https://www.kaggle.com/abhishek)  | -- | |\n\n\nThe original model by Abhishek is a XLM-roberta large from the `deepset/xlm-roberta-large-squad2` checkpoint, which is then finetuned for the task at hand. \n\nAbhishek released the train notebook: [hello friends tez se chaii train kar lo](https://www.kaggle.com/abhishek/hello-friends-tez-se-chaii-train-kar-lo).\n\nThe model uses a library of his own, tez, which can be found on github [here](https://github.com/abhishekkrthakur/tez).\n\n\n\n# 5. Miscelaneous\n\n\nFinally, there are 3 models that don't belong to these two systems and might be very useful in an ensemble, since they might introduce variance:\n\n* [Chaii_QnA_Pytorch_MLQA_XQUAD_10Folds_Trained](https://www.kaggle.com/rickykonwar/chaii-qna-pytorch-mlqa-xquad-10folds-trained)\n* [Chaii: Pytorch Inference [XLMRoberta-Large]](https://www.kaggle.com/maunish/chaii-pytorch-inference-xlmroberta-large)\n* [CHAIII QA Starter](https://www.kaggle.com/firefliesqn/chaiii-qa-starter)\n\nThese three models, although different from the ones of Torch and Abhishek, are based on pure `pytorch` or `tez` and strongly rensemble the main 2.\n\nAll of the use the pre and post-processing functionality that we covered in detail in the previous notebook of this series [ü§ó Pre & post processing](https://www.kaggle.com/julian3833/6-pre-post-processing-qa-for-qa-noobs/).\n\n&nbsp;\n&nbsp;\n\n<!-- What's next? Check the next notebook of the series, [8 - Ensemble [QA for QA noobs üáÆüá≥]](https://www.kaggle.com/julian3833/8-ensemble-qa-for-qa-noobs), where we combine the best perfoming model from Torch's lineage with Abhishek's model to obtain a score of `pending`. -->\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\n## Remember to upvote the notebook if you found it useful! ü§ó\n\n","metadata":{}}]}