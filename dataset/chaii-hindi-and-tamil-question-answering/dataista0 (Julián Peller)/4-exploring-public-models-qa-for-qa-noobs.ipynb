{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/RFR6UZX.jpg\" width=\"100%\"/>\n\n\n# _Note: I am working on an new version of this notebook, with the new best scoring models_\n\n\n\n\n# 4. Exploring Public Models\n### [chaii - Hindi and Tamil Question Answering](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering) - A quick overview for QA noobs\n\nHi and welcome! This is the fourth kernel of the series `chaii - Hindi and Tamil Question Answering - A quick overview for QA noobs`.\n\n**In this short kernel we will go over the current public models in the leaderboard.**\n\n\n---\n\nThe full series consists of the following notebooks:\n1. [The competition](https://www.kaggle.com/julian3833/1-the-competition-qa-for-qa-noobs)\n2. [The dataset](https://www.kaggle.com/julian3833/2-the-dataset-qa-for-qa-noobs)\n3. [The metric (Jaccard)](https://www.kaggle.com/julian3833/3-the-metric-jaccard-qa-for-qa-noobs)\n4. _[Exploring Public Models](https://www.kaggle.com/julian3833/4-exploring-public-models-qa-for-qa-noobs/) (This notebook)_\n5. [ðŸ¥‡ XLM-Roberta + Torch's extra data [LB: 0.749]](https://www.kaggle.com/julian3833/5-xlm-roberta-torch-s-extra-data-lb-0-749)\n6. [ðŸ¤— Pre & post processing](https://www.kaggle.com/julian3833/6-pre-post-processing-qa-for-qa-noobs/)\n\n\nThis is an ongoing project, so expect more notebooks to be added to the series soon. Actually, we are currently working on the following ones:\n* Exploring Public Models Revisited\n* Reviewing `squad2`, `mlqa` and others\n* About `xlm-roberta-large-squad2`\n* Own improvements\n\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# 1. Searching for public models\n\nWe can access the most relevant models sorting by best score in the [Code's tab](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/code?competitionId=30060&sortBy=scoreDescending) of the competition.\n\n<p style=\"text-align:center\">\n<img src=\"https://i.imgur.com/rsDxwFm.png\" width=\"50%\" height=\"50%\" />\n</p>\n\nRecall from the previous kernel - [3 - The metric (Jaccard) [QA for QA noobs]](https://www.kaggle.com/julian3833/3-the-metric-jaccard-qa-for-qa-noobs) - that the Jaccard coefficient saturates at 1. The competition has started 3 days ago, so a public model of `0.735` is quite impressive.\n\n&nbsp;\n&nbsp;\n\n# 2. The best models as of today\n\nHere is a summary of the four best models with a brief description of each of them. We will go into more detail below.\n\n| Score | Kernel |  Model | Description  | \n| ----- | ----- | :----|: ---|\n| 0.735 | [ChAII - EDA & Baseline](https://www.kaggle.com/thedrcat/chaii-eda-baseline)    | `xlm-roberta-large-squad2` | Model from 3rd position notebook [chaii-QA: multi-lingual pretrained baseline](https://www.kaggle.com/nbroad/chaii-qa-multi-lingual-pretrained-baseline), fine-tuned 1 epoch. |\n| 0.723 |[chaii chaii](https://www.kaggle.com/abhishek/chaii-chaii)   | `xmlrob` | Uses a custom pretraining steps that is private (and I cannot analyze) |\n| 0.571 |[chaii-QA: multi-lingual pretrained baseline](https://www.kaggle.com/nbroad/chaii-qa-multi-lingual-pretrained-baseline)    | `xlm-roberta-large-squad2` | HF pipeline API. No fine-tuning. User uploaded this model to Kaggle as a [dataset](https://www.kaggle.com/nbroad/xlm-roberta-squad2). | \n| 0.422 |[Chaii - Understanding the Comp + EDA ðŸ“ˆ + Baseline](https://www.kaggle.com/heyytanay/chaii-understanding-the-comp-eda-baseline)    | `bert-base-multilingual-cased-finetuned-squad` | HF pipeline API. Private model.  | \n\n&nbsp;\n&nbsp;\n\nThe four notebooks are using huggingface's [transformers](https://huggingface.co/transformers/) ðŸ¤—.\n\nI strongly recommend going over the [Quick tour](https://huggingface.co/transformers/quicktour.html) of the library, at least. That way you will get familiar with the main concepts that will appear again and again: `Tokenizer`, `Model`, `Trainer`, `Dataset`, and checkpoints (things like: `xml-roberta-large-squad2`).\n\nI have already gone over these topics but on other notebooks in other competitions ([here](https://www.kaggle.com/julian3833/2-learning-out-of-the-box-roberta-lb-0-53) and [here](https://www.kaggle.com/julian3833/1-learning-out-of-the-box-bert-lb-0-8102)).\n\n\n<p>\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n</p>\n\n\n## 2.1. `0.735` - ðŸ¤—  `xlm-roberta-large-squad2` by [thedrcat](https://www.kaggle.com/thedrcat)\n\nThe most relevant public model at stake right now is the `xlm-roberta-large-squad2` by the user [thedrcat](https://www.kaggle.com/thedrcat).\n\nThe notebook is large but conceptually straightforward: he fine-tunes the `xlm-roberta-large-squad2` that [nbroad]((https://www.kaggle.com/nbroad) ported from the modelhub for `1 epoch`, using the code provided by huggingface as a reference for doing this, obtaining an amazing `0.735` leaderboard score.\n\n[thedrcat](https://www.kaggle.com/thedrcat) adds also the following items:\n* He solves the problem of using the `datasets` package in the offline-fashion the competition requires\n* He finds good training parameters that increases the LB score \n\n(_This paragraph is a WIP, I just wanted at least to mention this to make honour to these features of the notebook that I overlooked before and are really important_)\n\n\nBoth the 1st and the 2nd notebooks reference the same fine-tuning example: [ðŸ¤— QA fine-tuning example](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb) as a reference to base their fine-tuning code.\n\nThis is a `xlm-roberta-large` already finetuned for a Question Answering dataset (`squad2`). I assume that this model is multi-lingual already and knows how to handle Indian languages (we will go deeper into this in future notebooks).\n\nI took note of the following items to go into further details in future notebooks:\n* The [ðŸ¤— QA fine-tuning example](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n* The QA dataset `squad2`\n* The model `xlm-roberta-large`\n\n<p>\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n</p>\n\n\n## 2.2 `0.422` & `0.571` - Pipeline out-of-the-box solutions by [nbroad]((https://www.kaggle.com/nbroad) and [heyytanay](https://www.kaggle.com/heyytanay)\n\nThe [3rd](https://www.kaggle.com/nbroad/chaii-qa-multi-lingual-pretrained-baseline) and the [4th](https://www.kaggle.com/heyytanay/chaii-understanding-the-comp-eda-baseline) notebooks are out-of-the-box baselines models with the [pipeline](https://huggingface.co/transformers/quicktour.html#getting-started-on-a-task-with-a-pipeline) interface,without fine-tuning at all, with short snippets of code of around 10 lines each (although the 4th one does EDA before).\n\nThe pipeline interface is the first thing you encounter reading the quick tour for huggingface. It's very straightforward, so you should [check it out](https://huggingface.co/transformers/quicktour.html#getting-started-on-a-task-with-a-pipeline) if you haven't already!:\n\n\nThey are very short notebooks and I encourage you to check them. Actually, I summarized the 3rd one [here](https://www.kaggle.com/julian3833/7-line-submission-lb-0-493) in 7 lines of code.\n\nFinally, [nbroad](https://www.kaggle.com/nbroad) ported the model he/she used to Kaggle as a [dataset](https://www.kaggle.com/nbroad/xlm-roberta-squad2). This is a requirement since the competition doesn't allow to use Internet and huggingface's modelhub requires internet to be accessed.\n\nYou can check [the first notebook of this series](https://www.kaggle.com/julian3833/1-the-competition-qa-for-qa-noobs) to understand this restriction and how to include datasets as inputs to your notebooks.\n\n<p>\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n</p>\n\n\n\n## 2.3. `0.723` - ðŸ¤— `xlm-roberta-large-squad2` by [abhishek](https://www.kaggle.com/abhishek)\n\nThis notebook uses a fine-tuned `xlm-roberta-large-squad2`, but the fine-tuning happens in a private notebook, and doesn't seem to be explained anywhere. So we cannot dig much here. There are a lot of references to the same training example used in the 1st place notebook, so this is probably a place to dig in ([ðŸ¤— QA fine-tuning example](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)).\n\nThe notebook loads checkpoints from [this dataset](https://www.kaggle.com/abhishek/f0test). Since the results are close and the competition just started, it is possibly a similar work as the one of  [thedrcat](https://www.kaggle.com/thedrcat).\n\n&nbsp;\n&nbsp;\n\n# References\n\n## Notebooks\n* [ChAII - EDA & Baseline](https://www.kaggle.com/thedrcat/chaii-eda-baseline)\n* [chaii chaii](https://www.kaggle.com/abhishek/chaii-chaii)\n* [chaii-QA: multi-lingual pretrained baseline](https://www.kaggle.com/nbroad/chaii-qa-multi-lingual-pretrained-baseline)\n* [Chaii - Understanding the Comp + EDA ðŸ“ˆ + Baseline](https://www.kaggle.com/heyytanay/chaii-understanding-the-comp-eda-baseline) \n\n## Datasets\n* [xlm-roberta-squad2](https://www.kaggle.com/nbroad/xlm-roberta-squad2)\n\n\n\n\n\n## Other resources\n* [ðŸ¤— QA fine-tuning example reference](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n\n## What's next?\n\nStay tuned for a deeper analysis of the main items involved in the current solutions (`squad2`, `xlm-roberta` and `ðŸ¤— QA fine-tuning example reference`).\n\nWe will be monitoring the public notebooks and write a revisit of this one when new models are shared.\n\n\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\n## Remember to upvote the notebook if you found it useful! ðŸ¤—\n\n","metadata":{}}]}