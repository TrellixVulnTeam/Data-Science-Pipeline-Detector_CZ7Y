{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport pandas as pd\n\nfrom transformers import AutoTokenizer, AutoConfig\n\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:11.567289Z","iopub.execute_input":"2021-10-30T09:44:11.56769Z","iopub.status.idle":"2021-10-30T09:44:22.426075Z","shell.execute_reply.started":"2021-10-30T09:44:11.567604Z","shell.execute_reply":"2021-10-30T09:44:22.424815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4'\ntrainFile = '../input/chaii-hindi-and-tamil-question-answering/train.csv'\ntestFile = '../input/chaii-hindi-and-tamil-question-answering/test.csv'","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:22.430293Z","iopub.execute_input":"2021-10-30T09:44:22.430944Z","iopub.status.idle":"2021-10-30T09:44:22.43602Z","shell.execute_reply.started":"2021-10-30T09:44:22.430893Z","shell.execute_reply":"2021-10-30T09:44:22.434954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_layer = hub.KerasLayer(tfhub_handle_encoder,trainable=True)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\nconfig = AutoConfig.from_pretrained('bert-base-multilingual-cased')\n\ntokenizer.save_pretrained('bert-base-multilingual-cased-tokenizer')\nconfig.save_pretrained('bert-base-multilingual-cased-tokenizer')","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:22.437785Z","iopub.execute_input":"2021-10-30T09:44:22.438347Z","iopub.status.idle":"2021-10-30T09:44:55.631433Z","shell.execute_reply.started":"2021-10-30T09:44:22.438301Z","shell.execute_reply":"2021-10-30T09:44:55.630467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindf = pd.read_csv(trainFile)\ntestdf = pd.read_csv(testFile)\nprint(traindf.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:55.63535Z","iopub.execute_input":"2021-10-30T09:44:55.635687Z","iopub.status.idle":"2021-10-30T09:44:56.422057Z","shell.execute_reply.started":"2021-10-30T09:44:55.635644Z","shell.execute_reply":"2021-10-30T09:44:56.421151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'bert-chaii'\nmax_seq_length = 384","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.423846Z","iopub.execute_input":"2021-10-30T09:44:56.424202Z","iopub.status.idle":"2021-10-30T09:44:56.429719Z","shell.execute_reply.started":"2021-10-30T09:44:56.424158Z","shell.execute_reply":"2021-10-30T09:44:56.428639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.431652Z","iopub.execute_input":"2021-10-30T09:44:56.432366Z","iopub.status.idle":"2021-10-30T09:44:56.46358Z","shell.execute_reply.started":"2021-10-30T09:44:56.43232Z","shell.execute_reply":"2021-10-30T09:44:56.462564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.46508Z","iopub.execute_input":"2021-10-30T09:44:56.465695Z","iopub.status.idle":"2021-10-30T09:44:56.481244Z","shell.execute_reply.started":"2021-10-30T09:44:56.465648Z","shell.execute_reply":"2021-10-30T09:44:56.480205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def context_offsets_index(offsets):\n    last_start = 0\n    idx = 0\n    for x, (ostart, oend) in enumerate(offsets):\n        if ostart >= last_start:\n            idx = x\n            last_start = ostart\n        else:\n            break\n    return idx+1","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.482996Z","iopub.execute_input":"2021-10-30T09:44:56.483651Z","iopub.status.idle":"2021-10-30T09:44:56.495216Z","shell.execute_reply.started":"2021-10-30T09:44:56.483607Z","shell.execute_reply":"2021-10-30T09:44:56.494095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_samples(question, context, start_char_idx=None, answer_text=None):\n\n    encoding = tokenizer.encode_plus(question, context,  return_tensors='np',\n                                     max_length=384, stride=128, return_overflowing_tokens=True,\n                                     padding=\"max_length\", truncation=True,\n                                     return_offsets_mapping=True)\n\n    input_word_ids = encoding.input_ids\n    token_type_ids = encoding.token_type_ids\n    attention_mask = encoding.attention_mask\n    offsets = encoding.offset_mapping\n\n    # check if end character index is in the context\n    end_char_idx = start_char_idx + len(answer_text)\n    if end_char_idx >= len(context):\n        return\n\n    # mark all the character indexes in context that are also in answer\n    is_char_in_ans = [0] * len(context)\n    for idx in range(start_char_idx, end_char_idx):\n        is_char_in_ans[idx] = 1\n    \n    inputs = []\n    for x in range(len(input_word_ids)):\n        ans_token_idx = []\n        context_index = context_offsets_index(offsets[x])\n        # find all the tokens that are in the answers\n        sample_offsets = offsets[x][context_index:]\n        #print('sample_offsets=', sample_offsets.shape)\n        for idx, (start, end) in enumerate(sample_offsets):\n            if sum(is_char_in_ans[start:end]) > 0:\n                ans_token_idx.append(idx)\n\n        if len(ans_token_idx) == 0:\n            continue\n\n        start_token_idx = ans_token_idx[0]\n        end_token_idx = ans_token_idx[-1]\n        \n        print('Question=', question)\n        print('answer text=', answer_text)\n        print('string=', tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_word_ids[x][context_index:][start_token_idx:end_token_idx+1])))\n\n        input = {'input_word_ids': input_word_ids[x], 'input_type_ids': token_type_ids[x],\n                 'input_mask': attention_mask[x], 'start_token_idx': start_token_idx,\n                 'end_token_idx': end_token_idx}\n\n        inputs.append(input)\n\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.497389Z","iopub.execute_input":"2021-10-30T09:44:56.497795Z","iopub.status.idle":"2021-10-30T09:44:56.51187Z","shell.execute_reply.started":"2021-10-30T09:44:56.497753Z","shell.execute_reply":"2021-10-30T09:44:56.51063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chaii_train_data(df):\n    train_data_samples = []\n    for idx, row in df.iterrows():\n        question = row['question']\n        context = row['context']\n        answer_start = row['answer_start']\n        answer_text = row['answer_text']\n        language = row['language']\n\n        samples = make_samples(question, context, answer_start, answer_text)\n\n        for s in samples:\n            train_data_samples.append(s)\n\n    return train_data_samples","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.515999Z","iopub.execute_input":"2021-10-30T09:44:56.516953Z","iopub.status.idle":"2021-10-30T09:44:56.525882Z","shell.execute_reply.started":"2021-10-30T09:44:56.51686Z","shell.execute_reply":"2021-10-30T09:44:56.524878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_bert_inputs(samples):\n    dataset_dict = {\n        \"input_word_ids\": [],\n        \"input_type_ids\": [],\n        \"input_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n\n    for item in samples:\n        for key in dataset_dict:\n            dataset_dict[key].append(item[key])\n\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [dataset_dict[\"input_word_ids\"],\n         dataset_dict[\"input_mask\"],\n         dataset_dict[\"input_type_ids\"]]\n\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.527902Z","iopub.execute_input":"2021-10-30T09:44:56.528286Z","iopub.status.idle":"2021-10-30T09:44:56.541614Z","shell.execute_reply.started":"2021-10-30T09:44:56.528246Z","shell.execute_reply":"2021-10-30T09:44:56.540351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def buildModel():\n    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n    input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n    \n    bert_inputs = {'input_word_ids': input_word_ids, 'input_mask': input_mask, 'input_type_ids': input_type_ids}\n    bert_outputs = bert_layer(bert_inputs)\n\n    pooled_output = bert_outputs[\"pooled_output\"]      # [batch_size, 768].\n    sequence_output = bert_outputs[\"sequence_output\"]  # [batch_size, seq_length, 768].\n\n    start_logits = tf.keras.layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n    start_logits = tf.keras.layers.Flatten()(start_logits)\n    end_logits = tf.keras.layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n    end_logits = tf.keras.layers.Flatten()(end_logits)\n    start_probs = tf.keras.layers.Activation(tf.keras.activations.softmax)(start_logits)\n    end_probs = tf.keras.layers.Activation(tf.keras.activations.softmax)(end_logits)\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.544237Z","iopub.execute_input":"2021-10-30T09:44:56.545453Z","iopub.status.idle":"2021-10-30T09:44:56.558596Z","shell.execute_reply.started":"2021-10-30T09:44:56.545407Z","shell.execute_reply":"2021-10-30T09:44:56.557362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    train_samples = chaii_train_data(traindf)\n    x, y = create_bert_inputs(train_samples)\n\n    print(f\"{len(train_samples)} training points created.\")\n\n    model = buildModel()\n\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = tf.keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    model.summary()\n\n    model.fit(x, y, epochs=17, batch_size=8)\n    model.save(model_name)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.560469Z","iopub.execute_input":"2021-10-30T09:44:56.561016Z","iopub.status.idle":"2021-10-30T09:44:56.573992Z","shell.execute_reply.started":"2021-10-30T09:44:56.560974Z","shell.execute_reply":"2021-10-30T09:44:56.572805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model():\n    model = tf.keras.models.load_model(model_name)\n    for idx, row in testdf.iterrows():\n        id = row['id']\n        question = row['question']\n        context = row['context']\n        encoding = tokenizer.encode_plus(question, context, return_tensors='np',\n                                            max_length=384, stride=128, return_overflowing_tokens=True,\n                                            padding=\"max_length\", truncation=True,\n                                            return_offsets_mapping=True)\n\n        input_word_ids = encoding.input_ids\n        token_type_ids = encoding.token_type_ids\n        attention_mask = encoding.attention_mask\n        offsets = encoding.offset_mapping\n\n        input = {'input_word_ids': input_word_ids, 'input_type_ids': token_type_ids, 'input_mask': attention_mask}\n\n        x = [input[\"input_word_ids\"],input[\"input_mask\"],input[\"input_type_ids\"]]\n\n        pred_start, pred_end = model.predict(x)\n\n        max = 0\n        answer = \"unknown\"\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            print('start probability=', start.max(), ' , end probability=', end.max())\n            start = np.argmax(start)\n            end = np.argmax(end)\n            \n            context_index = context_offsets_index(offsets[idx])\n            # find all the tokens that are in the answers\n            sample_offsets = offsets[idx][context_index:]\n\n            if start >= end : continue\n            if end >= len(sample_offsets): continue\n                \n            char_start = sample_offsets[start][0]\n            char_end = sample_offsets[end][1]\n            answer = context[char_start:char_end]\n\n            print(\"id=\", id, \", Q=\",  question, \", A=\", answer)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.575317Z","iopub.execute_input":"2021-10-30T09:44:56.575834Z","iopub.status.idle":"2021-10-30T09:44:56.591503Z","shell.execute_reply.started":"2021-10-30T09:44:56.575756Z","shell.execute_reply":"2021-10-30T09:44:56.590393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model()\ntest_model()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T09:44:56.593347Z","iopub.execute_input":"2021-10-30T09:44:56.593748Z","iopub.status.idle":"2021-10-30T10:05:51.369044Z","shell.execute_reply.started":"2021-10-30T09:44:56.593706Z","shell.execute_reply":"2021-10-30T10:05:51.367954Z"},"trusted":true},"execution_count":null,"outputs":[]}]}