{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Approach summaries. Knitty-gritties in corresponding versions\n\n- V1. V1: Blind XLMR, followed by Rembert followed by XLMR (squad1)\n- V2. Quantile scoring approach\n- V6. 3 Remberts, 2 XLMRs predicted, their top answer cleaned text and question embedding difference is fed to an LGBM Ranker. If all models are useless according to LGBM (How to guess this? +/- values?, fallback to rembert sq2ep3 ft)\n- V10: Top 3 answers from 3 models(5 fold Muril, Single rembert, 5 fold XLMR) are fed to an LGBM booster optimised for ndcg.\n- V16: Top 3 answers from 3 models(5 fold Muril, Single rembert, 5 fold XLMR) retrieve candidate texts using Stanza. These new question-goldensent pair are fed to bert ft on tydi ft on chaii. (V17, V18 = individual language stats)\n- V19-?: All three models ensemble by mapping offsets to characters and mean of softmax scores. Then retokenize using XLMR and assign mean of character scores to a given token and post process as usual. Few versions are variants of same approach.\n- (This notebook): Rudimentary, my dear Watson!\n\nJust get intersection of cleaned answers from all three models and use scaled scores to rank them and generate outputs.","metadata":{}},{"cell_type":"code","source":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links=../input/hf-datasets/wheels datasets -qq\n!pip uninstall transformers -qq -y\n!pip install --no-index --find-links=../input/transformers-latest-model transformers -qq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-31T06:42:05.736814Z","iopub.execute_input":"2021-10-31T06:42:05.737248Z","iopub.status.idle":"2021-10-31T06:42:26.831984Z","shell.execute_reply.started":"2021-10-31T06:42:05.737145Z","shell.execute_reply":"2021-10-31T06:42:26.831128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nimport numpy as np, json\nfrom hf_qa_utils import *\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import default_data_collator\ndata_collator = default_data_collator\n\ntorch.set_grad_enabled(False)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice_id = 0 if torch.cuda.is_available() else -1\n\nargs = TrainingArguments(\n    f\"dummy\",\n    report_to=[\"tensorboard\"],\n    per_device_eval_batch_size=256,\n)\n\nmkeys = [\"muril\", \"rembert\", \"xlmr\"]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-31T06:42:26.835285Z","iopub.execute_input":"2021-10-31T06:42:26.835541Z","iopub.status.idle":"2021-10-31T06:42:37.354399Z","shell.execute_reply.started":"2021-10-31T06:42:26.835494Z","shell.execute_reply":"2021-10-31T06:42:37.353615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\ntest_ds = Dataset.from_pandas(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:42:37.355817Z","iopub.execute_input":"2021-10-31T06:42:37.356069Z","iopub.status.idle":"2021-10-31T06:42:37.400668Z","shell.execute_reply.started":"2021-10-31T06:42:37.356038Z","shell.execute_reply":"2021-10-31T06:42:37.400042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get top 5 candidates per model type","metadata":{}},{"cell_type":"code","source":"topk = 5\nSCALE_SCORES = True\ndef get_top_candidates(candidates):\n    preddf = []\n    for qid, arr in candidates.items():\n        arr = sorted(arr, reverse=True, key=lambda x: x[\"score\"])[:topk]\n        for rec in arr:\n            rec[\"id\"] = qid\n        preddf.extend(arr)\n    \n    preddf = pd.DataFrame(preddf)\n    preddf = preddf.sort_values(\"score\", ascending=False)\n    preddf[\"aclean\"] = preddf[\"text\"].apply(clean_answer)\n    preddf = preddf.groupby([\"id\", \"aclean\"])[\"score\"].max().reset_index()\n    if SCALE_SCORES:\n        mn, mx = preddf[\"score\"].min(), preddf[\"score\"].max()\n        preddf[\"score\"] = (preddf[\"score\"]-mn)/(mx-mn)\n\n    return preddf\n","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:49:12.220183Z","iopub.execute_input":"2021-10-31T06:49:12.221007Z","iopub.status.idle":"2021-10-31T06:49:12.230537Z","shell.execute_reply.started":"2021-10-31T06:49:12.22096Z","shell.execute_reply":"2021-10-31T06:49:12.229632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"muril_ckpts = [\"../input/muril-finetuning-indicx-on-squad2-epoch-2/muril_lg_indix_sq2ep1/checkpoint-3480\", \\\n               \"../input/folds-1-and-2-muril-indicx-sq2ep2-finetuning/muril_lg_indix_sq2ep1_fold1/checkpoint-3463\", \\\n               \"../input/folds-1-and-2-muril-indicx-sq2ep2-finetuning/muril_lg_indix_sq2ep1_fold2/checkpoint-3452\", \\\n               \"../input/folds-3-and-4-muril-indicx-sq2ep2-finetuning/muril_lg_indix_sq2ep1_fold3/checkpoint-3485\", \\\n               \"../input/folds-3-and-4-muril-indicx-sq2ep2-finetuning/muril_lg_indix_sq2ep1_fold4/checkpoint-3472\"\n              ]\nrembert_ckpts = [\"../input/rembert-finetuning-indicx-on-sq2-epoch3/rembert_indicx_over_squad2/checkpoint-1021\"]\nxlmr_ckpts = [f\"../input/folds-consolidated-xlmr-qa-finetune-on-indix/fold{i}\" for i in range(5)]\n\nckpt_meta = [(\"muril\", muril_ckpts), (\"rembert\", rembert_ckpts), (\"xlmr\", xlmr_ckpts)]\n\npred_dfs = []\nfor model_type, ckpts in ckpt_meta:\n    tokenizer = AutoTokenizer.from_pretrained(ckpts[0])\n    tkwargs = {\"tokenizer\": tokenizer} \n    ds_feats = test_ds.map(prepare_validation_features, batched=True, remove_columns=test_ds.column_names, fn_kwargs=tkwargs)\n    \n    starts, ends = None, None\n    for mname in ckpts:\n        model = AutoModelForQuestionAnswering.from_pretrained(mname)\n        trainer = Trainer(model, args, data_collator=data_collator, tokenizer=tokenizer)\n        raw_vals = trainer.predict(ds_feats)\n        if starts is None:\n            starts, ends = raw_vals.predictions\n        else:\n            starts += raw_vals.predictions[0]\n            ends += raw_vals.predictions[1]\n    starts /= len(ckpts)\n    ends /= len(ckpts)\n    \n    ds_feats.set_format(type=ds_feats.format[\"type\"], columns=list(ds_feats.features.keys()))\n    _, candidates = postprocess_qa_predictions(test_ds, ds_feats, (starts, ends), \\\n                                               cls_token_id=tokenizer.cls_token_id, n_best_size=5, \\\n                                               pp_cleanup=False, return_candidates=True)\n    cdf = get_top_candidates(candidates)\n    pred_dfs.append((model_type, cdf))","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:49:12.464474Z","iopub.execute_input":"2021-10-31T06:49:12.465028Z","iopub.status.idle":"2021-10-31T06:54:16.523638Z","shell.execute_reply.started":"2021-10-31T06:49:12.464995Z","shell.execute_reply":"2021-10-31T06:54:16.52279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate best answer from all model types","metadata":{}},{"cell_type":"code","source":"highpred = pd.merge(pred_dfs[0][-1], pred_dfs[1][-1], \\\n                    on=[\"id\", \"aclean\"], how=\"outer\", suffixes=[\"_\"+pred_dfs[0][0], \"_\"+pred_dfs[1][0]])\nhighpred = pd.merge(highpred, pred_dfs[2][-1], on=[\"id\", \"aclean\"], how=\"outer\")\nhighpred.rename(columns={\"score\": f\"score_{pred_dfs[2][0]}\"}, inplace=True)\n\nmkeys = list(map(lambda x:x[0], ckpt_meta))\nfor mkey in mkeys:\n    minval = 0 if SCALE_SCORES else highpred[~highpred[f\"score_{mkey}\"].isna()][f\"score_{mkey}\"].min()-1\n    highpred[f\"score_{mkey}\"].fillna(minval, inplace=True)\n\nhighpred['tot_score'] = highpred[f\"score_{mkeys[0]}\"]\nfor mkey in mkeys[1:]:\n    highpred['tot_score'] += highpred[f\"score_{mkey}\"]\n\nhighpred = highpred.sort_values(\"tot_score\", ascending=False) \nhighpred = highpred.groupby([\"id\"]).head(1).reset_index(drop=True) \nhighpred.rename(columns={\"aclean\": \"PredictionString\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:54:16.527392Z","iopub.execute_input":"2021-10-31T06:54:16.527603Z","iopub.status.idle":"2021-10-31T06:54:16.568378Z","shell.execute_reply.started":"2021-10-31T06:54:16.527578Z","shell.execute_reply":"2021-10-31T06:54:16.567624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.merge(test_df, highpred, on=\"id\", how=\"left\")\ntest_df['PredictionString'].fillna('', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:54:16.570232Z","iopub.execute_input":"2021-10-31T06:54:16.570465Z","iopub.status.idle":"2021-10-31T06:54:16.586599Z","shell.execute_reply.started":"2021-10-31T06:54:16.570442Z","shell.execute_reply":"2021-10-31T06:54:16.585863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Postprocessing","metadata":{}},{"cell_type":"code","source":"import re\nyear_ptrn = re.compile(\"\\d{4}\")\n\ntime_prefixes = [\"கி.மு\", \"கி.பி\", \" ई\", \"ई.पू\", \"वर्ष\", \"सन\"]\ndef update_year_answer(pred_ans):\n    if any([tp in pred_ans for tp in time_prefixes]):\n        return pred_ans\n    ypreds = year_ptrn.findall(pred_ans)\n    if len(ypreds)!=1:\n        return pred_ans\n    return ypreds[0]\n\nyears = [\"எந்த ஆண்டு\", \"किस वर्ष\", \"किस साल\"]\nis_ans_year = (test_df[\"question\"].str.contains(\"|\".join(years), regex=True))\nif is_ans_year.any():\n    test_df.loc[is_ans_year, \"PredictionString\"] = test_df.loc[is_ans_year, \"PredictionString\"].apply(update_year_answer)\ntest_df['PredictionString'].fillna('', inplace=True)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-31T06:54:16.588565Z","iopub.execute_input":"2021-10-31T06:54:16.589347Z","iopub.status.idle":"2021-10-31T06:54:16.599761Z","shell.execute_reply.started":"2021-10-31T06:54:16.58931Z","shell.execute_reply":"2021-10-31T06:54:16.598919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = \"\"\"import unicodedata\nhin = [chr(i) for i in range(2406, 2416)]\nenn = [f\"{i}\" for i in range(10)]\n\nis_pred_hin = test_df[\"PredictionString\"].apply(lambda x: set(x)<=set(hin))\nif is_pred_hin.any():\n    test_df[\"trans\"] = test_df[\"PredictionString\"].copy()\n    test_df.loc[is_pred_hin, \"trans\"] = test_df.loc[is_pred_hin, \"trans\"].apply(lambda txt: \"\".join([enn[hin.index(c)] for c in txt]))\n\n    is_trans_in_context = test_df.apply(lambda row: row[\"trans\"] in row[\"context\"], axis=1)\n    if (is_pred_hin&is_trans_in_context).any():\n        test_df.loc[is_pred_hin&is_trans_in_context, \"PredictionString\"] = test_df.loc[is_pred_hin&is_trans_in_context, \"trans\"]\ntest_df['PredictionString'].fillna('', inplace=True)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:54:16.601165Z","iopub.execute_input":"2021-10-31T06:54:16.601872Z","iopub.status.idle":"2021-10-31T06:54:16.623068Z","shell.execute_reply.started":"2021-10-31T06:54:16.601837Z","shell.execute_reply":"2021-10-31T06:54:16.622321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['id', 'PredictionString']].to_csv('submission.csv', index=False) #With muril for hin and xlmr for tam and without excessive pp, public lb=0.82","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:54:16.624181Z","iopub.execute_input":"2021-10-31T06:54:16.625088Z","iopub.status.idle":"2021-10-31T06:54:16.639307Z","shell.execute_reply.started":"2021-10-31T06:54:16.625053Z","shell.execute_reply":"2021-10-31T06:54:16.638696Z"},"trusted":true},"execution_count":null,"outputs":[]}]}