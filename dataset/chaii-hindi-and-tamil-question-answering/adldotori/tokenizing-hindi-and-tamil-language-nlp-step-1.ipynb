{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We wants to solve open-domain QA task.\n\nMy process is as follows:\n#### 0. [Orientation](https://www.kaggle.com/adldotori/notebook-to-read-before-start-nlp-step-0/)\n#### 1. [Tokenization](https://www.kaggle.com/adldotori/tokenizing-hindi-and-tamil-language-nlp-step-1)\n   * ver 1 : init (2021/10/03)\n   * ver 2 : change transformer version (2021/10/03)\n   * ver 3 : update description (2021/10/05)\n\n#### 2. [Demo](https://www.kaggle.com/adldotori/demo-training-nlp-step-2/)\n#### 3. Research QA Model\n#### 4. Training\n#### 5. Inference","metadata":{}},{"cell_type":"code","source":"!pip3 install transformers==4.11.2","metadata":{"execution":{"iopub.status.busy":"2021-10-03T06:27:48.204853Z","iopub.execute_input":"2021-10-03T06:27:48.205138Z","iopub.status.idle":"2021-10-03T06:27:56.247382Z","shell.execute_reply.started":"2021-10-03T06:27:48.205106Z","shell.execute_reply":"2021-10-03T06:27:56.246064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save context depending on language","metadata":{"execution":{"iopub.status.busy":"2021-10-02T03:00:34.681046Z","iopub.execute_input":"2021-10-02T03:00:34.681317Z","iopub.status.idle":"2021-10-02T03:00:34.688331Z","shell.execute_reply.started":"2021-10-02T03:00:34.681289Z","shell.execute_reply":"2021-10-02T03:00:34.687443Z"}}},{"cell_type":"code","source":"import os\nimport os.path as osp\n\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-03T04:36:14.991807Z","iopub.execute_input":"2021-10-03T04:36:14.992332Z","iopub.status.idle":"2021-10-03T04:36:14.996507Z","shell.execute_reply.started":"2021-10-03T04:36:14.992291Z","shell.execute_reply":"2021-10-03T04:36:14.995531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = '../input/chaii-hindi-and-tamil-question-answering/'","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:36:15.047657Z","iopub.execute_input":"2021-10-03T04:36:15.047898Z","iopub.status.idle":"2021-10-03T04:36:15.051882Z","shell.execute_reply.started":"2021-10-03T04:36:15.047872Z","shell.execute_reply":"2021-10-03T04:36:15.051167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(osp.join(INPUT_PATH, 'train.csv'))\ntest = pd.read_csv(osp.join(INPUT_PATH, 'test.csv'))\nsub = pd.read_csv(osp.join(INPUT_PATH, 'sample_submission.csv'))","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:36:15.168892Z","iopub.execute_input":"2021-10-03T04:36:15.169386Z","iopub.status.idle":"2021-10-03T04:36:15.558767Z","shell.execute_reply.started":"2021-10-03T04:36:15.16936Z","shell.execute_reply":"2021-10-03T04:36:15.557987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.language == 'tamil'].head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:36:15.56019Z","iopub.execute_input":"2021-10-03T04:36:15.560915Z","iopub.status.idle":"2021-10-03T04:36:15.575764Z","shell.execute_reply.started":"2021-10-03T04:36:15.560874Z","shell.execute_reply":"2021-10-03T04:36:15.574919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.language == 'hindi'].head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:36:15.577067Z","iopub.execute_input":"2021-10-03T04:36:15.577477Z","iopub.status.idle":"2021-10-03T04:36:15.593579Z","shell.execute_reply.started":"2021-10-03T04:36:15.577428Z","shell.execute_reply":"2021-10-03T04:36:15.592698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tamil_context = train[train.language == 'tamil']['context'].str.cat(sep='\\n')\nhindi_context = train[train.language == 'hindi']['context'].str.cat(sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:36:15.609068Z","iopub.execute_input":"2021-10-03T04:36:15.609254Z","iopub.status.idle":"2021-10-03T04:36:15.632121Z","shell.execute_reply.started":"2021-10-03T04:36:15.609233Z","shell.execute_reply":"2021-10-03T04:36:15.63138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\n    '\\nlength of tamil characters : ', len(set(tamil_context)),\n    '\\nlength of hindi characters : ', len(set(hindi_context)),\n    '\\nlength of hindi & tamil characters : ', len(set(tamil_context) & set(hindi_context)),\n    '\\nlength of only tamil characters : ', len(set(tamil_context) - set(hindi_context)),\n    '\\nlength of only hindi characters : ', len(set(hindi_context) - set(tamil_context))\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:36:15.794659Z","iopub.execute_input":"2021-10-03T04:36:15.796866Z","iopub.status.idle":"2021-10-03T04:36:18.839923Z","shell.execute_reply.started":"2021-10-03T04:36:15.79683Z","shell.execute_reply":"2021-10-03T04:36:18.839162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since only 700 characters out of a total of 1400 characters overlap, two languages are separated during train tokenizers.","metadata":{}},{"cell_type":"code","source":"with open(\"tamil.txt\", \"w\") as f:\n    print(tamil_context, file=f)\n\nwith open(\"hindi.txt\", \"w\") as f:\n    print(hindi_context, file=f)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:36:18.841368Z","iopub.execute_input":"2021-10-03T04:36:18.841718Z","iopub.status.idle":"2021-10-03T04:36:18.949624Z","shell.execute_reply.started":"2021-10-03T04:36:18.841676Z","shell.execute_reply":"2021-10-03T04:36:18.948774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train BertWordPieceTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-10-02T03:12:59.322256Z","iopub.execute_input":"2021-10-02T03:12:59.323079Z","iopub.status.idle":"2021-10-02T03:12:59.32727Z","shell.execute_reply.started":"2021-10-02T03:12:59.323044Z","shell.execute_reply":"2021-10-02T03:12:59.326257Z"}}},{"cell_type":"code","source":"from tokenizers import BertWordPieceTokenizer\n\nos.makedirs('vocab', exist_ok=True)\n\ndef train_tokenizer(language: str):\n    print(f'>>> Training {language}...')\n    tokenizer = BertWordPieceTokenizer(\n        clean_text=True,\n        handle_chinese_chars=True,\n        strip_accents=False, # Must be False if cased model\n        lowercase=False,\n        wordpieces_prefix=\"##\"\n    )\n\n    tokenizer.train(\n        files=[f'{language}.txt'],\n        limit_alphabet=6000,\n        min_frequency=5,\n        show_progress=True,\n        vocab_size=30000\n    )\n\n    tokenizer.save(f'vocab/{language}', True)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:43:38.214574Z","iopub.execute_input":"2021-10-03T04:43:38.214899Z","iopub.status.idle":"2021-10-03T04:43:38.228597Z","shell.execute_reply.started":"2021-10-03T04:43:38.214863Z","shell.execute_reply":"2021-10-03T04:43:38.227272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tokenizer('tamil')\ntrain_tokenizer('hindi')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:43:38.626109Z","iopub.execute_input":"2021-10-03T04:43:38.626358Z","iopub.status.idle":"2021-10-03T04:43:51.537162Z","shell.execute_reply.started":"2021-10-03T04:43:38.626331Z","shell.execute_reply":"2021-10-03T04:43:51.535774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef save_vocab(language : str):\n    vocab_path = f'vocab/{language}'\n\n    vocab_txt_path = f'vocab/{language}.txt'\n\n    f = open(vocab_txt_path, 'w' ,encoding='utf-8')\n\n    with open(vocab_path) as json_file:\n        json_data = json.load(json_file)\n\n        for item in json_data[\"model\"][\"vocab\"].keys():\n            f.write(item + '\\n')\n    \n        f.close()\n    \n    print(f'{language} token Example:\\n')\n    for i,j in list(json_data['model']['vocab'].items())[3000:3020]:\n        print(f'{i} => {j}')\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:43:51.538974Z","iopub.execute_input":"2021-10-03T04:43:51.539284Z","iopub.status.idle":"2021-10-03T04:43:51.547639Z","shell.execute_reply.started":"2021-10-03T04:43:51.539244Z","shell.execute_reply":"2021-10-03T04:43:51.546712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_vocab(\"tamil\")\nsave_vocab(\"hindi\")","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:43:51.549124Z","iopub.execute_input":"2021-10-03T04:43:51.549437Z","iopub.status.idle":"2021-10-03T04:43:51.624774Z","shell.execute_reply.started":"2021-10-03T04:43:51.549397Z","shell.execute_reply":"2021-10-03T04:43:51.623379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test BertTokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ndef test_tokenizer(\n    language : str, \n    sample : int = 2\n):\n    \"\"\"\n    sample : sample count of testset\n    \"\"\"\n    print(f'{language} testing...\\n')\n    vocab_txt_path = f\"vocab/{language}.txt\"\n\n    tokenizer = BertTokenizer(vocab_file=vocab_txt_path, do_lower_case=False)\n\n    for i in range(sample):\n        test_str = train[train.language == language].iloc[i+100]['context']\n        test_str = test_str[:test_str.find('\\n') + 1]\n\n        print(f'{i+1}th  Test Sentence: ',test_str)\n\n        encoded_str = tokenizer.encode(test_str,add_special_tokens=False)\n        print(f'{i+1}th Sentence Encoding: ',encoded_str)\n\n        decoded_str = tokenizer.decode(encoded_str)\n        print(f'{i+1}th  Sentence Decoding: ',decoded_str, '\\n')\n    \n    print()\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:44:18.983113Z","iopub.execute_input":"2021-10-03T04:44:18.983368Z","iopub.status.idle":"2021-10-03T04:44:18.992142Z","shell.execute_reply.started":"2021-10-03T04:44:18.983341Z","shell.execute_reply":"2021-10-03T04:44:18.991333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tamil_tokenizer = test_tokenizer('tamil')\nhindi_tokenizer = test_tokenizer('hindi')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:44:18.994596Z","iopub.execute_input":"2021-10-03T04:44:18.994883Z","iopub.status.idle":"2021-10-03T04:44:19.08335Z","shell.execute_reply.started":"2021-10-03T04:44:18.994848Z","shell.execute_reply":"2021-10-03T04:44:19.081732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Checkpoints ","metadata":{}},{"cell_type":"code","source":"os.makedirs('tamil_checkpoint', exist_ok=True)\ntamil_tokenizer.save_pretrained('tamil_checkpoint')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:46:03.84083Z","iopub.execute_input":"2021-10-03T04:46:03.841096Z","iopub.status.idle":"2021-10-03T04:46:03.87332Z","shell.execute_reply.started":"2021-10-03T04:46:03.841069Z","shell.execute_reply":"2021-10-03T04:46:03.872653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs('hindi_checkpoint', exist_ok=True)\nhindi_tokenizer.save_pretrained('hindi_checkpoint')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:46:06.446015Z","iopub.execute_input":"2021-10-03T04:46:06.446638Z","iopub.status.idle":"2021-10-03T04:46:06.478104Z","shell.execute_reply.started":"2021-10-03T04:46:06.446602Z","shell.execute_reply":"2021-10-03T04:46:06.47749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RESULT","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ndef test_tokenizer_from_pretrained(\n    language : str, \n    sample : int = 2\n):\n    \"\"\"\n    sample : sample count of testset\n    \"\"\"\n    print(f'{language} testing...\\n')\n    vocab_txt_path = f\"vocab/{language}.txt\"\n\n    tokenizer = BertTokenizer.from_pretrained(f'./{language}_checkpoint')\n\n    for i in range(sample):\n        test_str = train[train.language == language].iloc[i+100]['context']\n        test_str = test_str[:test_str.find('\\n') + 1]\n\n        print(f'{i+1}th  Test Sentence: ',test_str)\n\n        encoded_str = tokenizer.encode(test_str,add_special_tokens=False)\n        print(f'{i+1}th Sentence Encoding: ',encoded_str)\n\n        decoded_str = tokenizer.decode(encoded_str)\n        print(f'{i+1}th  Sentence Decoding: ',decoded_str, '\\n')\n    \n    print()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:47:32.616249Z","iopub.execute_input":"2021-10-03T04:47:32.616977Z","iopub.status.idle":"2021-10-03T04:47:32.625633Z","shell.execute_reply.started":"2021-10-03T04:47:32.616936Z","shell.execute_reply":"2021-10-03T04:47:32.624698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tokenizer_from_pretrained('tamil')\ntest_tokenizer_from_pretrained('hindi')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:47:33.154174Z","iopub.execute_input":"2021-10-03T04:47:33.154546Z","iopub.status.idle":"2021-10-03T04:47:33.24174Z","shell.execute_reply.started":"2021-10-03T04:47:33.154511Z","shell.execute_reply":"2021-10-03T04:47:33.240281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use this tokenizer in the same way as above. Now let's do QA training using this tokenizer on the next notebook.","metadata":{}}]}