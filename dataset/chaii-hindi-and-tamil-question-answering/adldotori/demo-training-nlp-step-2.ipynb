{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We wants to solve open-domain QA task.\n\nMy process is as follows:\n\n#### 1. [Tokenization](https://www.kaggle.com/adldotori/tokenizing-hindi-and-tamil-language-nlp-step-1)\n#### 2. [Demo](https://www.kaggle.com/adldotori/demo-training-nlp-step-2/)\n* ver 1 : init (2021/10/03)\n* ver 2 : update validation (2021/10/05)\n* ver 3 : validation score 0.45 (2021/10/11)\n\n#### 3. Research QA Model\n#### 4. Training\n#### 5. Inference","metadata":{"execution":{"iopub.status.busy":"2021-10-03T05:13:35.832033Z","iopub.execute_input":"2021-10-03T05:13:35.832345Z","iopub.status.idle":"2021-10-03T05:13:35.83922Z","shell.execute_reply.started":"2021-10-03T05:13:35.832318Z","shell.execute_reply":"2021-10-03T05:13:35.83796Z"}}},{"cell_type":"code","source":"!pip3 install transformers==4.11.2","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:06:36.397197Z","iopub.execute_input":"2021-10-11T07:06:36.397831Z","iopub.status.idle":"2021-10-11T07:06:37.320411Z","shell.execute_reply.started":"2021-10-11T07:06:36.397718Z","shell.execute_reply":"2021-10-11T07:06:37.3196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport os.path as osp\n\nimport pandas as pd\n\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:06:37.323913Z","iopub.execute_input":"2021-10-11T07:06:37.32414Z","iopub.status.idle":"2021-10-11T07:06:43.229893Z","shell.execute_reply.started":"2021-10-11T07:06:37.32411Z","shell.execute_reply":"2021-10-11T07:06:43.22917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = '../input/chaii-hindi-and-tamil-question-answering/'\n\ntrain = pd.read_csv(osp.join(INPUT_PATH, 'train.csv'))\ntest = pd.read_csv(osp.join(INPUT_PATH, 'test.csv'))\nsub = pd.read_csv(osp.join(INPUT_PATH, 'sample_submission.csv'))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:25:19.949084Z","iopub.execute_input":"2021-10-11T07:25:19.950109Z","iopub.status.idle":"2021-10-11T07:25:20.694582Z","shell.execute_reply.started":"2021-10-11T07:25:19.950042Z","shell.execute_reply":"2021-10-11T07:25:20.693877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = train[:round(len(train) * 0.8)], train[round(len(train) * 0.8):]","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:06:44.052138Z","iopub.execute_input":"2021-10-11T07:06:44.052384Z","iopub.status.idle":"2021-10-11T07:06:44.059087Z","shell.execute_reply.started":"2021-10-11T07:06:44.052352Z","shell.execute_reply":"2021-10-11T07:06:44.056563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference Example","metadata":{}},{"cell_type":"code","source":"# https://huggingface.co/transformers/usage.html#extractive-question-answering\n\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n\ntext = r\"\"\"\nðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\narchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\nLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\nTensorFlow 2.0 and PyTorch.\n\"\"\"\n\nquestions = [\n    \"How many pretrained models are available in Transformers?\",\n    \"What does Transformers provide?\",\n    \"Transformers provides interoperability between which frameworks?\",\n]\n\nfor question in questions:\n    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n\n    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    result = model(**inputs)\n    answer_start_scores = result['start_logits']\n    answer_end_scores = result['end_logits']\n\n    answer_start = torch.argmax(\n        answer_start_scores\n    )  # Get the most likely beginning of answer with the argmax of the score\n    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:09:20.659117Z","iopub.execute_input":"2021-10-11T07:09:20.659902Z","iopub.status.idle":"2021-10-11T07:10:09.822037Z","shell.execute_reply.started":"2021-10-11T07:09:20.659856Z","shell.execute_reply":"2021-10-11T07:10:09.821124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning","metadata":{}},{"cell_type":"code","source":"# https://huggingface.co/transformers/custom_datasets.html#question-answering-with-squad-2-0\n\ndef read_dataset(df:pd.DataFrame):\n    \n    contexts = []\n    questions = []\n    answers = []\n    \n    for i, data in df.iterrows():\n        contexts.append(data['context'])\n        questions.append(data['question'])\n        \n        answer = {}\n        answer['text'] = data['answer_text']\n        answer['answer_start'] = data['answer_start']\n        answer['answer_end'] = data['answer_start'] + len(data['answer_text'])\n        answers.append(answer)\n    \n    return contexts, questions, answers","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:10:09.823907Z","iopub.execute_input":"2021-10-11T07:10:09.824178Z","iopub.status.idle":"2021-10-11T07:10:09.830684Z","shell.execute_reply.started":"2021-10-11T07:10:09.824144Z","shell.execute_reply":"2021-10-11T07:10:09.829905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_contexts, train_questions, train_answers = read_dataset(train_df)\nval_contexts, val_questions, val_answers = read_dataset(val_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:10:09.832253Z","iopub.execute_input":"2021-10-11T07:10:09.832562Z","iopub.status.idle":"2021-10-11T07:10:09.975657Z","shell.execute_reply.started":"2021-10-11T07:10:09.832528Z","shell.execute_reply":"2021-10-11T07:10:09.974787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('deepset/xlm-roberta-large-squad2')","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:10:09.977713Z","iopub.execute_input":"2021-10-11T07:10:09.977981Z","iopub.status.idle":"2021-10-11T07:10:17.195122Z","shell.execute_reply.started":"2021-10-11T07:10:09.977943Z","shell.execute_reply":"2021-10-11T07:10:17.194297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntrain_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\nval_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:10:17.196614Z","iopub.execute_input":"2021-10-11T07:10:17.19686Z","iopub.status.idle":"2021-10-11T07:10:47.859706Z","shell.execute_reply.started":"2021-10-11T07:10:17.196828Z","shell.execute_reply":"2021-10-11T07:10:47.858881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_token_positions(encodings, answers):\n    start_positions = []\n    end_positions = []\n    for i in range(len(answers)):\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n\n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length\n\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:10:47.863208Z","iopub.execute_input":"2021-10-11T07:10:47.864976Z","iopub.status.idle":"2021-10-11T07:10:47.876647Z","shell.execute_reply.started":"2021-10-11T07:10:47.864945Z","shell.execute_reply":"2021-10-11T07:10:47.875971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://huggingface.co/transformers/custom_datasets.html#question-answering-with-squad-2-0\n\nimport torch\n\nclass ChaiiDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\ntrain_dataset = ChaiiDataset(train_encodings)\nval_dataset = ChaiiDataset(val_encodings)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:10:47.877745Z","iopub.execute_input":"2021-10-11T07:10:47.877956Z","iopub.status.idle":"2021-10-11T07:10:47.90565Z","shell.execute_reply.started":"2021-10-11T07:10:47.877929Z","shell.execute_reply":"2021-10-11T07:10:47.904837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/xlm-roberta-base-squad2\")","metadata":{"execution":{"iopub.status.busy":"2021-10-10T10:33:44.210636Z","iopub.execute_input":"2021-10-10T10:33:44.211434Z","iopub.status.idle":"2021-10-10T10:34:24.103224Z","shell.execute_reply.started":"2021-10-10T10:33:44.211392Z","shell.execute_reply":"2021-10-10T10:34:24.102438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # https://huggingface.co/transformers/custom_datasets.html#question-answering-with-squad-2-0\n\n# from torch.utils.data import DataLoader\n# from transformers import AdamW\n\n# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# model.to(device)\n# model.train()\n\n# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# optim = AdamW(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T10:34:24.104606Z","iopub.execute_input":"2021-10-10T10:34:24.104862Z","iopub.status.idle":"2021-10-10T10:34:31.263359Z","shell.execute_reply.started":"2021-10-10T10:34:24.104828Z","shell.execute_reply":"2021-10-10T10:34:31.262608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Your Own GPU\nIt needs 24GB memory, so can't training in kaggle.","metadata":{}},{"cell_type":"code","source":"# from tqdm import tqdm\n\n# for epoch in range(30):\n#     pbar = tqdm(train_loader)\n#     total_loss = 0\n#     for batch in pbar:\n#         optim.zero_grad()\n#         input_ids = batch['input_ids'][:,:512].to(device)\n#         attention_mask = batch['attention_mask'][:,:512].to(device)\n#         start_positions = batch['start_positions'].to(device)\n#         end_positions = batch['end_positions'].to(device)\n#         outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n#         loss = outputs[0]\n#         total_loss += loss\n#         loss.backward()\n#         optim.step()\n        \n#         pbar.set_description(f\"Loss : {round(loss.item(), 3)}\")\n#     print(f\"[{epoch+1} EPOCH] Total Loss : {round((total_loss / len(pbar)).item(), 4)}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T10:34:31.264445Z","iopub.execute_input":"2021-10-10T10:34:31.266322Z","iopub.status.idle":"2021-10-10T10:34:31.272527Z","shell.execute_reply.started":"2021-10-10T10:34:31.26629Z","shell.execute_reply":"2021-10-10T10:34:31.271805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"markdown","source":"This checkpoint based by deepset/xlm-roberta-base-squad2 model.","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:23:47.717294Z","iopub.execute_input":"2021-10-11T07:23:47.718085Z","iopub.status.idle":"2021-10-11T07:23:47.72507Z","shell.execute_reply.started":"2021-10-11T07:23:47.718018Z","shell.execute_reply":"2021-10-11T07:23:47.724035Z"}}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"/kaggle/input/chaiick/checkpoint_all\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:16:03.605865Z","iopub.execute_input":"2021-10-11T07:16:03.606182Z","iopub.status.idle":"2021-10-11T07:16:18.334022Z","shell.execute_reply.started":"2021-10-11T07:16:03.606149Z","shell.execute_reply":"2021-10-11T07:16:18.333353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Jaccard Similiarity.","metadata":{}},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:16:18.33566Z","iopub.execute_input":"2021-10-11T07:16:18.335908Z","iopub.status.idle":"2021-10-11T07:16:18.340782Z","shell.execute_reply.started":"2021-10-11T07:16:18.335876Z","shell.execute_reply":"2021-10-11T07:16:18.34009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_index(target, base):\n    for i in range(len(base)):\n        if target == base[i:i + len(target)]:\n            return i,i+len(target)\n    return -1,-1","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:16:18.342079Z","iopub.execute_input":"2021-10-11T07:16:18.342467Z","iopub.status.idle":"2021-10-11T07:16:18.350065Z","shell.execute_reply.started":"2021-10-11T07:16:18.342433Z","shell.execute_reply":"2021-10-11T07:16:18.349388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\n\ndef test_idx(\n    df:pd.DataFrame,\n    idx:int, \n    is_valid:bool = False, \n    log:bool = False\n):\n    question = df.loc[idx]['question']\n    text = df.loc[idx]['context']\n    \n    if is_valid:\n        answer = df.loc[idx]['answer_text']\n    \n    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n    \n    if is_valid:\n        start, end = find_index(tokenizer(answer)[\"input_ids\"][1:-1], tokenizer(text)[\"input_ids\"])\n    input_ids = inputs[\"input_ids\"][:, :512].to(device)\n    attention_mask = inputs[\"attention_mask\"][:, :512].to(device)\n\n    result = model(input_ids, attention_mask=attention_mask)\n    answer_start_scores = result['start_logits']\n    answer_end_scores = result['end_logits']\n\n\n    answer_start = torch.argmax(\n        answer_start_scores\n    )  # Get the most likely beginning of answer with the argmax of the score\n    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n\n    answer = tokenizer.convert_tokens_to_string(\n        tokenizer.convert_ids_to_tokens(\n            input_ids[0][answer_start:answer_end]\n        )\n    )\n    \n    # delete bad tokens\n    bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"â€“\",  \",\", \";\"]\n    bad_endings = [\"...\", \"-\", \"(\", \")\", \"â€“\", \",\", \";\"]\n    if any([answer.startswith(token) for token in bad_starts]):\n        answer = answer[1:]\n    if any([answer.endswith(token) for token in bad_endings]):\n        answer = answer[:-1]\n    \n    if is_valid:\n        score = jaccard(df.loc[idx][\"answer_text\"], answer)\n    df.loc[idx, 'predict'] = answer\n    if is_valid:\n        df.loc[idx, 'ans_start'] = start\n        df.loc[idx, 'ans_end'] = end\n        df.loc[idx, 'score'] = score\n    df.loc[idx, 'pred_start'] = find_index(tokenizer(answer)[\"input_ids\"][1:-1], tokenizer(text)[\"input_ids\"])[0]\n    \n\n    if log:\n        print(f'Answer[{df.iloc[idx][\"answer_start\"]} - {df.iloc[idx][\"answer_start\"] + len(val_df.iloc[idx][\"answer_text\"])}] : {df.iloc[idx][\"answer_text\"]}')\n        print(f'Prediction[{int(answer_start)} - {int(answer_end)}] : {answer}\\n')\n        print(f'Score : {score}\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:31:55.843096Z","iopub.execute_input":"2021-10-11T07:31:55.843367Z","iopub.status.idle":"2021-10-11T07:31:55.860176Z","shell.execute_reply.started":"2021-10-11T07:31:55.843338Z","shell.execute_reply":"2021-10-11T07:31:55.859463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df = val_df.reset_index(drop=True)\nval_df[['predict', 'ans_start', 'ans_end', 'pred_start', 'score']] = 0\n\nfor i in range(len(val_df)):\n    test_idx(val_df, i, is_valid=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:32:13.54052Z","iopub.execute_input":"2021-10-11T07:32:13.540774Z","iopub.status.idle":"2021-10-11T07:32:39.075242Z","shell.execute_reply.started":"2021-10-11T07:32:13.540748Z","shell.execute_reply":"2021-10-11T07:32:39.074496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df.sort_values('score', ascending=False).head(15)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:32:39.07695Z","iopub.execute_input":"2021-10-11T07:32:39.077284Z","iopub.status.idle":"2021-10-11T07:32:39.100461Z","shell.execute_reply.started":"2021-10-11T07:32:39.077246Z","shell.execute_reply":"2021-10-11T07:32:39.099617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Validation Score : {round(val_df.score.mean(), 2)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:32:39.102344Z","iopub.execute_input":"2021-10-11T07:32:39.102707Z","iopub.status.idle":"2021-10-11T07:32:39.111047Z","shell.execute_reply.started":"2021-10-11T07:32:39.102671Z","shell.execute_reply":"2021-10-11T07:32:39.110234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:32:39.113635Z","iopub.execute_input":"2021-10-11T07:32:39.114107Z","iopub.status.idle":"2021-10-11T07:32:39.12809Z","shell.execute_reply.started":"2021-10-11T07:32:39.114068Z","shell.execute_reply":"2021-10-11T07:32:39.127389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.reset_index(drop=True)\ntest[['predict', 'pred_start']] = 0\n\nfor i in range(len(test)):\n    test_idx(test, i, is_valid=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:33:25.669322Z","iopub.execute_input":"2021-10-11T07:33:25.669581Z","iopub.status.idle":"2021-10-11T07:33:26.24532Z","shell.execute_reply.started":"2021-10-11T07:33:25.669553Z","shell.execute_reply":"2021-10-11T07:33:26.244421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:33:26.247034Z","iopub.execute_input":"2021-10-11T07:33:26.247378Z","iopub.status.idle":"2021-10-11T07:33:26.26237Z","shell.execute_reply.started":"2021-10-11T07:33:26.247341Z","shell.execute_reply":"2021-10-11T07:33:26.261652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, data in test.iterrows():\n    sub.loc[sub.id == data.id, 'PredictionString'] = data['predict']","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:34:11.749597Z","iopub.execute_input":"2021-10-11T07:34:11.74988Z","iopub.status.idle":"2021-10-11T07:34:11.763116Z","shell.execute_reply.started":"2021-10-11T07:34:11.749843Z","shell.execute_reply":"2021-10-11T07:34:11.762239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:34:12.0272Z","iopub.execute_input":"2021-10-11T07:34:12.027735Z","iopub.status.idle":"2021-10-11T07:34:12.041126Z","shell.execute_reply.started":"2021-10-11T07:34:12.027689Z","shell.execute_reply":"2021-10-11T07:34:12.039958Z"},"trusted":true},"execution_count":null,"outputs":[]}]}