{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nA novice attempt at Question and Answering in Hindi and Tamil. Will be improved over time.\n\n# Understanding the Problem\n\nGiven a pair of inputs, Context and Question, return a String that Answers the question for the given context. The Answers are drawn directly from the Context (Answers are a subset of Context). The Answer includes punctuation.\n\nAnswers will be evaluated using the word-level Jaccard Score as provided by the competition.","metadata":{}},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:18:51.938239Z","iopub.execute_input":"2021-08-21T06:18:51.938759Z","iopub.status.idle":"2021-08-21T06:18:51.952352Z","shell.execute_reply.started":"2021-08-21T06:18:51.938631Z","shell.execute_reply":"2021-08-21T06:18:51.951281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\nFirst load the dataset and tokenize the text.\n\nWe must also calculate the End Index from the start index and the answer text.\n\nTo-Do: Split the Context at predefined sizes in a way that allows the model to train.","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:18:51.954462Z","iopub.execute_input":"2021-08-21T06:18:51.954972Z","iopub.status.idle":"2021-08-21T06:18:51.967477Z","shell.execute_reply.started":"2021-08-21T06:18:51.954923Z","shell.execute_reply":"2021-08-21T06:18:51.966164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = \"/kaggle/input/chaii-hindi-and-tamil-question-answering/\"\ntest_path = base_path + \"test.csv\"\ntrain_path = base_path + \"train.csv\"\n\ntest_set = pd.read_csv(test_path)\ntrain_set = pd.read_csv(train_path)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:18:51.970115Z","iopub.execute_input":"2021-08-21T06:18:51.970933Z","iopub.status.idle":"2021-08-21T06:18:53.003543Z","shell.execute_reply.started":"2021-08-21T06:18:51.970877Z","shell.execute_reply":"2021-08-21T06:18:53.002669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add End Index\ntrain_set['answer_end'] = train_set['answer_text'].str.len() + train_set['answer_start']\ntrain_set.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:18:53.005577Z","iopub.execute_input":"2021-08-21T06:18:53.006352Z","iopub.status.idle":"2021-08-21T06:18:53.061213Z","shell.execute_reply.started":"2021-08-21T06:18:53.0063Z","shell.execute_reply":"2021-08-21T06:18:53.060068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find Max Context Length to be used as a hyperparameter\n# Don't have enough memory for training so we only get to look at the first 1660 characters\n\n# To-do: Improve this pipeline by splitting text into smaller pieces for processing.\nmax_sequence_length = int(train_set['context'].map(lambda x: len(x)).max() / 30 ) \nprint(max_sequence_length)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:18:53.062736Z","iopub.execute_input":"2021-08-21T06:18:53.063135Z","iopub.status.idle":"2021-08-21T06:18:53.071848Z","shell.execute_reply.started":"2021-08-21T06:18:53.063099Z","shell.execute_reply":"2021-08-21T06:18:53.07068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Keras for text preprocessing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:18:53.073195Z","iopub.execute_input":"2021-08-21T06:18:53.073512Z","iopub.status.idle":"2021-08-21T06:18:59.948369Z","shell.execute_reply.started":"2021-08-21T06:18:53.073482Z","shell.execute_reply":"2021-08-21T06:18:59.947046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the char_level value to true because the\n# size of the word_index is smaller and this saves memory when training the model.\n\n# To-Do: Optimize pipeline so words are used instead of char_level\ntokenizer = Tokenizer(char_level = True)\ntokenizer.fit_on_texts(train_set['context'])\nmax_word_index = len(tokenizer.word_index) + 1\nprint(max_word_index)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:18:59.950005Z","iopub.execute_input":"2021-08-21T06:18:59.950339Z","iopub.status.idle":"2021-08-21T06:19:04.997266Z","shell.execute_reply.started":"2021-08-21T06:18:59.950308Z","shell.execute_reply":"2021-08-21T06:19:04.996078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_text(texts, tokenizer, max_sequence_length):\n    text_sequences = tokenizer.texts_to_sequences(texts)\n    return pad_sequences(text_sequences, maxlen=max_sequence_length)\n\n# Convert each of the texts into sequences.\ntrain_context_sequence = prep_text(train_set['context'], tokenizer, max_sequence_length)\ntrain_question_sequence = prep_text(train_set['question'], tokenizer, max_sequence_length)\ntest_context_sequence = prep_text(test_set['context'], tokenizer, max_sequence_length)\ntest_question_sequence = prep_text(test_set['question'], tokenizer, max_sequence_length)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:19:04.998861Z","iopub.execute_input":"2021-08-21T06:19:04.999172Z","iopub.status.idle":"2021-08-21T06:19:09.605197Z","shell.execute_reply.started":"2021-08-21T06:19:04.999143Z","shell.execute_reply":"2021-08-21T06:19:09.603969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\nA test model with two inputs and two outputs.\nThe Inputs are the context and questions as integer sequences.\nInstead of One-Hot encoding the model learns the embeddings at training time, for both inputs.\nAn LSTM processes the input's Embeddings and their results are contatenated.\nThe concatenated layer values are passed to a Dense Layer for predictions of the start and end indicies.\n\nTo-Do: Replace the Dense Layers with something better.","metadata":{}},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Embedding, SpatialDropout1D, LSTM, concatenate, Dense\nfrom keras import Input\n\ntext_vocabulary_size = 1000\nquestion_vocabulary_size = 1000\nanswer_vocabulary_size = 1\n\ntext_input = Input(shape=(None,), dtype='int32', name='text')\nembedded_text = Embedding(max_word_index, text_vocabulary_size)(text_input)\nembedded_text = SpatialDropout1D(0.2)(embedded_text)\nencoded_text = LSTM(32)(embedded_text)\n\nquestion_input = Input(shape=(None,), dtype='int32', name='question')\nembedded_question = Embedding(max_word_index, question_vocabulary_size)(question_input)\nembedded_question = SpatialDropout1D(0.2)(embedded_question)\nencoded_question = LSTM(32)(embedded_question)\n\nconcatenated = concatenate([encoded_text, encoded_question])\n\nstart_index = Dense(1, activation='softmax')(concatenated)\n\nend_index = Dense(1, activation='softmax')(concatenated)\n\nmodel = Model([text_input, question_input], outputs=[start_index, end_index])\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:19:09.607756Z","iopub.execute_input":"2021-08-21T06:19:09.608075Z","iopub.status.idle":"2021-08-21T06:19:10.353252Z","shell.execute_reply.started":"2021-08-21T06:19:09.608046Z","shell.execute_reply":"2021-08-21T06:19:10.352188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit([train_context_sequence, train_question_sequence], [train_set['answer_start'], train_set['answer_end']], epochs=10, batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:19:10.35495Z","iopub.execute_input":"2021-08-21T06:19:10.355366Z","iopub.status.idle":"2021-08-21T06:31:04.943115Z","shell.execute_reply.started":"2021-08-21T06:19:10.35532Z","shell.execute_reply":"2021-08-21T06:31:04.942257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict([test_context_sequence, test_question_sequence])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:37:55.776865Z","iopub.execute_input":"2021-08-21T06:37:55.777313Z","iopub.status.idle":"2021-08-21T06:37:55.965495Z","shell.execute_reply.started":"2021-08-21T06:37:55.777275Z","shell.execute_reply":"2021-08-21T06:37:55.964572Z"},"trusted":true},"execution_count":null,"outputs":[]}]}