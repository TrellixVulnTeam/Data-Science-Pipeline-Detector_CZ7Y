{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# with MLQA and XQUAD hindi\n<h2>chaii QA - MuRIL BERT base Finetuning in Torch w/o Trainer API</h2>\n    \n<h3><span \"style: color=#444\">Introduction</span></h3>\n\nThe kernel finetunes Googles' MuRIL Language Model without using the Trainer API from HuggingFace.\n\n<h3><span \"style: color=#444\">References</span></h3>\nI would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n\n- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n\n- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n\n- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n\n- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n\n- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly \nrecommend everyone to give a detailed read.\n\n- [chaii XLM roberta](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit)","metadata":{}},{"cell_type":"markdown","source":"# APEX installation","metadata":{}},{"cell_type":"code","source":"%%writefile setup.sh\nexport CUDA_HOME=/usr/local/cuda-10.1\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --disable-pip-version-check --no-cache-dir ./","metadata":{"execution":{"iopub.status.busy":"2021-08-29T12:04:40.970283Z","iopub.status.idle":"2021-08-29T12:04:40.971051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\ngc.enable()\n\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn import Parameter\nfrom torch.utils.data import (\n    Dataset,\n    DataLoader,\n    SequentialSampler,\n    RandomSampler\n)\n\nfrom torch.utils.data.distributed import DistributedSampler\n\ntry:\n    from apex import amp\n    APEX_INSTALLED = True\nexcept ImportError:\n    APEX_INSTALLED = False\n\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    BertTokenizerFast,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:18.067108Z","iopub.execute_input":"2021-08-30T08:46:18.067745Z","iopub.status.idle":"2021-08-30T08:46:24.387503Z","shell.execute_reply.started":"2021-08-30T08:46:18.067706Z","shell.execute_reply":"2021-08-30T08:46:24.386662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\nprint(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\nprint(MODEL_CONFIG_CLASSES)\nprint()\nprint(MODEL_TYPES)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:24.388945Z","iopub.execute_input":"2021-08-30T08:46:24.389278Z","iopub.status.idle":"2021-08-30T08:46:24.401176Z","shell.execute_reply.started":"2021-08-30T08:46:24.389241Z","shell.execute_reply":"2021-08-30T08:46:24.400245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n    # model\n    model_name_or_path = \"../input/murilbasecased/pytorch_model.bin\"\n    config_name = \"../input/murilbasecased/config.json\"\n    fp16 = True if APEX_INSTALLED else False\n    fp16_opt_level = \"O1\"\n    gradient_accumulation_steps = 16\n    \n    # tokenizer\n    tokenizer_name = \"../input/murilbasecased/\"\n    max_seq_length = 384\n    doc_stride = 128\n    \n    # train\n    epochs = 3\n    train_batch_size = 8\n    eval_batch_size = 16\n    \n    # optimizer\n    optimizer_type = 'AdamW'\n    learning_rate = 3e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n    \n    # scheduler\n    decay_name = 'linear-warmup'\n    warmup_ratio = 0.1\n    \n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 2021","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:24.404767Z","iopub.execute_input":"2021-08-30T08:46:24.405009Z","iopub.status.idle":"2021-08-30T08:46:24.41285Z","shell.execute_reply.started":"2021-08-30T08:46:24.404987Z","shell.execute_reply":"2021-08-30T08:46:24.41197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset setup\n\n## External Hindi MLQA + XQUAD","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nexternal_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\nexternal_train = pd.concat([external_mlqa, external_xquad])\n\n# train = pd.concat([train, external_train]).reset_index(drop=True)\n\ndef convert_answers(row):\n    return {'answer_start': [row[0]], 'text': [row[1]]}\n\ntrain['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\nexternal_train['answers'] = external_train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\nexternal_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:24.416252Z","iopub.execute_input":"2021-08-30T08:46:24.416499Z","iopub.status.idle":"2021-08-30T08:46:25.166685Z","shell.execute_reply.started":"2021-08-30T08:46:24.416476Z","shell.execute_reply":"2021-08-30T08:46:25.16592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess data","metadata":{}},{"cell_type":"code","source":"def prepare_train_features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()\n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n\n    features = []\n    for i, offsets in enumerate(offset_mapping):\n        feature = {}\n\n        input_ids = tokenized_example[\"input_ids\"][i]\n        attention_mask = tokenized_example[\"attention_mask\"][i]\n\n        feature['input_ids'] = input_ids\n        feature['attention_mask'] = attention_mask\n        feature['offset_mapping'] = offsets\n\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_example.sequence_ids(i)\n\n        sample_index = sample_mapping[i]\n        answers = example[\"answers\"]\n\n        if len(answers[\"answer_start\"]) == 0:\n            feature[\"start_position\"] = cls_index\n            feature[\"end_position\"] = cls_index\n        else:\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                feature[\"start_position\"] = cls_index\n                feature[\"end_position\"] = cls_index\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                feature[\"start_position\"] = token_start_index - 1\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                feature[\"end_position\"] = token_end_index + 1\n\n        features.append(feature)\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.167983Z","iopub.execute_input":"2021-08-30T08:46:25.168322Z","iopub.status.idle":"2021-08-30T08:46:25.180829Z","shell.execute_reply.started":"2021-08-30T08:46:25.168287Z","shell.execute_reply":"2021-08-30T08:46:25.17993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Retriever","metadata":{}},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):\n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.182134Z","iopub.execute_input":"2021-08-30T08:46:25.18249Z","iopub.status.idle":"2021-08-30T08:46:25.193736Z","shell.execute_reply.started":"2021-08-30T08:46:25.182457Z","shell.execute_reply":"2021-08-30T08:46:25.192713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MuRIL Bert\n\n### Reinitializing the last encoder layer","metadata":{}},{"cell_type":"code","source":"# create model\nclass Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        self.muril = AutoModel.from_pretrained(modelname_or_path, config=config)\n        # reinitalize last encoder layer\n        self.muril.encoder.layer[-1].__init__(config)\n        self.outputs = nn.Linear(config.hidden_size, 2)\n        self._init_weights()\n    \n    def _init_weights(self):\n        if isinstance(self.outputs, nn.Linear):\n            self.outputs.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if self.outputs.bias is not None:\n                self.outputs.bias.data.zero_()\n    \n    def forward(self, input_ids, attention_mask=None):\n        outputs = self.muril(input_ids, attention_mask=attention_mask)\n        sequence_outputs = outputs[0]\n        pooled_outputs = outputs[1]\n        \n        logits = self.outputs(sequence_outputs)\n        \n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.195082Z","iopub.execute_input":"2021-08-30T08:46:25.195564Z","iopub.status.idle":"2021-08-30T08:46:25.206027Z","shell.execute_reply.started":"2021-08-30T08:46:25.195529Z","shell.execute_reply":"2021-08-30T08:46:25.205079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"def loss_fn(preds, labels):\n    start_preds, end_preds = preds\n    start_labels, end_labels = labels\n    \n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n    total_loss = (start_loss + end_loss) / 2\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.209256Z","iopub.execute_input":"2021-08-30T08:46:25.209701Z","iopub.status.idle":"2021-08-30T08:46:25.217148Z","shell.execute_reply.started":"2021-08-30T08:46:25.209663Z","shell.execute_reply":"2021-08-30T08:46:25.216294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        if val > self.max:\n            self.max = val\n        if val < self.min:\n            self.min = val","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.218453Z","iopub.execute_input":"2021-08-30T08:46:25.21991Z","iopub.status.idle":"2021-08-30T08:46:25.229329Z","shell.execute_reply.started":"2021-08-30T08:46:25.219875Z","shell.execute_reply":"2021-08-30T08:46:25.22857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_model(args):\n    config = AutoConfig.from_pretrained(args.config_name)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n    model = Model(args.model_name_or_path, config=config)\n    return config, tokenizer, model\n\n\ndef make_optimizer(args, model):\n    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    if args.optimizer_type == \"AdamW\":\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=args.learning_rate,\n            eps=args.epsilon,\n            correct_bias=True\n        )\n        return optimizer\n\ndef make_scheduler(\n    args, optimizer, \n    num_warmup_steps, \n    num_training_steps\n):\n    if args.decay_name == \"cosine-warmup\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    else:\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    return scheduler\n\ndef make_loader(\n    args, data, external_data,\n    tokenizer\n):\n    \n    # replace these two with folds when using cross validation\n    train_set = data[:-64].reset_index(drop=True)\n    valid_set = data[-64:].reset_index(drop=True)\n    \n    # concat external with train\n    train_set = pd.concat([train_set, external_data]).reset_index(drop=True)\n    \n    train_features, valid_features = [[] for _ in range(2)]\n    for i, row in train_set.iterrows():\n        train_features += prepare_train_features(args, row, tokenizer)\n    for i, row in valid_set.iterrows():\n        valid_features += prepare_train_features(args, row, tokenizer)\n\n    train_dataset = DatasetRetriever(train_features)\n    valid_dataset = DatasetRetriever(valid_features)\n    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n    \n    train_sampler = RandomSampler(train_dataset)\n    valid_sampler = SequentialSampler(valid_dataset)\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=args.train_batch_size,\n        sampler=train_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True,\n        drop_last=False \n    )\n\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=args.eval_batch_size, \n        sampler=valid_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True, \n        drop_last=False\n    )\n\n    return train_dataloader, valid_dataloader","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.232736Z","iopub.execute_input":"2021-08-30T08:46:25.233001Z","iopub.status.idle":"2021-08-30T08:46:25.248955Z","shell.execute_reply.started":"2021-08-30T08:46:25.232977Z","shell.execute_reply":"2021-08-30T08:46:25.248137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainer Setup","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(\n        self, model, tokenizer, \n        optimizer, scheduler\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def train(\n        self, args, \n        train_dataloader, \n        epoch, result_dict\n    ):\n        count = 0\n        losses = AverageMeter()\n        \n        self.model.zero_grad()\n        self.model.train()\n        \n        seed_everything(args.seed)\n        \n        for batch_idx, batch_data in enumerate(train_dataloader):\n            input_ids, attention_mask, targets_start, targets_end = \\\n                batch_data['input_ids'], batch_data['attention_mask'], \\\n                    batch_data['start_position'], batch_data['end_position']\n            \n            input_ids, attention_mask, targets_start, targets_end = \\\n                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n\n            outputs_start, outputs_end = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n            )\n            \n            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n            loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            count += input_ids.size(0)\n            losses.update(loss.item(), input_ids.size(0))\n\n            # if args.fp16:\n            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n            # else:\n            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n\n            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n                self.optimizer.step()\n                self.scheduler.step()\n                self.optimizer.zero_grad()\n\n            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n                _s = str(len(str(len(train_dataloader.sampler))))\n                ret = [\n                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n                    'Train Loss: {: >4.5f}'.format(losses.avg),\n                ]\n                print(', '.join(ret))\n\n        result_dict['train_loss'].append(losses.avg)\n        return result_dict","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.250959Z","iopub.execute_input":"2021-08-30T08:46:25.251661Z","iopub.status.idle":"2021-08-30T08:46:25.264901Z","shell.execute_reply.started":"2021-08-30T08:46:25.251622Z","shell.execute_reply":"2021-08-30T08:46:25.264203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluator Setup","metadata":{}},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model):\n        self.model = model\n    \n    def save(self, result, output_dir):\n        with open(f'{output_dir}/result_dict.json', 'w') as f:\n            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n\n    def evaluate(self, valid_dataloader, epoch, result_dict):\n        losses = AverageMeter()\n        for batch_idx, batch_data in enumerate(valid_dataloader):\n            self.model = self.model.eval()\n            input_ids, attention_mask, targets_start, targets_end = \\\n                batch_data['input_ids'], batch_data['attention_mask'], \\\n                    batch_data['start_position'], batch_data['end_position']\n            \n            input_ids, attention_mask, targets_start, targets_end = \\\n                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n            \n            with torch.no_grad():            \n                outputs_start, outputs_end = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                )\n                \n                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n                losses.update(loss.item(), input_ids.size(0))\n                \n        print('----Validation Results Summary----')\n        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n        result_dict['val_loss'].append(losses.avg)        \n        return result_dict","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.266152Z","iopub.execute_input":"2021-08-30T08:46:25.266496Z","iopub.status.idle":"2021-08-30T08:46:25.27839Z","shell.execute_reply.started":"2021-08-30T08:46:25.266462Z","shell.execute_reply":"2021-08-30T08:46:25.277626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_training(args, data, external_data):\n    seed_everything(args.seed)\n    \n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    \n    # model\n    model_config, tokenizer, model = make_model(args)\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n    \n    # data loaders\n    train_dataloader, valid_dataloader = make_loader(args, data, external_data, tokenizer)\n\n    # optimizer\n    optimizer = make_optimizer(args, model)\n\n    # scheduler\n    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n    if args.warmup_ratio > 0:\n        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n    else:\n        num_warmup_steps = 0\n    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n\n    # mixed precision training with NVIDIA Apex\n    if args.fp16:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    \n    result_dict = {\n        'epoch':[], \n        'train_loss': [], \n        'val_loss' : [], \n        'best_val_loss': np.inf\n    }\n\n    return (\n        model, model_config, tokenizer, optimizer, scheduler, \n        train_dataloader, valid_dataloader, result_dict\n    )","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.28025Z","iopub.execute_input":"2021-08-30T08:46:25.280506Z","iopub.status.idle":"2021-08-30T08:46:25.292926Z","shell.execute_reply.started":"2021-08-30T08:46:25.280474Z","shell.execute_reply":"2021-08-30T08:46:25.292149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(data, external_data):\n    args = Config()\n    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n        valid_dataloader, result_dict = init_training(args, data, external_data)\n    \n    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n    evaluator = Evaluator(model)\n\n    train_time_list = []\n    valid_time_list = []\n\n    for epoch in range(args.epochs):\n        result_dict['epoch'].append(epoch)\n\n        # Train\n        torch.cuda.synchronize()\n        tic1 = time.time()\n        result_dict = trainer.train(\n            args, train_dataloader, \n            epoch, result_dict\n        )\n        torch.cuda.synchronize()\n        tic2 = time.time() \n        train_time_list.append(tic2 - tic1)\n        \n        # Evaluate\n        torch.cuda.synchronize()\n        tic3 = time.time()\n        result_dict = evaluator.evaluate(\n            valid_dataloader, epoch, result_dict\n        )\n        torch.cuda.synchronize()\n        tic4 = time.time() \n        valid_time_list.append(tic4 - tic3)\n            \n        output_dir = os.path.join(args.output_dir, f\"checkpoint-muril\")\n        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n            \n            os.makedirs(output_dir, exist_ok=True)\n            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n            model_config.save_pretrained(output_dir)\n            tokenizer.save_pretrained(output_dir)\n            print(f\"Saving model checkpoint to {output_dir}.\")\n            \n        print()\n\n    evaluator.save(result_dict, output_dir)\n    \n    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n    \n    torch.cuda.empty_cache()\n    del trainer, evaluator\n    del model, model_config, tokenizer\n    del optimizer, scheduler\n    del result_dict, train_dataloader\n    gc.collect()\n    return valid_dataloader","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.294415Z","iopub.execute_input":"2021-08-30T08:46:25.294714Z","iopub.status.idle":"2021-08-30T08:46:25.307949Z","shell.execute_reply.started":"2021-08-30T08:46:25.294668Z","shell.execute_reply":"2021-08-30T08:46:25.306918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_dataloader = run(train, external_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T12:16:34.83706Z","iopub.execute_input":"2021-08-29T12:16:34.837408Z","iopub.status.idle":"2021-08-29T12:17:20.433186Z","shell.execute_reply.started":"2021-08-29T12:16:34.837379Z","shell.execute_reply":"2021-08-29T12:17:20.430909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_valid_features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()\n    \n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    features = []\n    for i in range(len(tokenized_example[\"input_ids\"])):\n        feature = {}\n        feature[\"example_id\"] = example['id']\n        feature['context'] = example['context']\n        feature['question'] = example['question']\n        feature['input_ids'] = tokenized_example['input_ids'][i]\n        feature['attention_mask'] = tokenized_example['attention_mask'][i]\n        feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n        features.append(feature)\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.309933Z","iopub.execute_input":"2021-08-30T08:46:25.310412Z","iopub.status.idle":"2021-08-30T08:46:25.319257Z","shell.execute_reply.started":"2021-08-30T08:46:25.310378Z","shell.execute_reply":"2021-08-30T08:46:25.318227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        predictions[example[\"id\"]] = best_answer[\"text\"]\n        \n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.320673Z","iopub.execute_input":"2021-08-30T08:46:25.321056Z","iopub.status.idle":"2021-08-30T08:46:25.33616Z","shell.execute_reply.started":"2021-08-30T08:46:25.321023Z","shell.execute_reply":"2021-08-30T08:46:25.335144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = Config()\nvalid_set = train[-64:].reset_index(drop=True)\ntokenizer = AutoTokenizer.from_pretrained(Config().tokenizer_name)    \nvalid_features =[]\nfor i, row in valid_set.iterrows():\n    valid_features += prepare_valid_features(args, row, tokenizer)\nvalid_dataset = DatasetRetriever(valid_features)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:25.337763Z","iopub.execute_input":"2021-08-30T08:46:25.338234Z","iopub.status.idle":"2021-08-30T08:46:26.629456Z","shell.execute_reply.started":"2021-08-30T08:46:25.338121Z","shell.execute_reply":"2021-08-30T08:46:26.62865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(checkpoint_path=os.path.join(args.output_dir, f\"checkpoint-muril/pytorch_model.bin\"), data_loader=None):\n    config, tokenizer, model = make_model(Config())\n    model.cuda();\n    model.load_state_dict(\n        torch.load(checkpoint_path)\n    );\n    \n    start_logits = []\n    end_logits = []\n    for batch in data_loader:\n        with torch.no_grad():\n            outputs_start, outputs_end = model(batch['input_ids'].cuda(), batch['attention_mask'].cuda())\n            start_logits.append(outputs_start.cpu().numpy().tolist())\n            end_logits.append(outputs_end.cpu().numpy().tolist())\n            del outputs_start, outputs_end\n    del model, tokenizer, config\n    gc.collect()\n    return np.vstack(start_logits), np.vstack(end_logits)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:46:33.358355Z","iopub.execute_input":"2021-08-30T08:46:33.358677Z","iopub.status.idle":"2021-08-30T08:46:33.366019Z","shell.execute_reply.started":"2021-08-30T08:46:33.358649Z","shell.execute_reply":"2021-08-30T08:46:33.364955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_logits, end_logits = get_predictions(data_loader=valid_dataloader)\nfinal_predictions = postprocess_qa_predictions(valid_set, valid_features, (start_logits, end_logits))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:48:42.351843Z","iopub.execute_input":"2021-08-30T08:48:42.352157Z","iopub.status.idle":"2021-08-30T08:48:52.205344Z","shell.execute_reply.started":"2021-08-30T08:48:42.352128Z","shell.execute_reply":"2021-08-30T08:48:52.202334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"references = [{\"id\": ex[1][\"id\"], \"answer\": ex[1][\"answers\"]['text'][0]} for ex in valid_set.iterrows()]","metadata":{"execution":{"iopub.status.busy":"2021-08-30T08:53:00.849728Z","iopub.execute_input":"2021-08-30T08:53:00.850075Z","iopub.status.idle":"2021-08-30T08:53:00.862255Z","shell.execute_reply.started":"2021-08-30T08:53:00.850043Z","shell.execute_reply":"2021-08-30T08:53:00.861198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Jaccard Score","metadata":{}},{"cell_type":"code","source":"def jaccard(row): \n    str1 = row[0]\n    str2 = row[1]\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Val set jaccard score","metadata":{}},{"cell_type":"code","source":"res = pd.DataFrame(references)\nres['prediction'] = res['id'].apply(lambda r: final_predictions[r])\nres['jaccard'] = res[['answer', 'prediction']].apply(jaccard, axis=1)\nres","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res.jaccard.mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare submission","metadata":{}},{"cell_type":"code","source":"test_set = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\ntest_features = []\nfor i, row in test_set.iterrows():\n    test_features += prepare_valid_features(args, row, tokenizer)\ntest_dataset = DatasetRetriever(test_features, mode=\"test\")\n\ntest_sampler = SequentialSampler(test_dataset)\n\ntest_dataloader = DataLoader(\n        test_dataset,\n        batch_size=args.eval_batch_size, \n        sampler=test_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True, \n        drop_last=False\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_logits, end_logits = get_predictions(data_loader=test_dataloader)\nfinal_predictions = postprocess_qa_predictions(test_set, test_features, (start_logits, end_logits))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\nsub['PredictionString'] = sub['id'].apply(lambda r: final_predictions[r])\nsub.head()","metadata":{},"execution_count":null,"outputs":[]}]}