{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brief Introduction\n\n### Tamil\n\n- Tamil is a Dravidian language spoken by Tamils in southern India, Sri Lanka, and elsewhere\n- Tamil language originated from Proto-Dravidian in 450BCE\n- Tamil language is derived from the Dravidian language family written in Tamil scripts. It is one of the four Dravidian languages along with Telegu, Malayalam, and Kannada\n- It is the oldest of all Dravidian languages\n- Tamil language witnesses itâ€™s existence for more than 2000 years making it the oldest and longest surviving classical language in the world\n- The Tamil language is spoken widely in India, Sri Lanka, Malaysia, Singapore, South Africa and Mauritius\n\n### Hindi\n\n- Hindi is an Indic language of northern India that derived from Vedic Sanskrit language\n- Hindi is written in the Devanagari script\n- Hindi language originated from the Indo-Aryans linguistic Family in the 17th century CE\n- It is one of the official languages of India which includes Tamil as well","metadata":{}},{"cell_type":"markdown","source":"# Competition Overview:\n\nIn this competition, the goal is to predict answers to real questions about Wikipedia articles. You will use chaii-1, a new question answering dataset with question-answer pairs. The dataset covers Hindi and Tamil, collected without the use of translation. It provides a realistic information-seeking task with questions written by native-speaking expert data annotators. ","metadata":{}},{"cell_type":"markdown","source":"\n# Competition Rules:\n- CPU Notebook <= 5 hours run-time\n-GPU Notebook <= 5 hours run-time\n-Internet access disabled\n- Freely & publicly available external data is allowed, including pre-trained models\n- Submission file must be named submission.csv","metadata":{}},{"cell_type":"markdown","source":"# Competition Metrics:\nThe metric in this competition is the word-level Jaccard score\n\n`def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))`","metadata":{}},{"cell_type":"markdown","source":"### Ref:\n- https://keras.io/examples/nlp/text_extraction_with_bert/\n- https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb","metadata":{}},{"cell_type":"markdown","source":"### External Datasets\n- This Notebook uses @rhtsingh's hindi dataset: [Hindi External](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing/data)\n- For Tamil, the dataset i have created: [Tamil External](https://www.kaggle.com/msafi04/squad-translated-to-tamil-for-chaii)","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nfrom tqdm.auto import tqdm\nimport collections\n\nimport os\n\nfrom pathlib import Path\n\nimport json\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nsns.set_palette('Set3_r')\n\npd.set_option(\"display.max_rows\", 20, \"display.max_columns\", None)\n\nprint(os.listdir('../input/'))\n        \nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport warnings\nwarnings.simplefilter(action = 'ignore', category = Warning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-03T04:59:23.616376Z","iopub.execute_input":"2021-09-03T04:59:23.616823Z","iopub.status.idle":"2021-09-03T04:59:24.796869Z","shell.execute_reply.started":"2021-09-03T04:59:23.61673Z","shell.execute_reply":"2021-09-03T04:59:24.79579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/chaii-hindi-and-tamil-question-answering/train.csv')\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:59:39.712542Z","iopub.execute_input":"2021-09-03T04:59:39.71293Z","iopub.status.idle":"2021-09-03T04:59:40.602151Z","shell.execute_reply.started":"2021-09-03T04:59:39.7129Z","shell.execute_reply":"2021-09-03T04:59:40.601301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/chaii-hindi-and-tamil-question-answering/test.csv')\nprint(test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:59:42.180149Z","iopub.execute_input":"2021-09-03T04:59:42.180721Z","iopub.status.idle":"2021-09-03T04:59:42.209258Z","shell.execute_reply.started":"2021-09-03T04:59:42.180671Z","shell.execute_reply":"2021-09-03T04:59:42.208079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\nprint(sub.shape)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:59:43.259772Z","iopub.execute_input":"2021-09-03T04:59:43.260142Z","iopub.status.idle":"2021-09-03T04:59:43.278698Z","shell.execute_reply.started":"2021-09-03T04:59:43.260113Z","shell.execute_reply":"2021-09-03T04:59:43.277745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"external_hindi1 = pd.read_csv('/kaggle/input/mlqa-hindi-processed/mlqa_hindi.csv')\nprint(external_hindi1.shape)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T04:59:44.847482Z","iopub.execute_input":"2021-09-03T04:59:44.847872Z","iopub.status.idle":"2021-09-03T04:59:45.233394Z","shell.execute_reply.started":"2021-09-03T04:59:44.847839Z","shell.execute_reply":"2021-09-03T04:59:45.232257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"external_hindi2 = pd.read_csv('/kaggle/input/mlqa-hindi-processed/xquad.csv')\nprint(external_hindi2.shape)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T04:59:46.274266Z","iopub.execute_input":"2021-09-03T04:59:46.274614Z","iopub.status.idle":"2021-09-03T04:59:46.385043Z","shell.execute_reply.started":"2021-09-03T04:59:46.274583Z","shell.execute_reply":"2021-09-03T04:59:46.383961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('External hindi dataset...')\nexternal_hindi = pd.concat([external_hindi1, external_hindi2])\nprint(external_hindi.shape)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T04:59:47.423919Z","iopub.execute_input":"2021-09-03T04:59:47.424283Z","iopub.status.idle":"2021-09-03T04:59:47.443014Z","shell.execute_reply.started":"2021-09-03T04:59:47.424251Z","shell.execute_reply":"2021-09-03T04:59:47.442012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('External Tamil dataset...')\nexternal_tamil = pd.read_csv('/kaggle/input/squad-translated-to-tamil-for-chaii/squad_translated_tamil.csv')\nexternal_tamil['language'] = 'tamil'\nprint(external_tamil.shape)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T05:00:29.877351Z","iopub.execute_input":"2021-09-03T05:00:29.877725Z","iopub.status.idle":"2021-09-03T05:00:30.171145Z","shell.execute_reply.started":"2021-09-03T05:00:29.87769Z","shell.execute_reply":"2021-09-03T05:00:30.170054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Combined External dataset...')\nexternal_df = pd.concat([external_hindi, external_tamil])\nexternal_df = external_df.sample(frac = 1).reset_index(drop = True)\nprint(external_df.shape)\nexternal_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T05:00:38.069567Z","iopub.execute_input":"2021-09-03T05:00:38.069921Z","iopub.status.idle":"2021-09-03T05:00:38.094793Z","shell.execute_reply.started":"2021-09-03T05:00:38.069893Z","shell.execute_reply":"2021-09-03T05:00:38.09381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del external_hindi1, external_hindi2, external_hindi, external_tamil\ngc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T05:02:21.755584Z","iopub.execute_input":"2021-09-03T05:02:21.756023Z","iopub.status.idle":"2021-09-03T05:02:21.877135Z","shell.execute_reply.started":"2021-09-03T05:02:21.755989Z","shell.execute_reply":"2021-09-03T05:02:21.876258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2)\nsns.countplot(x = 'language', data = external_df, ax = ax1).set_title('External Dataset Language Counts')\nfor p in ax1.patches:\n    ax1.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\nsns.countplot(x = 'language', data = test, ax = ax2).set_title('Test Language Counts')\nfor p in ax2.patches:\n    ax2.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T05:02:54.291673Z","iopub.execute_input":"2021-09-03T05:02:54.292183Z","iopub.status.idle":"2021-09-03T05:02:54.617544Z","shell.execute_reply.started":"2021-09-03T05:02:54.29214Z","shell.execute_reply":"2021-09-03T05:02:54.616605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Huggingface TF XLM RoBerta","metadata":{}},{"cell_type":"code","source":"import yaml\n\nhparams = {\n    'DEVICE': 'TPU',\n    'EPOCHS': 3,\n    'MODEL_2': '../input/jplu-tf-xlm-roberta-large',\n    'N_FOLDS': 5,\n    'SEED': 777,\n    'VERBOSE': 1,\n    'BATCH_SIZE': 16,\n    'MAX_LENGTH': 512,\n    'DOC_STRIDE': 128\n    \n}","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:13:04.211686Z","iopub.execute_input":"2021-09-01T07:13:04.212153Z","iopub.status.idle":"2021-09-01T07:13:04.222093Z","shell.execute_reply.started":"2021-09-01T07:13:04.212114Z","shell.execute_reply":"2021-09-01T07:13:04.220546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\n\nimport transformers\nfrom transformers import AutoTokenizer, TFXLMRobertaForQuestionAnswering, TFXLMRobertaModel\n\nprint(tf.__version__)\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:13:11.772126Z","iopub.execute_input":"2021-09-01T07:13:11.772585Z","iopub.status.idle":"2021-09-01T07:13:20.488609Z","shell.execute_reply.started":"2021-09-01T07:13:11.772547Z","shell.execute_reply":"2021-09-01T07:13:20.487666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = hparams['SEED']\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:13:20.490061Z","iopub.execute_input":"2021-09-01T07:13:20.490518Z","iopub.status.idle":"2021-09-01T07:13:20.495241Z","shell.execute_reply.started":"2021-09-01T07:13:20.490483Z","shell.execute_reply":"2021-09-01T07:13:20.494204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = hparams['MODEL_2']\nbatch_size = hparams['BATCH_SIZE']","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:13:21.676214Z","iopub.execute_input":"2021-09-01T07:13:21.676653Z","iopub.status.idle":"2021-09-01T07:13:21.682839Z","shell.execute_reply.started":"2021-09-01T07:13:21.676618Z","shell.execute_reply":"2021-09-01T07:13:21.680938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_special_tokens = True)\nprint(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:13:23.215396Z","iopub.execute_input":"2021-09-01T07:13:23.216095Z","iopub.status.idle":"2021-09-01T07:13:26.176398Z","shell.execute_reply.started":"2021-09-01T07:13:23.216038Z","shell.execute_reply":"2021-09-01T07:13:26.174887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['context_num_tokens'] = train['context'].apply(lambda x: len(tokenizer(x)['input_ids']))\ntrain['context_num_tokens'].max()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T05:54:01.152771Z","iopub.execute_input":"2021-08-31T05:54:01.153222Z","iopub.status.idle":"2021-08-31T05:54:43.207098Z","shell.execute_reply.started":"2021-08-31T05:54:01.153179Z","shell.execute_reply":"2021-08-31T05:54:43.206229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['context_num_tokens'].hist();","metadata":{"execution":{"iopub.status.busy":"2021-08-31T05:54:43.208506Z","iopub.execute_input":"2021-08-31T05:54:43.20903Z","iopub.status.idle":"2021-08-31T05:54:43.476229Z","shell.execute_reply.started":"2021-08-31T05:54:43.20899Z","shell.execute_reply":"2021-08-31T05:54:43.475371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The context length is too long, it has to be split into pieces before processing\n- Usually in NLP tasks long documents are truncated but in QA tasks truncating 'context' would lead to loss of answer\n- To avoid this, the long context is split into many input features each of length less than the max_length parameter\n- And if answer is at the split, we use overlapping of split features which is controlled by the parameter doc_stride","metadata":{}},{"cell_type":"code","source":"train = train.sample(frac = 1, random_state = 2021).reset_index(drop = True)\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:13:40.80366Z","iopub.execute_input":"2021-09-01T07:13:40.804142Z","iopub.status.idle":"2021-09-01T07:13:40.829937Z","shell.execute_reply.started":"2021-09-01T07:13:40.804104Z","shell.execute_reply":"2021-09-01T07:13:40.82873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split data to folds\nn_folds = hparams['N_FOLDS']\ntrain['kfold'] = -1\n\nskf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = SEED)\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X = train, y = train['language'].values)):\n    train.loc[val_idx, 'kfold'] = fold\ntrain.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:13:45.059683Z","iopub.execute_input":"2021-09-01T07:13:45.06018Z","iopub.status.idle":"2021-09-01T07:13:45.094898Z","shell.execute_reply.started":"2021-09-01T07:13:45.060136Z","shell.execute_reply":"2021-09-01T07:13:45.093837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set TPU","metadata":{}},{"cell_type":"code","source":"DEVICE = 'TPU'\n\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\n\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:13:52.787963Z","iopub.execute_input":"2021-09-01T07:13:52.788441Z","iopub.status.idle":"2021-09-01T07:13:58.56278Z","shell.execute_reply.started":"2021-09-01T07:13:52.788403Z","shell.execute_reply":"2021-09-01T07:13:58.561547Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = hparams['MAX_LENGTH'] #The maximum length of a feature (question and context)\ndoc_stride = hparams['DOC_STRIDE'] #The authorized overlap between two part of the context when splitting it if needed.\n\npad_on_right = tokenizer.padding_side == \"right\"","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:13:58.564522Z","iopub.execute_input":"2021-09-01T07:13:58.564853Z","iopub.status.idle":"2021-09-01T07:13:58.569528Z","shell.execute_reply.started":"2021-09-01T07:13:58.56482Z","shell.execute_reply":"2021-09-01T07:13:58.568696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_training(examples):\n    examples['question'] = [q.lstrip() for q in examples['question']] #remove leading white space\n    examples['question'] = [q.rstrip('?') for q in examples['question']] #remove '?' from the questions\n    \n    #Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    #in one example possible giving several features when a context is long, each of those features having a\n    #context that overlaps a bit the context of the previous feature.\n    \n    tokenized_examples = tokenizer(\n                list(examples['question' if pad_on_right else 'context'].values),\n                list(examples['context' if pad_on_right else 'question'].values),\n                truncation = 'only_second' if pad_on_right else 'only_first',\n                max_length = max_length,\n                stride = doc_stride,\n                return_overflowing_tokens = True,\n                return_offsets_mapping = True,\n                padding = 'max_length'\n            )\n    #Since one example might give us several features if it has a long context, we need a map from a feature to\n    #its corresponding example. This key gives us just that.\n    \n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    \n    #The offset mappings will give us a map from token to character position in the original context. This will\n    #help us compute the start_positions and end_positions.\n    \n    offset_mapping = tokenized_examples.pop('offset_mapping')\n    \n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        \n        sequence_ids = tokenized_examples.sequence_ids(i)\n        \n        #One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples.loc[sample_index, 'answer_text']\n        start_char = examples.loc[sample_index, 'answer_start']\n        \n        # If no answers are given, set the cls_index as answer.\n        if start_char is None:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            # Start/end character idx of the answer in the text.\n            end_char = start_char + len(answers)\n            \n             #Start token idx of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n            #Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                #Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                #Note: we could go after the last offset if the answer is the last word (edge case).\n                \n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                \n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:13:58.571959Z","iopub.execute_input":"2021-09-01T07:13:58.572432Z","iopub.status.idle":"2021-09-01T07:13:58.588724Z","shell.execute_reply.started":"2021-09-01T07:13:58.57238Z","shell.execute_reply":"2021-09-01T07:13:58.587431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_validation(examples):\n    examples['question'] = [q.lstrip() for q in examples['question']]\n    examples['question'] = [q.rstrip('?') for q in examples['question']]\n    \n    tokenized_examples = tokenizer(\n                list(examples['question' if pad_on_right else 'context'].values),\n                list(examples['context' if pad_on_right else 'question'].values),\n                truncation = 'only_second' if pad_on_right else 'only_first',\n                max_length = max_length,\n                stride = doc_stride,\n                return_overflowing_tokens = True,\n                return_offsets_mapping = True,\n                padding = 'max_length'\n            )\n    \n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    \n    #id column from the dataset\n    tokenized_examples['example_id'] = []\n\n    for i in range(len(tokenized_examples['input_ids'])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples.loc[sample_index, 'id'])\n        tokenized_examples['offset_mapping'][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples['offset_mapping'][i])\n        ]\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:14:00.194114Z","iopub.execute_input":"2021-09-01T07:14:00.194683Z","iopub.status.idle":"2021-09-01T07:14:00.205421Z","shell.execute_reply.started":"2021-09-01T07:14:00.194631Z","shell.execute_reply":"2021-09-01T07:14:00.204466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_tf_dataset(df, batch_size = 4, flag = 'train'):\n    \n    if flag == 'train':\n        features = prepare_training(df)\n    else:\n        features = prepare_validation(df)\n    \n    input_ids = features['input_ids']\n    attn_masks = features['attention_mask']\n    \n    if flag == 'train':\n        start_positions = features['start_positions']\n        end_positions = features['end_positions']\n        \n        train_dataset = tf.data.Dataset.from_tensor_slices((input_ids, attn_masks, start_positions, end_positions))\n        train_dataset = train_dataset.map(lambda x1, x2, y1, y2: ({'input_ids': x1, 'attention_mask': x2}, {'start_positions': y1, 'end_positions': y2}))\n        train_dataset = train_dataset.batch(batch_size)\n        train_dataset = train_dataset.shuffle(1000)\n        train_dataset = train_dataset.prefetch(AUTO)\n        \n        return train_dataset, features\n    \n    elif flag == 'valid':\n        dataset = tf.data.Dataset.from_tensor_slices((input_ids, attn_masks))\n        dataset = dataset.map(lambda x1, x2: ({'input_ids': x1, 'attention_mask': x2}))\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(buffer_size = AUTO)\n        \n        return dataset, features","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:14:01.470986Z","iopub.execute_input":"2021-09-01T07:14:01.471614Z","iopub.status.idle":"2021-09-01T07:14:01.48232Z","shell.execute_reply.started":"2021-09-01T07:14:01.471564Z","shell.execute_reply":"2021-09-01T07:14:01.481305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF XLM RoBerta Model","metadata":{}},{"cell_type":"code","source":"def build_model():\n    roberta = TFXLMRobertaModel.from_pretrained(model_checkpoint)\n    \n    input_ids = tf.keras.layers.Input(shape = (max_length, ), name = 'input_ids', dtype = tf.int32)\n    attention_mask = tf.keras.layers.Input(shape = (max_length, ), name = 'attention_mask', dtype = tf.int32)\n    \n    embeddings = roberta(input_ids = input_ids, attention_mask = attention_mask)[0]\n    \n    x1 = tf.keras.layers.Dropout(0.1)(embeddings) \n    x1 = tf.keras.layers.Dense(1, use_bias = False)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax', name = 'start_positions')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(embeddings) \n    x2 = tf.keras.layers.Dense(1, use_bias = False)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax', name = 'end_positions')(x2)\n\n    model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = [x1, x2])\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n    \n    model.compile(loss = [loss, loss], optimizer = optimizer)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:14:03.795998Z","iopub.execute_input":"2021-09-01T07:14:03.796707Z","iopub.status.idle":"2021-09-01T07:14:03.806788Z","shell.execute_reply.started":"2021-09-01T07:14:03.796657Z","shell.execute_reply":"2021-09-01T07:14:03.805925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:14:05.581687Z","iopub.execute_input":"2021-09-01T07:14:05.58218Z","iopub.status.idle":"2021-09-01T07:14:05.592101Z","shell.execute_reply.started":"2021-09-01T07:14:05.582127Z","shell.execute_reply":"2021-09-01T07:14:05.589612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post Process Predictions","metadata":{}},{"cell_type":"code","source":"def post_process_predictions(examples, features, start, end, n_best_size = 20, max_answer_length = 30):\n    \n    all_start_logits, all_end_logits = start, end\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    \n    for i, feature in enumerate(features['example_id']):\n        features_per_example[example_id_to_index[feature]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features['input_ids'])} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in examples.iterrows():\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n        min_null_score = None # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        context = example['context']\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features['offset_mapping'][feature_index]\n\n            # Update minimum null prediction.\n            cls_index = features['input_ids'][feature_index].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key = lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        #if not squad_v2:\n        #    predictions[example[\"id\"]] = best_answer[\"text\"]\n        #else:\n        answer = best_answer[\"text\"] \n        predictions[example['id']] = answer\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:14:07.246283Z","iopub.execute_input":"2021-09-01T07:14:07.246956Z","iopub.status.idle":"2021-09-01T07:14:07.264348Z","shell.execute_reply.started":"2021-09-01T07:14:07.246916Z","shell.execute_reply":"2021-09-01T07:14:07.263217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tune Model","metadata":{}},{"cell_type":"code","source":"strart_probs, end_probs = [], []\nepochs = hparams['EPOCHS']\njaccard_scores = []\n\nfor i, fold in enumerate(range(n_folds)):\n    print('#########' * 15)\n    print(f\"Fold: {fold + 1}\")\n    print('#########' * 15)\n    train_df = train[train['kfold'] != fold]\n    valid_df = train[train['kfold'] == fold]\n    \n    #concat external_df to train_df for training, no change in valid_df\n    train_df = pd.concat([train_df.iloc[:, 1:-1], external_df])\n\n    train_df = train_df.reset_index(drop = True)\n    valid_df = valid_df.reset_index(drop = True)\n    print(train_df.shape, valid_df.shape)\n    \n    train_dataset, train_enc = build_tf_dataset(train_df, batch_size = batch_size, flag = 'train')\n    valid_dataset, valid_enc = build_tf_dataset(valid_df, batch_size = batch_size, flag = 'valid')\n    \n    K.clear_session()\n    \n    with strategy.scope():\n        model = build_model()\n    if i == 0:\n        print(model.summary())\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'qa_model_{fold + 1}.h5', verbose = 1, monitor = 'loss', mode = 'min', save_best_only = True, \n                                                save_weights_only = True)\n\n    history = model.fit(train_dataset, \n                        epochs = epochs, \n                        batch_size = batch_size,\n                        callbacks = [checkpoint],\n                        verbose = 1\n                        )\n    print('Predicting valid dataset...')\n    start_pred, end_pred = model.predict(valid_dataset, batch_size = batch_size, verbose = 1)\n    print(start_pred.shape, end_pred.shape)\n    print('Post-process predictions...')\n    valid_preds = post_process_predictions(valid_df, valid_enc, start_pred, end_pred)\n    \n    score = []\n    for idx in range(len(valid_df)):\n        str1 = valid_df['answer_text'].values[idx]\n        str2 = valid_preds[valid_df.loc[idx, 'id']]\n        score.append(jaccard(str1, str2))\n    print(f'Jaccard Score for fold {fold + 1}: {np.mean(score)}')\n    jaccard_scores.append(np.mean(score))\n    \n    strart_probs.append(start_pred)\n    end_probs.append(end_pred)\n    \n    del train_dataset, valid_dataset, model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:14:10.740707Z","iopub.execute_input":"2021-09-01T07:14:10.741167Z","iopub.status.idle":"2021-09-01T07:17:13.494024Z","shell.execute_reply.started":"2021-09-01T07:14:10.741125Z","shell.execute_reply":"2021-09-01T07:17:13.484163Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save the hyperparameters and the validation scores for prediction notebook","metadata":{}},{"cell_type":"code","source":"hparams['JAC_SCORES'] = jaccard_scores\n\nwith open(r'hparams.yaml', 'w') as f:\n    yaml.dump(hparams, f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on Test Data","metadata":{}},{"cell_type":"code","source":"test_dataset, test_enc = build_tf_dataset(test, batch_size = batch_size, flag = 'valid')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T06:41:20.686524Z","iopub.execute_input":"2021-08-31T06:41:20.686929Z","iopub.status.idle":"2021-08-31T06:41:21.187866Z","shell.execute_reply.started":"2021-08-31T06:41:20.686896Z","shell.execute_reply":"2021-08-31T06:41:21.186842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_probs, end_probs = [], []\nfor fold in range(n_folds):\n    with strategy.scope():\n        model = build_model()\n    print('Loading trained model weights...')\n    model.load_weights(f'qa_model_{fold + 1}.h5')\n    print(f'Predicting testset - Fold: {fold + 1}...')\n    start_pred, end_pred = model.predict(test_dataset, batch_size = batch_size, verbose = 1)\n    print(start_pred.shape, end_pred.shape)\n    start_probs.append(start_pred)\n    end_probs.append(end_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T06:41:22.835514Z","iopub.execute_input":"2021-08-31T06:41:22.835892Z","iopub.status.idle":"2021-08-31T06:46:27.519036Z","shell.execute_reply.started":"2021-08-31T06:41:22.835859Z","shell.execute_reply":"2021-08-31T06:46:27.517885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_start_probs, test_end_probs = np.mean(start_probs, axis = 0), np.mean(end_probs, axis = 0)\npredictions = post_process_predictions(test, test_enc, test_start_probs, test_end_probs)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T06:53:17.823337Z","iopub.execute_input":"2021-08-31T06:53:17.823906Z","iopub.status.idle":"2021-08-31T06:53:17.851515Z","shell.execute_reply.started":"2021-08-31T06:53:17.82387Z","shell.execute_reply":"2021-08-31T06:53:17.850764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.DataFrame({'id': list(predictions.keys()), 'PredictionString': list(predictions.values())})\nsub_df.to_csv('./submission.csv', index = False)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T06:53:20.89218Z","iopub.execute_input":"2021-08-31T06:53:20.892547Z","iopub.status.idle":"2021-08-31T06:53:20.909857Z","shell.execute_reply.started":"2021-08-31T06:53:20.892517Z","shell.execute_reply":"2021-08-31T06:53:20.909108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:21:58.539399Z","iopub.status.idle":"2021-08-31T01:21:58.53986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks to Kaggle and fellow Kagglers for the all the learnings, nothing beats doing and learning!!!","metadata":{}}]}