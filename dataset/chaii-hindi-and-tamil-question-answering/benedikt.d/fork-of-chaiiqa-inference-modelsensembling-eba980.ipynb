{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U --no-build-isolation --no-deps ../input/transformers-master/ -qq","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:06.445877Z","iopub.execute_input":"2021-11-12T23:16:06.446157Z","iopub.status.idle":"2021-11-12T23:16:39.600216Z","shell.execute_reply.started":"2021-11-12T23:16:06.446093Z","shell.execute_reply":"2021-11-12T23:16:39.599106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport pickle\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nimport matplotlib.pyplot as plt\nimport transformers\n#from transformers import AutoTokenizer\nfrom glob import glob\nimport pytorch_lightning as pl\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom pathlib import Path\nimport collections\nimport seaborn as sns\nimport gc\n\n\nin_folder_path = Path('.')\ntrain_scripts_dir = Path(in_folder_path)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:39.602984Z","iopub.execute_input":"2021-11-12T23:16:39.603544Z","iopub.status.idle":"2021-11-12T23:16:43.020597Z","shell.execute_reply.started":"2021-11-12T23:16:39.6035Z","shell.execute_reply":"2021-11-12T23:16:43.01969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SINGLE_FOLD_MODE = True","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:43.022772Z","iopub.execute_input":"2021-11-12T23:16:43.023117Z","iopub.status.idle":"2021-11-12T23:16:43.029536Z","shell.execute_reply.started":"2021-11-12T23:16:43.023081Z","shell.execute_reply":"2021-11-12T23:16:43.028729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConfigTemplate:\n    def __init__(self, config):\n        self.__dict__.update(config)\n   ","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:43.03305Z","iopub.execute_input":"2021-11-12T23:16:43.033367Z","iopub.status.idle":"2021-11-12T23:16:43.038498Z","shell.execute_reply.started":"2021-11-12T23:16:43.033313Z","shell.execute_reply":"2021-11-12T23:16:43.037737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\ntest_df = test_df.rename(columns={'id': 'index'})\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:43.039735Z","iopub.execute_input":"2021-11-12T23:16:43.040195Z","iopub.status.idle":"2021-11-12T23:16:43.080129Z","shell.execute_reply.started":"2021-11-12T23:16:43.040158Z","shell.execute_reply":"2021-11-12T23:16:43.079254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict \nimport gc\n\ndef postprocess_qa_predictions_new(data_df, archs_features_files, archs_preds_files, archs_weights):\n    import matplotlib.pyplot as plt\n    plt.rcParams[\"figure.figsize\"] = (30, 15)\n    \n    DEBUG = False\n    ####################\n    START, END = 0, 1\n    max_answer_length = 80\n\n    archs_seqs = defaultdict(list)\n    \n    for arch_i, (arch_feats_file, arch_preds_file) in enumerate(zip(archs_feats_files, archs_preds_files)):\n        with open(arch_feats_file, 'rb') as f:\n            arch_feats = pickle.load(f)\n        \n        with open(arch_preds_file, 'rb') as f:\n            arch_preds = pickle.load(f) \n            \n        for _, row in data_df.iterrows():\n            sample_index = row['index']\n            context = row['context']\n            char_start_logits = np.zeros(len(context))\n            char_end_logits = np.zeros(len(context))\n\n            arch_char_start_logits = np.zeros(len(context))\n            arch_char_end_logits = np.zeros(len(context))\n            \n            for model_i, model_preds in enumerate(arch_preds):\n                model_sample_preds = model_preds[sample_index]\n                sample_features = arch_feats.query('index == @sample_index')\n                if len(sample_features) == 0:\n                    continue\n                offsets_mappings = sample_features.offset_mapping.values\n                sequences_ids = sample_features.sequence_ids.values\n\n                model_char_start_logits = np.zeros(len(context))\n                model_char_end_logits = np.zeros(len(context))\n                offsets_counter = collections.Counter()\n\n                for feat_i, (model_pred, sequence_ids, offsets_mapping) in enumerate(zip(model_sample_preds, sequences_ids, offsets_mappings)):\n                    sequence_ids = np.array(sequence_ids)\n                    offsets_mapping = np.array(offsets_mapping)\n\n                    context_offsets_mapping = offsets_mapping[sequence_ids==1]\n                    context_start_logits = np.array(model_pred[START])[sequence_ids==1]\n                    context_end_logits = np.array(model_pred[END])[sequence_ids==1]\n\n                    for (s, e), s_logit, e_logit in zip(context_offsets_mapping, context_start_logits, context_end_logits):\n                        if offsets_counter[(s, e)]:\n                            s_logit = max([s_logit, model_char_start_logits[s:e][0]])\n                            e_logit = max([e_logit,  model_char_end_logits[s:e][0]])\n\n                        model_char_start_logits[s:e] = s_logit\n                        model_char_end_logits[s:e] = e_logit\n                        offsets_counter[(s, e)] += 1\n\n                arch_char_start_logits += model_char_start_logits\n                arch_char_end_logits += model_char_end_logits\n            \n            #### AVERAGE arch_char_start_logits and arch_char_end_logits FIRST AND THEN APPLY WEIGHTS\n            arch_char_start_logits = arch_char_start_logits * archs_weights[arch_i]\n            arch_char_end_logits = arch_char_end_logits * archs_weights[arch_i]\n            \n            if not sample_index in archs_seqs:\n                archs_seqs[sample_index] = (arch_char_start_logits, arch_char_end_logits)\n            else:\n                start_logits, end_logits = archs_seqs[sample_index]\n                archs_seqs[sample_index] = (start_logits+arch_char_start_logits, end_logits+arch_char_end_logits)\n                \n        del arch_feats, arch_preds\n        gc.collect()\n            \n    predictions = collections.OrderedDict()  \n    for _, row in data_df.iterrows():\n        sample_index = row['index']\n        context = row['context']\n        \n        char_start_logits, char_end_logits = archs_seqs[sample_index]\n\n        char_start_logits_indices = (char_start_logits > 0).nonzero()[0]\n        char_start_logits = char_start_logits[char_start_logits_indices]\n\n        char_end_logits_indices = (char_end_logits > 0).nonzero()[0]\n        char_end_logits = char_end_logits[char_end_logits_indices]\n\n        if DEBUG:\n            plt.bar(range(char_start_logits.shape[0]), char_start_logits, color='black', edgecolor='black', )  \n            plt.xticks(range(char_start_logits.shape[0]), char_start_logits_indices, rotation=-20)\n\n            plt.show()\n\n            plt.bar(range(char_end_logits.shape[0]), char_end_logits, color='black', edgecolor='black')    \n            plt.xticks(range(char_end_logits.shape[0]), char_end_logits_indices, rotation=-20, )\n\n            plt.show()\n\n\n        inds = np.sort(np.unique(char_start_logits, return_index=True)[1])\n        char_start_logits_unique = char_start_logits[inds]\n\n        char_start_logits_indices_groups = np.split(char_start_logits_indices, inds)[1:]\n        char_start_logits_indices_groups = [arr[0] for arr in char_start_logits_indices_groups]\n\n\n        inds = np.sort(np.unique(char_end_logits, return_index=True)[1])\n        char_end_logits_unique = char_end_logits[inds]\n\n        char_end_logits_indices_groups = np.split(char_end_logits_indices, inds)[1:]\n\n        char_end_logits_indices_groups = [arr[-1] for arr in char_end_logits_indices_groups]\n\n        ####### get best probs\n        valid_answers = []\n        start_logits_to_indices = sorted(list(zip(char_start_logits_unique, char_start_logits_indices_groups)), reverse=True)\n        end_logits_to_indices = sorted(list(zip(char_end_logits_unique, char_end_logits_indices_groups)), reverse=True)\n\n        for start_logit, start_index in start_logits_to_indices:\n            for end_logit, end_index in end_logits_to_indices:\n\n                if (end_index < start_index or end_index - start_index + 1 > max_answer_length):\n                    continue\n                valid_answers.append(\n                    {\n                        'score': start_logit + end_logit,\n                        'text': context[start_index: end_index+1].strip(),\n                        'token_positions': (start_index, end_index),\n                        'logits': (start_logit, end_logit)\n                    }\n                )    \n        if len(valid_answers) > 0:\n            best_answers = sorted(valid_answers, key=lambda x: x['score'], reverse=True)\n\n            if DEBUG:\n                for ans in best_answers[:10]:\n                    print(ans)\n\n            best_answer = best_answers[0]\n        else:\n            best_answer = {'text': '', 'score': 0.0}\n        predictions[row['index']] = best_answer\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:43.081521Z","iopub.execute_input":"2021-11-12T23:16:43.081898Z","iopub.status.idle":"2021-11-12T23:16:43.10844Z","shell.execute_reply.started":"2021-11-12T23:16:43.081865Z","shell.execute_reply":"2021-11-12T23:16:43.107652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize(v):\n    temp = []\n    for dat in v:\n        norm = np.linalg.norm(dat)\n        if norm == 0: \n            temp.append(dat)\n        temp.append(dat/norm)\n    return temp","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:43.109853Z","iopub.execute_input":"2021-11-12T23:16:43.110477Z","iopub.status.idle":"2021-11-12T23:16:43.11858Z","shell.execute_reply.started":"2021-11-12T23:16:43.110425Z","shell.execute_reply":"2021-11-12T23:16:43.117774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_v2(v):\n    temp2 = []\n    for dat in v:\n        temp = []\n        for d in dat:\n            norm = np.linalg.norm(d)\n            if norm == 0: \n                temp.append(d)\n            temp.append(d/norm)\n        temp2.append(temp)\n    return temp2","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:43.122902Z","iopub.execute_input":"2021-11-12T23:16:43.123161Z","iopub.status.idle":"2021-11-12T23:16:43.129043Z","shell.execute_reply.started":"2021-11-12T23:16:43.123138Z","shell.execute_reply":"2021-11-12T23:16:43.127455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureExtractor:\n    OUT_COLUMNS = ['index', 'input_ids', 'attention_mask', 'offset_mapping', 'sequence_ids']\n    TRAIN_SPECIFIC_COLUMNS = ['start_position', 'end_position', 'kfold']\n   \n    \n    def __init__(self, tokenizer, cls_token, test=False):\n        self.tokenizer = tokenizer\n        self.test = test\n        self.columns = self.OUT_COLUMNS + [self.TRAIN_SPECIFIC_COLUMNS, []][self.test==True]\n#         self.cls_token = '<CLS>'\n        self.cls_token = cls_token\n        self.cls_token_id = self.tokenizer.encode(self.cls_token)[0]\n        \n    def extract_features(self, df):\n        features = []\n        for i in tqdm(range(len(df))):\n            row = df.iloc[i]\n            tokenized_sample = self.tokenizer(\n                row[\"question\"].strip(),\n                row[\"context\"],\n                truncation=\"only_second\",\n                max_length=Config.max_seq_length,\n                stride=Config.doc_stride,\n                return_overflowing_tokens=True,\n                return_offsets_mapping=True,\n                padding=\"max_length\",\n            )\n\n\n            sample_mapping = tokenized_sample.pop(\"overflow_to_sample_mapping\")\n            offset_mapping = tokenized_sample.pop(\"offset_mapping\")\n            \n#             if not isinstance(answers[\"text\"][0], str):\n#                 continue\n\n            for i, offsets in enumerate(offset_mapping):\n                feature = {}\n\n                input_ids = tokenized_sample[\"input_ids\"][i]\n                attention_mask = tokenized_sample[\"attention_mask\"][i]\n                feature['index'] = row['index']\n                feature['input_ids'] = input_ids\n                feature['attention_mask'] = attention_mask\n                feature['offset_mapping'] = offsets\n                feature['sequence_ids'] = [0 if i is None else i for i in tokenized_sample.sequence_ids(i)]\n                if not self.test:\n                    feature['kfold'] = row['kfold']\n                    cls_index = input_ids.index(self.cls_token_id)\n                    sequence_ids = tokenized_sample.sequence_ids(i)\n                    answers = row[\"answers\"]\n\n                    sample_index = sample_mapping[i]\n                    answers = row[\"answers\"]\n\n                    if len(answers[\"answer_start\"]) == 0:\n                        feature[\"start_position\"] = cls_index\n                        feature[\"end_position\"] = cls_index\n                    else:\n                        start_char = answers[\"answer_start\"][0]\n\n                        end_char = start_char + len(answers[\"text\"][0])\n\n                        token_start_index = 0\n                        while sequence_ids[token_start_index] != 1:\n                            token_start_index += 1\n\n                        token_end_index = len(input_ids) - 1\n                        while sequence_ids[token_end_index] != 1:\n                            token_end_index -= 1\n\n                        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                            feature[\"start_position\"] = cls_index\n                            feature[\"end_position\"] = cls_index\n                        else:\n                            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                                token_start_index += 1\n                            feature[\"start_position\"] = token_start_index - 1\n                            while offsets[token_end_index][1] >= end_char:\n                                token_end_index -= 1\n                            feature[\"end_position\"] = token_end_index + 1\n\n                features.append(feature)\n\n        \n        features_df = pd.DataFrame(data=features, columns=self.columns)\n        return features_df","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:43.130871Z","iopub.execute_input":"2021-11-12T23:16:43.131213Z","iopub.status.idle":"2021-11-12T23:16:43.148484Z","shell.execute_reply.started":"2021-11-12T23:16:43.131179Z","shell.execute_reply":"2021-11-12T23:16:43.147693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrainer = pl.Trainer(\n     gpus=1, precision=16\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:43.149908Z","iopub.execute_input":"2021-11-12T23:16:43.150448Z","iopub.status.idle":"2021-11-12T23:16:43.214362Z","shell.execute_reply.started":"2021-11-12T23:16:43.150412Z","shell.execute_reply":"2021-11-12T23:16:43.21367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\narch_paths = [\n        '../input/rembertnd',\n    '../input/murillargendv1',\n    '../input/xlmrobertalargend701',\n    '../input/infoxlmlargend'\n#     '../input/rembertcv684',\n]\n\narchs_paths_sf = [\n        [\n      '../input/rembertnd/fold_2_score_0.71650.pth',\n     '../input/rembertnd/fold_3_score_0.69884.pth',\n    ],\n    \n    [#'../input/murillargendv1/fold_0_score_0.74931.pth',\n    '../input/murillargendv1/fold_1_score_0.70923.pth',\n    '../input/murillargendv1/fold_2_score_0.72832.pth',\n    '../input/murillargendv1/fold_3_score_0.73089.pth',#,\n    '../input/murillargendv1/fold_4_score_0.66972.pth'\n    ],\n    \n    [#'../input/xlmrobertalargend701/fold_0_score_0.722.pth',\n    '../input/xlmrobertalargend701/fold_1_score_0.702.pth',\n    '../input/xlmrobertalargend701/fold_2_score_0.714.pth',\n    '../input/xlmrobertalargend701/fold_3_score_0.713.pth',\n    '../input/xlmrobertalargend701/fold_4_score_0.655.pth'\n    ],\n    [\n     #'../input/infoxlmlargend/fold_1_score_0.69163.pth',\n     '../input/infoxlmlargend/fold_2_score_0.72072.pth',\n     '../input/infoxlmlargend/fold_3_score_0.68688.pth',\n        '../input/infoxlmlargend/fold_4_score_0.65945.pth'\n    ]\n    \n#     [\n#      # '../input/rembertcv684/fold_2_score_0.709.pth',\n#      '../input/rembertcv684/fold_3_score_0.675.pth',\n#      '../input/rembertcv684/fold_4_score_0.670.pth'  \n#     ]\n]\ni = 0\n\nfor arch_path in arch_paths:\n    models_preds = []\n    print(arch_path)\n    #########################################\n    ############## import scripts ###########\n    #########################################\n    os.chdir(arch_path)\n    exec(Path(\"config.py\").read_text())\n    exec(Path(\"dataset.py\").read_text())\n    exec(Path(\"model.py\").read_text())\n#     exec(Path(\"postprocess.py\").read_text())\n#     exec(Path('preprocess.py').read_text()) \n    os.chdir('/kaggle/working')\n    if \"murillarge\" in arch_path:\n        Default_Config['model_type'] = '../input/muril-large-pt/muril-large-cased'\n        Default_Config['model_type_tokenizer'] = '../input/muril-large-pt/muril-large-cased'\n    Default_Config['eval_batch_size'] = 128\n    if 'rembert' in arch_path:\n        Default_Config['eval_batch_size'] = 64\n\n    Config = ConfigTemplate(Default_Config)\n\n        \n    ###########################################\n    \n    tokenizer = torch.load(Path(arch_path) / 'tokenizer.pth')\n    preprocessor = FeatureExtractor(tokenizer, cls_token=tokenizer.cls_token, test=True)\n    feats = preprocessor.extract_features(test_df)\n    test_ds = QADataset(feats, mode='test')\n    test_dl = DataLoader(\n        test_ds,\n        batch_size=Config.eval_batch_size, \n        shuffle=False,\n        num_workers=4,\n        pin_memory=True, \n        drop_last=False\n    )\n    \n    \n    for model_path in archs_paths_sf[i]:\n        print('\\t', model_path)\n        ptmodel = torch.load(model_path, map_location=torch.device('cuda'))\n        params = {\n            'tokenizer': tokenizer\n        }\n        model = Model(ptmodel, params=params)\n        preds = trainer.predict(model, test_dl)\n        sample_id_to_preds = collections.defaultdict(list)\n\n        for batch_indices, batch_start_preds, batch_end_preds in preds:\n            for index, feat_start_preds, feat_end_preds in zip(batch_indices, batch_start_preds, batch_end_preds):\n                sample_id_to_preds[index].append(normalize((feat_start_preds, feat_end_preds)))\n\n        models_preds.append(sample_id_to_preds)\n\n    open_preds = open(f\"preds_{arch_path.split('/')[-1]}.pkl\", \"wb\")\n    pickle.dump(models_preds, open_preds)\n    \n    open_feats = open(f\"feats_{arch_path.split('/')[-1]}.pkl\", \"wb\")\n    pickle.dump(feats, open_feats)\n        \n    torch.cuda.empty_cache()\n    del ptmodel, model, models_preds, sample_id_to_preds, feats\n    gc.collect()\n    i+=1\n","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:16:43.215482Z","iopub.execute_input":"2021-11-12T23:16:43.215809Z","iopub.status.idle":"2021-11-12T23:21:03.474847Z","shell.execute_reply.started":"2021-11-12T23:16:43.215775Z","shell.execute_reply":"2021-11-12T23:21:03.473939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodels_paths = [\n        '../input/wechselhindiv1/fold_0_score_0.75536.pth',\n        '../input/wechselhindiv1/fold_1_score_0.72663.pth',\n        '../input/wechselhindiv1/fold_2_score_0.76238.pth'\n]\n\n\nmodels_preds = []\narch_path = '../input/wechselhindiv1'\narch_paths.append(arch_path)\nprint(arch_path)\n#########################################\n############## import scripts ###########\n#########################################\nos.chdir(arch_path)\nexec(Path(\"config.py\").read_text())\nexec(Path(\"dataset.py\").read_text())\nexec(Path(\"model.py\").read_text())\n#     exec(Path(\"postprocess.py\").read_text())\n#     exec(Path('preprocess.py').read_text()) \nos.chdir('/kaggle/working')\nDefault_Config['eval_batch_size'] = 128\nConfig = ConfigTemplate(Default_Config)\n\n###########################################\n\n#     tokenizer = torch.load(Path(arch_path) / 'tokenizer.pth')\ntokenizer = torch.load('../input/chaiiqa-sampling/roberta-large-wechsel-hindi_tokenizer.pt')\npreprocessor = FeatureExtractor(tokenizer, cls_token=tokenizer.cls_token, test=True)\n\nfeats = preprocessor.extract_features(test_df.query(\"language == 'hindi'\"))\ntest_ds = QADataset(feats, mode='test')\ntest_dl = DataLoader(\n    test_ds,\n    batch_size=Config.eval_batch_size, \n    shuffle=False,\n    num_workers=4,\n    pin_memory=True, \n    drop_last=False\n)\n\n\nfor model_path in models_paths:\n    print('\\t', model_path)\n    ptmodel = torch.load(model_path, map_location=torch.device('cuda'))\n    params = {\n        'tokenizer': tokenizer\n    }\n    model = Model(ptmodel, params=params)\n    preds = trainer.predict(model, test_dl)\n    sample_id_to_preds = collections.defaultdict(list)\n\n    for batch_indices, batch_start_preds, batch_end_preds in preds:\n        for index, feat_start_preds, feat_end_preds in zip(batch_indices, batch_start_preds, batch_end_preds):\n            sample_id_to_preds[index].append(normalize((feat_start_preds, feat_end_preds)))\n\n    models_preds.append(sample_id_to_preds)\n\nopen_preds = open(f\"preds_{arch_path.split('/')[-1]}.pkl\", \"wb\")\npickle.dump(models_preds, open_preds)\n\nopen_feats = open(f\"feats_{arch_path.split('/')[-1]}.pkl\", \"wb\")\npickle.dump(feats, open_feats)\n\ndel ptmodel, model, models_preds, sample_id_to_preds, feats, open_preds, test_ds, test_dl\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:21:03.476489Z","iopub.execute_input":"2021-11-12T23:21:03.476851Z","iopub.status.idle":"2021-11-12T23:21:47.462584Z","shell.execute_reply.started":"2021-11-12T23:21:03.476806Z","shell.execute_reply":"2021-11-12T23:21:47.461715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_paths = [\n        '../input/wechseltamilv3/fold_0_score_0.59998.pth'\n]\n\n\nmodels_preds = []\narch_path = '../input/wechseltamilv2'\narch_paths.append(arch_path)\nprint(arch_path)\n#########################################\n############## import scripts ###########\n#########################################\nos.chdir(arch_path)\nexec(Path(\"config.py\").read_text())\nexec(Path(\"dataset.py\").read_text())\nexec(Path(\"model.py\").read_text())\n#     exec(Path(\"postprocess.py\").read_text())\n#     exec(Path('preprocess.py').read_text()) \nos.chdir('/kaggle/working')\nDefault_Config['eval_batch_size'] = 128\nConfig = ConfigTemplate(Default_Config)\n\n###########################################\n\n#     tokenizer = torch.load(Path(arch_path) / 'tokenizer.pth')\ntokenizer = torch.load('../input/chaiiqa-sampling/roberta-large-wechsel-tamil_tokenizer.pt')\npreprocessor = FeatureExtractor(tokenizer, cls_token=tokenizer.cls_token, test=True)\n\nfeats = preprocessor.extract_features(test_df.query(\"language == 'tamil'\"))\ntest_ds = QADataset(feats, mode='test')\ntest_dl = DataLoader(\n    test_ds,\n    batch_size=Config.eval_batch_size, \n    shuffle=False,\n    num_workers=4,\n    pin_memory=True, \n    drop_last=False\n)\n\n\nfor model_path in models_paths:\n    print('\\t', model_path)\n    ptmodel = torch.load(model_path, map_location=torch.device('cuda'))\n    params = {\n        'tokenizer': tokenizer\n    }\n    model = Model(ptmodel, params=params)\n    preds = trainer.predict(model, test_dl)\n    sample_id_to_preds = collections.defaultdict(list)\n\n    for batch_indices, batch_start_preds, batch_end_preds in preds:\n        for index, feat_start_preds, feat_end_preds in zip(batch_indices, batch_start_preds, batch_end_preds):\n            sample_id_to_preds[index].append(normalize((feat_start_preds, feat_end_preds)))\n\n    models_preds.append(sample_id_to_preds)\n\nopen_preds = open(f\"preds_{arch_path.split('/')[-1]}.pkl\", \"wb\")\npickle.dump(models_preds, open_preds)\n\nopen_feats = open(f\"feats_{arch_path.split('/')[-1]}.pkl\", \"wb\")\npickle.dump(feats, open_feats)\n\ndel ptmodel, model, models_preds, sample_id_to_preds, feats, open_preds, test_ds, test_dl\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:21:47.464003Z","iopub.execute_input":"2021-11-12T23:21:47.464342Z","iopub.status.idle":"2021-11-12T23:22:12.257792Z","shell.execute_reply.started":"2021-11-12T23:21:47.464307Z","shell.execute_reply":"2021-11-12T23:22:12.256886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\narchs_weights = [0.168,0.294,0.441,0.057,0.04,0.04]\nassert round(sum(archs_weights[:-1]), 3) == 1.\nassert archs_weights[-2] == archs_weights[-1]\n\narchs_preds_files = []\narchs_feats_files = []\n\nfor arch_path in arch_paths:\n    archs_preds_files.append(f\"preds_{arch_path.split('/')[-1]}.pkl\")\n    archs_feats_files.append(f\"feats_{arch_path.split('/')[-1]}.pkl\")\n\nprint(archs_preds_files)\nprint(archs_feats_files)\nnew_preds = postprocess_qa_predictions_new(test_df, archs_feats_files, archs_preds_files, archs_weights)\n\n# del archs_feats, archs_preds","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:27:17.984995Z","iopub.execute_input":"2021-11-12T23:27:17.985312Z","iopub.status.idle":"2021-11-12T23:27:21.111407Z","shell.execute_reply.started":"2021-11-12T23:27:17.985282Z","shell.execute_reply":"2021-11-12T23:27:21.110565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\nsubmission_df.PredictionString = submission_df.apply(lambda row: new_preds[row['id']]['text'], axis=1)\n\ntest_df = test_df.rename(columns={'index': 'id'})\ntest_data =pd.merge(left=test_df,right=submission_df,on='id')","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:27:22.048266Z","iopub.execute_input":"2021-11-12T23:27:22.048598Z","iopub.status.idle":"2021-11-12T23:27:22.080463Z","shell.execute_reply.started":"2021-11-12T23:27:22.048569Z","shell.execute_reply":"2021-11-12T23:27:22.079699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\ntamil_ad = \"கி.பி\"\ntamil_bc = \"கி.மு\"\ntamil_km = \"கி.மீ\"\nhindi_ad = \"ई\"\nhindi_bc = \"ई.पू\"\n\n\ncleaned_preds = []\nfor pred, context in test_data[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n\n    cleaned_preds.append(pred)\n\ntest_data[\"PredictionString\"] = cleaned_preds\ntest_data[['id', 'PredictionString']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:27:23.688533Z","iopub.execute_input":"2021-11-12T23:27:23.688908Z","iopub.status.idle":"2021-11-12T23:27:23.704715Z","shell.execute_reply.started":"2021-11-12T23:27:23.688876Z","shell.execute_reply":"2021-11-12T23:27:23.703875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head submission.csv","metadata":{"execution":{"iopub.status.busy":"2021-11-12T23:27:24.836459Z","iopub.execute_input":"2021-11-12T23:27:24.836911Z","iopub.status.idle":"2021-11-12T23:27:25.668321Z","shell.execute_reply.started":"2021-11-12T23:27:24.836869Z","shell.execute_reply":"2021-11-12T23:27:25.667153Z"},"trusted":true},"execution_count":null,"outputs":[]}]}