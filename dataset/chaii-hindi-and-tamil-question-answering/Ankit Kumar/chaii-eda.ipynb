{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T20:13:40.889801Z","iopub.execute_input":"2022-02-07T20:13:40.890696Z","iopub.status.idle":"2022-02-07T20:13:40.92156Z","shell.execute_reply.started":"2022-02-07T20:13:40.890595Z","shell.execute_reply":"2022-02-07T20:13:40.920994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exploratory data analysis: QnA \n\nCheckout what we have to submit?\n- The submission sample has 2 columns: *id* and *PredictionString*","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:14:09.345621Z","iopub.execute_input":"2022-02-07T20:14:09.345882Z","iopub.status.idle":"2022-02-07T20:14:09.365525Z","shell.execute_reply.started":"2022-02-07T20:14:09.345855Z","shell.execute_reply":"2022-02-07T20:14:09.364831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:14:11.581951Z","iopub.execute_input":"2022-02-07T20:14:11.582219Z","iopub.status.idle":"2022-02-07T20:14:11.606825Z","shell.execute_reply.started":"2022-02-07T20:14:11.58219Z","shell.execute_reply":"2022-02-07T20:14:11.606276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the train and test data that we have..","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/train.csv\")\ntrain_df.sample(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:14:15.850072Z","iopub.execute_input":"2022-02-07T20:14:15.850469Z","iopub.status.idle":"2022-02-07T20:14:16.772355Z","shell.execute_reply.started":"2022-02-07T20:14:15.850441Z","shell.execute_reply":"2022-02-07T20:14:16.771709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"language\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:14:16.773663Z","iopub.execute_input":"2022-02-07T20:14:16.774061Z","iopub.status.idle":"2022-02-07T20:14:16.785578Z","shell.execute_reply.started":"2022-02-07T20:14:16.774031Z","shell.execute_reply":"2022-02-07T20:14:16.784726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training data is relatively small, with 746 `hindi` and 368 `tamil` examples. \n\nWe have:\n- id\n- context \n- question\n- answer_text\n- answer_start\n\nTask: We have context available and we need to find the start and end span in that context that answers our given question. Similar example of such format is [SQUAD](https://rajpurkar.github.io/SQuAD-explorer/)","metadata":{}},{"cell_type":"code","source":"#test data\ntest_df = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\ntest_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:14:23.247114Z","iopub.execute_input":"2022-02-07T20:14:23.247413Z","iopub.status.idle":"2022-02-07T20:14:23.277952Z","shell.execute_reply.started":"2022-02-07T20:14:23.247382Z","shell.execute_reply":"2022-02-07T20:14:23.277191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In test data we have 5 sample data and we do not have *answer_text* and *answer_start* columns","metadata":{}},{"cell_type":"markdown","source":"### Metric\n\nEvaluation is based on Jaccard score, which is an intersection over union (IoU) metric. It measures how many words from the context we picked correctly.\n\n### Baseline model\n\nUsing the Huggingface library and this repo on github:\nhttps://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb","metadata":{}},{"cell_type":"code","source":"import transformers","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:14:27.606807Z","iopub.execute_input":"2022-02-07T20:14:27.607437Z","iopub.status.idle":"2022-02-07T20:14:28.869334Z","shell.execute_reply.started":"2022-02-07T20:14:27.60739Z","shell.execute_reply":"2022-02-07T20:14:28.86859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls \"../input/\"","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:15:20.72293Z","iopub.execute_input":"2022-02-07T20:15:20.723189Z","iopub.status.idle":"2022-02-07T20:15:21.480287Z","shell.execute_reply.started":"2022-02-07T20:15:20.723162Z","shell.execute_reply":"2022-02-07T20:15:21.479332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = \"deepset/xlm-roberta-large-squad2\"\nbatch_size = 4","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:16:07.78352Z","iopub.execute_input":"2022-02-07T20:16:07.784348Z","iopub.status.idle":"2022-02-07T20:16:07.789219Z","shell.execute_reply.started":"2022-02-07T20:16:07.784309Z","shell.execute_reply":"2022-02-07T20:16:07.788385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:16:35.334634Z","iopub.execute_input":"2022-02-07T20:16:35.335444Z","iopub.status.idle":"2022-02-07T20:16:48.753313Z","shell.execute_reply.started":"2022-02-07T20:16:35.33541Z","shell.execute_reply":"2022-02-07T20:16:48.752496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"num_tokens_context\"] = train_df[\"context\"].apply(lambda t: len(tokenizer(t)[\"input_ids\"]))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:20:40.243153Z","iopub.execute_input":"2022-02-07T20:20:40.244092Z","iopub.status.idle":"2022-02-07T20:21:21.939662Z","shell.execute_reply.started":"2022-02-07T20:20:40.244051Z","shell.execute_reply":"2022-02-07T20:21:21.938599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"num_tokens_context\"]","metadata":{},"execution_count":null,"outputs":[]}]}