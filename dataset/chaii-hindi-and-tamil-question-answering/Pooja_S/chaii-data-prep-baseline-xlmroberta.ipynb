{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{"id":"NXZwX11skjB7"}},{"cell_type":"code","source":"# !pip install transformers[sentencepiece]","metadata":{"id":"-5iRPDGwGwQT","outputId":"7c86130f-4056-44e1-f310-2e96744019bd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())\nprint(torch.cuda.device_count())\n# print(torch.cuda.current_device())\n# print(torch.cuda.get_device_name(torch.cuda.current_device()))","metadata":{"id":"ugvZQ5xFN1RS","outputId":"f1e9b3ff-a853-4f7c-e585-776fb2eb4a42"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.039662,"end_time":"2021-08-22T13:56:58.45271","exception":false,"start_time":"2021-08-22T13:56:58.413048","status":"completed"},"tags":[],"id":"eb9a0a74","execution":{"iopub.status.busy":"2021-08-23T08:31:51.526405Z","iopub.execute_input":"2021-08-23T08:31:51.526822Z","iopub.status.idle":"2021-08-23T08:31:51.541159Z","shell.execute_reply.started":"2021-08-23T08:31:51.526788Z","shell.execute_reply":"2021-08-23T08:31:51.539674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_FILE = pd.read_csv(\"/kaggle/input/chaii-hindi-and-tamil-question-answering/train.csv\")\nTEST_FILE = pd.read_csv(\"/kaggle/input/chaii-hindi-and-tamil-question-answering/test.csv\")\nSUBMISSION_FILE = pd.read_csv(\"/kaggle/input/chaii-hindi-and-tamil-question-answering/sample_submission.csv\")\n# change it to True if you want to use GPU\nuse_gpu = False","metadata":{"papermill":{"duration":0.994144,"end_time":"2021-08-22T13:56:59.464967","exception":false,"start_time":"2021-08-22T13:56:58.470823","status":"completed"},"tags":[],"id":"bc859102","execution":{"iopub.status.busy":"2021-08-23T08:32:19.041066Z","iopub.execute_input":"2021-08-23T08:32:19.04158Z","iopub.status.idle":"2021-08-23T08:32:20.115678Z","shell.execute_reply.started":"2021-08-23T08:32:19.041546Z","shell.execute_reply":"2021-08-23T08:32:20.114533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom pprint import pprint\nfrom torch.utils.data import Dataset, DataLoader\n# import torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.optim import Adam,AdamW\n\nfrom transformers import logging\n\nlogging.set_verbosity_error()","metadata":{"execution":{"iopub.execute_input":"2021-08-22T13:56:59.574348Z","iopub.status.busy":"2021-08-22T13:56:59.573281Z","iopub.status.idle":"2021-08-22T13:57:10.03808Z","shell.execute_reply":"2021-08-22T13:57:10.038603Z","shell.execute_reply.started":"2021-08-22T13:51:10.605053Z"},"papermill":{"duration":10.487173,"end_time":"2021-08-22T13:57:10.038806","exception":false,"start_time":"2021-08-22T13:56:59.551633","status":"completed"},"tags":[],"id":"62574354"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have used pretrained Roberta model with SQUAD dataset to fine tune for our use case. \n\nPretrained model available at :: https://huggingface.co/deepset/xlm-roberta-large-squad2","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation","metadata":{"papermill":{"duration":0.017495,"end_time":"2021-08-22T13:57:10.074432","exception":false,"start_time":"2021-08-22T13:57:10.056937","status":"completed"},"tags":[],"id":"97815eb9"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"deepset/xlm-roberta-large-squad2\")","metadata":{"execution":{"iopub.execute_input":"2021-08-22T13:57:10.116153Z","iopub.status.busy":"2021-08-22T13:57:10.115403Z","iopub.status.idle":"2021-08-22T13:57:14.669217Z","shell.execute_reply":"2021-08-22T13:57:14.669736Z","shell.execute_reply.started":"2021-08-22T13:51:24.566378Z"},"papermill":{"duration":4.577966,"end_time":"2021-08-22T13:57:14.669953","exception":false,"start_time":"2021-08-22T13:57:10.091987","status":"completed"},"tags":[],"id":"620ef43e","outputId":"d7bb4d8e-56c3-4b93-93c3-132ab6ae82cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Code adopted from reference link \"\"\"\nclass ChaiiDataset(Dataset):\n    \n    def __init__(self,df,max_len=356,doc_stride=128):\n        \n        self.df = df\n        self.max_len = max_len \n        self.doc_stride = doc_stride\n        self.labelled = 'answer_text' in df\n        self.tokenizer = AutoTokenizer.from_pretrained(\"deepset/xlm-roberta-large-squad2\",add_special_tokens=True)        \n        self.tokenized_samples = self.tokenizer(\n                                self.df['context'].values.tolist(),\n                                self.df['question'].values.tolist(),\n                                truncation=\"only_first\",\n                                max_length=self.max_len,\n                                stride=self.doc_stride,\n                                return_overflowing_tokens=True,\n                                return_offsets_mapping=True,\n                                padding=\"max_length\")\n        \n    def __getitem__(self,idx):\n        \n        data = {}\n        ids,mask,offset = self.tokenized_samples['input_ids'][idx],\\\n                        self.tokenized_samples['attention_mask'][idx],\\\n                        self.tokenized_samples['offset_mapping'][idx]\n        \n        data['index'] = idx\n        data['ids'] = torch.tensor(ids)\n        data['mask'] = torch.tensor(mask)\n        data['offset'] = offset\n        if self.labelled:\n            \n            answer_text,start,end = self.get_targets(idx)\n            data['answer_text'] = answer_text\n            data['start'] = torch.tensor(start)\n            data['end'] = torch.tensor(end)\n            \n        \n        return data\n    \n    def get_targets(self,idx):\n        \n        df_index = self.tokenized_samples['overflow_to_sample_mapping'][idx]\n        start_char = (self.df.iloc[df_index]['answer_start'])\n        end_char = start_char + len(self.df.iloc[df_index]['answer_text'])\n        offset = self.tokenized_samples['offset_mapping'][idx]\n        sequence_ids = self.tokenized_samples.sequence_ids(idx)\n        end_offset = len(self.tokenized_samples['input_ids'][idx])-1\n        start_offset = 1\n        while sequence_ids[end_offset] != 0:\n            end_offset -= 1\n            \n            \n        start_idx = 0;end_idx=0\n        ## answer not in context\n        if (start_char > offset[end_offset][0] or end_char < offset[start_offset][0]):\n            #print(\"In first loop\")\n            start_idx = 0;end_idx=0\n            answer_text=\"\"\n        \n            \n        ## answer partially in context\n        elif ((start_char <= offset[end_offset][0]) and (end_char >  offset[end_offset][0])):\n            #print(\"in second loop\")\n            start_idx = 0;end_idx=0\n            answer_text = \"\"\n        \n        ## answer fully inside context\n        else:\n            #print(\"In third loop\")\n            i=0\n            while (start_idx < len(offset) and offset[i][0]<=start_char and offset[i][1]<start_char):\n                start_idx+=1\n                i+=1\n            end_idx = i\n            while (end_idx < len(offset) and offset[i][1]<end_char):\n                end_idx+=1\n                i+=1\n            answer_text = self.df.iloc[df_index]['answer_text'].strip()\n            \n        \n        return answer_text,start_idx, end_idx \n    \n    def __len__(self):\n        return len(self.tokenized_samples['overflow_to_sample_mapping'])\n","metadata":{"execution":{"iopub.execute_input":"2021-08-22T13:57:14.768056Z","iopub.status.busy":"2021-08-22T13:57:14.766562Z","iopub.status.idle":"2021-08-22T13:57:14.785853Z","shell.execute_reply":"2021-08-22T13:57:14.786378Z","shell.execute_reply.started":"2021-08-22T13:51:37.924485Z"},"papermill":{"duration":0.046747,"end_time":"2021-08-22T13:57:14.786603","exception":false,"start_time":"2021-08-22T13:57:14.739856","status":"completed"},"tags":[],"id":"6b2b114c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"papermill":{"duration":0.021186,"end_time":"2021-08-22T13:57:14.829301","exception":false,"start_time":"2021-08-22T13:57:14.808115","status":"completed"},"tags":[],"id":"9fbd7760"}},{"cell_type":"code","source":"class ChaiiModel(torch.nn.Module):\n    def __init__(self):\n        # create neural network architecture here\n        super(ChaiiModel,self).__init__()\n        config = transformers.AutoConfig.from_pretrained(\"deepset/xlm-roberta-large-squad2\")\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.xlmrob = AutoModel.from_pretrained(\"deepset/xlm-roberta-large-squad2\", config=config)\n        self.l0 = torch.nn.Linear(1024, 2)\n        \n    def forward(self, ids, attention_mask):\n        # feed forward execution happens here\n        xlmrob_out = self.xlmrob(ids, attention_mask)\n        sequence_output = xlmrob_out[0]\n        logits = self.l0(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n    \n        return start_logits, end_logits","metadata":{"execution":{"iopub.execute_input":"2021-08-22T13:57:14.875377Z","iopub.status.busy":"2021-08-22T13:57:14.874645Z","iopub.status.idle":"2021-08-22T13:57:14.882827Z","shell.execute_reply":"2021-08-22T13:57:14.883355Z","shell.execute_reply.started":"2021-08-22T13:51:37.946625Z"},"papermill":{"duration":0.034256,"end_time":"2021-08-22T13:57:14.883576","exception":false,"start_time":"2021-08-22T13:57:14.84932","status":"completed"},"tags":[],"id":"c98ea9f4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss and Eval Function","metadata":{"papermill":{"duration":0.021416,"end_time":"2021-08-22T13:57:14.967328","exception":false,"start_time":"2021-08-22T13:57:14.945912","status":"completed"},"tags":[],"id":"f1764183"}},{"cell_type":"code","source":"\"\"\" Adopted some code from  reference link \"\"\"\ndef loss_fn(o1, o2, t1, t2):\n    l1 = torch.nn.BCEWithLogitsLoss()(o1, t1)\n    l2 = torch.nn.BCEWithLogitsLoss()(o2, t2)\n    return l1 + l2\n\ndef safe_div(x,y):\n    if y == 0:\n        return 1\n    return x / y\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return safe_div(float(len(c)) , (len(a) + len(b) - len(c)))\n\ndef get_jaccard_score(y_true,y_pred):\n    assert len(y_true)==len(y_pred)\n    score=0.0\n    for i in range(len(y_true)):\n        score += jaccard(y_true[i], y_pred[i])\n        \n    return score","metadata":{"execution":{"iopub.execute_input":"2021-08-22T13:57:15.012523Z","iopub.status.busy":"2021-08-22T13:57:15.011468Z","iopub.status.idle":"2021-08-22T13:57:15.022422Z","shell.execute_reply":"2021-08-22T13:57:15.023444Z","shell.execute_reply.started":"2021-08-22T13:51:43.453655Z"},"papermill":{"duration":0.035862,"end_time":"2021-08-22T13:57:15.023794","exception":false,"start_time":"2021-08-22T13:57:14.987932","status":"completed"},"tags":[],"id":"21dd5da9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"papermill":{"duration":0.020294,"end_time":"2021-08-22T13:57:15.06897","exception":false,"start_time":"2021-08-22T13:57:15.048676","status":"completed"},"tags":[],"id":"fd069d5e"}},{"cell_type":"code","source":"class AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.execute_input":"2021-08-22T13:57:15.11965Z","iopub.status.busy":"2021-08-22T13:57:15.118912Z","iopub.status.idle":"2021-08-22T13:57:15.121159Z","shell.execute_reply":"2021-08-22T13:57:15.121718Z","shell.execute_reply.started":"2021-08-22T13:51:44.956832Z"},"papermill":{"duration":0.030151,"end_time":"2021-08-22T13:57:15.121921","exception":false,"start_time":"2021-08-22T13:57:15.09177","status":"completed"},"tags":[],"id":"cb19ed84"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_fn(data_loader, model, optimizer):\n    if use_gpu == True:\n        model = model.to(device=\"cuda\")\n    model.train()\n    losses = AverageMeter()\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        attention_mask = d[\"mask\"]\n        targets_start = d[\"start\"]\n        targets_end = d[\"end\"]\n        if use_gpu == True:\n            ids = d[\"ids\"].to(device=\"cuda\")\n            attention_mask = d[\"mask\"].to(device=\"cuda\")\n            targets_start = d[\"start\"].to(device=\"cuda\")\n            targets_end = d[\"end\"].to(device=\"cuda\")\n\n        \n        optimizer.zero_grad()\n        o1, o2 = model(\n            ids = ids,\n            attention_mask = attention_mask\n        )\n        \n        loss = loss_fn(o1, o2, targets_start, targets_end)\n        loss.backward()\n        optimizer.step()\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg)\n        ","metadata":{"execution":{"iopub.execute_input":"2021-08-22T13:57:15.169058Z","iopub.status.busy":"2021-08-22T13:57:15.168272Z","iopub.status.idle":"2021-08-22T13:57:15.178067Z","shell.execute_reply":"2021-08-22T13:57:15.177356Z","shell.execute_reply.started":"2021-08-22T13:51:45.526002Z"},"papermill":{"duration":0.034944,"end_time":"2021-08-22T13:57:15.178243","exception":false,"start_time":"2021-08-22T13:57:15.143299","status":"completed"},"tags":[],"id":"a1fa093d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection\nEPOCH = 1\n\ndef run():\n    dfx = pd.read_csv(TRAINING_FILE)\n    \n    df_train, df_valid = model_selection.train_test_split(\n        dfx,\n        test_size = 0.1,\n        random_state = 42,\n    )\n    \n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n    \n    # pass it to dataset class and data loader - train df\n    train_dataset = ChaiiDataset(df_train)\n    train_dataloader =  DataLoader(train_dataset, batch_size=16, shuffle=True)\n    # pass it to dataset class and data loader - valid df\n    valid_dataset = ChaiiDataset(df_valid)\n    valid_dataloader =  DataLoader(valid_dataset, batch_size=16, shuffle=True)\n\n    \n    # initialize model\n    model = ChaiiModel()\n    # initialize optimizer\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.01,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters,lr=4e-5)\n    # calculate number of steps\n    steps = (len(df_train)*EPOCH)//16\n    # loop over epochs - train_fn\n    for epoch in range(EPOCH):\n        train_fn(train_dataloader, model, optimizer)\n    ","metadata":{"papermill":{"duration":0.786681,"end_time":"2021-08-22T13:57:15.98573","exception":false,"start_time":"2021-08-22T13:57:15.199049","status":"completed"},"tags":[],"id":"b6023772","execution":{"iopub.status.busy":"2021-08-23T08:38:32.923241Z","iopub.execute_input":"2021-08-23T08:38:32.92387Z","iopub.status.idle":"2021-08-23T08:38:33.959869Z","shell.execute_reply.started":"2021-08-23T08:38:32.9238Z","shell.execute_reply":"2021-08-23T08:38:33.958515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Invoke this function to start training\n# run()","metadata":{"execution":{"iopub.execute_input":"2021-08-22T13:57:16.031837Z","iopub.status.busy":"2021-08-22T13:57:16.03112Z","iopub.status.idle":"2021-08-22T13:57:16.033419Z","shell.execute_reply":"2021-08-22T13:57:16.03396Z"},"papermill":{"duration":0.027525,"end_time":"2021-08-22T13:57:16.034167","exception":false,"start_time":"2021-08-22T13:57:16.006642","status":"completed"},"tags":[],"id":"cb83c9ea","outputId":"3036e485-4c94-4bcc-b13c-0f21a45824c7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Refernces ::** https://www.kaggle.com/shahules/chaii-xlm-custom-qa-baseline-train-infer","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}