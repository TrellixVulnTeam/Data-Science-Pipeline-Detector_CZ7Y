{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport transformers\nimport torch\nimport random\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport time","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-11-09T15:20:01.254598Z","iopub.execute_input":"2021-11-09T15:20:01.255187Z","iopub.status.idle":"2021-11-09T15:20:03.878624Z","shell.execute_reply.started":"2021-11-09T15:20:01.255071Z","shell.execute_reply":"2021-11-09T15:20:03.877386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If GPU available\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device=torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' %torch.cuda.device_count())\n    print('We will use the GPU:',torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:20:03.889615Z","iopub.execute_input":"2021-11-09T15:20:03.889934Z","iopub.status.idle":"2021-11-09T15:20:03.94022Z","shell.execute_reply.started":"2021-11-09T15:20:03.889881Z","shell.execute_reply":"2021-11-09T15:20:03.938947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fix_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nfix_random_seed(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:20:03.94259Z","iopub.execute_input":"2021-11-09T15:20:03.943612Z","iopub.status.idle":"2021-11-09T15:20:03.951713Z","shell.execute_reply.started":"2021-11-09T15:20:03.94356Z","shell.execute_reply":"2021-11-09T15:20:03.950353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    plm = '../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'\n    split_ratio = 0.9\n    batch_size = 8\n    lr = 1e-5\n    epochs = 2\n    max_length = 256\n    stride = 128\n    \nCONFIG = config()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:20:03.956386Z","iopub.execute_input":"2021-11-09T15:20:03.95662Z","iopub.status.idle":"2021-11-09T15:20:03.965835Z","shell.execute_reply.started":"2021-11-09T15:20:03.95659Z","shell.execute_reply":"2021-11-09T15:20:03.964658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Follwing 2 files: Pre-Processed by @Rohit Singh (https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\nxquad_df = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\nmlqa_df = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n\ninitial_data_df = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ninitial_data_df = initial_data_df.append(mlqa_df)\ninitial_data_df = initial_data_df.append(xquad_df).sample(frac=1)\n\nsplit_idx = int(len(initial_data_df.index) * CONFIG.split_ratio)\n\ninitial_train_df = initial_data_df.iloc[:split_idx]\ninitial_valid_df = initial_data_df.iloc[split_idx:]","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:20:03.967694Z","iopub.execute_input":"2021-11-09T15:20:03.968243Z","iopub.status.idle":"2021-11-09T15:20:04.608941Z","shell.execute_reply.started":"2021-11-09T15:20:03.968197Z","shell.execute_reply":"2021-11-09T15:20:04.607787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_answer_end_col(df):\n    answer_ends = []\n    for index, row in df.iterrows():\n        answer_ends.append(row['answer_start'] + len(row['answer_text']))\n    \n    df.insert(loc = 5, column = \"answer_end\", value = answer_ends)\n    \n    return df\n\ninitial_train_df = add_answer_end_col(initial_train_df)\ninitial_valid_df = add_answer_end_col(initial_valid_df)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:20:04.61055Z","iopub.execute_input":"2021-11-09T15:20:04.610857Z","iopub.status.idle":"2021-11-09T15:20:05.429092Z","shell.execute_reply.started":"2021-11-09T15:20:04.610816Z","shell.execute_reply":"2021-11-09T15:20:05.427974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_train_df","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:20:05.43484Z","iopub.execute_input":"2021-11-09T15:20:05.437607Z","iopub.status.idle":"2021-11-09T15:20:05.487824Z","shell.execute_reply.started":"2021-11-09T15:20:05.437561Z","shell.execute_reply":"2021-11-09T15:20:05.486916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(plm):\n    model = transformers.AutoModelForQuestionAnswering.from_pretrained(plm)\n    model.to(device)\n    return model\n\ndef get_tokenizer(plm):\n    tokenizer = transformers.AutoTokenizer.from_pretrained(plm)\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:20:05.49278Z","iopub.execute_input":"2021-11-09T15:20:05.499652Z","iopub.status.idle":"2021-11-09T15:20:05.513136Z","shell.execute_reply.started":"2021-11-09T15:20:05.499571Z","shell.execute_reply":"2021-11-09T15:20:05.512029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_features(df_row, tokenizer):\n    batch_encoding = tokenizer (\n        df_row['question'],\n        df_row['context'],\n        truncation='only_second', padding=\"max_length\", max_length = CONFIG.max_length,\n        stride = CONFIG.stride, return_overflowing_tokens=True, return_offsets_mapping=True, return_token_type_ids=True\n    )\n    \n    features = []\n    for i, encoding in enumerate(batch_encoding.encodings):\n        feature = {}\n        feature['ids'] = encoding.ids\n        feature['attention_mask'] = encoding.attention_mask\n        feature['offset'] = encoding.offsets\n        feature['token'] = encoding.tokens\n        #feature['token_type_ids'] = encoding.type_ids\n        class_index = encoding.ids.index(tokenizer.cls_token_id)\n        \n        for j in range (len(encoding.sequence_ids)):\n            if encoding.sequence_ids[j] != None and encoding.sequence_ids[j] == 1:\n                context_start_idx = j\n                context_start_offset = encoding.offsets[j][0]\n                break\n\n        feature['start_position'] = feature['end_position'] = class_index\n        for j in range(context_start_idx, len(encoding.offsets)):\n            offset = encoding.offsets[j]\n            if offset[0] <= df_row['answer_start'] and df_row['answer_start'] < offset[1]:\n                feature['start_position'] = j\n            if offset[0] < df_row['answer_end'] and df_row['answer_end'] <= offset[1]:\n                feature['end_position'] = j\n                break\n        if feature['start_position'] == class_index or feature['end_position'] == class_index:\n            feature['start_position'] = feature['end_position'] = class_index\n        features.append(feature)            \n    return features\n\ntokenizer = get_tokenizer(CONFIG.plm)\n\ntrain_features = []\nfor _, row in initial_train_df.iterrows():\n    train_features += get_features(row, tokenizer)\n\nvalid_features = []\nfor _, row in initial_valid_df.iterrows():\n    valid_features += get_features(row, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:20:05.522128Z","iopub.execute_input":"2021-11-09T15:20:05.524447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print (train_features[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I don't need random sampler because data is already shuffled\ndef get_dataloader(features, batch_size):\n    ids = []\n    attention_masks = []\n    token_type_ids = []    \n    start_positions = []\n    end_positions = []\n    for feature in features:\n        ids.append(feature['ids'])\n        attention_masks.append(feature['attention_mask'])\n        start_positions.append(feature['start_position'])\n        end_positions.append(feature['end_position'])\n        #token_type_ids.append(feature['token_type_ids'])                \n    #data = TensorDataset(torch.tensor(ids), torch.tensor(attention_masks), torch.tensor(start_positions), torch.tensor(end_positions), torch.tensor(token_type_ids))\n    data = TensorDataset(torch.tensor(ids), torch.tensor(attention_masks), torch.tensor(start_positions), torch.tensor(end_positions))\n    sampler = RandomSampler(data)\n    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n    \n    return dataloader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_optimizer_scheduler(model, dataloader, epochs, lr):\n    #Set learning rate\n    optimizer=AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n    epochs=epochs\n\n    #Training steps is no_of_batches*no_of_epochs\n    total_steps=len(dataloader)*epochs\n\n    #Learning rate scheduler\n    scheduler=get_linear_schedule_with_warmup(optimizer, num_warmup_steps= total_steps * 0.2, num_training_steps = total_steps)\n    \n    return optimizer, scheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import SequentialSampler\nimport operator\n\ndef predict(device, model, tokenizer, question, context):\n    inputs = tokenizer (\n        question,\n        context,\n        truncation='only_second', padding=\"max_length\", max_length = CONFIG.max_length,\n        stride = CONFIG.stride, return_overflowing_tokens=True, return_offsets_mapping=True, return_token_type_ids=True\n    )\n    \n    CLASS_INDEX = inputs.input_ids[0].index(tokenizer.cls_token_id)\n    \n    for i in range(len(inputs.sequence_ids())):\n        if inputs.sequence_ids()[i] != None and inputs.sequence_ids()[i] == 1:\n            context_start = i\n            break\n    #data = TensorDataset(torch.tensor(inputs.input_ids), torch.tensor(inputs.attention_mask), torch.tensor(inputs.offset_mapping), torch.tensor(inputs.type_ids))\n    data = TensorDataset(torch.tensor(inputs.input_ids), torch.tensor(inputs.attention_mask), torch.tensor(inputs.offset_mapping))\n    sampler = SequentialSampler(data)\n    dataloader = DataLoader(data, sampler=sampler, batch_size=4)\n    \n    start_list = []\n    end_list = []\n    \n    answer_list = []\n    for batch_i, batch in enumerate(dataloader):\n        #outputs = model(batch[0].to(device), attention_mask=batch[1].to(device), token_type_ids=batch[3].to(device))    \n        outputs = model(batch[0].to(device), attention_mask=batch[1].to(device))    \n        answer_start_scores = outputs.start_logits\n        answer_end_scores = outputs.end_logits\n        \n        for i in range(len(outputs.start_logits)): # for each result in batch\n            max_start_score = 0.0\n            start_idx = 0\n            for j in range(context_start, len(outputs.start_logits[i])):\n                if answer_start_scores[i][j] > max_start_score:\n                    start_idx = j\n                    max_start_score = answer_start_scores[i][j]\n            max_end_score = 0.0\n            end_idx = 0\n            for j in range(start_idx, len(outputs.start_logits[i])):                    \n                if answer_end_scores[i][j] > max_end_score:\n                    end_idx = j\n                    max_end_score = answer_end_scores[i][j]\n            if start_idx < context_start or end_idx < context_start:\n                continue\n            answer_list.append((batch[2][i][start_idx][0].item(), batch[2][i][end_idx][1].item(), \n                                answer_start_scores[i][start_idx].item(), answer_end_scores[i][end_idx].item()))\n        \n    sorted_answer_list = sorted(answer_list, key=lambda tup: tup[2]+tup[3], reverse=True)\n    \n    if len(sorted_answer_list) != 0:\n        return context[sorted_answer_list[0][0]:sorted_answer_list[0][1]].strip(), sorted_answer_list, []\n    else:\n        return \"\", [], []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef score(model, valid_df):\n    jaccard_sum = 0.0\n    for index, row in valid_df.iterrows():\n        context = row['context']\n        question = row['question']\n        answer_gold = row['answer_text']\n        answer_result, start_score_list, end_score_list = predict(device, model, tokenizer, question, context)\n        jaccard_sum += jaccard(answer_result, answer_gold)\n    return jaccard_sum / len(valid_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(parameters):\n    print (parameters)\n    batch_size = CONFIG.batch_size\n    lr = parameters['lr']\n    epochs = CONFIG.epochs\n\n    train_dataloader = get_dataloader(train_features, CONFIG.batch_size)\n    print ('train_loader size: {}'.format(len(train_dataloader)))\n    model = get_model(CONFIG.plm)\n    optimizer, scheduler = get_optimizer_scheduler(model, train_dataloader, epochs, lr)\n\n    train_check_every = 200\n    model_id = int(time.time())\n    print ('model_id: {}'.format(model_id))\n    for epoch in range(epochs):\n        batch_count = 0\n        epoch_loss = 0.0\n        model.train()\n        for i, batch in enumerate(train_dataloader):\n            optimizer.zero_grad()\n            input_ids = batch[0].to(device)\n            attention_mask = batch[1].to(device)\n            start_positions = batch[2].to(device)\n            end_positions = batch[3].to(device)\n            #token_type_ids = batch[4].to(device)            \n            \n            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n            #outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions, token_type_ids=token_type_ids)\n            loss = outputs[0]\n            \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n            batch_count += 1\n            epoch_loss += float(loss)\n            if i % train_check_every == 0 and i != 0:\n                print ('epoch: {}, batch: {}, loss: {}'.format(epoch, i, loss))\n        print ('-----------------------------------------------------')\n        print ('epoch: {}, epoch avrage loss: {}'.format(epoch, epoch_loss / batch_count))\n        \n        model.eval()        \n        print (\"ecpoch: {}, Jaccard Score: {}\".format(epoch, score(model, initial_valid_df)))\n        model_path = './model/{}/{}'.format(model_id, epoch)\n        model.save_pretrained(model_path)\n        print ('Save epoch model: {}'.format(model_path))        \n        \n        print ('-----------------------------------------------------')\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'batch_size': CONFIG.batch_size, 'lr': CONFIG.lr, 'epochs': CONFIG.epochs}\nmodel = train (params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/chaii-hindi-and-tamil-question-answering/test.csv')\nmodel.eval()\nwith open('submission.csv', 'w') as fwp:\n    fwp.write('id,PredictionString\\n')\n    for index, row in test_df.iterrows():\n        try:\n            answer, _, _ = predict(device, model, tokenizer, row['question'], row['context'])\n            answer = answer.replace('\"', '')\n            fwp.write('{},\"{}\"\\n'.format(row['id'], answer))\n        except Exception as e:\n            fwp.write('{},\"\"\\n'.format(row['id']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}