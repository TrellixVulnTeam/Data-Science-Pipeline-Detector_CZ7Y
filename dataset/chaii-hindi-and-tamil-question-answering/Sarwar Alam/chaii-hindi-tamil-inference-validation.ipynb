{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/huggingface-accelerate')\nsys.path.append('../input/transformershuggingface/transformer_repo')","metadata":{"id":"ZBKvQdjJ1_99","execution":{"iopub.status.busy":"2021-11-07T22:44:24.942505Z","iopub.execute_input":"2021-11-07T22:44:24.9429Z","iopub.status.idle":"2021-11-07T22:44:25.045559Z","shell.execute_reply.started":"2021-11-07T22:44:24.942803Z","shell.execute_reply":"2021-11-07T22:44:25.044415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint='../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'","metadata":{"id":"zCAL3UJm1_-D","execution":{"iopub.status.busy":"2021-11-07T22:44:25.047779Z","iopub.execute_input":"2021-11-07T22:44:25.048192Z","iopub.status.idle":"2021-11-07T22:44:25.05314Z","shell.execute_reply.started":"2021-11-07T22:44:25.04815Z","shell.execute_reply":"2021-11-07T22:44:25.05207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq","metadata":{"id":"ImW8Iodz1_-F","execution":{"iopub.status.busy":"2021-11-07T22:44:25.054943Z","iopub.execute_input":"2021-11-07T22:44:25.056337Z","iopub.status.idle":"2021-11-07T22:44:37.542146Z","shell.execute_reply.started":"2021-11-07T22:44:25.056051Z","shell.execute_reply":"2021-11-07T22:44:37.541095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport collections\nimport pandas as pd\nimport gc\nimport os\nimport sys\nfrom IPython.display import FileLink\n%env WANDB_DISABLED=True\nimport torch\nimport datasets as d\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import DataLoader\nfrom src.accelerate import Accelerator\nimport math\nimport os\nimport random\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    default_data_collator,\n    get_scheduler,\n    SchedulerType,\n    set_seed)\n","metadata":{"id":"bwFxs1kk1_-F","execution":{"iopub.status.busy":"2021-11-07T22:44:37.544546Z","iopub.execute_input":"2021-11-07T22:44:37.545252Z","iopub.status.idle":"2021-11-07T22:44:49.91919Z","shell.execute_reply.started":"2021-11-07T22:44:37.545179Z","shell.execute_reply":"2021-11-07T22:44:49.91818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the seed\nseed=124\nrandom.seed(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nset_seed(seed)","metadata":{"id":"hwRZX8p-1_-G","execution":{"iopub.status.busy":"2021-11-07T22:44:49.922226Z","iopub.execute_input":"2021-11-07T22:44:49.922546Z","iopub.status.idle":"2021-11-07T22:44:49.935472Z","shell.execute_reply.started":"2021-11-07T22:44:49.922505Z","shell.execute_reply":"2021-11-07T22:44:49.934559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load  data\nimport pandas as pd\ntrain_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nsubmission_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\nsquad=pd.read_csv('../input/squadhindi/xquad.csv')\nmlqa=pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')","metadata":{"id":"l5xw8bYt1_-H","execution":{"iopub.status.busy":"2021-11-07T22:44:49.938299Z","iopub.execute_input":"2021-11-07T22:44:49.938834Z","iopub.status.idle":"2021-11-07T22:44:50.924674Z","shell.execute_reply.started":"2021-11-07T22:44:49.938779Z","shell.execute_reply":"2021-11-07T22:44:50.923659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"squad['id']=['sar'+str(item) for item in range(5427,5427+squad.shape[0])]\nmlqa['id']=['sar'+str(item) for item in range(mlqa.shape[0])]\n# merge train_df and squad\ntrain_df=pd.concat([train_df,squad,mlqa])","metadata":{"id":"_2veWPZx1_-I","execution":{"iopub.status.busy":"2021-11-07T22:44:50.926401Z","iopub.execute_input":"2021-11-07T22:44:50.926692Z","iopub.status.idle":"2021-11-07T22:44:50.955982Z","shell.execute_reply.started":"2021-11-07T22:44:50.926653Z","shell.execute_reply":"2021-11-07T22:44:50.954913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_answers_as_dict(r):\n    start = r[0]\n    text = r[1]\n    return {\n        'answer_start': [start],\n        'text': [text]\n    }\n\ntrain_df['answers'] = train_df[['answer_start', 'answer_text']].apply(get_answers_as_dict, axis=1)\n\ntrain_df.reset_index(drop=True,inplace=True)","metadata":{"id":"S5iIxzW51_-K","execution":{"iopub.status.busy":"2021-11-07T22:44:50.957718Z","iopub.execute_input":"2021-11-07T22:44:50.958335Z","iopub.status.idle":"2021-11-07T22:44:51.089952Z","shell.execute_reply.started":"2021-11-07T22:44:50.958294Z","shell.execute_reply":"2021-11-07T22:44:51.088806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyper parameters\nmax_length = 384\ndoc_stride = 128\neval_batch_size = 8","metadata":{"id":"ZMgPugrX3RnT","execution":{"iopub.status.busy":"2021-11-07T22:44:51.091701Z","iopub.execute_input":"2021-11-07T22:44:51.092073Z","iopub.status.idle":"2021-11-07T22:44:51.097644Z","shell.execute_reply.started":"2021-11-07T22:44:51.092029Z","shell.execute_reply":"2021-11-07T22:44:51.096455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_test_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","metadata":{"id":"mVotVzf61_-L","execution":{"iopub.status.busy":"2021-11-07T22:44:51.099571Z","iopub.execute_input":"2021-11-07T22:44:51.100267Z","iopub.status.idle":"2021-11-07T22:44:51.11351Z","shell.execute_reply.started":"2021-11-07T22:44:51.100157Z","shell.execute_reply":"2021-11-07T22:44:51.11235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_qa_predictions(\n    examples,\n    features,\n    predictions,\n    version_2_with_negative: bool = False,\n    n_best_size: int = 20,\n    max_answer_length: int = 30,\n    null_score_diff_threshold: float = 0.0):\n    \n    assert len(predictions) == 2, \"`predictions` should be a tuple with two elements (start_logits, end_logits).\"\n    all_start_logits, all_end_logits = predictions\n\n    assert len(predictions[0]) == len(features), f\"Got {len(predictions[0])} predictions and {len(features)} features.\"\n\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    all_predictions = collections.OrderedDict()\n    \n    \n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_prediction = None\n        prelim_predictions = []\n\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n            # available in the current feature.\n            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n\n            # Update minimum null prediction.\n            feature_null_score = start_logits[0] + end_logits[0]\n            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n                min_null_prediction = {\n                    \"offsets\": (0, 0),\n                    \"score\": feature_null_score,\n                    \"start_logit\": start_logits[0],\n                    \"end_logit\": end_logits[0],\n                }\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    # Don't consider answer that don't have the maximum context available (if such information is\n                    # provided).\n                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n                        continue\n                    prelim_predictions.append(\n                        {\n                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"start_logit\": start_logits[start_index],\n                            \"end_logit\": end_logits[end_index],\n                        }\n                    )\n        if version_2_with_negative:\n            # Add the minimum null prediction\n            prelim_predictions.append(min_null_prediction)\n            null_score = min_null_prediction[\"score\"]\n\n        # Only keep the best `n_best_size` predictions.\n        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n\n        # Add back the minimum null prediction if it was removed because of its low score.\n        if version_2_with_negative and not any(p[\"offsets\"] == (0, 0) for p in predictions):\n            predictions.append(min_null_prediction)\n\n        # Use the offsets to gather the answer text in the original context.\n        context = example[\"context\"]\n        for pred in predictions:\n            offsets = pred.pop(\"offsets\")\n            pred[\"text\"] = context[offsets[0] : offsets[1]]\n\n        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n        # failure.\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n\n        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n        # the LogSumExp trick).\n        scores = np.array([pred.pop(\"score\") for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n\n        # Include the probabilities in our predictions.\n        for prob, pred in zip(probs, predictions):\n            pred[\"probability\"] = prob\n\n        # Pick the best prediction. If the null answer is not possible, this is easy.\n        if not version_2_with_negative:\n            all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n        else:\n            # Otherwise we first need to find the best non-empty prediction.\n            i = 0\n            while predictions[i][\"text\"] == \"\":\n                i += 1\n            best_non_null_pred = predictions[i]\n\n            # Then we compare to the null prediction using the threshold.\n            score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example[\"id\"]] = \"\"\n            else:\n                all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n\n        \n\n    return all_predictions","metadata":{"id":"y7JGMlMf3oNz","execution":{"iopub.status.busy":"2021-11-07T22:44:51.115433Z","iopub.execute_input":"2021-11-07T22:44:51.115937Z","iopub.status.idle":"2021-11-07T22:44:51.147447Z","shell.execute_reply.started":"2021-11-07T22:44:51.115894Z","shell.execute_reply":"2021-11-07T22:44:51.146464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\ndef create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n\n        step = 0\n        # create a numpy array and fill it with -100.\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n        # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather\n        for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n            # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n            # And after every iteration we have to change the step\n\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n\n            if step + batch_size < len(dataset):\n                logits_concat[step : step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n\n            step += batch_size\n\n        return logits_concat","metadata":{"id":"DwhfdjlL1_-N","execution":{"iopub.status.busy":"2021-11-07T22:44:51.149063Z","iopub.execute_input":"2021-11-07T22:44:51.149653Z","iopub.status.idle":"2021-11-07T22:44:51.163153Z","shell.execute_reply.started":"2021-11-07T22:44:51.149612Z","shell.execute_reply":"2021-11-07T22:44:51.16214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def logit_predictions(test_data,checkpoint,accelerator):\n  config = AutoConfig.from_pretrained(checkpoint)\n  model = AutoModelForQuestionAnswering.from_pretrained(checkpoint,config=config)\n  device=torch.device(\"cuda\")\n  model.to(device)\n  test_dataset_for_model = test_data.remove_columns([\"example_id\", \"offset_mapping\"])\n  test_dataloader = DataLoader(\n            test_dataset_for_model, collate_fn=default_data_collator, batch_size=eval_batch_size)\n\n  all_start_logits = []\n  all_end_logits = []\n  for step, batch in enumerate(tqdm(test_dataloader)):\n        with torch.no_grad():\n            for key,value in batch.items():\n                batch[key]=value.to(device)\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n\n                \n            all_start_logits.append(accelerator.gather(start_logits).cpu().numpy())\n            all_end_logits.append(accelerator.gather(end_logits).cpu().numpy())\n\n  max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n  # concatenate the numpy array\n  start_logits_concat = create_and_fill_np_array(all_start_logits, test_data, max_len)\n  end_logits_concat = create_and_fill_np_array(all_end_logits, test_data, max_len)\n\n  # delete the list of numpy arrays\n  del all_start_logits\n  del all_end_logits\n  del model\n  del config\n  gc.collect()\n\n  return start_logits_concat, end_logits_concat","metadata":{"id":"3kh3b2E74vx0","execution":{"iopub.status.busy":"2021-11-07T22:44:51.165211Z","iopub.execute_input":"2021-11-07T22:44:51.165886Z","iopub.status.idle":"2021-11-07T22:44:51.181296Z","shell.execute_reply.started":"2021-11-07T22:44:51.165841Z","shell.execute_reply":"2021-11-07T22:44:51.180175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_examples=d.Dataset.from_pandas(test_df)\ntokenizer=AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)  \ntest_dataset = test_examples.map(\n            prepare_test_features,\n            batched=True,\n            remove_columns=test_examples.column_names\n        )\naccelerator=Accelerator()\ncheckpoints_dir=['../input/chaiikfold01234/fold0/fold0','../input/chaiikfold01234/fold1/fold1',\n                 '../input/chaiikfold01234/fold2/fold2',\n                '../input/chaiikfold01234/fold3/fold3','../input/chaiikfold01234/fold4/fold4']\noutput_numpy=[0.0,0.0]\nfor checkpoint in checkpoints_dir:\n  output=logit_predictions(test_dataset,checkpoint,accelerator)\n  output_numpy[0]+=output[0]\n  output_numpy[1]+=output[1]\noutput_numpy=(output_numpy[0]/5,output_numpy[1]/5)\nfinal_predictions = postprocess_qa_predictions(test_examples,test_dataset,output_numpy)\n\n","metadata":{"id":"bqbDE7TN1_-N","execution":{"iopub.status.busy":"2021-11-07T22:44:51.18621Z","iopub.execute_input":"2021-11-07T22:44:51.187296Z","iopub.status.idle":"2021-11-07T22:47:14.314013Z","shell.execute_reply.started":"2021-11-07T22:44:51.187217Z","shell.execute_reply":"2021-11-07T22:47:14.313011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaned predictions\ntest_df['PredictionString']=test_df['id'].apply(lambda r:final_predictions[r])\nbad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\ntamil_ad = \"கி.பி\"\ntamil_bc = \"கி.மு\"\ntamil_km = \"கி.மீ\"\nhindi_ad = \"ई\"\nhindi_bc = \"ई.पू\"\n\n\ncleaned_preds = []\nfor pred, context in test_df[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    if pred.endswith(\"...\"):\n            pred = pred[:-3]\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n        \n    cleaned_preds.append(pred)","metadata":{"id":"nfElRq-F-WeR","execution":{"iopub.status.busy":"2021-11-07T22:47:14.315606Z","iopub.execute_input":"2021-11-07T22:47:14.316691Z","iopub.status.idle":"2021-11-07T22:47:14.333864Z","shell.execute_reply.started":"2021-11-07T22:47:14.316632Z","shell.execute_reply":"2021-11-07T22:47:14.332887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission file\nsub=pd.DataFrame({'id':test_df['id']})\nsub['PredictionString']=cleaned_preds\nsub","metadata":{"id":"w_arGnaY-NtX","execution":{"iopub.status.busy":"2021-11-07T22:47:14.335703Z","iopub.execute_input":"2021-11-07T22:47:14.336068Z","iopub.status.idle":"2021-11-07T22:47:14.363416Z","shell.execute_reply.started":"2021-11-07T22:47:14.336011Z","shell.execute_reply":"2021-11-07T22:47:14.362434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\",index=False)","metadata":{"id":"f3SUhXP-AFnU","execution":{"iopub.status.busy":"2021-11-07T22:47:14.365731Z","iopub.execute_input":"2021-11-07T22:47:14.365968Z","iopub.status.idle":"2021-11-07T22:47:14.377157Z","shell.execute_reply.started":"2021-11-07T22:47:14.365943Z","shell.execute_reply":"2021-11-07T22:47:14.376103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}