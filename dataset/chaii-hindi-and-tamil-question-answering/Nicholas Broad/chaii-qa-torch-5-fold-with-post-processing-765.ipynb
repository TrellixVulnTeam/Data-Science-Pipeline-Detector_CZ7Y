{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Adding post-processing to add +0.01 to Torch's model\n\n###  This code is nearly identical to @rhtsingh's work here: https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer\n\n### I just added some basic post-processing that helped improve the leaderboard score. I'm sure there are more rules that can lead to even better performance.","metadata":{}},{"cell_type":"markdown","source":"### Import Dependencies","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nfrom torch.utils.data.distributed import DistributedSampler\n\ntry:\n    from apex import amp\n    APEX_INSTALLED = True\nexcept ImportError:\n    APEX_INSTALLED = False\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\nprint(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-17T20:47:18.480353Z","iopub.execute_input":"2021-08-17T20:47:18.480768Z","iopub.status.idle":"2021-08-17T20:47:25.78555Z","shell.execute_reply.started":"2021-08-17T20:47:18.480678Z","shell.execute_reply":"2021-08-17T20:47:25.784517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n    # model\n    model_type = 'xlm_roberta'\n    model_name_or_path = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/\"\n    config_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/\"\n    fp16 = True if APEX_INSTALLED else False\n    fp16_opt_level = \"O1\"\n    gradient_accumulation_steps = 2\n\n    # tokenizer\n    tokenizer_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/\"\n    max_seq_length = 384\n    doc_stride = 128\n\n    # train\n    epochs = 1\n    train_batch_size = 4\n    eval_batch_size = 32\n\n    # optimizer\n    optimizer_type = 'AdamW'\n    learning_rate = 1.5e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    decay_name = 'linear-warmup'\n    warmup_ratio = 0.1\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 2021","metadata":{"execution":{"iopub.status.busy":"2021-08-17T20:47:31.14864Z","iopub.execute_input":"2021-08-17T20:47:31.148975Z","iopub.status.idle":"2021-08-17T20:47:31.156052Z","shell.execute_reply.started":"2021-08-17T20:47:31.148944Z","shell.execute_reply":"2021-08-17T20:47:31.154971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Retriever","metadata":{}},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }","metadata":{"execution":{"iopub.status.busy":"2021-08-17T20:47:33.13824Z","iopub.execute_input":"2021-08-17T20:47:33.138582Z","iopub.status.idle":"2021-08-17T20:47:33.148342Z","shell.execute_reply.started":"2021-08-17T20:47:33.138552Z","shell.execute_reply":"2021-08-17T20:47:33.147454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def forward(\n        self, \n        input_ids, \n        attention_mask=None, \n    ):\n        outputs = self.xlm_roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits","metadata":{"execution":{"iopub.status.busy":"2021-08-17T20:47:35.254322Z","iopub.execute_input":"2021-08-17T20:47:35.254672Z","iopub.status.idle":"2021-08-17T20:47:35.265167Z","shell.execute_reply.started":"2021-08-17T20:47:35.25464Z","shell.execute_reply":"2021-08-17T20:47:35.262908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utilities","metadata":{}},{"cell_type":"code","source":"def make_model(args):\n    config = AutoConfig.from_pretrained(args.config_name)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n    model = Model(args.model_name_or_path, config=config)\n    return config, tokenizer, model","metadata":{"execution":{"iopub.status.busy":"2021-08-17T20:47:36.596512Z","iopub.execute_input":"2021-08-17T20:47:36.596846Z","iopub.status.idle":"2021-08-17T20:47:36.603533Z","shell.execute_reply.started":"2021-08-17T20:47:36.596814Z","shell.execute_reply":"2021-08-17T20:47:36.602562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Covert Examples to Features (Preprocess)","metadata":{}},{"cell_type":"code","source":"def prepare_test_features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()\n    \n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    features = []\n    for i in range(len(tokenized_example[\"input_ids\"])):\n        feature = {}\n        feature[\"example_id\"] = example['id']\n        feature['context'] = example['context']\n        feature['question'] = example['question']\n        feature['input_ids'] = tokenized_example['input_ids'][i]\n        feature['attention_mask'] = tokenized_example['attention_mask'][i]\n        feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n        features.append(feature)\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:12:19.176671Z","iopub.execute_input":"2021-08-17T14:12:19.177382Z","iopub.status.idle":"2021-08-17T14:12:19.187171Z","shell.execute_reply.started":"2021-08-17T14:12:19.177343Z","shell.execute_reply":"2021-08-17T14:12:19.186363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Postprocess QA Predictions","metadata":{}},{"cell_type":"code","source":"import collections\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        predictions[example[\"id\"]] = best_answer[\"text\"]\n        \n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:12:19.195317Z","iopub.execute_input":"2021-08-17T14:12:19.195729Z","iopub.status.idle":"2021-08-17T14:12:19.210639Z","shell.execute_reply.started":"2021-08-17T14:12:19.195636Z","shell.execute_reply":"2021-08-17T14:12:19.209493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Factory","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nbase_model_path = '../input/chaii-xlmroberta-large-v6/output/'\n\ntokenizer = AutoTokenizer.from_pretrained(Config().tokenizer_name)\n\ntest_features = []\nfor i, row in test.iterrows():\n    test_features += prepare_test_features(Config(), row, tokenizer)\n\nargs = Config()\ntest_dataset = DatasetRetriever(test_features, mode='test')\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=args.eval_batch_size, \n    sampler=SequentialSampler(test_dataset),\n    num_workers=optimal_num_of_loader_workers(),\n    pin_memory=True, \n    drop_last=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:12:19.212358Z","iopub.execute_input":"2021-08-17T14:12:19.212709Z","iopub.status.idle":"2021-08-17T14:12:20.196985Z","shell.execute_reply.started":"2021-08-17T14:12:19.212676Z","shell.execute_reply":"2021-08-17T14:12:20.196088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Intialize Inference","metadata":{}},{"cell_type":"code","source":"def get_predictions(checkpoint_path):\n    config, tokenizer, model = make_model(Config())\n    model.cuda();\n    model.load_state_dict(\n        torch.load(base_model_path + checkpoint_path)\n    );\n    \n    start_logits = []\n    end_logits = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            outputs_start, outputs_end = model(batch['input_ids'].cuda(), batch['attention_mask'].cuda())\n            start_logits.append(outputs_start.cpu().numpy().tolist())\n            end_logits.append(outputs_end.cpu().numpy().tolist())\n            del outputs_start, outputs_end\n    del model, tokenizer, config\n    gc.collect()\n    return np.vstack(start_logits), np.vstack(end_logits)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:12:20.198275Z","iopub.execute_input":"2021-08-17T14:12:20.198635Z","iopub.status.idle":"2021-08-17T14:12:20.208084Z","shell.execute_reply.started":"2021-08-17T14:12:20.198597Z","shell.execute_reply":"2021-08-17T14:12:20.205158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble 5-Folds","metadata":{}},{"cell_type":"code","source":"start_logits1, end_logits1 = get_predictions('checkpoint-fold-0/pytorch_model.bin')\nstart_logits2, end_logits2 = get_predictions('checkpoint-fold-1/pytorch_model.bin')\nstart_logits3, end_logits3 = get_predictions('checkpoint-fold-2/pytorch_model.bin')\nstart_logits4, end_logits4 = get_predictions('checkpoint-fold-3/pytorch_model.bin')\nstart_logits5, end_logits5 = get_predictions('checkpoint-fold-4/pytorch_model.bin')\n\nstart_logits = (start_logits1 + start_logits2 + start_logits3 + start_logits4 + start_logits5) / 5\nend_logits = (end_logits1 + end_logits2 + end_logits3 + end_logits4 + end_logits5) / 5\n\npredictions = postprocess_qa_predictions(test, test_features, (start_logits, end_logits))\n\ntest['PredictionString'] = test['id'].map(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:12:20.209704Z","iopub.execute_input":"2021-08-17T14:12:20.210259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post-Processing\n\n## Based on my EDA ([notebook here](https://www.kaggle.com/nbroad/chaii-qa-character-token-languages-eda)), I came up with simple rules that seem to slightly help scores\n\nBasically, if the answer ends in AD, BC, or km, it should probably have a period. This checks if such a span exists in the context, and if so it will correct the prediction.\n\nOther than that, this is just getting rid of punctuation and other characters at the beginning and end of words that will ruin the jaccard score.","metadata":{}},{"cell_type":"code","source":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\n\n# this part apparently doesn't do anything according to @amontgomerie\n# I'll comment out the code that uses it\ntamil_ad = \"கி.பி\"\ntamil_bc = \"கி.மு\"\ntamil_km = \"கி.மீ\"\nhindi_ad = \"ई\"\nhindi_bc = \"ई.पू\"\n\n\ncleaned_preds = []\nfor pred, context in test[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    \n#     if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n#         pred = pred+\".\"\n\n    cleaned_preds.append(pred)\n\ntest[\"PredictionString\"] = cleaned_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['id', 'PredictionString']].to_csv('submission.csv', index=False)\n\nprint(test[['id', 'PredictionString']])","metadata":{},"execution_count":null,"outputs":[]}]}