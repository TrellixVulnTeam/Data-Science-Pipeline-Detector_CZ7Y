{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U --no-build-isolation --no-deps ../input/transformers-master/ -qq","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:12:21.081341Z","iopub.execute_input":"2021-11-14T23:12:21.081732Z","iopub.status.idle":"2021-11-14T23:12:53.865068Z","shell.execute_reply.started":"2021-11-14T23:12:21.081632Z","shell.execute_reply":"2021-11-14T23:12:53.864118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib/\")\nimport collections\nimport numpy as np\nimport transformers\nimport pandas as pd\nfrom datasets import Dataset\nfrom functools import partial\nfrom tqdm import tqdm\nimport json\nimport torch\n\nfrom sklearn import metrics\nimport transformers\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport tez\nfrom string import punctuation","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:13:23.052253Z","iopub.execute_input":"2021-11-14T23:13:23.052814Z","iopub.status.idle":"2021-11-14T23:13:23.05847Z","shell.execute_reply.started":"2021-11-14T23:13:23.052774Z","shell.execute_reply":"2021-11-14T23:13:23.05756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChaiiModel(tez.Model):\n    def __init__(self, model_name, num_train_steps, steps_per_epoch, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.steps_per_epoch = steps_per_epoch\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = \"batch\"\n\n        hidden_dropout_prob: float = 0.0\n\n        config = transformers.AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = transformers.AutoModelForQuestionAnswering.from_pretrained(model_name, config=config)\n\n\n    def forward(self, ids, mask, token_type_ids=None, start_positions=None, end_positions=None):\n        transformer_out = self.transformer(ids, mask)\n\n        start_logits = transformer_out.start_logits\n        end_logits = transformer_out.end_logits\n        \n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        return (start_logits, end_logits), 0, {}","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:13:23.065557Z","iopub.execute_input":"2021-11-14T23:13:23.066075Z","iopub.status.idle":"2021-11-14T23:13:23.075107Z","shell.execute_reply.started":"2021-11-14T23:13:23.066038Z","shell.execute_reply":"2021-11-14T23:13:23.074388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChaiiDataset:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        return {\n            \"ids\": torch.tensor(self.data[item][\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(self.data[item][\"attention_mask\"], dtype=torch.long),\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:13:23.076728Z","iopub.execute_input":"2021-11-14T23:13:23.077131Z","iopub.status.idle":"2021-11-14T23:13:23.086352Z","shell.execute_reply.started":"2021-11-14T23:13:23.077089Z","shell.execute_reply":"2021-11-14T23:13:23.085459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_validation_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:13:23.087645Z","iopub.execute_input":"2021-11-14T23:13:23.08791Z","iopub.status.idle":"2021-11-14T23:13:23.097545Z","shell.execute_reply.started":"2021-11-14T23:13:23.087877Z","shell.execute_reply":"2021-11-14T23:13:23.096758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_qa_predictions(\n    examples, tokenizer, features, raw_predictions, n_best_size=20, max_answer_length=30, squad_v2=False\n):\n    all_start_logits, all_end_logits = raw_predictions\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n    all_answers = []\n\n    for example_index, example in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None  # Only used if squad_v2 is True.\n        valid_answers = []\n\n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": float(start_logits[start_index] + end_logits[end_index]),\n                            \"text\": context[start_char:end_char],\n                        }\n                    )\n\n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n        valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n        all_answers.append({\"id\": example[\"id\"], \"predictions\": valid_answers})\n    return all_answers, predictions","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:13:23.099542Z","iopub.execute_input":"2021-11-14T23:13:23.099979Z","iopub.status.idle":"2021-11-14T23:13:23.117558Z","shell.execute_reply.started":"2021-11-14T23:13:23.099945Z","shell.execute_reply":"2021-11-14T23:13:23.116808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\ntest_data[\"len\"] = [len(x) for x in test_data[\"context\"]]\n\ndo_inference = True #len(test_data) != 5\n\nchar_threshold = 25_000\n\nshort_data = test_data.copy()\n# short_data = test_data[test_data[\"len\"]<char_threshold].reset_index(drop=True)\n# long_data = test_data[test_data[\"len\"]>=char_threshold].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:13:23.118934Z","iopub.execute_input":"2021-11-14T23:13:23.119275Z","iopub.status.idle":"2021-11-14T23:13:23.138388Z","shell.execute_reply.started":"2021-11-14T23:13:23.119239Z","shell.execute_reply":"2021-11-14T23:13:23.137586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/murilbasecased\")\n    \n    pad_on_right = tokenizer.padding_side == \"right\"\n    max_length = 1024\n    doc_stride = 512\n\n    test_dataset = Dataset.from_pandas(short_data)\n    test_features = test_dataset.map(\n        partial(\n            prepare_validation_features, \n            tokenizer=tokenizer,\n            pad_on_right=pad_on_right, \n            max_length=max_length,\n            doc_stride=doc_stride\n        ),\n        batched=True,\n        remove_columns=test_dataset.column_names\n    )\n    test_feats_small = test_features.map(\n        lambda example: example, remove_columns=['example_id', 'offset_mapping']\n    )\n\n    fin_start_logits = None\n    fin_end_logits = None\n\n    data_loader = torch.utils.data.DataLoader(\n        ChaiiDataset(test_feats_small), \n        batch_size=32,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=False\n    )\n\n    \n    model_name = \"../input/muril-large-bigbird-1k-6f/nbroad/1k-shuf-squad-chaii-6f0\"\n    for fold in tqdm(range(6)):\n        model = ChaiiModel(model_name=model_name, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n        model.transformer.load_state_dict(torch.load(f\"../input/muril-large-bigbird-1k-6f/nbroad/1k-shuf-squad-chaii-6f{fold}/pytorch_model.bin\"))\n        model.to(\"cuda\")\n        model.eval()\n\n        start_logits = []\n        end_logits = []\n\n        for b_idx, data in enumerate(data_loader):\n            with torch.no_grad():\n                for key, value in data.items():\n                    data[key] = value.to(\"cuda\")\n                output, _, _ = model(**data)\n                start = output[0].detach().cpu().numpy()\n                end = output[1].detach().cpu().numpy()\n                start_logits.append(start)\n                end_logits.append(end)\n\n        start_logits = np.vstack(start_logits)\n        end_logits = np.vstack(end_logits)\n\n        if fin_start_logits is None:\n            fin_start_logits = start_logits\n            fin_end_logits = end_logits\n        else:\n            fin_start_logits += start_logits\n            fin_end_logits += end_logits\n            \n#         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n#         with open(f\"top-preds-muril-large-f{fold}.json\", \"w\") as fp:\n#             json.dump(to_save, fp)\n\n        del model\n        torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:15:32.134761Z","iopub.execute_input":"2021-11-14T23:15:32.135051Z","iopub.status.idle":"2021-11-14T23:16:50.855914Z","shell.execute_reply.started":"2021-11-14T23:15:32.134998Z","shell.execute_reply":"2021-11-14T23:16:50.855111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    fin_start_logits /= 6\n    fin_end_logits /= 6\n    \n    to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits))\n#     with open('muril-large-preds.json', \"w\") as fp:\n#         json.dump(to_save, fp)\n\n    short_data[\"PredictionString\"] = short_data[\"id\"].map(fin_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:11.988227Z","iopub.execute_input":"2021-11-14T23:17:11.988702Z","iopub.status.idle":"2021-11-14T23:17:12.339153Z","shell.execute_reply.started":"2021-11-14T23:17:11.988663Z","shell.execute_reply":"2021-11-14T23:17:12.338318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/bb-base-chaii\")","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:18.261108Z","iopub.execute_input":"2021-11-14T23:17:18.261722Z","iopub.status.idle":"2021-11-14T23:17:18.265546Z","shell.execute_reply.started":"2021-11-14T23:17:18.261681Z","shell.execute_reply":"2021-11-14T23:17:18.264709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     pad_on_right = tokenizer.padding_side == \"right\"\n#     max_length = 4096\n#     doc_stride = 2048\n\n#     test_dataset = Dataset.from_pandas(long_data)\n#     test_features = test_dataset.map(\n#         partial(\n#             prepare_validation_features, \n#             tokenizer=tokenizer,\n#             pad_on_right=pad_on_right, \n#             max_length=max_length,\n#             doc_stride=doc_stride\n#         ),\n#         batched=True,\n#         remove_columns=test_dataset.column_names\n#     )\n#     test_feats_small = test_features.map(\n#         lambda example: example, remove_columns=['example_id', 'offset_mapping']\n#     )\n\n#     fin_start_logits = None\n#     fin_end_logits = None\n\n#     models = [\n#         \"../input/bb-base-chaii\",\n#         \"../input/nbroad-flax-muril-bb-base-chaii-f2\",\n#         \"../input/nbroad-flax-muril-bb-base-chaii-f3\",\n#         \"../input/nbroad-flax-muril-bb-base-chaii-f4\",\n#         \"../input/nbroad-flax-bb-base-chaii-f5\",\n#         \"../input/nbroad-flax-muril-bb-base-chaii-f6\",\n#         \"../input/nbroad-flax-muril-bb-base-chaii-f7\", \n#     ]\n\n#     data_loader = torch.utils.data.DataLoader(\n#         ChaiiDataset(test_feats_small), \n#         batch_size=16,\n#         num_workers=4,\n#         pin_memory=True,\n#         shuffle=False\n#     )\n\n\n#     for fold, model_name in tqdm(enumerate(models)):\n#         model = ChaiiModel(model_name=model_name, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n#         model.transformer.load_state_dict(torch.load(f\"{model_name}/pytorch_model.bin\"))\n#         model.to(\"cuda\")\n#         model.eval()\n\n#         start_logits = []\n#         end_logits = []\n\n#         for b_idx, data in enumerate(data_loader):\n#             with torch.no_grad():\n#                 for key, value in data.items():\n#                     data[key] = value.to(\"cuda\")\n#                 output, _, _ = model(**data)\n#                 start = output[0].detach().cpu().numpy()\n#                 end = output[1].detach().cpu().numpy()\n#                 start_logits.append(start)\n#                 end_logits.append(end)\n\n#         start_logits = np.vstack(start_logits)\n#         end_logits = np.vstack(end_logits)\n\n#         if fin_start_logits is None:\n#             fin_start_logits = start_logits\n#             fin_end_logits = end_logits\n#         else:\n#             fin_start_logits += start_logits\n#             fin_end_logits += end_logits\n            \n#         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n#         with open(f\"top-preds-bb-f{fold}.json\", \"w\") as fp:\n#             json.dump(to_save, fp)\n\n#         del model\n#         torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:18.464761Z","iopub.execute_input":"2021-11-14T23:17:18.465143Z","iopub.status.idle":"2021-11-14T23:17:18.470827Z","shell.execute_reply.started":"2021-11-14T23:17:18.46511Z","shell.execute_reply":"2021-11-14T23:17:18.470065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     fin_start_logits /= len(models)\n#     fin_end_logits /= len(models)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:18.652757Z","iopub.execute_input":"2021-11-14T23:17:18.654915Z","iopub.status.idle":"2021-11-14T23:17:18.658238Z","shell.execute_reply.started":"2021-11-14T23:17:18.654878Z","shell.execute_reply":"2021-11-14T23:17:18.657483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits))","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:18.784374Z","iopub.execute_input":"2021-11-14T23:17:18.78459Z","iopub.status.idle":"2021-11-14T23:17:18.788532Z","shell.execute_reply.started":"2021-11-14T23:17:18.784565Z","shell.execute_reply":"2021-11-14T23:17:18.787699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     long_data[\"PredictionString\"] = long_data[\"id\"].map(fin_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:18.960564Z","iopub.execute_input":"2021-11-14T23:17:18.960901Z","iopub.status.idle":"2021-11-14T23:17:18.964922Z","shell.execute_reply.started":"2021-11-14T23:17:18.960873Z","shell.execute_reply":"2021-11-14T23:17:18.964158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class ChaiiModel(tez.Model):\n#     def __init__(self, model_name, num_train_steps, steps_per_epoch, learning_rate):\n#         super().__init__()\n#         self.learning_rate = learning_rate\n#         self.steps_per_epoch = steps_per_epoch\n#         self.model_name = model_name\n#         self.num_train_steps = num_train_steps\n#         self.step_scheduler_after = \"batch\"\n\n#         hidden_dropout_prob: float = 0.0\n#         layer_norm_eps: float = 1e-7\n\n#         config = transformers.AutoConfig.from_pretrained(model_name)\n#         config.update(\n#             {\n#                 \"output_hidden_states\": True,\n#                 \"hidden_dropout_prob\": hidden_dropout_prob,\n#                 \"layer_norm_eps\": layer_norm_eps,\n#                 \"add_pooling_layer\": False,\n#             }\n#         )\n#         self.transformer = transformers.AutoModel.from_pretrained(model_name, config=config)\n#         self.output = nn.Linear(config.hidden_size, config.num_labels)\n\n#     def forward(self, ids, mask, token_type_ids=None, start_positions=None, end_positions=None):\n#         transformer_out = self.transformer(ids, mask)\n#         sequence_output = transformer_out[0]\n#         logits = self.output(sequence_output)\n#         start_logits, end_logits = logits.split(1, dim=-1)\n#         start_logits = start_logits.squeeze(-1).contiguous()\n#         end_logits = end_logits.squeeze(-1).contiguous()\n\n#         return (start_logits, end_logits), 0, {}","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:19.127192Z","iopub.execute_input":"2021-11-14T23:17:19.127728Z","iopub.status.idle":"2021-11-14T23:17:19.133078Z","shell.execute_reply.started":"2021-11-14T23:17:19.127693Z","shell.execute_reply":"2021-11-14T23:17:19.132238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/xlmrob\")","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:19.257144Z","iopub.execute_input":"2021-11-14T23:17:19.257704Z","iopub.status.idle":"2021-11-14T23:17:19.262983Z","shell.execute_reply.started":"2021-11-14T23:17:19.257674Z","shell.execute_reply":"2021-11-14T23:17:19.262249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     pad_on_right = tokenizer.padding_side == \"right\"\n#     max_length = 384\n#     doc_stride = 128\n\n\n#     test_dataset = Dataset.from_pandas(short_data)\n#     test_features = test_dataset.map(\n#         partial(\n#             prepare_validation_features, \n#             tokenizer=tokenizer,\n#             pad_on_right=pad_on_right, \n#             max_length=max_length,\n#             doc_stride=doc_stride\n#         ),\n#         batched=True,\n#         remove_columns=test_dataset.column_names\n#     )\n#     test_feats_small = test_features.map(\n#         lambda example: example, remove_columns=['example_id', 'offset_mapping']\n#     )\n\n#     fin_start_logits = None\n#     fin_end_logits = None\n\n#     for fold_ in tqdm(range(10)):\n#         model = ChaiiModel(model_name=\"../input/xlmrob\", num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n#         model.load(f\"../input/deepsetsquad2-v2/pytorch_model_f{fold_}.bin\", weights_only=True)\n#         model.to(\"cuda\")\n#         model.eval()\n#         data_loader = torch.utils.data.DataLoader(\n#             ChaiiDataset(test_feats_small), \n#             batch_size=64,\n#             num_workers=4,\n#             pin_memory=True,\n#             shuffle=False\n#         )\n#         start_logits = []\n#         end_logits = []\n\n#         for b_idx, data in enumerate(data_loader):\n#             with torch.no_grad():\n#                 for key, value in data.items():\n#                     data[key] = value.to(\"cuda\")\n#                 output, _, _ = model(**data)\n#                 start = output[0].detach().cpu().numpy()\n#                 end = output[1].detach().cpu().numpy()\n#                 start_logits.append(start)\n#                 end_logits.append(end)\n\n#         start_logits = np.vstack(start_logits)\n#         end_logits = np.vstack(end_logits)\n\n#         if fin_start_logits is None:\n#             fin_start_logits = start_logits\n#             fin_end_logits = end_logits\n#         else:\n#             fin_start_logits += start_logits\n#             fin_end_logits += end_logits\n        \n#         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n#         with open(f\"top-preds-xlmr-f{fold_}.json\", \"w\") as fp:\n#             json.dump(to_save, fp)\n\n#         del model\n#         torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:19.450647Z","iopub.execute_input":"2021-11-14T23:17:19.450892Z","iopub.status.idle":"2021-11-14T23:17:19.456835Z","shell.execute_reply.started":"2021-11-14T23:17:19.450865Z","shell.execute_reply":"2021-11-14T23:17:19.455563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     fin_start_logits /= 10\n#     fin_end_logits /= 10","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:19.583552Z","iopub.execute_input":"2021-11-14T23:17:19.583873Z","iopub.status.idle":"2021-11-14T23:17:19.58753Z","shell.execute_reply.started":"2021-11-14T23:17:19.583843Z","shell.execute_reply":"2021-11-14T23:17:19.586719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits))\n#     with open('xlmr-large-preds.json', \"w\") as fp:\n#         json.dump(to_save, fp)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:19.752891Z","iopub.execute_input":"2021-11-14T23:17:19.753147Z","iopub.status.idle":"2021-11-14T23:17:19.756562Z","shell.execute_reply.started":"2021-11-14T23:17:19.753118Z","shell.execute_reply":"2021-11-14T23:17:19.755901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     short_data[\"PredictionString\"] = short_data[\"id\"].map(fin_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:19.896847Z","iopub.execute_input":"2021-11-14T23:17:19.897201Z","iopub.status.idle":"2021-11-14T23:17:19.901109Z","shell.execute_reply.started":"2021-11-14T23:17:19.897169Z","shell.execute_reply":"2021-11-14T23:17:19.900009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub2 = pd.DataFrame(submission, columns=[\"id\", \"PredictionString\"])\n\n# final = pd.concat([sub1, sub2], axis=0, ignore_index=True)\n# final = final.merge(test_data[[\"context\", \"question\", \"id\"]], on=\"id\")","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:20.072448Z","iopub.execute_input":"2021-11-14T23:17:20.072723Z","iopub.status.idle":"2021-11-14T23:17:20.076363Z","shell.execute_reply.started":"2021-11-14T23:17:20.072692Z","shell.execute_reply":"2021-11-14T23:17:20.075528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# short_data","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:20.24263Z","iopub.execute_input":"2021-11-14T23:17:20.243004Z","iopub.status.idle":"2021-11-14T23:17:20.246388Z","shell.execute_reply.started":"2021-11-14T23:17:20.24297Z","shell.execute_reply":"2021-11-14T23:17:20.245566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # folds have been averaged and have only 1 prediction file \n# if do_inference:\n#     model_fold_preds = {}\n#     with open('xlmr-large-preds.json') as fp:\n#         model_fold_preds[\"xlmr\"] = json.load(fp)\n#     with open('muril-large-preds.json') as fp:\n#         model_fold_preds[\"muril\"] = json.load(fp)\n\n#     from collections import Counter\n\n#     voted_preds = {}\n#     top_k = 4\n#     for i in range(len(short_data)):\n#         cnt = Counter()\n#         for fold_, preds in enumerate(model_fold_preds.values()):\n#             cnt.update([text[\"text\"] for text in preds[i][\"predictions\"]][:top_k])\n            \n#         most_common = cnt.most_common(top_k)\n#         voted_preds[preds[i][\"id\"]] = most_common[0][0]\n        \n#     short_data[\"PredictionString\"] = short_data[\"id\"].map(voted_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:20.442867Z","iopub.execute_input":"2021-11-14T23:17:20.443394Z","iopub.status.idle":"2021-11-14T23:17:20.450267Z","shell.execute_reply.started":"2021-11-14T23:17:20.443358Z","shell.execute_reply":"2021-11-14T23:17:20.449344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     model_fold_preds = {}\n#     model = \"xlmr\"\n#     for fold_ in range(10):\n#         with open(f\"top-preds-{model}-f{fold_}.json\") as fp:\n#             model_fold_preds[f\"{model}-{fold_}\"] = json.load(fp)\n\n#     from collections import Counter\n\n#     voted_preds = {}\n#     top_k = 100\n#     for i in range(len(short_data)):\n#         cnt = Counter()\n#         for fold_, preds in enumerate(model_fold_preds.values()):\n#             cnt.update([text[\"text\"] for text in preds[i][\"predictions\"]][:top_k])\n            \n#         most_common = cnt.most_common(10)\n#         voted_preds[preds[i][\"id\"]] = most_common[0][0]\n        \n#     short_data[\"PredictionString\"] = short_data[\"id\"].map(voted_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:20.621352Z","iopub.execute_input":"2021-11-14T23:17:20.621671Z","iopub.status.idle":"2021-11-14T23:17:20.626515Z","shell.execute_reply.started":"2021-11-14T23:17:20.621642Z","shell.execute_reply":"2021-11-14T23:17:20.625721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     model_fold_preds = {}\n    \n#     model = \"bb\"\n#     for fold_ in range(7):\n#         with open(f\"top-preds-{model}-f{fold_}.json\") as fp:\n#             model_fold_preds[f\"{model}-{fold_}\"] = json.load(fp)\n\n#     from collections import Counter\n\n#     voted_preds = {}\n#     for i in range(len(long_data)):\n#         cnt = Counter()\n#         for fold_, preds in enumerate(model_fold_preds.values()):\n#             cnt.update([text[\"text\"] for text in preds[i][\"predictions\"]][:top_k])\n            \n#         most_common = cnt.most_common(10)\n#         voted_preds[preds[i][\"id\"]] = most_common[0][0]\n        \n#     long_data[\"PredictionString\"] = long_data[\"id\"].map(voted_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:20.813568Z","iopub.execute_input":"2021-11-14T23:17:20.81423Z","iopub.status.idle":"2021-11-14T23:17:20.818275Z","shell.execute_reply.started":"2021-11-14T23:17:20.814197Z","shell.execute_reply":"2021-11-14T23:17:20.817191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     test_data = pd.concat([short_data, long_data], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:20.987285Z","iopub.execute_input":"2021-11-14T23:17:20.987517Z","iopub.status.idle":"2021-11-14T23:17:20.990763Z","shell.execute_reply.started":"2021-11-14T23:17:20.987491Z","shell.execute_reply":"2021-11-14T23:17:20.990094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    test_data = short_data.copy()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:21.550539Z","iopub.execute_input":"2021-11-14T23:17:21.550851Z","iopub.status.idle":"2021-11-14T23:17:21.556509Z","shell.execute_reply.started":"2021-11-14T23:17:21.550821Z","shell.execute_reply":"2021-11-14T23:17:21.555767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:22.083121Z","iopub.execute_input":"2021-11-14T23:17:22.083697Z","iopub.status.idle":"2021-11-14T23:17:22.097388Z","shell.execute_reply.started":"2021-11-14T23:17:22.083657Z","shell.execute_reply":"2021-11-14T23:17:22.096372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\nbad_endings = [\"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\ntamil_ad = \"கி.பி\"\ntamil_bc = \"கி.மு\"\ntamil_km = \"கி.மீ\"\nhindi_ad = \"ई\"\nhindi_bc = \"ई.पू\"\n\ncleaned_preds = []\nif do_inference:\n    for pred, context in test_data[[\"PredictionString\", \"context\"]].values:\n        if pred == \"\":\n            cleaned_preds.append(pred)\n            continue\n\n        # I haven't check sure if this makes a difference, but there is one answer in the training set that ends like this and I think it is an annotator mistake\n        # see my notebook here for details https://www.kaggle.com/nbroad/chaii-qa-character-token-languages-eda \n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n\n        pred = pred.lstrip(\"\".join(bad_starts))\n        pred = pred.rstrip(\"\".join(bad_endings))\n\n        if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n            pred = pred+\".\"\n\n\n        cleaned_preds.append(pred)\n\n    test_data[\"PredictionString\"] = cleaned_preds","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:22.582219Z","iopub.execute_input":"2021-11-14T23:17:22.58291Z","iopub.status.idle":"2021-11-14T23:17:22.599571Z","shell.execute_reply.started":"2021-11-14T23:17:22.582842Z","shell.execute_reply":"2021-11-14T23:17:22.598713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    test_data[\"pred_len\"] = [len(x) for x in test_data[\"PredictionString\"]]\n# do something if the prediction is too short","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:23.206688Z","iopub.execute_input":"2021-11-14T23:17:23.20749Z","iopub.status.idle":"2021-11-14T23:17:23.213841Z","shell.execute_reply.started":"2021-11-14T23:17:23.207436Z","shell.execute_reply":"2021-11-14T23:17:23.212977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    test_data[[\"id\", \"PredictionString\"]].to_csv(\"submission.csv\", index=False)\nelse:\n    test_data[\"PredictionString\"] = \"lol\"\n    test_data[[\"id\", \"PredictionString\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:24.170535Z","iopub.execute_input":"2021-11-14T23:17:24.170994Z","iopub.status.idle":"2021-11-14T23:17:24.178251Z","shell.execute_reply.started":"2021-11-14T23:17:24.170954Z","shell.execute_reply":"2021-11-14T23:17:24.177453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    print(test_data[[\"id\", \"PredictionString\"]])","metadata":{"execution":{"iopub.status.busy":"2021-11-14T23:17:24.965282Z","iopub.execute_input":"2021-11-14T23:17:24.967417Z","iopub.status.idle":"2021-11-14T23:17:24.976454Z","shell.execute_reply.started":"2021-11-14T23:17:24.967379Z","shell.execute_reply":"2021-11-14T23:17:24.973604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}