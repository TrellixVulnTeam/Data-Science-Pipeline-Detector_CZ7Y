{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem: Not enough training data üòü\n\n## Solution: üí°\n### 1. Train a model to make questions ‚ùì\n### 2. Use the question-generating model to create questions on new texts üìú‚ùì‚ùì\n### 3. Use a trained QA model to answer those questions, creating new training data. üìä\n### 4. Train a new and improved QA model using the pseudo-labels. üí™\n### 5. If unsatisfied with model, go to step 2 üîÑ\n### 6. ??? ü§∑‚Äç‚ôÇÔ∏è\n### 7. Profit üí∞\n\n#### Nearly everything in this notebook is from here: https://github.com/huggingface/transformers/blob/master/examples/pytorch/summarization/run_summarization.py\n\nü§ó üíñ\n\nThis is just a proof of concept using a small model. There are probably better hyperparameters and bigger, better models. If you have questions, comments, or feedback, you know where to leave them! üòä","metadata":{}},{"cell_type":"markdown","source":"# Seq2Seq for Question Generation\n\nJust like how models can make a summary from a text, why not train a model to make a question? We already have the dataset -- we can just use the `context` and `question` columns.","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install git+https://github.com/huggingface/transformers.git\n!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq\n!pip install -U wandb\n!pip install rouge_score\n# !pip install deepspeed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport datasets\nimport nltk  # Here to have a nice missing dependency error message early on\nimport numpy as np\nfrom datasets import load_dataset, load_metric\nimport torch\n\nimport transformers\nfrom filelock import FileLock\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    HfArgumentParser,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:45:28.634421Z","iopub.execute_input":"2021-08-14T02:45:28.634808Z","iopub.status.idle":"2021-08-14T02:45:31.994582Z","shell.execute_reply.started":"2021-08-14T02:45:28.634721Z","shell.execute_reply":"2021-08-14T02:45:31.993265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nchaii_df = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/train.csv\", usecols=[\"context\",\"question\"])\nmlqa_df = pd.read_csv(\"../input/mlqa-hindi-processed/mlqa_hindi.csv\", usecols=[\"context\",\"question\"])\nxquad_df = pd.read_csv(\"../input/mlqa-hindi-processed/xquad.csv\", usecols=[\"context\",\"question\"])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-14T02:45:31.997985Z","iopub.execute_input":"2021-08-14T02:45:31.998771Z","iopub.status.idle":"2021-08-14T02:45:32.408999Z","shell.execute_reply.started":"2021-08-14T02:45:31.99872Z","shell.execute_reply":"2021-08-14T02:45:32.408124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just use chaii datasets for validation\ntrain_df, val_df = train_test_split(chaii_df, test_size=0.2)\ntrain_df = pd.concat([train_df, mlqa_df, xquad_df], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:45:32.410975Z","iopub.execute_input":"2021-08-14T02:45:32.411326Z","iopub.status.idle":"2021-08-14T02:45:32.779566Z","shell.execute_reply.started":"2021-08-14T02:45:32.411291Z","shell.execute_reply":"2021-08-14T02:45:32.777744Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv(\"train.csv\", index=False)\nval_df.to_csv(\"val.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:45:32.780727Z","iopub.status.idle":"2021-08-14T02:45:32.781108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nkey = user_secrets.get_secret(\"wandb\")\n\nimport wandb\n\nwandb.login(key=key)\nos.environ[\"WANDB_PROJECT\"] = \"chaii-qa\"","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:45:32.782429Z","iopub.status.idle":"2021-08-14T02:45:32.78312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    run_name = \"mt5-base-q-gen-chaii-mlqa-xquad-hindi\"\n    \n    seed = 0\n    \n    model_name_or_path = \"google/mt5-base\"\n    train_file = \"train.csv\"\n    validation_file = \"val.csv\"\n    text_column = \"context\"\n    summary_column = \"question\"\n    \n    max_source_length = 1024\n    max_target_length = 128\n    val_max_target_length = 128\n    pad_to_max_length = False\n    num_beams = 4\n    output_dir = \"output\"\n    per_device_train_batch_size = 1\n    per_device_eval_batch_size = 1\n    gradient_accumulation_steps = 2\n    num_train_epochs = 5\n    evaluation_strategy = 'steps'\n    eval_steps = 75\n    learning_rate = 3e-4\n    weight_decay = 0.01\n    warmup_ratio = 0.1\n    logging_steps = 20\n    save_total_limit = 2\n\n    \n    source_prefix = \"question\"\n    ignore_pad_token_for_loss = True","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:45:42.327989Z","iopub.execute_input":"2021-08-14T02:45:42.32834Z","iopub.status.idle":"2021-08-14T02:45:42.334425Z","shell.execute_reply.started":"2021-08-14T02:45:42.328306Z","shell.execute_reply":"2021-08-14T02:45:42.333544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(CFG.seed)\n\ndata_files = {}\nif CFG.train_file is not None:\n    data_files[\"train\"] = CFG.train_file\n    extension = CFG.train_file.split(\".\")[-1]\nif CFG.validation_file is not None:\n    data_files[\"validation\"] = CFG.validation_file\n    extension = CFG.validation_file.split(\".\")[-1]\n\nraw_datasets = load_dataset(extension, data_files=data_files)\n\n# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n# https://huggingface.co/docs/datasets/loading_datasets.html.","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:45:45.128994Z","iopub.execute_input":"2021-08-14T02:45:45.129362Z","iopub.status.idle":"2021-08-14T02:45:45.366868Z","shell.execute_reply.started":"2021-08-14T02:45:45.12933Z","shell.execute_reply":"2021-08-14T02:45:45.366061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pretrained model and tokenizer\n#\n# Distributed training:\n# The .from_pretrained methods guarantee that only one local process can concurrently\n# download model & vocab.\nconfig = AutoConfig.from_pretrained(CFG.model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(CFG.model_name_or_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    CFG.model_name_or_path,\n    config=config,\n)\n\nmodel.resize_token_embeddings(len(tokenizer))\n\nif model.config.decoder_start_token_id is None:\n    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n\nprefix = CFG.source_prefix if CFG.source_prefix is not None else \"\"","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:45:45.369188Z","iopub.execute_input":"2021-08-14T02:45:45.369564Z","iopub.status.idle":"2021-08-14T02:45:55.502051Z","shell.execute_reply.started":"2021-08-14T02:45:45.369527Z","shell.execute_reply":"2021-08-14T02:45:55.501161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing the datasets.\n# We need to tokenize inputs and targets.\ncolumn_names = raw_datasets[\"train\"].column_names\n\n\n# Get the column names for input/target.\ntext_column = CFG.text_column\nsummary_column = CFG.summary_column\n\n\n# Temporarily set max_target_length for training.\nmax_target_length = CFG.max_target_length\npadding = \"max_length\" if CFG.pad_to_max_length else False\n\n\ndef preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[summary_column]\n    inputs = [prefix + inp for inp in inputs]\n    model_inputs = tokenizer(inputs, max_length=CFG.max_source_length, padding=padding, truncation=True)\n\n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n\n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\" and CFG.ignore_pad_token_for_loss:\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:45:55.503686Z","iopub.execute_input":"2021-08-14T02:45:55.503998Z","iopub.status.idle":"2021-08-14T02:45:55.512962Z","shell.execute_reply.started":"2021-08-14T02:45:55.503965Z","shell.execute_reply":"2021-08-14T02:45:55.510955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train dataset\ntrain_dataset = raw_datasets[\"train\"]\n\ntrain_dataset = train_dataset.map(\n        preprocess_function,\n        batched=True,\n        remove_columns=column_names,\n        desc=\"Running tokenizer on train dataset\",\n    )\n\n# Eval dataset\n\nmax_target_length = CFG.val_max_target_length\n\neval_dataset = raw_datasets[\"validation\"]\n\n\neval_dataset = eval_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=column_names,\n    desc=\"Running tokenizer on validation dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:45:55.514816Z","iopub.execute_input":"2021-08-14T02:45:55.515172Z","iopub.status.idle":"2021-08-14T02:46:27.581256Z","shell.execute_reply.started":"2021-08-14T02:45:55.515135Z","shell.execute_reply":"2021-08-14T02:46:27.580159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data collator\nlabel_pad_token_id = -100 if CFG.ignore_pad_token_for_loss else tokenizer.pad_token_id\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n)\n\n# Metric\nmetric = load_metric(\"rouge\")\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # rougeLSum expects newline after each sentence\n    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    if CFG.ignore_pad_token_for_loss:\n        # Replace -100 in the labels as we can't decode them.\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    # Extract a few results from ROUGE\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:46:27.582821Z","iopub.execute_input":"2021-08-14T02:46:27.583322Z","iopub.status.idle":"2021-08-14T02:46:28.072825Z","shell.execute_reply.started":"2021-08-14T02:46:27.583279Z","shell.execute_reply":"2021-08-14T02:46:28.072018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    do_train=True,\n    do_eval=True,\n    per_device_train_batch_size=CFG.per_device_train_batch_size,\n    per_device_eval_batch_size=CFG.per_device_eval_batch_size,\n    num_train_epochs=CFG.num_train_epochs,\n    output_dir=CFG.output_dir,\n    evaluation_strategy=CFG.evaluation_strategy,\n    learning_rate=CFG.learning_rate,\n    weight_decay=CFG.weight_decay,\n    warmup_ratio=CFG.warmup_ratio,\n    logging_steps=CFG.logging_steps,\n    save_total_limit=CFG.save_total_limit,\n    eval_steps=CFG.eval_steps,\n    run_name=CFG.run_name,\n    gradient_accumulation_steps=CFG.gradient_accumulation_steps\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:46:28.074168Z","iopub.execute_input":"2021-08-14T02:46:28.07453Z","iopub.status.idle":"2021-08-14T02:46:28.114037Z","shell.execute_reply.started":"2021-08-14T02:46:28.074492Z","shell.execute_reply":"2021-08-14T02:46:28.113223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize our Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset if training_args.do_train else None,\n    eval_dataset=eval_dataset if training_args.do_eval else None,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n)\n\n# Training\nif training_args.do_train:\n\n    train_result = trainer.train()\n    trainer.save_model()  # Saves the tokenizer too for easy upload\n\n    metrics = train_result.metrics\n\n    metrics[\"train_samples\"] = len(train_dataset)\n\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n\n# Evaluation\nresults = {}\nif training_args.do_eval:\n    print(\"*** Evaluate ***\")\n\n    metrics[\"eval_samples\"] = len(eval_dataset)\n\n    trainer.log_metrics(\"eval\", metrics)\n    trainer.save_metrics(\"eval\", metrics)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:46:28.117273Z","iopub.execute_input":"2021-08-14T02:46:28.117542Z","iopub.status.idle":"2021-08-14T02:51:08.723211Z","shell.execute_reply.started":"2021-08-14T02:46:28.117516Z","shell.execute_reply":"2021-08-14T02:51:08.721781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ndel trainer\ndel train_dataset\ndel eval_dataset\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import MT5ForConditionalGeneration\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = MT5ForConditionalGeneration.from_pretrained(CFG.output_dir).to(device)\n\nmodel.eval()\n\ndef show_context_and_question(context, model):\n    prefix = CFG.source_prefix\n    full_ctx = f\"{prefix}: \" + context\n    \n    with torch.no_grad():\n        inputs = tokenizer(full_ctx, return_tensors='pt') \n        inputs = {k:v.to(device) for k, v in inputs.items()}\n\n        output = model.generate(input_ids=inputs[\"input_ids\"])\n\n        question = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(output.detach().squeeze()))\n        \n        if question.startswith(\"<pad> <extra_id_0> \"):\n            question = question[len(\"<pad> <extra_id_0> \"):]\n        if question.endswith(\"</s>\"):\n            question = question[:-len(\"</s>\")]\n    \n    return (context, question)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T03:09:53.524559Z","iopub.execute_input":"2021-08-14T03:09:53.524884Z","iopub.status.idle":"2021-08-14T03:09:57.674729Z","shell.execute_reply.started":"2021-08-14T03:09:53.524852Z","shell.execute_reply":"2021-08-14T03:09:57.673845Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    ctx, q = show_context_and_question(val_df[\"context\"].values[i], model)\n    print(\"CONTEXT:\", ctx, \"\\n\\n\", \"QUESTION:\", q, \"\\n\\n\", \"*\"*100, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-08-14T03:10:28.015162Z","iopub.execute_input":"2021-08-14T03:10:28.015497Z","iopub.status.idle":"2021-08-14T03:10:29.079714Z","shell.execute_reply.started":"2021-08-14T03:10:28.015467Z","shell.execute_reply":"2021-08-14T03:10:29.078826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It might not be great, but I'm sure there are people out there who can make this a lot better! The concept is really what I'm trying to get across. üî•","metadata":{}}]}