{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Keras based notebook for QA\n\nThe code used in this notebook is largely derived from two excellent sources:\nhttps://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb\nhttps://keras.io/examples/nlp/text_extraction_with_bert/","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport re\nimport pandas as pd\nimport json\nimport string\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom transformers.modeling_tf_utils import shape_list\nfrom transformers import BertTokenizerFast, TFBertModel,TFXLMRobertaModel,XLMRobertaTokenizerFast\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-21T07:43:35.307131Z","iopub.execute_input":"2021-08-21T07:43:35.307613Z","iopub.status.idle":"2021-08-21T07:43:35.316438Z","shell.execute_reply.started":"2021-08-21T07:43:35.307584Z","shell.execute_reply":"2021-08-21T07:43:35.315501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 384\ndoc_stride = 128","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:42:50.79245Z","iopub.execute_input":"2021-08-21T07:42:50.792952Z","iopub.status.idle":"2021-08-21T07:42:50.796244Z","shell.execute_reply.started":"2021-08-21T07:42:50.79292Z","shell.execute_reply":"2021-08-21T07:42:50.795554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer = BertTokenizerFast.from_pretrained(\"../input/bert-base-multilingual-cased/bert-base-multilingual-cased\")\ntokenizer = XLMRobertaTokenizerFast.from_pretrained(\"../input/jplu-tf-xlm-roberta-large\")","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:43:44.013123Z","iopub.execute_input":"2021-08-21T07:43:44.01347Z","iopub.status.idle":"2021-08-21T07:43:46.22011Z","shell.execute_reply.started":"2021-08-21T07:43:44.01344Z","shell.execute_reply":"2021-08-21T07:43:46.219145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/chaii-hindi-and-tamil-question-answering/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:41:54.756759Z","iopub.execute_input":"2021-08-21T07:41:54.757175Z","iopub.status.idle":"2021-08-21T07:41:55.590967Z","shell.execute_reply.started":"2021-08-21T07:41:54.757141Z","shell.execute_reply":"2021-08-21T07:41:55.590034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nprint(test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:42:01.943211Z","iopub.execute_input":"2021-08-21T07:42:01.943688Z","iopub.status.idle":"2021-08-21T07:42:01.971033Z","shell.execute_reply.started":"2021-08-21T07:42:01.943644Z","shell.execute_reply":"2021-08-21T07:42:01.970117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train,val = train_test_split(train,test_size=0.20,random_state=2021)\ntrain = train.reset_index(drop=True)\nval = val.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:42:17.693111Z","iopub.execute_input":"2021-08-21T07:42:17.693453Z","iopub.status.idle":"2021-08-21T07:42:17.70242Z","shell.execute_reply.started":"2021-08-21T07:42:17.693417Z","shell.execute_reply":"2021-08-21T07:42:17.701343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        list(examples[\"question\"].values),\n        list(examples[\"context\"].values),\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n    \n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples.loc[sample_index,\"answer_text\"]\n        answer_start = examples.loc[sample_index,\"answer_start\"]\n        \n        # If no answers are given, set the cls_index as answer.\n        if answer_start is None:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answer_start\n            end_char = start_char + len(answers)\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:42:21.434377Z","iopub.execute_input":"2021-08-21T07:42:21.434734Z","iopub.status.idle":"2021-08-21T07:42:21.450354Z","shell.execute_reply.started":"2021-08-21T07:42:21.434703Z","shell.execute_reply":"2021-08-21T07:42:21.449263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_validation_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        list(examples[\"question\"].values),\n        list(examples[\"context\"].values),\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 \n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples.loc[sample_index,\"id\"])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:42:30.268428Z","iopub.execute_input":"2021-08-21T07:42:30.268967Z","iopub.status.idle":"2021-08-21T07:42:30.278005Z","shell.execute_reply.started":"2021-08-21T07:42:30.26892Z","shell.execute_reply":"2021-08-21T07:42:30.276817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encodings  =  prepare_train_features(train)\nval_encodings    =  prepare_validation_features(val)\ntest_encodings   =  prepare_validation_features(test)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:43:53.051633Z","iopub.execute_input":"2021-08-21T07:43:53.052001Z","iopub.status.idle":"2021-08-21T07:44:15.833156Z","shell.execute_reply.started":"2021-08-21T07:43:53.051957Z","shell.execute_reply":"2021-08-21T07:44:15.83218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\n    {key: train_encodings[key] for key in ['start_positions', 'end_positions']}\n))\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},\n    \n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: test_encodings[key] for key in ['input_ids', 'attention_mask']},\n    \n))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:44:56.614771Z","iopub.execute_input":"2021-08-21T07:44:56.61516Z","iopub.status.idle":"2021-08-21T07:45:34.385574Z","shell.execute_reply.started":"2021-08-21T07:44:56.615124Z","shell.execute_reply":"2021-08-21T07:45:34.384374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))\nval_dataset = val_dataset.map(lambda x: x)\ntest_dataset = test_dataset.map(lambda x: x)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:45:57.450556Z","iopub.execute_input":"2021-08-21T07:45:57.450919Z","iopub.status.idle":"2021-08-21T07:45:57.549198Z","shell.execute_reply.started":"2021-08-21T07:45:57.450888Z","shell.execute_reply":"2021-08-21T07:45:57.547964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n    ## encoder\n    #encoder = TFBertModel.from_pretrained(\"../input/bert-base-multilingual-cased/bert-base-multilingual-cased\")\n    encoder = TFXLMRobertaModel.from_pretrained(\"../input/jplu-tf-xlm-roberta-large\")\n    ## QA Model\n    input_ids = layers.Input(shape=(max_length,), dtype=tf.int32,name='input_ids')\n    #token_type_ids = layers.Input(shape=(max_length,), dtype=tf.int32,name='token_type_ids')\n    attention_mask = layers.Input(shape=(max_length,), dtype=tf.int32,name='attention_mask')\n    embedding = encoder(\n        input_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax,name='start_positions')(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax,name='end_positions')(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:47:24.451026Z","iopub.execute_input":"2021-08-21T07:47:24.451739Z","iopub.status.idle":"2021-08-21T07:47:24.464856Z","shell.execute_reply.started":"2021-08-21T07:47:24.451697Z","shell.execute_reply":"2021-08-21T07:47:24.464089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:47:30.013091Z","iopub.execute_input":"2021-08-21T07:47:30.013587Z","iopub.status.idle":"2021-08-21T07:48:05.717176Z","shell.execute_reply.started":"2021-08-21T07:47:30.013557Z","shell.execute_reply":"2021-08-21T07:48:05.7158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset.shuffle(1000).batch(8), epochs=1, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:04:09.862953Z","iopub.execute_input":"2021-08-21T06:04:09.863349Z","iopub.status.idle":"2021-08-21T06:22:05.656792Z","shell.execute_reply.started":"2021-08-21T06:04:09.8633Z","shell.execute_reply":"2021-08-21T06:22:05.656017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start,end = model.predict(val_dataset.batch(4))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T07:50:54.37932Z","iopub.execute_input":"2021-08-21T07:50:54.37972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start.shape,end.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:23:52.286711Z","iopub.execute_input":"2021-08-21T06:23:52.287055Z","iopub.status.idle":"2021-08-21T06:23:52.295427Z","shell.execute_reply.started":"2021-08-21T06:23:52.28702Z","shell.execute_reply":"2021-08-21T06:23:52.294403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport collections\n\ndef postprocess_qa_predictions(examples, features, start,end, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = start,end\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features[\"example_id\"]):\n        \n        features_per_example[example_id_to_index[feature]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features['input_ids'])} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples.context.values)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        context = example\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[\"offset_mapping\"][feature_index]\n\n            # Update minimum null prediction.\n            cls_index = features[\"input_ids\"][feature_index].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        #if not squad_v2:\n        #    predictions[example[\"id\"]] = best_answer[\"text\"]\n        #else:\n        answer = best_answer[\"text\"] \n        predictions[example_index] = answer\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:23:52.29712Z","iopub.execute_input":"2021-08-21T06:23:52.29769Z","iopub.status.idle":"2021-08-21T06:23:52.311898Z","shell.execute_reply.started":"2021-08-21T06:23:52.297652Z","shell.execute_reply":"2021-08-21T06:23:52.31094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_predictions = postprocess_qa_predictions(val, val_encodings, start,end)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:23:52.313392Z","iopub.execute_input":"2021-08-21T06:23:52.314153Z","iopub.status.idle":"2021-08-21T06:23:54.052252Z","shell.execute_reply.started":"2021-08-21T06:23:52.314113Z","shell.execute_reply":"2021-08-21T06:23:54.051275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:23:54.053641Z","iopub.execute_input":"2021-08-21T06:23:54.053993Z","iopub.status.idle":"2021-08-21T06:23:54.06013Z","shell.execute_reply.started":"2021-08-21T06:23:54.053956Z","shell.execute_reply":"2021-08-21T06:23:54.059016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = []\nfor idx in range(len(val)):\n    str1 = val.answer_text.values[idx]\n    str2 = val_predictions[idx]\n    score.append(jaccard(str1,str2))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:23:54.061479Z","iopub.execute_input":"2021-08-21T06:23:54.062296Z","iopub.status.idle":"2021-08-21T06:23:54.074818Z","shell.execute_reply.started":"2021-08-21T06:23:54.062257Z","shell.execute_reply":"2021-08-21T06:23:54.074096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(score).mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:26:20.759574Z","iopub.execute_input":"2021-08-21T06:26:20.759901Z","iopub.status.idle":"2021-08-21T06:26:20.768Z","shell.execute_reply.started":"2021-08-21T06:26:20.759871Z","shell.execute_reply":"2021-08-21T06:26:20.767036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:23:54.076243Z","iopub.execute_input":"2021-08-21T06:23:54.076588Z","iopub.status.idle":"2021-08-21T06:23:54.109048Z","shell.execute_reply.started":"2021-08-21T06:23:54.076554Z","shell.execute_reply":"2021-08-21T06:23:54.107959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start,end = model.predict(test_dataset.batch(4))\nstart.shape,end.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:23:54.110427Z","iopub.execute_input":"2021-08-21T06:23:54.110794Z","iopub.status.idle":"2021-08-21T06:23:56.207463Z","shell.execute_reply.started":"2021-08-21T06:23:54.11075Z","shell.execute_reply":"2021-08-21T06:23:56.206592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = postprocess_qa_predictions(test, test_encodings, start,end)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:23:56.210771Z","iopub.execute_input":"2021-08-21T06:23:56.211024Z","iopub.status.idle":"2021-08-21T06:23:56.284161Z","shell.execute_reply.started":"2021-08-21T06:23:56.210999Z","shell.execute_reply":"2021-08-21T06:23:56.283273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = []\nfor idx in range(len(test)):\n    test_preds.append(test_predictions[idx])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:27:21.137682Z","iopub.execute_input":"2021-08-21T06:27:21.138084Z","iopub.status.idle":"2021-08-21T06:27:21.142776Z","shell.execute_reply.started":"2021-08-21T06:27:21.13805Z","shell.execute_reply":"2021-08-21T06:27:21.141658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['PredictionString'] = test_preds\ntest[['id','PredictionString']].head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:28:47.645941Z","iopub.execute_input":"2021-08-21T06:28:47.646276Z","iopub.status.idle":"2021-08-21T06:28:47.660497Z","shell.execute_reply.started":"2021-08-21T06:28:47.646246Z","shell.execute_reply":"2021-08-21T06:28:47.659706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['id','PredictionString']].to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T06:23:56.621547Z","iopub.status.idle":"2021-08-21T06:23:56.62213Z"},"trusted":true},"execution_count":null,"outputs":[]}]}