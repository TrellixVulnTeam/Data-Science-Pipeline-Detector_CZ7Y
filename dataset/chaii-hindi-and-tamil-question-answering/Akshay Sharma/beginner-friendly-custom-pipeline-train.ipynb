{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-22T02:52:18.971561Z","iopub.execute_input":"2021-09-22T02:52:18.971816Z","iopub.status.idle":"2021-09-22T02:52:18.976035Z","shell.execute_reply.started":"2021-09-22T02:52:18.971791Z","shell.execute_reply":"2021-09-22T02:52:18.97498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Notebook I used for reference\n\nhttps://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n\nhttps://www.kaggle.com/thedrcat/chaii-eda-baseline/comments\n\nhttps://www.kaggle.com/shahules/chaii-xlm-base-custom-qa-train-infer/data?scriptVersionId=72059052\n\nhttps://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:19.004994Z","iopub.execute_input":"2021-09-22T02:52:19.005232Z","iopub.status.idle":"2021-09-22T02:52:19.898402Z","shell.execute_reply.started":"2021-09-22T02:52:19.005208Z","shell.execute_reply":"2021-09-22T02:52:19.8976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.language.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:19.899796Z","iopub.execute_input":"2021-09-22T02:52:19.900128Z","iopub.status.idle":"2021-09-22T02:52:19.910107Z","shell.execute_reply.started":"2021-09-22T02:52:19.900091Z","shell.execute_reply":"2021-09-22T02:52:19.909089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_answers(r):\n    start = r[0]\n    text = r[1]\n    return {\n        'answer_start': [start],\n        'text': [text]\n    }","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:19.912103Z","iopub.execute_input":"2021-09-22T02:52:19.912512Z","iopub.status.idle":"2021-09-22T02:52:19.918287Z","shell.execute_reply.started":"2021-09-22T02:52:19.912473Z","shell.execute_reply":"2021-09-22T02:52:19.917361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:19.920063Z","iopub.execute_input":"2021-09-22T02:52:19.920558Z","iopub.status.idle":"2021-09-22T02:52:19.958387Z","shell.execute_reply.started":"2021-09-22T02:52:19.920518Z","shell.execute_reply":"2021-09-22T02:52:19.957492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.to_csv('train.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:19.959642Z","iopub.execute_input":"2021-09-22T02:52:19.959972Z","iopub.status.idle":"2021-09-22T02:52:19.963449Z","shell.execute_reply.started":"2021-09-22T02:52:19.959938Z","shell.execute_reply":"2021-09-22T02:52:19.962632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading custom data in hugging face datasets lib","metadata":{}},{"cell_type":"code","source":"# ! pip install datasets transformers\n!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:19.9646Z","iopub.execute_input":"2021-09-22T02:52:19.965103Z","iopub.status.idle":"2021-09-22T02:52:29.484593Z","shell.execute_reply.started":"2021-09-22T02:52:19.965067Z","shell.execute_reply":"2021-09-22T02:52:29.483638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nfrom pprint import pprint","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:29.486185Z","iopub.execute_input":"2021-09-22T02:52:29.486529Z","iopub.status.idle":"2021-09-22T02:52:30.124695Z","shell.execute_reply.started":"2021-09-22T02:52:29.486489Z","shell.execute_reply":"2021-09-22T02:52:30.123722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# model_checkpoint = \"xlm-roberta-base\"\n# model_checkpoint = \"deepset/xlm-roberta-large-squad2\"\nmodel_checkpoint = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2\"\n# model_checkpoint = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n\nfrom transformers import XLMTokenizer,AutoTokenizer\n# tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:30.127153Z","iopub.execute_input":"2021-09-22T02:52:30.127571Z","iopub.status.idle":"2021-09-22T02:52:37.256398Z","shell.execute_reply.started":"2021-09-22T02:52:30.127503Z","shell.execute_reply":"2021-09-22T02:52:37.255518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Doc_stride is used to handle large text: tokens>512","metadata":{}},{"cell_type":"code","source":"max_length = 384 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:37.258126Z","iopub.execute_input":"2021-09-22T02:52:37.258509Z","iopub.status.idle":"2021-09-22T02:52:37.263232Z","shell.execute_reply.started":"2021-09-22T02:52:37.258471Z","shell.execute_reply":"2021-09-22T02:52:37.262263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# putting all together","metadata":{}},{"cell_type":"code","source":"# In some padding required on left side\npad_on_right = tokenizer.padding_side == \"right\"","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:37.264712Z","iopub.execute_input":"2021-09-22T02:52:37.265206Z","iopub.status.idle":"2021-09-22T02:52:37.271687Z","shell.execute_reply.started":"2021-09-22T02:52:37.265169Z","shell.execute_reply":"2021-09-22T02:52:37.270858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n#     print(len(offset_mapping))\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:37.273148Z","iopub.execute_input":"2021-09-22T02:52:37.273577Z","iopub.status.idle":"2021-09-22T02:52:37.288825Z","shell.execute_reply.started":"2021-09-22T02:52:37.273543Z","shell.execute_reply":"2021-09-22T02:52:37.288026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Fold ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfolds = 5\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n# df = train[:64]\ndf = train\nfor f, (t_, v_) in enumerate(kf.split(X=df, y=df.language.values)):\n        df.loc[v_, 'kfold'] = f","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:37.290142Z","iopub.execute_input":"2021-09-22T02:52:37.290652Z","iopub.status.idle":"2021-09-22T02:52:37.773711Z","shell.execute_reply.started":"2021-09-22T02:52:37.290615Z","shell.execute_reply":"2021-09-22T02:52:37.77288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset = Dataset.from_pandas(train[:-64])\n# eval_dataset = Dataset.from_pandas(train[-64:])\n\n#just to verify pipeline\n# dataset = Dataset.from_pandas(train[:32])\n# eval_dataset = Dataset.from_pandas(train[32:48])","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:37.775059Z","iopub.execute_input":"2021-09-22T02:52:37.775547Z","iopub.status.idle":"2021-09-22T02:52:37.779339Z","shell.execute_reply.started":"2021-09-22T02:52:37.775509Z","shell.execute_reply":"2021-09-22T02:52:37.778245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenized_train_datasets = dataset.map(prepare_train_features, batched=True, remove_columns=dataset.column_names)\n# tokenized_eval_datasets = eval_dataset.map(prepare_train_features, batched=True, remove_columns=eval_dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:37.780798Z","iopub.execute_input":"2021-09-22T02:52:37.781223Z","iopub.status.idle":"2021-09-22T02:52:37.788503Z","shell.execute_reply.started":"2021-09-22T02:52:37.781185Z","shell.execute_reply":"2021-09-22T02:52:37.78742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finetune start","metadata":{}},{"cell_type":"code","source":"import gc\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoConfig, AutoModel\nfrom transformers import default_data_collator\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam,AdamW\nfrom tqdm import tqdm\n\ndata_collator = default_data_collator\ngc.collect()\n%env WANDB_DISABLED=True\nconfig = {\n    'model': model_checkpoint,\n    'batch_size': 4,\n    \"epochs\": 3,#1,\n    'lr':0.001,\n    'weight_decay':0,\n    'grad_acc': 8\n}","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:37.78989Z","iopub.execute_input":"2021-09-22T02:52:37.79037Z","iopub.status.idle":"2021-09-22T02:52:38.164119Z","shell.execute_reply.started":"2021-09-22T02:52:37.790335Z","shell.execute_reply":"2021-09-22T02:52:38.163258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# def chaiiModel(model_checkpoint):\n    \n#      return AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\nclass chaiiModel(nn.Module):\n    \n    def __init__(self):\n        super(chaiiModel,self).__init__()\n        \n        self.model_config = AutoConfig.from_pretrained(config['model'])\n#         print(self.model_config)\n        self.model_config.return_dict=True\n#         self.model = AutoModelForQuestionAnswering.from_pretrained(config['model'],config=self.model_config)\n        self.model = AutoModel.from_pretrained(config['model'], config=self.model_config)\n        self.dropout = nn.Dropout(0.1)\n        self.fc = nn.Linear(self.model_config.hidden_size,2)\n        \n    def forward(self,input_ids,attention_mask):\n        \n        output = self.model(input_ids,attention_mask)\n#         print(output[0].shape)\n#         print(output[1].shape)\n        x = self.dropout(output[0])\n        x = self.fc(x)\n#         print(x.shape)\n        start_logits,end_logits = x.split(1,dim=-1)\n#         print(start_logits.shape)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n#         print(start_logits.shape)\n                \n        return start_logits, end_logits","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:38.165441Z","iopub.execute_input":"2021-09-22T02:52:38.165971Z","iopub.status.idle":"2021-09-22T02:52:38.17402Z","shell.execute_reply.started":"2021-09-22T02:52:38.165931Z","shell.execute_reply":"2021-09-22T02:52:38.173276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss and Jac score","metadata":{}},{"cell_type":"code","source":"def safe_div(x,y):\n    if y == 0:\n        return 1\n    return x / y\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return safe_div(float(len(c)) , (len(a) + len(b) - len(c)))\n\ndef get_jaccard_score(y_true,y_pred):\n    assert len(y_true)==len(y_pred)\n    score=0.0\n    for i in range(len(y_true)):\n        score += jaccard(y_true[i], y_pred[i])\n        \n    return score\n\ndef chaii_loss(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n#     print('chaii_loss')\n#     print(start_logits.shape)\n#     print(start_positions.shape)\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = (start_loss + end_loss)/2\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:38.176856Z","iopub.execute_input":"2021-09-22T02:52:38.177128Z","iopub.status.idle":"2021-09-22T02:52:38.187481Z","shell.execute_reply.started":"2021-09-22T02:52:38.177092Z","shell.execute_reply":"2021-09-22T02:52:38.1866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# generate Data","metadata":{}},{"cell_type":"code","source":"def getData(df, fold):\n    train = df[df['kfold']!=fold]\n    valid = df[df['kfold']==fold]\n    dataset = Dataset.from_pandas(train)\n    eval_dataset = Dataset.from_pandas(valid)\n    tokenized_train_datasets = dataset.map(prepare_train_features, batched=True, remove_columns=dataset.column_names)\n    tokenized_eval_datasets = eval_dataset.map(prepare_train_features, batched=True, remove_columns=eval_dataset.column_names)\n    \n    train_loader = DataLoader(tokenized_train_datasets,batch_size=config['batch_size'])\n    valid_loader = DataLoader(tokenized_eval_datasets,batch_size=config['batch_size'])\n    dataloaders = {'train':train_loader,'valid':valid_loader}\n    \n    return dataloaders","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:38.190469Z","iopub.execute_input":"2021-09-22T02:52:38.190763Z","iopub.status.idle":"2021-09-22T02:52:38.199689Z","shell.execute_reply.started":"2021-09-22T02:52:38.190737Z","shell.execute_reply":"2021-09-22T02:52:38.198843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and eval","metadata":{}},{"cell_type":"code","source":"def train_and_eval(model, dataloaders, criterion, optimizer, filename):\n    val_loss = 10000\n    for i in range(config['epochs']):\n        for j in ['train','valid']:\n            if j=='train':\n                print(f'training start for epoch {i}')\n                model.zero_grad()\n                model.train()\n                optimizer.zero_grad()\n            else:\n                print(f'eval start for epoch {i}')\n                model.eval()\n                \n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            for idx, data in enumerate(tqdm(dataloaders[j])):\n                input_ids, attention_mask = torch.stack(data['input_ids']).cuda(), torch.stack(data['attention_mask']).cuda()\n                start, end = torch.tensor(data['start_positions']).cuda(), torch.tensor(data['end_positions']).cuda()\n#                 input_ids, attention_mask = data['input_ids'], data['attention_mask']\n#                 start, end = data['start_positions'], data['end_positions']\n                start_logits, end_logits = model(\n                    input_ids=input_ids,\n                    attention_mask = attention_mask\n                )\n#                 print(start_logits.shape)\n                # The input is expected to contain raw, unnormalized scores for each class.    \n#                 start_logits = torch.softmax(start_logits, dim=1)#.cpu().detach().numpy()\n#                 end_logits = torch.softmax(end_logits, dim=1)#.cpu().detach().numpy()\n                start_logits = start_logits.permute(1,0)\n                end_logits = end_logits.permute(1,0)\n#                 start_logits = torch.softmax(start_logits, dim=1)\n#                 end_logits = torch.softmax(start_logits, dim=1)\n                loss = criterion(start_logits, end_logits, start, end)\n#                 print(loss.item())\n                epoch_loss+=loss.item()\n                if j =='train':\n#                     print('grad calculated')\n                    loss = loss/config['grad_acc']\n                    loss.backward()\n                    if idx%config['grad_acc']==0:\n#                         print('weights updated')\n                        optimizer.step()\n                        optimizer.zero_grad()\n            if j =='train': \n                optimizer.step()\n                optimizer.zero_grad()\n            avg_epoch_loss = epoch_loss/len(dataloaders[j])\n            print(f'{j} loss:{avg_epoch_loss} for epoch {i}')\n            if j == 'valid':\n                if val_loss>avg_epoch_loss:\n                    print(f'saving model initial val loss:{val_loss} which is now improve to {avg_epoch_loss}')\n                    val_loss = avg_epoch_loss\n                    torch.save(model.state_dict(), filename)\n                else:\n                    print('No improvement')\n    return 'Train val complete'\n            \n#     pass","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:52:53.31548Z","iopub.execute_input":"2021-09-22T02:52:53.315847Z","iopub.status.idle":"2021-09-22T02:52:53.327566Z","shell.execute_reply.started":"2021-09-22T02:52:53.315814Z","shell.execute_reply":"2021-09-22T02:52:53.326742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Start","metadata":{}},{"cell_type":"code","source":"# args = getTrainArgs(model_checkpoint)\nfor fold in range(folds):\n        print(f'Preparing data for fold number {fold}')\n        dataloaders = getData(df, fold)\n        print(f'model {fold} loading ...')\n        model = chaiiModel()\n        model.cuda()\n        criterion = chaii_loss\n        optimizer = AdamW(model.parameters(),lr=config['lr'], weight_decay=config['weight_decay'] )\n        train_and_eval(model, dataloaders, criterion, optimizer, f\"chaii-trained-model-{fold}\")\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n        print(f\"Training fold {fold}\")\n        print(\"----------------------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-22T02:53:00.028361Z","iopub.execute_input":"2021-09-22T02:53:00.028704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}