{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING=\"1\"","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:32:53.789668Z","iopub.execute_input":"2021-10-21T05:32:53.790033Z","iopub.status.idle":"2021-10-21T05:32:53.812654Z","shell.execute_reply.started":"2021-10-21T05:32:53.789959Z","shell.execute_reply":"2021-10-21T05:32:53.811933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ndata_path = '/kaggle/input/data-split/chaii_folds.csv'\ndf = pd.read_csv(data_path)\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-21T05:32:53.814123Z","iopub.execute_input":"2021-10-21T05:32:53.814512Z","iopub.status.idle":"2021-10-21T05:32:54.722705Z","shell.execute_reply.started":"2021-10-21T05:32:53.814474Z","shell.execute_reply":"2021-10-21T05:32:54.722006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AdamW,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:32:54.723887Z","iopub.execute_input":"2021-10-21T05:32:54.724144Z","iopub.status.idle":"2021-10-21T05:33:01.658892Z","shell.execute_reply.started":"2021-10-21T05:32:54.724109Z","shell.execute_reply":"2021-10-21T05:33:01.658013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\nclass Config():\n    #data\n    trainval_path = '/kaggle/input/data-split/chaii_folds.csv'\n    test_path = '/kaggle/input/chaii-hindi-and-tamil-question-answering/test.csv'\n    pretrained_files_path = '/kaggle/input/muril-large-pt/muril-large-cased'\n    # model\n    model_name = pretrained_files_path\n    config_name = pretrained_files_path\n    gradient_accumulation_steps = 2\n\n    # tokenizer\n    tokenizer_name = pretrained_files_path\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    max_seq_length = 384\n    doc_stride = 128\n\n    # train\n    epochs = 20\n    train_batch_size = 4\n    eval_batch_size = 8\n\n    # optimizer\n    optimizer_type = 'AdamW'\n    learning_rate = 1.5e-5\n    weight_decay = 1e-2\n    adam_epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    warmup_ratio = 0.1\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 42\n    \n    #inference\n    pred_topk_logits = 10\n    pred_max_length = 300\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:33:01.661451Z","iopub.execute_input":"2021-10-21T05:33:01.661716Z","iopub.status.idle":"2021-10-21T05:33:02.142318Z","shell.execute_reply.started":"2021-10-21T05:33:01.661681Z","shell.execute_reply":"2021-10-21T05:33:02.141495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChaiiDatamodule(pl.LightningDataModule):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config\n        self.tv_df = pd.read_csv(config.trainval_path)\n        self.test_df = pd.read_csv(config.test_path)\n        \n    def setup(self,val_fold=1,stage=None):\n#         column_names = ['id','context','question','answer_text','answer_start']\n#         self.training_data = self.tv_df[self.tv_df.folds!=val_fold][column_names]\n#         self.validation_data = self.tv_df[self.tv_df.folds==val_fold][column_names]\n        self.testing_data = self.test_df[['id','context','question']]\n        \n#         self.training_dataset = Dataset.from_pandas(self.training_data)\n#         self.training_dataset = self.preprocess(self.training_dataset,self.config.train_batch_size)\n        \n#         self.validation_dataset = Dataset.from_pandas(self.validation_data)\n#         self.validation_dataset= self.preprocess(self.validation_dataset,self.config.eval_batch_size)\n        \n        self.testing_dataset = Dataset.from_pandas(self.testing_data)\n        self.testing_dataset = self.preprocess(self.testing_dataset,self.config.eval_batch_size)\n    \n    def preprocess_(self,example):\n        tokenizer = self.config.tokenizer\n        \n        example[\"question\"] = [q.strip() for q in example[\"question\"]]\n        example[\"context\"] = [c.strip() for c in example[\"context\"]]\n        \n        tokenized_example = tokenizer(\n            example[\"question\"],\n            example[\"context\"],\n            truncation=\"only_second\",\n            max_length=self.config.max_seq_length,\n            stride=self.config.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n        sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n        offset_mapping = tokenized_example[\"offset_mapping\"]\n\n        tokenized_example['id']=[]\n        tokenized_example['start_idx']=[]\n        tokenized_example['end_idx']=[]\n        \n        for i, offsets in enumerate(offset_mapping):\n            input_ids = tokenized_example[\"input_ids\"][i]\n            attention_mask = tokenized_example[\"attention_mask\"][i]\n            \n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            sequence_ids = tokenized_example.sequence_ids(i)\n            sample_index = sample_mapping[i]\n            tokenized_example['id'].append(example['id'][sample_index])\n            \n            if \"answer_text\" not in example:\n                tokenized_example[\"start_idx\"].append(cls_index)\n                tokenized_example[\"end_idx\"].append(cls_index)\n                continue\n                \n            answer_text = example[\"answer_text\"][sample_index]\n            if len(answer_text) == 0:\n                tokenized_example[\"start_idx\"].append(cls_index)\n                tokenized_example[\"end_idx\"].append(cls_index)\n                continue\n            \n            start_char = example[\"answer_start\"][sample_index]\n            end_char = start_char + len(answer_text)\n\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            if offsets[token_start_index][0] > end_char or offsets[token_end_index][1] < start_char:\n                #answer and this span are disjoint -bugfix- why confuse model?\n                tokenized_example[\"start_idx\"].append(cls_index)\n                tokenized_example[\"end_idx\"].append(cls_index)\n                continue\n            \n            #binary search! ffffaaassstttt TODO\n            while token_start_index < len(offsets) and offsets[token_start_index][1] < start_char:\n                token_start_index += 1\n            tokenized_example[\"start_idx\"].append(token_start_index)\n            \n            while token_end_index > token_start_index and offsets[token_end_index][0] > end_char:\n                #assuming : answers are not subwords\n                token_end_index -= 1\n            tokenized_example[\"end_idx\"].append(token_end_index)\n    \n        return tokenized_example\n    \n    def preprocess(self,dataset,batch_size):\n        dataset = dataset.map( \n            self.preprocess_,\n            batched=True,\n            remove_columns=dataset.column_names\n        )\n        dataset.set_format(\n            type='torch',\n            columns=['attention_mask', 'input_ids', 'start_idx', 'end_idx'],\n            output_all_columns=True\n        )\n        return dataset\n    \n    def train_dataloader(self):\n        return DataLoader(self.training_dataset,self.config.train_batch_size)\n    \n    def val_dataloader(self):\n        return DataLoader(self.validation_dataset,self.config.eval_batch_size)\n    \n    def test_dataloader(self): \n        return DataLoader(self.testing_dataset,self.config.eval_batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:33:02.143739Z","iopub.execute_input":"2021-10-21T05:33:02.144042Z","iopub.status.idle":"2021-10-21T05:33:02.164665Z","shell.execute_reply.started":"2021-10-21T05:33:02.144005Z","shell.execute_reply":"2021-10-21T05:33:02.16366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import inf\nclass ChaiiHFmodel(pl.LightningModule):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config\n        self.model_config = AutoConfig.from_pretrained(config.config_name)\n        self.hf_model = AutoModel.from_pretrained(config.model_name,config = self.model_config)\n        self.linear_layer = nn.Linear(self.model_config.hidden_size,2) #hidden dim -> start/end logits\n    \n    def forward(self,input_ids,attention_mask):\n        outputs = self.hf_model(input_ids,attention_mask = attention_mask)\n        se_logits = self.linear_layer(outputs[0])\n        return (x.squeeze(-1) for x in se_logits.split(1,dim=-1))\n    \n    def loss_fn(self, s_h,e_h, s,e):\n        loss = nn.CrossEntropyLoss()\n        s_loss = loss(s_h,s)\n        e_loss = loss(e_h,e)\n        return (s_loss + e_loss)/2\n    \n    def shared_step(self, batch, compute_loss = True):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        s,e = self(input_ids,attention_mask)\n        if compute_loss:\n            return self.loss_fn(s,e,batch['start_idx'],batch['end_idx']),(s,e)\n        else:\n            return (s,e)\n    \n    def training_step(self, batch, batch_idx):\n        loss,_ = self.shared_step(batch)\n        self.log('train_loss',loss)\n        return loss\n    \n    def validation_step(self,batch, batch_idx):\n        loss,(s,e) = self.shared_step(batch)\n        self.log('val_loss',loss)\n        \n        start_index = batch['start_idx'].detach().cpu().tolist()\n        end_index = batch['end_idx'].detach().cpu().tolist()\n        input_ids = batch['input_ids'].detach().cpu().tolist()\n        \n        answers = [self.id_to_string(start_index[b],end_index[b],input_ids[b]) for b in range(len(input_ids))]\n        valid_range = [(-inf,inf) for b in range(len(input_ids))] #correct this TODO\n        \n        predicted_answers = self.postprocess_(s,e,valid_range,batch['input_ids'])\n        ious = [self.jaccard_score(x,y) for x,y in zip(predicted_answers,answers) if len(y)>0]\n        if len(ious)>0:\n            iou = sum(ious)/max(1,len(ious))\n            self.log('val_metric',iou)\n        return loss\n    \n    def setup(self, stage=None):\n        if stage != \"fit\":\n            return\n        # Get dataloader by calling it - train_dataloader() is called after setup() by default\n        train_loader = self.train_dataloader()\n\n        # Calculate total steps\n        tb_size = self.config.train_batch_size #* max(1, self.trainer.gpus)\n        ab_size = self.trainer.accumulate_grad_batches\n        print('Max epochs',self.trainer.max_epochs)\n        self.total_steps = (len(train_loader.dataset) // tb_size) // ab_size\n        print(self.total_steps,'Total size')\n\n\n    def configure_optimizers(self):\n        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n        model = self\n        args = self.config\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n        group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n        group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n        group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in model.hf_model.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n            {'params': [p for n, p in model.hf_model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n            {'params': [p for n, p in model.hf_model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n            {'params': [p for n, p in model.hf_model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n            {'params': [p for n, p in model.hf_model.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n            {'params': [p for n, p in model.hf_model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n            {'params': [p for n, p in model.hf_model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n            {'params': [p for n, p in model.hf_model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n            {'params': [p for n, p in model.named_parameters() if 'hf_model' not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n        ]\n\n#         for n,p in model.named_parameters():\n#             print(n)\n        \n        optimizer = AdamW([p for n,p in self.named_parameters()],lr = self.config.learning_rate, eps = self.config.adam_epsilon)\n#         optimizer = AdamW(optimizer_grouped_parameters, lr=self.config.learning_rate, eps=self.config.adam_epsilon)\n\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=self.total_steps * self.config.warmup_ratio,\n            num_training_steps=self.total_steps,\n        )\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optimizer], [scheduler]\n    \n    def jaccard_score(self, str1, str2):\n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n\n    def id_to_string(self,start_index,end_index,ids):\n        if start_index==end_index and start_index==0: #CLS\n            return \"\"\n        s = self.config.tokenizer.decode(ids[start_index:end_index+1])\n        return s\n    \n    def postprocess_(self, start, end, valid_range, input_ids):\n        start_logits = start.detach().cpu().numpy()\n        end_logits = end.detach().cpu().numpy()\n        n_best_size = self.config.pred_topk_logits\n        \n        start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n        end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n        \n        batch_answers = []\n        for b in range(len(start_indexes)):\n            valid_answers = []\n            for start_index in start_indexes[b]:\n                for end_index in end_indexes[b]:\n                    if start_index > end_index:\n                        continue\n                    if end_index - start_index + 1 > self.config.pred_max_length:\n                        continue\n                    if start_index < valid_range[b][0] or end_index > valid_range[b][1]:\n                        continue\n                     \n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[b][start_index] + end_logits[b][end_index], #<-Heuristic\n                            \"start_index\": start_index,\n                            \"end_index\": end_index,\n                        }\n                    )\n \n            best_answer = {\"text\": \"\", \"score\": 0.0} #dummy\n            if len(valid_answers) > 0:\n                best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n                best_answer['text'] = self.id_to_string(best_answer['start_index'],best_answer['end_index'],input_ids[b])\n                \n            cls_index = input_ids[b].detach().cpu().tolist().index(self.config.tokenizer.cls_token_id)\n            min_null_score = start_logits[b][cls_index] + end_logits[b][cls_index]\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            batch_answers.append(answer)\n        return batch_answers","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:33:02.166623Z","iopub.execute_input":"2021-10-21T05:33:02.166924Z","iopub.status.idle":"2021-10-21T05:33:02.2101Z","shell.execute_reply.started":"2021-10-21T05:33:02.166887Z","shell.execute_reply":"2021-10-21T05:33:02.209378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n\nlr_monitor = LearningRateMonitor(logging_interval='step')\n# Checkpoint\ncheckpoint_callback = ModelCheckpoint(monitor='val_metric',\n                                      save_top_k=1,\n                                      save_last=True,\n                                      save_weights_only=True,\n                                      filename='checkpoint/{epoch:02d}-{val_metric:.4f}-{val_loss:.4f}',\n                                      verbose=False,\n                                      mode='max')\n\n# Earlystopping\nearlystopping = EarlyStopping(monitor='val_metric', patience=5, mode='max')","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:33:02.211284Z","iopub.execute_input":"2021-10-21T05:33:02.212368Z","iopub.status.idle":"2021-10-21T05:33:02.241816Z","shell.execute_reply.started":"2021-10-21T05:33:02.212311Z","shell.execute_reply":"2021-10-21T05:33:02.241185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = Config()\ndatamodule = ChaiiDatamodule(config)\ndatamodule.setup()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:33:02.24305Z","iopub.execute_input":"2021-10-21T05:33:02.24329Z","iopub.status.idle":"2021-10-21T05:33:03.21195Z","shell.execute_reply.started":"2021-10-21T05:33:02.243258Z","shell.execute_reply":"2021-10-21T05:33:03.211204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH ='/kaggle/input/muril-large-train/lightning_logs/version_0/checkpoints/checkpoint/epoch=00-val_metric=0.6430-val_loss=4.0711.ckpt'\nmodel = ChaiiHFmodel.load_from_checkpoint(PATH,config=config)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:33:03.214483Z","iopub.execute_input":"2021-10-21T05:33:03.214719Z","iopub.status.idle":"2021-10-21T05:33:45.937497Z","shell.execute_reply.started":"2021-10-21T05:33:03.214685Z","shell.execute_reply":"2021-10-21T05:33:45.936693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference**","metadata":{}},{"cell_type":"code","source":"model.to('cuda')\nprint(model.device)\nmodel.eval()\nanswers = {}\nwith torch.no_grad():\n    for batch in datamodule.test_dataloader():\n        batch['attention_mask']=batch['attention_mask'].to('cuda')\n        batch['input_ids']=batch['input_ids'].to('cuda')\n        (s,e) = model.shared_step(batch, compute_loss = False)\n        valid_range = [(-inf,inf) for b in range(model.config.eval_batch_size)]\n        predicted_answers = model.postprocess_(s,e,valid_range,batch['input_ids'])\n        for b in range(len(predicted_answers)):\n            id_col = batch['id'][b]\n            if id_col not in answers or answers[id_col]=='':\n                answers[id_col]=predicted_answers[b]","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:33:45.939805Z","iopub.execute_input":"2021-10-21T05:33:45.940029Z","iopub.status.idle":"2021-10-21T05:34:07.07756Z","shell.execute_reply.started":"2021-10-21T05:33:45.940003Z","shell.execute_reply":"2021-10-21T05:34:07.07657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame\\\n            .from_dict(answers,orient='index',columns=['PredictionString'])\\\n            .reset_index()\\\n            .rename(columns={'index': 'id'})","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:34:07.07877Z","iopub.execute_input":"2021-10-21T05:34:07.079047Z","iopub.status.idle":"2021-10-21T05:34:07.087559Z","shell.execute_reply.started":"2021-10-21T05:34:07.079008Z","shell.execute_reply":"2021-10-21T05:34:07.086792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:34:07.088945Z","iopub.execute_input":"2021-10-21T05:34:07.089379Z","iopub.status.idle":"2021-10-21T05:34:07.101825Z","shell.execute_reply.started":"2021-10-21T05:34:07.089323Z","shell.execute_reply":"2021-10-21T05:34:07.10105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:34:07.103464Z","iopub.execute_input":"2021-10-21T05:34:07.104071Z","iopub.status.idle":"2021-10-21T05:34:07.113538Z","shell.execute_reply.started":"2021-10-21T05:34:07.103959Z","shell.execute_reply":"2021-10-21T05:34:07.112718Z"},"trusted":true},"execution_count":null,"outputs":[]}]}