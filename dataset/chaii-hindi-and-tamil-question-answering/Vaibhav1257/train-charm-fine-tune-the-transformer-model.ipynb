{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **fine-tune the transformer model for our use case**\n","metadata":{}},{"cell_type":"markdown","source":"\n\n### In this notebook we will demostrate the process of fine tuneing the pretrained hugging faces transformer model, I hope this notebook will help to understand the use of pretrained transformer model in the context of this compitation.\n\n### In my previous notebook([here](https://www.kaggle.com/vaibhavrmankar/simple-start-eda-submission)) I have done the EDA, if you are new in the competition you might wanna look into the notebook for a the understanding of the problem statement and given data.","metadata":{}},{"cell_type":"markdown","source":"### This notebook is build using the example notebook provided by hugging faces.\n### Ref :[here](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n\n","metadata":{}},{"cell_type":"markdown","source":"## SETUP","metadata":{}},{"cell_type":"markdown","source":"### we need to install the huggingface Datasets.","metadata":{}},{"cell_type":"code","source":"! pip install datasets transformers","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:18.102403Z","iopub.execute_input":"2021-09-18T11:15:18.10293Z","iopub.status.idle":"2021-09-18T11:15:26.752665Z","shell.execute_reply.started":"2021-09-18T11:15:18.102831Z","shell.execute_reply":"2021-09-18T11:15:26.75135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:26.763847Z","iopub.execute_input":"2021-09-18T11:15:26.764308Z","iopub.status.idle":"2021-09-18T11:15:28.49127Z","shell.execute_reply.started":"2021-09-18T11:15:26.764267Z","shell.execute_reply":"2021-09-18T11:15:28.490141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note: if you get the error message after running the following cell, please restart the kernel.\n","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:28.494832Z","iopub.execute_input":"2021-09-18T11:15:28.495172Z","iopub.status.idle":"2021-09-18T11:15:29.388559Z","shell.execute_reply.started":"2021-09-18T11:15:28.495143Z","shell.execute_reply":"2021-09-18T11:15:29.387052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import default_data_collator\nfrom transformers import BertTokenizer, pipeline","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:29.390332Z","iopub.execute_input":"2021-09-18T11:15:29.390781Z","iopub.status.idle":"2021-09-18T11:15:35.582144Z","shell.execute_reply.started":"2021-09-18T11:15:29.39074Z","shell.execute_reply":"2021-09-18T11:15:35.581046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:35.58484Z","iopub.execute_input":"2021-09-18T11:15:35.58517Z","iopub.status.idle":"2021-09-18T11:15:35.608343Z","shell.execute_reply.started":"2021-09-18T11:15:35.585139Z","shell.execute_reply":"2021-09-18T11:15:35.607292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Which Question answering model to fine-tune ? ","metadata":{}},{"cell_type":"markdown","source":"### There are several question-answering models to think of, We can try out different models and compare the results.\n","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"mrm8488/bert-tiny-5-finetuned-squadv2\"\nbatch_size = 4\n\nmax_length = 384 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:35.611825Z","iopub.execute_input":"2021-09-18T11:15:35.612155Z","iopub.status.idle":"2021-09-18T11:15:35.619748Z","shell.execute_reply.started":"2021-09-18T11:15:35.612096Z","shell.execute_reply":"2021-09-18T11:15:35.618562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## load the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv').sample(frac = 1)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:35.623562Z","iopub.execute_input":"2021-09-18T11:15:35.623912Z","iopub.status.idle":"2021-09-18T11:15:36.399482Z","shell.execute_reply.started":"2021-09-18T11:15:35.623883Z","shell.execute_reply":"2021-09-18T11:15:36.398277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As we know that there are a lot of data points of Hindi language and fewer data points for Tamil, here we will be using equal no. of data points for both the languages.\n","metadata":{}},{"cell_type":"code","source":"df_hindi = df[df['language'] == 'hindi'].head(368)\ndf_hindi","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:36.403718Z","iopub.execute_input":"2021-09-18T11:15:36.404076Z","iopub.status.idle":"2021-09-18T11:15:36.434129Z","shell.execute_reply.started":"2021-09-18T11:15:36.40403Z","shell.execute_reply":"2021-09-18T11:15:36.432653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tamil = df[df['language'] == 'tamil']\ndf_tamil","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:36.437169Z","iopub.execute_input":"2021-09-18T11:15:36.437746Z","iopub.status.idle":"2021-09-18T11:15:36.476846Z","shell.execute_reply.started":"2021-09-18T11:15:36.437652Z","shell.execute_reply":"2021-09-18T11:15:36.474744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df_hindi,df_tamil])\ndf","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:36.479782Z","iopub.execute_input":"2021-09-18T11:15:36.480322Z","iopub.status.idle":"2021-09-18T11:15:36.522043Z","shell.execute_reply.started":"2021-09-18T11:15:36.480278Z","shell.execute_reply":"2021-09-18T11:15:36.518869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### In order to use the given data set we need to convert the pandas data frame into hugging faces dataset object, which is done by the convert_to_dataset function.\n* loop through the dataset \n* represent the datapoint in the required format ( data dict )\n* convert the data dict to hugging faces dataset object\n","metadata":{}},{"cell_type":"code","source":"\ndef convert_to_dataset(DF): \n    data = {'answers':[],'context':[],'id':[],'question':[],'title':[]}\n\n    for i in range(len(DF)):\n\n        row = DF.iloc[i]\n        data['answers'].append({'answer_start': [row['answer_start']], 'text': [row['answer_text']]})\n        data['context'].append(row['context'])\n        data['id'].append(row['id'])\n        data['question'].append(row['question'])\n        data['title'].append('NA')\n\n\n    dataset = Dataset.from_dict(data)\n\n    return dataset\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:36.52435Z","iopub.execute_input":"2021-09-18T11:15:36.525638Z","iopub.status.idle":"2021-09-18T11:15:36.547556Z","shell.execute_reply.started":"2021-09-18T11:15:36.525585Z","shell.execute_reply":"2021-09-18T11:15:36.544023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split the data into train and test.","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(df, test_size=0.1)\n\ntrain_dataset = convert_to_dataset(train)\ntest_dataset = convert_to_dataset(test)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:36.549096Z","iopub.execute_input":"2021-09-18T11:15:36.549514Z","iopub.status.idle":"2021-09-18T11:15:36.911503Z","shell.execute_reply.started":"2021-09-18T11:15:36.549475Z","shell.execute_reply":"2021-09-18T11:15:36.910235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess data","metadata":{}},{"cell_type":"markdown","source":"### data preprocessing is the important step which includes the use of a tokenizer from the pre-trained model.\n\n### To do this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which ensures:\n\n* we get a tokenizer that corresponds to the model architecture we want to use,\n* we download the vocabulary used when pretraining this specific checkpoint.\n \n","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:36.914228Z","iopub.execute_input":"2021-09-18T11:15:36.914743Z","iopub.status.idle":"2021-09-18T11:15:43.661738Z","shell.execute_reply.started":"2021-09-18T11:15:36.914697Z","shell.execute_reply":"2021-09-18T11:15:43.660316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pad_on_right = tokenizer.padding_side == \"right\"","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:43.663363Z","iopub.execute_input":"2021-09-18T11:15:43.663839Z","iopub.status.idle":"2021-09-18T11:15:43.671846Z","shell.execute_reply.started":"2021-09-18T11:15:43.663797Z","shell.execute_reply":"2021-09-18T11:15:43.669216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:43.674351Z","iopub.execute_input":"2021-09-18T11:15:43.675324Z","iopub.status.idle":"2021-09-18T11:15:43.692478Z","shell.execute_reply.started":"2021-09-18T11:15:43.675278Z","shell.execute_reply":"2021-09-18T11:15:43.69082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_tokenized_dataset = train_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:15:43.69517Z","iopub.execute_input":"2021-09-18T11:15:43.696046Z","iopub.status.idle":"2021-09-18T11:16:00.335106Z","shell.execute_reply.started":"2021-09-18T11:15:43.695901Z","shell.execute_reply":"2021-09-18T11:16:00.333804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_tokenized_dataset = test_dataset.map(prepare_train_features, batched=True, remove_columns=test_dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:16:00.336801Z","iopub.execute_input":"2021-09-18T11:16:00.337222Z","iopub.status.idle":"2021-09-18T11:16:02.058122Z","shell.execute_reply.started":"2021-09-18T11:16:00.337181Z","shell.execute_reply":"2021-09-18T11:16:02.056277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we have prepared our data for the given task. We can start fine-tuning the model.\n","metadata":{}},{"cell_type":"markdown","source":"## Fine-tuning the model","metadata":{}},{"cell_type":"markdown","source":"### We first load the model using AutoModelForQuestionAnswering.from_pretrained function and then we fine-tune the model on our data.\n","metadata":{}},{"cell_type":"code","source":"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:16:02.063896Z","iopub.execute_input":"2021-09-18T11:16:02.064367Z","iopub.status.idle":"2021-09-18T11:16:04.985578Z","shell.execute_reply.started":"2021-09-18T11:16:02.064323Z","shell.execute_reply":"2021-09-18T11:16:04.984276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = model_checkpoint.split(\"/\")[-1]\n\nargs = TrainingArguments(\n    f\"test-squad\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=1, \n    weight_decay=0.01,\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:16:04.987578Z","iopub.execute_input":"2021-09-18T11:16:04.988086Z","iopub.status.idle":"2021-09-18T11:16:05.053495Z","shell.execute_reply.started":"2021-09-18T11:16:04.988025Z","shell.execute_reply":"2021-09-18T11:16:05.052498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We use the default data collector to batch our processed examples together.\n","metadata":{}},{"cell_type":"code","source":"data_collator = default_data_collator","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:16:05.054987Z","iopub.execute_input":"2021-09-18T11:16:05.055604Z","iopub.status.idle":"2021-09-18T11:16:05.060608Z","shell.execute_reply.started":"2021-09-18T11:16:05.055544Z","shell.execute_reply":"2021-09-18T11:16:05.059081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We use the trainer for model training.","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=train_tokenized_dataset,\n    eval_dataset=train_tokenized_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:16:05.062494Z","iopub.execute_input":"2021-09-18T11:16:05.063234Z","iopub.status.idle":"2021-09-18T11:16:12.073586Z","shell.execute_reply.started":"2021-09-18T11:16:05.063159Z","shell.execute_reply":"2021-09-18T11:16:12.072241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:16:12.075736Z","iopub.execute_input":"2021-09-18T11:16:12.076184Z","iopub.status.idle":"2021-09-18T11:27:07.328114Z","shell.execute_reply.started":"2021-09-18T11:16:12.076136Z","shell.execute_reply":"2021-09-18T11:27:07.32638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Once we get to good enough accuracy we save the model for future use ","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"test-squad-trained\")","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:27:20.545967Z","iopub.execute_input":"2021-09-18T11:27:20.546442Z","iopub.status.idle":"2021-09-18T11:27:20.722017Z","shell.execute_reply.started":"2021-09-18T11:27:20.546406Z","shell.execute_reply":"2021-09-18T11:27:20.720816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We need to chnage the file structure, for esly loading the model and tokenizer.","metadata":{}},{"cell_type":"code","source":"!mkdir tokenizer \n!mkdir model","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:27:21.471173Z","iopub.execute_input":"2021-09-18T11:27:21.471598Z","iopub.status.idle":"2021-09-18T11:27:23.204378Z","shell.execute_reply.started":"2021-09-18T11:27:21.471566Z","shell.execute_reply":"2021-09-18T11:27:23.202961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil  \nshutil.move('./test-squad-trained/config.json','./model/config.json')\nshutil.move('./test-squad-trained/pytorch_model.bin','./model/pytorch_model.bin')","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:27:23.208417Z","iopub.execute_input":"2021-09-18T11:27:23.208892Z","iopub.status.idle":"2021-09-18T11:27:23.222327Z","shell.execute_reply.started":"2021-09-18T11:27:23.20885Z","shell.execute_reply":"2021-09-18T11:27:23.220959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nos.rename('./test-squad-trained','./tokenizer')","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:27:23.225279Z","iopub.execute_input":"2021-09-18T11:27:23.22607Z","iopub.status.idle":"2021-09-18T11:27:23.233335Z","shell.execute_reply.started":"2021-09-18T11:27:23.226016Z","shell.execute_reply":"2021-09-18T11:27:23.23177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Saved model can be used offline for the submission.\n","metadata":{}},{"cell_type":"markdown","source":"## Use fine-tuned model for Submission \n","metadata":{}},{"cell_type":"markdown","source":"### After we have saved the model we need to use the model for generating the output.","metadata":{}},{"cell_type":"markdown","source":"### Load the saved model useing from_pretrained method.","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"./tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:27:26.15156Z","iopub.execute_input":"2021-09-18T11:27:26.152122Z","iopub.status.idle":"2021-09-18T11:27:26.215359Z","shell.execute_reply.started":"2021-09-18T11:27:26.152071Z","shell.execute_reply":"2021-09-18T11:27:26.214111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained(\"./model\")","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:27:26.530517Z","iopub.execute_input":"2021-09-18T11:27:26.530931Z","iopub.status.idle":"2021-09-18T11:27:26.720138Z","shell.execute_reply.started":"2021-09-18T11:27:26.530896Z","shell.execute_reply":"2021-09-18T11:27:26.71887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup the question-answering pipeline.","metadata":{}},{"cell_type":"code","source":"nlp = pipeline('question-answering', model=model, tokenizer=tokenizer,device = 0)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:27:27.260598Z","iopub.execute_input":"2021-09-18T11:27:27.261036Z","iopub.status.idle":"2021-09-18T11:27:27.430383Z","shell.execute_reply.started":"2021-09-18T11:27:27.260973Z","shell.execute_reply":"2021-09-18T11:27:27.429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Iterate through the submission dataset and add the predictions.\n","metadata":{}},{"cell_type":"code","source":"\ndata = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nSUB = pd.DataFrame(columns = ['id','PredictionString'])\n\nfor id_,C,Q,lan in data[[\"id\",\"context\", \"question\",\"language\"]].to_numpy():\n    \n    result = nlp(context=C, question=Q)    \n    SUB.loc[len(SUB.index)] = [id_,result['answer']] \n    \nSUB","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:27:29.855823Z","iopub.execute_input":"2021-09-18T11:27:29.856271Z","iopub.status.idle":"2021-09-18T11:28:02.427712Z","shell.execute_reply.started":"2021-09-18T11:27:29.856236Z","shell.execute_reply":"2021-09-18T11:28:02.426356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SUB.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T11:28:02.430345Z","iopub.execute_input":"2021-09-18T11:28:02.430874Z","iopub.status.idle":"2021-09-18T11:28:02.444885Z","shell.execute_reply.started":"2021-09-18T11:28:02.430818Z","shell.execute_reply":"2021-09-18T11:28:02.443454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thank you for reading, Happy to hear any thoughts/suggestions :) \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}