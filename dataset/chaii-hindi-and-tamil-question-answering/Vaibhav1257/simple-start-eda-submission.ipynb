{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Let’s Begin\n\n#### This notebook is aimed to give an end-to-end review of the problem statement and simple solution.\n#### This problem is related to the standard extractive question answering problem( i.e extracting an answer from a text given a question ).\n\n","metadata":{}},{"cell_type":"markdown","source":"\n## If it's just a standard extractive question answering, why there is a competition?\n\n#### While computers are getting stronger at understanding natural languages, computers are still not capable of completely understanding all the languages there.\n\n#### To build a strong natural language understanding model, along with a good knowledge of related filed one needs to generate a lot of data specific to that language ( or need to extract data from the huge amount of unstructured data available out there on the internet ).\n\n#### This is a research competition, which means a lot of research will be required not only in designing the powerful algorithms but also in gathering the relevant data ( language-specific )\n","metadata":{}},{"cell_type":"markdown","source":"# Problem statemnt \n\n#### Given a text ( eg. history of india ) and a Question ( How is the father of india ? ) find the answer from the text. \n\n#### well finding answer does not mean dedusding the answer, but it means that the answer lies in the given text we just have to find the start and end point of the answer from the text.\n\n#### EG. ( 100s of words ) ........ The Father of India is Mahatma Gandhi and he ........ ( 100s of words ) . \n\n#### As we are extracting the answer from the context ( given text ) it's called Extractive question answering.\n\n#### Here every thing is same just the difference is, Language is not english but it is eigther hindi or tamil.\n\n#### To solve such problems machine needs to develop good understanding of the language. \n","metadata":{}},{"cell_type":"markdown","source":"# EDA\n","metadata":{}},{"cell_type":"markdown","source":"## Lets have a look at the given data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nDF = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n\nDF.sample(frac=1).head()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:27:46.126398Z","iopub.execute_input":"2021-08-18T06:27:46.126744Z","iopub.status.idle":"2021-08-18T06:27:46.513487Z","shell.execute_reply.started":"2021-08-18T06:27:46.126711Z","shell.execute_reply":"2021-08-18T06:27:46.512642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Context, question, and answer_text are understandable but what is answer_start? \n\n#### answer_start defines the index of the character from where the answer begins ( this helps in the training  of the model ) \n\n#### language identifies the type of language being used \n","metadata":{}},{"cell_type":"markdown","source":"##  Let’s see the dataset division for Hindi and Tamil ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:27:46.575912Z","iopub.execute_input":"2021-08-18T06:27:46.576175Z","iopub.status.idle":"2021-08-18T06:27:46.58068Z","shell.execute_reply.started":"2021-08-18T06:27:46.576149Z","shell.execute_reply":"2021-08-18T06:27:46.579713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hindi_count = 0\ntamil_count = 0\n\nfor l in DF['language']:\n    if l == 'hindi': hindi_count+=1\n    else : tamil_count+=1\n\nlanguages  = ['Hindi', 'Tamil']\n    \ndata = [hindi_count,tamil_count]\n\nfig = plt.figure(figsize =(12, 9))\nplt.pie(data, labels = languages)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:27:46.670935Z","iopub.execute_input":"2021-08-18T06:27:46.671224Z","iopub.status.idle":"2021-08-18T06:27:46.76218Z","shell.execute_reply.started":"2021-08-18T06:27:46.671196Z","shell.execute_reply":"2021-08-18T06:27:46.761251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It looks like we have a lot of data points for Hindi, but relatively fewer data points for Tamil \n#### ( we might need to add some data points for Tamil )\n","metadata":{}},{"cell_type":"markdown","source":"## Let’s have a closer look at one of a data point/example","metadata":{}},{"cell_type":"code","source":"DF.iloc[700]['context']","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:27:47.136228Z","iopub.execute_input":"2021-08-18T06:27:47.136563Z","iopub.status.idle":"2021-08-18T06:27:47.145955Z","shell.execute_reply.started":"2021-08-18T06:27:47.136533Z","shell.execute_reply":"2021-08-18T06:27:47.144927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DF.iloc[700]['question']","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:27:47.235728Z","iopub.execute_input":"2021-08-18T06:27:47.236026Z","iopub.status.idle":"2021-08-18T06:27:47.242094Z","shell.execute_reply.started":"2021-08-18T06:27:47.235999Z","shell.execute_reply":"2021-08-18T06:27:47.241199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DF.iloc[700]['answer_text']","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:27:47.405972Z","iopub.execute_input":"2021-08-18T06:27:47.406274Z","iopub.status.idle":"2021-08-18T06:27:47.413225Z","shell.execute_reply.started":"2021-08-18T06:27:47.406247Z","shell.execute_reply":"2021-08-18T06:27:47.412066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This must-have cleared some confusion ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let’s find out more about the given data","metadata":{}},{"cell_type":"markdown","source":"#### In the above example, we saw that the context is quite large ( and  it's not even the largest )\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Does context size matter?\n\n#### The answer is YES,\n#### For an extractive question answering model, context and question is the input to the model and the model has to predict the output based on this, But high context size means high input size which means high processing power is required to process a huge input, that impacts the training/inference time","metadata":{}},{"cell_type":"markdown","source":"## Let’s have a look at the context size","metadata":{}},{"cell_type":"code","source":"\ndef plot_bar_graph(Values,Labels,color):\n    \n    fig = plt.figure(figsize = (50, 20))\n\n    X = Values[0]\n    Y = Values[1]\n\n    plt.bar(X,Y,color = color,width = 0.4)\n\n    plt.xlabel(Labels[0])\n    plt.ylabel(Labels[1])\n    plt.title(Labels[2])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:27:48.406072Z","iopub.execute_input":"2021-08-18T06:27:48.406394Z","iopub.status.idle":"2021-08-18T06:27:48.413833Z","shell.execute_reply.started":"2021-08-18T06:27:48.406364Z","shell.execute_reply":"2021-08-18T06:27:48.412633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ploting the context len\n\nlen_context = [len(c) for c in DF['context']]\nIds = [id_ for id_ in DF['id']]\n\nX_label = \"Ids\"\nY_label = \"Length\"\nLabel = \"Context Length \"\n\nValues = [Ids,len_context]\nLabels = [X_label,Y_label,Label]\n\nplot_bar_graph(Values,Labels,'maroon')","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:27:48.571458Z","iopub.execute_input":"2021-08-18T06:27:48.571985Z","iopub.status.idle":"2021-08-18T06:28:02.217711Z","shell.execute_reply.started":"2021-08-18T06:27:48.571944Z","shell.execute_reply":"2021-08-18T06:28:02.216926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### We can see that the context length can reach up to 50,000 ( and that's a large value for a single data point )\n    ","metadata":{}},{"cell_type":"markdown","source":"## How to solve this problem?\n\n#### There are several ways we can tackle this problem (a few are listed below)\n\n* #### Remove the data points with very large context length ( Data loss )\n* #### Use the Sliding windows approach ( divide the context in relatively small parts and use them instead)\n* #### Different Sliding windows\n    * #### Instead of dividing the context into fixed parts we just move over a fixed number of sentences.\n* #### ...\n \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Well looking at this one might ask\n\n","metadata":{}},{"cell_type":"markdown","source":"## Does answer size matter?\n\n#### well the answer is Yes and No\n\n#### Answer is the output of the model but we do not need to predict the entire answer instead we just predict the start and endpoint of the answer in the given text\n\n#### i.e the size of the output vector does not depend on the answer length, rather it's fixed.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Let’s have a closer look to find out more","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ploting the answer len\n\nlen_context = [len(c) for c in DF['answer_text']]\nIds = [id_ for id_ in DF['id']]\n\nX_label = \"Ids\"\nY_label = \"Length\"\nLabel = \"Answer Length \"\n\nValues = [Ids,len_context]\nLabels = [X_label,Y_label,Label]\n\nplot_bar_graph(Values,Labels,'blue')","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:28:02.219199Z","iopub.execute_input":"2021-08-18T06:28:02.219555Z","iopub.status.idle":"2021-08-18T06:28:16.413187Z","shell.execute_reply.started":"2021-08-18T06:28:02.21952Z","shell.execute_reply":"2021-08-18T06:28:16.411603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The above graph clearly shows that there are some very long answers, \n#### at the beginning of the model development, we might want to remove those few data points with large answer text.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There is still a lot to explore not only with the given data but also with the data out there on the internet, Research continues. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A simple way to generate output ( This will give some intuition about the problem )","metadata":{}},{"cell_type":"markdown","source":"## Using Pre-trained model provided by hugging faces\n\n\n\n\n#### In this part we will use the pre-trained model provided by the Huggeging faces, this will give us a good idea of how things flow in the extractive question answering model.\n\n#### (this knowledge will help us when we are developing the model from scratch )\n","metadata":{}},{"cell_type":"code","source":"# import \nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:28:16.414758Z","iopub.execute_input":"2021-08-18T06:28:16.415009Z","iopub.status.idle":"2021-08-18T06:28:16.418484Z","shell.execute_reply.started":"2021-08-18T06:28:16.414985Z","shell.execute_reply":"2021-08-18T06:28:16.417517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"deepset/xlm-roberta-large-squad2\")\n\n# define model\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"deepset/xlm-roberta-large-squad2\")","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:28:16.420474Z","iopub.execute_input":"2021-08-18T06:28:16.420857Z","iopub.status.idle":"2021-08-18T06:29:38.811814Z","shell.execute_reply.started":"2021-08-18T06:28:16.420815Z","shell.execute_reply":"2021-08-18T06:29:38.810885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* #### Tokenizer converts the input text into tensor (vector storing numbers )\n* #### this tensor goes into the model, \n* #### the model generates the prediction tensor \n* #### Again the output gets converted into answer text using tokenizer\n\n#### All this process is raped into a pipeline ( in some sense )\n","metadata":{}},{"cell_type":"code","source":"# define pipeline \n\nQA_pipeline = pipeline('question-answering', model = model, tokenizer = tokenizer, device = 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:29:38.81311Z","iopub.execute_input":"2021-08-18T06:29:38.813555Z","iopub.status.idle":"2021-08-18T06:29:44.6509Z","shell.execute_reply.started":"2021-08-18T06:29:38.813519Z","shell.execute_reply":"2021-08-18T06:29:44.650031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## let’s have a look over test.csv and subbmition.csv","metadata":{}},{"cell_type":"code","source":"td  = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\ntd","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:29:44.65223Z","iopub.execute_input":"2021-08-18T06:29:44.652559Z","iopub.status.idle":"2021-08-18T06:29:44.67587Z","shell.execute_reply.started":"2021-08-18T06:29:44.652524Z","shell.execute_reply":"2021-08-18T06:29:44.675127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sd = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\nsd","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:29:44.676945Z","iopub.execute_input":"2021-08-18T06:29:44.677259Z","iopub.status.idle":"2021-08-18T06:29:44.6949Z","shell.execute_reply.started":"2021-08-18T06:29:44.677226Z","shell.execute_reply":"2021-08-18T06:29:44.694152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can see that there are some context and questions in the test.csv for which we have to generate the answer text and put it in the sumbbmition.csv\n","metadata":{}},{"cell_type":"code","source":"submission_data = pd.DataFrame(columns = sd.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:29:44.696888Z","iopub.execute_input":"2021-08-18T06:29:44.69722Z","iopub.status.idle":"2021-08-18T06:29:44.70434Z","shell.execute_reply.started":"2021-08-18T06:29:44.697187Z","shell.execute_reply":"2021-08-18T06:29:44.70345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for id_, c, q in td[[\"id\",\"context\", \"question\"]].to_numpy():\n    \n    Output = QA_pipeline(context=c, question=q)\n    submission_data.loc[len(submission_data.index)] = [id_,Output['answer']] ","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:29:44.705748Z","iopub.execute_input":"2021-08-18T06:29:44.706242Z","iopub.status.idle":"2021-08-18T06:29:50.737011Z","shell.execute_reply.started":"2021-08-18T06:29:44.706199Z","shell.execute_reply":"2021-08-18T06:29:50.73617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:29:50.738179Z","iopub.execute_input":"2021-08-18T06:29:50.738553Z","iopub.status.idle":"2021-08-18T06:29:50.751093Z","shell.execute_reply.started":"2021-08-18T06:29:50.738519Z","shell.execute_reply":"2021-08-18T06:29:50.750127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the submission_data","metadata":{}},{"cell_type":"code","source":"submission_data.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T06:29:50.752376Z","iopub.execute_input":"2021-08-18T06:29:50.75294Z","iopub.status.idle":"2021-08-18T06:29:51.805839Z","shell.execute_reply.started":"2021-08-18T06:29:50.752901Z","shell.execute_reply":"2021-08-18T06:29:51.804902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n### Happy to hear your thoughts / Suggestions on this notebook :)\n\n### Thank you for readings ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}