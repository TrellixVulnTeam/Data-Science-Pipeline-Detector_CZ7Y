{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport math\nimport numpy as np\nimport pandas as pd \nimport random\nimport os\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Sampler, Dataset, DataLoader\nimport torch.nn.init as init\nfrom torch.nn import Parameter\nfrom torch.autograd.function import InplaceFunction\n\nfrom transformers import AutoConfig\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nimport shutil\nimport collections\n\ndevice = torch.device(\"cuda\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-23T22:55:34.447499Z","iopub.execute_input":"2021-08-23T22:55:34.447839Z","iopub.status.idle":"2021-08-23T22:55:41.16557Z","shell.execute_reply.started":"2021-08-23T22:55:34.447767Z","shell.execute_reply":"2021-08-23T22:55:41.164749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    # PATH\n    INPUT_DIR = \"/kaggle/input/chaii-hindi-and-tamil-question-answering\"\n    MODEL_NAME = \"/kaggle/input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2\"\n    HINDI_MODEL_PATH = '/kaggle/input/chaii-train-with-divided-hindi-and-tamil/exp0016_hindi/chaii_f0_best_jaccard_model.bin'\n    TAMIL_MODEL_PATH = '/kaggle/input/chaii-train-with-divided-hindi-and-tamil/exp0016_tamil/chaii_f0_best_jaccard_model.bin'\n    # parameter\n    SEED = 0\n    HIDDEN_DROPOUT_PROB = 0.1\n    LAYER_NORM_EPS = 1e-5\n    # tokenizer\n    TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)\n    NOT_WATCH_PARAM = [\"NOT_WATCH_PARAM\", \"TOKENIZER\", \"INPUT_DIR\", \"OUTPUT_DIR\"]\n    MAX_LEN = 384\n    STRIDE = 128\n    MAX_ANSWER_LEN = 30\n    CONTENT_ID= 1\n    N_BEST = 20","metadata":{"execution":{"iopub.status.busy":"2021-08-23T22:59:23.560339Z","iopub.execute_input":"2021-08-23T22:59:23.56069Z","iopub.status.idle":"2021-08-23T22:59:24.180509Z","shell.execute_reply.started":"2021-08-23T22:59:23.560647Z","shell.execute_reply":"2021-08-23T22:59:24.179688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2021-08-23T22:56:56.335913Z","iopub.execute_input":"2021-08-23T22:56:56.336237Z","iopub.status.idle":"2021-08-23T22:56:56.341977Z","shell.execute_reply.started":"2021-08-23T22:56:56.336207Z","shell.execute_reply":"2021-08-23T22:56:56.340676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cf: https://www.kaggle.com/thedrcat/chaii-eda-baseline\ndef encode_train_example(example):\n    question = example['question'].lstrip()\n    context = example['context']\n\n    tokenized_examples = config.TOKENIZER(\n            question,\n            context,\n            truncation=\"only_second\",\n            max_length=config.MAX_LEN,\n            stride=config.STRIDE,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples[\"offset_mapping\"]\n\n    tokenized_examples['sequence_ids'] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(config.TOKENIZER.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        tokenized_examples['sequence_ids'].append(sequence_ids)\n\n    return tokenized_examples\n\n\ndef extract_feature_example(df):\n    tokenized_examples = {\n        'input_ids': [],\n        'attention_mask': [],\n        'token_type_ids': [],\n        'offset_mapping': [],\n        'sequence_ids': [],\n        'example_id': [],\n    }\n    for _, row in df.iterrows():\n        tokenized_example = encode_train_example(row)\n        tokenized_examples['input_ids'].extend(tokenized_example['input_ids'])\n        tokenized_examples['attention_mask'].extend(tokenized_example['attention_mask'])\n        tokenized_examples['token_type_ids'].extend(tokenized_example['token_type_ids'])\n        tokenized_examples['offset_mapping'].extend(tokenized_example['offset_mapping'])\n        tokenized_examples['sequence_ids'].extend(tokenized_example['sequence_ids'])\n        tokenized_examples['example_id'].extend([row['id'] for _ in range(len(tokenized_example['input_ids']))])\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-08-23T22:56:56.809058Z","iopub.execute_input":"2021-08-23T22:56:56.809372Z","iopub.status.idle":"2021-08-23T22:56:56.820848Z","shell.execute_reply.started":"2021-08-23T22:56:56.809344Z","shell.execute_reply":"2021-08-23T22:56:56.819893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChAIIDataset(Dataset):\n    \n    def __init__(self, df, train):\n        self.feature_examples = extract_feature_example(df)\n        self.train = train\n        \n    def __len__(self):\n        return len(self.feature_examples['input_ids'])\n    \n    def __getitem__(self, item):\n        d = {\n            'input_ids': torch.tensor(self.feature_examples['input_ids'][item]),\n            'attention_mask': torch.tensor(self.feature_examples['attention_mask'][item]),\n            'token_type_ids': torch.tensor(self.feature_examples['token_type_ids'][item]),\n            'offset_mapping': torch.tensor(self.feature_examples['offset_mapping'][item]),\n            'example_id': self.feature_examples['example_id'][item],\n        }\n        if not self.train:\n            d['sequence_ids'] = self.feature_examples['sequence_ids'][item]\n        return d\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T22:57:00.476117Z","iopub.execute_input":"2021-08-23T22:57:00.476443Z","iopub.status.idle":"2021-08-23T22:57:00.485629Z","shell.execute_reply.started":"2021-08-23T22:57:00.476414Z","shell.execute_reply":"2021-08-23T22:57:00.484716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_test(models, dset, test_df):\n    all_logits, losses = [], []\n    for d in dset:\n        example_id = d['example_id']\n        lang = test_df.query(f'id==\"{example_id}\"')['language'].iloc[0]\n        model = models[lang]\n        model.eval()\n        with torch.no_grad():\n            outputs = model(\n                d['input_ids'].unsqueeze(0).to(device),\n                d['attention_mask'].unsqueeze(0).to(device),\n                #d['token_type_ids'].unsqueeze(0).to(device),\n            )\n        start_logits = outputs['start_logits'].cpu()\n        end_logits = outputs['end_logits'].cpu()\n        all_logits.append((start_logits, end_logits))\n    return all_logits","metadata":{"execution":{"iopub.status.busy":"2021-08-23T23:04:38.379373Z","iopub.execute_input":"2021-08-23T23:04:38.379706Z","iopub.status.idle":"2021-08-23T23:04:38.386746Z","shell.execute_reply.started":"2021-08-23T23:04:38.379674Z","shell.execute_reply":"2021-08-23T23:04:38.385779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_processing(all_logits, train_df, dset):\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(dset):\n        features_per_example[feature[\"example_id\"]].append(i)\n    \n    predicts = []\n    for example_id, feature_indices in features_per_example.items():\n        row = train_df.query(f'id==\"{example_id}\"').iloc[0]\n        context = row['context']\n        predict_answers = []\n        for feature_index in feature_indices:\n            assert dset.feature_examples['example_id'][feature_index] == example_id\n            \n            start_logits, end_logits = all_logits[feature_index]\n            offset_mapping = dset.feature_examples[\"offset_mapping\"][feature_index]\n            sequence_ids = dset.feature_examples[\"sequence_ids\"][feature_index]\n            offset_mapping = [o if i == config.CONTENT_ID else None for i, o in zip(sequence_ids, offset_mapping)]\n            \n            start_indexes = np.argsort(start_logits[0].numpy())[-config.N_BEST:]\n            end_indexes = np.argsort(end_logits[0].numpy())[-config.N_BEST:]\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if offset_mapping[start_index] is None or offset_mapping[end_index] is None:\n                        continue\n                    if len(offset_mapping) <= start_index or len(offset_mapping) <= end_index:\n                        continue\n                    if start_index > end_index or (end_index - start_index) > config.MAX_ANSWER_LEN:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n\n                    text = context[start_char:end_char]\n                    score = start_logits[0, start_index] + end_logits[0, end_index]\n\n                    predict_answers.append({\n                        'predict_text': text,\n                        'score': score.item()\n                    })\n            \n        if len(predict_answers) > 0:\n            best_answer = sorted(predict_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"predict_text\": \"\", \"score\": 0.0}\n            \n        best_answer['id'] = example_id\n        predicts.append(best_answer)\n    predict_df = pd.DataFrame(predicts)\n    return predict_df","metadata":{"execution":{"iopub.status.busy":"2021-08-23T22:57:06.7593Z","iopub.execute_input":"2021-08-23T22:57:06.759608Z","iopub.status.idle":"2021-08-23T22:57:06.772568Z","shell.execute_reply.started":"2021-08-23T22:57:06.759578Z","shell.execute_reply":"2021-08-23T22:57:06.771574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChaiiModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.transformer_config = AutoConfig.from_pretrained(config.MODEL_NAME)\n        self.transformer_config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": config.HIDDEN_DROPOUT_PROB,\n                \"layer_norm_eps\": config.LAYER_NORM_EPS,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(config.MODEL_NAME, config=self.transformer_config)\n        self.qa_outputs = nn.Linear(self.transformer_config.hidden_size, 2)\n        self.__init_weights(self.qa_outputs)\n        \n    def __init_weights(self,module):\n        if isinstance(module,nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.transformer_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        \n    def forward(self, input_ids, attention_mask, token_type_ids=None, start_positions=None, end_positions=None):\n        transformer_out = self.transformer(\n            input_ids,\n            attention_mask,\n            #token_type_ids,\n        )\n        sequence_output = transformer_out['last_hidden_state']  # 'last_hidden_state', 'pooler_output', 'hidden_states'\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1) #.contiguous()\n        end_logits = end_logits.squeeze(-1) #.contiguous()\n        \n        if start_positions is not None and end_positions is not None:\n            loss = self.loss_fn(start_logits, end_logits, start_positions, end_positions)\n        else:\n            loss = None\n\n        return {\n            'start_logits': start_logits,\n            'end_logits': end_logits,\n            'loss': loss,\n        }\n    \n    def loss_fn(self, start_logits, end_logits, start_positions, end_positions):\n        total_loss = None\n\n        ignored_index = start_logits.size(1)\n        start_positions = start_positions.clamp(0, ignored_index)\n        end_positions = end_positions.clamp(0, ignored_index)\n\n        loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n        start_loss = loss_fct(start_logits, start_positions)\n        end_loss = loss_fct(end_logits, end_positions)\n        total_loss = (start_loss + end_loss) / 2\n        \n        return total_loss","metadata":{"execution":{"iopub.status.busy":"2021-08-23T22:59:43.214053Z","iopub.execute_input":"2021-08-23T22:59:43.214379Z","iopub.status.idle":"2021-08-23T22:59:43.228735Z","shell.execute_reply.started":"2021-08-23T22:59:43.214352Z","shell.execute_reply":"2021-08-23T22:59:43.227791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    test_df = pd.read_csv(f'{config.INPUT_DIR}/test.csv')\n    sub_df = pd.read_csv(f'{config.INPUT_DIR}/sample_submission.csv')\n\n    test_dataset = ChAIIDataset(test_df, train=False)\n\n\n    models = {}\n    for lang in [\"hindi\", \"tamil\"]:\n        model = ChaiiModel()\n        if lang == \"hindi\":\n            model.load_state_dict(torch.load(config.HINDI_MODEL_PATH, map_location=torch.device('cpu')))\n        elif lang == \"tamil\":\n            model.load_state_dict(torch.load(config.TAMIL_MODEL_PATH, map_location=torch.device('cpu')))\n        else:\n            raise\n        model.to(device)\n        models[lang] = model\n        \n    set_seed()\n    all_logits = predict_test(models, test_dataset, test_df)\n    predict_df = post_processing(all_logits, test_df, test_dataset)\n\n    result_df = predict_df.merge(test_df, how='left', on='id')\n\n    result_df = result_df[['id', 'predict_text']]\n    result_df.columns = sub_df.columns\n\n    result_df.to_csv('submission.csv', index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T23:01:24.01832Z","iopub.execute_input":"2021-08-23T23:01:24.018669Z","iopub.status.idle":"2021-08-23T23:01:53.344496Z","shell.execute_reply.started":"2021-08-23T23:01:24.018619Z","shell.execute_reply":"2021-08-23T23:01:53.343645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T22:52:59.138918Z","iopub.execute_input":"2021-08-23T22:52:59.139248Z","iopub.status.idle":"2021-08-23T22:53:22.359822Z","shell.execute_reply.started":"2021-08-23T22:52:59.139215Z","shell.execute_reply":"2021-08-23T22:53:22.35893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head submission.csv","metadata":{"execution":{"iopub.status.busy":"2021-08-23T23:04:56.270977Z","iopub.execute_input":"2021-08-23T23:04:56.271311Z","iopub.status.idle":"2021-08-23T23:04:56.9594Z","shell.execute_reply.started":"2021-08-23T23:04:56.271281Z","shell.execute_reply":"2021-08-23T23:04:56.958499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}