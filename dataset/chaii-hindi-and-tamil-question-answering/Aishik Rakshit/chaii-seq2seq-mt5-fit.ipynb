{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"-Yndwa7r2sq2","outputId":"3e075c35-e1cf-4ffc-e813-5e82585eed68","execution":{"iopub.status.busy":"2021-11-16T14:59:38.935495Z","iopub.execute_input":"2021-11-16T14:59:38.935791Z","iopub.status.idle":"2021-11-16T14:59:39.74392Z","shell.execute_reply.started":"2021-11-16T14:59:38.935716Z","shell.execute_reply":"2021-11-16T14:59:39.743089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nfrom torch.utils.data.distributed import DistributedSampler\n\ntry:\n    from apex import amp\n    APEX_INSTALLED = True\nexcept ImportError:\n    APEX_INSTALLED = False\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    MT5ForConditionalGeneration,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\nprint(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"id":"_o82Ohod3Rtz","outputId":"d248c322-d18f-49f7-c72b-ea9abafcab2a","execution":{"iopub.status.busy":"2021-11-16T15:00:05.351812Z","iopub.execute_input":"2021-11-16T15:00:05.352383Z","iopub.status.idle":"2021-11-16T15:00:12.147886Z","shell.execute_reply.started":"2021-11-16T15:00:05.352344Z","shell.execute_reply":"2021-11-16T15:00:12.147144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    # model\n    debug=True\n    model_type = 'mT5'\n    model_name_or_path = \"mrm8488/mT5-small-finetuned-tydiqa-for-xqa\"\n    config_name = \"mrm8488/mT5-small-finetuned-tydiqa-for-xqa\"\n    fp16 = True if APEX_INSTALLED else False\n    fp16_opt_level = \"O1\"\n    gradient_accumulation_steps = 2\n\n    # tokenizer\n    tokenizer_name = \"mrm8488/mT5-small-finetuned-tydiqa-for-xqa\"\n    max_seq_length = 512\n    ans_max_seq_length=32\n    doc_stride = 128\n\n    # train\n    epochs = 2\n    train_batch_size = 2\n    eval_batch_size = 2\n\n    # optimizer\n    optimizer_type = 'AdamW'\n    learning_rate = 1.5e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    decay_name = 'linear-warmup'\n    warmup_ratio = 0.1\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 2021","metadata":{"id":"vWI4x6gr3SLD","execution":{"iopub.status.busy":"2021-11-16T15:00:12.149444Z","iopub.execute_input":"2021-11-16T15:00:12.150146Z","iopub.status.idle":"2021-11-16T15:00:12.157611Z","shell.execute_reply.started":"2021-11-16T15:00:12.150109Z","shell.execute_reply":"2021-11-16T15:00:12.156777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\ntrain_cleaned=pd.read_csv('../input/cleaned-data-for-chaii/cleaned_train.csv')\nexternal_mlqa = pd.read_csv('../input/extra-data/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv('../input/extra-data/xquad.csv')\nexternal_tamil = pd.read_csv(\"../input/squad-translated-to-tamil-for-chaii/squad_translated_tamil.csv\")\nexternal_tamil[\"language\"]=\"tamil\"\nexternal_train = pd.concat([external_mlqa,external_xquad,external_tamil])\ndef create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n        data.loc[v_, 'kfold'] = f\n    return data\ntrain = create_folds(train_cleaned, num_splits=5)\ntrain.head()\nexternal_train[\"kfold\"] = -1\n# external_train['id'] = list(np.arange(1, len(external_train)+1))\ntrain = pd.concat([train_cleaned, external_train]).reset_index(drop=True)\n\ndef convert_answers(row):\n    return {'answer_start': [row[0]], 'text': [row[1]]}\n\ntrain['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\ntrain.head()","metadata":{"id":"tqQAc43H3TyE","outputId":"2aab41e1-d545-4064-d1ce-812d69f16532","execution":{"iopub.status.busy":"2021-11-16T15:00:12.158639Z","iopub.execute_input":"2021-11-16T15:00:12.159339Z","iopub.status.idle":"2021-11-16T15:00:14.314271Z","shell.execute_reply.started":"2021-11-16T15:00:12.159299Z","shell.execute_reply":"2021-11-16T15:00:14.313443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Config().debug:\n    train=train[:100]","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:00:14.316003Z","iopub.execute_input":"2021-11-16T15:00:14.316906Z","iopub.status.idle":"2021-11-16T15:00:14.321722Z","shell.execute_reply.started":"2021-11-16T15:00:14.316861Z","shell.execute_reply":"2021-11-16T15:00:14.320952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class chaiiS2SDataset(Dataset):\n    def __init__(self,\n                data,\n                tokenizer,\n                 args):\n        self.data=data\n        self.tokenizer=tokenizer\n        self.args=args\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self,\n                   idx):\n        data_row=self.data.iloc[idx]\n        source_encoding = self.tokenizer(\n                                    data_row['question'],\n                                    data_row['context'],\n                                    max_length=self.args.max_seq_length,\n                                    padding='max_length',\n                                    truncation=\"only_second\",\n                                    return_attention_mask=True,\n                                    add_special_tokens=True,\n                                    return_tensors=\"pt\"\n                                    )\n\n        target_encoding = self.tokenizer(\n                                    data_row['answer_text'],\n                                    max_length=self.args.ans_max_seq_length,\n                                    padding='max_length',\n                                    truncation=True,\n                                    return_attention_mask=True,\n                                    add_special_tokens=True,\n                                    return_tensors=\"pt\"\n                                    )\n\n        labels = target_encoding['input_ids']\n#         labels[labels==0] = -100\n        return dict(\n                    question=data_row['question'],\n                    context=data_row['context'],\n                    answer_text=data_row['answer_text'],\n                    input_ids=source_encoding[\"input_ids\"].flatten(),\n                    attention_mask=source_encoding['attention_mask'].flatten(),\n                    labels=labels.flatten()\n                    )","metadata":{"id":"Mg34uvId3iCU","execution":{"iopub.status.busy":"2021-11-16T15:00:19.512945Z","iopub.execute_input":"2021-11-16T15:00:19.513224Z","iopub.status.idle":"2021-11-16T15:00:19.524687Z","shell.execute_reply.started":"2021-11-16T15:00:19.513192Z","shell.execute_reply":"2021-11-16T15:00:19.523939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_optimizer_grouped_parameters(args, model):\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n    ]\n    return optimizer_grouped_parameters","metadata":{"id":"_P66anB_3lBl","execution":{"iopub.status.busy":"2021-11-16T15:00:19.750464Z","iopub.execute_input":"2021-11-16T15:00:19.750671Z","iopub.status.idle":"2021-11-16T15:00:19.766945Z","shell.execute_reply.started":"2021-11-16T15:00:19.750645Z","shell.execute_reply":"2021-11-16T15:00:19.765773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        if val > self.max:\n            self.max = val\n        if val < self.min:\n            self.min = val","metadata":{"id":"JCgNqk-K3lhD","execution":{"iopub.status.busy":"2021-11-16T15:00:19.957764Z","iopub.execute_input":"2021-11-16T15:00:19.957965Z","iopub.status.idle":"2021-11-16T15:00:19.964319Z","shell.execute_reply.started":"2021-11-16T15:00:19.957941Z","shell.execute_reply":"2021-11-16T15:00:19.963396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_model(args):\n    config = AutoConfig.from_pretrained(args.config_name)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n    model = MT5ForConditionalGeneration.from_pretrained(args.model_name_or_path, return_dict = True)\n    return config, tokenizer, model\n\ndef make_optimizer(args, model):\n    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    if args.optimizer_type == \"AdamW\":\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=args.learning_rate,\n            eps=args.epsilon,\n            correct_bias=True\n        )\n        return optimizer\n\ndef make_scheduler(\n    args, optimizer, \n    num_warmup_steps, \n    num_training_steps\n):\n    if args.decay_name == \"cosine-warmup\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    else:\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    return scheduler    \n\ndef make_loader(\n    args, data, \n    tokenizer, fold\n):\n    train_df, val_df = model_selection.train_test_split(data, test_size=0.2)\n    \n#     train_features, valid_features = [[] for _ in range(2)]\n#     for i, row in train_set.iterrows():\n#         train_features += prepare_train_features(args, row, tokenizer)\n#     for i, row in valid_set.iterrows():\n#         valid_features += prepare_train_features(args, row, tokenizer)\n\n#     train_dataset = DatasetRetriever(train_features)\n#     valid_dataset = DatasetRetriever(valid_features)\n    \n    train_dataset=chaiiS2SDataset(train_df,tokenizer,args)\n    valid_dataset=chaiiS2SDataset(val_df,tokenizer,args)\n    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n    \n    train_sampler = RandomSampler(train_dataset)\n    valid_sampler = SequentialSampler(valid_dataset)\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=args.train_batch_size,\n        sampler=train_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True,\n        drop_last=False \n    )\n\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=args.eval_batch_size, \n        sampler=valid_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True, \n        drop_last=False\n    )\n\n    return train_dataloader, valid_dataloader","metadata":{"id":"DdacodGl3msU","execution":{"iopub.status.busy":"2021-11-16T15:00:20.20042Z","iopub.execute_input":"2021-11-16T15:00:20.200791Z","iopub.status.idle":"2021-11-16T15:00:20.21465Z","shell.execute_reply.started":"2021-11-16T15:00:20.20076Z","shell.execute_reply":"2021-11-16T15:00:20.21394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer:\n    def __init__(\n        self, model, tokenizer, \n        optimizer, scheduler\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def train(\n        self, args, \n        train_dataloader, \n        epoch, result_dict\n    ):\n        count = 0\n        losses = AverageMeter()\n        \n        self.model.zero_grad()\n        self.model.train()\n        \n        fix_all_seeds(args.seed)\n        \n        for batch_idx, batch_data in enumerate(train_dataloader):\n            input_ids, attention_mask, labels = \\\n                batch_data['input_ids'], batch_data['attention_mask'], \\\n                    batch_data['labels']\n            \n            input_ids, attention_mask, labels = \\\n                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            loss, logits=outputs.loss, outputs.logits\n#             loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n            loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            count += input_ids.size(0)\n            losses.update(loss.item(), input_ids.size(0))\n\n            # if args.fp16:\n            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n            # else:\n            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n\n            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n                self.optimizer.step()\n                self.scheduler.step()\n                self.optimizer.zero_grad()\n\n            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n                _s = str(len(str(len(train_dataloader.sampler))))\n                ret = [\n                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n                    'Train Loss: {: >4.5f}'.format(losses.avg),\n                ]\n                print(', '.join(ret))\n\n        result_dict['train_loss'].append(losses.avg)\n        return result_dict","metadata":{"id":"o7fhTin03nvc","execution":{"iopub.status.busy":"2021-11-16T15:00:20.424753Z","iopub.execute_input":"2021-11-16T15:00:20.424961Z","iopub.status.idle":"2021-11-16T15:00:20.439629Z","shell.execute_reply.started":"2021-11-16T15:00:20.424936Z","shell.execute_reply":"2021-11-16T15:00:20.438715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model):\n        self.model = model\n    \n    def save(self, result, output_dir):\n        with open(f'{output_dir}/result_dict.json', 'w') as f:\n            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n\n    def evaluate(self, valid_dataloader, epoch, result_dict):\n        losses = AverageMeter()\n        for batch_idx, batch_data in enumerate(valid_dataloader):\n            self.model = self.model.eval()\n            input_ids, attention_mask, labels = \\\n                batch_data['input_ids'], batch_data['attention_mask'], \\\n                    batch_data['labels']\n            \n            input_ids, attention_mask, labels = \\\n                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n            \n            with torch.no_grad():            \n                outputs = self.model(\n                                        input_ids=input_ids,\n                                        attention_mask=attention_mask,\n                                        labels=labels\n                                    )\n                loss,logits=outputs.loss, outputs.logits\n#                 loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n                losses.update(loss.item(), input_ids.size(0))\n                \n        print('----Validation Results Summary----')\n        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n        result_dict['val_loss'].append(losses.avg)        \n        return result_dict","metadata":{"id":"P1WE5ymo3pdE","execution":{"iopub.status.busy":"2021-11-16T15:00:20.951548Z","iopub.execute_input":"2021-11-16T15:00:20.951792Z","iopub.status.idle":"2021-11-16T15:00:20.961305Z","shell.execute_reply.started":"2021-11-16T15:00:20.951763Z","shell.execute_reply":"2021-11-16T15:00:20.960496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_training(args, data, fold):\n    fix_all_seeds(args.seed)\n    \n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    \n    # model\n    model_config, tokenizer, model = make_model(args)\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n    \n    # data loaders\n    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n\n    # optimizer\n    optimizer = make_optimizer(args, model)\n\n    # scheduler\n    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n    if args.warmup_ratio > 0:\n        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n    else:\n        num_warmup_steps = 0\n    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n\n    # mixed precision training with NVIDIA Apex\n    if args.fp16:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    \n    result_dict = {\n        'epoch':[], \n        'train_loss': [], \n        'val_loss' : [], \n        'best_val_loss': np.inf\n    }\n\n    return (\n        model, model_config, tokenizer, optimizer, scheduler, \n        train_dataloader, valid_dataloader, result_dict\n    )","metadata":{"id":"L9PLG-Yj3rVE","execution":{"iopub.status.busy":"2021-11-16T15:00:21.172689Z","iopub.execute_input":"2021-11-16T15:00:21.172917Z","iopub.status.idle":"2021-11-16T15:00:21.184781Z","shell.execute_reply.started":"2021-11-16T15:00:21.172892Z","shell.execute_reply":"2021-11-16T15:00:21.184014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(data, fold):\n    args = Config()\n    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n        valid_dataloader, result_dict = init_training(args, data, fold)\n    \n    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n    evaluator = Evaluator(model)\n\n    train_time_list = []\n    valid_time_list = []\n\n    for epoch in range(args.epochs):\n        result_dict['epoch'].append(epoch)\n\n        # Train\n        torch.cuda.synchronize()\n        tic1 = time.time()\n        result_dict = trainer.train(\n            args, train_dataloader, \n            epoch, result_dict\n        )\n        torch.cuda.synchronize()\n        tic2 = time.time() \n        train_time_list.append(tic2 - tic1)\n        \n        # Evaluate\n        torch.cuda.synchronize()\n        tic3 = time.time()\n        result_dict = evaluator.evaluate(\n            valid_dataloader, epoch, result_dict\n        )\n        torch.cuda.synchronize()\n        tic4 = time.time() \n        valid_time_list.append(tic4 - tic3)\n            \n        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n        if result_dict['val_loss'][-1] <= result_dict['best_val_loss']:\n            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n            \n            os.makedirs(output_dir, exist_ok=True)\n            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n            model_config.save_pretrained(output_dir)\n            tokenizer.save_pretrained(output_dir)\n            print(f\"Saving model checkpoint to {output_dir}.\")\n            \n        print()\n\n    evaluator.save(result_dict, output_dir)\n    \n    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n    \n    torch.cuda.empty_cache()\n    del trainer, evaluator\n    del model, model_config, tokenizer\n    del optimizer, scheduler\n    del train_dataloader, valid_dataloader, result_dict\n    gc.collect()","metadata":{"id":"NVP0wqVl3t2s","execution":{"iopub.status.busy":"2021-11-16T15:00:21.385779Z","iopub.execute_input":"2021-11-16T15:00:21.385969Z","iopub.status.idle":"2021-11-16T15:00:21.397618Z","shell.execute_reply.started":"2021-11-16T15:00:21.385946Z","shell.execute_reply":"2021-11-16T15:00:21.396582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(train,0)","metadata":{"id":"F6KznhDt3vJr","outputId":"8dd483b7-913b-488a-89ca-da0e824d25b2","execution":{"iopub.status.busy":"2021-11-16T15:00:21.598482Z","iopub.execute_input":"2021-11-16T15:00:21.599043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"PAECWRa9R4Jj"},"execution_count":null,"outputs":[]}]}