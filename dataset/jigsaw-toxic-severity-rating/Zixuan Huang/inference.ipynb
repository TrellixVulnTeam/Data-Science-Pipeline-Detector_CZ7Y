{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\nimport re\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel\n\n# Utils\nfrom tqdm import tqdm\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-14T04:05:25.751175Z","iopub.execute_input":"2021-12-14T04:05:25.751436Z","iopub.status.idle":"2021-12-14T04:05:25.758205Z","shell.execute_reply.started":"2021-12-14T04:05:25.751407Z","shell.execute_reply":"2021-12-14T04:05:25.757499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    seed = 42,\n    model_name = '../input/roberta-base',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])","metadata":{"execution":{"iopub.status.busy":"2021-12-14T04:05:25.760104Z","iopub.execute_input":"2021-12-14T04:05:25.760887Z","iopub.status.idle":"2021-12-14T04:05:25.847299Z","shell.execute_reply.started":"2021-12-14T04:05:25.760839Z","shell.execute_reply":"2021-12-14T04:05:25.846623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2021-12-14T04:05:25.848401Z","iopub.execute_input":"2021-12-14T04:05:25.848644Z","iopub.status.idle":"2021-12-14T04:05:25.854759Z","shell.execute_reply.started":"2021-12-14T04:05:25.848611Z","shell.execute_reply":"2021-12-14T04:05:25.853867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaseTokenizer(object):\n    def process_text(self, text):\n        raise NotImplemented\n\n    def process(self, texts):\n        for text in texts:\n            yield self.process_text(text)\n\n\nRE_PATTERNS = {\n    ' american ':\n        [\n            'amerikan'\n        ],\n    ' adolf ':\n        [\n            'adolf'\n        ],\n    ' hitler ':\n        [\n            'hitler'\n        ],\n    ' fuck':\n        [\n            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n            'feck ', ' fux ', 'f\\*\\*', \n            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n        ],\n    ' ass ':\n        [\n            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n        ],\n    ' ass hole ':\n        [\n            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n        ],\n    ' bitch ':\n        [\n            'b[w]*i[t]*ch', 'b!tch',\n            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n        ],\n    ' bastard ':\n        [\n            'ba[s|z]+t[e|a]+rd'\n        ],\n    ' trans gender':\n        [\n            'transgender'\n        ],\n    ' gay ':\n        [\n            'gay'\n        ],\n    ' cock ':\n        [\n            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n        ],\n    ' dick ':\n        [\n            ' dick[^aeiou]', 'deek', 'd i c k'\n        ],\n    ' suck ':\n        [\n            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n        ],\n    ' cunt ':\n        [\n            'cunt', 'c u n t'\n        ],\n    ' bull shit ':\n        [\n            'bullsh\\*t', 'bull\\$hit'\n        ],\n    ' homo sex ual':\n        [\n            'homosexual'\n        ],\n    ' jerk ':\n        [\n            'jerk'\n        ],\n    ' idiot ':\n        [\n            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n                                                                                      'i d i o t'\n        ],\n    ' dumb ':\n        [\n            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n        ],\n    ' shit ':\n        [\n            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n        ],\n    ' shit hole ':\n        [\n            'shythole'\n        ],\n    ' retard ':\n        [\n            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n        ],\n    ' rape ':\n        [\n            ' raped'\n        ],\n    ' dumb ass':\n        [\n            'dumbass', 'dubass'\n        ],\n    ' ass head':\n        [\n            'butthead'\n        ],\n    ' sex ':\n        [\n            'sexy', 's3x', 'sexuality'\n        ],\n    ' nigger ':\n        [\n            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n        ],\n    ' shut the fuck up':\n        [\n            'stfu'\n        ],\n    ' pussy ':\n        [\n            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n        ],\n    ' faggot ':\n        [\n            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n        ],\n    ' mother fucker':\n        [\n            ' motha ', ' motha f', ' mother f', 'motherucker',\n        ],\n    ' whore ':\n        [\n            'wh\\*\\*\\*', 'w h o r e'\n        ],\n}\n\n\nclass PatternTokenizer(BaseTokenizer):\n    def __init__(self, lower=True, initial_filters=r\"[^a-z0-9!@#\\$%\\^\\&\\*_\\-,\\.' ]\", patterns=RE_PATTERNS,\n                 remove_repetitions=True):\n        self.lower = lower\n        self.patterns = patterns\n        self.initial_filters = initial_filters\n        self.remove_repetitions = remove_repetitions\n\n    def process_text(self, text):\n        x = self._preprocess(text)\n        for target, patterns in self.patterns.items():\n            for pat in patterns:\n                x = re.sub(pat, target, x)\n        x = re.sub(r\"[^a-z' ]\", ' ', x)\n        return x.split()\n\n    def process_ds(self, ds):\n        ### ds = Data series\n\n        # lower\n        ds = copy.deepcopy(ds)\n        # if self.lower:\n        #     ds = ds.str.lower()\n\n        # remove special chars\n\n        # if self.initial_filters is not None:\n        #     ds = ds.str.replace(self.initial_filters, ' ')\n        # fuuuuck => fuck\n        if self.remove_repetitions:\n            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n            ds = ds.str.replace(pattern, r\"\\1\")\n\n        for target, patterns in self.patterns.items():\n            for pat in patterns:\n                ds = ds.str.replace(pat, target)\n\n        # ds = ds.str.replace(r\"[^a-zA-Z' ]\", ' ')\n\n        return ds.str.split()\n\n    def _preprocess(self, text):\n        # lower\n        if self.lower:\n            text = text.lower()\n\n        # remove special chars\n        if self.initial_filters is not None:\n            text = re.sub(self.initial_filters, ' ', text)\n\n        # fuuuuck => fuck\n        if self.remove_repetitions:\n            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n            text = pattern.sub(r\"\\1\", text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2021-12-14T04:05:25.856364Z","iopub.execute_input":"2021-12-14T04:05:25.856859Z","iopub.status.idle":"2021-12-14T04:05:25.880156Z","shell.execute_reply.started":"2021-12-14T04:05:25.856811Z","shell.execute_reply":"2021-12-14T04:05:25.87938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, preprocessed=False):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.preprocessed = preprocessed\n        self.text = df['text'].values\n        if self.preprocessed:\n            preproc_tokenizer = PatternTokenizer()\n            self.text = preproc_tokenizer.process_ds(df['text']).str.join(sep=\" \").values\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-12-14T04:05:25.882559Z","iopub.execute_input":"2021-12-14T04:05:25.882983Z","iopub.status.idle":"2021-12-14T04:05:25.936573Z","shell.execute_reply.started":"2021-12-14T04:05:25.882931Z","shell.execute_reply":"2021-12-14T04:05:25.935952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\nclass JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs\n@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS","metadata":{"execution":{"iopub.status.busy":"2021-12-14T04:05:25.937723Z","iopub.execute_input":"2021-12-14T04:05:25.937983Z","iopub.status.idle":"2021-12-14T04:05:25.949178Z","shell.execute_reply.started":"2021-12-14T04:05:25.937951Z","shell.execute_reply":"2021-12-14T04:05:25.948361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = nn.DataParallel(JigsawModel(CONFIG['model_name']))\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds","metadata":{"execution":{"iopub.status.busy":"2021-12-14T04:05:25.950569Z","iopub.execute_input":"2021-12-14T04:05:25.950985Z","iopub.status.idle":"2021-12-14T04:05:25.961113Z","shell.execute_reply.started":"2021-12-14T04:05:25.950952Z","shell.execute_reply":"2021-12-14T04:05:25.960166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL_PATHS = [\n#     \"../input/ckpt-best/best.ckpt\"\n# ]\nMODEL_PATHS = [\n    \"../input/ensemble/checkpoints/3/0_fold.ckpt\",\n    \"../input/ensemble-trainset/0_fold.ckpt\",\n    \"../input/ensemble-trainset/1_fold.ckpt\",\n    \"../input/ensemble-trainset/2_fold.ckpt\",\n    \"../input/ensemble-trainset/3_fold.ckpt\",\n    \"../input/ensemble-trainset/4_fold.ckpt\",\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'])\ndf['score'] = preds\ndf['score'] = df['score'].rank(method='first')\ndf.drop('text', axis=1, inplace=True)\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T04:05:25.962282Z","iopub.execute_input":"2021-12-14T04:05:25.962759Z","iopub.status.idle":"2021-12-14T04:05:37.149208Z","shell.execute_reply.started":"2021-12-14T04:05:25.962724Z","shell.execute_reply":"2021-12-14T04:05:37.147744Z"},"trusted":true},"execution_count":null,"outputs":[]}]}