{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src= \"https://clickhole.com/wp-content/uploads/2018/11/mdc4jnl2amnpmnajblxn.jpg\" alt =\"Jigsaw\" style='width: 1000px; height: 500px' align = 'center'>","metadata":{}},{"cell_type":"markdown","source":"<a href=\"https://fontmeme.com/saw-font/\"><img src=\"https://fontmeme.com/permalink/211129/18143700fb8aa5b735bd7a3d0587da71.png\" alt=\"saw-font\" border=\"0\"></a>","metadata":{}},{"cell_type":"code","source":"!pip install nltk\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#!pip install xgboost\nfrom xgboost import XGBRegressor \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:18.43978Z","iopub.execute_input":"2021-11-29T09:01:18.440055Z","iopub.status.idle":"2021-11-29T09:01:51.063776Z","shell.execute_reply.started":"2021-11-29T09:01:18.440026Z","shell.execute_reply":"2021-11-29T09:01:51.062824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:51.067345Z","iopub.execute_input":"2021-11-29T09:01:51.06771Z","iopub.status.idle":"2021-11-29T09:01:51.08016Z","shell.execute_reply.started":"2021-11-29T09:01:51.067648Z","shell.execute_reply":"2021-11-29T09:01:51.07905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"https://fontmeme.com/saw-font/\"><img src=\"https://fontmeme.com/permalink/211129/9567265046ea503e73e1222c8bfde7c1.png\" alt=\"saw-font\" border=\"0\"></a>","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(r'../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndata.head()\nn = len(data)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:51.081718Z","iopub.execute_input":"2021-11-29T09:01:51.081988Z","iopub.status.idle":"2021-11-29T09:01:51.205471Z","shell.execute_reply.started":"2021-11-29T09:01:51.081958Z","shell.execute_reply":"2021-11-29T09:01:51.204651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ntrain_data = pd.read_csv(r'../input/jigsaw-toxic-comment-classification-challenge/train.csv')\n#train_data = train_data[:n]\ntrain_data.head()\ntext = data['text']\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:51.206992Z","iopub.execute_input":"2021-11-29T09:01:51.207335Z","iopub.status.idle":"2021-11-29T09:01:53.360174Z","shell.execute_reply.started":"2021-11-29T09:01:51.207293Z","shell.execute_reply":"2021-11-29T09:01:53.359195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data['score'] = score\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:53.362788Z","iopub.execute_input":"2021-11-29T09:01:53.363053Z","iopub.status.idle":"2021-11-29T09:01:53.382867Z","shell.execute_reply.started":"2021-11-29T09:01:53.363021Z","shell.execute_reply":"2021-11-29T09:01:53.381791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"https://fontmeme.com/saw-font/\"><img src=\"https://fontmeme.com/permalink/211129/02a90505225940c50c884464c1593339.png\" alt=\"saw-font\" border=\"0\"></a>","metadata":{}},{"cell_type":"code","source":"#   Sentence length in \ntext.str.len().hist()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:53.384187Z","iopub.execute_input":"2021-11-29T09:01:53.384443Z","iopub.status.idle":"2021-11-29T09:01:53.68496Z","shell.execute_reply.started":"2021-11-29T09:01:53.384412Z","shell.execute_reply":"2021-11-29T09:01:53.684158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Word Length\narr = [ ]\nfor i in text:\n    tmp = i.split(' ')\n    arr.append(len(tmp))\nplt.figure(figsize = (10,8))\nplt.hist(arr)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:53.686291Z","iopub.execute_input":"2021-11-29T09:01:53.686531Z","iopub.status.idle":"2021-11-29T09:01:54.029282Z","shell.execute_reply.started":"2021-11-29T09:01:53.6865Z","shell.execute_reply":"2021-11-29T09:01:54.028517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop = set(stopwords.words('english'))\ncorpus = []\ntmp = []\nfor i in text : \n    tmp.append(i.split())\n    \nfor i in tmp:\n    for words in i :\n        corpus.append(words)\nfrom collections import defaultdict\ndic = defaultdict(int)\nfor i in corpus: \n    if i in stop :\n        dic[i] = dic[i] + 1\nval = dic.values()\nval = sorted(val,reverse = True)\nval = val[0:11]\nd = {}\nfor i in list(dic.keys()):\n    if dic[i] in val: \n        d[i] = dic[i]\n# Top 10 Stopwords in corpus\nplt.figure(figsize=(10,8))\nplt.bar(list(d.keys()), list(d.values()), color = 'Red')","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:54.03033Z","iopub.execute_input":"2021-11-29T09:01:54.031322Z","iopub.status.idle":"2021-11-29T09:01:54.61956Z","shell.execute_reply.started":"2021-11-29T09:01:54.031282Z","shell.execute_reply":"2021-11-29T09:01:54.618288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.util import ngrams\nimport collections\ntex = ''\n#word_tokenize = nltk.download('word_tokenize')\nfor i in text : \n    tex = tex + str(i.strip('[]'))\ntok = tex.split()\nb_grams = ngrams(tok, 2) \nt_grams = ngrams(tok,3)\nbigrams = collections.Counter(b_grams)\ntrigrams = collections.Counter(t_grams)\nbi= bigrams.most_common(10)\nti= trigrams.most_common(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:54.620995Z","iopub.execute_input":"2021-11-29T09:01:54.621242Z","iopub.status.idle":"2021-11-29T09:01:55.722178Z","shell.execute_reply.started":"2021-11-29T09:01:54.621215Z","shell.execute_reply":"2021-11-29T09:01:55.721208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_ngrams(bi):\n    word = []\n    idx = []\n    for i in range(len(bi)):\n        word.append(str(bi[i][0]))\n        idx.append(bi[i][1])\n    plt.figure(figsize = (10,8))\n    plt.bar(word,idx, color = 'Green')\n    #plt.xticks(word, word, rotation = 'vertical')\n    plt.xlabel('Ngrams')\n    plt.ylabel('Frequency')\n    plt.title('Ngram with Frequency')\n    plt.show()\n#Bigrams\nplot_ngrams(bi)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:55.723934Z","iopub.execute_input":"2021-11-29T09:01:55.724276Z","iopub.status.idle":"2021-11-29T09:01:55.98921Z","shell.execute_reply.started":"2021-11-29T09:01:55.724231Z","shell.execute_reply":"2021-11-29T09:01:55.988278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trigrams\nplot_ngrams(ti)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:55.990636Z","iopub.execute_input":"2021-11-29T09:01:55.991236Z","iopub.status.idle":"2021-11-29T09:01:56.26175Z","shell.execute_reply.started":"2021-11-29T09:01:55.991189Z","shell.execute_reply":"2021-11-29T09:01:56.260862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"https://fontmeme.com/saw-font/\"><img src=\"https://fontmeme.com/permalink/211129/08e28926ea46733dc2b519f7d145a7ab.png\" alt=\"saw-font\" border=\"0\"></a>","metadata":{}},{"cell_type":"code","source":"#Stopwords\nimport nltk\nnltk.download('stopwords')\nstop = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:01:56.262983Z","iopub.execute_input":"2021-11-29T09:01:56.263965Z","iopub.status.idle":"2021-11-29T09:02:16.304644Z","shell.execute_reply.started":"2021-11-29T09:01:56.263916Z","shell.execute_reply":"2021-11-29T09:02:16.303823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lowercase(te):\n    tmp = []\n    #Complete \n    for i in te : \n        tmp.append(i.lower())\n    return tmp\ndef remove_symbols(text):\n    #commplete\n    tmp = []\n    for i in text: \n        #i = re.sub(r'\\n','',i)\n       # i = re.sub(r'\"\"', ' ',i)\n        i = re.sub(r'[^\\w]',' ',i) #Remove all types of symbols from string\n        tmp.append(i.replace(') ',''))\n    return tmp\ndef remove_stopwords(text): \n    tmp = []\n    for i in text: \n        te = str(i)\n        words = word_tokenize(te)\n        token_without = [word for word in words if not word in stop]\n        s = ''\n        for w in token_without : \n            s = s + w + ' '\n        tmp.append(s)\n    return tmp\ndef stemming(text): \n    tmp = []\n    for i in text : \n        te = str(i)\n        lemmatizer = WordNetLemmatizer()\n        words = word_tokenize(te)\n        s = ''\n        for w in words:\n            rw = lemmatizer.lemmatize(w)\n            s = s + rw + ' '\n        tmp.append(s)\n    return tmp\ndef stemming_lem_stop(text):\n    tmp = []\n    for i in text:\n        te = str(i)\n        words = word_tokenize(te)\n        ps = PorterStemmer()\n        lemmatizer = WordNetLemmatizer()\n        token_without = [word for word in words if not word in stop] #Remove Stopwords from token\n        s = ''\n        for w in token_without: \n            rootword = ps.stem(w)\n            rw = lemmatizer.lemmatize(rootword) #Lemmatization\n            s = s + rw + ' '\n        tmp.append(s)\n    return tmp\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:02:16.306177Z","iopub.execute_input":"2021-11-29T09:02:16.306509Z","iopub.status.idle":"2021-11-29T09:02:16.321678Z","shell.execute_reply.started":"2021-11-29T09:02:16.306465Z","shell.execute_reply":"2021-11-29T09:02:16.32061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1 = lowercase(train_data['comment_text'])\ntrain2 = remove_symbols(train1)\ntrain4 = stemming(train2)\ntrain3 = remove_stopwords(train4)\ntest1 = lowercase(text)\ntest2 = remove_symbols(test1)\ntest4 = stemming(test2)\ntest3 = remove_stopwords(test4)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:02:16.325605Z","iopub.execute_input":"2021-11-29T09:02:16.326509Z","iopub.status.idle":"2021-11-29T09:07:26.264889Z","shell.execute_reply.started":"2021-11-29T09:02:16.326456Z","shell.execute_reply":"2021-11-29T09:07:26.263891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"https://fontmeme.com/saw-font/\"><img src=\"https://fontmeme.com/permalink/211129/3889dcaf8d87a0ebe7d2b7cef1c9e617.png\" alt=\"saw-font\" border=\"0\"></a>\nHere, I had calculated the score for training set with the help of TextBlob Sentiment Analysis \nfrom Jigsaw Toxic Comment Classification Competition Datasets","metadata":{}},{"cell_type":"code","source":"pol = []\nfor i in train3:\n    analysis = TextBlob(i)\n    x = analysis.sentiment.polarity\n    NewValue = (((x - (-1)) * 1) / 2) + 0 #NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n    if NewValue == 0.5 :\n        pol.append(0)\n    else : \n        pol.append(round(NewValue,2))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:07:26.266455Z","iopub.execute_input":"2021-11-29T09:07:26.26684Z","iopub.status.idle":"2021-11-29T09:08:34.844566Z","shell.execute_reply.started":"2021-11-29T09:07:26.266793Z","shell.execute_reply":"2021-11-29T09:08:34.84323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"com = []\ndi = {}\npol1 = []\nfor i in range(len(data['comment_id'])): \n    com.append(data['comment_id'][i])\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:08:34.846386Z","iopub.execute_input":"2021-11-29T09:08:34.84676Z","iopub.status.idle":"2021-11-29T09:08:34.920912Z","shell.execute_reply.started":"2021-11-29T09:08:34.846707Z","shell.execute_reply":"2021-11-29T09:08:34.919816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"https://fontmeme.com/saw-font/\"><img src=\"https://fontmeme.com/permalink/211129/81b94964d2426cbf298cc2086f45660a.png\" alt=\"saw-font\" border=\"0\"></a>","metadata":{}},{"cell_type":"code","source":"tfid = TfidfVectorizer(max_features = 50, min_df= 3, max_df=0.5, analyzer = 'word')\nres = tfid.fit_transform(train3).toarray()\nres1 = tfid.fit_transform(test3).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:08:34.922351Z","iopub.execute_input":"2021-11-29T09:08:34.922589Z","iopub.status.idle":"2021-11-29T09:08:44.623147Z","shell.execute_reply.started":"2021-11-29T09:08:34.922561Z","shell.execute_reply":"2021-11-29T09:08:44.622344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"https://fontmeme.com/saw-font/\"><img src=\"https://fontmeme.com/permalink/211129/78fa88cb341b3f050567ea9e93dbde80.png\" alt=\"saw-font\" border=\"0\"></a>","metadata":{}},{"cell_type":"code","source":"# Define initial best params and MAE\\\nfrom numpy import arange\n\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    #'objective':'reg:linear',\n}\ngridsearch_params = [\n    (max_depth, min_child_weight, eta)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n    for eta in arange(0.1,1,0.1)\n]\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\ndtrain = xgb.DMatrix(res,pol)\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight, eta in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}, eta={}\".format(\n                             max_depth,\n                             min_child_weight,\n                             eta ))    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    params['eta'] = eta\n # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        #num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=5\n    )    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight, eta)\n        print(\"Best params: {}, {}, {}, MAE: {}\".format(best_params[0], best_params[1],best_params[2], min_mae))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:08:44.624721Z","iopub.execute_input":"2021-11-29T09:08:44.625657Z","iopub.status.idle":"2021-11-29T09:35:54.909794Z","shell.execute_reply.started":"2021-11-29T09:08:44.625603Z","shell.execute_reply":"2021-11-29T09:35:54.908816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = best_params[0]\nc = best_params[1]\ne = best_params[2]","metadata":{"execution":{"iopub.status.busy":"2021-11-29T10:23:27.844315Z","iopub.execute_input":"2021-11-29T10:23:27.844873Z","iopub.status.idle":"2021-11-29T10:23:27.939205Z","shell.execute_reply.started":"2021-11-29T10:23:27.844751Z","shell.execute_reply":"2021-11-29T10:23:27.937691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XGBRegressor(n_estimators = 1000,max_depth = d,min_child_weight= c,eta = e, subsample=0.7, colsample_bytree=0.8)\nmodel.fit(res,pol)\ny_pred = abs(model.predict(res1))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:35:55.807417Z","iopub.status.idle":"2021-11-29T09:35:55.80778Z","shell.execute_reply.started":"2021-11-29T09:35:55.807586Z","shell.execute_reply":"2021-11-29T09:35:55.807608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"di = {'comment_id': com, 'score': y_pred}\ndf = pd.DataFrame(di)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:35:55.809219Z","iopub.status.idle":"2021-11-29T09:35:55.809564Z","shell.execute_reply.started":"2021-11-29T09:35:55.809379Z","shell.execute_reply":"2021-11-29T09:35:55.809401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:35:55.811204Z","iopub.status.idle":"2021-11-29T09:35:55.812042Z","shell.execute_reply.started":"2021-11-29T09:35:55.811793Z","shell.execute_reply":"2021-11-29T09:35:55.811824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output file\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T09:35:55.813644Z","iopub.status.idle":"2021-11-29T09:35:55.813993Z","shell.execute_reply.started":"2021-11-29T09:35:55.813816Z","shell.execute_reply":"2021-11-29T09:35:55.813836Z"},"trusted":true},"execution_count":null,"outputs":[]}]}