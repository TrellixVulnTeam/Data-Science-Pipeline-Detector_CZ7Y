{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"!pip install nltk\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#!pip install xgboost\nfrom xgboost import XGBRegressor ","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:43:31.349841Z","iopub.execute_input":"2021-11-25T10:43:31.350546Z","iopub.status.idle":"2021-11-25T10:44:02.706872Z","shell.execute_reply.started":"2021-11-25T10:43:31.35049Z","shell.execute_reply":"2021-11-25T10:44:02.70597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:02.709062Z","iopub.execute_input":"2021-11-25T10:44:02.709305Z","iopub.status.idle":"2021-11-25T10:44:02.721634Z","shell.execute_reply.started":"2021-11-25T10:44:02.709276Z","shell.execute_reply":"2021-11-25T10:44:02.720601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Reading a dataset**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(r'../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndata.head()\nn = len(data)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:02.723276Z","iopub.execute_input":"2021-11-25T10:44:02.723765Z","iopub.status.idle":"2021-11-25T10:44:02.828766Z","shell.execute_reply.started":"2021-11-25T10:44:02.723697Z","shell.execute_reply":"2021-11-25T10:44:02.827832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:02.830512Z","iopub.execute_input":"2021-11-25T10:44:02.830807Z","iopub.status.idle":"2021-11-25T10:44:02.839406Z","shell.execute_reply.started":"2021-11-25T10:44:02.830768Z","shell.execute_reply":"2021-11-25T10:44:02.83849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(r'../input/jigsaw-toxic-comment-classification-challenge/train.csv')\n#train_data = train_data[:n]\ntrain_data.head()\ntext = data['text']","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:02.841862Z","iopub.execute_input":"2021-11-25T10:44:02.842484Z","iopub.status.idle":"2021-11-25T10:44:04.792488Z","shell.execute_reply.started":"2021-11-25T10:44:02.842446Z","shell.execute_reply":"2021-11-25T10:44:04.791537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = []\nfor i in range(len(train_data)):\n    x = train_data.iloc[i]\n    s = sum(x['toxic':].values)\n    score.append(round(s/6, 3))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:04.793669Z","iopub.execute_input":"2021-11-25T10:44:04.793953Z","iopub.status.idle":"2021-11-25T10:44:35.396845Z","shell.execute_reply.started":"2021-11-25T10:44:04.793919Z","shell.execute_reply":"2021-11-25T10:44:35.395964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['score'] = score\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:35.398242Z","iopub.execute_input":"2021-11-25T10:44:35.398474Z","iopub.status.idle":"2021-11-25T10:44:35.473829Z","shell.execute_reply.started":"2021-11-25T10:44:35.398446Z","shell.execute_reply":"2021-11-25T10:44:35.47283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#   Sentence length in \ntext.str.len().hist()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:35.475101Z","iopub.execute_input":"2021-11-25T10:44:35.475329Z","iopub.status.idle":"2021-11-25T10:44:35.793294Z","shell.execute_reply.started":"2021-11-25T10:44:35.475301Z","shell.execute_reply":"2021-11-25T10:44:35.792304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word length Analysis\nimport matplotlib.pyplot as plt\narr = [ ]\nfor i in text:\n    \n    tmp = i.split(' ')\n    arr.append(len(tmp))\nplt.grid()\nplt.hist(arr)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:35.794925Z","iopub.execute_input":"2021-11-25T10:44:35.795276Z","iopub.status.idle":"2021-11-25T10:44:36.141856Z","shell.execute_reply.started":"2021-11-25T10:44:35.795229Z","shell.execute_reply":"2021-11-25T10:44:36.140806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')\nstop = set(stopwords.words('english'))\nimport numpy as np\ncorpus = []\ntmp = []\nfor i in text : \n    tmp.append(i.split())\n    \nfor i in tmp:\n    for words in i :\n        corpus.append(words)\nfrom collections import defaultdict\ndic = defaultdict(int)\nfor i in corpus: \n    if i in stop :\n        dic[i] = dic[i] + 1\nval = dic.values()\nval = sorted(val,reverse = True)\nval = val[0:11]\nd = {}\nfor i in list(dic.keys()):\n    if dic[i] in val: \n        d[i] = dic[i]\n# Top 10 Stopwords in corpus\nplt.bar(list(d.keys()), list(d.values()))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:36.143222Z","iopub.execute_input":"2021-11-25T10:44:36.143502Z","iopub.status.idle":"2021-11-25T10:44:56.74856Z","shell.execute_reply.started":"2021-11-25T10:44:36.14347Z","shell.execute_reply":"2021-11-25T10:44:56.747505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bigrams, trigrams\nfrom nltk.util import ngrams\nimport collections\ntex = ''\n#word_tokenize = nltk.download('word_tokenize')\nfor i in text : \n    tex = tex + str(i.strip('[]'))\ntok = tex.split()\nb_grams = ngrams(tok, 2) \nt_grams = ngrams(tok,3)\nbigrams = collections.Counter(b_grams)\ntrigrams = collections.Counter(t_grams)\nbi= bigrams.most_common(10)\nti= trigrams.most_common(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:56.749969Z","iopub.execute_input":"2021-11-25T10:44:56.750238Z","iopub.status.idle":"2021-11-25T10:44:57.995075Z","shell.execute_reply.started":"2021-11-25T10:44:56.750203Z","shell.execute_reply":"2021-11-25T10:44:57.994252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_ngrams(bi):\n    word = []\n    idx = []\n    for i in range(len(bi)):\n        word.append(str(bi[i][0]))\n        idx.append(bi[i][1])\n    plt.figure(figsize = (10,8))\n    plt.bar(word,idx)\n    plt.xticks(word, word, rotation = 'vertical')\n    plt.xlabel('Ngrams')\n    plt.ylabel('Frequency')\n    plt.title('Ngram with Frequency')\n    plt.show()\nplot_ngrams(bi)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:57.996331Z","iopub.execute_input":"2021-11-25T10:44:57.996592Z","iopub.status.idle":"2021-11-25T10:44:58.264343Z","shell.execute_reply.started":"2021-11-25T10:44:57.996559Z","shell.execute_reply":"2021-11-25T10:44:58.263363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_ngrams(ti)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:58.266206Z","iopub.execute_input":"2021-11-25T10:44:58.266571Z","iopub.status.idle":"2021-11-25T10:44:58.547479Z","shell.execute_reply.started":"2021-11-25T10:44:58.266512Z","shell.execute_reply":"2021-11-25T10:44:58.5465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"#Stopwords\nimport nltk\nnltk.download('stopwords')\nstop = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:44:58.552297Z","iopub.execute_input":"2021-11-25T10:44:58.552594Z","iopub.status.idle":"2021-11-25T10:45:18.590872Z","shell.execute_reply.started":"2021-11-25T10:44:58.552558Z","shell.execute_reply":"2021-11-25T10:45:18.589819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef lowercase(te):\n    tmp = []\n    #Complete \n    for i in te : \n        tmp.append(i.lower())\n    return tmp\ndef remove_symbols(text):\n    #commplete\n    tmp = []\n    for i in text: \n        #i = re.sub(r'\\n','',i)\n       # i = re.sub(r'\"\"', ' ',i)\n        i = re.sub(r'[^\\w]',' ',i) #Remove all types of symbols from string\n        tmp.append(i.replace(') ',''))\n    return tmp\ndef remove_stopwords(text): \n    tmp = []\n    for i in text: \n        te = str(i)\n        words = word_tokenize(te)\n        token_without = [word for word in words if not word in stop]\n        s = ''\n        for w in token_without : \n            s = s + w + ' '\n        tmp.append(s)\n    return tmp\ndef stemming(text): \n    tmp = []\n    for i in text : \n        te = str(i)\n        lemmatizer = WordNetLemmatizer()\n        words = word_tokenize(te)\n        s = ''\n        for w in words:\n            rw = lemmatizer.lemmatize(w)\n            s = s + rw + ' '\n        tmp.append(s)\n    return tmp\ndef stemming_lem_stop(text):\n    tmp = []\n    for i in text:\n        te = str(i)\n        words = word_tokenize(te)\n        ps = PorterStemmer()\n        lemmatizer = WordNetLemmatizer()\n        token_without = [word for word in words if not word in stop] #Remove Stopwords from token\n        s = ''\n        for w in token_without: \n            rootword = ps.stem(w)\n            rw = lemmatizer.lemmatize(rootword) #Lemmatization\n            s = s + rw + ' '\n        tmp.append(s)\n    return tmp\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:45:18.592299Z","iopub.execute_input":"2021-11-25T10:45:18.592539Z","iopub.status.idle":"2021-11-25T10:45:18.606691Z","shell.execute_reply.started":"2021-11-25T10:45:18.592511Z","shell.execute_reply":"2021-11-25T10:45:18.605624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1 = lowercase(train_data['comment_text'])\ntrain2 = remove_symbols(train1)\ntrain4 = stemming(train2)\ntrain3 = remove_stopwords(train4)\ntest1 = lowercase(text)\ntest2 = remove_symbols(test1)\ntest4 = stemming(test2)\ntest3 = remove_stopwords(test4)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:45:18.607984Z","iopub.execute_input":"2021-11-25T10:45:18.608573Z","iopub.status.idle":"2021-11-25T10:50:25.782421Z","shell.execute_reply.started":"2021-11-25T10:45:18.608517Z","shell.execute_reply":"2021-11-25T10:50:25.781469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Calculating Score for Training dataset**\nHere, I had calculated the score for training set with the help of TextBlob Sentiment Analysis \nfrom Jigsaw Toxic Comment Classification Competition Datasets","metadata":{}},{"cell_type":"code","source":"\npol = []\nfor i in train3:\n    analysis = TextBlob(i)\n    x = analysis.sentiment.polarity\n    NewValue = (((x - (-1)) * 1) / 2) + 0 #NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n    if NewValue == 0.5 :\n        pol.append(0)\n    else : \n        pol.append(round(NewValue,2))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:50:25.783654Z","iopub.execute_input":"2021-11-25T10:50:25.783926Z","iopub.status.idle":"2021-11-25T10:51:33.648133Z","shell.execute_reply.started":"2021-11-25T10:50:25.783895Z","shell.execute_reply":"2021-11-25T10:51:33.647275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train3)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:33.649675Z","iopub.execute_input":"2021-11-25T10:51:33.650114Z","iopub.status.idle":"2021-11-25T10:51:33.65556Z","shell.execute_reply.started":"2021-11-25T10:51:33.650078Z","shell.execute_reply":"2021-11-25T10:51:33.654766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"com = []\ndi = {}\npol1 = []\nfor i in range(len(data['comment_id'])): \n    com.append(data['comment_id'][i])\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:33.657058Z","iopub.execute_input":"2021-11-25T10:51:33.657334Z","iopub.status.idle":"2021-11-25T10:51:33.734413Z","shell.execute_reply.started":"2021-11-25T10:51:33.657288Z","shell.execute_reply":"2021-11-25T10:51:33.733487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Vectorization**","metadata":{}},{"cell_type":"code","source":"\ntfid = TfidfVectorizer(max_features = 50, min_df= 3, max_df=0.5, analyzer = 'word')\nres = tfid.fit_transform(train3).toarray()\nres1 = tfid.fit_transform(test3).toarray()\n#valid_res = tfid.fit_transform(valid_x).toarray()\n# dic = {}\n# for e1, e2 in zip(tfid.get_feature_names(), tfid.idf_): \n#     dic[e1] = e2\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:33.73596Z","iopub.execute_input":"2021-11-25T10:51:33.736272Z","iopub.status.idle":"2021-11-25T10:51:43.108702Z","shell.execute_reply.started":"2021-11-25T10:51:33.73623Z","shell.execute_reply":"2021-11-25T10:51:43.107758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res1[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:51:43.110129Z","iopub.execute_input":"2021-11-25T10:51:43.110364Z","iopub.status.idle":"2021-11-25T10:51:43.116286Z","shell.execute_reply.started":"2021-11-25T10:51:43.110334Z","shell.execute_reply":"2021-11-25T10:51:43.115449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define initial best params and MAE\\\nfrom numpy import arange\n\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    #'objective':'reg:linear',\n}\ngridsearch_params = [\n    (max_depth, min_child_weight, eta)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n    for eta in arange(0.1,1,0.1)\n]\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\ndtrain = xgb.DMatrix(res,pol)\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight, eta in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}, eta={}\".format(\n                             max_depth,\n                             min_child_weight,\n                             eta ))    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    params['eta'] = eta\n # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        #num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=5\n    )    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight, eta)\n        print(\"Best params: {}, {}, {}, MAE: {}\".format(best_params[0], best_params[1],best_params[2], min_mae))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:53:58.000536Z","iopub.execute_input":"2021-11-25T10:53:58.00085Z","iopub.status.idle":"2021-11-25T11:21:29.72519Z","shell.execute_reply.started":"2021-11-25T10:53:58.000815Z","shell.execute_reply":"2021-11-25T11:21:29.724046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = best_params[0]\nc = best_params[1]\ne = best_params[2]","metadata":{"execution":{"iopub.status.busy":"2021-11-25T11:21:29.728143Z","iopub.execute_input":"2021-11-25T11:21:29.729055Z","iopub.status.idle":"2021-11-25T11:21:29.735009Z","shell.execute_reply.started":"2021-11-25T11:21:29.728975Z","shell.execute_reply":"2021-11-25T11:21:29.733909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Modelling**","metadata":{}},{"cell_type":"code","source":"\nmodel = XGBRegressor(n_estimators = 1000,max_depth = d,min_child_weight= c,eta = e, subsample=0.7, colsample_bytree=0.8)\nmodel.fit(res,pol)\ny_pred = abs(model.predict(res1))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T11:21:35.344566Z","iopub.execute_input":"2021-11-25T11:21:35.344985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"di = {'comment_id': com, 'score': y_pred}\ndf = pd.DataFrame(di)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output file\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:53:25.296994Z","iopub.status.idle":"2021-11-25T10:53:25.297319Z","shell.execute_reply.started":"2021-11-25T10:53:25.297152Z","shell.execute_reply":"2021-11-25T10:53:25.297169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**If you like this Notebook, Please upvote.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}