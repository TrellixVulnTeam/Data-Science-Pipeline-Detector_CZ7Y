{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style='color:white;background-color:#f2dde8; height: 100px; border-radius: 25px;'><h1 style='text-align:center;padding: 3%'>Jigsaw Rate Severity of Toxic Comments Competition</h1></div>","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n* [Dataset Information](#data_information)\n    - Data description\n    - Files\n* [Preliminary Data Exploration](#preliminary_eda)\n    - Install and Import Libraries\n    - Load Data\n    - General Dataset Information\n* [Exploratory Data Analysis](#eda)\n    - [Comments to Score Dataset](#cts)\n        - Clean Text\n        - Language Detection\n        - Comment Length Distribution\n        - Word Count Distribution\n        - Distribution of Top Unigrams\n        - Distribution of Top Bigrams\n        - Distribution of Top Trigrams\n        - Unique Words Analysis\n        - Sentiment Polarity\n        - Word Clouds\n    - [Validation Dataset](#val)\n        - Clean Text\n        - Comment Length Distribution\n        - Word Count Distribution\n        - Distribution of Top Unigrams\n        - Distribution of Top Bigrams\n        - Distribution of Top Trigrams\n        - Unique Words Analysis\n        - Sentiment Polarity\n        - Word Clouds\n        - Worker Analysis","metadata":{"execution":{"iopub.status.busy":"2021-12-07T13:51:49.535501Z","iopub.execute_input":"2021-12-07T13:51:49.5358Z","iopub.status.idle":"2021-12-07T13:51:49.54137Z","shell.execute_reply.started":"2021-12-07T13:51:49.535765Z","shell.execute_reply":"2021-12-07T13:51:49.540412Z"}}},{"cell_type":"markdown","source":"<div style='color:#40192e;background-color:#f2dde8; height: 20px; border-radius: 5px;'></div>","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:51:03.08216Z","iopub.execute_input":"2021-12-12T11:51:03.082637Z","iopub.status.idle":"2021-12-12T11:51:03.102488Z","shell.execute_reply.started":"2021-12-12T11:51:03.082511Z","shell.execute_reply":"2021-12-12T11:51:03.100431Z"}}},{"cell_type":"markdown","source":"<a id='data_information'></a>\n# Dataset Information\n## Data Description","metadata":{"execution":{"iopub.status.busy":"2021-12-15T18:16:15.052188Z","iopub.execute_input":"2021-12-15T18:16:15.053807Z","iopub.status.idle":"2021-12-15T18:16:15.083691Z","shell.execute_reply.started":"2021-12-15T18:16:15.053146Z","shell.execute_reply":"2021-12-15T18:16:15.082843Z"}}},{"cell_type":"markdown","source":"<div>\nThe data used for this competition are Wikipedia Talk page comments. The purpose is to rank the severity of comment toxicity from innocuous to outrageous, where the middle matters as much as the extremes.</br>\n<b>Important</b>:\nThere is no training data for this competition. You can refer to previous Jigsaw competitions for data that might be useful to train models.\n\n<h3>Files</h3>\n<span style=\"background-color:#e1e6e3;\">comments_to_score.csv</span> - collection of comments </br>\n<span style=\"background-color:#e1e6e3;\">validation_data.csv</span> - pair rankings that can be used to validate models </br>\n<span style=\"background-color:#e1e6e3;\">sample_submission.csv</span> - a sample submission file in the correct format </br>\n</br>\n<b style='margin-top:1.5%;background-color:#fbffb3'><i>Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.</i></b></div>","metadata":{"execution":{"iopub.status.busy":"2021-12-07T13:26:35.94311Z","iopub.execute_input":"2021-12-07T13:26:35.943391Z","iopub.status.idle":"2021-12-07T13:26:35.949867Z","shell.execute_reply.started":"2021-12-07T13:26:35.943362Z","shell.execute_reply":"2021-12-07T13:26:35.948628Z"}}},{"cell_type":"markdown","source":"<div style='color:#40192e;background-color:#f2dde8; height: 20px; border-radius: 5px;'></div>","metadata":{}},{"cell_type":"markdown","source":"<a id='preliminary_eda'></a>\n# Preliminary Data Exploration","metadata":{}},{"cell_type":"markdown","source":"## Install and Import Libraries","metadata":{}},{"cell_type":"code","source":"! pip install langdetect","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-08T18:12:32.141986Z","iopub.execute_input":"2021-12-08T18:12:32.142571Z","iopub.status.idle":"2021-12-08T18:12:44.125703Z","shell.execute_reply.started":"2021-12-08T18:12:32.142476Z","shell.execute_reply":"2021-12-08T18:12:44.124823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport matplotlib.pyplot as plt\n\nfrom langdetect import detect\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\nimport random\n\n\nplt.style.use('ggplot')\n\npd.set_option('display.max_columns', None)\npd.set_option('max_colwidth', None)\npd.set_option('max_rows', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-15T08:04:01.17903Z","iopub.execute_input":"2021-12-15T08:04:01.17939Z","iopub.status.idle":"2021-12-15T08:04:01.271693Z","shell.execute_reply.started":"2021-12-15T08:04:01.179303Z","shell.execute_reply":"2021-12-15T08:04:01.270574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{"execution":{"iopub.status.busy":"2021-12-07T14:53:37.768729Z","iopub.execute_input":"2021-12-07T14:53:37.769023Z","iopub.status.idle":"2021-12-07T14:53:37.775089Z","shell.execute_reply.started":"2021-12-07T14:53:37.768992Z","shell.execute_reply":"2021-12-07T14:53:37.77424Z"}}},{"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\nval_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:12:45.713233Z","iopub.execute_input":"2021-12-08T18:12:45.713855Z","iopub.status.idle":"2021-12-08T18:12:46.324346Z","shell.execute_reply.started":"2021-12-08T18:12:45.713826Z","shell.execute_reply":"2021-12-08T18:12:46.323456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## General Dataset Information","metadata":{}},{"cell_type":"code","source":"print(f\"The shape of the Comments to Score dataset is {df.shape} \\n\"\nf\"The shape of the validation dataset is {val_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:12:46.326343Z","iopub.execute_input":"2021-12-08T18:12:46.326639Z","iopub.status.idle":"2021-12-08T18:12:46.332145Z","shell.execute_reply.started":"2021-12-08T18:12:46.3266Z","shell.execute_reply":"2021-12-08T18:12:46.331095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:12:46.33343Z","iopub.execute_input":"2021-12-08T18:12:46.333722Z","iopub.status.idle":"2021-12-08T18:12:46.36443Z","shell.execute_reply.started":"2021-12-08T18:12:46.333675Z","shell.execute_reply":"2021-12-08T18:12:46.3639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:12:46.365719Z","iopub.execute_input":"2021-12-08T18:12:46.366104Z","iopub.status.idle":"2021-12-08T18:12:46.380311Z","shell.execute_reply.started":"2021-12-08T18:12:46.366061Z","shell.execute_reply":"2021-12-08T18:12:46.379543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:12:46.381561Z","iopub.execute_input":"2021-12-08T18:12:46.382121Z","iopub.status.idle":"2021-12-08T18:12:46.395609Z","shell.execute_reply.started":"2021-12-08T18:12:46.38209Z","shell.execute_reply":"2021-12-08T18:12:46.39488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:12:46.396615Z","iopub.execute_input":"2021-12-08T18:12:46.39681Z","iopub.status.idle":"2021-12-08T18:12:46.40513Z","shell.execute_reply.started":"2021-12-08T18:12:46.396787Z","shell.execute_reply":"2021-12-08T18:12:46.404527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style='color:#40192e;background-color:#f2dde8; height: 20px; border-radius: 5px;'></div>","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:53:31.31387Z","iopub.execute_input":"2021-12-12T11:53:31.315033Z","iopub.status.idle":"2021-12-12T11:53:31.320002Z","shell.execute_reply.started":"2021-12-12T11:53:31.314984Z","shell.execute_reply":"2021-12-12T11:53:31.319016Z"}}},{"cell_type":"markdown","source":"<a id='eda'></a>\n# Exploratory Data Analysis","metadata":{"execution":{"iopub.status.busy":"2021-12-07T15:04:44.674648Z","iopub.execute_input":"2021-12-07T15:04:44.67531Z","iopub.status.idle":"2021-12-07T15:04:44.681217Z","shell.execute_reply.started":"2021-12-07T15:04:44.675237Z","shell.execute_reply":"2021-12-07T15:04:44.680153Z"}}},{"cell_type":"markdown","source":"<a id='cts'></a>\n# Comments to Score Dataset\n## Clean Text","metadata":{"execution":{"iopub.status.busy":"2021-12-07T15:10:09.567847Z","iopub.execute_input":"2021-12-07T15:10:09.568118Z","iopub.status.idle":"2021-12-07T15:10:09.574179Z","shell.execute_reply.started":"2021-12-07T15:10:09.568091Z","shell.execute_reply":"2021-12-07T15:10:09.57304Z"}}},{"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(r'<[^<]+?>', '', text)\n    text = text.replace('\\n', ' ')\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'<[^<]+?>', '', text) \n    text = text.replace('(\\xa0)', ' ')\n    text = text.replace('(&lt)', '')\n    text = text.replace('(&gt)', '')\n    text = text.replace(\"\\\\\", \"\")\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:12:46.406285Z","iopub.execute_input":"2021-12-08T18:12:46.406658Z","iopub.status.idle":"2021-12-08T18:12:46.41448Z","shell.execute_reply.started":"2021-12-08T18:12:46.40663Z","shell.execute_reply":"2021-12-08T18:12:46.413712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:12:46.416731Z","iopub.execute_input":"2021-12-08T18:12:46.416948Z","iopub.status.idle":"2021-12-08T18:12:46.65322Z","shell.execute_reply.started":"2021-12-08T18:12:46.416921Z","shell.execute_reply":"2021-12-08T18:12:46.652494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the first two clean comments:","metadata":{}},{"cell_type":"code","source":"df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:12:46.654333Z","iopub.execute_input":"2021-12-08T18:12:46.654784Z","iopub.status.idle":"2021-12-08T18:12:46.66449Z","shell.execute_reply.started":"2021-12-08T18:12:46.654744Z","shell.execute_reply":"2021-12-08T18:12:46.663708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Language Detection","metadata":{}},{"cell_type":"markdown","source":"Let's detect the language of each comment:","metadata":{}},{"cell_type":"code","source":"df['language'] = df['text'].apply(detect)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:12:46.665567Z","iopub.execute_input":"2021-12-08T18:12:46.666052Z","iopub.status.idle":"2021-12-08T18:13:29.216106Z","shell.execute_reply.started":"2021-12-08T18:12:46.666011Z","shell.execute_reply":"2021-12-08T18:13:29.215188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_all_language = df['language'].value_counts()\ncount_language_not_eng = df['language'][df.language != 'en'].value_counts()\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = count_all_language.plot(kind='bar', color = \"#640372\")\nax1.set_title('Frequency of languages in all comments')\nax1.set_xlabel(\"Languages\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = count_language_not_eng.plot(kind='bar', color = \"#640372\")\nax2.set_title('Frequency of languages in non-English comments')\nax2.set_xlabel(\"Languages\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:29.217174Z","iopub.execute_input":"2021-12-08T18:13:29.217365Z","iopub.status.idle":"2021-12-08T18:13:29.892899Z","shell.execute_reply.started":"2021-12-08T18:13:29.217341Z","shell.execute_reply":"2021-12-08T18:13:29.892086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comments classified as German:","metadata":{}},{"cell_type":"code","source":"df['text'][df.language=='de'].head(2)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-08T18:13:29.894125Z","iopub.execute_input":"2021-12-08T18:13:29.895055Z","iopub.status.idle":"2021-12-08T18:13:29.905119Z","shell.execute_reply.started":"2021-12-08T18:13:29.895007Z","shell.execute_reply":"2021-12-08T18:13:29.90424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comments classified as in Italian language:","metadata":{}},{"cell_type":"code","source":"df['text'][df.language=='it'].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:29.907294Z","iopub.execute_input":"2021-12-08T18:13:29.907573Z","iopub.status.idle":"2021-12-08T18:13:29.916934Z","shell.execute_reply.started":"2021-12-08T18:13:29.907533Z","shell.execute_reply":"2021-12-08T18:13:29.91612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Actually the comments are in English, even those classified as other language. Probably the high number of swear words/terms belonging to specific slangs affects the accuracy of \"langdetect\".","metadata":{}},{"cell_type":"markdown","source":"## Comment Length Distribution","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:02:53.468846Z","iopub.execute_input":"2021-12-07T16:02:53.469124Z","iopub.status.idle":"2021-12-07T16:02:53.474743Z","shell.execute_reply.started":"2021-12-07T16:02:53.469096Z","shell.execute_reply":"2021-12-07T16:02:53.473556Z"}}},{"cell_type":"code","source":"comment_length = df['text'].apply(len)\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = comment_length.plot(kind='hist', color = \"#640372\", bins=100)\nax1.set_title('Comment Length Distribution')\nax1.set_xlabel(\"Comment Length\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:29.918004Z","iopub.execute_input":"2021-12-08T18:13:29.918426Z","iopub.status.idle":"2021-12-08T18:13:30.429151Z","shell.execute_reply.started":"2021-12-08T18:13:29.918385Z","shell.execute_reply":"2021-12-08T18:13:30.428317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Count Distribution","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:16:54.141751Z","iopub.execute_input":"2021-12-07T16:16:54.142541Z","iopub.status.idle":"2021-12-07T16:16:54.148262Z","shell.execute_reply.started":"2021-12-07T16:16:54.142494Z","shell.execute_reply":"2021-12-07T16:16:54.147297Z"}}},{"cell_type":"code","source":"word_count = df['text'].apply(lambda x: len(str(x).split()))\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = word_count.plot(kind='hist', color = \"#640372\", bins=100)\nax1.set_title('Word Count Distribution')\nax1.set_xlabel(\"Word Count\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:30.430127Z","iopub.execute_input":"2021-12-08T18:13:30.430319Z","iopub.status.idle":"2021-12-08T18:13:30.81697Z","shell.execute_reply.started":"2021-12-08T18:13:30.430294Z","shell.execute_reply":"2021-12-08T18:13:30.816205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Top Unigrams","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:40:34.791656Z","iopub.execute_input":"2021-12-07T16:40:34.791946Z","iopub.status.idle":"2021-12-07T16:40:34.797871Z","shell.execute_reply.started":"2021-12-07T16:40:34.791913Z","shell.execute_reply":"2021-12-07T16:40:34.796664Z"}}},{"cell_type":"code","source":"def get_top_n_words(corpus, n=None, remove_stop_words=False, n_words=1): # if n_words=1 -> unigrams, if n_words=2 -> bigrams..\n    if remove_stop_words:\n        vec = CountVectorizer(stop_words = 'english', ngram_range=(n_words, n_words)).fit(corpus)\n    else:\n        vec = CountVectorizer(ngram_range=(n_words, n_words)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:30.818069Z","iopub.execute_input":"2021-12-08T18:13:30.818293Z","iopub.status.idle":"2021-12-08T18:13:30.824426Z","shell.execute_reply.started":"2021-12-08T18:13:30.818265Z","shell.execute_reply":"2021-12-08T18:13:30.823549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of top unigrams before removing stop words:","metadata":{"execution":{"iopub.status.busy":"2021-12-07T17:49:00.893409Z","iopub.execute_input":"2021-12-07T17:49:00.893708Z","iopub.status.idle":"2021-12-07T17:49:00.900478Z","shell.execute_reply.started":"2021-12-07T17:49:00.893678Z","shell.execute_reply":"2021-12-07T17:49:00.898875Z"}}},{"cell_type":"code","source":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=False, n_words=1)\nfor word, freq in common_words:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:30.825724Z","iopub.execute_input":"2021-12-08T18:13:30.825974Z","iopub.status.idle":"2021-12-08T18:13:31.789194Z","shell.execute_reply.started":"2021-12-08T18:13:30.825945Z","shell.execute_reply":"2021-12-08T18:13:31.788534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:31.790253Z","iopub.execute_input":"2021-12-08T18:13:31.791126Z","iopub.status.idle":"2021-12-08T18:13:32.088835Z","shell.execute_reply.started":"2021-12-08T18:13:31.791086Z","shell.execute_reply":"2021-12-08T18:13:32.088136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of top unigrams after removing stop words:","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:20:01.13868Z","iopub.execute_input":"2021-12-07T18:20:01.138964Z","iopub.status.idle":"2021-12-07T18:20:01.144167Z","shell.execute_reply.started":"2021-12-07T18:20:01.138935Z","shell.execute_reply":"2021-12-07T18:20:01.143176Z"}}},{"cell_type":"code","source":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=True, n_words=1)\nfor word, freq in common_words:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:32.089776Z","iopub.execute_input":"2021-12-08T18:13:32.090295Z","iopub.status.idle":"2021-12-08T18:13:32.92852Z","shell.execute_reply.started":"2021-12-08T18:13:32.090263Z","shell.execute_reply":"2021-12-08T18:13:32.9278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:32.929771Z","iopub.execute_input":"2021-12-08T18:13:32.930148Z","iopub.status.idle":"2021-12-08T18:13:33.202769Z","shell.execute_reply.started":"2021-12-08T18:13:32.930106Z","shell.execute_reply":"2021-12-08T18:13:33.202045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Top Bigrams","metadata":{"execution":{"iopub.status.busy":"2021-12-07T17:52:29.756885Z","iopub.execute_input":"2021-12-07T17:52:29.757189Z","iopub.status.idle":"2021-12-07T17:52:29.763929Z","shell.execute_reply.started":"2021-12-07T17:52:29.75715Z","shell.execute_reply":"2021-12-07T17:52:29.762682Z"}}},{"cell_type":"markdown","source":"Distribution of top bigrams before removing stop words:","metadata":{}},{"cell_type":"code","source":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=False, n_words=2)\nfor word, freq in common_words:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:33.20445Z","iopub.execute_input":"2021-12-08T18:13:33.204772Z","iopub.status.idle":"2021-12-08T18:13:35.621197Z","shell.execute_reply.started":"2021-12-08T18:13:33.204733Z","shell.execute_reply":"2021-12-08T18:13:35.620344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:35.622755Z","iopub.execute_input":"2021-12-08T18:13:35.623066Z","iopub.status.idle":"2021-12-08T18:13:35.918509Z","shell.execute_reply.started":"2021-12-08T18:13:35.623026Z","shell.execute_reply":"2021-12-08T18:13:35.91774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of top bigrams after removing stop words:","metadata":{"execution":{"iopub.status.busy":"2021-12-07T17:55:34.964474Z","iopub.execute_input":"2021-12-07T17:55:34.964753Z","iopub.status.idle":"2021-12-07T17:55:34.970629Z","shell.execute_reply.started":"2021-12-07T17:55:34.964715Z","shell.execute_reply":"2021-12-07T17:55:34.969397Z"}}},{"cell_type":"code","source":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=True, n_words=2)\nfor word, freq in common_words:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:35.920118Z","iopub.execute_input":"2021-12-08T18:13:35.920555Z","iopub.status.idle":"2021-12-08T18:13:37.756325Z","shell.execute_reply.started":"2021-12-08T18:13:35.920513Z","shell.execute_reply":"2021-12-08T18:13:37.755454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:37.7578Z","iopub.execute_input":"2021-12-08T18:13:37.758266Z","iopub.status.idle":"2021-12-08T18:13:38.04538Z","shell.execute_reply.started":"2021-12-08T18:13:37.758225Z","shell.execute_reply":"2021-12-08T18:13:38.044626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Top Trigrams","metadata":{}},{"cell_type":"markdown","source":"Distribution of top trigrams before removing stop words:","metadata":{}},{"cell_type":"code","source":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=False, n_words=3)\nfor word, freq in common_words:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:38.046733Z","iopub.execute_input":"2021-12-08T18:13:38.047204Z","iopub.status.idle":"2021-12-08T18:13:41.409818Z","shell.execute_reply.started":"2021-12-08T18:13:38.047161Z","shell.execute_reply":"2021-12-08T18:13:41.408896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Trigram Distribution')\nax1.set_xlabel(\"Trigram\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:41.415327Z","iopub.execute_input":"2021-12-08T18:13:41.415552Z","iopub.status.idle":"2021-12-08T18:13:41.714678Z","shell.execute_reply.started":"2021-12-08T18:13:41.415524Z","shell.execute_reply":"2021-12-08T18:13:41.713918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of top trigrams after removing stop words:","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:31:26.297003Z","iopub.execute_input":"2021-12-08T10:31:26.297251Z","iopub.status.idle":"2021-12-08T10:31:26.302624Z","shell.execute_reply.started":"2021-12-08T10:31:26.297225Z","shell.execute_reply":"2021-12-08T10:31:26.301391Z"}}},{"cell_type":"code","source":"common_words = get_top_n_words(df['text'], 20, remove_stop_words=True, n_words=3)\nfor word, freq in common_words:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:41.71578Z","iopub.execute_input":"2021-12-08T18:13:41.716029Z","iopub.status.idle":"2021-12-08T18:13:43.652548Z","shell.execute_reply.started":"2021-12-08T18:13:41.716001Z","shell.execute_reply":"2021-12-08T18:13:43.650586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:43.653659Z","iopub.execute_input":"2021-12-08T18:13:43.653855Z","iopub.status.idle":"2021-12-08T18:13:43.959488Z","shell.execute_reply.started":"2021-12-08T18:13:43.653831Z","shell.execute_reply":"2021-12-08T18:13:43.958708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The situation does not change much by keeping or removing the stop words in the case of trigrams.","metadata":{}},{"cell_type":"markdown","source":"## Unique Words Analysis","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:11:40.386148Z","iopub.execute_input":"2021-12-07T18:11:40.386489Z","iopub.status.idle":"2021-12-07T18:11:40.392088Z","shell.execute_reply.started":"2021-12-07T18:11:40.386457Z","shell.execute_reply":"2021-12-07T18:11:40.39124Z"}}},{"cell_type":"markdown","source":"You can see from these analyses that many comments are nothing more than repeated words. Let's try to do some analysis by considering only the unique words contained in a text. We perform this analysis by also removing anything that is not an alphabet character and making all text lowercase.","metadata":{}},{"cell_type":"code","source":"sorted(set([\"b\", \"a\"]))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:43.960407Z","iopub.execute_input":"2021-12-08T18:13:43.960903Z","iopub.status.idle":"2021-12-08T18:13:43.96651Z","shell.execute_reply.started":"2021-12-08T18:13:43.960853Z","shell.execute_reply":"2021-12-08T18:13:43.965799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_unique_words(string):\n    string = string.lower()\n    regex = re.compile('[^a-zA-Z]')\n    string = regex.sub(' ', string)\n    words = string.split()\n    new_string = \" \".join(sorted(set(words), key=words.index))\n    return new_string","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:43.967507Z","iopub.execute_input":"2021-12-08T18:13:43.968298Z","iopub.status.idle":"2021-12-08T18:13:43.977311Z","shell.execute_reply.started":"2021-12-08T18:13:43.968257Z","shell.execute_reply":"2021-12-08T18:13:43.976525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['set_of_words'] = df['text'].apply(get_unique_words)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:43.978428Z","iopub.execute_input":"2021-12-08T18:13:43.979022Z","iopub.status.idle":"2021-12-08T18:13:44.753794Z","shell.execute_reply.started":"2021-12-08T18:13:43.978986Z","shell.execute_reply":"2021-12-08T18:13:44.75302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:44.754963Z","iopub.execute_input":"2021-12-08T18:13:44.755173Z","iopub.status.idle":"2021-12-08T18:13:44.765223Z","shell.execute_reply.started":"2021-12-08T18:13:44.755148Z","shell.execute_reply":"2021-12-08T18:13:44.764331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the most frequent words after removing words present more than once from the texts.","metadata":{}},{"cell_type":"markdown","source":"Before removing stop words:","metadata":{}},{"cell_type":"code","source":"common_words = get_top_n_words(df['set_of_words'], 20, remove_stop_words=False, n_words=1)\nfor word, freq in common_words:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:44.768003Z","iopub.execute_input":"2021-12-08T18:13:44.768202Z","iopub.status.idle":"2021-12-08T18:13:45.435292Z","shell.execute_reply.started":"2021-12-08T18:13:44.768178Z","shell.execute_reply":"2021-12-08T18:13:45.434436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp = pd.DataFrame(common_words, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:45.436214Z","iopub.execute_input":"2021-12-08T18:13:45.436405Z","iopub.status.idle":"2021-12-08T18:13:45.704127Z","shell.execute_reply.started":"2021-12-08T18:13:45.436381Z","shell.execute_reply":"2021-12-08T18:13:45.703302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After removing stop words:","metadata":{}},{"cell_type":"code","source":"common_words = get_top_n_words(df['set_of_words'], 20, remove_stop_words=True, n_words=1)\nfor word, freq in common_words:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:45.705687Z","iopub.execute_input":"2021-12-08T18:13:45.705991Z","iopub.status.idle":"2021-12-08T18:13:46.314951Z","shell.execute_reply.started":"2021-12-08T18:13:45.705947Z","shell.execute_reply":"2021-12-08T18:13:46.314307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp = pd.DataFrame(common_words, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:46.315807Z","iopub.execute_input":"2021-12-08T18:13:46.316161Z","iopub.status.idle":"2021-12-08T18:13:46.588481Z","shell.execute_reply.started":"2021-12-08T18:13:46.316127Z","shell.execute_reply":"2021-12-08T18:13:46.587708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Several swear words are found to lose positions relative to the most frequent words, after removing repeated words. This can be traced to the fact that there are many comments within the dataset with swear words repeated many times. For example:","metadata":{}},{"cell_type":"code","source":"df['text'].iloc[3028]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-08T18:13:46.589613Z","iopub.execute_input":"2021-12-08T18:13:46.590305Z","iopub.status.idle":"2021-12-08T18:13:46.595281Z","shell.execute_reply.started":"2021-12-08T18:13:46.590274Z","shell.execute_reply":"2021-12-08T18:13:46.59456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].iloc[4949]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-08T18:13:46.59678Z","iopub.execute_input":"2021-12-08T18:13:46.597422Z","iopub.status.idle":"2021-12-08T18:13:46.607304Z","shell.execute_reply.started":"2021-12-08T18:13:46.597375Z","shell.execute_reply":"2021-12-08T18:13:46.606377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentiment Polarity","metadata":{}},{"cell_type":"markdown","source":"Let's use TextBlob to calculate sentiment polarity. The polarity value lies in the range of [-1, 1] where 1 means positive sentiment and -1 means a negative sentiment:","metadata":{}},{"cell_type":"code","source":"polarity = df['text'].map(lambda text: TextBlob(text).sentiment.polarity)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:46.6086Z","iopub.execute_input":"2021-12-08T18:13:46.609116Z","iopub.status.idle":"2021-12-08T18:13:50.701936Z","shell.execute_reply.started":"2021-12-08T18:13:46.609067Z","shell.execute_reply":"2021-12-08T18:13:50.701161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,8))\n\nax1 = polarity.plot(kind='hist', color = \"#640372\", bins=100)\nax1.set_title('Polarity Distribution')\nax1.set_xlabel(\"Sentiment\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:50.703165Z","iopub.execute_input":"2021-12-08T18:13:50.703442Z","iopub.status.idle":"2021-12-08T18:13:51.069423Z","shell.execute_reply.started":"2021-12-08T18:13:50.703407Z","shell.execute_reply":"2021-12-08T18:13:51.06859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most comments have zero polarity, so neutral sentiment, let's see if the polarity found in this way is reliable:","metadata":{}},{"cell_type":"code","source":"df['polarity'] = df['text'].map(lambda text: TextBlob(text).sentiment.polarity)\nprint(f\"\"\"A comment with the most neutral polarity: \\n {df['text'][df.polarity == 0].sample(1, random_state=42).values[0]} \\n\nA comment with negative polarity: \\n {df['text'][df.polarity == -1].sample(1, random_state=42).values[0]} \\n\nA comment with positive polarity: \\n {df['text'][df.polarity == 1].sample(1, random_state=42).values[0]}\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:51.070539Z","iopub.execute_input":"2021-12-08T18:13:51.070786Z","iopub.status.idle":"2021-12-08T18:13:55.052241Z","shell.execute_reply.started":"2021-12-08T18:13:51.070757Z","shell.execute_reply":"2021-12-08T18:13:55.051355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take 10 random comments with positive polarity:","metadata":{}},{"cell_type":"code","source":"print(\"10 comments with neutral polarity: \\n\")\ncomments = df.loc[df.polarity == 0, ['text']].sample(10, random_state=42).values\nfor comment in comments:\n    print(f\"\"\"- {comment[0]}\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:55.054942Z","iopub.execute_input":"2021-12-08T18:13:55.055359Z","iopub.status.idle":"2021-12-08T18:13:55.064356Z","shell.execute_reply.started":"2021-12-08T18:13:55.055321Z","shell.execute_reply":"2021-12-08T18:13:55.063879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"10 comments with positive polarity: \\n\")\ncomments = df.loc[df.polarity == 1, ['text']].sample(10, random_state=42).values\nfor comment in comments:\n    print(f\"\"\"- {comment[0]}\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:55.0651Z","iopub.execute_input":"2021-12-08T18:13:55.065528Z","iopub.status.idle":"2021-12-08T18:13:55.079297Z","shell.execute_reply.started":"2021-12-08T18:13:55.065496Z","shell.execute_reply":"2021-12-08T18:13:55.078546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can see that the polarity is not very accurate.","metadata":{}},{"cell_type":"markdown","source":"## Word Clouds","metadata":{}},{"cell_type":"markdown","source":"Word cloud of all the comments:","metadata":{}},{"cell_type":"code","source":"mask = np.array(Image.open(\"../input/wiki-img/Wikipedia_W.png\"))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:55.080505Z","iopub.execute_input":"2021-12-08T18:13:55.080742Z","iopub.status.idle":"2021-12-08T18:13:55.111921Z","shell.execute_reply.started":"2021-12-08T18:13:55.080698Z","shell.execute_reply":"2021-12-08T18:13:55.111263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = mask[:,:,3]\ntext = df.text.values","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:55.112924Z","iopub.execute_input":"2021-12-08T18:13:55.113232Z","iopub.status.idle":"2021-12-08T18:13:55.11694Z","shell.execute_reply.started":"2021-12-08T18:13:55.113206Z","shell.execute_reply":"2021-12-08T18:13:55.116236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def purple_color_func(word, font_size, position, orientation, random_state=None,\n                    **kwargs):\n    return f\"hsl(312, {random.randint(20, 60)}%, {random.randint(20, 60)}%)\"","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:55.11784Z","iopub.execute_input":"2021-12-08T18:13:55.118375Z","iopub.status.idle":"2021-12-08T18:13:55.128054Z","shell.execute_reply.started":"2021-12-08T18:13:55.118347Z","shell.execute_reply":"2021-12-08T18:13:55.127245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc= WordCloud(background_color=\"#fcebff\",max_words=1000,mask=mask,stopwords=set(STOPWORDS))\nwc.generate(\" \".join(text))\nplt.figure(figsize=(15,10))\nplt.axis(\"off\")\nplt.title(\"Word Cloud\", fontsize=20)\nplt.imshow(wc.recolor(color_func=purple_color_func, random_state=42),\n           interpolation=\"bilinear\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:55.129114Z","iopub.execute_input":"2021-12-08T18:13:55.129466Z","iopub.status.idle":"2021-12-08T18:13:59.200461Z","shell.execute_reply.started":"2021-12-08T18:13:55.12944Z","shell.execute_reply":"2021-12-08T18:13:59.199639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Word clouds based on sentiment:","metadata":{}},{"cell_type":"code","source":"mask_joy = np.array(Image.open(\"../input/emoji-imgs/joy.png\"))\nmask_sad = np.array(Image.open(\"../input/emoji-imgs/sad.png\"))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:59.201469Z","iopub.execute_input":"2021-12-08T18:13:59.202059Z","iopub.status.idle":"2021-12-08T18:13:59.248037Z","shell.execute_reply.started":"2021-12-08T18:13:59.20201Z","shell.execute_reply":"2021-12-08T18:13:59.247211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_joy = mask_joy[:,:,1]\nmask_sad = mask_sad[:,:,3]","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:59.249053Z","iopub.execute_input":"2021-12-08T18:13:59.249403Z","iopub.status.idle":"2021-12-08T18:13:59.253313Z","shell.execute_reply.started":"2021-12-08T18:13:59.249373Z","shell.execute_reply":"2021-12-08T18:13:59.25272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_positive_polarity = df[(df.polarity > 0.9)].text.values\ntext_negative_polarity = df[(df.polarity < -0.9)].text.values","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:59.254204Z","iopub.execute_input":"2021-12-08T18:13:59.254623Z","iopub.status.idle":"2021-12-08T18:13:59.268013Z","shell.execute_reply.started":"2021-12-08T18:13:59.254592Z","shell.execute_reply":"2021-12-08T18:13:59.267169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc_positive_polarity = WordCloud(background_color=\"#fcebff\",max_words=1000,mask=mask_joy,stopwords=set(STOPWORDS))\nwc_negative_polarity = WordCloud(background_color=\"#fcebff\",max_words=1000,mask=mask_sad,stopwords=set(STOPWORDS))\n\nwc_positive_polarity.generate(\" \".join(text_positive_polarity))\nwc_negative_polarity.generate(\" \".join(text_negative_polarity))\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_positive_polarity.recolor(color_func=purple_color_func, random_state=42),\n           interpolation=\"bilinear\")\nax1 = plt.title(\"Positive Polarity Comments\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_negative_polarity.recolor(color_func=purple_color_func, random_state=42),\n           interpolation=\"bilinear\")\nax2 = plt.title(\"Negative Polarity Comments\", fontsize=20)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:13:59.269064Z","iopub.execute_input":"2021-12-08T18:13:59.269361Z","iopub.status.idle":"2021-12-08T18:14:05.762367Z","shell.execute_reply.started":"2021-12-08T18:13:59.26933Z","shell.execute_reply":"2021-12-08T18:14:05.761746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style='color:#40192e;background-color:#f2dde8; height: 20px; border-radius: 5px;'></div>","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:53:56.942244Z","iopub.execute_input":"2021-12-12T11:53:56.942521Z","iopub.status.idle":"2021-12-12T11:53:56.948033Z","shell.execute_reply.started":"2021-12-12T11:53:56.942492Z","shell.execute_reply":"2021-12-12T11:53:56.947114Z"}}},{"cell_type":"markdown","source":"<a id='val'></a>\n# Validation Dataset","metadata":{"execution":{"iopub.status.busy":"2021-12-07T15:14:39.357103Z","iopub.execute_input":"2021-12-07T15:14:39.357444Z","iopub.status.idle":"2021-12-07T15:14:39.363072Z","shell.execute_reply.started":"2021-12-07T15:14:39.357407Z","shell.execute_reply":"2021-12-07T15:14:39.362105Z"}}},{"cell_type":"markdown","source":"Let's proceed with the exploration of the validation dataset, in which we have for each row two comments, classified as 'more toxic' or 'less toxic'. This evaluation is done by considering only those two comments.","metadata":{}},{"cell_type":"code","source":"val_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:05.76334Z","iopub.execute_input":"2021-12-08T18:14:05.763621Z","iopub.status.idle":"2021-12-08T18:14:05.7727Z","shell.execute_reply.started":"2021-12-08T18:14:05.763595Z","shell.execute_reply":"2021-12-08T18:14:05.771977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean Text","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:47:08.812088Z","iopub.execute_input":"2021-12-08T10:47:08.812364Z","iopub.status.idle":"2021-12-08T10:47:08.817913Z","shell.execute_reply.started":"2021-12-08T10:47:08.812335Z","shell.execute_reply":"2021-12-08T10:47:08.816851Z"}}},{"cell_type":"code","source":"val_df['less_toxic'] = val_df['less_toxic'].apply(clean_text)\nval_df['more_toxic'] = val_df['more_toxic'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:05.773735Z","iopub.execute_input":"2021-12-08T18:14:05.773974Z","iopub.status.idle":"2021-12-08T18:14:07.563013Z","shell.execute_reply.started":"2021-12-08T18:14:05.773938Z","shell.execute_reply":"2021-12-08T18:14:07.56214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comment Length Distribution","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:55:44.669331Z","iopub.execute_input":"2021-12-08T10:55:44.669752Z","iopub.status.idle":"2021-12-08T10:55:44.674897Z","shell.execute_reply.started":"2021-12-08T10:55:44.669722Z","shell.execute_reply":"2021-12-08T10:55:44.673914Z"}}},{"cell_type":"markdown","source":"Let's look at the length of the comments according to the column they belong to:","metadata":{}},{"cell_type":"code","source":"comment_length_less_toxic = val_df['less_toxic'].apply(len)\ncomment_length_more_toxic = val_df['more_toxic'].apply(len)\n\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = comment_length_less_toxic.plot(kind='hist', color = \"#ff7033\", bins=100, alpha=1)\nax1 = comment_length_more_toxic.plot(kind='hist', color = \"#622864\", bins=100, alpha=0.6)\nax1.set_title('Comment Length Distribution More Toxic vs Less Toxic')\nax1.set_xlabel(\"Comment Length\")\nax1.set_ylabel(\"Frequency\")\nax1.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:07.564299Z","iopub.execute_input":"2021-12-08T18:14:07.565113Z","iopub.status.idle":"2021-12-08T18:14:08.198184Z","shell.execute_reply.started":"2021-12-08T18:14:07.565076Z","shell.execute_reply":"2021-12-08T18:14:08.197125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note how the most toxic comments tend to have shorter lengths in general, however, with a peak for length 500.","metadata":{}},{"cell_type":"markdown","source":"## Word Count Distribution","metadata":{"execution":{"iopub.status.busy":"2021-12-08T11:05:10.140718Z","iopub.execute_input":"2021-12-08T11:05:10.141147Z","iopub.status.idle":"2021-12-08T11:05:10.147065Z","shell.execute_reply.started":"2021-12-08T11:05:10.141116Z","shell.execute_reply":"2021-12-08T11:05:10.145955Z"}}},{"cell_type":"code","source":"word_count_less_toxic = val_df['less_toxic'].apply(lambda x: len(str(x).split()))\nword_count_more_toxic = val_df['more_toxic'].apply(lambda x: len(str(x).split()))\n\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = word_count_less_toxic.plot(kind='hist', color = \"#ff7033\", bins=100)\nax1 = word_count_more_toxic.plot(kind='hist', color = \"#622864\", bins=100, alpha=0.7)\nax1.set_title('Word Count Distribution More Toxic vs Less Toxic')\nax1.set_xlabel(\"Word Count\")\nax1.set_ylabel(\"Frequency\")\nax1.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:08.199516Z","iopub.execute_input":"2021-12-08T18:14:08.199793Z","iopub.status.idle":"2021-12-08T18:14:09.095585Z","shell.execute_reply.started":"2021-12-08T18:14:08.199752Z","shell.execute_reply":"2021-12-08T18:14:09.094813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In terms of word count, the most toxic comments have a lower word count on average than the least toxic comments.","metadata":{}},{"cell_type":"markdown","source":"## Distribution of Top Unigrams","metadata":{}},{"cell_type":"markdown","source":"Distribution of top unigrams before removing stop words:","metadata":{"execution":{"iopub.status.busy":"2021-12-08T11:19:17.411203Z","iopub.execute_input":"2021-12-08T11:19:17.411492Z","iopub.status.idle":"2021-12-08T11:19:17.416671Z","shell.execute_reply.started":"2021-12-08T11:19:17.41145Z","shell.execute_reply":"2021-12-08T11:19:17.415614Z"}}},{"cell_type":"code","source":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=False, n_words=1)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=False, n_words=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:09.096941Z","iopub.execute_input":"2021-12-08T18:14:09.097458Z","iopub.status.idle":"2021-12-08T18:14:15.710546Z","shell.execute_reply.started":"2021-12-08T18:14:09.097419Z","shell.execute_reply":"2021-12-08T18:14:15.709907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Unigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:15.711445Z","iopub.execute_input":"2021-12-08T18:14:15.711992Z","iopub.status.idle":"2021-12-08T18:14:16.197231Z","shell.execute_reply.started":"2021-12-08T18:14:15.711953Z","shell.execute_reply":"2021-12-08T18:14:16.196381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of top unigrams before removing stop words:","metadata":{}},{"cell_type":"code","source":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=True, n_words=1)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=True, n_words=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:16.198695Z","iopub.execute_input":"2021-12-08T18:14:16.199124Z","iopub.status.idle":"2021-12-08T18:14:22.394078Z","shell.execute_reply.started":"2021-12-08T18:14:16.199085Z","shell.execute_reply":"2021-12-08T18:14:22.393207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Unigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:22.395624Z","iopub.execute_input":"2021-12-08T18:14:22.395912Z","iopub.status.idle":"2021-12-08T18:14:22.95099Z","shell.execute_reply.started":"2021-12-08T18:14:22.395872Z","shell.execute_reply":"2021-12-08T18:14:22.950164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see a lot of difference between the top unigrams of the least toxic comments, where there are few swear words, compared to those of the most toxic comments.","metadata":{}},{"cell_type":"markdown","source":"## Distribution of Top Bigrams","metadata":{}},{"cell_type":"markdown","source":"Distribution of top bigrams before removing stop words:","metadata":{}},{"cell_type":"code","source":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=False, n_words=2)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=False, n_words=2)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:22.952359Z","iopub.execute_input":"2021-12-08T18:14:22.952634Z","iopub.status.idle":"2021-12-08T18:14:35.90471Z","shell.execute_reply.started":"2021-12-08T18:14:22.952596Z","shell.execute_reply":"2021-12-08T18:14:35.903814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Bigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Bigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:35.905785Z","iopub.execute_input":"2021-12-08T18:14:35.906028Z","iopub.status.idle":"2021-12-08T18:14:36.533634Z","shell.execute_reply.started":"2021-12-08T18:14:35.905999Z","shell.execute_reply":"2021-12-08T18:14:36.532786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As for bigrams, even without removing the stop words we can see how the comments differ between the two categories.","metadata":{}},{"cell_type":"markdown","source":"Distribution of top bigrams after removing stop words:","metadata":{"execution":{"iopub.status.busy":"2021-12-08T11:25:20.170122Z","iopub.execute_input":"2021-12-08T11:25:20.170422Z","iopub.status.idle":"2021-12-08T11:25:20.175772Z","shell.execute_reply.started":"2021-12-08T11:25:20.170388Z","shell.execute_reply":"2021-12-08T11:25:20.175006Z"}}},{"cell_type":"code","source":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=True, n_words=2)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=True, n_words=2)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:36.534955Z","iopub.execute_input":"2021-12-08T18:14:36.535686Z","iopub.status.idle":"2021-12-08T18:14:46.779037Z","shell.execute_reply.started":"2021-12-08T18:14:36.535639Z","shell.execute_reply":"2021-12-08T18:14:46.778329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Bigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Bigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:46.780126Z","iopub.execute_input":"2021-12-08T18:14:46.780334Z","iopub.status.idle":"2021-12-08T18:14:47.385613Z","shell.execute_reply.started":"2021-12-08T18:14:46.78031Z","shell.execute_reply":"2021-12-08T18:14:47.384821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Several 'toxic' words can be spotted in the less toxic comments. This is because one comment is defined as less toxic than another, there is no guarantee that it is not toxic in general.","metadata":{}},{"cell_type":"markdown","source":"## Distribution of Top Trigrams","metadata":{"execution":{"iopub.status.busy":"2021-12-08T11:27:53.284847Z","iopub.execute_input":"2021-12-08T11:27:53.285158Z","iopub.status.idle":"2021-12-08T11:27:53.290998Z","shell.execute_reply.started":"2021-12-08T11:27:53.285126Z","shell.execute_reply":"2021-12-08T11:27:53.289829Z"}}},{"cell_type":"markdown","source":"Distribution of top trigrams before removing stop words:","metadata":{"execution":{"iopub.status.busy":"2021-12-08T11:28:15.429358Z","iopub.execute_input":"2021-12-08T11:28:15.429993Z","iopub.status.idle":"2021-12-08T11:28:15.43524Z","shell.execute_reply.started":"2021-12-08T11:28:15.429954Z","shell.execute_reply":"2021-12-08T11:28:15.434242Z"}}},{"cell_type":"code","source":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=False, n_words=3)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=False, n_words=3)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:14:47.386892Z","iopub.execute_input":"2021-12-08T18:14:47.387648Z","iopub.status.idle":"2021-12-08T18:15:03.876701Z","shell.execute_reply.started":"2021-12-08T18:14:47.387597Z","shell.execute_reply":"2021-12-08T18:15:03.875881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Trigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Trigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:15:03.877725Z","iopub.execute_input":"2021-12-08T18:15:03.877954Z","iopub.status.idle":"2021-12-08T18:15:04.518037Z","shell.execute_reply.started":"2021-12-08T18:15:03.877925Z","shell.execute_reply":"2021-12-08T18:15:04.517206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of top trigrams after removing stop words:","metadata":{}},{"cell_type":"code","source":"common_words_less_toxic = get_top_n_words(val_df['less_toxic'], 20, remove_stop_words=True, n_words=3)\ncommon_words_more_toxic = get_top_n_words(val_df['more_toxic'], 20, remove_stop_words=True, n_words=3)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:15:04.519109Z","iopub.execute_input":"2021-12-08T18:15:04.519313Z","iopub.status.idle":"2021-12-08T18:15:15.11736Z","shell.execute_reply.started":"2021-12-08T18:15:04.519287Z","shell.execute_reply":"2021-12-08T18:15:15.116616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Trigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Trigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:15:15.12059Z","iopub.execute_input":"2021-12-08T18:15:15.120826Z","iopub.status.idle":"2021-12-08T18:15:16.005219Z","shell.execute_reply.started":"2021-12-08T18:15:15.120797Z","shell.execute_reply":"2021-12-08T18:15:16.00425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same thing about bigrams applies here. Toxic trigrams are very frequently present in less toxic comments, this is because one comment is defined as less toxic than another, there is no guarantee that it is not toxic in general.","metadata":{}},{"cell_type":"markdown","source":"## Unique Words Analysis","metadata":{}},{"cell_type":"markdown","source":"Like for the dataset of the previous section, we go to see the unigrams present in the text most frequently after selecting only the unique words of the comments.","metadata":{}},{"cell_type":"code","source":"val_df['set_of_words_less_toxic'] = val_df['less_toxic'].apply(get_unique_words)\nval_df['set_of_words_more_toxic'] = val_df['more_toxic'].apply(get_unique_words)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:15:16.00655Z","iopub.execute_input":"2021-12-08T18:15:16.006876Z","iopub.status.idle":"2021-12-08T18:15:21.714653Z","shell.execute_reply.started":"2021-12-08T18:15:16.00682Z","shell.execute_reply":"2021-12-08T18:15:21.714072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of top unigrams before removing stop words:","metadata":{"execution":{"iopub.status.busy":"2021-12-08T11:41:36.470583Z","iopub.execute_input":"2021-12-08T11:41:36.471217Z","iopub.status.idle":"2021-12-08T11:41:36.476401Z","shell.execute_reply.started":"2021-12-08T11:41:36.471169Z","shell.execute_reply":"2021-12-08T11:41:36.475583Z"}}},{"cell_type":"code","source":"common_words_less_toxic = get_top_n_words(val_df['set_of_words_less_toxic'], 20, remove_stop_words=False, n_words=1)\ncommon_words_more_toxic = get_top_n_words(val_df['set_of_words_more_toxic'], 20, remove_stop_words=False, n_words=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:15:21.716096Z","iopub.execute_input":"2021-12-08T18:15:21.716401Z","iopub.status.idle":"2021-12-08T18:15:26.325036Z","shell.execute_reply.started":"2021-12-08T18:15:21.716358Z","shell.execute_reply":"2021-12-08T18:15:26.324206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Unigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:15:26.326354Z","iopub.execute_input":"2021-12-08T18:15:26.326566Z","iopub.status.idle":"2021-12-08T18:15:26.794649Z","shell.execute_reply.started":"2021-12-08T18:15:26.326538Z","shell.execute_reply":"2021-12-08T18:15:26.79397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of top unigrams after removing stop words:","metadata":{"execution":{"iopub.status.busy":"2021-12-08T11:41:43.731824Z","iopub.execute_input":"2021-12-08T11:41:43.732668Z","iopub.status.idle":"2021-12-08T11:41:43.737772Z","shell.execute_reply.started":"2021-12-08T11:41:43.732626Z","shell.execute_reply":"2021-12-08T11:41:43.736617Z"}}},{"cell_type":"code","source":"common_words_less_toxic = get_top_n_words(val_df['set_of_words_less_toxic'], 20, remove_stop_words=True, n_words=1)\ncommon_words_more_toxic = get_top_n_words(val_df['set_of_words_more_toxic'], 20, remove_stop_words=True, n_words=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:15:26.795688Z","iopub.execute_input":"2021-12-08T18:15:26.796456Z","iopub.status.idle":"2021-12-08T18:15:31.098806Z","shell.execute_reply.started":"2021-12-08T18:15:26.796412Z","shell.execute_reply":"2021-12-08T18:15:31.098113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp_less_toxic = pd.DataFrame(common_words_less_toxic, columns = ['set_of_words' , 'count'])\ndf_tmp_more_toxic = pd.DataFrame(common_words_more_toxic, columns = ['set_of_words' , 'count'])\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_less_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax1.set_title('Unigram Distribution for Less Toxic Comments')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_more_toxic.groupby('set_of_words').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#640372\")\nax2.set_title('Unigram Distribution for More Toxic Comments')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:15:31.100033Z","iopub.execute_input":"2021-12-08T18:15:31.100259Z","iopub.status.idle":"2021-12-08T18:15:31.565581Z","shell.execute_reply.started":"2021-12-08T18:15:31.100232Z","shell.execute_reply":"2021-12-08T18:15:31.564734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen before several swear words are found to lose positions relative to the most frequent words, after removing repeated words.This can be traced to the fact that there are many comments within the dataset with swear words repeated many times.","metadata":{}},{"cell_type":"markdown","source":"## Sentiment Polarity","metadata":{"execution":{"iopub.status.busy":"2021-12-08T11:45:47.562628Z","iopub.execute_input":"2021-12-08T11:45:47.563282Z","iopub.status.idle":"2021-12-08T11:45:47.569186Z","shell.execute_reply.started":"2021-12-08T11:45:47.563235Z","shell.execute_reply":"2021-12-08T11:45:47.567907Z"}}},{"cell_type":"markdown","source":"Let's use TextBlob to calculate sentiment polarity. The sentiment polarity value lies in the range of [-1, 1] where 1 means positive sentiment and -1 means a negative sentiment:","metadata":{}},{"cell_type":"code","source":"polarity_less_toxic = val_df['less_toxic'].map(lambda text: TextBlob(text).sentiment.polarity)\npolarity_more_toxic = val_df['more_toxic'].map(lambda text: TextBlob(text).sentiment.polarity)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:15:31.567131Z","iopub.execute_input":"2021-12-08T18:15:31.56739Z","iopub.status.idle":"2021-12-08T18:16:03.529021Z","shell.execute_reply.started":"2021-12-08T18:15:31.567362Z","shell.execute_reply":"2021-12-08T18:16:03.528243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,8))\n\nax1 = polarity_less_toxic.plot(kind='hist', color = \"#622864\", bins=100)\nax1 = polarity_more_toxic.plot(kind='hist', color = \"#ff7033\", bins=100, alpha=0.7)\nax1.set_title('Polarity Distribution Less vs More Toxic Comments')\nax1.set_xlabel(\"Sentiment\")\nax1.set_ylabel(\"Frequency\")\nax1.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:16:03.530413Z","iopub.execute_input":"2021-12-08T18:16:03.530779Z","iopub.status.idle":"2021-12-08T18:16:04.17376Z","shell.execute_reply.started":"2021-12-08T18:16:03.530739Z","shell.execute_reply":"2021-12-08T18:16:04.173221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the less toxic comments have a more neutral or positive polarity. While comments classified as toxic have a more negative polarity.","metadata":{}},{"cell_type":"markdown","source":"## Word Clouds","metadata":{}},{"cell_type":"code","source":"text_less_toxic = val_df['less_toxic'].values\ntext_more_toxic = val_df['more_toxic'].values","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:16:04.174683Z","iopub.execute_input":"2021-12-08T18:16:04.175061Z","iopub.status.idle":"2021-12-08T18:16:04.179217Z","shell.execute_reply.started":"2021-12-08T18:16:04.175033Z","shell.execute_reply":"2021-12-08T18:16:04.17844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc_less_toxic = WordCloud(background_color=\"#fcebff\",max_words=1000,mask=mask_joy,stopwords=set(STOPWORDS))\nwc_more_toxic = WordCloud(background_color=\"#fcebff\",max_words=1000,mask=mask_sad,stopwords=set(STOPWORDS))\n\nwc_less_toxic.generate(\" \".join(text_less_toxic))\nwc_more_toxic.generate(\" \".join(text_more_toxic))\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_less_toxic.recolor(color_func=purple_color_func, random_state=42),\n           interpolation=\"bilinear\")\nax1 = plt.title(\"Word Cloud Less Toxic Comments\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_more_toxic.recolor(color_func=purple_color_func, random_state=42),\n           interpolation=\"bilinear\")\nax2 = plt.title(\"Word Cloud More Toxic Comments\", fontsize=20)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:16:04.180324Z","iopub.execute_input":"2021-12-08T18:16:04.180518Z","iopub.status.idle":"2021-12-08T18:16:29.747632Z","shell.execute_reply.started":"2021-12-08T18:16:04.180495Z","shell.execute_reply":"2021-12-08T18:16:29.746908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Worker Analysis","metadata":{"execution":{"iopub.status.busy":"2021-12-08T12:49:19.705188Z","iopub.execute_input":"2021-12-08T12:49:19.705469Z","iopub.status.idle":"2021-12-08T12:49:19.711798Z","shell.execute_reply.started":"2021-12-08T12:49:19.705443Z","shell.execute_reply":"2021-12-08T12:49:19.710857Z"}}},{"cell_type":"code","source":"print(f'We have {val_df.worker.nunique()} different workers')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:16:29.748964Z","iopub.execute_input":"2021-12-08T18:16:29.749684Z","iopub.status.idle":"2021-12-08T18:16:29.754715Z","shell.execute_reply.started":"2021-12-08T18:16:29.749641Z","shell.execute_reply":"2021-12-08T18:16:29.754042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see if the order of toxicity between two comments belonging to a pair is consistent across workers.","metadata":{}},{"cell_type":"code","source":"dict_check = {}\nfor i in range(val_df.shape[0]):\n    comment = [val_df['less_toxic'][i], val_df['more_toxic'][i]]\n    key_comment = comment.copy()\n    key_comment.sort()\n    key_comment = tuple(key_comment)\n    if key_comment not in dict_check.keys():\n        dict_check[key_comment] = {}\n        dict_check[key_comment]['less_more'] = 0\n        dict_check[key_comment]['more_less'] = 0\n        if comment[0]<comment[1]:\n            dict_check[key_comment]['less_more'] += 1\n        else:\n            dict_check[key_comment]['more_less'] += 1\n    else:\n        if comment[0]<comment[1]:\n            dict_check[key_comment]['less_more'] += 1\n        else:\n            dict_check[key_comment]['more_less'] += 1","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:16:29.755878Z","iopub.execute_input":"2021-12-08T18:16:29.756261Z","iopub.status.idle":"2021-12-08T18:16:30.143843Z","shell.execute_reply.started":"2021-12-08T18:16:29.756231Z","shell.execute_reply":"2021-12-08T18:16:30.142794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Unlike the total pairs of comments in the dataset ({val_df.shape[0]}) the unique pairs of comments are much less:\\n'\n      f'number of unique pairs of comments: {len(dict_check.keys())}')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:16:30.145094Z","iopub.execute_input":"2021-12-08T18:16:30.145334Z","iopub.status.idle":"2021-12-08T18:16:30.150463Z","shell.execute_reply.started":"2021-12-08T18:16:30.145302Z","shell.execute_reply":"2021-12-08T18:16:30.149561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at how many comments were ranked differently by different workers:","metadata":{"execution":{"iopub.status.busy":"2021-12-08T15:03:44.361817Z","iopub.execute_input":"2021-12-08T15:03:44.362451Z","iopub.status.idle":"2021-12-08T15:03:44.368757Z","shell.execute_reply.started":"2021-12-08T15:03:44.362409Z","shell.execute_reply":"2021-12-08T15:03:44.367856Z"}}},{"cell_type":"code","source":"key_col = []\nless_more_col = []\nmore_less_col = []\nfor key in dict_check.keys():\n    key_col.append(key)\n    less_more_col.append(dict_check[key]['less_more'])\n    more_less_col.append(dict_check[key]['more_less'])\nfinal_df = pd.concat([pd.Series(key_col), pd.Series(less_more_col), pd.Series(more_less_col)], axis = 1)\nfinal_df.columns = ['comments', 'less_more', 'more_less']","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:16:30.158296Z","iopub.execute_input":"2021-12-08T18:16:30.158668Z","iopub.status.idle":"2021-12-08T18:16:30.1838Z","shell.execute_reply.started":"2021-12-08T18:16:30.15864Z","shell.execute_reply":"2021-12-08T18:16:30.182785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_time = np.max(final_df['less_more']+final_df['more_less'])\nprint(f'Maximum number of times a single comment pair was rated: {max_time}')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:16:30.184984Z","iopub.execute_input":"2021-12-08T18:16:30.185306Z","iopub.status.idle":"2021-12-08T18:16:30.200143Z","shell.execute_reply.started":"2021-12-08T18:16:30.185278Z","shell.execute_reply":"2021-12-08T18:16:30.199285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"differently_classified = final_df[(final_df.less_more>0)&(final_df.more_less>0)]","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:16:30.201537Z","iopub.execute_input":"2021-12-08T18:16:30.202003Z","iopub.status.idle":"2021-12-08T18:16:30.21061Z","shell.execute_reply.started":"2021-12-08T18:16:30.201964Z","shell.execute_reply":"2021-12-08T18:16:30.210027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The number of comment pairs ranked differently based on who ranked them is: {differently_classified.shape[0]} out of a total of {len(dict_check.keys())}. \\n\"\n\"This is expected, often both comments contain terms that are considered toxic, so it is subjective how these two comments are categorized into 'more toxic' or 'less toxic'.\")","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:17:15.8912Z","iopub.execute_input":"2021-12-08T18:17:15.891956Z","iopub.status.idle":"2021-12-08T18:17:15.896133Z","shell.execute_reply.started":"2021-12-08T18:17:15.891921Z","shell.execute_reply":"2021-12-08T18:17:15.895455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style='color:white;background-color:#f2dde8; height: 50px; border-radius: 25px;'><h1 style='text-align:center;padding: 1%'>The End</h1></div>","metadata":{}},{"cell_type":"markdown","source":"This notebook ends here, I will try to update it as I go along with new analysis. Thanks for making it to the end :) !","metadata":{}}]}