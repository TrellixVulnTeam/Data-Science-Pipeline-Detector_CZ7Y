{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAPIDS Forest Inference Library! TRAIN NOTEBOOK\n# Kaggle Toxic Comp 2022 Solution - Silver 52nd Place\nSince [RAPIDS Forest Inference Library][1] can infer Tree Based Models blazing fast at 100 million rows per second!!, we can approach this problem in a way that no other team did. During inference, we need to predict 14,000 toxic scores for each of the test data comments. Instead of predicting 14,000 numbers, we will predict 196,000,000 million numbers! We create every pair of comments `14,000 x 14,000 = 196,000,000`, and then for each of these 196 million pairs, we feed `1792 x 2 = 3584` columns of features and use [RAPIDS Forest Inference Library][1] to predict whether the second comment is more toxic than the first comment. After inferring all 196,000,000 pair probabilities, we convert these into 14,000 toxic scores.\n\n[1]: https://medium.com/rapids-ai/rapids-forest-inference-library-prediction-at-100-million-rows-per-second-19558890bc35","metadata":{}},{"cell_type":"markdown","source":"# Load Leak Free Folds\nWe will load leak free K-folds from notebook [here][1]. Each pair of comments in `validation_data.csv` has two comments. These two comments are also contained in other pairs. When creating folds, we must put all instances of each comment in its own fold.\n\n[1]: https://www.kaggle.com/its7171/jigsaw-cv-strategy","metadata":{}},{"cell_type":"code","source":"import pandas as pd, numpy as np, gc, os\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nVER = 100\nFOLDS = 11\n\ntrain = pd.read_csv(f'../input/toxiccompscripts/folds-{FOLDS}.csv')\ntrain = train.drop(['less_id','more_id','less_gid','more_gid'],axis=1)\nprint( train.shape )\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:31:05.85679Z","iopub.execute_input":"2022-02-08T02:31:05.857418Z","iopub.status.idle":"2022-02-08T02:31:06.102645Z","shell.execute_reply.started":"2022-02-08T02:31:05.85738Z","shell.execute_reply":"2022-02-08T02:31:06.101769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Text Features\nFor each of the 30,108 pairs of comments in the file `validation_data.csv` we extract embeddings from `roBERTa-base` and `roBERTa-large` offline. The `roBERTa-base` embeddings have 768 columns and the `roBERTa-large` embeddings have 1024 columns. Then we load the embeddings here. A pair is two comments, so each row of train data will have `2 x (768 + 1024) = 3584` columns of features.\n\nNote that both `roBERTa` models are pretrained on old comp data to predict old comp targets. Then we remove the pretrain head, and just extract the last hidden layer activations as embeddings.","metadata":{}},{"cell_type":"code","source":"less_embed = np.load('../input/roberta-7/embed1_v7.npy')\nmore_embed = np.load('../input/roberta-7/embed2_v7.npy')\nprint('roBERa-base embeddings have shape',less_embed.shape, more_embed.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:31:06.104249Z","iopub.execute_input":"2022-02-08T02:31:06.104592Z","iopub.status.idle":"2022-02-08T02:31:09.709596Z","shell.execute_reply.started":"2022-02-08T02:31:06.104554Z","shell.execute_reply":"2022-02-08T02:31:09.708828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_embed2 = np.load('../input/roberta-8/embed1_v8.npy')\nmore_embed2 = np.load('../input/roberta-8/embed2_v8.npy')\nprint('roBERa-large embeddings have shape',less_embed2.shape, more_embed2.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:31:09.711043Z","iopub.execute_input":"2022-02-08T02:31:09.711298Z","iopub.status.idle":"2022-02-08T02:31:12.39326Z","shell.execute_reply.started":"2022-02-08T02:31:09.711262Z","shell.execute_reply":"2022-02-08T02:31:12.391387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_embed = np.concatenate([less_embed,less_embed2],axis=1)\nmore_embed = np.concatenate([more_embed,more_embed2],axis=1)\nWORDS = less_embed.shape[1]\nprint('Each comment has',WORDS,'features. These are roBERTa-base and roBERTa-large embeddings')\ndel less_embed2, more_embed2\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:31:12.395223Z","iopub.execute_input":"2022-02-08T02:31:12.395493Z","iopub.status.idle":"2022-02-08T02:31:12.694649Z","shell.execute_reply.started":"2022-02-08T02:31:12.39545Z","shell.execute_reply":"2022-02-08T02:31:12.69376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PUT EMBEDDINGS INTO A DATAFRAME\ndf_l = pd.DataFrame(less_embed,columns=[f'f_{x}' for x in range(WORDS)])\ndf_m = pd.DataFrame(more_embed,columns=[f'f_{x}' for x in range(WORDS)])","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:31:12.696106Z","iopub.execute_input":"2022-02-08T02:31:12.69645Z","iopub.status.idle":"2022-02-08T02:31:12.711767Z","shell.execute_reply.started":"2022-02-08T02:31:12.69641Z","shell.execute_reply":"2022-02-08T02:31:12.711013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train XGB\nWe will now train 11 folds of XGB models to accept two comments and predict if the second comment is more toxic. The model receives 1792 columns of features for each comment and outputs a probability. We will save the 11 fold models to a Kaggle dataset and then load it into another inference notebook for submission. We achieve 11-Fold CV score of 0.707 accuracy.","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nprint('XGB Version',xgb.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:31:12.714131Z","iopub.execute_input":"2022-02-08T02:31:12.715838Z","iopub.status.idle":"2022-02-08T02:31:12.832119Z","shell.execute_reply.started":"2022-02-08T02:31:12.715796Z","shell.execute_reply":"2022-02-08T02:31:12.831175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_parms = { \n    'max_depth':2, \n    'learning_rate':0.01, \n    'subsample':0.4,\n    'colsample_bytree':0.3, \n    'eval_metric':'logloss',\n    'objective':'binary:logistic',\n    'tree_method':'gpu_hist',\n    'predictor':'gpu_predictor',\n    'random_state':42,\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:31:12.836991Z","iopub.execute_input":"2022-02-08T02:31:12.839559Z","iopub.status.idle":"2022-02-08T02:31:12.847192Z","shell.execute_reply.started":"2022-02-08T02:31:12.839518Z","shell.execute_reply":"2022-02-08T02:31:12.846492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\nimportances = []\noof1 = np.zeros((len(train)))\noof2 = np.zeros((len(train)))\n\nfor fold in range(FOLDS):\n    print('#'*25)\n    print('### Fold',fold+1)\n    print('#'*25)\n    \n    # TRAIN DATA FROM VALIDATION_DATA.CSV\n    t_data = train.loc[train.fold!=fold].copy()\n    v_data = train.loc[train.fold==fold].copy()\n    \n    t_data['y'] = 1 \n    v_data['y'] = 1\n            \n    # WE WILL COPY EVERY PAIR IN VALIDATION DATA AND REVERSE THE ORDER AND TARGET\n    t_data2 = t_data.copy()\n    t_data2['y'] = 0\n    t_data = pd.concat([t_data, t_data2],axis=0, ignore_index=True)\n    del t_data2\n    \n    v_data2 = v_data.copy()\n    v_data2['y'] = 0\n    v_data = pd.concat([v_data, v_data2],axis=0, ignore_index=True)\n    del v_data2    \n    \n    # MERGE FEATURES FROM ROBERTA-BASE AND ROBERTA-LARGE\n    FEATURES = []\n    more_train = pd.concat([df_m.loc[train.fold!=fold],df_l.loc[train.fold!=fold]],axis=0,ignore_index=True)\n    more_valid = pd.concat([df_m.loc[train.fold==fold],df_l.loc[train.fold==fold]],axis=0,ignore_index=True)\n    FEATURES = FEATURES + list( more_train.columns )\n    t_data = pd.concat([t_data,more_train],axis=1)\n    v_data = pd.concat([v_data,more_valid],axis=1)\n    \n    more_train = pd.concat([df_l.loc[train.fold!=fold],df_m.loc[train.fold!=fold]],axis=0,ignore_index=True)\n    more_train.columns = [f'g_{x}' for x in range(WORDS)]\n    more_valid = pd.concat([df_l.loc[train.fold==fold],df_m.loc[train.fold==fold]],axis=0,ignore_index=True)\n    more_valid.columns = [f'g_{x}' for x in range(WORDS)]\n    FEATURES = FEATURES + list( more_train.columns )\n    t_data = pd.concat([t_data,more_train],axis=1)\n    v_data = pd.concat([v_data,more_valid],axis=1)\n    \n    dtrain = xgb.DMatrix(data=t_data[FEATURES], label=t_data.y )\n    dvalid = xgb.DMatrix(data=v_data[FEATURES], label=v_data.y )\n    \n    # TRAIN MODEL FOLD K\n    model = xgb.train(xgb_parms, \n                dtrain=dtrain,\n                evals=[(dtrain,'train'),(dvalid,'valid')],\n                num_boost_round=9999,\n                early_stopping_rounds=100,\n                verbose_eval=500) \n    model.save_model(f'XGB_v{VER}_f{fold}.xgb')\n    \n    # GET FEATURE IMPORTANCE FOR FOLD K\n    dd = model.get_score(importance_type='weight')\n    tmp = pd.DataFrame({'feature':dd.keys(),f'importance_{fold}':dd.values()})\n    importances.append(tmp)\n    \n    # INFER OOF FOLD K\n    oof_preds = model.predict(dvalid)\n    auc = roc_auc_score(v_data.y.values, oof_preds)\n    acc = accuracy_score(v_data.y.values, (oof_preds>0.5).astype('int32'))\n    print('AUC =',auc,'ACC =',acc)\n    \n    oof1[train.fold.values==fold] = oof_preds[:len(oof_preds)//2]\n    oof2[train.fold.values==fold] = oof_preds[len(oof_preds)//2:]\n    \nprint()\noof = np.concatenate([oof1,oof2])\ntrue = np.concatenate([ np.ones(len(train)),np.zeros(len(train)) ])\n\nacc = accuracy_score(true, (oof>0.5).astype('int32'))\nauc = roc_auc_score(true, oof)\nprint('OVERALL AUC =',auc,'ACC =',acc)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-08T02:31:12.852575Z","iopub.execute_input":"2022-02-08T02:31:12.855224Z","iopub.status.idle":"2022-02-08T02:44:16.697265Z","shell.execute_reply.started":"2022-02-08T02:31:12.855129Z","shell.execute_reply":"2022-02-08T02:44:16.696553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGB Feature Importances","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndff = importances[0].copy()\nfor k in range(1,FOLDS): dff = dff.merge(importances[k], on='feature', how='left')\ndff['importance'] = dff.iloc[:,1:].mean(axis=1)\ndff = dff.sort_values('importance',ascending=False)\ndff.to_csv(f'xgb_feature_importance_v{VER}_toxic.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:44:16.698492Z","iopub.execute_input":"2022-02-08T02:44:16.698843Z","iopub.status.idle":"2022-02-08T02:44:16.765214Z","shell.execute_reply.started":"2022-02-08T02:44:16.698797Z","shell.execute_reply":"2022-02-08T02:44:16.764587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FEATURES = 20\nplt.figure(figsize=(10,5*NUM_FEATURES//10))\nplt.barh(np.arange(NUM_FEATURES,0,-1), dff.importance.values[:NUM_FEATURES])\nplt.yticks(np.arange(NUM_FEATURES,0,-1), dff.feature.values[:NUM_FEATURES])\nplt.title(f'XGB Feature Importance - Top {NUM_FEATURES}')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:44:16.767418Z","iopub.execute_input":"2022-02-08T02:44:16.767679Z","iopub.status.idle":"2022-02-08T02:44:17.044688Z","shell.execute_reply.started":"2022-02-08T02:44:16.767645Z","shell.execute_reply":"2022-02-08T02:44:17.04401Z"},"trusted":true},"execution_count":null,"outputs":[]}]}