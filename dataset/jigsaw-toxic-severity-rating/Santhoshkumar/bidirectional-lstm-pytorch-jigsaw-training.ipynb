{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport json\nimport time\nimport math\nimport random\nfrom datetime import datetime\nfrom collections import Counter, defaultdict\nimport joblib\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nfrom tqdm import tqdm\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-13T09:35:26.1905Z","iopub.execute_input":"2021-11-13T09:35:26.191222Z","iopub.status.idle":"2021-11-13T09:35:32.753406Z","shell.execute_reply.started":"2021-11-13T09:35:26.191168Z","shell.execute_reply":"2021-11-13T09:35:32.752599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    BATCH_SIZE=32\n    NUM_WORKERS = 4\n    WEIGHT_DECAY=1e-6\n    LR=1e-4\n    EPOCHS=20\n    N_FOLDS=5\n    N_LAYERS = 2\n    SEQ_LEN = 200\n    OUTPUT_SIZE = 1\n    HIDDEN_DIM = 128","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:52:38.758159Z","iopub.execute_input":"2021-11-13T08:52:38.758667Z","iopub.status.idle":"2021-11-13T08:52:38.762855Z","shell.execute_reply.started":"2021-11-13T08:52:38.758623Z","shell.execute_reply":"2021-11-13T08:52:38.762158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:44:17.003512Z","iopub.execute_input":"2021-11-13T08:44:17.004071Z","iopub.status.idle":"2021-11-13T08:44:17.008572Z","shell.execute_reply.started":"2021-11-13T08:44:17.004034Z","shell.execute_reply":"2021-11-13T08:44:17.007915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:44:58.913728Z","iopub.execute_input":"2021-11-13T08:44:58.914004Z","iopub.status.idle":"2021-11-13T08:44:58.920031Z","shell.execute_reply.started":"2021-11-13T08:44:58.913975Z","shell.execute_reply":"2021-11-13T08:44:58.918998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\nporter_stemmer = PorterStemmer()\n\ndef remove_stopwords(text):\n    output= [i for i in text if i not in stopwords]\n    return output\n\ndef stemming(text):\n    stem_text = [porter_stemmer.stem(word) for word in text]\n    return stem_text\n\ndef tokenization(text):\n    tokens = text.split(' ')\n    return tokens\n\ndef text_to_sequences(word2idx, seq):\n    for i, sentence in enumerate(seq):\n        seq[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n    return seq\n\ndef pad_sequences(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ndef build_vocab(seq):\n    words = Counter()  \n    for i, sentence in enumerate(seq):\n        for word in sentence:  \n            words.update([word.lower()])  \n    words = {k:v for k,v in words.items() if v>1}\n    words = sorted(words, key=words.get, reverse=True)\n\n    words = ['_PAD','_UNK'] + words\n    word2idx = {o:i for i,o in enumerate(words)}\n    idx2word = {i:o for i,o in enumerate(words)}\n    return words, word2idx, idx2word\n\ndef load_vectors():   \n    path_to_glove_file = os.path.join(\n        '../input/glove6b100dtxt', \"glove.6B.100d.txt\"\n    )\n    \n    embeddings_index = {}\n    with open(path_to_glove_file, encoding=\"utf8\") as f:\n        for line in f:\n            word, coefs = line.split(maxsplit=1)\n            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n            embeddings_index[word] = coefs\n    \n    print(\"Found %s word vectors.\" % len(embeddings_index))\n    return embeddings_index\n        \ndef create_embedding_matrix(word_index, embedding_dict, embedding_dim=100):\n    hits = 0\n    misses = 0\n    \n    # Prepare embedding matrix\n    embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = embedding_dict.get(word)\n        if embedding_vector is not None:\n            # Words not found in embedding index will be all-zeros.\n            # This includes the representation for \"padding\" and \"OOV\"\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n    print(\"Converted %d words (%d misses)\" % (hits, misses))\n    return embedding_matrix\n\n\ndef preprocess(df):\n    df['more_toxic_text']= df['more_toxic'].str.replace('\\d+', '0')\n    df['more_toxic_text']= df['more_toxic_text'].str.replace('\\W+', ' ')\n    df['more_toxic_text']= df['more_toxic_text'].apply(lambda x: tokenization(x))\n    df['more_toxic_text']= df['more_toxic_text'].apply(lambda x:remove_stopwords(x))\n\n    df['less_toxic_text']= df['less_toxic'].str.replace('\\d+', '0')\n    df['less_toxic_text']= df['less_toxic_text'].str.replace('\\W+', ' ')\n    df['less_toxic_text']= df['less_toxic_text'].apply(lambda x: tokenization(x))\n    df['less_toxic_text']= df['less_toxic_text'].apply(lambda x:remove_stopwords(x))\n    return df\n\n\ndef build_embedding(df):\n    train_sequenceses = list(df['more_toxic_text'].values) + list(df['less_toxic_text'].values)\n    words, word2idx, idx2word = build_vocab(np.unique(train_sequenceses))\n    joblib.dump(word2idx, 'word2idx.pkl', compress=1)\n    \n    print(\"Loading embeddings\")\n    embedding_dict = load_vectors()\n    embedding_matrix = create_embedding_matrix(\n        word2idx, embedding_dict\n        )\n    joblib.dump(embedding_matrix, 'embedding_matrix.pkl', compress=1)\n    \n    return word2idx, embedding_matrix\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:48:45.152692Z","iopub.execute_input":"2021-11-13T08:48:45.153042Z","iopub.status.idle":"2021-11-13T08:48:45.186165Z","shell.execute_reply.started":"2021-11-13T08:48:45.153006Z","shell.execute_reply":"2021-11-13T08:48:45.185421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_step(model, data, optimizer, scheduler, loss_fn, device):\n    optimizer.zero_grad()\n    for key, value in data.items():\n        data[key] = value.to(device)\n    more_toxic = data['more_toxic']\n    less_toxic = data['less_toxic']\n    target = data['target']\n    logit1 = model(more_toxic)\n    logit2 = model(less_toxic)      \n    loss = loss_fn(logit1, logit2, target)   \n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n    return loss\n\n\ndef train_one_epoch(model, train_loader, optimizer, scheduler, loss_fn, device):\n    model.train()\n    losses = AverageMeter()\n    tk0 = tqdm(train_loader, total=len(train_loader))\n    for idx, data in enumerate(tk0):\n        loss = train_one_step(model, data, optimizer, scheduler, loss_fn, device)\n        losses.update(loss.item(), train_loader.batch_size)\n        tk0.set_postfix(loss=losses.avg, stage=\"train\")\n    tk0.close()\n    return losses.avg \n\n\ndef valid_one_step(model, data, loss_fn, device):\n    for key, value in data.items():\n        data[key] = value.to(device)    \n    more_toxic = data['more_toxic']\n    less_toxic = data['less_toxic']\n    target = data['target']\n    logit1 = model(more_toxic)\n    logit2 = model(less_toxic)\n    loss = loss_fn(logit1, logit2, target)\n    return loss\n\n \ndef valid_one_epoch(model, valid_loader, loss_fn, device):\n    model.eval()\n    losses = AverageMeter()\n    tk0 = tqdm(valid_loader, total=len(valid_loader))\n    for idx, data in enumerate(tk0):\n        with torch.no_grad():\n            loss = valid_one_step(model, data, loss_fn, device)\n        losses.update(loss.item(), valid_loader.batch_size)\n        tk0.set_postfix(loss=losses.avg, stage=\"valid\")\n    tk0.close()\n    return losses.avg\n\n \n\ndef predict_one_step(model, data, device):\n    for key, value in data.items():\n        data[key] = value.to(device)\n    toxic = data['toxic']   \n    logit = model(toxic)\n    return logit\n\n \n\ndef predict_one_epoch(model, test_loader, device):\n    model.eval()\n    predictions = []\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for idx, data in enumerate(tk0):\n        with torch.no_grad():\n            logit = predict_one_step(model, data, device)\n        predictions.append(logit.view(-1).detach().cpu().numpy())\n    return np.concatenate(predictions)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:46:33.336442Z","iopub.execute_input":"2021-11-13T08:46:33.336686Z","iopub.status.idle":"2021-11-13T08:46:33.352793Z","shell.execute_reply.started":"2021-11-13T08:46:33.33666Z","shell.execute_reply":"2021-11-13T08:46:33.351426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ToxicModel(nn.Module):\n    def __init__(self, output_size, embedding_matrix, hidden_dim, n_layers, drop_prob=0.3):\n         super(ToxicModel, self).__init__()\n         self.output_size = output_size\n         self.n_layers = n_layers\n         self.hidden_dim = hidden_dim\n         num_words = embedding_matrix.shape[0]\n         embed_dim =  embedding_matrix.shape[1]\n        \n         self.embedding = nn.Embedding(num_words, embed_dim)\n        \n         self.embedding.weight = nn.Parameter(\n         torch.tensor(\n         embedding_matrix,\n         dtype=torch.float32\n         )\n         )     \n\n         self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=drop_prob, bidirectional=True, batch_first=True)\n         self.fc = nn.Linear(256*2, output_size)            \n            \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().to('cuda'),\n                      weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().to('cuda'))\n        return hidden\n    \n    def forward(self, toxic):\n        batch_size = toxic.size(0)\n        h = self.init_hidden(batch_size)\n        h = tuple([e.data for e in h])\n        x = toxic.long()\n        embeds = self.embedding(x)\n        lstm_out, _ = self.lstm(embeds, h)\n        mean_  = torch.mean(lstm_out,1)\n        max_ , _ = torch.max(lstm_out,1)\n        out = torch.cat((mean_, max_), 1)\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:46:49.579747Z","iopub.execute_input":"2021-11-13T08:46:49.580433Z","iopub.status.idle":"2021-11-13T08:46:49.59314Z","shell.execute_reply.started":"2021-11-13T08:46:49.580393Z","shell.execute_reply":"2021-11-13T08:46:49.592095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ToxicDataset:\n    def __init__(self, more_toxic, less_toxic):\n        self.more_toxic = more_toxic\n        self.less_toxic = less_toxic\n\n    def __len__(self):\n        return len(self.more_toxic)\n\n    def __getitem__(self, item):\n\n        more_toxic = self.more_toxic[item, :]\n        less_toxic = self.less_toxic[item, :]\n        return {\n              \"more_toxic\": torch.tensor(more_toxic, dtype=torch.long),\n              \"less_toxic\": torch.tensor(less_toxic, dtype=torch.long),\n              \"target\": torch.tensor(1, dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:47:41.664681Z","iopub.execute_input":"2021-11-13T08:47:41.66495Z","iopub.status.idle":"2021-11-13T08:47:41.671706Z","shell.execute_reply.started":"2021-11-13T08:47:41.664904Z","shell.execute_reply":"2021-11-13T08:47:41.671065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_one_epoch(\n                  fold,\n                  seed,       \n                  train_sentences_more,\n                  val_sentences_more,\n                  train_sentences_less,\n                  val_sentences_less,\n                  embedding_matrix\n    ):\n    \n    seed_everything(seed)\n    \n    train_dataset = ToxicDataset(\n        train_sentences_more,\n        train_sentences_less\n        )\n    \n    valid_dataset = ToxicDataset(\n         val_sentences_more,\n         val_sentences_less\n        )\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.BATCH_SIZE,\n                              shuffle=True,\n                              num_workers=CFG.NUM_WORKERS, pin_memory=True, drop_last=True)\n\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.BATCH_SIZE,\n                              shuffle=False,\n                              num_workers=CFG.NUM_WORKERS, pin_memory=True, drop_last=False)\n\n\n    model = ToxicModel(CFG.OUTPUT_SIZE, embedding_matrix, CFG.HIDDEN_DIM, CFG.N_LAYERS)\n    model.to(device)\n \n    optimizer = AdamW(model.parameters(), lr=CFG.LR, weight_decay=CFG.WEIGHT_DECAY)\n    num_train_steps = int(len(train_sentences_more)/ CFG.BATCH_SIZE * CFG.EPOCHS)\n\n    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=1, eta_min=1e-7, last_epoch=-1)\n\n    criterion = nn.MarginRankingLoss(margin=0.5)\n    \n    best_score = np.inf\n\n    for epoch in range(CFG.EPOCHS):\n\n        avg_loss = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n        score = valid_one_epoch(model, valid_loader, criterion, device)\n        if score < best_score:\n            best_score = score\n            print(f'Epoch {epoch+1} - Save Best Score: {score:.4f} Model')\n            torch.save({'model': model.state_dict(),},f\"fold{fold}_best.pth\")\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    return best_score","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:04:06.179436Z","iopub.execute_input":"2021-11-13T09:04:06.179726Z","iopub.status.idle":"2021-11-13T09:04:06.191249Z","shell.execute_reply.started":"2021-11-13T09:04:06.179694Z","shell.execute_reply":"2021-11-13T09:04:06.190315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_fold(fold, seed, df, word2idx, embedding_matrix):\n    \n    df_train=df.loc[df.kfold!=fold].reset_index(drop=True)\n    df_valid=df.loc[df.kfold==fold].reset_index(drop=True)\n    \n    train_sequenceses_more = text_to_sequences(word2idx, list(df_train['more_toxic_text'].values))\n    val_sequenceses_more = text_to_sequences(word2idx, list(df_valid['more_toxic_text'].values))\n\n    train_sequenceses_less = text_to_sequences(word2idx, list(df_train['less_toxic_text'].values))\n    val_sequenceses_less = text_to_sequences(word2idx, list(df_valid['less_toxic_text'].values))\n\n    train_sentences_more = pad_sequences(train_sequenceses_more, CFG.SEQ_LEN)\n    val_sentences_more = pad_sequences(val_sequenceses_more, CFG.SEQ_LEN)\n\n    train_sentences_less = pad_sequences(train_sequenceses_less, CFG.SEQ_LEN)\n    val_sentences_less = pad_sequences(val_sequenceses_less, CFG.SEQ_LEN)\n    \n    #run the training\n    score = fit_one_epoch(\n                 fold, \n                 seed, \n                 train_sentences_more,\n                 val_sentences_more,\n                 train_sentences_less,\n                 val_sentences_less,\n                 embedding_matrix\n        \n    )\n    \n    return score","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:00:39.447492Z","iopub.execute_input":"2021-11-13T09:00:39.447744Z","iopub.status.idle":"2021-11-13T09:00:39.457498Z","shell.execute_reply.started":"2021-11-13T09:00:39.447717Z","shell.execute_reply":"2021-11-13T09:00:39.4568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training():\n    df = pd.read_csv(os.path.join('../input/training-data','toxic_valid_folds.csv'))\n    \n    #Preprocess the data\n    df = preprocess(df)\n    \n    #Save embedding\n    word2idx, embedding_matrix = build_embedding(df)\n    \n    for f in range(CFG.N_FOLDS):\n        score = run_fold(f, 42, df, word2idx, embedding_matrix)\n        print(f'fold:{f} training completed!!! best score:{score}')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:00:43.540694Z","iopub.execute_input":"2021-11-13T09:00:43.541043Z","iopub.status.idle":"2021-11-13T09:00:43.54531Z","shell.execute_reply.started":"2021-11-13T09:00:43.54101Z","shell.execute_reply":"2021-11-13T09:00:43.544522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__=='__main__':\n    run_training()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:00:45.882539Z","iopub.execute_input":"2021-11-13T09:00:45.88323Z","iopub.status.idle":"2021-11-13T09:03:15.104028Z","shell.execute_reply.started":"2021-11-13T09:00:45.883197Z","shell.execute_reply":"2021-11-13T09:03:15.102767Z"},"trusted":true},"execution_count":null,"outputs":[]}]}