{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport json\nimport time\nimport math\nimport random\nfrom datetime import datetime\nfrom collections import Counter, defaultdict\nimport joblib\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nfrom tqdm import tqdm\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"70fcc2e9-ef4a-46e2-a633-4963fe144f27","_cell_guid":"c196920b-9bf8-4d88-ab80-d0b82906988b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-13T13:42:55.557658Z","iopub.execute_input":"2021-11-13T13:42:55.558423Z","iopub.status.idle":"2021-11-13T13:43:01.885076Z","shell.execute_reply.started":"2021-11-13T13:42:55.558331Z","shell.execute_reply":"2021-11-13T13:43:01.884311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    BATCH_SIZE=32\n    NUM_WORKERS = 4\n    WEIGHT_DECAY=1e-6\n    LR=1e-4\n    EPOCHS=20\n    N_FOLDS=5\n    N_LAYERS = 2\n    SEQ_LEN = 200\n    OUTPUT_SIZE = 1\n    HIDDEN_DIM = 128","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:43:05.670702Z","iopub.execute_input":"2021-11-13T13:43:05.671329Z","iopub.status.idle":"2021-11-13T13:43:05.676349Z","shell.execute_reply.started":"2021-11-13T13:43:05.671294Z","shell.execute_reply":"2021-11-13T13:43:05.675648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:43:07.746739Z","iopub.execute_input":"2021-11-13T13:43:07.747393Z","iopub.status.idle":"2021-11-13T13:43:07.752975Z","shell.execute_reply.started":"2021-11-13T13:43:07.747356Z","shell.execute_reply":"2021-11-13T13:43:07.75193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:43:10.305293Z","iopub.execute_input":"2021-11-13T13:43:10.305891Z","iopub.status.idle":"2021-11-13T13:43:10.314584Z","shell.execute_reply.started":"2021-11-13T13:43:10.305855Z","shell.execute_reply":"2021-11-13T13:43:10.313808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestToxicDataset:\n    def __init__(self, toxic):\n        self.toxic = toxic\n\n    def __len__(self):\n        return len(self.toxic)\n\n    def __getitem__(self, item):\n        toxic = self.toxic[item, :]\n        \n        return {\n              \"toxic\": torch.tensor(toxic, dtype=torch.long),\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:43:12.415097Z","iopub.execute_input":"2021-11-13T13:43:12.415354Z","iopub.status.idle":"2021-11-13T13:43:12.421843Z","shell.execute_reply.started":"2021-11-13T13:43:12.415327Z","shell.execute_reply":"2021-11-13T13:43:12.419921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\nporter_stemmer = PorterStemmer()\n\ndef remove_stopwords(text):\n    output= [i for i in text if i not in stopwords]\n    return output\n\ndef stemming(text):\n    stem_text = [porter_stemmer.stem(word) for word in text]\n    return stem_text\n\ndef tokenization(text):\n    tokens = text.split(' ')\n    return tokens\n\ndef text_to_sequences(word2idx, seq):\n    for i, sentence in enumerate(seq):\n        seq[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n    return seq\n\ndef pad_sequences(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ndef preprocess(df):\n    df['toxic_text']= df['text'].str.replace('\\d+', '0')\n    df['toxic_text']= df['toxic_text'].str.replace('\\W+', ' ')\n    df['toxic_text']= df['toxic_text'].apply(lambda x: tokenization(x))\n    df['toxic_text']= df['toxic_text'].apply(lambda x:remove_stopwords(x))\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:43:14.049737Z","iopub.execute_input":"2021-11-13T13:43:14.050433Z","iopub.status.idle":"2021-11-13T13:43:14.064408Z","shell.execute_reply.started":"2021-11-13T13:43:14.050397Z","shell.execute_reply":"2021-11-13T13:43:14.063697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_one_step(model, data, device):\n    for key, value in data.items():\n        data[key] = value.to(device)\n    toxic = data['toxic']   \n    logit = model(toxic)\n    return logit\n\n \n\ndef predict_one_epoch(model, test_loader, device):\n    model.eval()\n    predictions = []\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for idx, data in enumerate(tk0):\n        with torch.no_grad():\n            logit = predict_one_step(model, data, device)\n        predictions.append(logit.view(-1).detach().cpu().numpy())\n    return np.concatenate(predictions)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:43:17.786967Z","iopub.execute_input":"2021-11-13T13:43:17.787698Z","iopub.status.idle":"2021-11-13T13:43:17.794495Z","shell.execute_reply.started":"2021-11-13T13:43:17.78765Z","shell.execute_reply":"2021-11-13T13:43:17.793633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ToxicModel(nn.Module):\n    def __init__(self, output_size, embedding_matrix, hidden_dim, n_layers, drop_prob=0.3):\n         super(ToxicModel, self).__init__()\n         self.output_size = output_size\n         self.n_layers = n_layers\n         self.hidden_dim = hidden_dim\n         num_words = embedding_matrix.shape[0]\n         embed_dim =  embedding_matrix.shape[1]\n        \n         self.embedding = nn.Embedding(num_words, embed_dim)\n        \n         self.embedding.weight = nn.Parameter(\n         torch.tensor(\n         embedding_matrix,\n         dtype=torch.float32\n         )\n         )     \n\n         self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=drop_prob, bidirectional=True, batch_first=True)\n         self.fc = nn.Linear(256*2, output_size)            \n            \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().to('cuda'),\n                      weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().to('cuda'))\n        return hidden\n    \n    def forward(self, toxic):\n        batch_size = toxic.size(0)\n        h = self.init_hidden(batch_size)\n        h = tuple([e.data for e in h])\n        x = toxic.long()\n        embeds = self.embedding(x)\n        lstm_out, _ = self.lstm(embeds, h)\n        mean_  = torch.mean(lstm_out,1)\n        max_ , _ = torch.max(lstm_out,1)\n        out = torch.cat((mean_, max_), 1)\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:43:20.429891Z","iopub.execute_input":"2021-11-13T13:43:20.430416Z","iopub.status.idle":"2021-11-13T13:43:20.441174Z","shell.execute_reply.started":"2021-11-13T13:43:20.43038Z","shell.execute_reply":"2021-11-13T13:43:20.440459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(fold, seed, test_sentences, embedding_matrix):   \n    #seed_everything(seed)\n\n    test_dataset = TestToxicDataset(\n         test_sentences,\n        )\n    \n    test_loader = DataLoader(test_dataset,\n                              batch_size=CFG.BATCH_SIZE,\n                              shuffle=False,\n                              num_workers=CFG.NUM_WORKERS, pin_memory=True, drop_last=False)\n    \n    model = ToxicModel(CFG.OUTPUT_SIZE, embedding_matrix, CFG.HIDDEN_DIM, CFG.N_LAYERS)\n    state = torch.load(f'../input/jigsaw-metadata/fold{fold}_best.pth', map_location=torch.device(device))\n    model.load_state_dict(state['model'])\n    model.to('cuda')\n    \n    preds = predict_one_epoch(model, test_loader, device)\n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-11-13T14:00:59.141463Z","iopub.execute_input":"2021-11-13T14:00:59.142073Z","iopub.status.idle":"2021-11-13T14:00:59.151565Z","shell.execute_reply.started":"2021-11-13T14:00:59.142033Z","shell.execute_reply":"2021-11-13T14:00:59.150814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_inference():\n    df = pd.read_csv(os.path.join('../input/jigsaw-toxic-severity-rating','comments_to_score.csv'))\n\n    predictions = 0\n    \n    embedding_matrix = joblib.load('../input/jigsaw-metadata/embedding_matrix.pkl')\n    word2idx = joblib.load('../input/jigsaw-metadata/word2idx.pkl')\n\n    df = preprocess(df)\n\n    test_sequenceses = text_to_sequences(word2idx, list(df['toxic_text']))\n    test_sentences = pad_sequences(test_sequenceses, CFG.SEQ_LEN)\n    for f in range(CFG.N_FOLDS):\n        preds = inference_fn(f, 42, test_sentences, embedding_matrix)\n        predictions +=preds\n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-11-13T14:01:02.417741Z","iopub.execute_input":"2021-11-13T14:01:02.418428Z","iopub.status.idle":"2021-11-13T14:01:02.424292Z","shell.execute_reply.started":"2021-11-13T14:01:02.41839Z","shell.execute_reply":"2021-11-13T14:01:02.423603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = run_inference()\nsub = pd.read_csv(os.path.join('../input/jigsaw-toxic-severity-rating','sample_submission.csv'))\nsub['score'] = preds\nsub['score'] = sub['score'].rank(method='first')\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T14:01:04.540575Z","iopub.execute_input":"2021-11-13T14:01:04.541054Z","iopub.status.idle":"2021-11-13T14:01:23.877535Z","shell.execute_reply.started":"2021-11-13T14:01:04.541018Z","shell.execute_reply":"2021-11-13T14:01:23.876679Z"},"trusted":true},"execution_count":null,"outputs":[]}]}