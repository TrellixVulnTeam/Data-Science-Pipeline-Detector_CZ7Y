{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q '/kaggle/input/libraries/pytorch_lightning-1.5.4-py3-none-any.whl'\n!pip freeze | grep 'pytorch-lightning'","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:23:09.39574Z","iopub.execute_input":"2021-12-08T21:23:09.396118Z","iopub.status.idle":"2021-12-08T21:23:46.433275Z","shell.execute_reply.started":"2021-12-08T21:23:09.396025Z","shell.execute_reply":"2021-12-08T21:23:46.432129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nfrom argparse import Namespace\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import LightningDataModule, LightningModule, Trainer\n\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW\nfrom sklearn.model_selection import StratifiedKFold\n\npl.__version__","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:45:25.973182Z","iopub.execute_input":"2021-12-08T21:45:25.973804Z","iopub.status.idle":"2021-12-08T21:45:30.243745Z","shell.execute_reply.started":"2021-12-08T21:45:25.973769Z","shell.execute_reply":"2021-12-08T21:45:30.242696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = Namespace(\n    seed = 7,\n    n_folds = 5,\n    \n    trainer = Namespace(\n        precision = 32,\n        max_epochs = 1,\n        gpus = -1 if torch.cuda.is_available() else 0,\n        enable_checkpointing=False,\n#         fast_dev_run=2\n    ),\n    \n    model = Namespace(\n        loss_margin = 0.5,\n        \n        # model layers\n        model_path = '/kaggle/input/roberta-base',\n        dropout = 0.2,\n        num_classes = 1,\n        hidden_size = 256,\n        \n        # scheduler\n        T_max = 500,\n        min_lr = 1e-6,\n        \n        # optimizer\n        lr = 1e-4,\n        weight_decay = 1e-6,\n    ),\n    \n    data = Namespace(\n        # tokenizer\n        tokenizer_path = '/kaggle/input/roberta-base',\n        max_length = 128,\n        \n        # dataloader\n        train_batch_size = 32,\n        val_batch_size = 64,\n        predict_batch_size = 64,\n        num_workers = os.cpu_count(),\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:45:30.24617Z","iopub.execute_input":"2021-12-08T21:45:30.2468Z","iopub.status.idle":"2021-12-08T21:45:30.292988Z","shell.execute_reply.started":"2021-12-08T21:45:30.246753Z","shell.execute_reply":"2021-12-08T21:45:30.291844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ToxicDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        super().__init__()\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, idx):\n        less_toxic_text, more_toxic_text = self.df[['less_toxic', 'more_toxic']].iloc[idx]\n        encoded_less_toxic = self.tokenizer.encode_plus(less_toxic_text, truncation=True, add_special_tokens=True, max_length=self.max_length, padding='max_length')\n        encoded_more_toxic = self.tokenizer.encode_plus(more_toxic_text, truncation=True, add_special_tokens=True, max_length=self.max_length, padding='max_length')\n        \n        less_toxic_input_ids, less_toxic_attention_mask = encoded_less_toxic['input_ids'], encoded_less_toxic['attention_mask']\n        more_toxic_input_ids, more_toxic_attention_mask = encoded_more_toxic['input_ids'], encoded_more_toxic['attention_mask']\n        \n        return {\n            'less_toxic_input_ids': torch.tensor(less_toxic_input_ids, dtype=torch.long),\n            'less_toxic_attention_mask': torch.tensor(less_toxic_attention_mask, dtype=torch.long),\n            'more_toxic_input_ids': torch.tensor(more_toxic_input_ids, dtype=torch.long),\n            'more_toxic_attention_mask': torch.tensor(more_toxic_attention_mask, dtype=torch.long),\n            'target': torch.tensor(1, dtype=torch.long)\n        }\n    \n    \nclass ToxicDatasetPredict(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        super().__init__()\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, idx):\n        toxic_text = self.df['text'].iloc[idx]\n        encoded_toxic = self.tokenizer.encode_plus(toxic_text, truncation=True, add_special_tokens=True, max_length=self.max_length, padding='max_length')\n        \n        toxic_input_ids, toxic_attention_mask = encoded_toxic['input_ids'], encoded_toxic['attention_mask']\n\n        return {\n            'toxic_input_ids': torch.tensor(toxic_input_ids, dtype=torch.long),\n            'toxic_attention_mask': torch.tensor(toxic_attention_mask, dtype=torch.long),\n        }\n        \n\nclass ToxicDataModule(LightningDataModule):\n    def __init__(self, df, predict_df, fold, **kwargs):\n        super().__init__()\n        self.df = df\n        self.predict_df = predict_df\n        self.fold = fold\n        self.save_hyperparameters(ignore=['df', 'predict_df', 'fold'])\n    \n    def setup(self, stage):\n        tokenizer = AutoTokenizer.from_pretrained(self.hparams.tokenizer_path)\n        \n        if stage == 'fit':\n            self.train_ds = ToxicDataset(self.df.loc[self.df['fold'] != self.fold], tokenizer, self.hparams.max_length)\n            self.valid_ds = ToxicDataset(self.df.loc[self.df['fold'] == self.fold], tokenizer, self.hparams.max_length)\n        elif stage == 'predict':\n            self.predict_ds = ToxicDatasetPredict(self.predict_df, tokenizer, self.hparams.max_length)\n    \n    def train_dataloader(self):\n        return DataLoader(self.train_ds, batch_size=self.hparams.train_batch_size, shuffle=True, num_workers=self.hparams.num_workers)\n    \n    def val_dataloader(self):\n        return DataLoader(self.valid_ds, batch_size=self.hparams.val_batch_size, shuffle=False, num_workers=self.hparams.num_workers)\n    \n    def predict_dataloader(self):\n        return DataLoader(self.predict_ds, batch_size=self.hparams.predict_batch_size, shuffle=False, num_workers=self.hparams.num_workers)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:45:30.295642Z","iopub.execute_input":"2021-12-08T21:45:30.296131Z","iopub.status.idle":"2021-12-08T21:45:30.323914Z","shell.execute_reply.started":"2021-12-08T21:45:30.296081Z","shell.execute_reply":"2021-12-08T21:45:30.322857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ToxicModule(LightningModule):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        base_model_config = AutoConfig.from_pretrained(self.hparams.model_path)\n        self.base_model = AutoModel.from_pretrained(self.hparams.model_path, return_dict=False)\n        self.layer_norm = nn.LayerNorm(base_model_config.hidden_size)\n        self.dropout = nn.Dropout(self.hparams.dropout)\n        self.dense = nn.Sequential(\n            nn.Linear(base_model_config.hidden_size, self.hparams.hidden_size),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.Dropout(self.hparams.dropout),\n            nn.Linear(self.hparams.hidden_size, self.hparams.num_classes)\n        )\n        \n        self.loss = nn.MarginRankingLoss(margin=self.hparams.loss_margin)\n        \n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = self.layer_norm(pooled_output)\n        pooled_output = self.dropout(pooled_output)\n        preds = self.dense(pooled_output)\n        return preds\n    \n    def training_step(self, batch, batch_idx):\n        less_toxic_logits = self(batch['less_toxic_input_ids'], batch['less_toxic_attention_mask'])\n        more_toxic_logits = self(batch['more_toxic_input_ids'], batch['more_toxic_attention_mask'])\n        target = batch['target']\n        loss = self.loss(more_toxic_logits, less_toxic_logits, target)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        less_toxic_logits = self(batch['less_toxic_input_ids'], batch['less_toxic_attention_mask'])\n        more_toxic_logits = self(batch['more_toxic_input_ids'], batch['more_toxic_attention_mask'])\n        target = batch['target']\n        loss = self.loss(more_toxic_logits, less_toxic_logits, target)\n        self.log('val_loss', loss)\n    \n    def predict_step(self, batch, batch_idx):\n        return self(batch['toxic_input_ids'], batch['toxic_attention_mask'])\n    \n    def configure_optimizers(self):\n        opt = AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n        sched = optim.lr_scheduler.CosineAnnealingLR(optimizer=opt, eta_min=self.hparams.min_lr, T_max=self.hparams.T_max)\n        lr_sched_dict = {'scheduler': sched, 'interval': 'step'}\n        return {'optimizer': opt, 'lr_scheduler': lr_sched_dict}","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:45:30.326143Z","iopub.execute_input":"2021-12-08T21:45:30.326584Z","iopub.status.idle":"2021-12-08T21:45:30.347712Z","shell.execute_reply.started":"2021-12-08T21:45:30.326532Z","shell.execute_reply":"2021-12-08T21:45:30.346318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.seed_everything(config.seed)\n\ndf = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv')\npredict_df = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n\nskf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=config.seed)\ndf['fold'] = -1\n\nfor fold, (_, val_idxs) in enumerate(skf.split(X=df, y=df['worker'])):\n    df.loc[val_idxs , 'fold'] = fold\n    \ndf['fold'].value_counts(normalize=True)*100","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:45:30.349828Z","iopub.execute_input":"2021-12-08T21:45:30.350204Z","iopub.status.idle":"2021-12-08T21:45:30.691905Z","shell.execute_reply.started":"2021-12-08T21:45:30.350154Z","shell.execute_reply":"2021-12-08T21:45:30.690788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = None\n\nfor fold in range(config.n_folds):\n    print(f\"{'#'*50} FOLD: {fold+1} {'#'*50}\")\n\n    dm = ToxicDataModule(df, predict_df, fold=fold, **vars(config.data))\n    model = ToxicModule(**vars(config.model))\n    trainer = Trainer(**vars(config.trainer))\n\n    trainer.fit(model, datamodule=dm)\n    preds = trainer.predict(model, datamodule=dm)\n    preds = torch.cat(preds).view(-1) / config.n_folds\n\n    if final_preds is None:\n        final_preds = preds\n    else:\n        final_preds += preds\n\n    del trainer, dm, model\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:45:30.693532Z","iopub.execute_input":"2021-12-08T21:45:30.695169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_df['score'] = final_preds.numpy()\npredict_df['score'] = predict_df['score'].rank(method='first')\npredict_df = predict_df.drop('text', axis=1)\npredict_df.to_csv(\"submission.csv\", header=True, index=False)\n\nprint(predict_df.shape)\npredict_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:42:30.875349Z","iopub.execute_input":"2021-12-08T21:42:30.875647Z","iopub.status.idle":"2021-12-08T21:42:30.924455Z","shell.execute_reply.started":"2021-12-08T21:42:30.875615Z","shell.execute_reply":"2021-12-08T21:42:30.923407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}