{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"A variation of https://www.kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768 ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import rankdata","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:58:25.114967Z","iopub.execute_input":"2021-11-22T12:58:25.115359Z","iopub.status.idle":"2021-11-22T12:58:26.159454Z","shell.execute_reply.started":"2021-11-22T12:58:25.115231Z","shell.execute_reply":"2021-11-22T12:58:26.158543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create train data\n\nUsing data from [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n\nThe target was multioutput, we turn it into linear,  using weighted toxic behaviors\n\nThe types of toxicity are: 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'","metadata":{}},{"cell_type":"code","source":"jc_train_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\nprint(f\"jc_train_df:{jc_train_df.shape}\")\njc_test_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n\ntemp_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\")\njc_test_df = jc_test_df.merge ( temp_df, on =\"id\")\nprint(f\"jc_test_df:{jc_test_df.shape}\")\njc_test_df = jc_test_df.query (\"toxic != -1\")\nprint(f\"jc_test_df:{jc_test_df.shape}\")\ndf = jc_train_df.append(jc_test_df)\n\n\ndf[\"toxic_flag\"] = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].sum(axis=1)\ndf = df.rename(columns={'comment_text': 'text'})\n\n\n\n#undersample non toxic comments  on Toxic Comment Classification Challenge\nmin_len = (df['toxic_flag'] >= 1).sum() \ndf_y0_undersample = df[df['toxic_flag'] == 0].sample(n=int(min_len*2.5),random_state=201)\ndf = pd.concat([df[df['toxic_flag'] >= 1], df_y0_undersample])\n\ntoxic = 0.71\nsevere_toxic = 0.75\nobscene = 1.47\nthreat = 0.0\ninsult = 0.66\nidentity_hate = 1.36 \n\n\ndf['y'] = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].max(axis=1)\ndf['y'] = df[\"y\"]+df['toxic']*toxic\ndf['y'] = df[\"y\"]+df['severe_toxic']*severe_toxic\ndf['y'] = df[\"y\"]+df['obscene']*obscene\ndf['y'] = df[\"y\"]+df['threat']*threat\ndf['y'] = df[\"y\"]+df['insult']*insult\ndf['y'] = df[\"y\"]+df['identity_hate']*identity_hate\ny = df['y'].values","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:58:26.161323Z","iopub.execute_input":"2021-11-22T12:58:26.161625Z","iopub.status.idle":"2021-11-22T12:58:30.362428Z","shell.execute_reply.started":"2021-11-22T12:58:26.161586Z","shell.execute_reply":"2021-11-22T12:58:30.36148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF","metadata":{}},{"cell_type":"code","source":"vec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(3, 5) )\n\nX = vec.fit_transform(df['text'])\n\nX.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:59:38.840236Z","iopub.execute_input":"2021-11-22T12:59:38.840578Z","iopub.status.idle":"2021-11-22T13:00:27.016166Z","shell.execute_reply.started":"2021-11-22T12:59:38.840545Z","shell.execute_reply":"2021-11-22T13:00:27.015337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"model = Ridge(alpha = 1.0)\nmodel.fit(X, df['y'])\n\n\n### validate\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n\nX_less_toxic = vec.transform(df_val['less_toxic'])\nX_more_toxic = vec.transform(df_val['more_toxic'])\n\np1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)\n\n# Validation Accuracy\n(p1< p2).mean()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T13:00:43.673395Z","iopub.execute_input":"2021-11-22T13:00:43.673657Z","iopub.status.idle":"2021-11-22T13:01:39.523537Z","shell.execute_reply.started":"2021-11-22T13:00:43.67363Z","shell.execute_reply":"2021-11-22T13:01:39.5229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nX_test = vec.transform(df_sub['text'])\nscore = model.predict(X_test)\n\n\n## to enforce unique values on score\ndf_sub['score'] = rankdata(score, method='ordinal')\n\ndf_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n\ndf_sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T13:01:49.289259Z","iopub.execute_input":"2021-11-22T13:01:49.290133Z","iopub.status.idle":"2021-11-22T13:01:54.429064Z","shell.execute_reply.started":"2021-11-22T13:01:49.290093Z","shell.execute_reply":"2021-11-22T13:01:54.428224Z"},"trusted":true},"execution_count":null,"outputs":[]}]}