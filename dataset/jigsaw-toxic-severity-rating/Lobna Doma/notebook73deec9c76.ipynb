{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re \nimport string\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-25T17:05:45.074206Z","iopub.execute_input":"2022-01-25T17:05:45.074459Z","iopub.status.idle":"2022-01-25T17:05:45.175699Z","shell.execute_reply.started":"2022-01-25T17:05:45.074431Z","shell.execute_reply":"2022-01-25T17:05:45.175017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.tokenize import TabTokenizer\ntokenizer = TabTokenizer()\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:05:46.092169Z","iopub.execute_input":"2022-01-25T17:05:46.092428Z","iopub.status.idle":"2022-01-25T17:05:56.743089Z","shell.execute_reply.started":"2022-01-25T17:05:46.0924Z","shell.execute_reply":"2022-01-25T17:05:56.742343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training ","metadata":{}},{"cell_type":"code","source":"pip install torch","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:05:56.744859Z","iopub.execute_input":"2022-01-25T17:05:56.745313Z","iopub.status.idle":"2022-01-25T17:06:04.829026Z","shell.execute_reply.started":"2022-01-25T17:05:56.745276Z","shell.execute_reply":"2022-01-25T17:06:04.828135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:06:04.830632Z","iopub.execute_input":"2022-01-25T17:06:04.831312Z","iopub.status.idle":"2022-01-25T17:06:12.035947Z","shell.execute_reply.started":"2022-01-25T17:06:04.831266Z","shell.execute_reply":"2022-01-25T17:06:12.035072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:06:12.038638Z","iopub.execute_input":"2022-01-25T17:06:12.038926Z","iopub.status.idle":"2022-01-25T17:06:13.15319Z","shell.execute_reply.started":"2022-01-25T17:06:12.038887Z","shell.execute_reply":"2022-01-25T17:06:13.152431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport requests\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline\nimport datasets\nfrom datasets import Dataset\n!pip3 install deberta\n#pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:06:13.154462Z","iopub.execute_input":"2022-01-25T17:06:13.154704Z","iopub.status.idle":"2022-01-25T17:06:23.733775Z","shell.execute_reply.started":"2022-01-25T17:06:13.154671Z","shell.execute_reply":"2022-01-25T17:06:23.732856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install datasets","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:06:23.737466Z","iopub.execute_input":"2022-01-25T17:06:23.737726Z","iopub.status.idle":"2022-01-25T17:06:31.639133Z","shell.execute_reply.started":"2022-01-25T17:06:23.737695Z","shell.execute_reply":"2022-01-25T17:06:31.638221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import DebertaTokenizer, DebertaModel","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:06:31.640654Z","iopub.execute_input":"2022-01-25T17:06:31.641267Z","iopub.status.idle":"2022-01-25T17:06:31.716366Z","shell.execute_reply.started":"2022-01-25T17:06:31.641225Z","shell.execute_reply":"2022-01-25T17:06:31.715607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:06:48.525009Z","iopub.execute_input":"2022-01-25T17:06:48.525788Z","iopub.status.idle":"2022-01-25T17:06:54.623657Z","shell.execute_reply.started":"2022-01-25T17:06:48.525752Z","shell.execute_reply":"2022-01-25T17:06:54.622879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n#replace the html characters with \" \"\n    text=re.sub('<.*?>', ' ', text)  \n#remove the punctuations\n    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n#consider only alphabets and numerics\n    text = re.sub('[^a-zA-Z]',' ',text)  \n#replace newline with space\n    text = re.sub(\"\\n\",\" \",text)\n#convert to lower case\n    text = text.lower()\n#split and join the words\n    text=' '.join(text.split())\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:09:13.32904Z","iopub.execute_input":"2022-01-25T17:09:13.329679Z","iopub.status.idle":"2022-01-25T17:09:13.335233Z","shell.execute_reply.started":"2022-01-25T17:09:13.329633Z","shell.execute_reply":"2022-01-25T17:09:13.334445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# url = \"https://raw.githubusercontent.com/lobnadoma/data/main/comments_to_score.csv\"\n# df = pd.read_csv(url)\ncts = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nvalidation = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\")\nsample = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:09:13.434859Z","iopub.execute_input":"2022-01-25T17:09:13.435351Z","iopub.status.idle":"2022-01-25T17:09:14.024218Z","shell.execute_reply.started":"2022-01-25T17:09:13.435319Z","shell.execute_reply":"2022-01-25T17:09:14.023441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ndf3 = df2[['id','comment_text','toxic','severe_toxic']]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:09:14.025773Z","iopub.execute_input":"2022-01-25T17:09:14.026031Z","iopub.status.idle":"2022-01-25T17:09:15.834613Z","shell.execute_reply.started":"2022-01-25T17:09:14.025994Z","shell.execute_reply":"2022-01-25T17:09:15.833827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:09:15.836399Z","iopub.execute_input":"2022-01-25T17:09:15.83669Z","iopub.status.idle":"2022-01-25T17:09:15.856503Z","shell.execute_reply.started":"2022-01-25T17:09:15.836643Z","shell.execute_reply":"2022-01-25T17:09:15.855861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2[(df2['insult']==1) & (df2['toxic']==1)]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:09:15.858091Z","iopub.execute_input":"2022-01-25T17:09:15.858484Z","iopub.status.idle":"2022-01-25T17:09:15.879285Z","shell.execute_reply.started":"2022-01-25T17:09:15.858447Z","shell.execute_reply":"2022-01-25T17:09:15.878653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"df3['comment_text'] = df3['comment_text'].replace(regex=['<.*?>'],value=' ')\ndf3['comment_text'] = df3['comment_text'].replace(regex=['[^a-zA-Z]'],value=' ')\ndf3['comment_text'] = df3['comment_text'].replace(regex=['\\n'],value=' ')","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:09:54.72741Z","iopub.execute_input":"2022-01-25T17:09:54.727688Z","iopub.status.idle":"2022-01-25T17:10:01.422072Z","shell.execute_reply.started":"2022-01-25T17:09:54.727658Z","shell.execute_reply":"2022-01-25T17:10:01.421171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df3['comment_text'] = df3['comment_text'].apply(lambda x: ''.join([w for w in clean_text(x)]))\ncts['text'] = cts['text'].apply(lambda x: ''.join([w for w in clean_text(x)]))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:10:01.423962Z","iopub.execute_input":"2022-01-25T17:10:01.424248Z","iopub.status.idle":"2022-01-25T17:10:01.871333Z","shell.execute_reply.started":"2022-01-25T17:10:01.42421Z","shell.execute_reply":"2022-01-25T17:10:01.870611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2['other'] = df2['obscene']+df2['threat']+df2['insult']+df2['identity_hate']","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:10:01.87244Z","iopub.execute_input":"2022-01-25T17:10:01.872681Z","iopub.status.idle":"2022-01-25T17:10:01.879703Z","shell.execute_reply.started":"2022-01-25T17:10:01.872646Z","shell.execute_reply":"2022-01-25T17:10:01.879038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2[((df2['toxic']==0) & (df2['other']>0))]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:10:01.881529Z","iopub.execute_input":"2022-01-25T17:10:01.881958Z","iopub.status.idle":"2022-01-25T17:10:01.904372Z","shell.execute_reply.started":"2022-01-25T17:10:01.881921Z","shell.execute_reply":"2022-01-25T17:10:01.903644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3['toxic'] = df3['toxic'] + df3['severe_toxic']","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:14:59.888578Z","iopub.execute_input":"2022-01-25T15:14:59.889032Z","iopub.status.idle":"2022-01-25T15:14:59.896364Z","shell.execute_reply.started":"2022-01-25T15:14:59.888982Z","shell.execute_reply":"2022-01-25T15:14:59.895151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3[df3['toxic']>0]\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:14:59.898621Z","iopub.execute_input":"2022-01-25T15:14:59.899476Z","iopub.status.idle":"2022-01-25T15:14:59.922427Z","shell.execute_reply.started":"2022-01-25T15:14:59.899439Z","shell.execute_reply":"2022-01-25T15:14:59.921257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df4 = df3[['toxic','comment_text']]\ndf4.columns = ['label','text']","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:14:59.924455Z","iopub.execute_input":"2022-01-25T15:14:59.924793Z","iopub.status.idle":"2022-01-25T15:14:59.937449Z","shell.execute_reply.started":"2022-01-25T15:14:59.924745Z","shell.execute_reply":"2022-01-25T15:14:59.936322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df4 = df4.sample(frac=0.1, replace=False, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:14:59.940556Z","iopub.execute_input":"2022-01-25T15:14:59.941333Z","iopub.status.idle":"2022-01-25T15:14:59.957851Z","shell.execute_reply.started":"2022-01-25T15:14:59.941235Z","shell.execute_reply":"2022-01-25T15:14:59.956854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df4['text'], df4['label'], test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:14:59.959138Z","iopub.execute_input":"2022-01-25T15:14:59.959373Z","iopub.status.idle":"2022-01-25T15:14:59.974629Z","shell.execute_reply.started":"2022-01-25T15:14:59.959341Z","shell.execute_reply":"2022-01-25T15:14:59.973283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = {'text': X_train.values.tolist(), 'label': y_train.values.tolist()}\ntrain = pd.DataFrame(data=d)\nd = {'text': X_test.values.tolist(), 'label': y_test.values.tolist()}\ntest = pd.DataFrame(data=d)\ntrain['label'] = train.label.astype(\"category\").cat.codes\ntest['label'] = test.label.astype(\"category\").cat.codes","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:14:59.976241Z","iopub.execute_input":"2022-01-25T15:14:59.977208Z","iopub.status.idle":"2022-01-25T15:15:00.006001Z","shell.execute_reply.started":"2022-01-25T15:14:59.977156Z","shell.execute_reply":"2022-01-25T15:15:00.004979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_d = Dataset.from_pandas(train)\ntest_d = Dataset.from_pandas(test)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:15:00.007545Z","iopub.execute_input":"2022-01-25T15:15:00.00786Z","iopub.status.idle":"2022-01-25T15:15:00.031334Z","shell.execute_reply.started":"2022-01-25T15:15:00.007814Z","shell.execute_reply":"2022-01-25T15:15:00.0304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_d","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:15:00.033652Z","iopub.execute_input":"2022-01-25T15:15:00.03463Z","iopub.status.idle":"2022-01-25T15:15:00.042382Z","shell.execute_reply.started":"2022-01-25T15:15:00.034581Z","shell.execute_reply":"2022-01-25T15:15:00.041203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"],  padding=\"max_length\", truncation=True)\n\n\ntokenized_train = train_d.map(tokenize_function, batched=True)\ntokenized_test= test_d.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:15:00.044415Z","iopub.execute_input":"2022-01-25T15:15:00.045008Z","iopub.status.idle":"2022-01-25T15:15:08.090132Z","shell.execute_reply.started":"2022-01-25T15:15:00.044959Z","shell.execute_reply":"2022-01-25T15:15:08.088958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n#DebertaModel\nmodel = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', problem_type=\"multi_label_classification\")\n#model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=7)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:11:13.815654Z","iopub.execute_input":"2022-01-25T13:11:13.816005Z","iopub.status.idle":"2022-01-25T13:11:16.055568Z","shell.execute_reply.started":"2022-01-25T13:11:13.815967Z","shell.execute_reply":"2022-01-25T13:11:16.054792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\ntraining_args = TrainingArguments(\"test_trainer\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:11:48.004351Z","iopub.execute_input":"2022-01-25T13:11:48.00461Z","iopub.status.idle":"2022-01-25T13:11:48.023512Z","shell.execute_reply.started":"2022-01-25T13:11:48.00458Z","shell.execute_reply":"2022-01-25T13:11:48.022877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args, \n    train_dataset=tokenized_train, \n    eval_dataset=tokenized_test\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:12:08.236045Z","iopub.execute_input":"2022-01-25T13:12:08.236496Z","iopub.status.idle":"2022-01-25T13:12:16.43912Z","shell.execute_reply.started":"2022-01-25T13:12:08.236458Z","shell.execute_reply":"2022-01-25T13:12:16.438168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:12:29.457252Z","iopub.execute_input":"2022-01-25T13:12:29.457549Z","iopub.status.idle":"2022-01-25T13:12:48.15794Z","shell.execute_reply.started":"2022-01-25T13:12:29.457516Z","shell.execute_reply":"2022-01-25T13:12:48.155358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained('deberta-model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\nclf = LogisticRegression(max_iter = 200).fit(X_train_tfidf, y_train)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test_tfidf, y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:34:04.683069Z","iopub.execute_input":"2022-01-18T17:34:04.683283Z","iopub.status.idle":"2022-01-18T17:34:06.991148Z","shell.execute_reply.started":"2022-01-18T17:34:04.683258Z","shell.execute_reply":"2022-01-18T17:34:06.989868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict_proba(vectorizer.transform(cts['text']))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:34:11.136828Z","iopub.execute_input":"2022-01-18T17:34:11.137486Z","iopub.status.idle":"2022-01-18T17:34:11.164813Z","shell.execute_reply.started":"2022-01-18T17:34:11.137447Z","shell.execute_reply":"2022-01-18T17:34:11.163495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.classes_","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:34:11.316697Z","iopub.execute_input":"2022-01-18T17:34:11.317303Z","iopub.status.idle":"2022-01-18T17:34:11.340754Z","shell.execute_reply.started":"2022-01-18T17:34:11.317272Z","shell.execute_reply":"2022-01-18T17:34:11.339033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = pd.DataFrame(y_pred, columns = ['not','toxic','severe'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs['combined'] = probs['toxic'] + probs['severe']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cts2 = cts.copy()\ncts2['score'] = probs['combined']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cts2 = cts2[['comment_id','score']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cts2['score'] = cts2['score'].round(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cts2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cts2.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Model","metadata":{}},{"cell_type":"code","source":"pip install wandb --upgrade","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:45:54.067797Z","iopub.execute_input":"2022-01-19T14:45:54.068322Z","iopub.status.idle":"2022-01-19T14:46:03.373747Z","shell.execute_reply.started":"2022-01-19T14:45:54.068288Z","shell.execute_reply":"2022-01-19T14:46:03.372873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModel, AutoTokenizer\ntransformers.__version__","metadata":{"execution":{"iopub.status.busy":"2022-01-23T14:26:40.944583Z","iopub.execute_input":"2022-01-23T14:26:40.945124Z","iopub.status.idle":"2022-01-23T14:26:42.097566Z","shell.execute_reply.started":"2022-01-23T14:26:40.945083Z","shell.execute_reply":"2022-01-23T14:26:42.096755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path_or_name = '../input/transformers/roberta-base'\nmodel     = AutoModel.from_pretrained(model_path_or_name)\ntokenizer = AutoTokenizer.from_pretrained(model_path_or_name)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:06:07.76425Z","iopub.execute_input":"2022-01-19T11:06:07.764662Z","iopub.status.idle":"2022-01-19T11:06:08.65229Z","shell.execute_reply.started":"2022-01-19T11:06:07.764596Z","shell.execute_reply":"2022-01-19T11:06:08.651175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport requests\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline\n# import datasets\n# from datasets import Dataset\n!pip3 install deberta\n#pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-01-23T14:26:49.632111Z","iopub.execute_input":"2022-01-23T14:26:49.632639Z","iopub.status.idle":"2022-01-23T14:27:21.21145Z","shell.execute_reply.started":"2022-01-23T14:26:49.632599Z","shell.execute_reply":"2022-01-23T14:27:21.210638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh |  bash\n!apt-get install -y --allow-unauthenticated git-lfs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git lfs install\n!git clone https://huggingface.co/GroNLP/hateBERT\n# if you want to clone without large files – just their pointers\n# prepend your git clone with the following env var:\n!GIT_LFS_SKIP_SMUDGE=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def setup_tensorflow_1_13():\n    \n    # Install `tensorflow-gpu==1.13.1` from pre-downloaded wheels\n    PATH_TO_TF_WHEELS = '/kaggle/input/tensorflow1131-offline-bert/tensorflow_gpu_1_13_1_with_deps_whl/tensorflow_gpu_1_13_1_with_deps_whl'\n    # yes, mixing up Python code and bash is ugly. But it's handy \n    !pip install --no-deps $PATH_TO_TF_WHEELS/*.whl\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ruddit = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\nprint(ruddit.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:06:27.144821Z","iopub.execute_input":"2022-01-19T11:06:27.145095Z","iopub.status.idle":"2022-01-19T11:06:27.21599Z","shell.execute_reply.started":"2022-01-19T11:06:27.145064Z","shell.execute_reply":"2022-01-19T11:06:27.214727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ruddit.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:06:27.344535Z","iopub.execute_input":"2022-01-19T11:06:27.344734Z","iopub.status.idle":"2022-01-19T11:06:27.357212Z","shell.execute_reply.started":"2022-01-19T11:06:27.34471Z","shell.execute_reply":"2022-01-19T11:06:27.356261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ruddit = ruddit[['txt', 'offensiveness_score']].rename(columns={'txt': 'text',\n                                                                'offensiveness_score':'y'})","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:06:29.961887Z","iopub.execute_input":"2022-01-19T11:06:29.962381Z","iopub.status.idle":"2022-01-19T11:06:29.971162Z","shell.execute_reply.started":"2022-01-19T11:06:29.962327Z","shell.execute_reply":"2022-01-19T11:06:29.970252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Rescale values to range between 0 and 1\nruddit['y'] = (ruddit['y'] - ruddit.y.min()) / (ruddit.y.max() - ruddit.y.min()) \nruddit.y.hist()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:06:30.182546Z","iopub.execute_input":"2022-01-19T11:06:30.182982Z","iopub.status.idle":"2022-01-19T11:06:30.436925Z","shell.execute_reply.started":"2022-01-19T11:06:30.182952Z","shell.execute_reply":"2022-01-19T11:06:30.436209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install pytorch_transformers","metadata":{"execution":{"iopub.status.busy":"2022-01-24T15:52:28.161963Z","iopub.execute_input":"2022-01-24T15:52:28.162173Z","iopub.status.idle":"2022-01-24T15:52:37.211787Z","shell.execute_reply.started":"2022-01-24T15:52:28.162151Z","shell.execute_reply":"2022-01-24T15:52:37.211196Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pytorch_transformers\nimport datasets\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2022-01-24T15:52:37.213063Z","iopub.execute_input":"2022-01-24T15:52:37.213257Z","iopub.status.idle":"2022-01-24T15:52:37.933128Z","shell.execute_reply.started":"2022-01-24T15:52:37.213235Z","shell.execute_reply":"2022-01-24T15:52:37.932663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom transformers import AutoTokenizer,AutoModelForSequenceClassification\nconfig = AutoConfig.from_pretrained(f'../input/hatebert/')\ntokenizer = AutoTokenizer.from_pretrained(f'../input/hatebert/')\nmodel = AutoModelForSequenceClassification.from_pretrained(f'../input/hatebert/', num_labels = 3)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:47:01.205799Z","iopub.execute_input":"2022-01-19T14:47:01.206332Z","iopub.status.idle":"2022-01-19T14:47:08.043301Z","shell.execute_reply.started":"2022-01-19T14:47:01.206296Z","shell.execute_reply":"2022-01-19T14:47:08.042596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df4 = df3[['toxic','comment_text']]\ndf4.columns = ['label','text']","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:47:33.673044Z","iopub.execute_input":"2022-01-19T14:47:33.673321Z","iopub.status.idle":"2022-01-19T14:47:33.700175Z","shell.execute_reply.started":"2022-01-19T14:47:33.673291Z","shell.execute_reply":"2022-01-19T14:47:33.698272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df4 = df4.sample(frac=0.25, replace=False, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:47:33.865266Z","iopub.execute_input":"2022-01-19T14:47:33.865535Z","iopub.status.idle":"2022-01-19T14:47:33.884019Z","shell.execute_reply.started":"2022-01-19T14:47:33.865505Z","shell.execute_reply":"2022-01-19T14:47:33.883283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df4['text'], df4['label'], test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:47:35.506486Z","iopub.execute_input":"2022-01-19T14:47:35.507079Z","iopub.status.idle":"2022-01-19T14:47:35.521982Z","shell.execute_reply.started":"2022-01-19T14:47:35.507021Z","shell.execute_reply":"2022-01-19T14:47:35.521269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = {'text': X_train.values.tolist(), 'label': y_train.values.tolist()}\ntrain = pd.DataFrame(data=d)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:47:37.330325Z","iopub.execute_input":"2022-01-19T14:47:37.330583Z","iopub.status.idle":"2022-01-19T14:47:37.352097Z","shell.execute_reply.started":"2022-01-19T14:47:37.330555Z","shell.execute_reply":"2022-01-19T14:47:37.351129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = {'text': X_train.values.tolist(), 'label': y_train.values.tolist()}\ntrain = pd.DataFrame(data=d)\nd = {'text': X_test.values.tolist(), 'label': y_test.values.tolist()}\ntest = pd.DataFrame(data=d)\ntrain['label'] = train.label.astype(\"category\").cat.codes\ntest['label'] = test.label.astype(\"category\").cat.codes","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:47:37.48459Z","iopub.execute_input":"2022-01-19T14:47:37.485235Z","iopub.status.idle":"2022-01-19T14:47:37.524049Z","shell.execute_reply.started":"2022-01-19T14:47:37.485198Z","shell.execute_reply":"2022-01-19T14:47:37.523272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_d['label'][0]","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:47:37.628627Z","iopub.execute_input":"2022-01-19T14:47:37.62915Z","iopub.status.idle":"2022-01-19T14:47:37.687443Z","shell.execute_reply.started":"2022-01-19T14:47:37.629113Z","shell.execute_reply":"2022-01-19T14:47:37.686532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_d['text'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_d = Dataset.from_pandas(train)\ntest_d = Dataset.from_pandas(test)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:47:44.760108Z","iopub.execute_input":"2022-01-19T14:47:44.761048Z","iopub.status.idle":"2022-01-19T14:47:44.812908Z","shell.execute_reply.started":"2022-01-19T14:47:44.760995Z","shell.execute_reply":"2022-01-19T14:47:44.812129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized_train = train_d.map(tokenize_function, batched=True)\ntokenized_test= test_d.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:47:48.12209Z","iopub.execute_input":"2022-01-19T14:47:48.12286Z","iopub.status.idle":"2022-01-19T14:48:04.189511Z","shell.execute_reply.started":"2022-01-19T14:47:48.122805Z","shell.execute_reply":"2022-01-19T14:48:04.188793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\ntraining_args = TrainingArguments(\"test_trainer\")","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:48:16.374382Z","iopub.execute_input":"2022-01-19T14:48:16.37466Z","iopub.status.idle":"2022-01-19T14:48:16.391107Z","shell.execute_reply.started":"2022-01-19T14:48:16.374629Z","shell.execute_reply":"2022-01-19T14:48:16.390429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args, \n    train_dataset=tokenized_train, \n    eval_dataset=tokenized_test\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:48:16.887261Z","iopub.execute_input":"2022-01-19T14:48:16.887882Z","iopub.status.idle":"2022-01-19T14:48:24.931931Z","shell.execute_reply.started":"2022-01-19T14:48:16.887841Z","shell.execute_reply":"2022-01-19T14:48:24.930984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train['input_ids'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:49:25.977938Z","iopub.execute_input":"2022-01-19T14:49:25.978217Z","iopub.status.idle":"2022-01-19T15:52:57.89471Z","shell.execute_reply.started":"2022-01-19T14:49:25.978184Z","shell.execute_reply":"2022-01-19T15:52:57.893551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git lfs install\n!git clone https://huggingface.co/GroNLP/hateBERT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# New Section","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport string\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Utils\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AdamW\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:10:23.19762Z","iopub.execute_input":"2022-01-25T15:10:23.198798Z","iopub.status.idle":"2022-01-25T15:10:23.21127Z","shell.execute_reply.started":"2022-01-25T15:10:23.198757Z","shell.execute_reply":"2022-01-25T15:10:23.209782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:10:27.904031Z","iopub.execute_input":"2022-01-25T15:10:27.904362Z","iopub.status.idle":"2022-01-25T15:10:28.815857Z","shell.execute_reply.started":"2022-01-25T15:10:27.904328Z","shell.execute_reply":"2022-01-25T15:10:28.814722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def id_generator(size=12, chars=string.ascii_lowercase + string.digits):\n    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n\nHASH_NAME = id_generator(size=12)\nprint(HASH_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:10:39.396361Z","iopub.execute_input":"2022-01-25T15:10:39.396688Z","iopub.status.idle":"2022-01-25T15:10:39.413672Z","shell.execute_reply.started":"2022-01-25T15:10:39.396637Z","shell.execute_reply":"2022-01-25T15:10:39.412577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"seed\": 42,\n          \"epochs\": 3,\n          \"model_name\": \"GroNLP/hateBERT\",\n          \"train_batch_size\": 16,\n          \"valid_batch_size\": 32,\n          'gradient_accumulation_steps' : 2,\n          \"max_length\": 256,\n          \"learning_rate\": 1e-5,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-6,\n          \"T_max\": 500,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 5,\n          \"n_accumulate\": 1,\n          \"num_classes\": 1,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          \"hash_name\": HASH_NAME\n          }\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\nCONFIG['group'] = f'{HASH_NAME}-Baseline'\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T15:10:41.472954Z","iopub.execute_input":"2022-01-25T15:10:41.473553Z","iopub.status.idle":"2022-01-25T15:10:49.072532Z","shell.execute_reply.started":"2022-01-25T15:10:41.473516Z","shell.execute_reply":"2022-01-25T15:10:49.071512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:34:29.416805Z","iopub.execute_input":"2022-01-23T22:34:29.41707Z","iopub.status.idle":"2022-01-23T22:34:29.425972Z","shell.execute_reply.started":"2022-01-23T22:34:29.417041Z","shell.execute_reply":"2022-01-23T22:34:29.425271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:34:38.006716Z","iopub.execute_input":"2022-01-23T22:34:38.007261Z","iopub.status.idle":"2022-01-23T22:34:38.534529Z","shell.execute_reply.started":"2022-01-23T22:34:38.007223Z","shell.execute_reply":"2022-01-23T22:34:38.533829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n\nfor fold, ( _, val_) in enumerate(skf.split(X=df, y=df.worker)):\n    df.loc[val_ , \"kfold\"] = int(fold)\n    \ndf[\"kfold\"] = df[\"kfold\"].astype(int)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:35:02.368815Z","iopub.execute_input":"2022-01-23T22:35:02.369068Z","iopub.status.idle":"2022-01-23T22:35:02.419346Z","shell.execute_reply.started":"2022-01-23T22:35:02.36904Z","shell.execute_reply":"2022-01-23T22:35:02.41855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Class","metadata":{}},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_lenth\n        self.text = df['text'].values\n        self.toxicity = df['toxicity'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        toxicity = self.toxicity[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        \n        return {\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Dataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_lenth\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        \n        \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        \n        return {\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n    def __len__(self):\n        return len(self.df)\n    \n    class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs\n    \n    \n    def criterion(outputs1, outputs2, targets):\n    return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        \n        return {\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:35:51.513126Z","iopub.execute_input":"2022-01-23T22:35:51.513411Z","iopub.status.idle":"2022-01-23T22:35:51.523376Z","shell.execute_reply.started":"2022-01-23T22:35:51.513381Z","shell.execute_reply":"2022-01-23T22:35:51.522704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Class","metadata":{}},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:36:14.69318Z","iopub.execute_input":"2022-01-23T22:36:14.693865Z","iopub.status.idle":"2022-01-23T22:36:14.700827Z","shell.execute_reply.started":"2022-01-23T22:36:14.693825Z","shell.execute_reply":"2022-01-23T22:36:14.700167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Function\n","metadata":{}},{"cell_type":"code","source":"def criterion(outputs1, outputs2, targets):\n    return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:39:27.434994Z","iopub.execute_input":"2022-01-23T22:39:27.435483Z","iopub.status.idle":"2022-01-23T22:39:27.439651Z","shell.execute_reply.started":"2022-01-23T22:39:27.435444Z","shell.execute_reply":"2022-01-23T22:39:27.438606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training function","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n        \n        batch_size = more_toxic_ids.size(0)\n\n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        loss = loss / CONFIG['n_accumulate']\n        loss.backward()\n    \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            optimizer.step()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:41:21.792543Z","iopub.execute_input":"2022-01-23T22:41:21.793029Z","iopub.status.idle":"2022-01-23T22:41:21.804705Z","shell.execute_reply.started":"2022-01-23T22:41:21.79299Z","shell.execute_reply":"2022-01-23T22:41:21.803064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Modified\ndef train_all_epoch(model, optimizer, scheduler, dataloader, device, epoch, num_epochs):\n    train_loss = []\n    \n    for epoch in range(1, num_epochs + 1): \n        model.train()\n\n        dataset_size = 0\n        running_loss = 0.0\n\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:\n            more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n            more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n            less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n            less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n            targets = data['target'].to(device, dtype=torch.long)\n\n            batch_size = more_toxic_ids.size(0)\n\n            more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n            less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n\n            loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n            loss = loss / CONFIG['n_accumulate']\n            loss.backward()\n\n            if (step + 1) % CONFIG['n_accumulate'] == 0:\n                optimizer.step()\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                if scheduler is not None:\n                    scheduler.step()\n\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n\n            epoch_loss = running_loss / dataset_size\n\n            bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                            LR=optimizer.param_groups[0]['lr'])\n        gc.collect()\n\n        train_loss.append(epoch_loss)\n\n\n\n# def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n#     # To automatically log gradients\n#     wandb.watch(model, log_freq=100)\n    \n#     if torch.cuda.is_available():\n#         print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n#     start = time.time()\n#     best_model_wts = copy.deepcopy(model.state_dict())\n#     best_epoch_loss = np.inf\n#     history = defaultdict(list)\n    \n#     for epoch in range(1, num_epochs + 1): \n#         gc.collect()\n#         train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n#                                            dataloader=train_loader, \n#                                            device=CONFIG['device'], epoch=epoch)\n        \n#         val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n#                                          epoch=epoch)\n    \n#         history['Train Loss'].append(train_epoch_loss)\n#         history['Valid Loss'].append(val_epoch_loss)\n        \n#         # Log the metrics\n#         wandb.log({\"Train Loss\": train_epoch_loss})\n#         wandb.log({\"Valid Loss\": val_epoch_loss})\n        \n#         # deep copy the model\n#         if val_epoch_loss <= best_epoch_loss:\n#             print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n#             best_epoch_loss = val_epoch_loss\n#             run.summary[\"Best Loss\"] = best_epoch_loss\n#             best_model_wts = copy.deepcopy(model.state_dict())\n#             PATH = f\"Loss-Fold-{fold}.bin\"\n#             torch.save(model.state_dict(), PATH)\n#             # Save a model file from the current directory\n#             print(f\"Model Saved{sr_}\")\n            \n#         print()\n    \n#     end = time.time()\n#     time_elapsed = end - start\n#     print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n#         time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n#     print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n    \n#     # load best model weights\n#     model.load_state_dict(best_model_wts)\n    \n#     return model, history\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation Function","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n        \n        batch_size = more_toxic_ids.size(0)\n\n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    gc.collect()\n    \n    return epoch_loss\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:41:45.783586Z","iopub.execute_input":"2022-01-23T22:41:45.78386Z","iopub.status.idle":"2022-01-23T22:41:45.792923Z","shell.execute_reply.started":"2022-01-23T22:41:45.78383Z","shell.execute_reply":"2022-01-23T22:41:45.792116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Training","metadata":{}},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n    # To automatically log gradients\n    wandb.watch(model, log_freq=100)\n    \n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_loss = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG['device'], epoch=epoch)\n        \n        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n                                         epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        \n        # Log the metrics\n        wandb.log({\"Train Loss\": train_epoch_loss})\n        wandb.log({\"Valid Loss\": val_epoch_loss})\n        \n        # deep copy the model\n        if val_epoch_loss <= best_epoch_loss:\n            print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n            best_epoch_loss = val_epoch_loss\n            run.summary[\"Best Loss\"] = best_epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"Loss-Fold-{fold}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            print(f\"Model Saved{sr_}\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:42:15.39251Z","iopub.execute_input":"2022-01-23T22:42:15.393044Z","iopub.status.idle":"2022-01-23T22:42:15.406172Z","shell.execute_reply.started":"2022-01-23T22:42:15.393007Z","shell.execute_reply":"2022-01-23T22:42:15.404951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = JigsawDataset(df_train, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=2, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:42:24.441523Z","iopub.execute_input":"2022-01-23T22:42:24.442209Z","iopub.status.idle":"2022-01-23T22:42:24.449539Z","shell.execute_reply.started":"2022-01-23T22:42:24.442157Z","shell.execute_reply":"2022-01-23T22:42:24.448691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:42:31.760023Z","iopub.execute_input":"2022-01-23T22:42:31.760419Z","iopub.status.idle":"2022-01-23T22:42:31.767222Z","shell.execute_reply.started":"2022-01-23T22:42:31.760387Z","shell.execute_reply":"2022-01-23T22:42:31.765449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Start Training","metadata":{}},{"cell_type":"code","source":"for fold in range(0, CONFIG['n_fold']):\n    print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n    run = wandb.init(project='Jigsaw', \n                     config=CONFIG,\n                     job_type='Train',\n                     group=CONFIG['group'],\n                     tags=['hateBERT', f'{HASH_NAME}', 'margin-loss'],\n                     name=f'{HASH_NAME}-fold-{fold}',\n                     anonymous='must')\n    \n    # Create Dataloaders\n    train_loader, valid_loader = prepare_loaders(fold=fold)\n    \n    model = JigsawModel(CONFIG['model_name'])\n    model.to(CONFIG['device'])\n    \n    # Define Optimizer and Scheduler\n    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n    scheduler = fetch_scheduler(optimizer)\n    \n    model, history = run_training(model, optimizer, scheduler,\n                                  device=CONFIG['device'],\n                                  num_epochs=CONFIG['epochs'],\n                                  fold=fold)\n    \n    run.finish()\n    \n    del model, history, train_loader, valid_loader\n    _ = gc.collect()\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T22:42:52.193442Z","iopub.execute_input":"2022-01-23T22:42:52.193922Z","iopub.status.idle":"2022-01-24T04:04:01.871995Z","shell.execute_reply.started":"2022-01-23T22:42:52.193885Z","shell.execute_reply":"2022-01-24T04:04:01.871127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}