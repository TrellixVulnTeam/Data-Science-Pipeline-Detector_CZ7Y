{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport logging\nfrom sklearn.preprocessing import MinMaxScaler\n\nclass DataUtils:\n    def __init__(self) -> None:\n        pass\n\n    def load_different_data(self):\n        logging.info(\"Loading Jigsaw, C3 and ruddit data\")\n        try:\n\n            Jigsaw = pd.read_csv(\n                \"../input/alldataraw/train_data.csv\"\n            )\n            c3_data = pd.read_csv(\n                \"../input/alldataraw/C3_anonymized.csv\"\n            )\n            ruddit_data = pd.read_csv(\n                \"../input/alldataraw/ruddit_with_text.csv\"\n            )\n            logging.info(\"Data loaded successfully\")\n            return Jigsaw, c3_data, ruddit_data\n\n        except Exception as e:\n            logging.error(f\"Error loading data: {e}\")\n            raise e\n\n    def prepare_data(self):\n        logging.info(\"Preparing data for Further processing\")\n        try:\n            Jigsaw, c3_data, ruddit_data = self.load_different_data()\n            Jigsaw.drop([\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1, inplace=True)\n            # drop all the columns except comment_text and agree_toxicity_expt\n            c3_data = c3_data[[\"comment_text\", \"agree_toxicity_expt\"]]\n            ruddit_data = ruddit_data[[\"txt\", \"offensiveness_score\"]] \n            # rename the columns \n            c3_data.columns = [\"text\", \"y\"] \n            ruddit_data.columns = [\"text\", \"y\"]\n            # scaling all target values between 0-1\n            scaler = MinMaxScaler()\n            ruddit_data[\"y\"] = scaler.fit_transform(ruddit_data[['y']])\n            c3_data[\"y\"] = scaler.fit_transform(c3_data[[\"y\"]])\n            Jigsaw[\"y\"] = scaler.fit_transform(Jigsaw[[\"y\"]])\n            logging.info(\"Data prepared successfully\")\n            return Jigsaw, c3_data, ruddit_data\n        except Exception as e:\n            logging.error(f\"Error preparing data: {e}\")\n            raise e\n\n    def concatenate_dfs(self, Jigsaw, c3_data, ruddit_data): \n        logging.info(\"Concatenating dataframes\") \n        try: \n            # concatenate all the dataframes\n            data = pd.concat([Jigsaw, c3_data, ruddit_data], ignore_index=True)\n            logging.info(\"Data concatenated successfully\")\n            return data \n        except Exception as e: \n            logging.error(f\"Error concatenating data: {e}\")\n            raise e ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:19:00.909845Z","iopub.execute_input":"2022-01-25T11:19:00.910059Z","iopub.status.idle":"2022-01-25T11:19:01.801201Z","shell.execute_reply.started":"2022-01-25T11:19:00.910036Z","shell.execute_reply":"2022-01-25T11:19:01.800286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\n\n\n\nclass EDA:\n    def __init__(self, data, jigsaw, c3data, ruddit) -> None:\n        \"\"\" \n        Input: \n            data: dataframe ( concatenated with Jigsaw, C3 and ruddit data) \n            jigsaw: dataframe (Jigsaw data) \n            c3data: dataframe (C3 data) \n            ruddit: dataframe (ruddit data)\n        \"\"\"\n        self.data = data\n        self.jigsaw = jigsaw\n        self.c3data = c3data\n        self.ruddit = ruddit\n\n    def data_info(self):\n        \"\"\" \n        Basic Data Exploration like shape, info and etc \n        \"\"\"\n        logging.info(\"Basic Data Exploration\")\n        try:\n            print(\"Shape of the data: \", self.data.shape)\n            print(\"Info of the data: \", self.data.info())\n            print(\"Data types: \", self.data.dtypes)\n            print(\"Data head: \", self.data.head())\n        except Exception as e:\n            logging.error(f\"Error in basic data exploration: {e}\")\n            raise e\n\n    def exploring_distributions(self):\n        \"\"\" \n        distributions of target labels \n        \"\"\"\n        # exploring the distribution of target labels\n        logging.info(\"Exploring the distribution of target labels\")\n        try:\n            print(\"Showing Distribution of concatenated data\")\n            sns.distplot(self.data[\"y\"])\n            plt.show()\n            print(\"Showing Distribution of Jigsaw data\")\n            sns.distplot(self.jigsaw[\"y\"])\n            plt.show()\n            print(\"Showing Distribution of C3 data\")\n            sns.displot(self.c3data[\"y\"])\n            plt.show()\n            print(\"Showing Distribution of ruddit data\")\n            sns.distplot(self.ruddit[\"y\"])\n            plt.show()\n        except Exception as e:\n            logging.error(f\"Error in exploring distribution: {e}\")\n            raise e\n\n    def word_cloud(self):\n        \"\"\" \n        WordCloud \n        \"\"\"\n        logging.info(\"WordCloud of the data\")\n        try:\n            print(\"WordCloud of the data\")\n            # Generate a word cloud image\n            wordcloud = WordCloud(\n                background_color=\"white\",\n                stopwords=STOPWORDS,\n                max_words=200,\n                max_font_size=40,\n                random_state=42,\n            ).generate(str(self.data[\"text\"]))\n\n            # Display the generated image:\n            plt.figure(figsize=(20, 20))\n            plt.imshow(wordcloud, interpolation=\"bilinear\")\n            plt.axis(\"off\")\n            plt.show()\n            print(\"WordCloud of the Jigsaw data\")\n            wordcloud = WordCloud(\n                background_color=\"white\",\n                stopwords=STOPWORDS,\n                max_words=200,\n                max_font_size=40,\n                random_state=42,\n            ).generate(str(self.jigsaw[\"text\"]))\n\n            # Display the generated image:\n            plt.figure(figsize=(20, 20))\n            plt.imshow(wordcloud, interpolation=\"bilinear\")\n            plt.axis(\"off\")\n            plt.show()\n            print(\"WordCloud of the C3 data\")\n            wordcloud = WordCloud(\n                background_color=\"white\",\n                stopwords=STOPWORDS,\n                max_words=200,\n                max_font_size=40,\n                random_state=42,\n            ).generate(str(self.c3data[\"text\"]))\n\n            # Display the generated image:\n            plt.figure(figsize=(20, 20))\n            plt.imshow(wordcloud, interpolation=\"bilinear\")\n            plt.axis(\"off\")\n            plt.show()\n            print(\"WordCloud of the ruddit data\")\n            wordcloud = WordCloud(\n                background_color=\"white\",\n                stopwords=STOPWORDS,\n                max_words=200,\n                max_font_size=40,\n                random_state=42,\n            ).generate(str(self.ruddit[\"text\"]))\n\n            # Display the generated image:\n            plt.figure(figsize=(20, 20))\n            plt.imshow(wordcloud, interpolation=\"bilinear\")\n            plt.axis(\"off\")\n            plt.show()\n        except Exception as e:\n            logging.error(f\"Error in word cloud: {e}\")\n            raise e\n\n\n# if __name__ == \"__main__\":\n#     # =======================================================================\n#     # Load data\n#     data_utils = DataUtils()\n#     jigsaw_data, c3_data, ruddit_data = data_utils.load_different_data()\n#     Jigsaw, c3_data, ruddit_data = data_utils.prepare_data()\n#     data = data_utils.concatenate_dfs(Jigsaw, c3_data, ruddit_data)\n#     # =======================================================================\n\n#     # =======================================================================\n#     # Exploring data\n#     eda = EDA(data, jigsaw_data, c3_data, ruddit_data)\n#     eda.data_info() \n#     eda.exploring_distributions() \n#     eda.word_cloud() \n#     eda.skeweness_and_kurtosis() \n    # ========================================================================","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:19:01.803219Z","iopub.execute_input":"2022-01-25T11:19:01.803413Z","iopub.status.idle":"2022-01-25T11:19:01.994132Z","shell.execute_reply.started":"2022-01-25T11:19:01.803391Z","shell.execute_reply":"2022-01-25T11:19:01.993662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport logging\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.model_selection import train_test_split\n\nclass DataProcess:\n    def __init__(self, data) -> None:\n        self.data = data\n\n    def check_null_values(self):\n        \"\"\" \n        Check for null values in dataframe \n        \"\"\"\n        logging.info(\"Checking for null values in dataframe\")\n        try:\n            print(\"Checking for null values in dataframe\")\n            print(self.data.isnull().sum())\n        except Exception as e:\n            logging.error(f\"Error in checking null values: {e}\")\n            raise e\n\n    def apply_all_processing_on_train_test_data(self):\n        # apply all the Review processing methods on train and test data\n        logging.info(\n            \"Applying all the Review processing methods on train and test data\"\n        )\n        try:\n            self.data[\"text\"] = self.data[\"text\"].apply(\n                lambda x: self.Review_processing(x)\n            )\n            self.data[\"text\"] = self.data[\"text\"].apply(\n                lambda x: self.remove_punctuation(x)\n            )\n            self.data[\"text\"] = self.data[\"text\"].apply(\n                lambda x: self.remove_numbers(x)\n            )\n            self.data[\"text\"] = self.data[\"text\"].apply(\n                lambda x: self.remove_special_characters(x)\n            )\n            self.data[\"text\"] = self.data[\"text\"].apply(\n                lambda x: self.remove_short_words(x)\n            )\n            self.data[\"text\"] = self.data[\"text\"].apply(\n                lambda x: self.remove_stopwords(x)\n            )\n            self.data[\"text\"] = self.data[\"text\"].apply(\n                lambda x: self.lemmatization(x)\n            )\n\n            return self.data \n\n        except Exception as e:\n            logging.error(\n                \"Error in applying all the Review processing methods on train and test data\"\n            )\n            logging.error(e)\n            return None\n\n    def Review_processing(self, Review):\n        logging.info(\"Applying Review processing methods on train and test data\")\n        try:\n            Review = Review.lower()\n            Review = Review.replace(\"\\n\", \" \")\n            Review = Review.replace(\"\\r\", \" \")\n            Review = Review.replace(\"\\t\", \" \")\n            Review = Review.replace(\"\\xa0\", \" \")\n            Review = Review.replace(\"\\u200b\", \" \")\n            Review = Review.replace(\"\\u200c\", \" \")\n            Review = Review.replace(\"\\u200d\", \" \")\n            Review = Review.replace(\"\\ufeff\", \" \")\n            Review = Review.replace(\"\\ufeef\", \" \")\n        except Exception as e:\n            logging.error(\n                \"Error in applying Review processing methods on train and test data\"\n            )\n            logging.error(e)\n            return None\n        return Review\n\n    def stemming(self, Review):\n        logging.info(\"Applying stemming methods on train and test data\")\n        try:\n            Review = Review.split()\n            ps = PorterStemmer()\n            Review = [ps.stem(word) for word in Review]\n            Review = \" \".join(Review)\n        except Exception as e:\n            logging.error(\"Error in applying stemming methods on train and test data\")\n            logging.error(e)\n            return None\n        return Review\n\n    def lemmatization(self, Review):\n        Review = Review.split()\n        lem = WordNetLemmatizer()\n        Review = [lem.lemmatize(word) for word in Review]\n        Review = \" \".join(Review)\n        return Review\n\n    def remove_stopwords(self, Review):\n        Review = Review.split()\n        stop_words = set(stopwords.words(\"english\"))\n        Review = [word for word in Review if not word in stop_words]\n        Review = \" \".join(Review)\n        return Review\n\n    def remove_punctuation(self, Review):\n        # remove all punctuation except full stop, exclaimation mark and question mark\n        Review = Review.split()\n        Review = [word for word in Review if word.isalpha()]\n        Review = \" \".join(Review)\n\n        return Review\n\n    def remove_numbers(self, Review):\n        Review = Review.split()\n        Review = [word for word in Review if not word.isnumeric()]\n        Review = \" \".join(Review)\n        return Review\n\n    def remove_special_characters(self, Review):\n        Review = Review.split()\n        Review = [word for word in Review if word.isalpha()]\n        Review = \" \".join(Review)\n        return Review\n\n    def remove_short_words(self, Review):\n        Review = Review.split()\n        Review = [word for word in Review if len(word) > 2]\n        Review = \" \".join(Review)\n        return Review\n\n    def remove_stopwords_and_punctuation(self, Review):\n        Review = Review.split()\n        stop_words = set(stopwords.words(\"english\"))\n        Review = [word for word in Review if not word in stop_words]\n        Review = [word for word in Review if word.isalpha()]\n        Review = \" \".join(Review)\n        return Review\n\n    def remove_stopwords_and_punctuation_and_numbers(self, Review):\n        Review = Review.split()\n        stop_words = set(stopwords.words(\"english\"))\n        Review = [word for word in Review if not word in stop_words]\n        Review = [word for word in Review if word.isalpha()]\n        Review = [word for word in Review if not word.isnumeric()]\n        Review = \" \".join(Review)\n        return Review\n\n    def remove_nan_values(self, df):\n        # fill nan values with UNKOWN and return the dataframe\n        df = df.fillna(\"UNKOWN\")\n        return df\n\nclass DataValidation:\n    def __init__(self , data) -> None:\n        self.data = data\n    \n    def data_splitting(self): \n        logging.info(\"Data Splitting\")\n        try: \n            X = self.data.drop('y' , axis=1)\n            Y = self.data['y']\n\n            x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.25)\n            return x_train, x_test, y_train, y_test \n        except Exception as e:\n            logging.error(f\"Error loading data: {e}\")\n            raise e   \n# if __name__ == \"__main__\":\n#     # data_utils = DataUtils()\n#     # jigsaw_data, c3_data, ruddit_data = data_utils.load_different_data()\n#     # Jigsaw, c3_data, ruddit_data = data_utils.prepare_data()\n#     # data = data_utils.concatenate_dfs(Jigsaw, c3_data, ruddit_data)\n#     # data_process = DataProcess(data, jigsaw_data, c3_data, ruddit_data)\n#     # data_process.check_null_values()\n#     # data_processed = data_process.apply_all_processing_on_train_test_data() \n#     # data_processed.head() \n#     pass ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:19:02.508097Z","iopub.execute_input":"2022-01-25T11:19:02.509256Z","iopub.status.idle":"2022-01-25T11:19:03.070477Z","shell.execute_reply.started":"2022-01-25T11:19:02.50919Z","shell.execute_reply":"2022-01-25T11:19:03.070003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport logging  \nfrom sklearn.feature_extraction.text import TfidfVectorizer \nimport joblib  \nimport nltk\nfrom nltk import sent_tokenize,word_tokenize\n\nclass FeatureEngineering:\n\n    def __init__(self,data) -> None:\n        self.data = data\n\n    def get_count_of_words(self):\n        # get count of words in data\n        logging.info(\"Get count of words in data\")\n        word_count = self.data[\"text\"].apply(lambda x: len(x.split()))\n        return word_count\n    \n    def get_count_of_sentences(self):\n        # get count of sentences in data\n        logging.info(\"Get count of sentences in data\")\n        sentences_count = self.data[\"text\"].apply(\n            lambda x: len(sent_tokenize(x))\n        )\n        return sentences_count\n\n    def get_average_word_length(self):\n        # get average word length in data\n        logging.info(\"Get average word length in data\")\n        average_word_length = self.data[\"text\"].apply(\n            lambda x: np.mean([len(word) for word in x.split()])\n        )\n        return average_word_length\n\n    def get_average_sentence_length(self):\n        # get average sentence length in data\n        logging.info(\"Get average sentence length in data\")\n        average_sentence_length = self.data[\"text\"].apply(\n            lambda x: np.mean([len(sentence) for sentence in sent_tokenize(x)])\n        )\n        return average_sentence_length\n\n    def get_average_sentence_complexity(self):\n        # get average sentence complexity in data\n        logging.info(\"Get average sentence complexity in data\")\n        average_sentence_complexity = self.data[\"text\"].apply(\n            lambda x: np.mean(\n                [len(word_tokenize(sentence)) for sentence in sent_tokenize(x)]\n            )\n        )\n        return average_sentence_complexity\n\n    def get_average_word_complexity(self):\n        # get average word complexity in data\n        logging.info(\"Get average word complexity in data\")\n        average_word_complexity = self.data[\"text\"].apply(\n            lambda x: np.mean([len(word_tokenize(word)) for word in x.split()])\n        )\n        return average_word_complexity\n\n    def add_features(self):\n        logging.info(\"Add features\")\n        word_count = self.get_count_of_words()\n        sentences_count = self.get_count_of_sentences()\n        average_word_length = self.get_average_word_length()\n        average_sentence_length = self.get_average_sentence_length()\n        average_sentence_complexity = self.get_average_sentence_complexity()\n        average_word_complexity = self.get_average_word_complexity()\n        \n        self.data[\"count_of_words\"] = word_count\n        self.data[\"count_of_setences\"] = sentences_count\n        self.data[\"average_word_length\"] = average_word_length\n        self.data[\"average_sentence_length\"] = average_sentence_length\n        self.data[\"average_sentence_complexity\"] = average_sentence_complexity\n        self.data[\"average_word_complexity\"] = average_word_complexity\n\n        return self.data\n\n\nclass Vectorization:  \n    \"\"\" \n    NOT BEING USED\n    \"\"\"\n    def __init__(self, df) -> None:\n        self.df = df \n\n    def vectorize(self) -> pd.DataFrame:  \n        \"\"\" \n        Only vectorize concatenated data \n        \"\"\"\n        vectorizer = TfidfVectorizer(max_features=5000)\n\n        extracted_data = list(\n            vectorizer.fit_transform(self.df[\"text\"]).toarray()\n        )\n        extracted_data = pd.DataFrame(extracted_data)\n        extracted_data.head()\n        extracted_data.columns = vectorizer.get_feature_names()\n\n        vocab = vectorizer.vocabulary_\n        mapping = vectorizer.get_feature_names()\n        keys = list(vocab.keys())\n\n        extracted_data.shape\n        Modified_df = extracted_data.copy()\n        print(Modified_df.shape)\n        Modified_df.head()\n        Modified_df.reset_index(drop=True, inplace=True)\n        self.df.reset_index(drop=True, inplace=True)\n\n        Final_Training_data = pd.concat([self.df, Modified_df], axis=1)\n\n        Final_Training_data.head()\n        print(Final_Training_data.shape)\n        Final_Training_data.drop([\"text\"], axis=1, inplace=True)\n        Final_Training_data.head()\n        Final_Training_data.to_csv(\"Final_Training_vectorized.csv\", index=False)\n\n        # dff_test = list(vectorizer.transform(self.test_data[\"Review\"]).toarray())\n        # vocab_test = vectorizer.vocabulary_\n        # keys_test = list(vocab_test.keys())\n        # dff_test_df = pd.DataFrame(dff_test, columns=keys_test)\n        # dff_test_df.reset_index(drop=True, inplace=True)\n        # self.test_data.reset_index(drop=True, inplace=True)\n        # Final_Test = pd.concat([self.test_data, dff_test_df], axis=1)\n        # Final_Test.drop([\"Review\"], axis=1, inplace=True)\n        # Final_Test.to_csv(\"Final_Test_vectorized\", index=False)\n\n        # save the vectorizer to disk\n        joblib.dump(vectorizer, \"vectorizer.pkl\")\n        return Final_Training_data\n\n\n# if __name__ == '__main__': \n#     # data_utils = DataUtils()\n#     # jigsaw_data, c3_data, ruddit_data = data_utils.load_different_data()\n#     # Jigsaw, c3_data, ruddit_data = data_utils.prepare_data()\n#     # data = data_utils.concatenate_dfs(Jigsaw, c3_data, ruddit_data)\n#     # data = data.sample(5000)\n#     # # data.reset_index(inplace=True)\n    \n#     # data_process = DataProcess(data, jigsaw_data, c3_data, ruddit_data)\n#     # data_process.check_null_values()\n#     # data_processed = data_process.apply_all_processing_on_train_test_data() \n#     # # vectrorize = Vectorization(data) \n#     # # Final_Training_data = vectrorize.vectorize() \n#     # print(data.shape)\n#     # print(data.head())\n#     # fe = FeatureEngineering(data)\n#     # data = fe.add_features()\n#     # print(data.shape)\n#     # print(data.head())","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:19:03.352953Z","iopub.execute_input":"2022-01-25T11:19:03.353661Z","iopub.status.idle":"2022-01-25T11:19:03.372278Z","shell.execute_reply.started":"2022-01-25T11:19:03.353604Z","shell.execute_reply":"2022-01-25T11:19:03.37112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib.externals.loky.backend.spawn import import_main_path\nfrom numpy.lib.function_base import gradient\nimport pandas as pd\nimport numpy as np\nimport logging\n\nfrom scipy.sparse import data\nfrom scipy.sparse.construct import random\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nimport optuna\nfrom sklearn import linear_model\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom catboost import CatBoostClassifier\nimport catboost\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom mlxtend.regressor import StackingRegressor\n\nclass Hyperparameters_Optimization:\n    def __init__(self, x_train, y_train, x_test, y_test) -> None:\n        self.x_train = x_train\n        self.y_train = y_train\n        self.x_test = x_test\n        self.y_test = y_test\n\n    def optimize_decisiontrees(self, trial):\n        # criterion = trial.suggest_categorical(\"criterion\", (\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"))\n        max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n        reg = DecisionTreeRegressor(\n            max_depth=max_depth, min_samples_split=min_samples_split,\n        )\n        reg.fit(self.x_train, self.y_train)\n        val_accuracy = reg.score(self.x_test, self.y_test)\n        return val_accuracy\n\n    def optimize_randomforest(self, trial):\n        logging.info(\"optimize_randomforest\")\n        n_estimators = trial.suggest_int(\"n_estimators\", 1, 200)\n        max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n        reg = RandomForestRegressor(\n            n_estimators=n_estimators,\n            max_depth=max_depth,\n            min_samples_split=min_samples_split,\n        )\n        reg.fit(self.x_train, self.y_train)\n        val_accuracy = reg.score(self.x_test, self.y_test)\n        return val_accuracy\n\n    def Optimize_Adaboost_regressor(self, trial):\n        logging.info(\"Optimize_Adaboost_regressor\")\n        n_estimators = trial.suggest_int(\"n_estimators\", 1, 200)\n        learning_rate = trial.suggest_uniform(\"learning_rate\", 0.01, 0.99)\n        reg = AdaBoostRegressor(n_estimators=n_estimators, learning_rate=learning_rate)\n        reg.fit(self.x_train, self.y_train)\n        val_accuracy = reg.score(self.x_test, self.y_test)\n        return val_accuracy\n\n    def Optimize_LightGBM(self, trial):\n        logging.info(\"Optimize_LightGBM\")\n        n_estimators = trial.suggest_int(\"n_estimators\", 1, 200)\n        max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n        learning_rate = trial.suggest_uniform(\"learning_rate\", 0.01, 0.99)\n        reg = LGBMRegressor(\n            n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth\n        )\n        reg.fit(self.x_train, self.y_train)\n        val_accuracy = reg.score(self.x_test, self.y_test)\n        return val_accuracy\n\n    def Optimize_Xgboost_regressor(self, trial):\n        logging.info(\"Optimize_Xgboost_regressor\")\n        param = {\n            \"max_depth\": trial.suggest_int(\"max_depth\", 1, 30),\n            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-7, 10.0),\n            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1, 200),\n        }\n        reg = xgb.XGBRegressor(**param)\n        reg.fit(self.x_train, self.y_train)\n        val_accuracy = reg.score(self.x_test, self.y_test)\n        return val_accuracy\n\n    def Optimize_Catboost_regressor(self, trial):\n        logging.info(\"Optimize_Catboost_regressor\")\n        param = {\n            \"iterations\": trial.suggest_int(\"iterations\", 1, 200),\n            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-7, 1.0),\n            \"depth\": trial.suggest_int(\"depth\", 1, 16),\n            \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-7, 10.0),\n            \"border_count\": trial.suggest_int(\"border_count\", 1, 20),\n            \"rsm\": trial.suggest_uniform(\"rsm\", 0.5, 1.0),\n            \"od_type\": trial.suggest_categorical(\n                \"od_type\", (\"IncToDec\", \"Iter\", \"None\")\n            ),\n            \"od_wait\": trial.suggest_int(\"od_wait\", 1, 20),\n            \"random_seed\": trial.suggest_int(\"random_seed\", 1, 20),\n            \"loss_function\": trial.suggest_categorical(\n                \"loss_function\", (\"RMSE\", \"MAE\")\n            ),\n        }\n        reg = CatBoostRegressor(**param)\n        reg.fit(self.x_train, self.y_train)\n        val_accuracy = reg.score(self.x_test, self.y_test)\n        return val_accuracy\n\n\nclass ModelTraining:\n    def __init__(self, x_train, y_train, x_test, y_test) -> None:\n        self.x_train = x_train\n        self.y_train = y_train\n        self.x_test = x_test\n        self.y_test = y_test\n\n    def decision_trees(self, fine_tuning=True):\n        logging.info(\"Entered for training Decision Trees model\")\n        try:\n            if fine_tuning:\n                hyper_opt = Hyperparameters_Optimization(\n                    self.x_train, self.y_train, self.x_test, self.y_test\n                )\n                study = optuna.create_study(direction=\"maximize\")\n                study.optimize(hyper_opt.optimize_decisiontrees, n_trials=100)\n                trial = study.best_trial\n                # criterion = trial.params[\"criterion\"]\n                max_depth = trial.params[\"max_depth\"]\n                min_samples_split = trial.params[\"min_samples_split\"]\n                print(\"Best parameters : \", trial.params)\n                reg = DecisionTreeRegressor(\n                    max_depth=max_depth, min_samples_split=min_samples_split,\n                )\n                reg.fit(self.x_train, self.y_train)\n                return reg\n            else:\n                model = DecisionTreeRegressor(\n                    criterion=\"squared_error\", max_depth=7, min_samples_split=13\n                )\n\n                model.fit(self.x_train, self.y_train)\n                return model\n        except Exception as e:\n            logging.error(\"Error in training Decision Trees model\")\n            logging.error(e)\n            return None\n\n    def random_forest(self, fine_tuning=True):\n        logging.info(\"Entered for training Random Forest model\")\n        try:\n            if fine_tuning:\n                hyper_opt = Hyperparameters_Optimization(\n                    self.x_train, self.y_train, self.x_test, self.y_test\n                )\n                study = optuna.create_study(direction=\"maximize\")\n                study.optimize(hyper_opt.optimize_randomforest, n_trials=100)\n                trial = study.best_trial\n                n_estimators = trial.params[\"n_estimators\"]\n                max_depth = trial.params[\"max_depth\"]\n                min_samples_split = trial.params[\"min_samples_split\"]\n                print(\"Best parameters : \", trial.params)\n                reg = RandomForestRegressor(\n                    n_estimators=n_estimators,\n                    max_depth=max_depth,\n                    min_samples_split=min_samples_split,\n                )\n                reg.fit(self.x_train, self.y_train)\n                return reg\n            else:\n                model = RandomForestRegressor(\n                    n_estimators=152, max_depth=20, min_samples_split=17\n                )\n                model.fit(self.x_train, self.y_train)\n                return model\n        except Exception as e:\n            logging.error(\"Error in training Random Forest model\")\n            logging.error(e)\n            return None\n\n    def adabooost_regressor(self, fine_tuning=True):\n        logging.info(\"Entered for training Adaboost regressor model\")\n        try:\n            if fine_tuning:\n                hyper_opt = Hyperparameters_Optimization(\n                    self.x_train, self.y_train, self.x_test, self.y_test\n                )\n                study = optuna.create_study(direction=\"maximize\")\n                study.optimize(hyper_opt.Optimize_Adaboost_regressor, n_trials=100)\n                trial = study.best_trial\n                n_estimators = trial.params[\"n_estimators\"]\n                learning_rate = trial.params[\"learning_rate\"]\n                reg = AdaBoostRegressor(\n                    n_estimators=n_estimators, learning_rate=learning_rate\n                )\n                reg.fit(self.x_train, self.y_train)\n                return reg\n            else:\n                model = AdaBoostRegressor(n_estimators=200, learning_rate=0.01)\n                model.fit(self.x_train, self.y_train)\n                return model\n        except Exception as e:\n            logging.error(\"Error in training Adaboost regressor model\")\n            logging.error(e)\n            return None\n\n    def LightGBM(self, fine_tuning=True):\n        logging.info(\"Entered for training LightGBM model\")\n        try:\n            if fine_tuning:\n                hyper_opt = Hyperparameters_Optimization(\n                    self.x_train, self.y_train, self.x_test, self.y_test\n                )\n                study = optuna.create_study(direction=\"maximize\")\n                study.optimize(hyper_opt.Optimize_LightGBM, n_trials=100)\n                trial = study.best_trial\n                n_estimators = trial.params[\"n_estimators\"]\n                max_depth = trial.params[\"max_depth\"]\n                learning_rate = trial.params[\"learning_rate\"]\n                reg = LGBMRegressor(\n                    n_estimators=n_estimators,\n                    learning_rate=learning_rate,\n                    max_depth=max_depth,\n                )\n                reg.fit(self.x_train, self.y_train)\n                return reg\n            else:\n                model = LGBMRegressor(\n                    n_estimators=200, learning_rate=0.01, max_depth=20\n                )\n                model.fit(self.x_train, self.y_train)\n                return model\n        except Exception as e:\n            logging.error(\"Error in training LightGBM model\")\n            logging.error(e)\n            return None\n\n    def xgboost(self, fine_tuning=True):\n        logging.info(\"Entered for training XGBoost model\")\n        try:\n            if fine_tuning:\n                hy_opt = Hyperparameters_Optimization(\n                    self.x_train, self.y_train, self.x_test, self.y_test\n                )\n                study = optuna.create_study(direction=\"maximize\")\n                study.optimize(hy_opt.Optimize_Xgboost_regressor, n_trials=100)\n                trial = study.best_trial\n                n_estimators = trial.params[\"n_estimators\"]\n                learning_rate = trial.params[\"learning_rate\"]\n                max_depth = trial.params[\"max_depth\"]\n                reg = xgb.XGBRegressor(\n                    n_estimators=n_estimators,\n                    learning_rate=learning_rate,\n                    max_depth=max_depth,\n                )\n                reg.fit(self.x_train, self.y_train)\n                return reg\n\n            else:\n                model = xgb.XGBRegressor(\n                    n_estimators=200, learning_rate=0.01, max_depth=20\n                )\n                model.fit(self.x_train, self.y_train)\n                return model\n        except Exception as e:\n            logging.error(\"Error in training XGBoost model\")\n            logging.error(e)\n            return None\n\n    def Catboost(self, fine_tuning=True):\n        logging.info(\"Entered for training Catboost model\")\n        try:\n            if fine_tuning:\n                hy_opt = Hyperparameters_Optimization(\n                    self.x_train, self.y_train, self.x_test, self.y_test\n                )\n                study = optuna.create_study(direction=\"maximize\")\n                study.optimize(hy_opt.Optimize_Catboost_regressor, n_trials=100)\n                trial = study.best_trial\n                iterations = trial.params[\"iterations\"]\n                depth = trial.params[\"depth\"]\n                l2_leaf_reg = trial.params[\"l2_leaf_reg\"]\n                learning_rate = trial.params[\"learning_rate\"]\n                logging.info(\"Best parameters : \", trial.params)\n                reg = CatBoostRegressor(\n                    iterations=iterations,\n                    depth=depth,\n                    l2_leaf_reg=l2_leaf_reg,\n                    learning_rate=learning_rate,\n                )\n                reg.fit(self.x_train, self.y_train)\n                return reg\n            else:\n                model = CatBoostRegressor(\n                    iterations=200, depth=20, l2_leaf_reg=0.01, learning_rate=0.01\n                )\n                model.fit(self.x_train, self.y_train)\n                return model\n        except Exception as e:\n            logging.error(\"Error in training Catboost model\")\n            logging.error(e)\n            return None\n\n    def stacking_regression(self):\n        logging.info(\"Entered for stacking model\")\n        try:\n            rf_tree = RandomForestRegressor(\n                n_estimators=152, max_depth=20, min_samples_split=17\n            )\n            adaboost = AdaBoostRegressor(n_estimators=200, learning_rate=0.01)\n            xgb_reg = XGBRegressor(n_estimators=200, learning_rate=0.01, max_depth=20)\n            # cat_reg = CatBoostRegressor(\n            #     iterations=200, depth=20, l2_leaf_reg=0.01, learning_rate=1e-7\n            # )\n            # lr = LinearRegression() # use random forest here\n            reg = StackingRegressor(\n                regressors=[rf_tree, adaboost, xgb_reg],\n                meta_regressor=rf_tree,\n            )\n            reg.fit(self.x_train, self.y_train)\n            return reg\n        except Exception as e:\n            logging.error(\"Error in stacking model\")\n            logging.error(e)\n            return None\n\n\n# if __name__ == \"__main__\":\n#     data_utils = DataUtils()\n#     jigsaw_data, c3_data, ruddit_data = data_utils.load_different_data()\n#     Jigsaw, c3_data, ruddit_data = data_utils.prepare_data()\n#     data = data_utils.concatenate_dfs(Jigsaw, c3_data, ruddit_data)\n#     data = data.sample(100)\n#     data_process = DataProcess(data, jigsaw_data, c3_data, ruddit_data)\n#     data_process.check_null_values()\n#     data_processed = data_process.apply_all_processing_on_train_test_data()\n#     vectrorize = Vectorization(data)\n#     Final_Training_data = vectrorize.vectorize()\n#     data_val = DataValidation(Final_Training_data)\n#     x_train, x_test, y_train, y_test = data_val.data_splitting()\n#     print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n\n#     model_train = ModelTraining(x_train, y_train, x_test, y_test)\n#     # ran_for = model_train.random_forest(fine_tuning=True)\n#     # ada_for = model_train.adabooost_regressor(fine_tuning=True)\n#     # lgbm_for = model_train.LightGBM(fine_tuning=True)\n#     # xgb_for = model_train.xgboost(fine_tuning=True)\n#     # catboost_for = model_train.Catboost(fine_tuning=True)\n#     stack_for = model_train.stacking_regression()\n#     print(stack_for)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:21:08.609329Z","iopub.execute_input":"2022-01-25T11:21:08.60957Z","iopub.status.idle":"2022-01-25T11:21:08.693032Z","shell.execute_reply.started":"2022-01-25T11:21:08.609547Z","shell.execute_reply":"2022-01-25T11:21:08.692132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing\n\nfrom gensim.models import Word2Vec\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\nstop_words = nltk.corpus.stopwords.words('english')\n\ndef normalize_document(doc):\n    # lower case and remove special characters\\whitespaces\n    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n    doc = doc.lower()\n    doc = doc.strip()\n    # tokenize document\n    tokens = nltk.word_tokenize(doc)\n    #filter stopwords out of document\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    # re-create document from filtered tokens\n    doc = ' '.join(filtered_tokens)\n    return doc\n\ndef  train_gensim_model(train=True): \n    logging.info(\"Training the model\") \n    try: \n        data_utils = DataUtils()\n        jigsaw_data, c3_data, ruddit_data = data_utils.load_different_data()\n        Jigsaw, c3_data, ruddit_data = data_utils.prepare_data()  \n        data = data_utils.concatenate_dfs(Jigsaw, c3_data, ruddit_data) \n        \n        data_process = DataProcess(data)  \n        data_process.check_null_values()\n        data_processed = data_process.apply_all_processing_on_train_test_data()  \n        \n        corpus = data_processed['text'] \n        # convert corpus to array \n        corpus = corpus.values \n        corpus = [normalize_document(doc) for doc in corpus] \n        \n        w2v_model = Word2Vec(min_count=20,\n                             window=40,\n                             vector_size=300,\n                             sample=6e-5, \n                             alpha=0.03, \n                             min_alpha=0.0007, \n                             negative=20, \n                             workers=cores-1\n                             )\n\n        w2v_model.build_vocab(corpus, progress_per=10000) \n        w2v_model.train(corpus, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1) \n        return w2v_model \n    except Exception as e: \n            logging.error(f\"Error: {e}\")\n            raise e ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:19:09.669863Z","iopub.execute_input":"2022-01-25T11:19:09.670126Z","iopub.status.idle":"2022-01-25T11:19:09.965845Z","shell.execute_reply.started":"2022-01-25T11:19:09.6701Z","shell.execute_reply":"2022-01-25T11:19:09.964847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = train_gensim_model()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:19:24.718807Z","iopub.execute_input":"2022-01-25T11:19:24.719023Z","iopub.status.idle":"2022-01-25T11:19:37.426784Z","shell.execute_reply.started":"2022-01-25T11:19:24.719Z","shell.execute_reply":"2022-01-25T11:19:37.425587Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:19:37.427479Z","iopub.status.idle":"2022-01-25T11:19:37.427759Z","shell.execute_reply.started":"2022-01-25T11:19:37.427597Z","shell.execute_reply":"2022-01-25T11:19:37.427615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd  \nfrom gensim.models import FastText \n\nclass WordEmbeddings: \n    def __init__(self) -> None:\n        pass \n    \n    def averaged_word2vec_vectorizer(self, corpus, model, num_features):\n        vocabulary = set(model.wv.index_to_key)\n        \n        def average_word_vectors(words, model, vocabulary, num_features):\n            feature_vector = np.zeros((num_features,), dtype=\"float64\")\n            nwords = 0.\n            \n            for word in words:\n                if word in vocabulary: \n                    nwords = nwords + 1.\n                    feature_vector = np.add(feature_vector, model.wv[word])\n            if nwords:\n                feature_vector = np.divide(feature_vector, nwords)\n\n            return feature_vector\n\n        features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n                        for tokenized_sentence in corpus]\n        return np.array(features) \ndata_utils = DataUtils()\njigsaw_data, c3_data, ruddit_data = data_utils.load_different_data()\nJigsaw, c3_data, ruddit_data = data_utils.prepare_data()  \ndata = data_utils.concatenate_dfs(Jigsaw, c3_data, ruddit_data) \n        \ndata_process = DataProcess(data)  \ndata_process.check_null_values()\ndata_processed = data_process.apply_all_processing_on_train_test_data()  \n        \ncorpus = data_processed['text'] \n        # convert corpus to array \ncorpus = corpus.values \ncorpus = [normalize_document(doc) for doc in corpus] \n\nprediction = WordEmbeddings()\ndoc_vecs_ft = prediction.averaged_word2vec_vectorizer(corpus, word2vec, 300)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:19:44.848205Z","iopub.execute_input":"2022-01-25T11:19:44.848425Z","iopub.status.idle":"2022-01-25T11:21:08.600085Z","shell.execute_reply.started":"2022-01-25T11:19:44.848403Z","shell.execute_reply":"2022-01-25T11:21:08.598585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec \nword2vec = Word2Vec.load(\"../input/word2vectrained/word2vec.model\") ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:19:43.508648Z","iopub.execute_input":"2022-01-25T11:19:43.508932Z","iopub.status.idle":"2022-01-25T11:19:43.525286Z","shell.execute_reply.started":"2022-01-25T11:19:43.508906Z","shell.execute_reply":"2022-01-25T11:19:43.52462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.save(\"word2vec.model\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T07:25:25.760919Z","iopub.execute_input":"2022-01-25T07:25:25.761739Z","iopub.status.idle":"2022-01-25T07:25:25.766512Z","shell.execute_reply.started":"2022-01-25T07:25:25.761698Z","shell.execute_reply":"2022-01-25T07:25:25.765873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(doc_vecs_ft)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:21:08.602565Z","iopub.execute_input":"2022-01-25T11:21:08.603261Z","iopub.status.idle":"2022-01-25T11:21:08.608149Z","shell.execute_reply.started":"2022-01-25T11:21:08.603229Z","shell.execute_reply":"2022-01-25T11:21:08.606918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport logging\n\nfrom pandas import core \nimport joblib  \nimport nltk\nimport re\nimport numpy as np\n\nstop_words = nltk.corpus.stopwords.words('english')\n\n    #------------------------ data ingestion process--------------------\ndata_utils = DataUtils()\njigsaw_data, c3_data, ruddit_data = data_utils.load_different_data()\nJigsaw, c3_data, ruddit_data = data_utils.prepare_data()\ndata = data_utils.concatenate_dfs(Jigsaw, c3_data, ruddit_data)\n    \n    #------------------------- data preprocessing------------------------\ndf[\"y\"] = data[\"y\"]    \n    #------------------------- Word Embeddings ----------------------------\n    \ndata_val = DataValidation(df)\nx_train, x_test, y_train, y_test = data_val.data_splitting()\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n\n    #-------------------------- model training------------------------------\nmodel_train = ModelTraining(x_train, y_train, x_test, y_test)\nlightgbm = model_train.stacking_regression()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:21:08.694591Z","iopub.execute_input":"2022-01-25T11:21:08.694907Z","iopub.status.idle":"2022-01-25T12:37:03.790678Z","shell.execute_reply.started":"2022-01-25T11:21:08.69488Z","shell.execute_reply":"2022-01-25T12:37:03.789579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:59:08.081382Z","iopub.execute_input":"2022-01-25T09:59:08.081662Z","iopub.status.idle":"2022-01-25T09:59:08.090927Z","shell.execute_reply.started":"2022-01-25T09:59:08.081633Z","shell.execute_reply":"2022-01-25T09:59:08.090005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comms_to_score = pd.read_csv(r\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\") \n\n\ndata_process = DataProcess(comms_to_score)  \ndata_process.check_null_values()\ndata_processed = data_process.apply_all_processing_on_train_test_data()  \n        \ncorpus = data_processed['text'] \n        # convert corpus to array \ncorpus = corpus.values \ncorpus = [normalize_document(doc) for doc in corpus] \n\nprediction = WordEmbeddings()\ndoc_vecs_ft_submit = prediction.averaged_word2vec_vectorizer(corpus, word2vec, 300)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T12:42:07.555657Z","iopub.execute_input":"2022-01-25T12:42:07.555956Z","iopub.status.idle":"2022-01-25T12:42:20.110346Z","shell.execute_reply.started":"2022-01-25T12:42:07.555928Z","shell.execute_reply":"2022-01-25T12:42:20.109517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs_submit = pd.DataFrame(doc_vecs_ft_submit) \npreds = lightgbm.predict(dfs_submit)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T12:42:20.112372Z","iopub.execute_input":"2022-01-25T12:42:20.11284Z","iopub.status.idle":"2022-01-25T12:42:21.755157Z","shell.execute_reply.started":"2022-01-25T12:42:20.1128Z","shell.execute_reply":"2022-01-25T12:42:21.754343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comms_to_score[\"score\"] = preds\ncomms_to_score.drop(\"text\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T12:42:23.319991Z","iopub.execute_input":"2022-01-25T12:42:23.320235Z","iopub.status.idle":"2022-01-25T12:42:23.326697Z","shell.execute_reply.started":"2022-01-25T12:42:23.32021Z","shell.execute_reply":"2022-01-25T12:42:23.326206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comms_to_score","metadata":{"execution":{"iopub.status.busy":"2022-01-25T12:42:35.260534Z","iopub.execute_input":"2022-01-25T12:42:35.261228Z","iopub.status.idle":"2022-01-25T12:42:35.279288Z","shell.execute_reply.started":"2022-01-25T12:42:35.261192Z","shell.execute_reply":"2022-01-25T12:42:35.278293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comms_to_score.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T12:42:39.874241Z","iopub.execute_input":"2022-01-25T12:42:39.875383Z","iopub.status.idle":"2022-01-25T12:42:39.900692Z","shell.execute_reply.started":"2022-01-25T12:42:39.875344Z","shell.execute_reply":"2022-01-25T12:42:39.899918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}