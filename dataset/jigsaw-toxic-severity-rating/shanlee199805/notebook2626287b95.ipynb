{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:35:28.128155Z","iopub.execute_input":"2022-01-13T08:35:28.129329Z","iopub.status.idle":"2022-01-13T08:35:28.186569Z","shell.execute_reply.started":"2022-01-13T08:35:28.129192Z","shell.execute_reply":"2022-01-13T08:35:28.185515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \nfrom IPython.display import display, HTML\nfrom pprint import pprint\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\npd.options.display.max_colwidth=1000","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:35:28.188996Z","iopub.execute_input":"2022-01-13T08:35:28.189956Z","iopub.status.idle":"2022-01-13T08:35:29.410755Z","shell.execute_reply.started":"2022-01-13T08:35:28.189897Z","shell.execute_reply":"2022-01-13T08:35:29.409837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training data\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/train-csv/train.csv\")\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:35:29.412237Z","iopub.execute_input":"2022-01-13T08:35:29.41274Z","iopub.status.idle":"2022-01-13T08:35:31.257694Z","shell.execute_reply.started":"2022-01-13T08:35:29.412694Z","shell.execute_reply":"2022-01-13T08:35:31.256465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*  Adjust diffent column weights\n*  Normalize y to range [0,1]","metadata":{}},{"cell_type":"code","source":"# Adjust weights later\ndf[\"severe_toxic\"] = df.severe_toxic * 2\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf['y'] = df['y']/df['y'].max() \ndf = df[['comment_text', 'y']].rename(columns={'comment_text':'text'})","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:35:31.259292Z","iopub.execute_input":"2022-01-13T08:35:31.259574Z","iopub.status.idle":"2022-01-13T08:35:31.300495Z","shell.execute_reply.started":"2022-01-13T08:35:31.259533Z","shell.execute_reply":"2022-01-13T08:35:31.29936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.head(5))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:35:31.30455Z","iopub.execute_input":"2022-01-13T08:35:31.304945Z","iopub.status.idle":"2022-01-13T08:35:31.318135Z","shell.execute_reply.started":"2022-01-13T08:35:31.3049Z","shell.execute_reply":"2022-01-13T08:35:31.316456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['y'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:35:31.320437Z","iopub.execute_input":"2022-01-13T08:35:31.320763Z","iopub.status.idle":"2022-01-13T08:35:31.336987Z","shell.execute_reply.started":"2022-01-13T08:35:31.320721Z","shell.execute_reply":"2022-01-13T08:35:31.335444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 7\n\nfrac_1 = 0.3\nfrac_1_factor = 1.2\n\n# 將dataset分為7個folds, 每個folds 包含: \"y!=0\" 的數量為 dataset 裡面的 30%（抽樣0.3） , \"y＝0\" 的數量 為y＝0抽樣數量的1.2倍 \n# 因為分為7個fold抽樣為0.3 所以每個folde可能會有抽樣到重複的情況\n# DataFrame.sample() , random_state代表隨機的狀態（當random_state設定同一個數值時，同一群資料被隨機抽出來的結果會一樣）\n\nfor fld in range(n_folds):\n    tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)), \n                           df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor),\n                                              random_state = 10*(fld+1))], axis=0)\n    \n    tmp_df.to_csv(f'/kaggle/working/df_{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:35:31.339401Z","iopub.execute_input":"2022-01-13T08:35:31.339785Z","iopub.status.idle":"2022-01-13T08:35:32.648636Z","shell.execute_reply.started":"2022-01-13T08:35:31.339742Z","shell.execute_reply":"2022-01-13T08:35:32.646688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean data\n* 利用 nltk 將 stopwords 去除\n* 分開合併詞(exp: what's -> what is , can't -> cannot)\n* 去除換行字元\n* 去除重複的字元，若超過3個重複的就以3個計 (exp: woooooooow -> wooow, !!!!!!!! -> !!!)\n* 將重複字元前後放置空白(white space)\n* 若一個詞有多個重複字元則第二個重複字元以兩個計 (exp: yahhhhhoooo -> yahhhoo)\n\n---\n\n備註: regular expression\n* '\\1'the first capturing group in the matched expression   ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in text]\n\ndef clean(data, col):\n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ') \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:35:32.650261Z","iopub.execute_input":"2022-01-13T08:35:32.650684Z","iopub.status.idle":"2022-01-13T08:35:33.138568Z","shell.execute_reply.started":"2022-01-13T08:35:32.650631Z","shell.execute_reply":"2022-01-13T08:35:33.137602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test clean function\ntest_clean_df = pd.DataFrame({\"text\":\n                              [\"heyy\\n\\nkkdsfj\",\n                               \"hi   how/are/you ???\",\n                               \"hey?????\",\n                               \"noooo!!!!!!!!!   comeone !! \",\n                              \"cooooooooool     brooooooooooo  coool brooo\",\n                              \"naaaahhhhhhh\"]})\ndisplay(test_clean_df)\nclean(test_clean_df,'text')","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:35:33.140376Z","iopub.execute_input":"2022-01-13T08:35:33.140666Z","iopub.status.idle":"2022-01-13T08:35:33.17889Z","shell.execute_reply.started":"2022-01-13T08:35:33.140623Z","shell.execute_reply":"2022-01-13T08:35:33.177854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = clean(df,'text')","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:35:33.180352Z","iopub.execute_input":"2022-01-13T08:35:33.180688Z","iopub.status.idle":"2022-01-13T08:36:39.46044Z","shell.execute_reply.started":"2022-01-13T08:35:33.180643Z","shell.execute_reply":"2022-01-13T08:36:39.459467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.y==0]","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:36:39.462218Z","iopub.execute_input":"2022-01-13T08:36:39.462877Z","iopub.status.idle":"2022-01-13T08:36:39.510347Z","shell.execute_reply.started":"2022-01-13T08:36:39.46283Z","shell.execute_reply":"2022-01-13T08:36:39.509209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ruddit data","metadata":{}},{"cell_type":"code","source":"df_ = pd.read_csv(\"../input/ruddit/ruddit_with_text.csv\")\nprint(df_.shape)\ndf_ = df_[['txt', 'offensiveness_score']].rename(columns={'txt': 'text', 'offensiveness_score':'y'})\ndf_['y'] = (df_['y'] - df_.y.min()) / (df_.y.max() - df_.y.min())\ndf_.y.hist()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:36:39.512076Z","iopub.execute_input":"2022-01-13T08:36:39.512473Z","iopub.status.idle":"2022-01-13T08:36:39.889152Z","shell.execute_reply.started":"2022-01-13T08:36:39.512426Z","shell.execute_reply":"2022-01-13T08:36:39.888228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 7\n\nfrac_1 = 0.7\n\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = df_.sample(frac=frac_1, random_state = 10*(fld+1))\n    tmp_df.to_csv(f'/kaggle/working/df2_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:36:39.892107Z","iopub.execute_input":"2022-01-13T08:36:39.892649Z","iopub.status.idle":"2022-01-13T08:36:40.240214Z","shell.execute_reply.started":"2022-01-13T08:36:39.892602Z","shell.execute_reply":"2022-01-13T08:36:40.239301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tmp_df, df_; \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:36:40.245261Z","iopub.execute_input":"2022-01-13T08:36:40.245496Z","iopub.status.idle":"2022-01-13T08:36:40.380977Z","shell.execute_reply.started":"2022-01-13T08:36:40.245466Z","shell.execute_reply":"2022-01-13T08:36:40.379754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the csv file of clean data","metadata":{}},{"cell_type":"code","source":"n_folds = 7\n\nfrac_1 = 0.3\nfrac_1_factor = 1.2\n\nfor fld in range(n_folds):\n    tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n\n    tmp_df.to_csv(f'/kaggle/working/df_clean_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:36:40.38332Z","iopub.execute_input":"2022-01-13T08:36:40.383706Z","iopub.status.idle":"2022-01-13T08:36:41.503131Z","shell.execute_reply.started":"2022-01-13T08:36:40.383652Z","shell.execute_reply":"2022-01-13T08:36:41.502079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df,tmp_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:36:41.504758Z","iopub.execute_input":"2022-01-13T08:36:41.505335Z","iopub.status.idle":"2022-01-13T08:36:41.639145Z","shell.execute_reply.started":"2022-01-13T08:36:41.505294Z","shell.execute_reply":"2022-01-13T08:36:41.637863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Validation and Test data\n","metadata":{}},{"cell_type":"code","source":"# Validation data \n\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n# Test data\n\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:36:41.641209Z","iopub.execute_input":"2022-01-13T08:36:41.641738Z","iopub.status.idle":"2022-01-13T08:36:42.192251Z","shell.execute_reply.started":"2022-01-13T08:36:41.641691Z","shell.execute_reply":"2022-01-13T08:36:42.191293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Sklean pipline\n* TFIDF - Take 'char_wb' as analyzer to capture subwords well\n* Ridge - Ridge is a simple regression algorithm that will reduce overfitting","metadata":{}},{"cell_type":"code","source":"class LengthUpperTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return sparse.csr_matrix([[sum([1 for y in x if y.isupper()])/len(x)] for x in X])\n    def get_feature_names(self):\n        return [\"lngth_uppercase\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:36:42.193647Z","iopub.execute_input":"2022-01-13T08:36:42.193967Z","iopub.status.idle":"2022-01-13T08:36:42.201229Z","shell.execute_reply.started":"2022-01-13T08:36:42.193926Z","shell.execute_reply":"2022-01-13T08:36:42.199729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val['upper_1'] = np.array(LengthUpperTransformer().transform(df_val['less_toxic']).todense()).reshape(-1,1)\ndf_val['upper_2'] = np.array(LengthUpperTransformer().transform(df_val['more_toxic']).todense()).reshape(-1,1)\n\nprint(df_val['upper_1'].mean(), df_val['upper_1'].std())\nprint(df_val['upper_2'].mean(), df_val['upper_2'].std())\n\ndf_val['upper_1'].hist(bins=100)\ndf_val['upper_2'].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:36:42.202976Z","iopub.execute_input":"2022-01-13T08:36:42.203608Z","iopub.status.idle":"2022-01-13T08:36:45.035684Z","shell.execute_reply.started":"2022-01-13T08:36:42.203563Z","shell.execute_reply":"2022-01-13T08:36:45.03484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train pipline\n* load folds data\n* train pipeline\n* predict on validation data\n* predict on test data","metadata":{}},{"cell_type":"markdown","source":"## Train on Original data","metadata":{}},{"cell_type":"code","source":"val_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arr = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    print(\"\\n\\n\")\n    print(f' ****************************** FOLD: {fld} ******************************')\n    df = pd.read_csv(f'/kaggle/working/df_{fld}.csv')\n    print(df.shape)\n\n    features = FeatureUnion([\n        #('vect1', LengthTransformer()),\n        #('vect2', LengthUpperTransformer()),\n        (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n    ])\n    pipeline = Pipeline(\n        [\n            (\"features\", features),\n            #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n            (\"clf\", Ridge()),\n            #(\"clf\",LinearRegression())\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline.fit(df['text'], df['y'])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    pprint(feature_wts[:30])\n    \n    print(\"\\npredict validation data \")\n    val_preds_arr1[:,fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n    print(\"\\npredict test data \")\n    test_preds_arr[:,fld] = pipeline.predict(df_sub['text'])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:36:45.040509Z","iopub.execute_input":"2022-01-13T08:36:45.042895Z","iopub.status.idle":"2022-01-13T08:45:04.882713Z","shell.execute_reply.started":"2022-01-13T08:36:45.042852Z","shell.execute_reply":"2022-01-13T08:45:04.881763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train on Toxic clean data","metadata":{}},{"cell_type":"code","source":"val_preds_arr1c = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2c = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arrc = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    print(\"\\n\\n\")\n    print(f' ****************************** FOLD: {fld} ******************************')\n    df = pd.read_csv(f'/kaggle/working/df_clean_fld{fld}.csv')\n    print(df.shape)\n\n    features = FeatureUnion([\n        #('vect1', LengthTransformer()),\n        #('vect2', LengthUpperTransformer()),\n        (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n    ])\n    pipeline = Pipeline(\n        [\n            (\"features\", features),\n            #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n            (\"clf\", Ridge()),\n            #(\"clf\",LinearRegression())\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline.fit(df['text'], df['y'])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    pprint(feature_wts[:30])\n    \n    print(\"\\npredict validation data \")\n    val_preds_arr1c[:,fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2c[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n    print(\"\\npredict test data \")\n    test_preds_arrc[:,fld] = pipeline.predict(df_sub['text'])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:45:04.884187Z","iopub.execute_input":"2022-01-13T08:45:04.884503Z","iopub.status.idle":"2022-01-13T08:53:09.4262Z","shell.execute_reply.started":"2022-01-13T08:45:04.884426Z","shell.execute_reply":"2022-01-13T08:53:09.425211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train on ruddit data","metadata":{}},{"cell_type":"code","source":"val_preds_arr1_ = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2_ = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arr_ = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    print(\"\\n\\n\")\n    print(f' ****************************** FOLD: {fld} ******************************')\n    df = pd.read_csv(f'/kaggle/working/df2_fld{fld}.csv')\n    print(df.shape)\n\n    features = FeatureUnion([\n        #('vect1', LengthTransformer()),\n        #('vect2', LengthUpperTransformer()),\n        (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n    ])\n    pipeline = Pipeline(\n        [\n            (\"features\", features),\n            #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n            (\"clf\", Ridge()),\n            #(\"clf\",LinearRegression())\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline.fit(df['text'], df['y'])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    pprint(feature_wts[:30])\n    \n    print(\"\\npredict validation data \")\n    val_preds_arr1_[:,fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2_[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n    print(\"\\npredict test data \")\n    test_preds_arr_[:,fld] = pipeline.predict(df_sub['text'])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T08:53:09.427931Z","iopub.execute_input":"2022-01-13T08:53:09.428218Z","iopub.status.idle":"2022-01-13T09:00:19.117742Z","shell.execute_reply.started":"2022-01-13T08:53:09.428175Z","shell.execute_reply":"2022-01-13T09:00:19.11682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validate pipeline","metadata":{}},{"cell_type":"code","source":"print(\" Toxic data \")\np1 = val_preds_arr1.mean(axis=1)\np2 = val_preds_arr2.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')\n\nprint(\" Ruddit data \")\np3 = val_preds_arr1_.mean(axis=1)\np4 = val_preds_arr2_.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p3 < p4).mean() * 100,2)}')\n\nprint(\" Toxic CLEAN data \")\np5 = val_preds_arr1c.mean(axis=1)\np6 = val_preds_arr2c.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p5 < p6).mean() * 100,2)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:00:19.119202Z","iopub.execute_input":"2022-01-13T09:00:19.119496Z","iopub.status.idle":"2022-01-13T09:00:19.138806Z","shell.execute_reply.started":"2022-01-13T09:00:19.119441Z","shell.execute_reply":"2022-01-13T09:00:19.137775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Find right weight\")\n\nwts_acc = []\nfor i in range(30,70,1):\n    for j in range(0,20,1):\n        w1 = i/100\n        w2 = (100 - i - j)/100\n        w3 = (1 - w1 - w2 )\n        p1_wt = w1*p1 + w2*p3 + w3*p5\n        p2_wt = w1*p2 + w2*p4 + w3*p6\n        wts_acc.append( (w1,w2,w3, \n                         np.round((p1_wt < p2_wt).mean() * 100,2))\n                      )\nsorted(wts_acc, key=lambda x:x[3], reverse=True)[:5]","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:00:19.141548Z","iopub.execute_input":"2022-01-13T09:00:19.141831Z","iopub.status.idle":"2022-01-13T09:00:19.453698Z","shell.execute_reply.started":"2022-01-13T09:00:19.141768Z","shell.execute_reply":"2022-01-13T09:00:19.45269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1,w2,w3,_ = sorted(wts_acc, key=lambda x:x[2], reverse=True)[0]\n#print(best_wts)\n\np1_wt = w1*p1 + w2*p3 + w3*p5\np2_wt = w1*p2 + w2*p4 + w3*p6","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:00:19.455166Z","iopub.execute_input":"2022-01-13T09:00:19.4557Z","iopub.status.idle":"2022-01-13T09:00:19.462792Z","shell.execute_reply.started":"2022-01-13T09:00:19.455652Z","shell.execute_reply":"2022-01-13T09:00:19.461818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyze bad predictions\n* Incorrect predictions with similar scores\n* Incorrect predictions with different scores\n\n---\n\n* p1 less toxic, p2 more toxic","metadata":{}},{"cell_type":"code","source":"df_val['p1'] = p1_wt\ndf_val['p2'] = p2_wt\ndf_val['diff'] = np.abs(p2_wt - p1_wt)\n\ndf_val['correct'] = (p1_wt < p2_wt).astype('int')\n# correct-> df_val['correct']=1 , incorrect->df_val['incorrect']=0","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:00:19.464393Z","iopub.execute_input":"2022-01-13T09:00:19.4661Z","iopub.status.idle":"2022-01-13T09:00:19.478462Z","shell.execute_reply.started":"2022-01-13T09:00:19.466054Z","shell.execute_reply":"2022-01-13T09:00:19.47739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Incorrect predictions with similar scores\ndf_val[df_val.correct == 0].sort_values('diff', ascending=True).head(20)\n\n### Incorrect predictions with dis-similar scores\ndf_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:00:19.480353Z","iopub.execute_input":"2022-01-13T09:00:19.480704Z","iopub.status.idle":"2022-01-13T09:00:19.517385Z","shell.execute_reply.started":"2022-01-13T09:00:19.48066Z","shell.execute_reply":"2022-01-13T09:00:19.516467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict using pipeline\n# test_preds_arr: original data\n# test_preds_arr_: ruddit data\n# test_preds_arrc: clean data\n\ndf_sub['score'] = w1*test_preds_arr.mean(axis=1) + w2*test_preds_arr_.mean(axis=1) + w3*test_preds_arrc.mean(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:00:19.518795Z","iopub.execute_input":"2022-01-13T09:00:19.519701Z","iopub.status.idle":"2022-01-13T09:00:19.528298Z","shell.execute_reply.started":"2022-01-13T09:00:19.519658Z","shell.execute_reply":"2022-01-13T09:00:19.527041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correct ranking ordering ","metadata":{}},{"cell_type":"code","source":"# Cases with duplicates scores\n\ndf_sub['score'].count() - df_sub['score'].nunique()\nsame_score = df_sub['score'].value_counts().reset_index()[:10]\ndf_sub[df_sub['score'].isin(same_score['index'].tolist())]","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:00:19.530523Z","iopub.execute_input":"2022-01-13T09:00:19.530902Z","iopub.status.idle":"2022-01-13T09:00:19.555753Z","shell.execute_reply.started":"2022-01-13T09:00:19.53086Z","shell.execute_reply":"2022-01-13T09:00:19.554781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bert Ensemble","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel\n\n# Utils\nfrom tqdm import tqdm\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nCONFIG = dict(\n    seed = 42,\n    model_name = '../input/roberta-base', # https://huggingface.co/roberta-base\n    test_batch_size = 128,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\nMODEL_PATHS = [\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-0.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-1.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-2.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-3.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-4.bin'\n]\n\ndef set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    \nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']        \n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }    \n\n    \nclass JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs\n    \n@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS\n\n\ndef inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JigsawModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds\n\n\nset_seed(CONFIG['seed'])\ndf = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf.head()\n\ntest_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\npreds1 = inference(MODEL_PATHS, test_loader, CONFIG['device'])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:12:24.95228Z","iopub.execute_input":"2022-01-13T09:12:24.952652Z","iopub.status.idle":"2022-01-13T09:15:24.789354Z","shell.execute_reply.started":"2022-01-13T09:12:24.95262Z","shell.execute_reply":"2022-01-13T09:15:24.787822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = (preds1-preds1.min())/(preds1.max()-preds1.min())\nprint(preds)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:16:14.526156Z","iopub.execute_input":"2022-01-13T09:16:14.52643Z","iopub.status.idle":"2022-01-13T09:16:14.533928Z","shell.execute_reply.started":"2022-01-13T09:16:14.5264Z","shell.execute_reply":"2022-01-13T09:16:14.532775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score'] = df_sub['score']*0.85+preds*0.15\nprint(df_sub['score'] )","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:16:27.087419Z","iopub.execute_input":"2022-01-13T09:16:27.088071Z","iopub.status.idle":"2022-01-13T09:16:27.098082Z","shell.execute_reply.started":"2022-01-13T09:16:27.088032Z","shell.execute_reply":"2022-01-13T09:16:27.097021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:17:17.238822Z","iopub.execute_input":"2022-01-13T09:17:17.239121Z","iopub.status.idle":"2022-01-13T09:17:17.277192Z","shell.execute_reply.started":"2022-01-13T09:17:17.239087Z","shell.execute_reply":"2022-01-13T09:17:17.276268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}