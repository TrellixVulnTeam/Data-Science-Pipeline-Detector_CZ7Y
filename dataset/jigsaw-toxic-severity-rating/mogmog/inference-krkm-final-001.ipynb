{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pickle\n\n# General\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport os\nimport random\nimport gc\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nimport scipy\n\nimport torch\nfrom tqdm.auto import tqdm        ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-20T07:18:47.187785Z","iopub.execute_input":"2022-02-20T07:18:47.188206Z","iopub.status.idle":"2022-02-20T07:18:49.834242Z","shell.execute_reply.started":"2022-02-20T07:18:47.188089Z","shell.execute_reply":"2022-02-20T07:18:49.833163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:18:49.836354Z","iopub.execute_input":"2022-02-20T07:18:49.836736Z","iopub.status.idle":"2022-02-20T07:18:49.84963Z","shell.execute_reply.started":"2022-02-20T07:18:49.83669Z","shell.execute_reply":"2022-02-20T07:18:49.848503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# calc_weight","metadata":{}},{"cell_type":"code","source":"# kysky\nkfsky_df = pd.read_csv(\"../input/fork-of-notebook7d0b44182b/valid_kfsky.csv\")\n\n# kma###\ndrop_cols = [\"cleaning_less\",\"cleaning_more\"]\n# kma_df = pd.read_csv(\"../input/kazuma-model5/kazuma_val.csv\")\nkma_df = pd.read_csv(\"../input/1st-ruddit/kazuma_val.csv\")\nkma_df = kma_df.drop(drop_cols,axis = 1)\nkma_df2= pd.read_csv(\"../input/external-data-ridge/valid_external.csv\").loc[:,[\"preds_less_ext_kazuma\",\"preds_more_ext_kazuma\"]]\nkma_df = pd.concat([kma_df,kma_df2],axis = 1)\n\n# mogmog\nmogmog_df = pd.read_csv(\"../input/mogmog-train-12/validation_df_add_pred.csv\")\n\n\nprint(kfsky_df.shape)\nprint(kma_df.shape)\nprint(mogmog_df.shape)\n\n# concat\nconcat_df = pd.concat([kfsky_df.iloc[:,1:],\n                       #hori_df.iloc[:,3:],\n                       kma_df.iloc[:,3:],\n                       mogmog_df.iloc[:,3:]\n                      ],axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:18:49.85183Z","iopub.execute_input":"2022-02-20T07:18:49.852615Z","iopub.status.idle":"2022-02-20T07:18:52.888227Z","shell.execute_reply.started":"2022-02-20T07:18:49.852533Z","shell.execute_reply":"2022-02-20T07:18:52.887262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_cs = concat_df.columns[concat_df.columns.str.contains('less')]\nmore_cs = concat_df.columns[concat_df.columns.str.contains('more')]\n\ncolumns_list = ['text'] + [f\"model_{i}\" for i in range(len(less_cs)-1)]\nless_df = concat_df[less_cs]\nmore_df = concat_df[more_cs]\nless_df.columns = columns_list\nmore_df.columns = columns_list\nval_unique_df = pd.concat([less_df, more_df]).drop_duplicates('text').reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:18:52.892255Z","iopub.execute_input":"2022-02-20T07:18:52.893715Z","iopub.status.idle":"2022-02-20T07:18:52.972237Z","shell.execute_reply.started":"2022-02-20T07:18:52.893665Z","shell.execute_reply":"2022-02-20T07:18:52.971216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\ntmp = scaler.fit_transform(val_unique_df.iloc[:,1:])\nval_unique_df.iloc[:,1:] = tmp","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:18:52.973747Z","iopub.execute_input":"2022-02-20T07:18:52.974674Z","iopub.status.idle":"2022-02-20T07:18:53.02485Z","shell.execute_reply.started":"2022-02-20T07:18:52.974627Z","shell.execute_reply":"2022-02-20T07:18:53.023877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scoring(pred):\n    _val_unique_df = val_unique_df.copy()\n    _val_unique_df[\"pred\"] = pred\n    val_unique_df_selected = _val_unique_df[[\"text\",\"pred\"]].set_index(\"text\")[\"pred\"].to_dict()\n    \n    concat_df[\"less_toxic_pred\"] = concat_df[\"less_toxic\"].map(val_unique_df_selected)\n    concat_df[\"more_toxic_pred\"] = concat_df[\"more_toxic\"].map(val_unique_df_selected)\n    concat_df[\"correct\"] = (concat_df[\"less_toxic_pred\"]  < concat_df[\"more_toxic_pred\"]).astype(int)\n    \n    return concat_df[\"correct\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:18:53.026628Z","iopub.execute_input":"2022-02-20T07:18:53.026946Z","iopub.status.idle":"2022-02-20T07:18:53.035081Z","shell.execute_reply.started":"2022-02-20T07:18:53.026903Z","shell.execute_reply":"2022-02-20T07:18:53.033398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_models = val_unique_df.shape[1]-1\nprint(num_models)\ndef ensemble_score(weight_array): \n    avg_pred = np.sum([val_unique_df.loc[:,f\"model_{i}\"].values*weight_array[i] for i in range(num_models)],axis = 0)\n    return -scoring(avg_pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:18:53.03635Z","iopub.execute_input":"2022-02-20T07:18:53.037681Z","iopub.status.idle":"2022-02-20T07:18:53.046892Z","shell.execute_reply.started":"2022-02-20T07:18:53.037502Z","shell.execute_reply":"2022-02-20T07:18:53.045761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1,num_models+1):\n    l = concat_df.iloc[:,2*i]\n    m = concat_df.iloc[:,2*i+1]\n    print(np.mean((l< m).astype(int)))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:18:53.048578Z","iopub.execute_input":"2022-02-20T07:18:53.049655Z","iopub.status.idle":"2022-02-20T07:18:53.072224Z","shell.execute_reply.started":"2022-02-20T07:18:53.049609Z","shell.execute_reply":"2022-02-20T07:18:53.071199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_array = np.array([1.0/num_models]*num_models)\nweight_array[-1] = weight_array[-1]/50\n\nres = scipy.optimize.minimize(ensemble_score, weight_array, method='powell') \nprint(res.x)\nprint(ensemble_score(res.x))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:18:53.074488Z","iopub.execute_input":"2022-02-20T07:18:53.075046Z","iopub.status.idle":"2022-02-20T07:19:42.457192Z","shell.execute_reply.started":"2022-02-20T07:18:53.074997Z","shell.execute_reply":"2022-02-20T07:19:42.456023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del (kfsky_df, mogmog_df, kma_df, kma_df2, concat_df,less_df,more_df, val_unique_df)\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:19:42.462084Z","iopub.execute_input":"2022-02-20T07:19:42.46253Z","iopub.status.idle":"2022-02-20T07:19:42.718041Z","shell.execute_reply.started":"2022-02-20T07:19:42.462482Z","shell.execute_reply":"2022-02-20T07:19:42.716388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# kazuma section","metadata":{}},{"cell_type":"code","source":"comments_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:19:42.719736Z","iopub.execute_input":"2022-02-20T07:19:42.720657Z","iopub.status.idle":"2022-02-20T07:19:42.812982Z","shell.execute_reply.started":"2022-02-20T07:19:42.720509Z","shell.execute_reply":"2022-02-20T07:19:42.812016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_pkl(file_name, processor):\n    OUTPUT_DIR = './'\n    file_name = os.path.join(OUTPUT_DIR,file_name)\n    pickle.dump(processor,open(file_name, 'wb'))\n    print(\"FINISH\")\ndef load_pkl(file_path):\n    out_object = pickle.load(open(file_path, 'rb'))   \n    return out_object\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n#     text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\ndef ensemble(data, preds, cv_num):\n    pred = np.zeros((data.shape[0],))\n\n    for v in preds:\n      pred += v\n\n    return pred/cv_num\n\ndef multi_predict(test_x, models):\n    preds = []\n    for model in models:\n        pred = model.predict(test_x)\n        preds.append(pred)\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:19:42.815557Z","iopub.execute_input":"2022-02-20T07:19:42.816056Z","iopub.status.idle":"2022-02-20T07:19:42.829588Z","shell.execute_reply.started":"2022-02-20T07:19:42.81601Z","shell.execute_reply":"2022-02-20T07:19:42.828678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dont need cleaning\next_models = load_pkl('../input/external-data-ridge/ext_models.pkl')\next_tfv = load_pkl('../input/external-data-ridge/ext_tfv.pkl')\n\n# Need cleaning\ntc_models = load_pkl('../input/1st-ruddit/kazuma_models_toxic.pkl')\ntc_processor = load_pkl('../input/1st-ruddit/kazuma_processor.pkl')\nrud_models = load_pkl('../input/1st-ruddit/kazuma_models_ruddit.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:19:42.832704Z","iopub.execute_input":"2022-02-20T07:19:42.833925Z","iopub.status.idle":"2022-02-20T07:19:44.766123Z","shell.execute_reply.started":"2022-02-20T07:19:42.833874Z","shell.execute_reply":"2022-02-20T07:19:44.765095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\ncomments_df['cleaning'] = comments_df['text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:19:44.768606Z","iopub.execute_input":"2022-02-20T07:19:44.769267Z","iopub.status.idle":"2022-02-20T07:19:48.467574Z","shell.execute_reply.started":"2022-02-20T07:19:44.769192Z","shell.execute_reply":"2022-02-20T07:19:48.466592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfv_tc = tc_processor['toxc_comment_preprocessor']\ntfv_rud = tc_processor['ruddit_preprocessor']","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:19:48.469025Z","iopub.execute_input":"2022-02-20T07:19:48.469304Z","iopub.status.idle":"2022-02-20T07:19:48.474645Z","shell.execute_reply.started":"2022-02-20T07:19:48.469262Z","shell.execute_reply":"2022-02-20T07:19:48.473698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfv_col_comments = tfv_tc.transform(comments_df[\"cleaning\"])\ntfv_rud_col_comments = tfv_rud.transform(comments_df[\"cleaning\"])\ntfv_ext_col_comments = ext_tfv.transform(comments_df[\"text\"]) # dont need cleaning","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:19:48.476338Z","iopub.execute_input":"2022-02-20T07:19:48.476975Z","iopub.status.idle":"2022-02-20T07:20:08.798299Z","shell.execute_reply.started":"2022-02-20T07:19:48.476931Z","shell.execute_reply":"2022-02-20T07:20:08.796221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = multi_predict(tfv_col_comments, tc_models)\npreds_rud = multi_predict(tfv_rud_col_comments, rud_models)\npreds_ext = multi_predict(tfv_ext_col_comments, ext_models)\n\n\npreds_ens = ensemble(tfv_col_comments, preds, 3)\npreds_rud_ens = ensemble(tfv_rud_col_comments, preds_rud, 3)\npreds_ext_ens = ensemble(tfv_ext_col_comments, preds_ext, 3)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:08.802204Z","iopub.execute_input":"2022-02-20T07:20:08.803074Z","iopub.status.idle":"2022-02-20T07:20:09.01487Z","shell.execute_reply.started":"2022-02-20T07:20:08.803025Z","shell.execute_reply":"2022-02-20T07:20:09.013603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub[\"comment_id\"] = comments_df[\"comment_id\"]\nsub[\"kazuma_tc_score\"] = preds_ens\nsub[\"kazuma_rud_score\"] = preds_rud_ens\nsub[\"kazuma_ext_score\"] = preds_ext_ens\nsub","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:09.021123Z","iopub.execute_input":"2022-02-20T07:20:09.024018Z","iopub.status.idle":"2022-02-20T07:20:09.059082Z","shell.execute_reply.started":"2022-02-20T07:20:09.023971Z","shell.execute_reply":"2022-02-20T07:20:09.058221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## gc","metadata":{}},{"cell_type":"code","source":"del tfv_tc\ndel tfv_rud\ndel tfv_col_comments\ndel tfv_rud_col_comments\ndel tfv_ext_col_comments\ndel comments_df\ndel preds_ens\ndel preds_rud_ens\ndel preds_ext_ens\ndel tc_models\ndel rud_models\ndel ext_models\ndel ext_tfv\ndel tc_processor\n\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:09.063538Z","iopub.execute_input":"2022-02-20T07:20:09.066181Z","iopub.status.idle":"2022-02-20T07:20:09.551388Z","shell.execute_reply.started":"2022-02-20T07:20:09.066139Z","shell.execute_reply":"2022-02-20T07:20:09.550452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# kfsky section","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom time import time\nfrom contextlib import contextmanager\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\nfrom sklearn.pipeline import Pipeline\nimport re\n\nimport fasttext\nimport fasttext.util\n\nimport os\nimport pickle\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:09.554604Z","iopub.execute_input":"2022-02-20T07:20:09.555232Z","iopub.status.idle":"2022-02-20T07:20:09.607949Z","shell.execute_reply.started":"2022-02-20T07:20:09.555189Z","shell.execute_reply":"2022-02-20T07:20:09.606938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nvalid_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:09.609639Z","iopub.execute_input":"2022-02-20T07:20:09.609935Z","iopub.status.idle":"2022-02-20T07:20:11.976096Z","shell.execute_reply.started":"2022-02-20T07:20:09.60989Z","shell.execute_reply":"2022-02-20T07:20:11.975121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text = train_df[[\"id\", \"comment_text\"]]\n\nvalid_text_l = pd.DataFrame()\nvalid_text_l[\"comment_text\"] = valid_df[\"less_toxic\"]\nvalid_text_l[\"id\"] = \"less\"\n\nvalid_text_m = pd.DataFrame()\nvalid_text_m[\"comment_text\"] = valid_df[\"more_toxic\"]\nvalid_text_m[\"id\"] = \"more\"\n\nvalid_text = pd.concat([valid_text_m, valid_text_l],axis=0)\n\ncheck_df = pd.concat([train_text, valid_text], axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:11.977535Z","iopub.execute_input":"2022-02-20T07:20:11.977868Z","iopub.status.idle":"2022-02-20T07:20:12.01629Z","shell.execute_reply.started":"2022-02-20T07:20:11.977825Z","shell.execute_reply":"2022-02-20T07:20:12.01539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop_id\ndrop_id = check_df[(check_df[\"comment_text\"].duplicated(keep=False))&(check_df[\"id\"]!=\"less\")&(check_df[\"id\"]!=\"more\")][\"id\"].unique()\ntrain_df = train_df[~train_df[\"id\"].isin(drop_id)]","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:12.017632Z","iopub.execute_input":"2022-02-20T07:20:12.017971Z","iopub.status.idle":"2022-02-20T07:20:12.23145Z","shell.execute_reply.started":"2022-02-20T07:20:12.017924Z","shell.execute_reply":"2022-02-20T07:20:12.230246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del check_df\ndel train_text\ndel valid_text\ndel valid_text_l\ndel valid_text_m\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:12.233116Z","iopub.execute_input":"2022-02-20T07:20:12.233489Z","iopub.status.idle":"2022-02-20T07:20:12.491536Z","shell.execute_reply.started":"2022-02-20T07:20:12.233422Z","shell.execute_reply":"2022-02-20T07:20:12.490444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    train_df[category] = train_df[category] * cat_mtpl[category]\n    \ntrain_df[\"score\"] = train_df.loc[:, \"toxic\": \"identity_hate\"].sum(axis=1)\ntrain_df[\"y\"] = train_df[\"score\"]\n\nmin_len = (train_df[\"y\"]>=0.1).sum()\ny0_undersample = train_df[train_df[\"y\"]==0].sample(n=min_len, random_state=2021)\nnew_train_df = pd.concat([train_df[train_df[\"y\"]>=0.1], y0_undersample])\n\nnew_train_df = new_train_df[[\"id\", \"comment_text\", \"y\"]].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:12.492863Z","iopub.execute_input":"2022-02-20T07:20:12.493164Z","iopub.status.idle":"2022-02-20T07:20:12.561153Z","shell.execute_reply.started":"2022-02-20T07:20:12.493119Z","shell.execute_reply":"2022-02-20T07:20:12.560236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_df\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:12.562627Z","iopub.execute_input":"2022-02-20T07:20:12.563647Z","iopub.status.idle":"2022-02-20T07:20:12.83145Z","shell.execute_reply.started":"2022-02-20T07:20:12.563524Z","shell.execute_reply":"2022-02-20T07:20:12.830257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\nclass BaseBlock(object):\n    def fit(self, input_df, y=None):\n        return self.transform(input_df)\n    \n    def transform(self, input_df):\n        raise NotImplementedError()\n\n        \nclass TfidfBlock(BaseBlock):\n    def __init__(self, column: str, whole_df: pd.DataFrame, decomposition: str, n_compose: int):\n        self.column = column\n        self.whole_df = whole_df\n        self.decomposition = decomposition\n        self.n_compose = n_compose\n\n    def fit(self, input_df, y=None):\n        master_df = self.whole_df\n        text = self.whole_df[self.column].fillna(\"\")\n\n        if self.decomposition == \"svd\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        elif self.decomposition == \"NMF\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", NMF(n_components=self.n_compose, random_state=71))\n            ])\n        elif self.decomposition == \"LDA\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", LatentDirichletAllocation(n_components=self.n_compose, random_state=71))\n            ])\n        else:\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        self.pipeline_.fit(text)\n\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        text = input_df[self.column].fillna(\"\")\n        z = self.pipeline_.transform(text)\n\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix(f'{self.column}_tfidf_{self.decomposition}_')\n    \n\nclass StringLengthBlock(BaseBlock):\n    def __init__(self, column):\n        self.column = column\n\n    def transform(self, input_df):\n        output_df = pd.DataFrame()\n        output_df[self.column] = input_df[self.column].str.len()\n        return output_df.add_prefix(\"StringLength_\")\n\n    \nclass CountVectorizerBlock(BaseBlock):\n    def __init__(self, column: str, whole_df: pd.DataFrame, decomposition: str, n_compose: int):\n        self.column = column\n        self.whole_df = whole_df\n        self.decomposition = decomposition\n        self.n_compose = n_compose\n\n    def fit(self, input_df, y=None):\n        master_df = self.whole_df\n        text = self.whole_df[self.column].fillna(\"\")\n\n        if self.decomposition == \"svd\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        elif self.decomposition == \"NMF\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", NMF(n_components=self.n_compose, random_state=71))\n            ])\n        elif self.decomposition == \"LDA\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", LatentDirichletAllocation(n_components=self.n_compose, random_state=71))\n            ])\n        else:\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        self.pipeline_.fit(text)\n\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        text = input_df[self.column].fillna(\"\")\n        z = self.pipeline_.transform(text)\n\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix(f'{self.column}_CountVectorizer_{self.decomposition}_')\n\n    \nclass WordCountBlock(BaseBlock):\n    def __init__(self, column):\n        self.column = column\n\n    def transform(self, input_df):\n        output_df = pd.DataFrame()\n        output_df[self.column] = input_df[self.column].astype(str).map(lambda x: len(x.split()))\n        return output_df.add_prefix(\"WordCount_\")\n    \n\n## FastText\ndef get_text_series(input_df: pd.DataFrame, column: str, sep='&'):\n    out_series = None\n    for i, c in enumerate(column.split(sep)):\n        text_i = input_df[c].astype(str)\n        if out_series is None:\n            out_series = text_i\n        else:\n            out_series = out_series + ' ' + text_i\n    return out_series\n\ndef create_embedding(document: str, model):\n    words = document.split(\" \")\n    x = [model.get_word_vector(w) for w in words]\n    x = np.max(x, axis=0)\n    return x\n\n\ndef load_fasttext_model():\n    ft = fasttext.load_model(\"../input/fast100/cc.en.100.bin\")\n    ft = fasttext.util.reduce_model(ft, 100)\n\n    return ft\n\n\nclass FasttextEmbeddingBlock(BaseBlock):\n    def __init__(self, column: str):\n        self.column = column\n\n    def fit(self, input_df, y=None, **kwargs):\n        self.ft = load_fasttext_model()\n\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        emb = np.stack(input_df['comment_text'].map(lambda x: self.ft.get_sentence_vector(x)).values)\n        output_df = pd.DataFrame(emb)\n        return output_df.add_prefix(f'{self.column}_FastText')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:12.833741Z","iopub.execute_input":"2022-02-20T07:20:12.834137Z","iopub.status.idle":"2022-02-20T07:20:12.878826Z","shell.execute_reply.started":"2022-02-20T07:20:12.834093Z","shell.execute_reply":"2022-02-20T07:20:12.877752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@contextmanager\ndef timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time()\n    yield\n    d = time() - start\n    out_str = format_str.format(d)\n    if logger:\n        logger.info(out_str)\n    else:\n        print(out_str)\n\n\ndef get_function(block, is_train):\n    s = mapping = {\n        True: 'fit',\n        False: 'transform'\n    }.get(is_train)\n    return getattr(block, s)\n\n\ndef to_feature(input_df,\n               blocks,\n               is_train=False):\n    out_df = pd.DataFrame()\n\n    for block in tqdm(blocks, total=len(blocks)):\n        func = get_function(block, is_train)\n\n        with timer(prefix='create ' + str(block) + ' '):\n            _df = func(input_df)\n        assert len(_df) == len(input_df), func.__name__\n        out_df = pd.concat([out_df, _df], axis=1)\n    return reduce_mem_usage(out_df)\n\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\n              .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:12.887358Z","iopub.execute_input":"2022-02-20T07:20:12.887646Z","iopub.status.idle":"2022-02-20T07:20:12.909726Z","shell.execute_reply.started":"2022-02-20T07:20:12.887618Z","shell.execute_reply":"2022-02-20T07:20:12.908745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.rename(columns = {\"text\": \"comment_text\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:12.911688Z","iopub.execute_input":"2022-02-20T07:20:12.912843Z","iopub.status.idle":"2022-02-20T07:20:12.925413Z","shell.execute_reply.started":"2022-02-20T07:20:12.912772Z","shell.execute_reply":"2022-02-20T07:20:12.924188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\nnew_train_df['comment_text'] = new_train_df['comment_text'].progress_apply(text_cleaning)\ntest_df['comment_text'] = test_df['comment_text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:12.928824Z","iopub.execute_input":"2022-02-20T07:20:12.930496Z","iopub.status.idle":"2022-02-20T07:20:28.299009Z","shell.execute_reply.started":"2022-02-20T07:20:12.930463Z","shell.execute_reply":"2022-02-20T07:20:28.297863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_blocks = [\n    FasttextEmbeddingBlock(\"comment_text\"),\n    TfidfBlock(\"comment_text\" ,whole_df=new_train_df, decomposition=\"svd\", n_compose=500),\n    CountVectorizerBlock(\"comment_text\" ,whole_df=new_train_df, decomposition=\"svd\", n_compose=500),\n    StringLengthBlock(\"comment_text\"),\n    WordCountBlock(\"comment_text\")\n]\n\n\ntrain_x = to_feature(new_train_df, process_blocks, is_train=True)\ntest_x = to_feature(test_df, process_blocks)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:20:28.301066Z","iopub.execute_input":"2022-02-20T07:20:28.301607Z","iopub.status.idle":"2022-02-20T07:25:19.456793Z","shell.execute_reply.started":"2022-02-20T07:20:28.301559Z","shell.execute_reply":"2022-02-20T07:25:19.455753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_x.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:19.458459Z","iopub.execute_input":"2022-02-20T07:25:19.459012Z","iopub.status.idle":"2022-02-20T07:25:19.504009Z","shell.execute_reply.started":"2022-02-20T07:25:19.458964Z","shell.execute_reply":"2022-02-20T07:25:19.503064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict lightGBM\nmodel_lightgbm = []\nfor i in range(5):\n    model = pickle.load(open(f\"../input/fork-of-lgbm-ridge-kfksy-131a72/lightGBM_kfsky_fold{i}\", 'rb'))\n    model_lightgbm.append(model)\n\npred_lightgbm = np.array([model.predict(test_x.values) for model in model_lightgbm])\npred_lightgbm = np.mean(pred_lightgbm, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:19.506497Z","iopub.execute_input":"2022-02-20T07:25:19.507344Z","iopub.status.idle":"2022-02-20T07:25:36.94488Z","shell.execute_reply.started":"2022-02-20T07:25:19.5073Z","shell.execute_reply":"2022-02-20T07:25:36.943828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec = pickle.load(open(\"../input/fork-of-lgbm-ridge-kfksy-131a72/Ridge_tfidf\", 'rb'))\n#X = vec.fit_transform(new_train_df['comment_text'])\nX_test = vec.transform(test_df['comment_text'])","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:36.946925Z","iopub.execute_input":"2022-02-20T07:25:36.94725Z","iopub.status.idle":"2022-02-20T07:25:46.24156Z","shell.execute_reply.started":"2022-02-20T07:25:36.947209Z","shell.execute_reply":"2022-02-20T07:25:46.240598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_model = pickle.load(open(\"../input/fork-of-lgbm-ridge-kfksy-131a72/Ridge_kfsky\", 'rb'))\npred_ridge = ridge_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:46.243231Z","iopub.execute_input":"2022-02-20T07:25:46.243625Z","iopub.status.idle":"2022-02-20T07:25:46.276914Z","shell.execute_reply.started":"2022-02-20T07:25:46.243547Z","shell.execute_reply":"2022-02-20T07:25:46.275922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_ridge","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:46.278361Z","iopub.execute_input":"2022-02-20T07:25:46.279109Z","iopub.status.idle":"2022-02-20T07:25:46.288853Z","shell.execute_reply.started":"2022-02-20T07:25:46.279063Z","shell.execute_reply":"2022-02-20T07:25:46.287365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_kfsky = pd.DataFrame()\nsub_kfsky[\"comment_id\"] = test_df[\"comment_id\"]\nsub_kfsky[\"kfsky_lightgbm\"] = pred_lightgbm\nsub_kfsky[\"kfsky_ridge\"] = pred_ridge\nsub_kfsky.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:46.291091Z","iopub.execute_input":"2022-02-20T07:25:46.292164Z","iopub.status.idle":"2022-02-20T07:25:46.311057Z","shell.execute_reply.started":"2022-02-20T07:25:46.292102Z","shell.execute_reply":"2022-02-20T07:25:46.309884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del new_train_df\ndel test_df\ndel train_x\ndel test_x\ndel X_test\ndel vec\ndel model_lightgbm\ndel pred_lightgbm\ndel ridge_model\ndel pred_ridge\ndel process_blocks\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:46.312569Z","iopub.execute_input":"2022-02-20T07:25:46.312961Z","iopub.status.idle":"2022-02-20T07:25:46.979414Z","shell.execute_reply.started":"2022-02-20T07:25:46.312916Z","shell.execute_reply":"2022-02-20T07:25:46.978443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mogmog section","metadata":{}},{"cell_type":"code","source":"class CFG:\n    debug=False\n    seed=42\n    n_fold = 4\n    model_name = \"../input/roberta-base-edited\"\n    max_len = 256\n    text=\"text_clean2\"\n    target=\"target\"\n    target_size = 1\n    hidden_size = 768\n    fc_dropout = 0.\n    batch_size = 32\n    num_workers = 4\n    model_dir = \"../input/roberta-001-train-jigsaw2\"","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:46.981259Z","iopub.execute_input":"2022-02-20T07:25:46.981626Z","iopub.status.idle":"2022-02-20T07:25:46.988331Z","shell.execute_reply.started":"2022-02-20T07:25:46.981578Z","shell.execute_reply":"2022-02-20T07:25:46.986954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport time\nimport math\nimport random\nimport string\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\n\nfrom gensim import models\nfrom gensim.models import KeyedVectors,FastText\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression,chi2, f_regression\nfrom bs4 import BeautifulSoup\n\nfrom sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\nfrom scipy import sparse\nimport scipy\nimport seaborn as sns\nfrom unicodedata import category, name, normalize\n\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import OneCycleLR\n\n# NLP\nfrom transformers import AutoTokenizer, AutoModel,get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option(\"max_columns\",100)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:46.99065Z","iopub.execute_input":"2022-02-20T07:25:46.991072Z","iopub.status.idle":"2022-02-20T07:25:53.098187Z","shell.execute_reply.started":"2022-02-20T07:25:46.990943Z","shell.execute_reply":"2022-02-20T07:25:53.097193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:53.099764Z","iopub.execute_input":"2022-02-20T07:25:53.100085Z","iopub.status.idle":"2022-02-20T07:25:53.160422Z","shell.execute_reply.started":"2022-02-20T07:25:53.100041Z","shell.execute_reply":"2022-02-20T07:25:53.159246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- reference\n- https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage","metadata":{}},{"cell_type":"code","source":"spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\ndef remove_space(text):\n    \"\"\"\n    remove extra spaces and ending space if any\n    \"\"\"\n    for space in spaces:\n        text = text.replace(space, ' ')\n    text = text.strip()\n    text = re.sub('\\s+', ' ', text)\n    return text\n\n\nspecial_punc_mappings = {\"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '”': '\"', \"″\": '\"', '“': '\"', '•': '.', '−': '-',\n                         \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','،':'','„':'',\n                         '…': ' ... ', '\\ufeff': ''}\ndef clean_special_punctuations(text):\n    for punc in special_punc_mappings:\n        if punc in text:\n            text = text.replace(punc, special_punc_mappings[punc])\n    #text = remove_diacritics(text)\n    return text\n\n\ndef clean_number(text):\n    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n    \n    return text\n\n\nrare_words_mapping = {' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA',\n                      'u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA', ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',\n                      ' U.s ': 'USA', ' u.S ': ' USA ', 'fu.k': 'fuck', 'U.K.': 'UK', ' u.k ': ' UK ',\n                      ' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n                      'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n                      'Ra apist': 'Rapist', '2fifth': 'twenty fifth', '2third': 'twenty third',\n                      '2nineth': 'twenty nineth', '2fourth': 'twenty fourth', '#metoo': 'MeToo',\n                      'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n                      'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin',\n                      'fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n                      'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n                      'culturr': 'culture',\n                      'weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n                      'indans': 'indians', 'mastuburate': 'masturbate', 'f**k': 'fuck', 'F**k': 'fuck', 'F**K': 'fuck',\n                      ' u r ': ' you are ', ' u ': ' you ', '操你妈': 'fuck your mother', 'e.g.': 'for example',\n                      'i.e.': 'in other words', '...': '.', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n                      'f***': 'fuck', 'f**': 'fuc', 'F***': 'fuck', 'F**': 'fuc','f*ck':'fuck',\n                      'a****': 'assho', 'a**': 'ass', 'h***': 'hole', 'A****': 'assho', 'A**': 'ass', 'H***': 'hole',\n                      's***': 'shit', 's**': 'shi', 'S***': 'shit', 'S**': 'shi', 'Sh**': 'shit','sh*t':'shit',\n                      'p****': 'pussy', 'p*ssy': 'pussy', 'P****': 'pussy',\n                      'p***': 'porn', 'p*rn': 'porn', 'P***': 'porn',\n                      'st*up*id': 'stupid',\n                      'd***': 'dick', 'di**': 'dick', 'h*ck': 'hack',\n                      'b*tch': 'bitch', 'bi*ch': 'bitch', 'bit*h': 'bitch', 'bitc*': 'bitch', 'b****': 'bitch',\n                      'b***': 'bitc', 'b**': 'bit', 'b*ll': 'bull'\n                      }\n\ndef pre_clean_rare_words(text):\n    for rare_word in rare_words_mapping:\n        if rare_word in text:\n            text = text.replace(rare_word, rare_words_mapping[rare_word])\n    return text\n\n\n\n# de-contract the contraction\ndef decontracted(text):\n    # specific\n    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n\n    # general\n    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)in(\\'|\\’)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\’)s \", \" is \", text)\n    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n    return text\n\n\n\nregular_punct = list(string.punctuation)\nextra_punct = [\n    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\nall_punct = list(set(regular_punct + extra_punct))\n# do not spacing - and .\nall_punct.remove('-')\nall_punct.remove('.')\n#all_punct.remove('!') \n\ndef spacing_punctuation(text):\n    \"\"\"\n    add space before and after punctuation and symbols\n    \"\"\"\n    for punc in all_punct:\n        if punc in text:\n            text = text.replace(punc, f' {punc} ')\n    return text\ndef remove_punctuation(text):\n    for punc in all_punct:\n        if punc in text:\n            text = text.replace(punc, ' ')\n    return text\n\n\n\nmis_connect_list = ['(W|w)hat', '(W|w)hy', '(H|h)ow', '(W|w)hich', '(W|w)here', '(W|w)ill']\nmis_connect_re = re.compile('(%s)' % '|'.join(mis_connect_list))\n\nmis_spell_mapping = {'whattsup': 'WhatsApp', 'whatasapp':'WhatsApp', 'whatsupp':'WhatsApp', \n                      'whatcus':'what cause', 'arewhatsapp': 'are WhatsApp', 'Hwhat':'what',\n                      'Whwhat': 'What', 'whatshapp':'WhatsApp', 'howhat':'how that',\n                      # why\n                      'Whybis':'Why is', \n                      # How\n                      \"Howddo\":\"How do\", 'Howeber':'However'}\ndef spacing_some_connect_words(text):\n    \"\"\"\n    'Whyare' -> 'Why are'\n    \"\"\"\n    ori = text\n    for error in mis_spell_mapping:\n        if error in text:\n            text = text.replace(error, mis_spell_mapping[error])\n            \n    # what\n    text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n    text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n    text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n    # why\n    text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n    text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n    # How\n    text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n    text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n    # which\n    text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n    text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n    # where\n    text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n    text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n    # \n    text = mis_connect_re.sub(r\" \\1 \", text)\n    text = text.replace(\"What sApp\", 'WhatsApp')\n    \n    text = remove_space(text)\n    return text\n\n\n\ndef remove_number(text):\n    text = re.sub(r'[0-9]+', '', text)\n    return text\n\n\ndef get_lower(text):\n    text = text.lower()\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:53.162452Z","iopub.execute_input":"2022-02-20T07:25:53.163104Z","iopub.status.idle":"2022-02-20T07:25:53.216417Z","shell.execute_reply.started":"2022-02-20T07:25:53.163055Z","shell.execute_reply":"2022-02-20T07:25:53.215279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/manabendrarout/pytorch-roberta-ranking-baseline-jrstc-train/notebook\ndef text_cleaning_1(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = remove_space(text)\n    text = clean_special_punctuations(text)\n    text = clean_number(text)\n    text = pre_clean_rare_words(text)\n    text = decontracted(text)\n    text = spacing_punctuation(text)\n    text = spacing_some_connect_words(text)\n    text = remove_space(text)\n    \n    #text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    #text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    #text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\ndef text_cleaning_2(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\n\n# https://www.kaggle.com/manabendrarout/pytorch-roberta-ranking-baseline-jrstc-train/notebook\ndef text_cleaning_3(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = remove_number(text) \n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n    text = get_lower(text)\n\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:53.218639Z","iopub.execute_input":"2022-02-20T07:25:53.219222Z","iopub.status.idle":"2022-02-20T07:25:53.238481Z","shell.execute_reply.started":"2022-02-20T07:25:53.219174Z","shell.execute_reply":"2022-02-20T07:25:53.237285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_basic_feat(input_df):\n    out_df = pd.DataFrame()\n    out_df[\"Num_character\"] = input_df[\"text\"].apply(lambda x:len(x))\n    out_df[\"Num_word\"] = input_df[\"text\"].apply(lambda x:len(x.split()))\n    \n    return out_df\n\ndef get_gensim_embed(texts,ndim):\n    swem_embedding = np.zeros((len(texts), ndim))\n    \n    for i, text in enumerate(tqdm(texts)):\n        embeddings = [w2v_model.get_vector(word)\n                      if w2v_model.key_to_index.get(word) is not None\n                      else np.zeros(ndim, dtype=np.float32)\n                      for word in text.split()\n                     ]\n        if len(embeddings) > 0:\n            mean_vector = np.mean(np.stack(embeddings), axis=0)\n            swem_embedding[i] = mean_vector\n    return swem_embedding\n\ndef get_fasttext_embed(texts,ndim):\n    swem_embedding = np.zeros((len(texts), ndim))\n    \n    for i, text in enumerate(tqdm(texts)):\n        tokens = [word for word in text.split()]\n        if len(tokens)>0:\n            mean_vector = np.mean(fmodel.wv[tokens], axis = 0)\n            #print(mean_vector.shape)\n            swem_embedding[i] = mean_vector\n    return swem_embedding","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:53.240287Z","iopub.execute_input":"2022-02-20T07:25:53.241503Z","iopub.status.idle":"2022-02-20T07:25:53.258587Z","shell.execute_reply.started":"2022-02-20T07:25:53.241468Z","shell.execute_reply":"2022-02-20T07:25:53.257612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_cols = [\n    \"text_clean1\",\n    \"text_clean2\",\n    \"text_clean3\"\n]\ndef get_val_tfidf_dict(vec_dict, val_df_for_pred):\n    out_dict = {}\n    for _name,_vec in tqdm(vec_dict.items()):\n        _col = \"_\".join(_name.split(\"_\")[-2:]) \n        out_dict[_name] = _vec.transform(val_df_for_pred[_col])\n    return out_dict\n\ndef get_val_select_topfeat_and_svd(tfidf_df_val_dict,selector_dict,svd_transfomer_dict):\n    svd_matrix_list = []\n    for i in tqdm(range(6)):\n        _mat = list(tfidf_df_val_dict.values())[i]\n        _selector = list(selector_dict.values())[i]\n        _svd_transformer = list(svd_transfomer_dict.values())[i]\n\n        _matrix_filtered = _selector.transform(_mat)\n        _svd_matrix = _svd_transformer.transform(_matrix_filtered)\n        svd_matrix_list.append(_svd_matrix)\n\n    val_svd_matrix =  np.hstack(svd_matrix_list)\n    return val_svd_matrix","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:53.262157Z","iopub.execute_input":"2022-02-20T07:25:53.264403Z","iopub.status.idle":"2022-02-20T07:25:53.273392Z","shell.execute_reply.started":"2022-02-20T07:25:53.264369Z","shell.execute_reply":"2022-02-20T07:25:53.272253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid(model_dict, df_val_dict):\n    preds_list = []\n    for _name,_model_list in model_dict.items():\n        _preds =[]\n        for i in range(5):\n            pred = _model_list[i].predict(df_val_dict[_name])\n            _preds.append(pred)\n        preds_list.append(np.mean(_preds, axis=0))\n    return preds_list\n\ndef valid2(lgb_model_dict):\n    preds_list = []\n    for i in tqdm(range(3)):\n        _gen = list(gensim_df_val_dict.values())[i]\n        _fast = list(fasttext_df_val_dict.values())[i]\n\n        mat = np.hstack([_gen,\n                         _fast,\n                         basic_feat_val_df[use_basic_feat].values\n                        ])\n\n        _preds =[]\n        _model_list = list(lgb_model_dict.values())[i]\n\n        for j in range(5):\n            pred = _model_list[j].predict(mat)\n            _preds.append(pred)\n        preds_list.append(np.mean(_preds, axis=0))\n    return preds_list","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:53.275569Z","iopub.execute_input":"2022-02-20T07:25:53.276106Z","iopub.status.idle":"2022-02-20T07:25:53.288691Z","shell.execute_reply.started":"2022-02-20T07:25:53.276047Z","shell.execute_reply":"2022-02-20T07:25:53.287629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    \ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\nclass JigsawDataset(Dataset):\n    def __init__(self, CFG, input_df, is_train=True):\n        self.CFG = CFG\n        self.is_train = is_train\n        self.text = input_df[self.CFG.text].values\n        self.tokenizer = AutoTokenizer.from_pretrained(self.CFG.model_name)\n        if self.is_train:\n            self.labels = input_df[self.CFG.target].values       \n             \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        text =  self.text[idx]\n        encoded = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.CFG.max_len,\n            padding='max_length'\n        )\n        input_ids = torch.tensor(encoded['input_ids'])\n        attention_mask = torch.tensor(encoded['attention_mask'])\n        \n        if self.is_train:\n            #label = torch.tensor(self.labels[idx]).float()\n            label = torch.tensor(self.labels[idx])\n            return input_ids, attention_mask, label\n        return input_ids, attention_mask\n    \nclass JigsawModel(nn.Module):\n    def __init__(self, CFG):\n        super().__init__()\n        self.CFG = CFG\n        self.model = AutoModel.from_pretrained(self.CFG.model_name)\n        self.fc_dropout = nn.Dropout(self.CFG.fc_dropout)\n        self.fc = nn.Linear(self.CFG.hidden_size, self.CFG.target_size)\n    \n    def forward(self, input_ids, attention_mask):\n        out = self.model(input_ids = input_ids, \n                         attention_mask = attention_mask)        \n        out = self.fc_dropout(out[1])\n        outputs = self.fc(out)\n        return outputs\n\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    \n    for step, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total = len(test_loader)):\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        \n        with torch.no_grad():\n            y_preds = model(input_ids, attention_mask)\n        preds.append(y_preds.to(\"cpu\").numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:53.289918Z","iopub.execute_input":"2022-02-20T07:25:53.291704Z","iopub.status.idle":"2022-02-20T07:25:53.315176Z","shell.execute_reply.started":"2022-02-20T07:25:53.291665Z","shell.execute_reply":"2022-02-20T07:25:53.313909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_pkl(file_path):\n    out_object = pickle.load(open(file_path, 'rb'))   \n    return out_object","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:53.317411Z","iopub.execute_input":"2022-02-20T07:25:53.31822Z","iopub.status.idle":"2022-02-20T07:25:53.329578Z","shell.execute_reply.started":"2022-02-20T07:25:53.318173Z","shell.execute_reply":"2022-02-20T07:25:53.328576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vec_dict = load_pkl(\"../input/mogmog-train-13/tfidf_vec_dict.pkl\")\ntfidf_vec_dict_ruddit = load_pkl(\"../input/mogmog-train-13/tfidf_vec_dict_ruddit.pkl\")\ntfidf_feat_selector = load_pkl(\"../input/mogmog-train-13/tfidf_feat_selector.pkl\")\ntfidf_svd_transfomer = load_pkl(\"../input/mogmog-train-13/tfidf_svd_transfomer.pkl\")\ntfidf_feat_selector_ruddit = load_pkl(\"../input/mogmog-train-13/tfidf_feat_selector_ruddit.pkl\")\ntfidf_svd_transfomer_ruddit = load_pkl(\"../input/mogmog-train-13/tfidf_svd_transfomer_ruddit.pkl\")\n\nridge_model_dict_1 = load_pkl(\"../input/mogmog-train-13/ridge_model_dict_1.pkl\")\nridge_model_dict_1_ruddit = load_pkl(\"../input/mogmog-train-13/ridge_model_dict_1_ruddit.pkl\")\nlgb_model_dict_2 = load_pkl(\"../input/mogmog-train-13/lgb_model_dict_2.pkl\")\nlgb_model_dict_1_ruddit = load_pkl(\"../input/mogmog-train-13/lgb_model_dict_1_ruddit.pkl\")\nlgb_model_dict_3 = load_pkl(\"../input/mogmog-train-13/lgb_model_dict_3.pkl\")\nlgb_model_dict_2_ruddit = load_pkl(\"../input/mogmog-train-13/lgb_model_dict_2_ruddit.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:53.34384Z","iopub.execute_input":"2022-02-20T07:25:53.344363Z","iopub.status.idle":"2022-02-20T07:25:58.816852Z","shell.execute_reply.started":"2022-02-20T07:25:53.344318Z","shell.execute_reply":"2022-02-20T07:25:58.815812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\ntqdm.pandas()\ntest_df[\"text_clean1\"] = test_df['text'].progress_apply(text_cleaning_1)\ntest_df[\"text_clean2\"] = test_df['text'].progress_apply(text_cleaning_2)\ntest_df[\"text_clean3\"] = test_df['text'].progress_apply(text_cleaning_3)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:58.818523Z","iopub.execute_input":"2022-02-20T07:25:58.821187Z","iopub.status.idle":"2022-02-20T07:26:15.448484Z","shell.execute_reply.started":"2022-02-20T07:25:58.82114Z","shell.execute_reply":"2022-02-20T07:26:15.447515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_df_test_dict = get_val_tfidf_dict(tfidf_vec_dict, test_df)\ntfidf_df_test_dict_ruddit = get_val_tfidf_dict(tfidf_vec_dict_ruddit, test_df)\n\ntest_svd_matrix = get_val_select_topfeat_and_svd(tfidf_df_test_dict,\n                                                tfidf_feat_selector,\n                                                tfidf_svd_transfomer)\ntest_svd_matrix_ruddit = get_val_select_topfeat_and_svd(tfidf_df_test_dict_ruddit,\n                                                       tfidf_feat_selector_ruddit,\n                                                       tfidf_svd_transfomer_ruddit)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:26:15.449821Z","iopub.execute_input":"2022-02-20T07:26:15.450499Z","iopub.status.idle":"2022-02-20T07:26:58.859438Z","shell.execute_reply.started":"2022-02-20T07:26:15.450435Z","shell.execute_reply":"2022-02-20T07:26:58.85855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = KeyedVectors.load(\"../input/gensim-googlenewsvectorsnegative300/GoogleNews-vectors-negative300.gensim\")\ngensim_df_test_dict = {}\nfor _col in tqdm(use_cols):\n    gensim_df_test_dict[f\"gensim_{_col}\"] = get_gensim_embed(test_df[_col],ndim=300)\n    \ndel w2v_model\ngc.collect()\n\nfmodel = FastText.load('../input/jigsaw-regression-based-data/FastText-jigsaw-256D/Jigsaw-Fasttext-Word-Embeddings-256D.bin')\nfasttext_df_test_dict = {}\nfor _col in tqdm(use_cols):\n    fasttext_df_test_dict[f\"fasttext_{_col}\"] = get_fasttext_embed(test_df[_col],ndim=256)\n\ndel fmodel\ngc.collect()\n\nbasic_feat_test_df = make_basic_feat(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:26:58.860962Z","iopub.execute_input":"2022-02-20T07:26:58.861789Z","iopub.status.idle":"2022-02-20T07:29:20.198512Z","shell.execute_reply.started":"2022-02-20T07:26:58.861743Z","shell.execute_reply":"2022-02-20T07:29:20.197466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_preds_list1 = valid(ridge_model_dict_1, tfidf_df_test_dict)\npred1 = np.mean(_preds_list1,axis = 0)\n\n_preds_list2 = valid(ridge_model_dict_1_ruddit, tfidf_df_test_dict_ruddit)\npred2 = np.mean(_preds_list2,axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:29:20.200528Z","iopub.execute_input":"2022-02-20T07:29:20.200886Z","iopub.status.idle":"2022-02-20T07:29:20.558088Z","shell.execute_reply.started":"2022-02-20T07:29:20.200835Z","shell.execute_reply":"2022-02-20T07:29:20.557061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_basic_feat = [\"Num_character\", \"Num_word\"]\npreds_list = []\npreds_list_ruddit = []\nfor i in range(3):\n    _gen = list(gensim_df_test_dict.values())[i]\n    _fast = list(fasttext_df_test_dict.values())[i]\n    \n    mat = np.hstack([_gen,\n                     _fast,\n                     basic_feat_test_df[use_basic_feat].values\n                    ])\n    \n    _preds =[]\n    _preds_ruddit =[]\n    \n    _model_list = list(lgb_model_dict_2.values())[i] \n    _model_list_ruddit = list(lgb_model_dict_1_ruddit.values())[i] \n    \n    for j in range(5):\n        pred = _model_list[j].predict(mat)\n        pred_ruddit = _model_list_ruddit[j].predict(mat)\n        _preds.append(pred)\n        _preds_ruddit.append(pred_ruddit)\n        \n    preds_list.append(np.mean(_preds, axis=0))\n    preds_list_ruddit.append(np.mean(_preds_ruddit, axis=0))\n\npred3 = np.mean(preds_list,axis = 0)\npred4 = np.mean(preds_list_ruddit,axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:29:20.559449Z","iopub.execute_input":"2022-02-20T07:29:20.559748Z","iopub.status.idle":"2022-02-20T07:29:44.050615Z","shell.execute_reply.started":"2022-02-20T07:29:20.559707Z","shell.execute_reply":"2022-02-20T07:29:44.049622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds5 = []\nfor _model in lgb_model_dict_3['TF-IDF(SVD)_600dim']:\n    pred = _model.predict(test_svd_matrix)\n    preds5.append(pred)\npred5 = np.mean(preds5,axis = 0)\n\npreds6 = []\nfor _model in lgb_model_dict_2_ruddit['TF-IDF(SVD)_600dim']:\n    pred = _model.predict(test_svd_matrix_ruddit)\n    preds6.append(pred)\npred6 = np.mean(preds6,axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:29:44.055965Z","iopub.execute_input":"2022-02-20T07:29:44.058436Z","iopub.status.idle":"2022-02-20T07:29:49.325977Z","shell.execute_reply.started":"2022-02-20T07:29:44.058391Z","shell.execute_reply":"2022-02-20T07:29:49.324887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = JigsawDataset(CFG, test_df, is_train = False)\ntest_loader = DataLoader(test_dataset, \n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, \n                         pin_memory=True, \n                         drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:29:49.327469Z","iopub.execute_input":"2022-02-20T07:29:49.327817Z","iopub.status.idle":"2022-02-20T07:29:49.712019Z","shell.execute_reply.started":"2022-02-20T07:29:49.327776Z","shell.execute_reply":"2022-02-20T07:29:49.710983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds7 = []\nfor fold in range(CFG.n_fold):\n    model = JigsawModel(CFG)\n    state = torch.load(CFG.model_dir+f\"/{CFG.model_name.split('/')[-1]}_fold{fold}_best.pth\", map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    preds7.append(prediction)\n    del model, state; gc.collect()\n    torch.cuda.empty_cache()\npred7 = np.mean(preds7,axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:29:49.713444Z","iopub.execute_input":"2022-02-20T07:29:49.715005Z","iopub.status.idle":"2022-02-20T07:34:39.465824Z","shell.execute_reply.started":"2022-02-20T07:29:49.714958Z","shell.execute_reply":"2022-02-20T07:34:39.464853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_dataset, test_loader\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:34:39.467462Z","iopub.execute_input":"2022-02-20T07:34:39.46785Z","iopub.status.idle":"2022-02-20T07:34:40.004859Z","shell.execute_reply.started":"2022-02-20T07:34:39.467806Z","shell.execute_reply":"2022-02-20T07:34:40.003713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\npr1_scaled = scaler.fit_transform(pred1.reshape(-1,1))\npr2_scaled = scaler.fit_transform(pred2.reshape(-1,1))\npr3_scaled = scaler.fit_transform(pred3.reshape(-1,1))\npr4_scaled = scaler.fit_transform(pred4.reshape(-1,1))\npr5_scaled = scaler.fit_transform(pred5.reshape(-1,1))\npr6_scaled = scaler.fit_transform(pred6.reshape(-1,1))\npr7_scaled = scaler.fit_transform(pred7.reshape(-1,1))\n\npred_scaled_mat_test = np.hstack([pr1_scaled,\n                                  pr2_scaled,\n                                  pr3_scaled,\n                                  pr4_scaled,\n                                  pr5_scaled,\n                                  pr6_scaled,\n                                  pr7_scaled\n                            ])\npred_scaled_mat_test = pd.DataFrame(pred_scaled_mat_test,\n                                    columns = [\"ridge\",\"ridge_rud\",\"lgb_emb\",\"lgb_emb_rud\",\"lgb_svd\",\"lgb_svd_rud\",\"roberta\"])\npred_scaled_mat_test = pred_scaled_mat_test.add_prefix(\"mogmog_\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:34:40.007135Z","iopub.execute_input":"2022-02-20T07:34:40.007711Z","iopub.status.idle":"2022-02-20T07:34:40.02584Z","shell.execute_reply.started":"2022-02-20T07:34:40.007659Z","shell.execute_reply":"2022-02-20T07:34:40.024311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mogmog_sub = pd.read_csv('../input/jigsaw-toxic-severity-rating/sample_submission.csv').drop(\"score\",axis = 1)\nmogmog_sub = pd.concat([mogmog_sub,pred_scaled_mat_test],axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:34:40.027488Z","iopub.execute_input":"2022-02-20T07:34:40.027941Z","iopub.status.idle":"2022-02-20T07:34:40.047863Z","shell.execute_reply.started":"2022-02-20T07:34:40.027892Z","shell.execute_reply":"2022-02-20T07:34:40.046888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mogmog_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:34:40.050992Z","iopub.execute_input":"2022-02-20T07:34:40.051211Z","iopub.status.idle":"2022-02-20T07:34:40.07078Z","shell.execute_reply.started":"2022-02-20T07:34:40.051182Z","shell.execute_reply":"2022-02-20T07:34:40.069584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del (tfidf_vec_dict,\n     tfidf_vec_dict_ruddit,\n     tfidf_feat_selector,\n     tfidf_svd_transfomer,\n     tfidf_feat_selector_ruddit,\n     tfidf_svd_transfomer_ruddit,\n     ridge_model_dict_1,\n     ridge_model_dict_1_ruddit,\n     lgb_model_dict_2,\n     lgb_model_dict_1_ruddit,\n     lgb_model_dict_3,\n     lgb_model_dict_2_ruddit,\n     pred_scaled_mat_test,\n     tfidf_df_test_dict,\n     tfidf_df_test_dict_ruddit,\n     test_svd_matrix,\n     test_svd_matrix_ruddit,\n     gensim_df_test_dict,\n     fasttext_df_test_dict\n    )\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:34:40.072486Z","iopub.execute_input":"2022-02-20T07:34:40.072863Z","iopub.status.idle":"2022-02-20T07:34:41.056412Z","shell.execute_reply.started":"2022-02-20T07:34:40.072806Z","shell.execute_reply":"2022-02-20T07:34:41.055449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"all_sub_df = pd.concat([\n    sub_kfsky,\n    sub.iloc[:,1:],#kma\n    mogmog_sub.iloc[:,1:]#mogmog\n],axis = 1)\n\nall_sub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:34:41.058358Z","iopub.execute_input":"2022-02-20T07:34:41.058751Z","iopub.status.idle":"2022-02-20T07:34:41.095413Z","shell.execute_reply.started":"2022-02-20T07:34:41.058693Z","shell.execute_reply":"2022-02-20T07:34:41.094114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\nall_sub_df.iloc[:,1:] = pd.DataFrame(scaler.fit_transform(all_sub_df.iloc[:,1:]))\n\nall_sub_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:34:41.097491Z","iopub.execute_input":"2022-02-20T07:34:41.099015Z","iopub.status.idle":"2022-02-20T07:34:41.251943Z","shell.execute_reply.started":"2022-02-20T07:34:41.098404Z","shell.execute_reply":"2022-02-20T07:34:41.246979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sub_df[\"score\"] = np.sum(all_sub_df.iloc[:,1:]*res.x,axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:34:41.253144Z","iopub.execute_input":"2022-02-20T07:34:41.25346Z","iopub.status.idle":"2022-02-20T07:34:41.274034Z","shell.execute_reply.started":"2022-02-20T07:34:41.253417Z","shell.execute_reply":"2022-02-20T07:34:41.268583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sub_df['score'] = all_sub_df['score'].rank(method='first')\nall_sub_df[['comment_id', 'score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:34:41.27585Z","iopub.execute_input":"2022-02-20T07:34:41.276403Z","iopub.status.idle":"2022-02-20T07:34:41.336566Z","shell.execute_reply.started":"2022-02-20T07:34:41.276212Z","shell.execute_reply":"2022-02-20T07:34:41.335182Z"},"trusted":true},"execution_count":null,"outputs":[]}]}