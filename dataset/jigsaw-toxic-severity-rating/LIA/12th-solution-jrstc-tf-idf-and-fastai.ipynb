{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"papermill":{"duration":0.042932,"end_time":"2021-12-19T09:15:15.064179","exception":false,"start_time":"2021-12-19T09:15:15.021247","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/fasthugs')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T23:40:37.95948Z","iopub.execute_input":"2022-01-23T23:40:37.959752Z","iopub.status.idle":"2022-01-23T23:40:37.982472Z","shell.execute_reply.started":"2022-01-23T23:40:37.959672Z","shell.execute_reply":"2022-01-23T23:40:37.981629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \nfrom IPython.display import display, HTML\nfrom pprint import pprint\nimport warnings\nimport joblib\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\n\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.max_colwidth=300","metadata":{"papermill":{"duration":0.981566,"end_time":"2021-12-19T09:15:16.082667","exception":false,"start_time":"2021-12-19T09:15:15.101101","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T23:40:37.984046Z","iopub.execute_input":"2022-01-23T23:40:37.984533Z","iopub.status.idle":"2022-01-23T23:40:39.157605Z","shell.execute_reply.started":"2022-01-23T23:40:37.984495Z","shell.execute_reply":"2022-01-23T23:40:39.156881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training data \n\n## Convert the label to SUM of all toxic labels (This might help with maintaining toxicity order of comments)","metadata":{"papermill":{"duration":0.035128,"end_time":"2021-12-19T09:15:16.153483","exception":false,"start_time":"2021-12-19T09:15:16.118355","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nlemmatizer = WordNetLemmatizer()\nstop = stopwords.words('english')\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+')  # Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) # Remove special Charecters\n    text = re.sub(' +', ' ', text) # Remove Extra Spaces\n    text = text.strip().lower() # remove spaces at the beginning and at the end of string and make string lower\n    \n    # lemmatization\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n    # del stopwords\n    text = ' '.join([word for word in text.split(' ') if word not in stop])\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-01-23T23:40:39.158834Z","iopub.execute_input":"2022-01-23T23:40:39.159071Z","iopub.status.idle":"2022-01-23T23:40:39.532066Z","shell.execute_reply.started":"2022-01-23T23:40:39.15904Z","shell.execute_reply":"2022-01-23T23:40:39.531388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(data, col):\n    \n    data[col] = data[col].str.replace('https?://\\S+|www\\.\\S+', ' social medium ')      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\") \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\")\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \")\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \")\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you')\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace('\\s+', ' ')  # will remove more than one whitespace character\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1') # 2 or more characters are replaced by 2 characters\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\"[:|♣|'|§|♠|*|/|?|=|%|&|-|#|•|~|^|>|<|►|_]\", '')\n    \n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-01-23T23:40:39.535183Z","iopub.execute_input":"2022-01-23T23:40:39.535425Z","iopub.status.idle":"2022-01-23T23:40:39.684198Z","shell.execute_reply.started":"2022-01-23T23:40:39.535392Z","shell.execute_reply":"2022-01-23T23:40:39.683417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def P1_train(dataset='class', cleaned=False, df_muls=None, n_folds=7, frac_factor=1.5):\n    if dataset=='class':\n        df = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n        cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n    elif dataset=='bias':\n        df = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n        cols = ['toxic', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n    elif dataset=='ruddit':\n        df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n    else:\n        assert 1==0 # use aformentioned datasets.\n\n    if df_muls is None:\n        df_muls = [1, 2, 1, 1, 1, 1]\n    df['y'] = 0\n    for col, mul in zip(cols, df_muls):\n        df['y'] = df['y'] + df[col] * mul\n    df['y'] = df['y']/df['y'].max()\n    df = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n    if cleaned:\n        df = clean(df, 'text')\n        tqdm.pandas()\n        df['text'] = df['text'].progress_apply(text_cleaning)\n\n    frac = 0.4\n    for fld in range(n_folds):\n        print(f'Fold: {fld}')\n        tmp_df = pd.concat([df[df.y>0].sample(frac=frac, random_state = 10*(fld+1)) , \n                            df[df.y==0].sample(n=int(len(df[df.y>0])*frac*frac_factor) , random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n        tmp_df.to_csv(f'/kaggle/working/df_fld{fld}.csv', index=False)\n\n    for fld in range(n_folds):\n        print(\"\\nTrain:\")\n        print(f' ****************************** FOLD: {fld} ******************************')\n        df = pd.read_csv(f'/kaggle/working/df_fld{fld}.csv')\n        print(df.shape)\n\n        features = FeatureUnion([\n            #('vect1', LengthTransformer()),\n            #('vect2', LengthUpperTransformer()),\n            (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n            #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n        ])\n        pipeline = Pipeline([\n                (\"features\", features),  #(\"clf\", RandomForestRegressor(n_esatimators = 5, min_sample_leaf=3)),\n                (\"clf\", Ridge()),  #(\"clf\",LinearRegression())\n            ])\n        # Train the pipeline\n        pipeline.fit(df['text'].values.astype('U'), df['y'])\n\n        # What are the important features for toxicity\n        if cleaned:\n            joblib.dump(pipeline, f'{dataset}_c_{fld}.pkl')\n        else:\n            joblib.dump(pipeline, f'{dataset}_d_{fld}.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T23:40:39.686959Z","iopub.execute_input":"2022-01-23T23:40:39.687159Z","iopub.status.idle":"2022-01-23T23:40:39.702144Z","shell.execute_reply.started":"2022-01-23T23:40:39.687135Z","shell.execute_reply":"2022-01-23T23:40:39.701494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#P1_train(dataset='class', cleaned=True , frac_factor=1.5, n_folds=7)\n#P1_train(dataset='bias' , cleaned=True,  frac_factor=0.3, n_folds=7)\nif False:   # trained pipelines are saved\n    P1_train(dataset='class', cleaned=False, frac_factor=1.5, n_folds=7)\n    P1_train(dataset='class', cleaned=True , frac_factor=1.5, n_folds=7)\n\n    P1_train(dataset='bias' , cleaned=False, frac_factor=0.3, n_folds=7)\n    P1_train(dataset='bias' , cleaned=True,  frac_factor=0.3, n_folds=7)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T23:40:39.704911Z","iopub.execute_input":"2022-01-23T23:40:39.705108Z","iopub.status.idle":"2022-01-23T23:40:39.714299Z","shell.execute_reply.started":"2022-01-23T23:40:39.705085Z","shell.execute_reply":"2022-01-23T23:40:39.71335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def P1_eval(dataset='class', cleaned=False, n_folds=7, test_only=False):\n\n    \"\"\"\n    #print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n    #pprint(feature_wts[:30])\n    \"\"\"\n    # Validation and Evaluation\n    df_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n    df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    val_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\n    val_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\n    test_preds_arr = np.zeros((df_sub.shape[0], n_folds))\n    if cleaned:\n        tqdm.pandas()\n        df_sub = clean(df_sub, 'text')\n        df_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\n        if not test_only:\n            df_val = clean(df_val, 'less_toxic')\n            df_val = clean(df_val, 'more_toxic')\n            df_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\n            df_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)\n\n                \n    for fld in range(n_folds):\n        print(\"\\nEval:\")\n        print(f' ****************************** FOLD: {fld} ******************************')\n\n        if cleaned:\n            pipeline = joblib.load(f'../input/jrstc-linear-model/{dataset}_c_{fld}.pkl')\n        else:\n            pipeline = joblib.load(f'../input/jrstc-linear-comp/{dataset}_d_{fld}.pkl')\n            \n        if not test_only:\n            print(\"\\npredict validation data \")\n            val_preds_arr1[:,fld] = pipeline.predict(df_val['less_toxic'])\n            val_preds_arr2[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n        print(\"\\npredict test data \")\n        test_preds_arr[:,fld] = pipeline.predict(df_sub['text'])\n    return val_preds_arr1, val_preds_arr2, test_preds_arr","metadata":{"execution":{"iopub.status.busy":"2022-01-23T23:40:39.715829Z","iopub.execute_input":"2022-01-23T23:40:39.71638Z","iopub.status.idle":"2022-01-23T23:40:39.728453Z","shell.execute_reply.started":"2022-01-23T23:40:39.716299Z","shell.execute_reply":"2022-01-23T23:40:39.727761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    c_d_l, c_d_r, c_d_t = P1_eval(dataset='class', cleaned=False, n_folds=7)\n    b_d_l, b_d_r, b_d_t = P1_eval(dataset='bias' , cleaned=False, n_folds=7)\n    def sort_good_weight(L, R, L_, R_):\n        wts_acc = []\n        for i in range(1, 100, 1):\n            alpha = i/100\n            L_wt = alpha * L + (1-alpha) * L_\n            R_wt = alpha * R + (1-alpha) * R_\n            wts_acc.append( (alpha, 1-alpha, \n                                 np.round((L_wt < R_wt).mean() * 100,2))\n                              )\n        print(sorted(wts_acc, key=lambda x:x[2], reverse=True)[:5])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:11:21.557736Z","iopub.execute_input":"2022-01-18T05:11:21.558196Z","iopub.status.idle":"2022-01-18T05:11:21.568355Z","shell.execute_reply.started":"2022-01-18T05:11:21.558158Z","shell.execute_reply":"2022-01-18T05:11:21.567549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(np.round((L_1<R_1).mean()*100, 2)) # --> 68.37\n#print(np.round((L_3<R_3).mean()*100, 2)) # --> 68.64\n# sort_good_weight(L_1, R_1, L_3, R_3) --> [(0.47, 0.53, 69.21), (0.48, 0.52, 69.2), (0.49, 0.51, 69.19), (0.39, 0.61, 69.16), (0.4, 0.6, 69.16)]\n# 69.18?","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:11:21.572817Z","iopub.execute_input":"2022-01-18T05:11:21.573021Z","iopub.status.idle":"2022-01-18T05:11:21.578875Z","shell.execute_reply.started":"2022-01-18T05:11:21.572996Z","shell.execute_reply":"2022-01-18T05:11:21.578016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sort_good_weight(L, R, L_, R_):\n    wts_acc = []\n    for i in range(1, 100, 1):\n        alpha = i/100\n        L_wt = alpha * L + (1-alpha) * L_\n        R_wt = alpha * R + (1-alpha) * R_\n        wts_acc.append( (alpha, 1-alpha, \n                             np.round((L_wt < R_wt).mean() * 100,2))\n                          )\n    print(sorted(wts_acc, key=lambda x:x[2], reverse=True)[:5])","metadata":{"execution":{"iopub.status.busy":"2022-01-23T23:40:39.729663Z","iopub.execute_input":"2022-01-23T23:40:39.730401Z","iopub.status.idle":"2022-01-23T23:40:39.74029Z","shell.execute_reply.started":"2022-01-23T23:40:39.730363Z","shell.execute_reply":"2022-01-23T23:40:39.739617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#c_d_l, c_d_r, c_d_t = P1_eval(dataset='class', cleaned=False, n_folds=7, test_only=True)\n#b_d_l, b_d_r, b_d_t = P1_eval(dataset='bias' , cleaned=False, n_folds=7, test_only=True)\n#T_1 = c_d_t.mean(axis=1)\n#T_3 = b_d_t.mean(axis=1)\n\ntest_only = True\nif True:\n    c_d_l, c_d_r, c_d_t = P1_eval(dataset='class', cleaned=False, n_folds=7, test_only=test_only)\n    c_c_l, c_c_r, c_c_t = P1_eval(dataset='class', cleaned=True , n_folds=7, test_only=test_only)\n\n    b_d_l, b_d_r, b_d_t = P1_eval(dataset='bias' , cleaned=False, n_folds=7, test_only=test_only)\n    b_c_l, b_c_r, b_c_t = P1_eval(dataset='bias' , cleaned=True , n_folds=7, test_only=test_only)\n\n    L_1 = c_d_l.mean(axis=1)\n    R_1 = c_d_r.mean(axis=1)\n    L_2 = c_c_l.mean(axis=1)\n    R_2 = c_c_r.mean(axis=1)\n\n    L_3 = b_d_l.mean(axis=1)\n    R_3 = b_d_r.mean(axis=1)\n    L_4 = b_c_l.mean(axis=1)\n    R_4 = b_c_r.mean(axis=1)\n\n    T_1 = c_d_t.mean(axis=1)\n    T_2 = c_c_t.mean(axis=1)\n    T_3 = b_d_t.mean(axis=1)\n    T_4 = b_c_t.mean(axis=1)\n    \n    x = 0.45\n    y = 0.68\n    z = 0.39\n    \n    L_c = x * L_1 + (1-x) * L_2\n    R_c = x * R_1 + (1-x) * R_2\n    L_b = y * L_3 + (1-y) * L_4\n    R_b = y * R_3 + (1-y) * R_4\n    L_y = z * L_c + (1-z) * L_b\n    R_y = z * R_c + (1-z) * R_b\n    \n    T_c = x * T_1 + (1-x) * T_2\n    T_b = y * T_3 + (1-y) * T_4\n    T_y = z * T_c + (1-z) * T_b","metadata":{"execution":{"iopub.status.busy":"2022-01-23T23:40:55.379308Z","iopub.execute_input":"2022-01-23T23:40:55.379643Z","iopub.status.idle":"2022-01-24T00:04:03.275932Z","shell.execute_reply.started":"2022-01-23T23:40:55.379605Z","shell.execute_reply":"2022-01-24T00:04:03.275217Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/fasthugs')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T00:04:03.277618Z","iopub.execute_input":"2022-01-24T00:04:03.27786Z","iopub.status.idle":"2022-01-24T00:04:03.281469Z","shell.execute_reply.started":"2022-01-24T00:04:03.277828Z","shell.execute_reply":"2022-01-24T00:04:03.280714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.text.all import *\nfrom fasthugs.data import TransformersTextBlock, TextGetter\nfrom fasthugs.learner import TransLearner\n\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nimport pandas as pd\nfrom bayes_opt import BayesianOptimization\nimport os\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nSEED=2021","metadata":{"execution":{"iopub.status.busy":"2022-01-24T00:04:03.282797Z","iopub.execute_input":"2022-01-24T00:04:03.283174Z","iopub.status.idle":"2022-01-24T00:04:13.638029Z","shell.execute_reply.started":"2022-01-24T00:04:03.28314Z","shell.execute_reply":"2022-01-24T00:04:13.637295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_model(model_name='distilroberta-base', bs=16, idx=1):\n    df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    df_sub['comment_text'] = df_sub.text\n    df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n    df['all_tox'] = 0\n    model_path = f'../input/roberta-transformers-pytorch/{model_name}'\n\n    dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_path), RegressionBlock(n_out=6)],\n               get_x=TextGetter('comment_text'),\n               get_y=ItemGetter('all_tox'))\n\n    dls = dblock.dataloaders(df, bs=bs, val_bs=bs*1, num_workers=1)\n    p_hdrop = 0.1\n    #learn.fit_one_cycle(10, 1e-5, cbs=[SaveModelCallback(), EarlyStoppingCallback(comp=np.less, patience=3)])\n    preds_all = []\n    for i in range(5):\n        model = AutoModelForSequenceClassification.from_pretrained(f'../input/jrstc-test/{model_name}_df{idx}_{i}', \n                    num_labels=6, hidden_dropout_prob=p_hdrop)\n        metrics = [rmse, R2Score(), PearsonCorrCoef(), SpearmanCorrCoef()]\n        opt_func = Adam\n        learn = TransLearner(dls, model, loss_func=BCEWithLogitsLossFlat(), metrics=metrics, opt_func=opt_func)\n\n        test_dl = dls.test_dl(df_sub)\n        preds_, _ = learn.tta(dl=test_dl, n=1, beta=0)\n        preds_all.append(preds_)\n        #preds_l, _ = learn.get_preds(dl=test_dl, n=1, beta=0)\n        if False:\n            df_sub['comment_text'] = df_val.more_toxic\n            test_dl = dls.test_dl(df_val)\n            preds_r, _ = learn.tta(dl=test_dl, n=1, beta=0)\n            preds_l_all.append(preds_l)\n            preds_r_all.append(preds_r)\n        learn = None\n        gc.collect()\n        torch.cuda.empty_cache()\n    return preds_all","metadata":{"execution":{"iopub.status.busy":"2022-01-24T00:10:21.267188Z","iopub.execute_input":"2022-01-24T00:10:21.268123Z","iopub.status.idle":"2022-01-24T00:10:21.283124Z","shell.execute_reply.started":"2022-01-24T00:10:21.268078Z","shell.execute_reply":"2022-01-24T00:10:21.282303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if True:\n    T_c_1 = valid_model(model_name='distilroberta-base', idx=1, bs=16)\n    T_b_1 = valid_model(model_name='distilroberta-base', idx=2, bs=16)\n\n    T_c_2 = valid_model(model_name='roberta-base', idx=1, bs=4)\n    T_b_2 = valid_model(model_name='roberta-base', idx=2, bs=4)\n\n    T_c_3 = valid_model(model_name='roberta-large', idx=1, bs=4)\n    T_b_3 = valid_model(model_name='roberta-large', idx=2, bs=4)\n    \n    T_c_1 = np.array([ll.numpy() for ll in T_c_1])\n    T_b_1 = np.array([ll.numpy() for ll in T_b_1])\n    T_c_2 = np.array([ll.numpy() for ll in T_c_2])\n    T_b_2 = np.array([ll.numpy() for ll in T_b_2])\n    T_c_3 = np.array([ll.numpy() for ll in T_c_3])\n    T_b_3 = np.array([ll.numpy() for ll in T_b_3])\n    \n    \n    w = np.array([1, 2, 1, 1, 1, 1])\n\n    T_c_1 = np.einsum('ijk, k->j', T_c_1, w)\n    T_b_1 = np.einsum('ijk, k->j', T_b_1, w)\n    T_c_2 = np.einsum('ijk, k->j', T_c_2, w)\n    T_b_2 = np.einsum('ijk, k->j', T_b_2, w)\n    T_c_3 = np.einsum('ijk, k->j', T_c_3, w)\n    T_b_3 = np.einsum('ijk, k->j', T_b_3, w)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:20:54.829958Z","iopub.status.idle":"2022-01-18T05:20:54.830532Z","shell.execute_reply.started":"2022-01-18T05:20:54.83029Z","shell.execute_reply":"2022-01-18T05:20:54.830316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if True:\n    w = np.array([1, 2, 1, 1, 1, 1])\n    L_c_1 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/L_c_1.npy'), w)\n    L_c_2 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/L_c_2.npy'), w)\n    L_c_3 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/L_c_3.npy'), w)\n    L_b_1 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/L_b_1.npy'), w)\n    L_b_2 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/L_b_2.npy'), w)\n    L_b_3 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/L_b_3.npy'), w)\n\n    R_c_1 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/R_c_1.npy'), w)\n    R_c_2 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/R_c_2.npy'), w)\n    R_c_3 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/R_c_3.npy'), w)\n    R_b_1 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/R_b_1.npy'), w)\n    R_b_2 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/R_b_2.npy'), w)\n    R_b_3 = np.einsum('ijk, k->j', np.load('../input/jrstc-test/R_b_3.npy'), w)\n    sort_good_weight(L_b_1, R_b_1, L_c_1, R_c_1) # [(0.57, 0.43000000000000005, 70.49), (0.48, 0.52, 70.48), (0.51, 0.49, 70.48), (0.52, 0.48, 70.48), (0.53, 0.47, 70.48)]\n    sort_good_weight(L_b_2, R_b_2, L_c_2, R_c_2) # [(0.61, 0.39, 70.43), (0.64, 0.36, 70.41), (0.6, 0.4, 70.39), (0.62, 0.38, 70.39), (0.63, 0.37, 70.39)]\n    sort_good_weight(L_b_3, R_b_3, L_c_3, R_c_3) # [(0.09, 0.91, 69.94), (0.08, 0.92, 69.93), (0.1, 0.9, 69.93), (0.25, 0.75, 69.93), (0.07, 0.9299999999999999, 69.92)]\n    x = 0.5\n    L__1 = x * L_b_1 + (1-x) * L_c_1\n    R__1 = x * R_b_1 + (1-x) * R_c_1\n\n    x = 0.6\n    L__2 = x * L_b_2 + (1-x) * L_c_2\n    R__2 = x * R_b_2 + (1-x) * R_c_2\n\n    x = 0.1\n    L__3 = x * L_b_3 + (1-x) * L_c_3\n    R__3 = x * R_b_3 + (1-x) * R_c_3\n    #sort_good_weight(L_1, R_1, L_2, R_2) # [(0.85, 0.15000000000000002, 70.53), (0.86, 0.14, 70.53), (0.76, 0.24, 70.52), (0.77, 0.22999999999999998, 70.52), (0.78, 0.21999999999999997, 70.52)]\n    #sort_good_weight(L_x, R_x, L_3, R_3) # [(0.59, 0.41000000000000003, 70.56), (0.56, 0.43999999999999995, 70.55), (0.57, 0.43000000000000005, 70.55), (0.61, 0.39, 70.55), (0.58, 0.42000000000000004, 70.54)]\n    yy = 0.8\n    L_x = L__1 * yy + (1-yy) * L__2\n    R_x = R__1 * yy + (1-yy) * R__2\n    yy = 0.6\n    L_x = L_x * yy + (1-yy) * L__3\n    R_x = R_x * yy + (1-yy) * R__3","metadata":{"execution":{"iopub.status.busy":"2022-01-24T00:14:30.759694Z","iopub.execute_input":"2022-01-24T00:14:30.759953Z","iopub.status.idle":"2022-01-24T00:14:30.876861Z","shell.execute_reply.started":"2022-01-24T00:14:30.759916Z","shell.execute_reply":"2022-01-24T00:14:30.876144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nx, y, z = 0.5, 0.5, 0.5 \nT_y = z*(x*T_1+(1-x)*T_2) + (1-z)*(y*T_3+(1-y)*T_4)\nx_, y_, z_ = 0.5, 0.5, 0.1\nxx_, yy_ = 0.5, 0.6\nT_x = (xx_*(x_*T_b_1+(1-x_)*(T_c_1))+(1-xx_)*(y_*T_b_2+(1-y_)*(T_c_2))) * yy_ + (1-yy_)*(z_*T_b_3+(1-z_)*(T_c_3))\nll_ = 0.5\nT_ = T_x * ll_ + (1-ll_) * T_y\ndf_sub['score'] = T_\ndf_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\ndf_sub[['comment_id', 'score']].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    x, y, z = 0.45, 0.68, 0.39 \n    T_y = z*(x*T_1+(1-x)*T_2) + (1-z)*(y*T_3+(1-y)*T_4)\n    x_, y_, z_ = 0.5, 0.6, 0.1\n    xx_, yy_ = 0.8, 0.6\n    T_x = (xx_*(x_*T_b_1+(1-x_)*(T_c_1))+(1-xx_)*(y_*T_b_2+(1-y_)*(T_c_2))) * yy_ + (1-yy_)*(z_*T_b_3+(1-z_)*(T_c_3))\n    ll_ = 0.4\n    T_ = T_x * ll_ + (1-ll_) * T_y\n    df_sub['score'] = T_\n    df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n    df_sub[['comment_id', 'score']].head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:20:54.83343Z","iopub.status.idle":"2022-01-18T05:20:54.834003Z","shell.execute_reply.started":"2022-01-18T05:20:54.833768Z","shell.execute_reply":"2022-01-18T05:20:54.833793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    # Predict using pipeline\n    df_sub['score'] = test_preds_arr.mean(axis=1)\n    # Cases with duplicates scores\n\n    df_sub['score'].count() - df_sub['score'].nunique()\n    df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n    df_sub[['comment_id', 'score']].head()","metadata":{"papermill":{"duration":0.080043,"end_time":"2021-12-19T09:37:18.12218","exception":false,"start_time":"2021-12-19T09:37:18.042137","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T05:20:54.835103Z","iopub.status.idle":"2022-01-18T05:20:54.835657Z","shell.execute_reply.started":"2022-01-18T05:20:54.835415Z","shell.execute_reply":"2022-01-18T05:20:54.83544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    wts_acc = []\n    for i in range(30,70,1):\n        for j in range(0,20,1):\n            w1 = i/100\n            w2 = (100 - i - j)/100\n            w3 = (1 - w1 - w2 )\n            p1_wt = w1*p1 + w2*p3 + w3*p5\n            p2_wt = w1*p2 + w2*p4 + w3*p6\n            wts_acc.append( (w1,w2,w3, \n                             np.round((p1_wt < p2_wt).mean() * 100,2))\n                          )\n    sorted(wts_acc, key=lambda x:x[3], reverse=True)[:5]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:20:54.83678Z","iopub.status.idle":"2022-01-18T05:20:54.837431Z","shell.execute_reply.started":"2022-01-18T05:20:54.837147Z","shell.execute_reply":"2022-01-18T05:20:54.837178Z"},"trusted":true},"execution_count":null,"outputs":[]}]}