{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RIDGE","metadata":{}},{"cell_type":"markdown","source":"### Import libs","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport hashlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nimport re\nfrom bs4 import BeautifulSoup\nimport pickle\nimport json\n\nimport scipy\nfrom scipy import sparse","metadata":{"id":"mNtdW_h2gevK","execution":{"iopub.status.busy":"2022-02-01T15:20:31.034382Z","iopub.execute_input":"2022-02-01T15:20:31.034733Z","iopub.status.idle":"2022-02-01T15:20:31.624204Z","shell.execute_reply.started":"2022-02-01T15:20:31.034648Z","shell.execute_reply":"2022-02-01T15:20:31.623451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge","metadata":{"id":"rXOmowDpFWyB","execution":{"iopub.status.busy":"2022-02-01T15:20:31.627705Z","iopub.execute_input":"2022-02-01T15:20:31.627937Z","iopub.status.idle":"2022-02-01T15:20:32.35734Z","shell.execute_reply.started":"2022-02-01T15:20:31.627899Z","shell.execute_reply":"2022-02-01T15:20:32.356621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments_data_path=\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\ncomments_data=pd.read_csv(comments_data_path)\ncomments_data.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:20:32.358719Z","iopub.execute_input":"2022-02-01T15:20:32.358976Z","iopub.status.idle":"2022-02-01T15:20:32.465434Z","shell.execute_reply.started":"2022-02-01T15:20:32.358942Z","shell.execute_reply":"2022-02-01T15:20:32.464758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","metadata":{"id":"fGbAhlXzqbTZ","execution":{"iopub.status.busy":"2022-02-01T15:20:32.466818Z","iopub.execute_input":"2022-02-01T15:20:32.467077Z","iopub.status.idle":"2022-02-01T15:20:32.474274Z","shell.execute_reply.started":"2022-02-01T15:20:32.467044Z","shell.execute_reply":"2022-02-01T15:20:32.473235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\npuncts=string.punctuation\ndef get_extra_features(text, max_len):\n\n  \n\n  count_puncts=0\n  count_upper=0\n  for ch in text:\n    if ch in puncts: count_puncts+=1\n    if ch.upper()==ch: count_upper+=1\n  \n  result=np.asarray([count_puncts/len(text), count_upper/len(text), len(text)/max_len])\n  return result","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:20:32.477278Z","iopub.execute_input":"2022-02-01T15:20:32.47793Z","iopub.status.idle":"2022-02-01T15:20:32.4844Z","shell.execute_reply.started":"2022-02-01T15:20:32.47788Z","shell.execute_reply":"2022-02-01T15:20:32.483619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\n\nnltk_tokenizer = RegexpTokenizer(r'\\w+')\nlemmatizer = WordNetLemmatizer()\n\nswear_data=pd.read_csv(\"../input/ridge-models1/swear-words.csv\", names=[\"word\"])\nswear_data[\"lemma\"]=[lemmatizer.lemmatize(w) for w in swear_data[\"word\"]]\nswear_lemmas=set(swear_data[\"lemma\"])\n\npuncts=string.punctuation\nwhitespaces=string.whitespace\n\n\ndef get_extra_features_with_swear(text, max_len):\n\n  count_puncts=0\n  count_upper=0\n  for ch in text:\n    if ch in puncts: count_puncts+=1\n    if ch.upper()==ch and ch not in whitespaces: count_upper+=1\n\n  words=nltk_tokenizer.tokenize(text.lower())\n  lemmas = [lemmatizer.lemmatize(w) for w in words]\n\n  count_swear=0\n  for lemma in lemmas:\n    if lemma in swear_lemmas:\n      count_swear+=1\n  \n  result=np.asarray([count_puncts/len(text), count_upper/len(text), len(text)/max_len, count_swear/max_len])\n  return result","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:20:32.485846Z","iopub.execute_input":"2022-02-01T15:20:32.486219Z","iopub.status.idle":"2022-02-01T15:20:35.042737Z","shell.execute_reply.started":"2022-02-01T15:20:32.486076Z","shell.execute_reply":"2022-02-01T15:20:35.042025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### prediction RIDGE","metadata":{"id":"mV__Nb0w7Bk9"}},{"cell_type":"code","source":"tqdm.pandas()\n\nmax_len=5000\nextra_features=np.asarray([get_extra_features(text, max_len) for text in comments_data['text']])\nextra_features=sparse.csr_matrix(extra_features)\n\nextra_features_swear=np.asarray([get_extra_features_with_swear(text, max_len) for text in comments_data['text']])\nextra_features_swear=sparse.csr_matrix(extra_features_swear)\n\ncomments_data['text_cleaned'] = comments_data['text'].progress_apply(text_cleaning)","metadata":{"id":"m8IkGo7n5iQz","outputId":"7082dc8d-77cb-4f00-8d69-5d82f97dc553","execution":{"iopub.status.busy":"2022-02-01T15:20:35.043924Z","iopub.execute_input":"2022-02-01T15:20:35.044186Z","iopub.status.idle":"2022-02-01T15:20:41.407932Z","shell.execute_reply.started":"2022-02-01T15:20:35.044155Z","shell.execute_reply":"2022-02-01T15:20:41.407206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path=\"../input/ridge-dataset/val_data_opt_neg1.0_weights_v19\"\n\nnorm_predictions_ridge=None\n\n\nfor num_fold in tqdm([1, 5, 7]):\n\n    with open(f\"{path}/vectorizer_{num_fold}.pickle\", \"rb\") as f:\n        vec=pickle.load(f)\n\n    with open(f\"{path}/ridge_{num_fold}.pickle\", \"rb\") as f:\n        ridge=pickle.load(f)\n\n    with open(f\"{path}/best_params_{num_fold}.json\", \"r\") as f:\n        best_param=json.load(f)\n\n    X_comments = vec.transform(comments_data['text_cleaned'])\n    X_comments = scipy.sparse.hstack([X_comments, extra_features_swear])\n\n    y_comments=ridge.predict(X_comments)\n    y_comments=(y_comments - y_comments.min())/(y_comments.max() - y_comments.min())\n\n    norm_predictions_ridge=y_comments if norm_predictions_ridge is None else np.vstack([norm_predictions_ridge, y_comments])\n\nnorm_predictions_ridge=norm_predictions_ridge.transpose()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:20:41.409442Z","iopub.execute_input":"2022-02-01T15:20:41.409696Z","iopub.status.idle":"2022-02-01T15:20:53.162192Z","shell.execute_reply.started":"2022-02-01T15:20:41.40966Z","shell.execute_reply":"2022-02-01T15:20:53.161484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# prediction regression BERT","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\nimport gc\nimport os\nimport random\nfrom importlib import reload\nimport shutil\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:20:53.163457Z","iopub.execute_input":"2022-02-01T15:20:53.16394Z","iopub.status.idle":"2022-02-01T15:20:53.169664Z","shell.execute_reply.started":"2022-02-01T15:20:53.163885Z","shell.execute_reply":"2022-02-01T15:20:53.168967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:20:53.170871Z","iopub.execute_input":"2022-02-01T15:20:53.171595Z","iopub.status.idle":"2022-02-01T15:20:54.45535Z","shell.execute_reply.started":"2022-02-01T15:20:53.171558Z","shell.execute_reply":"2022-02-01T15:20:54.454609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\nfrom transformers.data.data_collator import default_data_collator, DataCollatorWithPadding\n\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom transformers.modeling_outputs import BaseModelOutput\nfrom transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:20:54.457605Z","iopub.execute_input":"2022-02-01T15:20:54.458116Z","iopub.status.idle":"2022-02-01T15:20:59.974242Z","shell.execute_reply.started":"2022-02-01T15:20:54.458077Z","shell.execute_reply":"2022-02-01T15:20:59.973395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:20:59.975877Z","iopub.execute_input":"2022-02-01T15:20:59.976166Z","iopub.status.idle":"2022-02-01T15:21:00.025881Z","shell.execute_reply.started":"2022-02-01T15:20:59.976128Z","shell.execute_reply":"2022-02-01T15:21:00.024941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(transformers.__version__)\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:21:00.028062Z","iopub.execute_input":"2022-02-01T15:21:00.02865Z","iopub.status.idle":"2022-02-01T15:21:00.034625Z","shell.execute_reply.started":"2022-02-01T15:21:00.028608Z","shell.execute_reply":"2022-02-01T15:21:00.033753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EvalConfig:\n  \n    max_length=128\n    eval_batch_size=32\n    dropout=0.2","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:21:00.038564Z","iopub.execute_input":"2022-02-01T15:21:00.039095Z","iopub.status.idle":"2022-02-01T15:21:00.043399Z","shell.execute_reply.started":"2022-02-01T15:21:00.039058Z","shell.execute_reply":"2022-02-01T15:21:00.042619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n\n    def __init__(self, data, tokenizer, max_len=256, is_test=False):\n        \n        super().__init__()\n\n        self._tokenizer = tokenizer\n        self._data = data\n        self._max_len = max_len\n        self._is_test=is_test\n\n   \n    def __len__(self):\n        return self._data.shape[0]\n\n    def __getitem__(self, idx):\n       \n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        text=self._data.iloc[idx]['text']\n        encoded_text=self._tokenizer(text, \n                                     max_length=self._max_len, \n                                     padding = 'longest',\n                                     truncation=True, \n                                     return_attention_mask=True)\n        \n        encoded_text['input_ids']=torch.tensor(encoded_text['input_ids'])\n        encoded_text['attention_mask']=torch.tensor(encoded_text['attention_mask'])\n\n\n        if not self._is_test : \n            encoded_text['target']=torch.tensor(self._data.iloc[idx]['y'], dtype=torch.float32)\n            \n        return encoded_text","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:21:00.044821Z","iopub.execute_input":"2022-02-01T15:21:00.04523Z","iopub.status.idle":"2022-02-01T15:21:00.056025Z","shell.execute_reply.started":"2022-02-01T15:21:00.045193Z","shell.execute_reply":"2022-02-01T15:21:00.055156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawLinearModel(torch.nn.Module):\n    \n    def __init__(self, bert_path=None,  dropout=0.15,  reinit_last_n_layers=0):\n        \n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(bert_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(bert_path)\n        self.bert_model=AutoModel.from_pretrained(bert_path, config=self.config )\n        self._bertsize=self.bert_model.config.hidden_size\n\n        linear_=torch.nn.Linear(self._bertsize, 1, bias=False)\n        self._regressor = torch.nn.Sequential(torch.nn.Dropout(dropout), linear_)\n\n        self._init_weights(linear_)\n\n        # if reinit_last_n_layers>0:\n        #   for layer in self.bert_model.encoder.layer[-reinit_last_n_layers:]:\n        #     for module in layer.modules():\n        #       if isinstance(module, torch.nn.Linear):\n        #         module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        #         if module.bias is not None:\n        #             module.bias.data.zero_()\n        #       elif isinstance(module, torch.nn.Embedding):\n        #         module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        #         if module.padding_idx is not None:\n        #             module.weight.data[module.padding_idx].zero_()\n        #       elif isinstance(module, torch.nn.LayerNorm):\n        #         module.bias.data.zero_()\n        #         module.weight.data.fill_(1.0)\n\n\n    def _init_weights(self, module):\n        if isinstance(module, torch.nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def save_model(self, path):\n\n      if os.path.exists(path):\n        shutil.rmtree(path)\n        os.mkdir(path)\n      else:\n        os.mkdir(path)\n\n      torch.save(self.state_dict(), f\"{path}/pytorch_model.bin\")\n      self.config.save_pretrained(path)\n      self.tokenizer.save_pretrained(path)\n        \n\n        \n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, target=None, features=None, print_shapes=False):\n        \n       \n        out_bert_= self.bert_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True )\n        if print_shapes: print(f\"===== shapes of layers report ===========\")\n\n        out_=self._regressor(out_bert_.hidden_states[-1][:,0,:]) \n                                 \n        if print_shapes: print(f\"final out_ shape = {out_.shape}\") \n        if print_shapes: print(f\"target out_ shape = {target.shape}\") \n            \n        if print_shapes: print(f\"==================================\")\n\n        result=None\n        if not target is None:\n            loss=torch.nn.MSELoss(reduction='mean')\n            loss_=loss(out_.view(-1), target.view(-1))\n            result=SequenceClassifierOutput(loss=loss_, logits=out_)\n        else:\n            result=SequenceClassifierOutput(logits=out_)\n        return result","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:21:00.057607Z","iopub.execute_input":"2022-02-01T15:21:00.057958Z","iopub.status.idle":"2022-02-01T15:21:00.075378Z","shell.execute_reply.started":"2022-02-01T15:21:00.057908Z","shell.execute_reply":"2022-02-01T15:21:00.074566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel_name=\"model\"\npath=\"../input/subm2-model-toxic-comments-unary-toxic-rober\"\nmodel_paths =[   f\"{path}/htqa_1\",  f\"{path}/htqa_2\",  f\"{path}/htqa_3\"]\nbatch_sizes = [64, 64, 64]\n\n\n\nassert len(model_paths) == len(batch_sizes)\n\nnorm_predictions_bert=None\n\nfor i, model_path in enumerate(model_paths):\n\n    logits=np.array([])\n\n\n    print(f\"Model {model_path}\")\n\n\n    tokenizer=AutoTokenizer.from_pretrained(model_path)\n    collator=DataCollatorWithPadding(tokenizer=tokenizer, max_length=EvalConfig.max_length)\n\n    texts_dataset_=JigsawDataset(data=comments_data, \n                                 tokenizer=tokenizer, \n                                 max_len=EvalConfig.max_length, \n                                 is_test=True)\n\n    texts_dataloader_=DataLoader(dataset=texts_dataset_, \n                                 shuffle=False, \n                                 collate_fn=collator, \n                                 batch_size=batch_sizes[i])\n\n\n\n    eval_model_state =  torch.load(f\"{model_path}/pytorch_model.bin\")\n    \n    transformers.logging.set_verbosity_error()\n    eval_model = JigsawLinearModel(bert_path=model_path, dropout=EvalConfig.dropout) \n    eval_model.load_state_dict(eval_model_state)\n\n\n\n    _=eval_model.cuda()\n    eval_model.eval()\n\n    for eval_batch in tqdm(texts_dataloader_):\n        with torch.no_grad():\n            eval_batch={k:eval_batch[k].to(device) for k in eval_batch}\n            out_=eval_model(**eval_batch)\n\n        logits=np.hstack([logits, out_.logits.view(-1).cpu().numpy()])\n\n    min_logits=np.min(logits)\n    max_logits=np.max(logits)\n    \n    logits = (logits - min_logits)/ (max_logits - min_logits)\n    \n    norm_predictions_bert=logits if norm_predictions_bert is None else np.vstack([norm_predictions_bert, logits])\n\n    del eval_model\n    gc.collect()\n    \nnorm_predictions_bert=norm_predictions_bert.transpose()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:21:00.077782Z","iopub.execute_input":"2022-02-01T15:21:00.078332Z","iopub.status.idle":"2022-02-01T15:23:18.870899Z","shell.execute_reply.started":"2022-02-01T15:21:00.078296Z","shell.execute_reply":"2022-02-01T15:23:18.87019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# prediction att BERT","metadata":{}},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n\n    def __init__(self, data, tokenizer, max_len=256, is_test=False):\n        \n        super().__init__()\n\n        self._tokenizer = tokenizer\n        self._data = data\n        self._max_len = max_len\n        self._is_test=is_test\n\n   \n    def __len__(self):\n        return self._data.shape[0]\n\n    def __getitem__(self, idx):\n       \n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        text=self._data.iloc[idx]['text']\n        encoded_text=self._tokenizer(text, \n                                     max_length=self._max_len, \n                                     padding = 'longest',\n                                     truncation=True, \n                                     return_attention_mask=True)\n        \n        encoded_text['input_ids']=torch.tensor(encoded_text['input_ids'])\n        encoded_text['attention_mask']=torch.tensor(encoded_text['attention_mask'])\n\n\n        if not self._is_test : \n            encoded_text['target']=torch.tensor(self._data.iloc[idx]['y'], dtype=torch.float32)\n            \n        return encoded_text","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:23:18.873081Z","iopub.execute_input":"2022-02-01T15:23:18.873587Z","iopub.status.idle":"2022-02-01T15:23:18.882724Z","shell.execute_reply.started":"2022-02-01T15:23:18.873548Z","shell.execute_reply":"2022-02-01T15:23:18.882041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawLinearModel(torch.nn.Module):\n    \n    def __init__(self, bert_path=None,  dropout=0.15,  reinit_last_n_layers=0):\n        \n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(bert_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(bert_path)\n        self.bert_model=AutoModel.from_pretrained(bert_path, config=self.config )\n        self._bertsize=self.bert_model.config.hidden_size\n\n        linear_att1_=torch.nn.Linear(self._bertsize, 128)\n        linear_att2_=torch.nn.Linear(128, 1)\n        self._attention=torch.nn.Sequential(linear_att1_, torch.nn.Tanh(), linear_att2_, torch.nn.Softmax(dim=1))\n\n        linear_=torch.nn.Linear(self._bertsize, 1, bias=False)\n        self._regressor = torch.nn.Sequential(torch.nn.Dropout(dropout), linear_)\n\n        self._init_weights(linear_)\n        self._init_weights(linear_att1_)\n        self._init_weights(linear_att2_)\n\n        # if reinit_last_n_layers>0:\n        #   for layer in self.bert_model.encoder.layer[-reinit_last_n_layers:]:\n        #     for module in layer.modules():\n        #       if isinstance(module, torch.nn.Linear):\n        #         module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        #         if module.bias is not None:\n        #             module.bias.data.zero_()\n        #       elif isinstance(module, torch.nn.Embedding):\n        #         module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        #         if module.padding_idx is not None:\n        #             module.weight.data[module.padding_idx].zero_()\n        #       elif isinstance(module, torch.nn.LayerNorm):\n        #         module.bias.data.zero_()\n        #         module.weight.data.fill_(1.0)\n\n\n    def _init_weights(self, module):\n        if isinstance(module, torch.nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def save_model(self, path):\n\n      if os.path.exists(path):\n        shutil.rmtree(path)\n        os.mkdir(path)\n      else:\n        os.mkdir(path)\n\n      torch.save(self.state_dict(), f\"{path}/pytorch_model.bin\")\n      self.config.save_pretrained(path)\n      self.tokenizer.save_pretrained(path)\n        \n\n        \n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, target=None, features=None, print_shapes=False):\n        \n       \n        out_bert_= self.bert_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True )\n        if print_shapes: print(f\"===== shapes of layers report ===========\")\n\n        # last_hidden_states=torch.cat([out_bert_.hidden_states[-1][:,0,:], out_bert_.hidden_states[-3][:,0,:], out_bert_.hidden_states[-5][:,0,:]], dim=-1)\n        # if print_shapes: print(f\"final last_hidden_states shape = {last_hidden_states.shape}\") \n\n        last_hidden_states=out_bert_.hidden_states[-1]\n        if print_shapes: print(f\"final last_hidden_states shape = {last_hidden_states.shape}\") \n\n        attention_weights=self._attention(last_hidden_states)\n        if print_shapes: print(f\"final attention_weights shape = {attention_weights.shape}\")\n\n        last_hidden_states_attention = torch.sum(last_hidden_states*attention_weights, dim=1)\n        if print_shapes: print(f\"final last_hidden_states_attention shape = {last_hidden_states_attention.shape}\") \n\n        out_=self._regressor(last_hidden_states_attention) \n                                 \n        if print_shapes: print(f\"final out_ shape = {out_.shape}\") \n        if print_shapes: print(f\"target out_ shape = {target.shape}\") \n            \n        if print_shapes: print(f\"==================================\")\n\n        result=None\n        if not target is None:\n            loss=torch.nn.MSELoss(reduction='mean')\n            loss_=loss(out_.view(-1), target.view(-1))\n            result=SequenceClassifierOutput(loss=loss_, logits=out_)\n        else:\n            result=SequenceClassifierOutput(logits=out_)\n        return result","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:23:18.884061Z","iopub.execute_input":"2022-02-01T15:23:18.884544Z","iopub.status.idle":"2022-02-01T15:23:18.903021Z","shell.execute_reply.started":"2022-02-01T15:23:18.884502Z","shell.execute_reply":"2022-02-01T15:23:18.902139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel_name=\"model\"\npath=\"../input/subm2-model-toxic-comments-unitary-rob-att\"\nmodel_paths=[   f\"{path}/htqa_2\", f\"{path}/htqa_3\" ]\nbatch_sizes = [64, 64]\n\n\n\nassert len(model_paths) == len(batch_sizes)\n\nnorm_predictions_att_bert=None\n\nfor i, model_path in enumerate(model_paths):\n\n    logits=np.array([])\n\n\n    print(f\"Model {model_path}\")\n\n\n    tokenizer=AutoTokenizer.from_pretrained(model_path)\n    collator=DataCollatorWithPadding(tokenizer=tokenizer, max_length=EvalConfig.max_length)\n\n    texts_dataset_=JigsawDataset(data=comments_data, \n                                 tokenizer=tokenizer, \n                                 max_len=EvalConfig.max_length, \n                                 is_test=True)\n\n    texts_dataloader_=DataLoader(dataset=texts_dataset_, \n                                 shuffle=False, \n                                 collate_fn=collator, \n                                 batch_size=batch_sizes[i])\n\n\n\n    eval_model_state =  torch.load(f\"{model_path}/pytorch_model.bin\")\n    \n    transformers.logging.set_verbosity_error()\n    eval_model = JigsawLinearModel(bert_path=model_path, dropout=EvalConfig.dropout) \n    eval_model.load_state_dict(eval_model_state)\n\n\n\n    _=eval_model.cuda()\n    eval_model.eval()\n\n    for eval_batch in tqdm(texts_dataloader_):\n        with torch.no_grad():\n            eval_batch={k:eval_batch[k].to(device) for k in eval_batch}\n            out_=eval_model(**eval_batch)\n\n        logits=np.hstack([logits, out_.logits.view(-1).cpu().numpy()])\n\n    min_logits=np.min(logits)\n    max_logits=np.max(logits)\n    \n    logits = (logits - min_logits)/ (max_logits - min_logits)\n    \n    norm_predictions_att_bert=logits if norm_predictions_att_bert is None else np.vstack([norm_predictions_att_bert, logits])\n\n    del eval_model\n    gc.collect()\n    \nnorm_predictions_att_bert=norm_predictions_att_bert.transpose()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:23:18.906411Z","iopub.execute_input":"2022-02-01T15:23:18.906635Z","iopub.status.idle":"2022-02-01T15:24:47.135345Z","shell.execute_reply.started":"2022-02-01T15:23:18.906595Z","shell.execute_reply":"2022-02-01T15:24:47.134566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# prediction compare BERT","metadata":{"execution":{"iopub.status.busy":"2022-01-31T10:31:07.331045Z","iopub.execute_input":"2022-01-31T10:31:07.331374Z","iopub.status.idle":"2022-01-31T10:31:07.338719Z","shell.execute_reply.started":"2022-01-31T10:31:07.331333Z","shell.execute_reply":"2022-01-31T10:31:07.337469Z"}}},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n\n    def __init__(self, data, tokenizer, max_len=256, is_test=False):\n        \n        super().__init__()\n\n        self._tokenizer = tokenizer\n        self._data = data\n        self._max_len = max_len\n        self._is_test=is_test\n\n   \n    def __len__(self):\n        return self._data.shape[0]\n\n    def __getitem__(self, idx):\n       \n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        text=self._data.iloc[idx]['text']\n        encoded_text=self._tokenizer(text, \n                                     max_length=self._max_len, \n                                     padding = 'longest',\n                                     truncation=True, \n                                     return_attention_mask=True)\n        \n        encoded_text['input_ids']=torch.tensor(encoded_text['input_ids'])\n        encoded_text['attention_mask']=torch.tensor(encoded_text['attention_mask'])\n\n\n        if not self._is_test : \n            encoded_text['target']=torch.tensor(self._data.iloc[idx]['y'], dtype=torch.float32)\n            \n        return encoded_text","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:24:47.136837Z","iopub.execute_input":"2022-02-01T15:24:47.137591Z","iopub.status.idle":"2022-02-01T15:24:47.147906Z","shell.execute_reply.started":"2022-02-01T15:24:47.137542Z","shell.execute_reply":"2022-02-01T15:24:47.14712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawLinearModel(torch.nn.Module):\n    \n    def __init__(self, bert_path=None,  dropout=0.15,  reinit_last_n_layers=0):\n        \n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(bert_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(bert_path)\n        self.bert_model=AutoModel.from_pretrained(bert_path, config=self.config )\n        self._bertsize=self.bert_model.config.hidden_size\n\n        linear_=torch.nn.Linear(self._bertsize, 1)\n        self._regressor = torch.nn.Sequential(torch.nn.Dropout(dropout), linear_)\n\n        self._init_weights(linear_)\n\n        # if reinit_last_n_layers>0:\n        #   for layer in self.bert_model.encoder.layer[-reinit_last_n_layers:]:\n        #     for module in layer.modules():\n        #       if isinstance(module, torch.nn.Linear):\n        #         module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        #         if module.bias is not None:\n        #             module.bias.data.zero_()\n        #       elif isinstance(module, torch.nn.Embedding):\n        #         module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        #         if module.padding_idx is not None:\n        #             module.weight.data[module.padding_idx].zero_()\n        #       elif isinstance(module, torch.nn.LayerNorm):\n        #         module.bias.data.zero_()\n        #         module.weight.data.fill_(1.0)\n\n\n    def _init_weights(self, module):\n        if isinstance(module, torch.nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def save_model(self, path):\n\n      if os.path.exists(path):\n        shutil.rmtree(path)\n        os.mkdir(path)\n      else:\n        os.mkdir(path)\n\n      torch.save(self.state_dict(), f\"{path}/pytorch_model.bin\")\n      self.config.save_pretrained(path)\n      self.tokenizer.save_pretrained(path)\n        \n\n        \n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, target=None, features=None, print_shapes=False):\n        \n       \n        out_bert_= self.bert_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True )\n        if print_shapes: print(f\"===== shapes of layers report ===========\")\n\n        out_=self._regressor(out_bert_.hidden_states[-1][:,0,:]) \n                                 \n        if print_shapes: print(f\"final out_ shape = {out_.shape}\") \n        if print_shapes: print(f\"target out_ shape = {target.shape}\") \n            \n        if print_shapes: print(f\"==================================\")\n\n        result=None\n        if not target is None:\n            loss=torch.nn.MSELoss(reduction='mean')\n            loss_=loss(out_.view(-1), target.view(-1))\n            result=SequenceClassifierOutput(loss=loss_, logits=out_)\n        else:\n            result=SequenceClassifierOutput(logits=out_)\n        return result","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:24:47.151389Z","iopub.execute_input":"2022-02-01T15:24:47.151632Z","iopub.status.idle":"2022-02-01T15:24:47.170484Z","shell.execute_reply.started":"2022-02-01T15:24:47.151603Z","shell.execute_reply":"2022-02-01T15:24:47.169558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel_name=\"model\"\npath=\"../input/subm2-roberta-large-ruddit\"\nmodel_paths=[   f\"{path}/htqa_1\", f\"{path}/htqa_3\" ]\nbatch_sizes = [32, 32]\n\n\npath=\"../input/subm2-model-ruddit-unary-toxic\"\n\nmodel_paths +=[ f\"{path}/htqa_0\", f\"{path}/htqa_1\", f\"{path}/htqa_2\" ]\nbatch_sizes += [64, 64, 64]\n\n\n\nassert len(model_paths) == len(batch_sizes)\n\nnorm_predictions_comp_bert=None\n\nfor i, model_path in enumerate(model_paths):\n\n    logits=np.array([])\n\n\n    print(f\"Model {model_path}\")\n\n\n    tokenizer=AutoTokenizer.from_pretrained(model_path)\n    collator=DataCollatorWithPadding(tokenizer=tokenizer, max_length=EvalConfig.max_length)\n\n    texts_dataset_=JigsawDataset(data=comments_data, \n                                 tokenizer=tokenizer, \n                                 max_len=EvalConfig.max_length, \n                                 is_test=True)\n\n    texts_dataloader_=DataLoader(dataset=texts_dataset_, \n                                 shuffle=False, \n                                 collate_fn=collator, \n                                 batch_size=batch_sizes[i])\n\n\n\n    eval_model_state =  torch.load(f\"{model_path}/pytorch_model.bin\")\n    \n    transformers.logging.set_verbosity_error()\n    eval_model = JigsawLinearModel(bert_path=model_path, dropout=EvalConfig.dropout) \n    eval_model.load_state_dict(eval_model_state)\n\n\n\n    _=eval_model.cuda()\n    eval_model.eval()\n\n    for eval_batch in tqdm(texts_dataloader_):\n        with torch.no_grad():\n            eval_batch={k:eval_batch[k].to(device) for k in eval_batch}\n            out_=eval_model(**eval_batch)\n\n        logits=np.hstack([logits, out_.logits.view(-1).cpu().numpy()])\n\n    min_logits=np.min(logits)\n    max_logits=np.max(logits)\n    \n    logits = (logits - min_logits)/ (max_logits - min_logits)\n    \n    norm_predictions_comp_bert=logits if norm_predictions_comp_bert is None else np.vstack([norm_predictions_comp_bert, logits])\n\n    del eval_model\n    gc.collect()\n    \nnorm_predictions_comp_bert=norm_predictions_comp_bert.transpose()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:24:47.171849Z","iopub.execute_input":"2022-02-01T15:24:47.172866Z","iopub.status.idle":"2022-02-01T15:26:56.063031Z","shell.execute_reply.started":"2022-02-01T15:24:47.172656Z","shell.execute_reply":"2022-02-01T15:26:56.061851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SUBMITION","metadata":{}},{"cell_type":"code","source":"list_predictions=[norm_predictions_ridge, norm_predictions_bert, norm_predictions_comp_bert, norm_predictions_att_bert]","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:33:46.885128Z","iopub.execute_input":"2022-02-01T15:33:46.885453Z","iopub.status.idle":"2022-02-01T15:33:46.892838Z","shell.execute_reply.started":"2022-02-01T15:33:46.885418Z","shell.execute_reply":"2022-02-01T15:33:46.891984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(list_predictions)):\n    if len(list_predictions[i].shape)==1:\n        list_predictions[i]=list_predictions[i].reshape((list_predictions[i].shape[0],1))\n    print(list_predictions[i].shape)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:34:53.397576Z","iopub.execute_input":"2022-02-01T15:34:53.397829Z","iopub.status.idle":"2022-02-01T15:34:53.406363Z","shell.execute_reply.started":"2022-02-01T15:34:53.397798Z","shell.execute_reply":"2022-02-01T15:34:53.405572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=np.hstack(list_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:35:18.478005Z","iopub.execute_input":"2022-02-01T15:35:18.478749Z","iopub.status.idle":"2022-02-01T15:35:18.484261Z","shell.execute_reply.started":"2022-02-01T15:35:18.478713Z","shell.execute_reply":"2022-02-01T15:35:18.483393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments_data['mean']=predictions.mean(axis=1)","metadata":{"id":"0x2ghh-685zV","execution":{"iopub.status.busy":"2022-02-01T15:35:20.339349Z","iopub.execute_input":"2022-02-01T15:35:20.340074Z","iopub.status.idle":"2022-02-01T15:35:20.345524Z","shell.execute_reply.started":"2022-02-01T15:35:20.340028Z","shell.execute_reply":"2022-02-01T15:35:20.344788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments_data['score']=comments_data['mean'].rank(method='first')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:35:21.893622Z","iopub.execute_input":"2022-02-01T15:35:21.894055Z","iopub.status.idle":"2022-02-01T15:35:21.907499Z","shell.execute_reply.started":"2022-02-01T15:35:21.894017Z","shell.execute_reply":"2022-02-01T15:35:21.906767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments_data[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"id":"fsOLXdpR88pf","execution":{"iopub.status.busy":"2022-02-01T15:35:22.151595Z","iopub.execute_input":"2022-02-01T15:35:22.151863Z","iopub.status.idle":"2022-02-01T15:35:22.182359Z","shell.execute_reply.started":"2022-02-01T15:35:22.151835Z","shell.execute_reply":"2022-02-01T15:35:22.181644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments_data[['comment_id', 'score']]","metadata":{"id":"oTsuZPRm9AoO","outputId":"db416655-f4cb-48b0-8041-827d6edf7c02","execution":{"iopub.status.busy":"2022-02-01T15:35:23.067753Z","iopub.execute_input":"2022-02-01T15:35:23.068268Z","iopub.status.idle":"2022-02-01T15:35:23.088737Z","shell.execute_reply.started":"2022-02-01T15:35:23.068225Z","shell.execute_reply":"2022-02-01T15:35:23.087986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"RDQzyOqF9SjT"},"execution_count":null,"outputs":[]}]}