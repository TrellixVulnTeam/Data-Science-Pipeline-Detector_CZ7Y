{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Dec 25 11:37:52 2021\n\n@https://www.kaggle.com/toru59er/0-86-tfidf-ridge-simple-baseline?scriptVersionId=82701355\n\"\"\"\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport re \nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n\nfrom sklearn.linear_model import Ridge\n\ndf_train = pd.read_csv(\"../input/train-csv-for-use/train.csv\")\n# df_test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n# df_test_label = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\").replace(-1,0)\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\ndf_train.head(2)\n\n# Create a score that measure how much toxic is a comment\ncat_mtpl = {'obscene': 18, 'toxic': 41, 'threat': 121, 'insult': 83, 'severe_toxic': 129, 'identity_hate': 157}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\ndf_train_new.head(2)\n\ndf_train = df_train.rename(columns={'comment_text':'text'})\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\ntqdm.pandas()\ndf_train['text'] = df_train['text'].progress_apply(text_cleaning)\n\ndf = df_train.copy()\ndf['y'].value_counts()\n\n##Undersampling\ndf['y'].value_counts(normalize=True)\n\nmin_len = (df['y'] >= 0.1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\ndf = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\ndf['y'].value_counts()\n\n##TF-IDF\nvec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))\nX = vec.fit_transform(df['text'])\nX\n\n##Fit Ridge\nmodel = Ridge(alpha=0.5)\nmodel.fit(X, df['y'])\n\n##Prepare validation data\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ndf_val.head()\n\ntqdm.pandas()\ndf_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\ndf_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)\n\nX_less_toxic = vec.transform(df_val['less_toxic'])\nX_more_toxic = vec.transform(df_val['more_toxic'])\n\np1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)\n\n# Validation Accuracy\n(p1 < p2).mean()\n\n##Prepare submission data\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\ntqdm.pandas()\ndf_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\n\n##Prediction\nX_test = vec.transform(df_sub['text'])\np3 = model.predict(X_test)\n\ndf_sub['score'] = p3\ndf_sub['score'].count()\ndf_sub['score'] = df_sub['score']\n# 9 comments will fail if compared one with the other\ndf_sub['score'].nunique()\n\n##Prepare submission file\ndf_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}