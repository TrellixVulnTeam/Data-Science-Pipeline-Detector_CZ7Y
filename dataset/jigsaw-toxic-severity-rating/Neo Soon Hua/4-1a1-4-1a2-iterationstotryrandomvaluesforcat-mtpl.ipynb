{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Dec 25 11:37:52 2021\n\n@https://www.kaggle.com/toru59er/0-86-tfidf-ridge-simple-baseline?scriptVersionId=82701355\n\"\"\"\n\ntrain_with_30 = False\nnum_iter = 300\nsave_every_how_many_iter = 1\nmany_cat_mtpl = []\nmany_val_accu = []\nfirst_iter = True\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport re \nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n\nfrom sklearn.linear_model import Ridge\nimport random\n\ndf_read = pd.read_csv('../input/train-csv-for-use/train.csv')\n\nfor iter in range(num_iter):\n    if train_with_30:\n        df_train = df_read[:30].copy()\n    else:\n        df_train = df_read.copy()\n    df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    \n    # df_train.head(2)\n    \n    # Create a measure of toxicity of each classification\n    measure_for_toxic = random.randrange(22, 43)\n    \n    cat_mtpl = {'obscene': random.randrange(11, 22), 'toxic': measure_for_toxic, 'threat': random.randrange(121, 181), \n                'insult': random.randrange(44, 85), 'severe_toxic': random.randrange(121, 181), 'identity_hate': random.randrange(121, 181)}\n    many_cat_mtpl.append(cat_mtpl)\n    \n    for category in cat_mtpl:\n        df_train[category] = df_train[category] * cat_mtpl[category]\n    \n    df_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n    \n    df_train['y'] = df_train['score']\n    \n    min_len = (df_train['y'] > 0).sum()  # len of toxic comments\n    df_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\n    df_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\n    df_train_new.head(2)\n    \n    df_train = df_train.rename(columns={'comment_text':'text'})\n    \n    def text_cleaning(text):\n        '''\n        Cleans text into a basic form for NLP. Operations include the following:-\n        1. Remove special charecters like &, #, etc\n        2. Removes extra spaces\n        3. Removes embedded URL links\n        4. Removes HTML tags\n        5. Removes emojis\n        \n        text - Text piece to be cleaned.\n        '''\n        template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n        text = template.sub(r'', text)\n        \n        soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n        only_text = soup.get_text()\n        text = only_text\n        \n        emoji_pattern = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   u\"\\U00002702-\\U000027B0\"\n                                   u\"\\U000024C2-\\U0001F251\"\n                                   \"]+\", flags=re.UNICODE)\n        text = emoji_pattern.sub(r'', text)\n        \n        text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n        text = re.sub(' +', ' ', text) #Remove Extra Spaces\n        text = text.strip() # remove spaces at the beginning and at the end of string\n    \n        return text\n    \n    tqdm.pandas()\n    df_train['text'] = df_train['text'].progress_apply(text_cleaning)\n    \n    df = df_train.copy()\n    df['y'].value_counts()\n    \n    ##Undersampling\n    df['y'].value_counts(normalize=True)\n    \n    min_len = (df['y'] >= 0.1).sum()\n    df_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\n    df = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\n    df['y'].value_counts()\n    \n    ##TF-IDF\n    vec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))\n    X = vec.fit_transform(df['text'])\n    X\n    \n    ##Fit Ridge\n    model = Ridge(alpha=0.5)\n    model.fit(X, df['y'])\n    \n    ##Prepare validation data\n    df_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n    df_val.head()\n    \n    tqdm.pandas()\n    df_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\n    df_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)\n    \n    X_less_toxic = vec.transform(df_val['less_toxic'])\n    X_more_toxic = vec.transform(df_val['more_toxic'])\n    \n    p1 = model.predict(X_less_toxic)\n    p2 = model.predict(X_more_toxic)\n    \n    # Validation Accuracy\n    val_accu = (p1 < p2).mean()\n    many_val_accu.append(val_accu)\n    \n    if first_iter:\n        max_val_accu = val_accu #val_accu for validation accuracy\n        best_cat_mtpl = cat_mtpl\n        best_model = model\n        first_iter = False\n    else:\n        if val_accu > max_val_accu:\n            max_val_accu = val_accu\n            best_cat_mtpl = cat_mtpl\n            best_model = model\n            \n    if (iter%save_every_how_many_iter==0 or iter==num_iter-1):\n        #Save output\n        pd.DataFrame(many_cat_mtpl).to_csv(\"many_cat_mtpl.csv\")\n        pd.DataFrame(many_val_accu).to_csv(\"many_val_accu.csv\")\n        print(\"iter:\", iter)\n        print(\"max_val_accu:\", max_val_accu)\n        print(\"best_cat_mtpl:\", best_cat_mtpl)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}