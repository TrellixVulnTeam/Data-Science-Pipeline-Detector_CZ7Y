{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Training an ensemble of three TF-IDF linear models using the dataset described in this [paper](https://arxiv.org/pdf/2009.10277.pdf)\nOriginal dataset link [here](https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import rankdata","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:35:21.338762Z","iopub.execute_input":"2022-01-21T14:35:21.339359Z","iopub.status.idle":"2022-01-21T14:35:22.483497Z","shell.execute_reply.started":"2022-01-21T14:35:21.339264Z","shell.execute_reply":"2022-01-21T14:35:22.482628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"validation = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\nhspeech = pd.read_csv('../input/measuring-hate-speech/measuring_hate_speech.csv')\nsubmission = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:35:22.485071Z","iopub.execute_input":"2022-01-21T14:35:22.485303Z","iopub.status.idle":"2022-01-21T14:35:27.526798Z","shell.execute_reply.started":"2022-01-21T14:35:22.485276Z","shell.execute_reply":"2022-01-21T14:35:27.525683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get mean scores for each comment_id\nscores_dict = hspeech.groupby('comment_id')['hate_speech_score'].apply(np.mean).to_dict()\n\n# drop duplicate comment_ids\nhspeech = hspeech.drop_duplicates(subset='comment_id')\nhspeech['hate_speech_score'] = hspeech['comment_id'].map(scores_dict)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:35:27.528149Z","iopub.execute_input":"2022-01-21T14:35:27.530333Z","iopub.status.idle":"2022-01-21T14:35:30.050101Z","shell.execute_reply.started":"2022-01-21T14:35:27.528956Z","shell.execute_reply":"2022-01-21T14:35:30.049413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plots","metadata":{}},{"cell_type":"markdown","source":"## Plot Hate Speech Scores Histogram","metadata":{}},{"cell_type":"code","source":"hspeech['hate_speech_score'].plot.hist(bins=100, title='Hate speech scores')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:35:30.051907Z","iopub.execute_input":"2022-01-21T14:35:30.052136Z","iopub.status.idle":"2022-01-21T14:35:30.522073Z","shell.execute_reply.started":"2022-01-21T14:35:30.05211Z","shell.execute_reply":"2022-01-21T14:35:30.520865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compare text size with competition validation data","metadata":{}},{"cell_type":"code","source":"hspeech['text'].apply(lambda x: len(x.split())).plot.hist(bins=100)\npd.concat([\n    validation['less_toxic'],\n    validation['more_toxic']\n]).to_frame('text')['text'].apply(lambda x: len(x.split())).plot.hist(bins=100, alpha=0.5, figsize=(14, 7), title='Text size') #blue validation - orange hspeech data","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:35:30.523595Z","iopub.execute_input":"2022-01-21T14:35:30.524048Z","iopub.status.idle":"2022-01-21T14:35:31.618765Z","shell.execute_reply.started":"2022-01-21T14:35:30.524011Z","shell.execute_reply":"2022-01-21T14:35:31.616106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text cleaning","metadata":{}},{"cell_type":"markdown","source":"The text is already quite clean but some extra pre-processing is added:\n* Remove URL with 'url'\n* Remove unicode strings\n* Remove numbers\n* Lemmatization","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n\nwordnet_lemmatizer = WordNetLemmatizer()\ndef replaceURL(text):\n    \"\"\" Replaces url address with \"url\" \"\"\"\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    return text\n\ndef replaceAbbrev(text):\n    text = re.sub(r\"what's\", \"what is \",text)    \n    text = re.sub(r\"\\'ve\", \" have \",text)\n    text = re.sub(r\"can't\", \"cannot \",text)\n    text = re.sub(r\"n't\", \" not \",text)\n    text = re.sub(r\"i'm\", \"i am \",text)\n    text = re.sub(r\"\\'re\", \" are \",text)\n    text = re.sub(r\"\\'d\", \" would \",text)\n    text = re.sub(r\"\\'ll\", \" will \",text)\n    text = re.sub(r\"\\'scuse\", \" excuse \",text)\n    text = re.sub(r\"\\'s\", \" \",text)\n    return text\n\ndef removeUnicode(text):\n    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r' ', text)       \n    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n    return text\ndef removeRepeatPattern(text):\n    text=re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1',text)\n    text=re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1',text)\n    text=re.sub(r'[ ]{2,}',' ',text)\n    return text\n\ndef replaceAtUser(text):\n    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n    text = re.sub('@[^\\s]+','atUser',text)\n    return text\n\ndef replaceMultiToxicWords(text):\n    text = re.sub(r'(fuckfuck)','fuck fuck ',text)\n    text = re.sub(r'(f+)( *)([u|*|_]+)( *)([c|*|_]+)( *)(k)+','fuck',text)\n    text = re.sub(r'(h+)(a+)(h+)(a+)','ha ha ',text)\n    text = re.sub(r'(s+ *h+ *[i|!]+ *t+)','shit',text)\n    text = re.sub(r'\\b(n+)(i+)(g+)(a+)\\b','nigga',text)\n    text = re.sub(r'\\b(n+)([i|!]+)(g+)(e+)(r+)\\b','nigger',text)\n    text = re.sub(r'\\b(d+)(o+)(u+)(c+)(h+)(e+)( *)(b+)(a+)(g+)\\b','douchebag',text)\n    text = re.sub(r'([a|@][$|s][s|$])','ass',text)\n    text = re.sub(r'(\\bfuk\\b)','fuck',text)\n    return text\n\ndef removeNumbers(text):\n    \"\"\" Removes integers \"\"\"\n    text = re.sub(r\"(^|\\W)\\d+\", \" \", text)\n    text = re.sub(\"5\",\"s\",text)\n    text = re.sub(\"1\",\"i\",text)\n    text = re.sub(\"0\",\"o\",text)\n    return text\n                  \ndef replaceMultiPunc(text):\n    text=re.sub(r'([!])\\1\\1{2,}',r' mxm ',text)\n    text=re.sub(r'([?])\\1\\1{2,}',r' mqm ',text)\n    text=re.sub(r'([*])\\1\\1{2,}',r'*',text)\n    return text\n\n\nreplace_pun = {}\nseparators = set('\"%&\\'()+,-./:;<=>@[\\\\]^_`{|}~')\nfor punc in separators:\n    replace_pun[punc] = ' '\nreplace_pun['&']=' and '\n\ndef my_cleaner(s):\n    #s = s.lower()\n    s=replaceURL(s)\n    s=removeUnicode(s)\n    s=removeNumbers(s)\n    s=replaceAbbrev(s)\n    s=replaceMultiToxicWords(s)\n    s=replaceMultiPunc(s)\n    s=removeRepeatPattern(s)\n    \n    for punc in separators:\n        s= s.replace(punc,replace_pun[punc])                   # remove & replace punctuations\n    tokens = nltk.tokenize.word_tokenize(s)                    # split a string into words (tokens)\n    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n    return ' '.join(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:35:31.62029Z","iopub.execute_input":"2022-01-21T14:35:31.620646Z","iopub.status.idle":"2022-01-21T14:35:32.239761Z","shell.execute_reply.started":"2022-01-21T14:35:31.620611Z","shell.execute_reply":"2022-01-21T14:35:32.238924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clean all texts (train data, validation, and submission)","metadata":{}},{"cell_type":"code","source":"hspeech['text_clean'] = hspeech['text'].apply(my_cleaner)\nvalidation['less_toxic'] = validation['less_toxic'].apply(my_cleaner)\nvalidation['more_toxic'] = validation['more_toxic'].apply(my_cleaner)\nsubmission['text'] = submission['text'].apply(my_cleaner)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:35:32.24107Z","iopub.execute_input":"2022-01-21T14:35:32.24137Z","iopub.status.idle":"2022-01-21T14:37:46.285046Z","shell.execute_reply.started":"2022-01-21T14:35:32.241329Z","shell.execute_reply":"2022-01-21T14:37:46.283989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorize the text with TF-IDF","metadata":{}},{"cell_type":"markdown","source":"The data is vectorized with analyzer 'char_wb'. Datasets such as toxic comments and unbias greatly benefit from this.","metadata":{}},{"cell_type":"code","source":"vec = TfidfVectorizer(lowercase=True, stop_words=['english'], analyzer='char_wb', ngram_range = (3,5))\nX = vec.fit_transform(hspeech['text_clean'])\nx_less_toxic =  vec.transform(validation['less_toxic'])\nx_more_toxic = vec.transform(validation['more_toxic'])\nx_test = vec.transform(submission['text'])","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:37:46.286521Z","iopub.execute_input":"2022-01-21T14:37:46.287522Z","iopub.status.idle":"2022-01-21T14:38:51.444645Z","shell.execute_reply.started":"2022-01-21T14:37:46.287479Z","shell.execute_reply":"2022-01-21T14:38:51.443746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build 3 Ridge Models and build an Ensemble","metadata":{}},{"cell_type":"markdown","source":"Build three  ridges models with varying regularization parameters","metadata":{}},{"cell_type":"code","source":"model_1 = Ridge(alpha=0.5)\nmodel_1.fit(X, hspeech['hate_speech_score'])\nprint(f'Model 1 validation accuracy score:  {(model_1.predict(x_less_toxic) < model_1.predict(x_more_toxic)).mean()}')\n\nmodel_2 = Ridge(alpha=1)\nmodel_2.fit(X, hspeech['hate_speech_score'])\nprint(f'Model 2 validation accuracy score:  {(model_2.predict(x_less_toxic) < model_2.predict(x_more_toxic)).mean()}')\n\n\nmodel_3 = Ridge(alpha=2)\nmodel_3.fit(X, hspeech['hate_speech_score'])\nprint(f'Model 3 validation accuracy score: {(model_3.predict(x_less_toxic) < model_3.predict(x_more_toxic)).mean()}')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:38:51.445911Z","iopub.execute_input":"2022-01-21T14:38:51.446235Z","iopub.status.idle":"2022-01-21T14:39:05.44187Z","shell.execute_reply.started":"2022-01-21T14:38:51.446171Z","shell.execute_reply":"2022-01-21T14:39:05.441174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Average the model scores and submit","metadata":{}},{"cell_type":"code","source":"p1 = model_1.predict(x_test)\np2 = model_2.predict(x_test)\np3 = model_3.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:39:05.443977Z","iopub.execute_input":"2022-01-21T14:39:05.44435Z","iopub.status.idle":"2022-01-21T14:39:05.482358Z","shell.execute_reply.started":"2022-01-21T14:39:05.44432Z","shell.execute_reply":"2022-01-21T14:39:05.481685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['score'] = (p1 + p2 + p3) / 3","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:39:05.483727Z","iopub.execute_input":"2022-01-21T14:39:05.483942Z","iopub.status.idle":"2022-01-21T14:39:05.488278Z","shell.execute_reply.started":"2022-01-21T14:39:05.483916Z","shell.execute_reply":"2022-01-21T14:39:05.487689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy\nfrom scipy.stats import rankdata\n\ntest = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ntest['score'] = rankdata(submission['score'].values, method='ordinal')\ntest[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:39:05.489375Z","iopub.execute_input":"2022-01-21T14:39:05.489733Z","iopub.status.idle":"2022-01-21T14:39:05.570282Z","shell.execute_reply.started":"2022-01-21T14:39:05.489691Z","shell.execute_reply":"2022-01-21T14:39:05.569437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.sort_values('score', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:39:05.571884Z","iopub.execute_input":"2022-01-21T14:39:05.572196Z","iopub.status.idle":"2022-01-21T14:39:05.59159Z","shell.execute_reply.started":"2022-01-21T14:39:05.572154Z","shell.execute_reply":"2022-01-21T14:39:05.59073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#trustyourcv!","metadata":{}}]}