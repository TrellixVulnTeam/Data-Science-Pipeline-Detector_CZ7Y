{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- ver_1 : アンサンブルのウェイトは最適化して実施。","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 注意点\n- それぞれがtest に対する推測だけ行うこと。\n- 自分のセクションの最後ではpred 以外全て削除。gc.collect()を二回走らせること。","metadata":{}},{"cell_type":"code","source":"\nimport pickle\n\n\n# General\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport os\nimport random\nimport gc\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nimport scipy\n\nimport torch\nfrom tqdm.auto import tqdm        \n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-06T10:18:00.126157Z","iopub.execute_input":"2022-02-06T10:18:00.126502Z","iopub.status.idle":"2022-02-06T10:18:02.530568Z","shell.execute_reply.started":"2022-02-06T10:18:00.126411Z","shell.execute_reply":"2022-02-06T10:18:02.529812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:02.532091Z","iopub.execute_input":"2022-02-06T10:18:02.532755Z","iopub.status.idle":"2022-02-06T10:18:02.541374Z","shell.execute_reply.started":"2022-02-06T10:18:02.532723Z","shell.execute_reply":"2022-02-06T10:18:02.540674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# calc_weight","metadata":{}},{"cell_type":"code","source":"# kysky\nkfsky_df = pd.read_csv(\"../input/toxic-kfsky-validaton-result/valid_kfsky.csv\")\n\n# makoto hori\n#hori_df = pd.read_csv(\"../input/toxic-hori-validaton-result/valid_hori (1).csv\")\n\n# kma###\ndrop_cols = [\"Unnamed: 0\",\"cleaning_less\",\"cleaning_more\"]\nkma_df = pd.read_csv(\"../input/kazuma-model5/kazuma_val.csv\")\nkma_df = kma_df.drop(drop_cols,axis = 1)\nkma_df2= pd.read_csv(\"../input/kazuma-model5/valid_external.csv\").loc[:,[\"preds_less_ext_kazuma\",\"preds_more_ext_kazuma\"]]\nkma_df = pd.concat([kma_df,kma_df2],axis = 1)\n\n# mogmog\nmogmog_df = pd.read_csv(\"../input/validaton-result-morita/validation_df_add_pred.csv\")\n\nprint(kfsky_df.shape)\n#print(hori_df.shape)\nprint(kma_df.shape)\nprint(mogmog_df.shape)\n\n# concat\nconcat_df = pd.concat([kfsky_df.iloc[:,1:],\n                       #hori_df.iloc[:,3:],\n                       kma_df.iloc[:,3:],\n                       mogmog_df.iloc[:,3:]\n                      ],axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:04.750041Z","iopub.execute_input":"2022-02-06T10:18:04.750297Z","iopub.status.idle":"2022-02-06T10:18:07.598407Z","shell.execute_reply.started":"2022-02-06T10:18:04.750269Z","shell.execute_reply":"2022-02-06T10:18:07.597654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ユニークな文のみ取り出す。\nless_cs = concat_df.columns[concat_df.columns.str.contains('less')]\nmore_cs = concat_df.columns[concat_df.columns.str.contains('more')]\n\ncolumns_list = ['text'] + [f\"model_{i}\" for i in range(len(less_cs)-1)]\nless_df = concat_df[less_cs]\nmore_df = concat_df[more_cs]\nless_df.columns = columns_list\nmore_df.columns = columns_list\nval_unique_df = pd.concat([less_df, more_df]).drop_duplicates('text').reset_index(drop=True)#.iloc[:,:-1]","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:07.600116Z","iopub.execute_input":"2022-02-06T10:18:07.600335Z","iopub.status.idle":"2022-02-06T10:18:07.674165Z","shell.execute_reply.started":"2022-02-06T10:18:07.600307Z","shell.execute_reply":"2022-02-06T10:18:07.673392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_unique_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:07.675697Z","iopub.execute_input":"2022-02-06T10:18:07.675974Z","iopub.status.idle":"2022-02-06T10:18:07.698411Z","shell.execute_reply.started":"2022-02-06T10:18:07.675938Z","shell.execute_reply":"2022-02-06T10:18:07.697581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# スケーリング揃える\nscaler = MinMaxScaler()\ntmp = scaler.fit_transform(val_unique_df.iloc[:,1:])\nval_unique_df.iloc[:,1:] = tmp","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:07.700681Z","iopub.execute_input":"2022-02-06T10:18:07.700944Z","iopub.status.idle":"2022-02-06T10:18:07.747983Z","shell.execute_reply.started":"2022-02-06T10:18:07.700909Z","shell.execute_reply":"2022-02-06T10:18:07.747217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scoring(pred):\n    _val_unique_df = val_unique_df.copy()\n    _val_unique_df[\"pred\"] = pred\n    val_unique_df_selected = _val_unique_df[[\"text\",\"pred\"]].set_index(\"text\")[\"pred\"].to_dict()\n    \n    concat_df[\"less_toxic_pred\"] = concat_df[\"less_toxic\"].map(val_unique_df_selected)\n    concat_df[\"more_toxic_pred\"] = concat_df[\"more_toxic\"].map(val_unique_df_selected)\n    concat_df[\"correct\"] = (concat_df[\"less_toxic_pred\"]  < concat_df[\"more_toxic_pred\"]).astype(int)\n    # スコア\n    return concat_df[\"correct\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:07.749389Z","iopub.execute_input":"2022-02-06T10:18:07.749909Z","iopub.status.idle":"2022-02-06T10:18:07.757121Z","shell.execute_reply.started":"2022-02-06T10:18:07.74987Z","shell.execute_reply":"2022-02-06T10:18:07.756275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_models = val_unique_df.shape[1]-1\nprint(num_models)\ndef ensemble_score(weight_array): \n    avg_pred = np.sum([val_unique_df.loc[:,f\"model_{i}\"].values*weight_array[i] for i in range(num_models)],axis = 0)\n    return -scoring(avg_pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:07.75865Z","iopub.execute_input":"2022-02-06T10:18:07.758947Z","iopub.status.idle":"2022-02-06T10:18:07.768698Z","shell.execute_reply.started":"2022-02-06T10:18:07.758911Z","shell.execute_reply":"2022-02-06T10:18:07.767789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1,num_models+1):\n    l = concat_df.iloc[:,2*i]\n    #print(l.head())\n    m = concat_df.iloc[:,2*i+1]\n    #print(m.head())\n    print(np.mean((l< m).astype(int)))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:07.770029Z","iopub.execute_input":"2022-02-06T10:18:07.770791Z","iopub.status.idle":"2022-02-06T10:18:07.792083Z","shell.execute_reply.started":"2022-02-06T10:18:07.770751Z","shell.execute_reply":"2022-02-06T10:18:07.791426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 堀さんなしver\nweight_array = np.array([1.0/num_models]*num_models)\nweight_array[-1] = weight_array[-1]/50 # bertの重み初期値を小さくしておく\n\nres = scipy.optimize.minimize(ensemble_score, weight_array, method='powell') \nprint(res.x)\nprint(ensemble_score(res.x))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:07.793435Z","iopub.execute_input":"2022-02-06T10:18:07.793679Z","iopub.status.idle":"2022-02-06T10:18:51.808929Z","shell.execute_reply.started":"2022-02-06T10:18:07.793647Z","shell.execute_reply":"2022-02-06T10:18:51.808014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del (kfsky_df, mogmog_df, kma_df, kma_df2, concat_df,less_df,more_df, val_unique_df)\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:51.810325Z","iopub.execute_input":"2022-02-06T10:18:51.810606Z","iopub.status.idle":"2022-02-06T10:18:52.048858Z","shell.execute_reply.started":"2022-02-06T10:18:51.810566Z","shell.execute_reply":"2022-02-06T10:18:52.048027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 432.8 MB","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# kazuma section","metadata":{}},{"cell_type":"code","source":"comments_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:52.052288Z","iopub.execute_input":"2022-02-06T10:18:52.052782Z","iopub.status.idle":"2022-02-06T10:18:52.165134Z","shell.execute_reply.started":"2022-02-06T10:18:52.05274Z","shell.execute_reply":"2022-02-06T10:18:52.164371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_pkl(file_name, processor):\n    OUTPUT_DIR = './'\n    file_name = os.path.join(OUTPUT_DIR,file_name)\n    pickle.dump(processor,open(file_name, 'wb'))\n    print(\"FINISH\")\ndef load_pkl(file_path):\n    out_object = pickle.load(open(file_path, 'rb'))   \n    return out_object\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n#     text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\ndef ensemble(data, preds, cv_num):\n    pred = np.zeros((data.shape[0],))\n\n    for v in preds:\n      pred += v\n\n    return pred/cv_num\n\ndef multi_predict(test_x, models):\n    preds = []\n    for model in models:\n        pred = model.predict(test_x)\n        preds.append(pred)\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:52.166957Z","iopub.execute_input":"2022-02-06T10:18:52.167375Z","iopub.status.idle":"2022-02-06T10:18:52.178772Z","shell.execute_reply.started":"2022-02-06T10:18:52.167326Z","shell.execute_reply":"2022-02-06T10:18:52.178055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dont need cleaning\next_models = load_pkl('../input/kazuma-model5/ext_models.pkl')\next_tfv = load_pkl('../input/kazuma-model5/ext_tfv.pkl')\n\n# Need cleaning\ntc_models = load_pkl('../input/kazuma-model5/kazuma_models_toxic.pkl')\ntc_processor = load_pkl('../input/kazuma-model5/kazuma_processor.pkl')\nrud_models = load_pkl('../input/kazuma-model5/kazuma_models_ruddit.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:52.18055Z","iopub.execute_input":"2022-02-06T10:18:52.181075Z","iopub.status.idle":"2022-02-06T10:18:53.579624Z","shell.execute_reply.started":"2022-02-06T10:18:52.181035Z","shell.execute_reply":"2022-02-06T10:18:53.578874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\ncomments_df['cleaning'] = comments_df['text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:53.580997Z","iopub.execute_input":"2022-02-06T10:18:53.581233Z","iopub.status.idle":"2022-02-06T10:18:56.235188Z","shell.execute_reply.started":"2022-02-06T10:18:53.581202Z","shell.execute_reply":"2022-02-06T10:18:56.234466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfv_tc = tc_processor['toxc_comment_preprocessor']\ntfv_rud = tc_processor['ruddit_preprocessor']","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:56.236331Z","iopub.execute_input":"2022-02-06T10:18:56.237677Z","iopub.status.idle":"2022-02-06T10:18:56.241777Z","shell.execute_reply.started":"2022-02-06T10:18:56.237635Z","shell.execute_reply":"2022-02-06T10:18:56.240874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfv_col_comments = tfv_tc.transform(comments_df[\"cleaning\"])\ntfv_rud_col_comments = tfv_rud.transform(comments_df[\"cleaning\"])\ntfv_ext_col_comments = ext_tfv.transform(comments_df[\"text\"]) # dont need cleaning","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:56.243401Z","iopub.execute_input":"2022-02-06T10:18:56.243916Z","iopub.status.idle":"2022-02-06T10:19:14.10763Z","shell.execute_reply.started":"2022-02-06T10:18:56.243833Z","shell.execute_reply":"2022-02-06T10:19:14.106832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = multi_predict(tfv_col_comments, tc_models)\npreds_rud = multi_predict(tfv_rud_col_comments, rud_models)\npreds_ext = multi_predict(tfv_ext_col_comments, ext_models)\n\n\npreds_ens = ensemble(tfv_col_comments, preds, 3)\npreds_rud_ens = ensemble(tfv_rud_col_comments, preds_rud, 3)\npreds_ext_ens = ensemble(tfv_ext_col_comments, preds_ext, 3)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:14.108983Z","iopub.execute_input":"2022-02-06T10:19:14.109226Z","iopub.status.idle":"2022-02-06T10:19:14.219735Z","shell.execute_reply.started":"2022-02-06T10:19:14.109193Z","shell.execute_reply":"2022-02-06T10:19:14.218959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub[\"comment_id\"] = comments_df[\"comment_id\"]\nsub[\"kazuma_tc_score\"] = preds_ens\nsub[\"kazuma_rud_score\"] = preds_rud_ens\nsub[\"kazuma_ext_score\"] = preds_ext_ens\nsub","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:14.221161Z","iopub.execute_input":"2022-02-06T10:19:14.221434Z","iopub.status.idle":"2022-02-06T10:19:14.241286Z","shell.execute_reply.started":"2022-02-06T10:19:14.221398Z","shell.execute_reply":"2022-02-06T10:19:14.240604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## gc","metadata":{}},{"cell_type":"code","source":"del tfv_tc\ndel tfv_rud\ndel tfv_col_comments\ndel tfv_rud_col_comments\ndel tfv_ext_col_comments\ndel comments_df\ndel preds_ens\ndel preds_rud_ens\ndel preds_ext_ens\ndel tc_models\ndel rud_models\ndel ext_models\ndel ext_tfv\ndel tc_processor\n\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:14.242476Z","iopub.execute_input":"2022-02-06T10:19:14.242893Z","iopub.status.idle":"2022-02-06T10:19:14.587681Z","shell.execute_reply.started":"2022-02-06T10:19:14.242848Z","shell.execute_reply":"2022-02-06T10:19:14.586983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 511.7 MB","metadata":{}},{"cell_type":"markdown","source":"# fujii section","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom time import time\nfrom contextlib import contextmanager\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\nfrom sklearn.pipeline import Pipeline\nimport re\n\nimport fasttext\nimport fasttext.util\n\nimport os\nimport pickle\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:14.589083Z","iopub.execute_input":"2022-02-06T10:19:14.589335Z","iopub.status.idle":"2022-02-06T10:19:14.63675Z","shell.execute_reply.started":"2022-02-06T10:19:14.5893Z","shell.execute_reply":"2022-02-06T10:19:14.635914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nvalid_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:14.638245Z","iopub.execute_input":"2022-02-06T10:19:14.638771Z","iopub.status.idle":"2022-02-06T10:19:17.037741Z","shell.execute_reply.started":"2022-02-06T10:19:14.63873Z","shell.execute_reply":"2022-02-06T10:19:17.036953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text = train_df[[\"id\", \"comment_text\"]]\n\nvalid_text_l = pd.DataFrame()\nvalid_text_l[\"comment_text\"] = valid_df[\"less_toxic\"]\nvalid_text_l[\"id\"] = \"less\"\n\nvalid_text_m = pd.DataFrame()\nvalid_text_m[\"comment_text\"] = valid_df[\"more_toxic\"]\nvalid_text_m[\"id\"] = \"more\"\n\nvalid_text = pd.concat([valid_text_m, valid_text_l],axis=0)\n\ncheck_df = pd.concat([train_text, valid_text], axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:17.039372Z","iopub.execute_input":"2022-02-06T10:19:17.039907Z","iopub.status.idle":"2022-02-06T10:19:17.074465Z","shell.execute_reply.started":"2022-02-06T10:19:17.039866Z","shell.execute_reply":"2022-02-06T10:19:17.07371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop_id\ndrop_id = check_df[(check_df[\"comment_text\"].duplicated(keep=False))&(check_df[\"id\"]!=\"less\")&(check_df[\"id\"]!=\"more\")][\"id\"].unique()\ntrain_df = train_df[~train_df[\"id\"].isin(drop_id)]","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:17.075593Z","iopub.execute_input":"2022-02-06T10:19:17.075839Z","iopub.status.idle":"2022-02-06T10:19:17.281681Z","shell.execute_reply.started":"2022-02-06T10:19:17.075806Z","shell.execute_reply":"2022-02-06T10:19:17.28092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check\nfrom matplotlib_venn import venn2\nvenn2([set(train_df['comment_text']), set(valid_df['more_toxic'])], set_labels=('train', 'test_more'));","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:17.28288Z","iopub.execute_input":"2022-02-06T10:19:17.283369Z","iopub.status.idle":"2022-02-06T10:19:17.490667Z","shell.execute_reply.started":"2022-02-06T10:19:17.283322Z","shell.execute_reply":"2022-02-06T10:19:17.489459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"venn2([set(train_df['comment_text']), set(valid_df['less_toxic'])], set_labels=('train', 'test_less'));","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:17.492368Z","iopub.execute_input":"2022-02-06T10:19:17.492683Z","iopub.status.idle":"2022-02-06T10:19:17.669786Z","shell.execute_reply.started":"2022-02-06T10:19:17.492645Z","shell.execute_reply":"2022-02-06T10:19:17.668906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del check_df\ndel train_text\ndel valid_text\ndel valid_text_l\ndel valid_text_m\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:17.671333Z","iopub.execute_input":"2022-02-06T10:19:17.671598Z","iopub.status.idle":"2022-02-06T10:19:17.958002Z","shell.execute_reply.started":"2022-02-06T10:19:17.671564Z","shell.execute_reply":"2022-02-06T10:19:17.957309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 学習データのスコアを決定する➟これは調整とのこと\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    train_df[category] = train_df[category] * cat_mtpl[category]\n    \ntrain_df[\"score\"] = train_df.loc[:, \"toxic\": \"identity_hate\"].sum(axis=1)\ntrain_df[\"y\"] = train_df[\"score\"]\n\nmin_len = (train_df[\"y\"]>=0.1).sum()\ny0_undersample = train_df[train_df[\"y\"]==0].sample(n=min_len, random_state=2021)\nnew_train_df = pd.concat([train_df[train_df[\"y\"]>=0.1], y0_undersample])\n\nnew_train_df = new_train_df[[\"id\", \"comment_text\", \"y\"]].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:17.959292Z","iopub.execute_input":"2022-02-06T10:19:17.959772Z","iopub.status.idle":"2022-02-06T10:19:18.02092Z","shell.execute_reply.started":"2022-02-06T10:19:17.959733Z","shell.execute_reply":"2022-02-06T10:19:18.020176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_df\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:18.022321Z","iopub.execute_input":"2022-02-06T10:19:18.02261Z","iopub.status.idle":"2022-02-06T10:19:18.270641Z","shell.execute_reply.started":"2022-02-06T10:19:18.022574Z","shell.execute_reply":"2022-02-06T10:19:18.269796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\nclass BaseBlock(object):\n    def fit(self, input_df, y=None):\n        return self.transform(input_df)\n    \n    def transform(self, input_df):\n        raise NotImplementedError()\n\n        \n##max_featuresを調整することでスコアが改善する可能性\nclass TfidfBlock(BaseBlock):\n    def __init__(self, column: str, whole_df: pd.DataFrame, decomposition: str, n_compose: int):\n        self.column = column\n        self.whole_df = whole_df\n        self.decomposition = decomposition\n        self.n_compose = n_compose\n\n    def fit(self, input_df, y=None):\n        master_df = self.whole_df\n        text = self.whole_df[self.column].fillna(\"\")\n\n        if self.decomposition == \"svd\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        elif self.decomposition == \"NMF\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", NMF(n_components=self.n_compose, random_state=71))\n            ])\n        elif self.decomposition == \"LDA\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", LatentDirichletAllocation(n_components=self.n_compose, random_state=71))\n            ])\n        else:\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        self.pipeline_.fit(text)\n\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        text = input_df[self.column].fillna(\"\")\n        z = self.pipeline_.transform(text)\n\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix(f'{self.column}_tfidf_{self.decomposition}_')\n    \n\nclass StringLengthBlock(BaseBlock):\n    def __init__(self, column):\n        self.column = column\n\n    def transform(self, input_df):\n        output_df = pd.DataFrame()\n        output_df[self.column] = input_df[self.column].str.len()\n        return output_df.add_prefix(\"StringLength_\")\n\n    \nclass CountVectorizerBlock(BaseBlock):\n    def __init__(self, column: str, whole_df: pd.DataFrame, decomposition: str, n_compose: int):\n        self.column = column\n        self.whole_df = whole_df\n        self.decomposition = decomposition\n        self.n_compose = n_compose\n\n    def fit(self, input_df, y=None):\n        master_df = self.whole_df\n        text = self.whole_df[self.column].fillna(\"\")\n\n        if self.decomposition == \"svd\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        elif self.decomposition == \"NMF\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", NMF(n_components=self.n_compose, random_state=71))\n            ])\n        elif self.decomposition == \"LDA\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", LatentDirichletAllocation(n_components=self.n_compose, random_state=71))\n            ])\n        else:\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        self.pipeline_.fit(text)\n\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        text = input_df[self.column].fillna(\"\")\n        z = self.pipeline_.transform(text)\n\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix(f'{self.column}_CountVectorizer_{self.decomposition}_')\n\n    \nclass WordCountBlock(BaseBlock):\n    def __init__(self, column):\n        self.column = column\n\n    def transform(self, input_df):\n        output_df = pd.DataFrame()\n        output_df[self.column] = input_df[self.column].astype(str).map(lambda x: len(x.split()))\n        return output_df.add_prefix(\"WordCount_\")\n    \n\n## FastText\n\n# 今回使用せず\ndef get_text_series(input_df: pd.DataFrame, column: str, sep='&'):\n    out_series = None\n    for i, c in enumerate(column.split(sep)):\n        text_i = input_df[c].astype(str)\n        if out_series is None:\n            out_series = text_i\n        else:\n            out_series = out_series + ' ' + text_i\n    return out_series\n\n# 今回使用せず\ndef create_embedding(document: str, model):\n    words = document.split(\" \")\n    x = [model.get_word_vector(w) for w in words]\n    x = np.max(x, axis=0)\n    return x\n\n\ndef load_fasttext_model():\n    ft = fasttext.load_model(\"../input/fast100/cc.en.100.bin\")\n    ft = fasttext.util.reduce_model(ft, 100)\n\n    return ft\n\n\nclass FasttextEmbeddingBlock(BaseBlock):\n    def __init__(self, column: str):\n        self.column = column\n\n    def fit(self, input_df, y=None, **kwargs):\n        self.ft = load_fasttext_model()\n\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        # この書き方知らなかった。\n        emb = np.stack(input_df['comment_text'].map(lambda x: self.ft.get_sentence_vector(x)).values)\n        output_df = pd.DataFrame(emb)\n        return output_df.add_prefix(f'{self.column}_FastText')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:18.274943Z","iopub.execute_input":"2022-02-06T10:19:18.275148Z","iopub.status.idle":"2022-02-06T10:19:18.311465Z","shell.execute_reply.started":"2022-02-06T10:19:18.275123Z","shell.execute_reply":"2022-02-06T10:19:18.310656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@contextmanager\ndef timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time()\n    yield\n    d = time() - start\n    out_str = format_str.format(d)\n    if logger:\n        logger.info(out_str)\n    else:\n        print(out_str)\n\n\ndef get_function(block, is_train):\n    s = mapping = {\n        True: 'fit',\n        False: 'transform'\n    }.get(is_train)\n    return getattr(block, s)\n\n\ndef to_feature(input_df,\n               blocks,\n               is_train=False):\n    out_df = pd.DataFrame()\n\n    for block in tqdm(blocks, total=len(blocks)):\n        func = get_function(block, is_train)\n\n        with timer(prefix='create ' + str(block) + ' '):\n            _df = func(input_df)\n        assert len(_df) == len(input_df), func.__name__\n        out_df = pd.concat([out_df, _df], axis=1)\n    return reduce_mem_usage(out_df)\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\n              .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:18.313178Z","iopub.execute_input":"2022-02-06T10:19:18.313497Z","iopub.status.idle":"2022-02-06T10:19:18.335225Z","shell.execute_reply.started":"2022-02-06T10:19:18.313458Z","shell.execute_reply":"2022-02-06T10:19:18.334398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.rename(columns = {\"text\": \"comment_text\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:18.337013Z","iopub.execute_input":"2022-02-06T10:19:18.337299Z","iopub.status.idle":"2022-02-06T10:19:18.347131Z","shell.execute_reply.started":"2022-02-06T10:19:18.337261Z","shell.execute_reply":"2022-02-06T10:19:18.346293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\nnew_train_df['comment_text'] = new_train_df['comment_text'].progress_apply(text_cleaning)\ntest_df['comment_text'] = test_df['comment_text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:18.348404Z","iopub.execute_input":"2022-02-06T10:19:18.348851Z","iopub.status.idle":"2022-02-06T10:19:29.07192Z","shell.execute_reply.started":"2022-02-06T10:19:18.348812Z","shell.execute_reply":"2022-02-06T10:19:29.071182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_blocks = [\n    FasttextEmbeddingBlock(\"comment_text\"),\n    TfidfBlock(\"comment_text\" ,whole_df=new_train_df, decomposition=\"svd\", n_compose=500),\n    CountVectorizerBlock(\"comment_text\" ,whole_df=new_train_df, decomposition=\"svd\", n_compose=500),\n    StringLengthBlock(\"comment_text\"),\n    WordCountBlock(\"comment_text\")\n]\n\n\ntrain_x = to_feature(new_train_df, process_blocks, is_train=True)\ntest_x = to_feature(test_df, process_blocks)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:19:29.073229Z","iopub.execute_input":"2022-02-06T10:19:29.073878Z","iopub.status.idle":"2022-02-06T10:23:37.18192Z","shell.execute_reply.started":"2022-02-06T10:19:29.073839Z","shell.execute_reply":"2022-02-06T10:23:37.180391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 13GB超える。\n\n最初から100次元に圧縮したbinary呼ぶことで対応","metadata":{}},{"cell_type":"code","source":"# predict lightGBM\nmodel_lightgbm = []\nfor i in range(5):\n    model = pickle.load(open(f\"../input/kfsky-model1/lightGBM_kfsky_fold{i}\", 'rb'))\n    model_lightgbm.append(model)\n\npred_lightgbm = np.array([model.predict(test_x.values) for model in model_lightgbm])\npred_lightgbm = np.mean(pred_lightgbm, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:23:37.183264Z","iopub.execute_input":"2022-02-06T10:23:37.183542Z","iopub.status.idle":"2022-02-06T10:23:53.127205Z","shell.execute_reply.started":"2022-02-06T10:23:37.183493Z","shell.execute_reply":"2022-02-06T10:23:53.126423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec = pickle.load(open(\"../input/kfsky-model1/Ridge_tfidf\", 'rb'))\n#X = vec.fit_transform(new_train_df['comment_text'])\nX_test = vec.transform(test_df['comment_text'])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:23:53.128852Z","iopub.execute_input":"2022-02-06T10:23:53.129118Z","iopub.status.idle":"2022-02-06T10:24:01.187149Z","shell.execute_reply.started":"2022-02-06T10:23:53.129082Z","shell.execute_reply":"2022-02-06T10:24:01.18638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_model = pickle.load(open(\"../input/kfsky-model1/Ridge_kfsky\", 'rb'))\npred_ridge = ridge_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:25.919805Z","iopub.execute_input":"2022-02-06T10:42:25.920214Z","iopub.status.idle":"2022-02-06T10:42:25.964739Z","shell.execute_reply.started":"2022-02-06T10:42:25.92018Z","shell.execute_reply":"2022-02-06T10:42:25.964052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_ridge","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:30.697869Z","iopub.execute_input":"2022-02-06T10:42:30.698121Z","iopub.status.idle":"2022-02-06T10:42:30.705106Z","shell.execute_reply.started":"2022-02-06T10:42:30.698093Z","shell.execute_reply":"2022-02-06T10:42:30.70442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_kfsky = pd.DataFrame()\nsub_kfsky[\"comment_id\"] = test_df[\"comment_id\"]\nsub_kfsky[\"kfsky_lightgbm\"] = pred_lightgbm\nsub_kfsky[\"kfsky_ridge\"] = pred_ridge\nsub_kfsky.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:33.929134Z","iopub.execute_input":"2022-02-06T10:42:33.929394Z","iopub.status.idle":"2022-02-06T10:42:33.944252Z","shell.execute_reply.started":"2022-02-06T10:42:33.929364Z","shell.execute_reply":"2022-02-06T10:42:33.943386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del new_train_df\ndel test_df\ndel train_x\ndel test_x\ndel X_test\ndel vec\ndel model_lightgbm\ndel pred_lightgbm\ndel ridge_model\ndel pred_ridge\ndel process_blocks\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:42.268324Z","iopub.execute_input":"2022-02-06T10:42:42.268885Z","iopub.status.idle":"2022-02-06T10:42:42.858324Z","shell.execute_reply.started":"2022-02-06T10:42:42.268845Z","shell.execute_reply":"2022-02-06T10:42:42.857533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mogmog section","metadata":{}},{"cell_type":"code","source":"class CFG:\n    debug=False\n    seed=42\n    n_fold = 4\n    model_name = \"../input/roberta-base-edited\"\n    max_len = 256\n    text=\"text_clean2\"\n    target=\"target\"\n    target_size = 1\n    hidden_size = 768\n    fc_dropout = 0.\n    batch_size = 32\n    num_workers = 4\n    model_dir = \"../input/roberta-001-train-jigsaw1\"","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:44.879459Z","iopub.execute_input":"2022-02-06T10:42:44.880217Z","iopub.status.idle":"2022-02-06T10:42:44.887922Z","shell.execute_reply.started":"2022-02-06T10:42:44.880174Z","shell.execute_reply":"2022-02-06T10:42:44.88711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport time\nimport math\nimport random\nimport string\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\n\nfrom gensim import models\nfrom gensim.models import KeyedVectors,FastText\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression,chi2, f_regression\nfrom bs4 import BeautifulSoup\n\nfrom sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\nfrom scipy import sparse\nimport scipy\nimport seaborn as sns\nfrom unicodedata import category, name, normalize\n\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import OneCycleLR\n\n# NLP\nfrom transformers import AutoTokenizer, AutoModel,get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option(\"max_columns\",100)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:45.081316Z","iopub.execute_input":"2022-02-06T10:42:45.081598Z","iopub.status.idle":"2022-02-06T10:42:50.47676Z","shell.execute_reply.started":"2022-02-06T10:42:45.081568Z","shell.execute_reply":"2022-02-06T10:42:50.475981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:50.480189Z","iopub.execute_input":"2022-02-06T10:42:50.480401Z","iopub.status.idle":"2022-02-06T10:42:50.530217Z","shell.execute_reply.started":"2022-02-06T10:42:50.480373Z","shell.execute_reply":"2022-02-06T10:42:50.529232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 余計なスペースの削除\nspaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\ndef remove_space(text):\n    \"\"\"\n    remove extra spaces and ending space if any\n    \"\"\"\n    for space in spaces:\n        text = text.replace(space, ' ')\n    text = text.strip()\n    text = re.sub('\\s+', ' ', text)\n    return text\n\n\nspecial_punc_mappings = {\"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '”': '\"', \"″\": '\"', '“': '\"', '•': '.', '−': '-',\n                         \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','،':'','„':'',\n                         '…': ' ... ', '\\ufeff': ''}\ndef clean_special_punctuations(text):\n    for punc in special_punc_mappings:\n        if punc in text:\n            text = text.replace(punc, special_punc_mappings[punc])\n    #text = remove_diacritics(text)\n    return text\n\n# 数字\ndef clean_number(text):\n    # 正直数字全消しでもいい気がするけど...\n    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n    \n    return text\n\n\n# nbのまま持ってきたけど、意味ない場合もあり。\nrare_words_mapping = {' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA',\n                      'u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA', ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',\n                      ' U.s ': 'USA', ' u.S ': ' USA ', 'fu.k': 'fuck', 'U.K.': 'UK', ' u.k ': ' UK ',\n                      ' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n                      'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n                      'Ra apist': 'Rapist', '2fifth': 'twenty fifth', '2third': 'twenty third',\n                      '2nineth': 'twenty nineth', '2fourth': 'twenty fourth', '#metoo': 'MeToo',\n                      'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n                      'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin',\n                      'fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n                      'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n                      'culturr': 'culture',\n                      'weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n                      'indans': 'indians', 'mastuburate': 'masturbate', 'f**k': 'fuck', 'F**k': 'fuck', 'F**K': 'fuck',\n                      ' u r ': ' you are ', ' u ': ' you ', '操你妈': 'fuck your mother', 'e.g.': 'for example',\n                      'i.e.': 'in other words', '...': '.', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n                      'f***': 'fuck', 'f**': 'fuc', 'F***': 'fuck', 'F**': 'fuc','f*ck':'fuck',\n                      'a****': 'assho', 'a**': 'ass', 'h***': 'hole', 'A****': 'assho', 'A**': 'ass', 'H***': 'hole',\n                      's***': 'shit', 's**': 'shi', 'S***': 'shit', 'S**': 'shi', 'Sh**': 'shit','sh*t':'shit',\n                      'p****': 'pussy', 'p*ssy': 'pussy', 'P****': 'pussy',\n                      'p***': 'porn', 'p*rn': 'porn', 'P***': 'porn',\n                      'st*up*id': 'stupid',\n                      'd***': 'dick', 'di**': 'dick', 'h*ck': 'hack',\n                      'b*tch': 'bitch', 'bi*ch': 'bitch', 'bit*h': 'bitch', 'bitc*': 'bitch', 'b****': 'bitch',\n                      'b***': 'bitc', 'b**': 'bit', 'b*ll': 'bull'\n                      }\n\ndef pre_clean_rare_words(text):\n    for rare_word in rare_words_mapping:\n        if rare_word in text:\n            text = text.replace(rare_word, rare_words_mapping[rare_word])\n    return text\n\n\n\n# de-contract the contraction\ndef decontracted(text):\n    # specific\n    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n\n    # general\n    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)in(\\'|\\’)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\’)s \", \" is \", text)\n    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n    return text\n\n\n\nregular_punct = list(string.punctuation)\nextra_punct = [\n    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\nall_punct = list(set(regular_punct + extra_punct))\n# do not spacing - and .\nall_punct.remove('-')\nall_punct.remove('.')\n#all_punct.remove('!') # !の連続は重要な気がするので、あえて間を開けなくてもいいかな？？？\n\ndef spacing_punctuation(text):\n    \"\"\"\n    add space before and after punctuation and symbols\n    \"\"\"\n    for punc in all_punct:\n        if punc in text:\n            text = text.replace(punc, f' {punc} ')\n    return text\ndef remove_punctuation(text):\n    for punc in all_punct:\n        if punc in text:\n            text = text.replace(punc, ' ')\n    return text\n\n\n\nmis_connect_list = ['(W|w)hat', '(W|w)hy', '(H|h)ow', '(W|w)hich', '(W|w)here', '(W|w)ill']\nmis_connect_re = re.compile('(%s)' % '|'.join(mis_connect_list))\n\nmis_spell_mapping = {'whattsup': 'WhatsApp', 'whatasapp':'WhatsApp', 'whatsupp':'WhatsApp', \n                      'whatcus':'what cause', 'arewhatsapp': 'are WhatsApp', 'Hwhat':'what',\n                      'Whwhat': 'What', 'whatshapp':'WhatsApp', 'howhat':'how that',\n                      # why\n                      'Whybis':'Why is', \n                      # How\n                      \"Howddo\":\"How do\", 'Howeber':'However'}\ndef spacing_some_connect_words(text):\n    \"\"\"\n    'Whyare' -> 'Why are'\n    \"\"\"\n    ori = text\n    for error in mis_spell_mapping:\n        if error in text:\n            text = text.replace(error, mis_spell_mapping[error])\n            \n    # what\n    text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n    text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n    text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n    # why\n    text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n    text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n    # How\n    text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n    text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n    # which\n    text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n    text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n    # where\n    text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n    text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n    # \n    text = mis_connect_re.sub(r\" \\1 \", text)\n    text = text.replace(\"What sApp\", 'WhatsApp')\n    \n    text = remove_space(text)\n    return text\n\n\n\ndef remove_number(text):\n    text = re.sub(r'[0-9]+', '', text)\n    return text\n\n\ndef get_lower(text):\n    text = text.lower()\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:50.531717Z","iopub.execute_input":"2022-02-06T10:42:50.532269Z","iopub.status.idle":"2022-02-06T10:42:50.5739Z","shell.execute_reply.started":"2022-02-06T10:42:50.532229Z","shell.execute_reply":"2022-02-06T10:42:50.573027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/manabendrarout/pytorch-roberta-ranking-baseline-jrstc-train/notebook\ndef text_cleaning_1(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n        ↑これは実施せず。多分しない方がマシなような気がする。たぶん.\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    # 引用nbより\n    text = remove_space(text)\n    text = clean_special_punctuations(text)\n    text = clean_number(text)\n    text = pre_clean_rare_words(text)\n    text = decontracted(text)\n    text = spacing_punctuation(text)\n    text = spacing_some_connect_words(text)\n    text = remove_space(text)\n    \n    # 元々関数\n    #text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    #text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    #text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\ndef text_cleaning_2(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n        ↑これは実施せず。多分しない方がマシなような気がする。たぶん.\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\n\n# https://www.kaggle.com/manabendrarout/pytorch-roberta-ranking-baseline-jrstc-train/notebook\ndef text_cleaning_3(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n        ↑これは実施せず。多分しない方がマシなような気がする。たぶん.\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    # 元々関数\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = remove_number(text) #数字も消す\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n    text = get_lower(text)\n\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:50.577986Z","iopub.execute_input":"2022-02-06T10:42:50.578302Z","iopub.status.idle":"2022-02-06T10:42:50.595363Z","shell.execute_reply.started":"2022-02-06T10:42:50.578271Z","shell.execute_reply":"2022-02-06T10:42:50.594706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_basic_feat(input_df):\n    out_df = pd.DataFrame()\n    # 文字数\n    out_df[\"Num_character\"] = input_df[\"text\"].apply(lambda x:len(x))\n    # 単語数\n    out_df[\"Num_word\"] = input_df[\"text\"].apply(lambda x:len(x.split()))\n    \n    return out_df\n\ndef get_gensim_embed(texts,ndim):\n    # 埋め込み用のarray\n    swem_embedding = np.zeros((len(texts), ndim))\n    \n    # 埋め込み取得\n    for i, text in enumerate(tqdm(texts)):\n        embeddings = [w2v_model.get_vector(word)\n                      if w2v_model.key_to_index.get(word) is not None\n                      else np.zeros(ndim, dtype=np.float32)\n                      for word in text.split()\n                     ]\n        if len(embeddings) > 0:\n            mean_vector = np.mean(np.stack(embeddings), axis=0)\n            swem_embedding[i] = mean_vector\n    return swem_embedding\n\ndef get_fasttext_embed(texts,ndim):\n    # 埋め込み用のarray\n    swem_embedding = np.zeros((len(texts), ndim))\n    \n    # 埋め込み取得\n    for i, text in enumerate(tqdm(texts)):\n        tokens = [word for word in text.split()]\n        if len(tokens)>0:\n            mean_vector = np.mean(fmodel.wv[tokens], axis = 0)\n            #print(mean_vector.shape)\n            swem_embedding[i] = mean_vector\n    return swem_embedding","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:50.598416Z","iopub.execute_input":"2022-02-06T10:42:50.598924Z","iopub.status.idle":"2022-02-06T10:42:50.6103Z","shell.execute_reply.started":"2022-02-06T10:42:50.598895Z","shell.execute_reply":"2022-02-06T10:42:50.609467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_cols = [\n    \"text_clean1\",\n    \"text_clean2\",\n    \"text_clean3\"\n]\ndef get_val_tfidf_dict(vec_dict, val_df_for_pred):\n    out_dict = {}\n    for _name,_vec in tqdm(vec_dict.items()):\n        _col = \"_\".join(_name.split(\"_\")[-2:]) \n        out_dict[_name] = _vec.transform(val_df_for_pred[_col])\n    return out_dict\n\ndef get_val_select_topfeat_and_svd(tfidf_df_val_dict,selector_dict,svd_transfomer_dict):\n    svd_matrix_list = []\n    for i in tqdm(range(6)):\n        _mat = list(tfidf_df_val_dict.values())[i]\n        _selector = list(selector_dict.values())[i]\n        _svd_transformer = list(svd_transfomer_dict.values())[i]\n\n        _matrix_filtered = _selector.transform(_mat)\n        _svd_matrix = _svd_transformer.transform(_matrix_filtered)\n        svd_matrix_list.append(_svd_matrix)\n\n    val_svd_matrix =  np.hstack(svd_matrix_list)\n    return val_svd_matrix","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:50.611497Z","iopub.execute_input":"2022-02-06T10:42:50.611992Z","iopub.status.idle":"2022-02-06T10:42:50.622977Z","shell.execute_reply.started":"2022-02-06T10:42:50.611803Z","shell.execute_reply":"2022-02-06T10:42:50.62216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid(model_dict, df_val_dict):\n    preds_list = []\n    for _name,_model_list in model_dict.items():\n        _preds =[]\n        for i in range(5):\n            pred = _model_list[i].predict(df_val_dict[_name])\n            _preds.append(pred)\n        preds_list.append(np.mean(_preds, axis=0))\n    return preds_list\n\ndef valid2(lgb_model_dict):\n    preds_list = []\n    for i in tqdm(range(3)):\n        _gen = list(gensim_df_val_dict.values())[i]\n        _fast = list(fasttext_df_val_dict.values())[i]\n\n        mat = np.hstack([_gen,\n                         _fast,\n                         basic_feat_val_df[use_basic_feat].values\n                        ])\n\n        _preds =[]\n        _model_list = list(lgb_model_dict.values())[i]\n\n        for j in range(5):\n            pred = _model_list[j].predict(mat)\n            _preds.append(pred)\n        preds_list.append(np.mean(_preds, axis=0))\n    return preds_list","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:50.624266Z","iopub.execute_input":"2022-02-06T10:42:50.624728Z","iopub.status.idle":"2022-02-06T10:42:50.633804Z","shell.execute_reply.started":"2022-02-06T10:42:50.624691Z","shell.execute_reply":"2022-02-06T10:42:50.632977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    \ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\nclass JigsawDataset(Dataset):\n    def __init__(self, CFG, input_df, is_train=True):\n        self.CFG = CFG\n        self.is_train = is_train # 推論時のlabelがない時はFalseにしておく。\n        self.text = input_df[self.CFG.text].values\n        self.tokenizer = AutoTokenizer.from_pretrained(self.CFG.model_name)\n        if self.is_train:\n            self.labels = input_df[self.CFG.target].values       \n             \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        text =  self.text[idx]\n        encoded = self.tokenizer.encode_plus(\n            text,\n            truncation=True,# 最大長で切り捨てを行う。\n            add_special_tokens=True,#「スペシャルトークン」は[CLS][SEP]みたいなやつ\n            max_length=self.CFG.max_len,\n            padding='max_length'\n        )\n        input_ids = torch.tensor(encoded['input_ids'])\n        attention_mask = torch.tensor(encoded['attention_mask'])\n        \n        if self.is_train:\n            #label = torch.tensor(self.labels[idx]).float()\n            label = torch.tensor(self.labels[idx])\n            return input_ids, attention_mask, label\n        return input_ids, attention_mask\n    \nclass JigsawModel(nn.Module):\n    '''\n    あとで隠れ層取り出せるような構造にしておく(可視化or別のモデルに突っ込みたい)。カズマに教えてもらえるかも。\n    - https://www.kaggle.com/yasufuminakama/jigsaw4-luke-base-starter-train/notebook\n    - https://qiita.com/niship2/items/f84751aed893da869cec\n    '''\n    def __init__(self, CFG):\n        super().__init__()\n        self.CFG = CFG\n        self.model = AutoModel.from_pretrained(self.CFG.model_name)\n        self.fc_dropout = nn.Dropout(self.CFG.fc_dropout)\n        self.fc = nn.Linear(self.CFG.hidden_size, self.CFG.target_size)\n    \n    def forward(self, input_ids, attention_mask):\n        out = self.model(input_ids = input_ids, \n                         attention_mask = attention_mask)\n        # outのshapeは[batch, 系列長, 隠れ層次元]　。文章の頭には[CLS]トークンがいるので、それを持ってきている?? あとでちゃんと理解して、工夫する必要あれば実施。\n        # https://www.kaggle.com/debarshichanda/pytorch-w-b-jigsaw-starter\n        # これはいろんな考え方があるので、いじれるはず。\n        # https://www.kaggle.com/yasufuminakama/jigsaw4-luke-base-starter-train/notebook\n        \n        out = self.fc_dropout(out[1])\n        outputs = self.fc(out)\n        return outputs\n\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    \n    for step, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total = len(test_loader)):\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        \n        with torch.no_grad():\n            y_preds = model(input_ids, attention_mask)\n        preds.append(y_preds.to(\"cpu\").numpy())\n        #preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:50.635401Z","iopub.execute_input":"2022-02-06T10:42:50.635668Z","iopub.status.idle":"2022-02-06T10:42:50.655416Z","shell.execute_reply.started":"2022-02-06T10:42:50.635633Z","shell.execute_reply":"2022-02-06T10:42:50.654646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_pkl(file_path):\n    out_object = pickle.load(open(file_path, 'rb'))   \n    return out_object","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:50.658352Z","iopub.execute_input":"2022-02-06T10:42:50.658607Z","iopub.status.idle":"2022-02-06T10:42:50.665012Z","shell.execute_reply.started":"2022-02-06T10:42:50.658583Z","shell.execute_reply":"2022-02-06T10:42:50.664347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pklファイルの読み込み\ntfidf_vec_dict = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/tfidf_vec_dict.pkl\")\ntfidf_vec_dict_ruddit = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/tfidf_vec_dict_ruddit.pkl\")\ntfidf_feat_selector = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/tfidf_feat_selector.pkl\")\ntfidf_svd_transfomer = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/tfidf_svd_transfomer.pkl\")\ntfidf_feat_selector_ruddit = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/tfidf_feat_selector_ruddit.pkl\")\ntfidf_svd_transfomer_ruddit = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/tfidf_svd_transfomer_ruddit.pkl\")\n\nridge_model_dict_1 = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/ridge_model_dict_1.pkl\")\nridge_model_dict_1_ruddit = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/ridge_model_dict_1_ruddit.pkl\")\nlgb_model_dict_2 = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/lgb_model_dict_2.pkl\")\nlgb_model_dict_1_ruddit = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/lgb_model_dict_1_ruddit.pkl\")\nlgb_model_dict_3 = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/lgb_model_dict_3.pkl\")\nlgb_model_dict_2_ruddit = load_pkl(\"../input/jigsaw1st-ruddit-ensemble-addbert-001-savepkl/lgb_model_dict_2_ruddit.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:50.667959Z","iopub.execute_input":"2022-02-06T10:42:50.66826Z","iopub.status.idle":"2022-02-06T10:42:56.695504Z","shell.execute_reply.started":"2022-02-06T10:42:50.668225Z","shell.execute_reply":"2022-02-06T10:42:56.694727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\n# クリーニング\ntqdm.pandas()\ntest_df[\"text_clean1\"] = test_df['text'].progress_apply(text_cleaning_1)\ntest_df[\"text_clean2\"] = test_df['text'].progress_apply(text_cleaning_2)\ntest_df[\"text_clean3\"] = test_df['text'].progress_apply(text_cleaning_3)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:42:56.696918Z","iopub.execute_input":"2022-02-06T10:42:56.697162Z","iopub.status.idle":"2022-02-06T10:43:10.483648Z","shell.execute_reply.started":"2022-02-06T10:42:56.697128Z","shell.execute_reply":"2022-02-06T10:43:10.482874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_df_test_dict = get_val_tfidf_dict(tfidf_vec_dict, test_df)\ntfidf_df_test_dict_ruddit = get_val_tfidf_dict(tfidf_vec_dict_ruddit, test_df)\n\ntest_svd_matrix = get_val_select_topfeat_and_svd(tfidf_df_test_dict,\n                                                tfidf_feat_selector,\n                                                tfidf_svd_transfomer)\ntest_svd_matrix_ruddit = get_val_select_topfeat_and_svd(tfidf_df_test_dict_ruddit,\n                                                       tfidf_feat_selector_ruddit,\n                                                       tfidf_svd_transfomer_ruddit)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:43:10.484709Z","iopub.execute_input":"2022-02-06T10:43:10.485648Z","iopub.status.idle":"2022-02-06T10:43:45.560692Z","shell.execute_reply.started":"2022-02-06T10:43:10.485608Z","shell.execute_reply":"2022-02-06T10:43:45.55993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = KeyedVectors.load(\"../input/gensim-googlenewsvectorsnegative300/GoogleNews-vectors-negative300.gensim\")\ngensim_df_test_dict = {}\nfor _col in tqdm(use_cols):\n    gensim_df_test_dict[f\"gensim_{_col}\"] = get_gensim_embed(test_df[_col],ndim=300)\n    \ndel w2v_model\ngc.collect()\n\nfmodel = FastText.load('../input/jigsaw-regression-based-data/FastText-jigsaw-256D/Jigsaw-Fasttext-Word-Embeddings-256D.bin')\nfasttext_df_test_dict = {}\nfor _col in tqdm(use_cols):\n    fasttext_df_test_dict[f\"fasttext_{_col}\"] = get_fasttext_embed(test_df[_col],ndim=256)\n\ndel fmodel\ngc.collect()\n\nbasic_feat_test_df = make_basic_feat(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:43:45.561989Z","iopub.execute_input":"2022-02-06T10:43:45.562381Z","iopub.status.idle":"2022-02-06T10:45:49.509172Z","shell.execute_reply.started":"2022-02-06T10:43:45.562342Z","shell.execute_reply":"2022-02-06T10:45:49.508427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 推論","metadata":{}},{"cell_type":"code","source":"_preds_list1 = valid(ridge_model_dict_1, tfidf_df_test_dict)\npred1 = np.mean(_preds_list1,axis = 0)\n\n_preds_list2 = valid(ridge_model_dict_1_ruddit, tfidf_df_test_dict_ruddit)\npred2 = np.mean(_preds_list2,axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:45:49.51048Z","iopub.execute_input":"2022-02-06T10:45:49.510934Z","iopub.status.idle":"2022-02-06T10:45:49.771803Z","shell.execute_reply.started":"2022-02-06T10:45:49.510895Z","shell.execute_reply":"2022-02-06T10:45:49.771028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_basic_feat = [\"Num_character\", \"Num_word\"]\npreds_list = []\npreds_list_ruddit = []\nfor i in range(3):\n    _gen = list(gensim_df_test_dict.values())[i]\n    _fast = list(fasttext_df_test_dict.values())[i]\n    \n    mat = np.hstack([_gen,\n                     _fast,\n                     basic_feat_test_df[use_basic_feat].values\n                    ])\n    \n    _preds =[]\n    _preds_ruddit =[]\n    \n    _model_list = list(lgb_model_dict_2.values())[i] \n    _model_list_ruddit = list(lgb_model_dict_1_ruddit.values())[i] \n    \n    for j in range(5):\n        pred = _model_list[j].predict(mat)\n        pred_ruddit = _model_list_ruddit[j].predict(mat)\n        _preds.append(pred)\n        _preds_ruddit.append(pred_ruddit)\n        \n    preds_list.append(np.mean(_preds, axis=0))\n    preds_list_ruddit.append(np.mean(_preds_ruddit, axis=0))\n\npred3 = np.mean(preds_list,axis = 0)\npred4 = np.mean(preds_list_ruddit,axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:45:49.773112Z","iopub.execute_input":"2022-02-06T10:45:49.77345Z","iopub.status.idle":"2022-02-06T10:46:10.095339Z","shell.execute_reply.started":"2022-02-06T10:45:49.773411Z","shell.execute_reply":"2022-02-06T10:46:10.094649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds5 = []\nfor _model in lgb_model_dict_3['TF-IDF(SVD)_600dim']:\n    pred = _model.predict(test_svd_matrix)\n    preds5.append(pred)\npred5 = np.mean(preds5,axis = 0)\n\npreds6 = []\nfor _model in lgb_model_dict_2_ruddit['TF-IDF(SVD)_600dim']:\n    pred = _model.predict(test_svd_matrix_ruddit)\n    preds6.append(pred)\npred6 = np.mean(preds6,axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:46:10.096669Z","iopub.execute_input":"2022-02-06T10:46:10.096926Z","iopub.status.idle":"2022-02-06T10:46:14.092918Z","shell.execute_reply.started":"2022-02-06T10:46:10.096892Z","shell.execute_reply":"2022-02-06T10:46:14.092041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = JigsawDataset(CFG, test_df, is_train = False)\ntest_loader = DataLoader(test_dataset, \n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, \n                         pin_memory=True, \n                         drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:46:14.094315Z","iopub.execute_input":"2022-02-06T10:46:14.094631Z","iopub.status.idle":"2022-02-06T10:46:14.407487Z","shell.execute_reply.started":"2022-02-06T10:46:14.094587Z","shell.execute_reply":"2022-02-06T10:46:14.406729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds7 = []\nfor fold in range(CFG.n_fold):\n    model = JigsawModel(CFG)\n    state = torch.load(CFG.model_dir+f\"/{CFG.model_name.split('/')[-1]}_fold{fold}_best.pth\", map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    preds7.append(prediction)\n    del model, state; gc.collect()\n    torch.cuda.empty_cache()\npred7 = np.mean(preds7,axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:46:14.408884Z","iopub.execute_input":"2022-02-06T10:46:14.40914Z","iopub.status.idle":"2022-02-06T10:51:03.242571Z","shell.execute_reply.started":"2022-02-06T10:46:14.409105Z","shell.execute_reply":"2022-02-06T10:51:03.241766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_dataset, test_loader\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:51:03.244114Z","iopub.execute_input":"2022-02-06T10:51:03.244375Z","iopub.status.idle":"2022-02-06T10:51:03.704419Z","shell.execute_reply.started":"2022-02-06T10:51:03.244334Z","shell.execute_reply":"2022-02-06T10:51:03.703595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\npr1_scaled = scaler.fit_transform(pred1.reshape(-1,1))\npr2_scaled = scaler.fit_transform(pred2.reshape(-1,1))\npr3_scaled = scaler.fit_transform(pred3.reshape(-1,1))\npr4_scaled = scaler.fit_transform(pred4.reshape(-1,1))\npr5_scaled = scaler.fit_transform(pred5.reshape(-1,1))\npr6_scaled = scaler.fit_transform(pred6.reshape(-1,1))\npr7_scaled = scaler.fit_transform(pred7.reshape(-1,1))\n\npred_scaled_mat_test = np.hstack([pr1_scaled,\n                                  pr2_scaled,\n                                  pr3_scaled,\n                                  pr4_scaled,\n                                  pr5_scaled,\n                                  pr6_scaled,\n                                  pr7_scaled\n                            ])\npred_scaled_mat_test = pd.DataFrame(pred_scaled_mat_test,\n                                    columns = [\"ridge\",\"ridge_rud\",\"lgb_emb\",\"lgb_emb_rud\",\"lgb_svd\",\"lgb_svd_rud\",\"roberta\"])\npred_scaled_mat_test = pred_scaled_mat_test.add_prefix(\"mogmog_\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:51:03.705794Z","iopub.execute_input":"2022-02-06T10:51:03.706116Z","iopub.status.idle":"2022-02-06T10:51:03.720369Z","shell.execute_reply.started":"2022-02-06T10:51:03.706077Z","shell.execute_reply":"2022-02-06T10:51:03.71968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mogmog_sub = pd.read_csv('../input/jigsaw-toxic-severity-rating/sample_submission.csv').drop(\"score\",axis = 1)\nmogmog_sub = pd.concat([mogmog_sub,pred_scaled_mat_test],axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:51:03.723023Z","iopub.execute_input":"2022-02-06T10:51:03.723208Z","iopub.status.idle":"2022-02-06T10:51:03.742478Z","shell.execute_reply.started":"2022-02-06T10:51:03.723185Z","shell.execute_reply":"2022-02-06T10:51:03.741859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mogmog_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:51:03.743554Z","iopub.execute_input":"2022-02-06T10:51:03.743866Z","iopub.status.idle":"2022-02-06T10:51:03.759914Z","shell.execute_reply.started":"2022-02-06T10:51:03.74383Z","shell.execute_reply":"2022-02-06T10:51:03.759261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del (tfidf_vec_dict,\n     tfidf_vec_dict_ruddit,\n     tfidf_feat_selector,\n     tfidf_svd_transfomer,\n     tfidf_feat_selector_ruddit,\n     tfidf_svd_transfomer_ruddit,\n     ridge_model_dict_1,\n     ridge_model_dict_1_ruddit,\n     lgb_model_dict_2,\n     lgb_model_dict_1_ruddit,\n     lgb_model_dict_3,\n     lgb_model_dict_2_ruddit,\n     pred_scaled_mat_test,\n     tfidf_df_test_dict,\n     tfidf_df_test_dict_ruddit,\n     test_svd_matrix,\n     test_svd_matrix_ruddit,\n     gensim_df_test_dict,\n     fasttext_df_test_dict\n    )\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:51:03.763578Z","iopub.execute_input":"2022-02-06T10:51:03.764216Z","iopub.status.idle":"2022-02-06T10:51:04.632208Z","shell.execute_reply.started":"2022-02-06T10:51:03.764178Z","shell.execute_reply":"2022-02-06T10:51:04.63141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- あとは重みres.xをかけてやればok。","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# アンサンブル\n- 最適化した重みでサブミット","metadata":{}},{"cell_type":"code","source":"all_sub_df = pd.concat([\n    sub_kfsky,\n    sub.iloc[:,1:],#kma\n    mogmog_sub.iloc[:,1:]#mogmog\n],axis = 1)\n\nall_sub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:51:04.633536Z","iopub.execute_input":"2022-02-06T10:51:04.633779Z","iopub.status.idle":"2022-02-06T10:51:04.655499Z","shell.execute_reply.started":"2022-02-06T10:51:04.633747Z","shell.execute_reply":"2022-02-06T10:51:04.654823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(20,12))\nsns.heatmap(all_sub_df.corr(),square = True,annot = True, fmt=\".1f\",ax = axes[0], vmax = 1, vmin =-1,cmap = \"RdBu\")\naxes[0].set_title(\"corr\")\nsns.heatmap(all_sub_df.corr(method = \"kendall\"),square = True,annot = True, fmt=\".1f\",ax = axes[1], vmax = 1, vmin = -1,cmap = \"RdBu\")\naxes[1].set_title(\"corr(kendall)\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:51:04.656894Z","iopub.execute_input":"2022-02-06T10:51:04.657345Z","iopub.status.idle":"2022-02-06T10:51:07.039083Z","shell.execute_reply.started":"2022-02-06T10:51:04.657307Z","shell.execute_reply":"2022-02-06T10:51:07.03831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- kfsky ridgeが他のモデルと大きく違う","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\nall_sub_df.iloc[:,1:] = pd.DataFrame(scaler.fit_transform(all_sub_df.iloc[:,1:]))\n\nall_sub_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T05:07:18.281693Z","iopub.execute_input":"2022-02-06T05:07:18.28216Z","iopub.status.idle":"2022-02-06T05:07:18.33823Z","shell.execute_reply.started":"2022-02-06T05:07:18.282123Z","shell.execute_reply":"2022-02-06T05:07:18.337452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sub_df[\"score\"] = np.sum(all_sub_df.iloc[:,1:]*res.x,axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T05:07:18.3395Z","iopub.execute_input":"2022-02-06T05:07:18.340432Z","iopub.status.idle":"2022-02-06T05:07:18.348138Z","shell.execute_reply.started":"2022-02-06T05:07:18.340387Z","shell.execute_reply":"2022-02-06T05:07:18.347321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sub_df[\"score\"].hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T05:07:18.349619Z","iopub.execute_input":"2022-02-06T05:07:18.350218Z","iopub.status.idle":"2022-02-06T05:07:18.725311Z","shell.execute_reply.started":"2022-02-06T05:07:18.350173Z","shell.execute_reply":"2022-02-06T05:07:18.724591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sub_df['score'] = all_sub_df['score'].rank(method='first')\nall_sub_df[['comment_id', 'score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T05:07:18.726701Z","iopub.execute_input":"2022-02-06T05:07:18.726968Z","iopub.status.idle":"2022-02-06T05:07:18.756257Z","shell.execute_reply.started":"2022-02-06T05:07:18.726933Z","shell.execute_reply":"2022-02-06T05:07:18.755639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## eqauly_weight\n- CVの最適化が怪しいので、各自のモデルを単純平均して一人1つの結果にした後で、最後単純平均アンサンブルする.","metadata":{}},{"cell_type":"code","source":"# def scale_and_averaging(df):\n#     scaler = MinMaxScaler()\n#     df.iloc[:,1:] = pd.DataFrame(scaler.fit_transform(df.iloc[:,1:]))\n#     return df.iloc[:,1:].mean(axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T05:10:37.519288Z","iopub.execute_input":"2022-02-06T05:10:37.519552Z","iopub.status.idle":"2022-02-06T05:10:37.523584Z","shell.execute_reply.started":"2022-02-06T05:10:37.519521Z","shell.execute_reply":"2022-02-06T05:10:37.522813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kfsky_pred_final = scale_and_averaging(sub_kfsky)\n# kma_pred_final = scale_and_averaging(sub)\n# mogmog_pred_final = scale_and_averaging(mogmog_sub)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T05:10:39.706538Z","iopub.execute_input":"2022-02-06T05:10:39.707107Z","iopub.status.idle":"2022-02-06T05:10:39.710383Z","shell.execute_reply.started":"2022-02-06T05:10:39.707065Z","shell.execute_reply":"2022-02-06T05:10:39.709642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(kfsky_pred_final,alpha = 0.4,bins = 100, label=\"kfsky\")\n# plt.hist(kma_pred_final,alpha = 0.4,bins = 100,label=\"kma\")\n# plt.hist(mogmog_pred_final,alpha = 0.4,bins = 100,label = \"mogmog\")\n# plt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-02-06T05:10:42.180877Z","iopub.execute_input":"2022-02-06T05:10:42.181206Z","iopub.status.idle":"2022-02-06T05:10:42.185255Z","shell.execute_reply.started":"2022-02-06T05:10:42.18117Z","shell.execute_reply":"2022-02-06T05:10:42.184493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/sample_submission.csv\")\n# sub[\"score\"] = (kfsky_pred_final + kma_pred_final + mogmog_pred_final)\n# sub['score'] = sub['score'].rank(method='first')\n# sub[['comment_id', 'score']].to_csv('submission.csv', index=False)\n# sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T05:10:44.831425Z","iopub.execute_input":"2022-02-06T05:10:44.83198Z","iopub.status.idle":"2022-02-06T05:10:44.835349Z","shell.execute_reply.started":"2022-02-06T05:10:44.83194Z","shell.execute_reply":"2022-02-06T05:10:44.834599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}