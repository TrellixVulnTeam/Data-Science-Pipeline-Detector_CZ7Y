{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## lightGBM+Ridge","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom tqdm import tqdm\nfrom time import time\nfrom contextlib import contextmanager\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\nfrom sklearn.pipeline import Pipeline\nimport re\nfrom bs4 import BeautifulSoup\nimport gc\n\nimport fasttext\nimport fasttext.util\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:22.306718Z","iopub.execute_input":"2022-02-06T06:12:22.307209Z","iopub.status.idle":"2022-02-06T06:12:22.591152Z","shell.execute_reply.started":"2022-02-06T06:12:22.307172Z","shell.execute_reply":"2022-02-06T06:12:22.590044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    train_path = \"../input/jigsaw-toxic-comment-classification-challenge/train.csv\"\n    valid_path = \"../input/jigsaw-toxic-severity-rating/validation_data.csv\"\n    test_path = \"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\n    submission_path = \"../input/jigsaw-toxic-severity-rating/sample_submission.csv\"\n    seed = 71\n    fold = 5\n    params = {\n        \"n_estimators\": 20000,\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.01,\n        \"num_leaves\": 1023,\n        \"n_jobs\": -1,\n        \"importance_type\": \"gain\",\n        \"colsample_bytree\": .8,\n        \"colsample_bynode\": .5,\n        \"max_depth\": 7,\n    }","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:22.899385Z","iopub.execute_input":"2022-02-06T06:12:22.899695Z","iopub.status.idle":"2022-02-06T06:12:22.905982Z","shell.execute_reply.started":"2022-02-06T06:12:22.899659Z","shell.execute_reply":"2022-02-06T06:12:22.905164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(CFG.train_path)\nvalid_df = pd.read_csv(CFG.valid_path)\ntest_df = pd.read_csv(CFG.test_path)\nsubmission_df = pd.read_csv(CFG.submission_path)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:23.153031Z","iopub.execute_input":"2022-02-06T06:12:23.153473Z","iopub.status.idle":"2022-02-06T06:12:25.929192Z","shell.execute_reply.started":"2022-02-06T06:12:23.153434Z","shell.execute_reply":"2022-02-06T06:12:25.928206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### drop_duplicates text","metadata":{}},{"cell_type":"code","source":"train_text = train_df[[\"id\", \"comment_text\"]]\n\nvalid_text_l = pd.DataFrame()\nvalid_text_l[\"comment_text\"] = valid_df[\"less_toxic\"]\nvalid_text_l[\"id\"] = \"less\"\n\nvalid_text_m = pd.DataFrame()\nvalid_text_m[\"comment_text\"] = valid_df[\"more_toxic\"]\nvalid_text_m[\"id\"] = \"more\"\n\nvalid_text = pd.concat([valid_text_m, valid_text_l],axis=0)\n\ncheck_df = pd.concat([train_text, valid_text], axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:25.930655Z","iopub.execute_input":"2022-02-06T06:12:25.930903Z","iopub.status.idle":"2022-02-06T06:12:25.982661Z","shell.execute_reply.started":"2022-02-06T06:12:25.930865Z","shell.execute_reply":"2022-02-06T06:12:25.981796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop_id\ndrop_id = check_df[(check_df[\"comment_text\"].duplicated(keep=False))&(check_df[\"id\"]!=\"less\")&(check_df[\"id\"]!=\"more\")][\"id\"].unique()\ntrain_df = train_df[~train_df[\"id\"].isin(drop_id)]","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:25.983713Z","iopub.execute_input":"2022-02-06T06:12:25.983945Z","iopub.status.idle":"2022-02-06T06:12:26.231186Z","shell.execute_reply.started":"2022-02-06T06:12:25.983918Z","shell.execute_reply":"2022-02-06T06:12:26.230315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check\nfrom matplotlib_venn import venn2\nvenn2([set(train_df['comment_text']), set(valid_df['more_toxic'])], set_labels=('train', 'test_more'));","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:26.232941Z","iopub.execute_input":"2022-02-06T06:12:26.233163Z","iopub.status.idle":"2022-02-06T06:12:26.445266Z","shell.execute_reply.started":"2022-02-06T06:12:26.233135Z","shell.execute_reply":"2022-02-06T06:12:26.444381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"venn2([set(train_df['comment_text']), set(valid_df['less_toxic'])], set_labels=('train', 'test_less'));","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:26.44709Z","iopub.execute_input":"2022-02-06T06:12:26.447336Z","iopub.status.idle":"2022-02-06T06:12:26.600997Z","shell.execute_reply.started":"2022-02-06T06:12:26.447307Z","shell.execute_reply":"2022-02-06T06:12:26.600144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del check_df\ndel train_text\ndel valid_text\ndel valid_text_l\ndel valid_text_m\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:26.602454Z","iopub.execute_input":"2022-02-06T06:12:26.60294Z","iopub.status.idle":"2022-02-06T06:12:26.738779Z","shell.execute_reply.started":"2022-02-06T06:12:26.602894Z","shell.execute_reply":"2022-02-06T06:12:26.737901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create y","metadata":{}},{"cell_type":"code","source":"cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    train_df[category] = train_df[category] * cat_mtpl[category]\n    \ntrain_df[\"score\"] = train_df.loc[:, \"toxic\": \"identity_hate\"].sum(axis=1)\ntrain_df[\"y\"] = train_df[\"score\"]\n\nmin_len = (train_df[\"y\"]>=0.1).sum()\ny0_undersample = train_df[train_df[\"y\"]==0].sample(n=min_len, random_state=2021)\nnew_train_df = pd.concat([train_df[train_df[\"y\"]>=0.1], y0_undersample])\n\nnew_train_df = new_train_df[[\"id\", \"comment_text\", \"y\"]].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:28.452381Z","iopub.execute_input":"2022-02-06T06:12:28.452694Z","iopub.status.idle":"2022-02-06T06:12:28.551389Z","shell.execute_reply.started":"2022-02-06T06:12:28.452659Z","shell.execute_reply":"2022-02-06T06:12:28.550326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:29.576176Z","iopub.execute_input":"2022-02-06T06:12:29.577059Z","iopub.status.idle":"2022-02-06T06:12:29.592282Z","shell.execute_reply.started":"2022-02-06T06:12:29.577014Z","shell.execute_reply":"2022-02-06T06:12:29.591634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:29.846607Z","iopub.execute_input":"2022-02-06T06:12:29.847437Z","iopub.status.idle":"2022-02-06T06:12:29.984197Z","shell.execute_reply.started":"2022-02-06T06:12:29.847389Z","shell.execute_reply":"2022-02-06T06:12:29.983209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess","metadata":{}},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:12.914951Z","iopub.execute_input":"2022-02-06T06:12:12.91524Z","iopub.status.idle":"2022-02-06T06:12:12.924069Z","shell.execute_reply.started":"2022-02-06T06:12:12.915206Z","shell.execute_reply":"2022-02-06T06:12:12.923139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaseBlock(object):\n    def fit(self, input_df, y=None):\n        return self.transform(input_df)\n    \n    def transform(self, input_df):\n        raise NotImplementedError()\n\n        \nclass TfidfBlock(BaseBlock):\n    def __init__(self, column: str, whole_df: pd.DataFrame, decomposition: str, n_compose: int):\n        self.column = column\n        self.whole_df = whole_df\n        self.decomposition = decomposition\n        self.n_compose = n_compose\n\n    def fit(self, input_df, y=None):\n        master_df = self.whole_df\n        text = self.whole_df[self.column].fillna(\"\")\n\n        if self.decomposition == \"svd\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        elif self.decomposition == \"NMF\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", NMF(n_components=self.n_compose, random_state=71))\n            ])\n        elif self.decomposition == \"LDA\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", LatentDirichletAllocation(n_components=self.n_compose, random_state=71))\n            ])\n        else:\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        self.pipeline_.fit(text)\n\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        text = input_df[self.column].fillna(\"\")\n        z = self.pipeline_.transform(text)\n\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix(f'{self.column}_tfidf_{self.decomposition}_')\n    \n\nclass StringLengthBlock(BaseBlock):\n    def __init__(self, column):\n        self.column = column\n\n    def transform(self, input_df):\n        output_df = pd.DataFrame()\n        output_df[self.column] = input_df[self.column].str.len()\n        return output_df.add_prefix(\"StringLength_\")\n\n    \nclass CountVectorizerBlock(BaseBlock):\n    def __init__(self, column: str, whole_df: pd.DataFrame, decomposition: str, n_compose: int):\n        self.column = column\n        self.whole_df = whole_df\n        self.decomposition = decomposition\n        self.n_compose = n_compose\n\n    def fit(self, input_df, y=None):\n        master_df = self.whole_df\n        text = self.whole_df[self.column].fillna(\"\")\n\n        if self.decomposition == \"svd\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        elif self.decomposition == \"NMF\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", NMF(n_components=self.n_compose, random_state=71))\n            ])\n        elif self.decomposition == \"LDA\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"NMF\", LatentDirichletAllocation(n_components=self.n_compose, random_state=71))\n            ])\n        else:\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", CountVectorizer(min_df= 3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5), max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        self.pipeline_.fit(text)\n\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        text = input_df[self.column].fillna(\"\")\n        z = self.pipeline_.transform(text)\n\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix(f'{self.column}_CountVectorizer_{self.decomposition}_')\n\n    \nclass WordCountBlock(BaseBlock):\n    def __init__(self, column):\n        self.column = column\n\n    def transform(self, input_df):\n        output_df = pd.DataFrame()\n        output_df[self.column] = input_df[self.column].astype(str).map(lambda x: len(x.split()))\n        return output_df.add_prefix(\"WordCount_\")\n    \n\n## FastText\n\n# No Use\ndef get_text_series(input_df: pd.DataFrame, column: str, sep='&'):\n    out_series = None\n    for i, c in enumerate(column.split(sep)):\n        text_i = input_df[c].astype(str)\n        if out_series is None:\n            out_series = text_i\n        else:\n            out_series = out_series + ' ' + text_i\n    return out_series\n\n# No Use\ndef create_embedding(document: str, model):\n    words = document.split(\" \")\n    x = [model.get_word_vector(w) for w in words]\n    x = np.max(x, axis=0)\n    return x\n\n\ndef load_fasttext_model():\n    ft = fasttext.load_model(\"../input/fasttext-english/cc.en.300.bin\")\n    ft = fasttext.util.reduce_model(ft, 100)\n\n    return ft\n\n\nclass FasttextEmbeddingBlock(BaseBlock):\n    def __init__(self, column: str):\n        self.column = column\n\n    def fit(self, input_df, y=None, **kwargs):\n        self.ft = load_fasttext_model()\n\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        # この書き方知らなかった。\n        emb = np.stack(input_df['comment_text'].map(lambda x: self.ft.get_sentence_vector(x)).values)\n        output_df = pd.DataFrame(emb)\n        return output_df.add_prefix(f'{self.column}_FastText')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:35.896266Z","iopub.execute_input":"2022-02-06T06:12:35.896544Z","iopub.status.idle":"2022-02-06T06:12:35.933951Z","shell.execute_reply.started":"2022-02-06T06:12:35.896514Z","shell.execute_reply":"2022-02-06T06:12:35.933062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@contextmanager\ndef timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time()\n    yield\n    d = time() - start\n    out_str = format_str.format(d)\n    if logger:\n        logger.info(out_str)\n    else:\n        print(out_str)\n\n\ndef get_function(block, is_train):\n    s = mapping = {\n        True: 'fit',\n        False: 'transform'\n    }.get(is_train)\n    return getattr(block, s)\n\n\ndef to_feature(input_df,\n               blocks,\n               is_train=False):\n    out_df = pd.DataFrame()\n\n    for block in tqdm(blocks, total=len(blocks)):\n        func = get_function(block, is_train)\n\n        with timer(prefix='create ' + str(block) + ' '):\n            _df = func(input_df)\n        assert len(_df) == len(input_df), func.__name__\n        out_df = pd.concat([out_df, _df], axis=1)\n    return reduce_mem_usage(out_df)\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\n              .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:37.004403Z","iopub.execute_input":"2022-02-06T06:12:37.004678Z","iopub.status.idle":"2022-02-06T06:12:37.025464Z","shell.execute_reply.started":"2022-02-06T06:12:37.004647Z","shell.execute_reply":"2022-02-06T06:12:37.024479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df_less = valid_df[[\"worker\", \"less_toxic\"]].copy()\nvalid_df_less.rename(columns = {\"less_toxic\": \"comment_text\"}, inplace=True)\n\nvalid_df_more = valid_df[[\"worker\", \"more_toxic\"]].copy()\nvalid_df_more.rename(columns = {\"more_toxic\": \"comment_text\"}, inplace=True)\n\ntest_df.rename(columns = {\"text\": \"comment_text\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:38.490908Z","iopub.execute_input":"2022-02-06T06:12:38.491173Z","iopub.status.idle":"2022-02-06T06:12:38.502924Z","shell.execute_reply.started":"2022-02-06T06:12:38.491143Z","shell.execute_reply":"2022-02-06T06:12:38.502043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\nnew_train_df['comment_text'] = new_train_df['comment_text'].progress_apply(text_cleaning)\nvalid_df_less['comment_text'] = valid_df_less['comment_text'].progress_apply(text_cleaning)\nvalid_df_more['comment_text'] = valid_df_more['comment_text'].progress_apply(text_cleaning)\ntest_df['comment_text'] = test_df['comment_text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:12:40.12595Z","iopub.execute_input":"2022-02-06T06:12:40.126571Z","iopub.status.idle":"2022-02-06T06:13:15.072705Z","shell.execute_reply.started":"2022-02-06T06:12:40.126531Z","shell.execute_reply":"2022-02-06T06:13:15.071715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df_less","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:13:15.075177Z","iopub.execute_input":"2022-02-06T06:13:15.07551Z","iopub.status.idle":"2022-02-06T06:13:15.089351Z","shell.execute_reply.started":"2022-02-06T06:13:15.075465Z","shell.execute_reply":"2022-02-06T06:13:15.088343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_blocks = [\n    FasttextEmbeddingBlock(\"comment_text\"),\n    TfidfBlock(\"comment_text\" ,whole_df=new_train_df, decomposition=\"svd\", n_compose=500),\n    CountVectorizerBlock(\"comment_text\" ,whole_df=new_train_df, decomposition=\"svd\", n_compose=500),\n    StringLengthBlock(\"comment_text\"),\n    WordCountBlock(\"comment_text\")\n]\n\ntrain_y = new_train_df[\"y\"]\ntrain_x = to_feature(new_train_df, process_blocks, is_train=True)\nvalid_less_x = to_feature(valid_df_less, process_blocks)\nvalid_more_x = to_feature(valid_df_more, process_blocks)\ntest_x = to_feature(test_df, process_blocks)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:13:15.090732Z","iopub.execute_input":"2022-02-06T06:13:15.090993Z","iopub.status.idle":"2022-02-06T06:13:15.103198Z","shell.execute_reply.started":"2022-02-06T06:13:15.090952Z","shell.execute_reply":"2022-02-06T06:13:15.10204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:13:15.104759Z","iopub.execute_input":"2022-02-06T06:13:15.1055Z","iopub.status.idle":"2022-02-06T06:13:15.118683Z","shell.execute_reply.started":"2022-02-06T06:13:15.105458Z","shell.execute_reply":"2022-02-06T06:13:15.117999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\nOUTPUT_DIR = './'   \n\nprocess_blocks2 = [\n    #FasttextEmbeddingBlock(\"comment_text\"),\n    TfidfBlock(\"comment_text\" ,whole_df=new_train_df, decomposition=\"svd\", n_compose=600),\n    CountVectorizerBlock(\"comment_text\" ,whole_df=new_train_df, decomposition=\"svd\", n_compose=600),\n    StringLengthBlock(\"comment_text\"),\n    WordCountBlock(\"comment_text\")\n]\n\nfile_name = os.path.join(OUTPUT_DIR,\"test_prrocess\")\npickle.dump(process_blocks2,open(file_name, 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM Model","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\n\ndef get_scores(less_data, more_data, models):\n    \n    scores = []\n    less_scores = []\n    more_scores = []\n    \n    for i ,model in enumerate(models):\n        less_score = model.predict(less_data)\n        more_score = model.predict(more_data)\n        score = (less_score < more_score).mean()\n        \n        less_scores.append(less_score)\n        more_scores.append(more_score)\n        scores.append(score)\n        print(f\">> model_{i} score = {score:.4f}\")\n        \n    final_score = np.mean(scores)\n    print(f\">> total_score = {final_score:.4f}\")\n        \n    return scores, less_scores, more_scores\n\n\ndef fit_lgbm(X, y, cv, params):\n    metrics = mean_squared_error\n    \n    if params is None:\n        params = {}\n        \n    models = []\n    oof_pred = np.zeros_like(y, dtype=np.float)\n    \n    for i, (idx_train, idx_valid) in enumerate(cv):\n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        \n        model = lgb.LGBMRegressor(**params)\n        \n        with timer(prefix=\"fit fold={} \".format(i+1)):\n            model.fit(x_train, y_train,\n                     eval_set=[(x_valid, y_valid)],\n                     early_stopping_rounds=100,\n                     verbose=1000)\n            pickle.dump(model, open(f\"./lightGBM_kfsky_fold{i}\", 'wb'))\n            \n        pred_i = model.predict(x_valid)\n        oof_pred[idx_valid] = pred_i\n        \n        models.append(model)\n        \n        print(f\"Fold {i} RMSE: {metrics(y_valid, pred_i)**0.5: .4f}\")\n    \n    score = metrics(y, oof_pred)**0.5\n    print(\"TRAIN FINISHED | WHOLE RMSE: {:.4f}\".format(score))\n    return oof_pred, models\n\n\ndef run(train_x, train_y, less_data, more_data, test):\n    fold = KFold(n_splits=CFG.fold, shuffle=True, random_state=CFG.seed)\n    cv = list(fold.split(train_x, train_y.reset_index(drop=True)))\n    \n    print(\"==================== Start Training!! ====================\")\n    \n    oof, models = fit_lgbm(train_x.values, train_y.reset_index(drop=True), cv, params=CFG.params)\n    print(\"==================== Validation Score ====================\")\n    scores, less_scores, more_scores = get_scores(less_data.values, more_data.values, models)\n    \n    oof_df_train = pd.DataFrame()\n    oof_df_train[\"oof_train\"] = oof\n    \n    oof_df_valid = pd.DataFrame()\n    oof_df_valid[\"oof_less\"] = np.mean(less_scores, axis=0)\n    oof_df_valid[\"oof_more\"] = np.mean(more_scores, axis=0)\n    \n    print(\"==================== End Training!! ====================\")\n    \n    print(\"==================== Start Predict!! ====================\")\n    \n    preds = []\n    \n    for i, model in enumerate(models):\n        with timer(prefix=\"predict fold={} \".format(i+1)):\n            pred = model.predict(test.values)\n            preds.append(pred)\n            \n    submission_score = np.mean(preds, axis=0)\n    submission_df[\"score\"] = submission_score\n    \n    submission_df.to_csv(\"submission.csv\", index=False)\n    oof_df_train.to_csv(\"oof_train.csv\", index=False)\n    oof_df_valid.to_csv(\"oof_valid.csv\", index=False)\n    \n    print(\"==================== End Predict!! ====================\")\n    \n\nrun(train_x, train_y, valid_less_x, valid_more_x, test_x)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:29:38.135493Z","iopub.execute_input":"2022-01-28T06:29:38.135756Z","iopub.status.idle":"2022-01-28T06:54:39.611876Z","shell.execute_reply.started":"2022-01-28T06:29:38.135723Z","shell.execute_reply":"2022-01-28T06:54:39.610829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:54:39.614134Z","iopub.execute_input":"2022-01-28T06:54:39.614513Z","iopub.status.idle":"2022-01-28T06:54:39.62858Z","shell.execute_reply.started":"2022-01-28T06:54:39.614417Z","shell.execute_reply":"2022-01-28T06:54:39.627479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_lgbm = submission_df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:54:39.630364Z","iopub.execute_input":"2022-01-28T06:54:39.630773Z","iopub.status.idle":"2022-01-28T06:54:39.638241Z","shell.execute_reply.started":"2022-01-28T06:54:39.630725Z","shell.execute_reply":"2022-01-28T06:54:39.637519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_lgbm = pd.read_csv(\"./oof_valid.csv\")\noof_lgbm","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:54:39.639814Z","iopub.execute_input":"2022-01-28T06:54:39.640269Z","iopub.status.idle":"2022-01-28T06:54:39.679849Z","shell.execute_reply.started":"2022-01-28T06:54:39.640221Z","shell.execute_reply":"2022-01-28T06:54:39.678982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ridge Model\nhttps://www.kaggle.com/kengofujii/0-866-tfidf-ridge-simple-baseline/notebook?scriptVersionId=84856895","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:13:19.551651Z","iopub.execute_input":"2022-02-06T06:13:19.552582Z","iopub.status.idle":"2022-02-06T06:13:19.556827Z","shell.execute_reply.started":"2022-02-06T06:13:19.552533Z","shell.execute_reply":"2022-02-06T06:13:19.555869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\nvec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (2,5),max_features=46000)\nX = vec.fit_transform(new_train_df['comment_text'])\npickle.dump(vec, open(f\"./Ridge_tfidf\", 'wb'))\nX","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:14:52.375441Z","iopub.execute_input":"2022-02-06T06:14:52.37622Z","iopub.status.idle":"2022-02-06T06:15:18.737654Z","shell.execute_reply.started":"2022-02-06T06:14:52.376154Z","shell.execute_reply":"2022-02-06T06:15:18.737063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Ridge(alpha=0.5)\n# model = Ridge(alpha=0.485)\nmodel.fit(X, train_y)\npickle.dump(model, open(f\"./Ridge_kfsky\", 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:21:02.898537Z","iopub.execute_input":"2022-02-06T06:21:02.899333Z","iopub.status.idle":"2022-02-06T06:21:06.900917Z","shell.execute_reply.started":"2022-02-06T06:21:02.89928Z","shell.execute_reply":"2022-02-06T06:21:06.899601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df_less","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:55:09.610003Z","iopub.execute_input":"2022-01-28T06:55:09.610592Z","iopub.status.idle":"2022-01-28T06:55:09.631498Z","shell.execute_reply.started":"2022-01-28T06:55:09.610547Z","shell.execute_reply":"2022-01-28T06:55:09.630506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df_more","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:55:09.641047Z","iopub.execute_input":"2022-01-28T06:55:09.644505Z","iopub.status.idle":"2022-01-28T06:55:09.668154Z","shell.execute_reply.started":"2022-01-28T06:55:09.644405Z","shell.execute_reply":"2022-01-28T06:55:09.667208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_less_toxic = vec.transform(valid_df_less['comment_text'])\nX_more_toxic = vec.transform(valid_df_more['comment_text'])\n\np1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:55:09.674067Z","iopub.execute_input":"2022-01-28T06:55:09.676991Z","iopub.status.idle":"2022-01-28T06:56:28.124853Z","shell.execute_reply.started":"2022-01-28T06:55:09.676912Z","shell.execute_reply":"2022-01-28T06:56:28.12399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Validation Accuracy\n(p1 < p2).mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:56:28.126144Z","iopub.execute_input":"2022-01-28T06:56:28.126395Z","iopub.status.idle":"2022-01-28T06:56:28.134373Z","shell.execute_reply.started":"2022-01-28T06:56:28.126367Z","shell.execute_reply":"2022-01-28T06:56:28.133482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_ridge = pd.DataFrame({\n    \"oof_less_ridge\": p1,\n    \"oof_more_ridge\": p2\n})\noof_ridge","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:56:28.135515Z","iopub.execute_input":"2022-01-28T06:56:28.135727Z","iopub.status.idle":"2022-01-28T06:56:28.155522Z","shell.execute_reply.started":"2022-01-28T06:56:28.135701Z","shell.execute_reply":"2022-01-28T06:56:28.154628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_average = pd.concat([oof_lgbm, oof_ridge], axis=1)\noof_average","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:56:28.156794Z","iopub.execute_input":"2022-01-28T06:56:28.157585Z","iopub.status.idle":"2022-01-28T06:56:28.177478Z","shell.execute_reply.started":"2022-01-28T06:56:28.157549Z","shell.execute_reply":"2022-01-28T06:56:28.176642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_ratio = None\nridge_ratio = None\nbest_score = -999\nfor i in range(1, 100):\n    ratio = i/100\n    _less_p = oof_average[\"oof_less\"]*ratio + oof_average[\"oof_less_ridge\"]*(1-ratio)\n    _more_p = oof_average[\"oof_more\"]*ratio + oof_average[\"oof_more_ridge\"]*(1-ratio)\n    \n    score = (_less_p < _more_p).mean()\n    \n    if best_score < score:\n        best_score = score\n        print(\"=\"*30)\n        print(f\"LightGBM:Ridge = {i}:{100-i}\")\n        print(f\"Best_Score is {round(best_score,4)}\")\n        \n        lgbm_ratio = ratio\n        ridge_ratio = 1-ratio\n        \n    else:\n        pass","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:58:46.920107Z","iopub.execute_input":"2022-01-28T06:58:46.920408Z","iopub.status.idle":"2022-01-28T06:58:47.080912Z","shell.execute_reply.started":"2022-01-28T06:58:46.920379Z","shell.execute_reply":"2022-01-28T06:58:47.080008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lgbm_ratio)\nprint(ridge_ratio)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:58:53.861926Z","iopub.execute_input":"2022-01-28T06:58:53.862238Z","iopub.status.idle":"2022-01-28T06:58:53.86702Z","shell.execute_reply.started":"2022-01-28T06:58:53.862207Z","shell.execute_reply":"2022-01-28T06:58:53.866239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8:2がベストっぽい？","metadata":{}},{"cell_type":"markdown","source":"## Submission Data","metadata":{}},{"cell_type":"code","source":"X_test = vec.transform(test_df['comment_text'])\np3 = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:15:22.685802Z","iopub.execute_input":"2022-02-06T06:15:22.686774Z","iopub.status.idle":"2022-02-06T06:15:33.005568Z","shell.execute_reply.started":"2022-02-06T06:15:22.68672Z","shell.execute_reply":"2022-02-06T06:15:33.004509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p3","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:15:33.00728Z","iopub.execute_input":"2022-02-06T06:15:33.007527Z","iopub.status.idle":"2022-02-06T06:15:33.014465Z","shell.execute_reply.started":"2022-02-06T06:15:33.007497Z","shell.execute_reply":"2022-02-06T06:15:33.013586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_model = pickle.load(open(\"./Ridge_kfsky\", 'rb'))\n_p3 = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:21:45.50925Z","iopub.execute_input":"2022-02-06T06:21:45.509756Z","iopub.status.idle":"2022-02-06T06:21:45.525442Z","shell.execute_reply.started":"2022-02-06T06:21:45.509717Z","shell.execute_reply":"2022-02-06T06:21:45.524555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_p3","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:21:48.517368Z","iopub.execute_input":"2022-02-06T06:21:48.517638Z","iopub.status.idle":"2022-02-06T06:21:48.522806Z","shell.execute_reply.started":"2022-02-06T06:21:48.517609Z","shell.execute_reply":"2022-02-06T06:21:48.522245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_pred = sub_lgbm[\"score\"]*lgbm_ratio + p3*ridge_ratio","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:59:14.447799Z","iopub.execute_input":"2022-01-28T06:59:14.44827Z","iopub.status.idle":"2022-01-28T06:59:14.45355Z","shell.execute_reply.started":"2022-01-28T06:59:14.448217Z","shell.execute_reply":"2022-01-28T06:59:14.452659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df[\"score\"] = final_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:59:15.727309Z","iopub.execute_input":"2022-01-28T06:59:15.727813Z","iopub.status.idle":"2022-01-28T06:59:15.732807Z","shell.execute_reply.started":"2022-01-28T06:59:15.727771Z","shell.execute_reply":"2022-01-28T06:59:15.731945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:59:16.01342Z","iopub.execute_input":"2022-01-28T06:59:16.013881Z","iopub.status.idle":"2022-01-28T06:59:16.026759Z","shell.execute_reply.started":"2022-01-28T06:59:16.013845Z","shell.execute_reply":"2022-01-28T06:59:16.025814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)\noof_average.to_csv(\"final_oof.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T06:59:18.696813Z","iopub.execute_input":"2022-01-28T06:59:18.697103Z","iopub.status.idle":"2022-01-28T06:59:19.019148Z","shell.execute_reply.started":"2022-01-28T06:59:18.697074Z","shell.execute_reply":"2022-01-28T06:59:19.017885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}