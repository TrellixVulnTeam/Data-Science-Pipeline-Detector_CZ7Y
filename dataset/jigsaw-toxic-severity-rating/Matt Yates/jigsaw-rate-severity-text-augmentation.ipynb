{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jigsaw Rate Severity - Simple LSTM\n\n**Work:**\n - Forked https://www.kaggle.com/elcaiseri/jigsaw-keras-embedding-lstm\n - Revised data prep and model architecture to run with single input (text) and get single score (relative severity of toxicity)\n     - Target is created by using the (less) and (more) information to assign a value that adheres to all (less) and (more) information\n - Revised optimizer and manually tuned learning rate for better performance\n - Added text augmentation\n\n**References and Acknowledgements:**\n - https://www.kaggle.com/elcaiseri/jigsaw-keras-embedding-lstm\n - https://www.kaggle.com/elcaiseri\n - https://www.kaggle.com/c/jigsaw-toxic-severity-rating/overview\n - https://github.com/tensorflow/tensorflow/issues/38613\n - https://www.kaggle.com/yeayates21/commonlit-text-augmentation-eng-to-fre-to-eng/notebook","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nfrom random import sample\nimport time\n\nimport os\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom textblob import TextBlob","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-11T16:02:20.651462Z","iopub.execute_input":"2021-11-11T16:02:20.652077Z","iopub.status.idle":"2021-11-11T16:02:26.472885Z","shell.execute_reply.started":"2021-11-11T16:02:20.651986Z","shell.execute_reply":"2021-11-11T16:02:26.472099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Wrangling","metadata":{}},{"cell_type":"code","source":"PATH = '/kaggle/input/jigsaw-toxic-severity-rating/'\nvalid_data = pd.read_csv(PATH + 'validation_data.csv')\ncomment_data = pd.read_csv(PATH + 'comments_to_score.csv')\nsub = pd.read_csv(PATH + 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:02:26.474594Z","iopub.execute_input":"2021-11-11T16:02:26.474834Z","iopub.status.idle":"2021-11-11T16:02:27.074368Z","shell.execute_reply.started":"2021-11-11T16:02:26.4748Z","shell.execute_reply":"2021-11-11T16:02:27.073635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_data.sort_values('worker', inplace=True)\nvalid_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:02:27.075507Z","iopub.execute_input":"2021-11-11T16:02:27.075766Z","iopub.status.idle":"2021-11-11T16:02:27.102941Z","shell.execute_reply.started":"2021-11-11T16:02:27.075734Z","shell.execute_reply":"2021-11-11T16:02:27.102293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_data.values.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:02:27.10588Z","iopub.execute_input":"2021-11-11T16:02:27.106068Z","iopub.status.idle":"2021-11-11T16:02:27.11324Z","shell.execute_reply.started":"2021-11-11T16:02:27.106044Z","shell.execute_reply":"2021-11-11T16:02:27.11255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quick EDA\n\nCan text be found more than once in either column?  - Answer: Yes","metadata":{}},{"cell_type":"code","source":"txteg = valid_data.values[0,2] # get text example from more_toxic\nvalid_data[valid_data['less_toxic']==txteg].head() # look for example in less_toxic","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:02:27.114792Z","iopub.execute_input":"2021-11-11T16:02:27.115375Z","iopub.status.idle":"2021-11-11T16:02:27.133755Z","shell.execute_reply.started":"2021-11-11T16:02:27.115339Z","shell.execute_reply":"2021-11-11T16:02:27.132937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"#### Create Target\n\n- We use the (less) and (more) information to assign values that adhere to all (less) and (more) information\n- We assign values to each text, then we loop through the data repeatedly, revising the values each time if there are cases where the value does not adhere\n- If all values adhere to the (less) and (more) information, then we should see fewer revisision with each round","metadata":{}},{"cell_type":"code","source":"#################################\n# get all unique texts\n#################################\nuts = list(set(valid_data['more_toxic'].values.tolist() + valid_data['less_toxic'].values.tolist()))\n# store texts in a dictionary with default value -1\nut_dict = {}\nfor ut in uts:\n    ut_dict[ut] = -1\n\n#################################\n# set values for unique texts given information from valid_data (relatively more or less)\n#################################\nepochs = 60 # number of times to loop over the dataset and make revisions to dictionary\nlrevisions = []\nlreversals = []\nfor i in range(epochs+1):\n    revisions = 0\n    reversals = 0\n    for index, row in valid_data.iterrows():\n        if (ut_dict[row['less_toxic']]==-1) and (ut_dict[row['more_toxic']]==-1): # both undefined\n            ut_dict[row['less_toxic']]=random.uniform(0, 100)\n            ut_dict[row['more_toxic']]=random.uniform(0, 100)\n            revisions += 2\n        elif (ut_dict[row['less_toxic']]!=-1) and (ut_dict[row['more_toxic']]==-1): # less defined, more not\n            cap = ut_dict[row['less_toxic']]\n            val = random.uniform(cap, 100)\n            ut_dict[row['more_toxic']] = val\n            revisions += 1\n        elif (ut_dict[row['less_toxic']]==-1) and (ut_dict[row['more_toxic']]!=-1): # less not defined, more defined\n            cap = ut_dict[row['more_toxic']]\n            val = random.uniform(0, cap)\n            ut_dict[row['less_toxic']] = val\n            revisions += 1\n        else: # both defined\n            if ut_dict[row['less_toxic']]<ut_dict[row['more_toxic']]:\n                pass # this is good to go\n            else: # more < less, which is wrong\n                changeType = random.choice([1,2,3]) # select 1 of 3 different types of revisions\n                if changeType==1: # reverse values\n                    more = ut_dict[row['more_toxic']]\n                    less = ut_dict[row['less_toxic']]\n                    ut_dict[row['more_toxic']] = less + random.uniform(-1, 1) # more = less + jitter\n                    ut_dict[row['less_toxic']] = more + random.uniform(-1, 1) # less = more + jitter\n                elif changeType==2: # set more to less + 1-ish\n                    ut_dict[row['more_toxic']] = ut_dict[row['less_toxic']] + random.uniform(0, 1)\n                elif changeType==3: # set less to more - 1-ish\n                    ut_dict[row['less_toxic']] = ut_dict[row['more_toxic']] - random.uniform(0, 1)\n                revisions += 1\n                reversals += 1\n    lrevisions.append(revisions)\n    lreversals.append(reversals)\n    if i % 5 == 0:\n        print(\"Round {} completed with {} total revisions and {} reversals.\".format(i,revisions,reversals))\nprint(\"All rounds completed.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:02:27.134998Z","iopub.execute_input":"2021-11-11T16:02:27.135905Z","iopub.status.idle":"2021-11-11T16:04:41.265497Z","shell.execute_reply.started":"2021-11-11T16:02:27.135876Z","shell.execute_reply":"2021-11-11T16:04:41.263991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Algorithm Performance - all runs\")\npd.DataFrame({'Revisions':lrevisions,'Reversals':lreversals}).plot(figsize=(12, 6));","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:04:41.266742Z","iopub.execute_input":"2021-11-11T16:04:41.267069Z","iopub.status.idle":"2021-11-11T16:04:41.547074Z","shell.execute_reply.started":"2021-11-11T16:04:41.267032Z","shell.execute_reply":"2021-11-11T16:04:41.546327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Algorithm Performance - excluding the 1st run\")\npd.DataFrame({'Revisions':lrevisions[1:],'Reversals':lreversals[1:]}).plot(figsize=(12, 6));","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:04:41.548217Z","iopub.execute_input":"2021-11-11T16:04:41.548559Z","iopub.status.idle":"2021-11-11T16:04:41.778773Z","shell.execute_reply.started":"2021-11-11T16:04:41.548511Z","shell.execute_reply":"2021-11-11T16:04:41.776771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Compile Training Data & Target\n\n- Add all text in valid_data to a training dataset/list with a corresponding target\n- Apply some augmentation to each text since we have duplicate texts\n- Apply some jitter to the target value since we have duplicate texts and augmentations, and just for some regularization","metadata":{}},{"cell_type":"code","source":"# initialize lists\ntoxic_text = []\ntarget = []\naugmentation_percent = 0.90\n\n#################################\n# loop through valid_data and add text & target to training data lists\n# - also add a small jitter since we have duplicate text examples, for some regularization\n#################################\nfor index, row in tqdm(valid_data.iterrows()):\n    if random.uniform(0, 1)<augmentation_percent: # only augment x% of the time\n        try: # augmentations\n            augshuf = random.uniform(0, 1)\n            if augshuf<0.35:\n                french_translation = str(TextBlob(row['more_toxic']).translate(to='fr'))\n                more_toxic = str(TextBlob(french_translation).translate(to='en')) # back to Eng\n                french_translation = str(TextBlob(row['less_toxic']).translate(to='fr'))\n                less_toxic = str(TextBlob(french_translation).translate(to='en')) # back to Eng\n            else: # remove a random word\n                rand_word = sample(list(set(row['more_toxic'].split(\" \"))))[0]\n                more_toxic = row['more_toxic'].replace(rand_word, '')\n                rand_word = sample(list(set(row['less_toxic'].split(\" \"))))[0]\n                more_toxic = row['less_toxic'].replace(rand_word, '')\n            toxic_text.append(more_toxic)\n            target.append(ut_dict[row['more_toxic']] + random.uniform(-1, 1)) # value plus small jitter\n            toxic_text.append(less_toxic)\n            target.append(ut_dict[row['less_toxic']] + random.uniform(-1, 1)) # value plus small jitter\n        except:\n            toxic_text.append(row['more_toxic'])\n            target.append(ut_dict[row['more_toxic']] + random.uniform(-1, 1)) # value plus small jitter\n            toxic_text.append(row['less_toxic'])\n            target.append(ut_dict[row['less_toxic']] + random.uniform(-1, 1)) # value plus small jitter\n    else:\n        toxic_text.append(row['more_toxic'])\n        target.append(ut_dict[row['more_toxic']] + random.uniform(-1, 1)) # value plus small jitter\n        toxic_text.append(row['less_toxic'])\n        target.append(ut_dict[row['less_toxic']] + random.uniform(-1, 1)) # value plus small jitter","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:04:41.781244Z","iopub.execute_input":"2021-11-11T16:04:41.781898Z","iopub.status.idle":"2021-11-11T16:04:44.215448Z","shell.execute_reply.started":"2021-11-11T16:04:41.781858Z","shell.execute_reply":"2021-11-11T16:04:44.214662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Final Training Data","metadata":{}},{"cell_type":"code","source":"print(\"Text list length: \", len(toxic_text))\nprint(\"Target list length: \", len(target))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:04:44.218714Z","iopub.execute_input":"2021-11-11T16:04:44.219056Z","iopub.status.idle":"2021-11-11T16:04:44.224762Z","shell.execute_reply.started":"2021-11-11T16:04:44.219018Z","shell.execute_reply":"2021-11-11T16:04:44.223995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = pd.DataFrame()\ntraining_data['text'] = toxic_text\ntraining_data['target'] = target\ntraining_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:08:22.853038Z","iopub.execute_input":"2021-11-11T16:08:22.853308Z","iopub.status.idle":"2021-11-11T16:08:22.862787Z","shell.execute_reply.started":"2021-11-11T16:08:22.853256Z","shell.execute_reply":"2021-11-11T16:08:22.861862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(target, label='training target distribution');\nplt.legend();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data.to_csv('jigsaw_rate_severity_training_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T16:08:22.864297Z","iopub.execute_input":"2021-11-11T16:08:22.864658Z","iopub.status.idle":"2021-11-11T16:08:22.891509Z","shell.execute_reply.started":"2021-11-11T16:08:22.864588Z","shell.execute_reply":"2021-11-11T16:08:22.890799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}