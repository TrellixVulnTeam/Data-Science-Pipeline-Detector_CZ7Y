{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reference\n\n* https://www.kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768 \n* https://www.kaggle.com/steubk/jrsotc-ridgeregression\n* https://www.kaggle.com/samarthagarwal23/the-benchmark-0-81-tfidf-ridge\n* https://www.kaggle.com/steubk/jrsotc-ridgeregression-ensemble-of-3\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge, SGDRegressor,  LinearRegression, BayesianRidge, ElasticNet, Lasso\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import rankdata\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.kernel_ridge import KernelRidge\ndef model_cv (vec, X, y, X_test, folds, stratified ):\n    kf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=42)\n    val_scores = []\n    rmse_scores = []\n    X_less_toxics = []\n    X_more_toxics = []\n    val_scores2 = []\n    rmse_scores2 = []\n    X_less_toxics2 = []\n    X_more_toxics2 = []\n\n    preds = []\n    preds2 = []\n    for fold, (train_index,val_index) in enumerate(kf.split(X,stratified)):\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]\n        model = Ridge()\n        model.fit(X_train, y_train)\n        \n        model2 =  RandomForestRegressor()\n        model2.fit(X_train, y_train)\n        \n        rmse_score = mean_squared_error ( model.predict (X_val), y_val, squared = False) \n        rmse_scores.append (rmse_score)\n        \n        rmse_score2 = mean_squared_error ( model2.predict (X_val), y_val, squared = False) \n        rmse_scores2.append (rmse_score2)\n\n        X_less_toxic = vec.transform(df_val['less_toxic'])\n        X_more_toxic = vec.transform(df_val['more_toxic'])\n\n        p1 = model.predict(X_less_toxic)\n        p2 = model.predict(X_more_toxic)\n\n        pp1 = model2.predict(X_less_toxic)\n        pp2 = model2.predict(X_more_toxic)\n        \n        \n        X_less_toxics.append( p1 )\n        X_more_toxics.append( p2 )\n        \n        X_less_toxics2.append( pp1 )\n        X_more_toxics2.append( pp2 )\n\n        # Validation Accuracy\n        val_acc = (p1< p2).mean()\n        val_scores.append(val_acc)\n\n        val_acc2 = (pp1< pp2).mean()\n        val_scores2.append(val_acc)\n        \n        pred = model.predict(X_test)\n        preds.append(pred)\n        \n        pred2 = model2.predict(X_test)\n        preds2.append(pred2)\n\n        print(f\"Ridge *** FOLD:{fold}, rmse_fold:{rmse_score:.5f}, val_acc:{val_acc:.5f}\")\n        print(f\"RF *** FOLD:{fold}, rmse_fold:{rmse_score2:.5f}, val_acc:{val_acc2:.5f}\")\n\n    mean_val_acc = np.mean (val_scores)\n    mean_rmse_score = np.mean (rmse_scores)\n\n    mean_val_acc2 = np.mean (val_scores2)\n    mean_rmse_score2 = np.mean (rmse_scores2)\n    \n    p1 = np.mean ( np.vstack(X_less_toxics), axis=0 )\n    p2 = np.mean ( np.vstack(X_more_toxics), axis=0 )\n\n    pp1 = np.mean ( np.vstack(X_less_toxics2), axis=0 )\n    pp2 = np.mean ( np.vstack(X_more_toxics2), axis=0 )\n    \n    val_acc = (p1< p2).mean()\n    val_acc2 = (pp1< pp2).mean()\n\n    print(f\"Ridge OOF : val_acc:{val_acc:.5f}, mean val_acc:{mean_val_acc:.5f}, mean rmse_score:{mean_rmse_score:.5f}\")\n    print(f\"RF OOF: val_acc:{val_acc2:.5f}, mean val_acc:{mean_val_acc2:.5f}, mean rmse_score:{mean_rmse_score2:.5f}\")\n    \n    preds = np.mean ( np.vstack(preds), axis=0 )\n    preds2 = np.mean ( np.vstack(preds2), axis=0 )\n    return p1, p2, preds, pp1, pp2, preds2\n","metadata":{"execution":{"iopub.status.busy":"2021-11-18T04:32:33.321491Z","iopub.execute_input":"2021-11-18T04:32:33.321901Z","iopub.status.idle":"2021-11-18T04:32:35.327794Z","shell.execute_reply.started":"2021-11-18T04:32:33.321809Z","shell.execute_reply":"2021-11-18T04:32:35.326933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ndf_test = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-18T04:32:35.329715Z","iopub.execute_input":"2021-11-18T04:32:35.329999Z","iopub.status.idle":"2021-11-18T04:32:35.988795Z","shell.execute_reply.started":"2021-11-18T04:32:35.329963Z","shell.execute_reply":"2021-11-18T04:32:35.987958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Toxic Comment Classification Challenge\n\nUsing data from [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)","metadata":{}},{"cell_type":"code","source":"jc_train_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\nprint(f\"jc_train_df:{jc_train_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-18T04:32:35.989967Z","iopub.execute_input":"2021-11-18T04:32:35.990813Z","iopub.status.idle":"2021-11-18T04:32:37.789545Z","shell.execute_reply.started":"2021-11-18T04:32:35.990773Z","shell.execute_reply":"2021-11-18T04:32:37.788697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic = 1.0\nsevere_toxic = 2.0\nobscene = 1.0\nthreat = 1.0\ninsult = 1.0\nidentity_hate = 2.0\n\ndef create_train (df):\n    df['y'] = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].max(axis=1)\n    df['y'] = df[\"y\"]+df['severe_toxic']*severe_toxic\n    df['y'] = df[\"y\"]+df['obscene']*obscene\n    df['y'] = df[\"y\"]+df['threat']*threat\n    df['y'] = df[\"y\"]+df['insult']*insult\n    df['y'] = df[\"y\"]+df['identity_hate']*identity_hate\n    \n    \n    \n    df = df[['comment_text', 'y', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].rename(columns={'comment_text': 'text'})\n\n    #undersample non toxic comments  on Toxic Comment Classification Challenge\n    min_len = (df['y'] >= 1).sum()\n    df_y0_undersample = df[df['y'] == 0].sample(n=int(min_len*1.5),random_state=42)\n    df = pd.concat([df[df['y'] >= 1], df_y0_undersample])\n                                                \n    return df\n \njc_train_df = create_train (jc_train_df)\n                           \ndf = jc_train_df\nprint(df['y'].value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2021-11-18T04:32:37.791336Z","iopub.execute_input":"2021-11-18T04:32:37.791535Z","iopub.status.idle":"2021-11-18T04:32:37.891546Z","shell.execute_reply.started":"2021-11-18T04:32:37.79151Z","shell.execute_reply":"2021-11-18T04:32:37.890693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLDS = 7\n\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(3, 6),smooth_idf=2 )\nX = vec.fit_transform(df['text'])\ny = df[\"y\"].values\nX_test = vec.transform(df_test['text'])\n\nstratified = np.around ( y )\njc_p1, jc_p2, jc_preds, jc_pp1, jc_pp2, jc_preds2 =  model_cv (vec, X, y, X_test, FOLDS, stratified )","metadata":{"execution":{"iopub.status.busy":"2021-11-18T04:32:37.893065Z","iopub.execute_input":"2021-11-18T04:32:37.893268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Unintended Bias in Toxicity Classification\n\nUsing data from [Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data)\n","metadata":{}},{"cell_type":"code","source":"juc_train_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\nprint(f\"juc_train_df:{juc_train_df.shape}\")\njuc_train_df = juc_train_df.query (\"toxicity_annotator_count > 5\")\nprint(f\"juc_train_df:{juc_train_df.shape}\")\n\njuc_train_df['y'] = juc_train_df[[ 'severe_toxicity', 'obscene', 'sexual_explicit','identity_attack', 'insult', 'threat']].sum(axis=1)\n\njuc_train_df['y'] = juc_train_df.apply(lambda row: row[\"target\"] if row[\"target\"] <= 0.5 else row[\"y\"] , axis=1)\njuc_train_df = juc_train_df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\nmin_len = (juc_train_df['y'] > 0.5).sum()\ndf_y0_undersample = juc_train_df[juc_train_df['y'] <= 0.5].sample(n=int(min_len*1.5),random_state=42)\njuc_train_df = pd.concat([juc_train_df[juc_train_df['y'] > 0.5], df_y0_undersample])\n\ndf = juc_train_df\nprint(df['y'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLDS = 7\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(3, 6),smooth_idf=2 )\nX = vec.fit_transform(df['text'])\ny = df[\"y\"].values\nX_test = vec.transform(df_test['text'])\n\nstratified = (np.around ( y, decimals = 1  )*10).astype(int)\njuc_p1, juc_p2, juc_preds, juc_pp1, juc_pp2, juc_preds2 =  model_cv (vec, X, y, X_test, FOLDS, stratified )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Ruddit: Norms of Offensiveness for English Reddit Comments\n\nUsing data from [Ruddit](https://github.com/hadarishav/Ruddit)","metadata":{}},{"cell_type":"code","source":"rud_df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\nprint(f\"rud_df:{rud_df.shape}\")\nrud_df['y'] = rud_df['offensiveness_score'].map(lambda x: 0.0 if x <=0 else x)\nrud_df = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\nmin_len = (rud_df['y'] < 0.5).sum()\nprint(rud_df['y'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLDS = 7\ndf = rud_df\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(3, 6) ,smooth_idf=2 )\nX = vec.fit_transform(df['text'])\ny = df[\"y\"].values\nX_test = vec.transform(df_test['text'])\n\nstratified = (np.around ( y, decimals = 1  )*10).astype(int)\nrud_p1, rud_p2, rud_preds, rud_pp1, rud_pp2, rud_preds2 =  model_cv (vec, X, y, X_test, FOLDS, stratified )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble","metadata":{}},{"cell_type":"code","source":"jc_max = max(jc_p1.max() , jc_p2.max())\njuc_max = max(juc_p1.max() , juc_p2.max())\nrud_max = max(rud_p1.max() , rud_p2.max())\n\n\np1 = jc_p1/jc_max + juc_p1/juc_max + rud_p1/rud_max\np2 = jc_p2/jc_max + juc_p2/juc_max + rud_p2/rud_max\n\nval_acc = (p1< p2).mean()\nprint(f\"Ridge Ensemble: val_acc:{val_acc:.5f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jc_max2 = max(jc_pp1.max() , jc_pp2.max())\njuc_max2 = max(juc_pp1.max() , juc_pp2.max())\nrud_max2 = max(rud_pp1.max() , rud_pp2.max())\n\n\npp1 = jc_pp1/jc_max2 + juc_pp1/juc_max2 + rud_pp1/rud_max2\npp2 = jc_pp2/jc_max2 + juc_pp2/juc_max2 + rud_pp2/rud_max2\n\nval_acc2 = (pp1< pp2).mean()\nprint(f\"RF Ensemble: val_acc:{val_acc2:.5f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score1 = jc_preds/jc_max + juc_preds/juc_max + rud_preds/rud_max  \nscore2 = jc_preds2/jc_max2 + juc_preds2/juc_max2 + rud_preds2/rud_max2\n## to enforce unique values on score\nscore = score1+score2\ndf_test['score'] = rankdata(score, method='ordinal')\n\ndf_test[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['jc_preds']=jc_preds\ndf_test['juc_preds']=juc_preds\ndf_test['rud_preds']=rud_preds\ndf_test['jc_preds2']=jc_preds2\ndf_test['juc_preds2']=juc_preds2\ndf_test['rud_preds2']=rud_preds2\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = BayesianRidge()\ntrain_fea = df_test[['jc_preds','juc_preds','rud_preds','jc_preds2','juc_preds2','rud_preds2']]\ny_train = df_test['score']\nclf.fit(train_fea,y_train)\ny_ens = clf.predict(train_fea)\nvfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\nminmin= min(y_ens)\nmaxmax= max(y_ens)\ny_res=vfunc(y_ens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['score'] = y_res\ndf_test[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}