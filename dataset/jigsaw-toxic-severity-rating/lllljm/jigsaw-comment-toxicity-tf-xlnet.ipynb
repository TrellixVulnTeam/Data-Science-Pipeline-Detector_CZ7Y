{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## XLNet dataset can be found [here](https://www.kaggle.com/devkhant24/tensorlfow-xlnet).","metadata":{}},{"cell_type":"code","source":"# Importing libraries\n\nimport math\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport re\nimport unidecode\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-25T10:15:39.566065Z","iopub.execute_input":"2022-01-25T10:15:39.56676Z","iopub.status.idle":"2022-01-25T10:15:48.205621Z","shell.execute_reply.started":"2022-01-25T10:15:39.566671Z","shell.execute_reply":"2022-01-25T10:15:48.204902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining constants\n\nAUTO = tf.data.experimental.AUTOTUNE\n\nmodel_name = \"../input/tensorlfow-xlnet\"\nMax_len = 512\nBatch_size = 8\n\n\ndata_prev_comp = \"../input/toxic-comment/jigsaw-toxic-comment-train.csv\"\ndata_cur_comp = \"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\n\n\ndef seed_everything():\n    np.random.seed(123)\n    random.seed(123)\n    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n    os.environ['PYTHONHASHSEED'] = str(123)\n\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:15:48.207233Z","iopub.execute_input":"2022-01-25T10:15:48.207471Z","iopub.status.idle":"2022-01-25T10:15:48.214008Z","shell.execute_reply.started":"2022-01-25T10:15:48.207436Z","shell.execute_reply":"2022-01-25T10:15:48.21341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for cleaning comments\n\ndef clean_data(sent):\n    sent = sent.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n    soup = BeautifulSoup(sent, \"html.parser\")\n    sent = soup.get_text(separator=\" \")\n    remove_https = re.sub(r'http\\S+', '', sent)\n    sent = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n    sent = unidecode.unidecode(sent)\n    sent = sent.lower()\n    sent = re.sub(r\"[^a-zA-Z0-9:$-,()%.?!]+\", ' ', sent) \n    sent = re.sub(r\"[:$-,()%.?!]+\", ' ',sent)\n    stoplist = stopwords.words(\"english\")\n    sent = [word for word in word_tokenize(sent) if word not in stoplist]\n    sent = \" \".join(sent)\n    \n    return sent","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:15:48.215223Z","iopub.execute_input":"2022-01-25T10:15:48.215673Z","iopub.status.idle":"2022-01-25T10:15:48.231419Z","shell.execute_reply.started":"2022-01-25T10:15:48.215638Z","shell.execute_reply":"2022-01-25T10:15:48.230667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading train file from previous competition\n\ndf = pd.read_csv(data_prev_comp)\ny_features = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\ndf[\"severe_toxic\"] = df[\"severe_toxic\"] * 2\ndf[\"y\"] = (df[y_features].sum(axis=1)).astype(int)\ndf[\"y\"] = df[\"y\"] / df[\"y\"].max()\ndf.drop(y_features, axis=1, inplace = True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:15:48.234987Z","iopub.execute_input":"2022-01-25T10:15:48.23525Z","iopub.status.idle":"2022-01-25T10:15:50.962464Z","shell.execute_reply.started":"2022-01-25T10:15:48.235173Z","shell.execute_reply":"2022-01-25T10:15:50.961777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For dropping 170000 rows of y with value 0\n# To balance dataset\n\ndf.drop(df[df[\"y\"] == 0].sample(180000).index.tolist(), inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:15:50.963855Z","iopub.execute_input":"2022-01-25T10:15:50.964266Z","iopub.status.idle":"2022-01-25T10:15:51.066076Z","shell.execute_reply.started":"2022-01-25T10:15:50.964229Z","shell.execute_reply":"2022-01-25T10:15:51.065319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seeing that dataset is imbalanced\n\ndf[\"y\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:15:51.067487Z","iopub.execute_input":"2022-01-25T10:15:51.067771Z","iopub.status.idle":"2022-01-25T10:15:51.076883Z","shell.execute_reply.started":"2022-01-25T10:15:51.067734Z","shell.execute_reply":"2022-01-25T10:15:51.076156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating column clean_text for cleaned comments\n\ndf[\"comment_text\"] = df[\"comment_text\"].map(clean_data)\n\nxtrain, xtest, ytrain, ytest = train_test_split(df[\"comment_text\"], df[\"y\"], test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:15:51.078306Z","iopub.execute_input":"2022-01-25T10:15:51.07877Z","iopub.status.idle":"2022-01-25T10:16:36.214563Z","shell.execute_reply.started":"2022-01-25T10:15:51.078729Z","shell.execute_reply":"2022-01-25T10:16:36.213778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for creating word encodings\n#用于创建单词编码的函数\ndef roberta_encode(texts, tokenizer):\n    ct = len(texts)\n    input_ids = np.ones((ct, Max_len), dtype='int32')\n    attention_mask = np.zeros((ct, Max_len), dtype='int32')\n\n    for k, text in enumerate(texts):\n        encoded_text = tokenizer.tokenize(text)\n        \n        # Truncate and convert tokens to numerical IDs\n        enc_text = tokenizer.convert_tokens_to_ids(encoded_text[:(Max_len-2)])\n        \n        input_length = len(enc_text) + 2\n        input_length = input_length if input_length < Max_len else Max_len\n        \n        # Add tokens [CLS] and [SEP] at the beginning and the end\n        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n        \n        # Set to 1s in the attention input\n        attention_mask[k,:input_length] = 1\n        \n    return{\n        'input_words_ids': input_ids,\n        'input_mask': attention_mask,\n    }","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:16:36.215999Z","iopub.execute_input":"2022-01-25T10:16:36.216282Z","iopub.status.idle":"2022-01-25T10:16:36.224738Z","shell.execute_reply.started":"2022-01-25T10:16:36.216246Z","shell.execute_reply":"2022-01-25T10:16:36.22385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing Tokenizer\n#初始化标记器\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\n# Creating encodings for train and validation data\nX_train = roberta_encode(xtrain, tokenizer)\nX_validation = roberta_encode(xtest, tokenizer)\n\nY_train = np.asarray(ytrain, dtype=\"int32\")\nY_validation = np.asarray(ytest, dtype=\"int32\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:16:36.226661Z","iopub.execute_input":"2022-01-25T10:16:36.226963Z","iopub.status.idle":"2022-01-25T10:16:49.205641Z","shell.execute_reply.started":"2022-01-25T10:16:36.226911Z","shell.execute_reply":"2022-01-25T10:16:49.204901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for building the Roberta Model\n#用于构建Roberta模型的函数\ndef build_model():\n    input_words_ids = tf.keras.Input(shape=(Max_len, ), dtype = tf.int32, name=\"input_words_ids\")\n    input_mask = tf.keras.Input(shape=(Max_len, ), dtype = tf.int32, name=\"input_mask\")\n    \n    roberta_model = TFAutoModel.from_pretrained(model_name)\n    x = roberta_model(input_words_ids, attention_mask = input_mask)\n    x = tf.keras.layers.Dropout(0.2)(x[0])\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(256, activation='relu')(x)\n    x = tf.keras.layers.Dense(1, activation = \"sigmoid\")(x)\n    \n    model = tf.keras.Model(inputs = [input_words_ids, input_mask], outputs = x)\n    \n    model.compile(\n        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),\n        loss = \"BinaryCrossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:16:49.208186Z","iopub.execute_input":"2022-01-25T10:16:49.208458Z","iopub.status.idle":"2022-01-25T10:16:49.216205Z","shell.execute_reply.started":"2022-01-25T10:16:49.208421Z","shell.execute_reply":"2022-01-25T10:16:49.21537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\n\nmodel = build_model()\n\nhistory = model.fit(\n            X_train,\n            Y_train,\n            epochs = 1,\n            batch_size = Batch_size,\n            validation_data = (X_validation, Y_validation)\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:16:49.217658Z","iopub.execute_input":"2022-01-25T10:16:49.218194Z","iopub.status.idle":"2022-01-25T11:47:45.357424Z","shell.execute_reply.started":"2022-01-25T10:16:49.218158Z","shell.execute_reply":"2022-01-25T11:47:45.356578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the test data\n\ntest = pd.read_csv(data_cur_comp)\n\ntest[\"text\"] = test[\"text\"].map(clean_data)\nX_test = roberta_encode(test[\"text\"], tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:47:45.359076Z","iopub.execute_input":"2022-01-25T11:47:45.359849Z","iopub.status.idle":"2022-01-25T11:47:56.208493Z","shell.execute_reply.started":"2022-01-25T11:47:45.35981Z","shell.execute_reply":"2022-01-25T11:47:56.20772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making prediction and creating submission file\n\npred = model.predict(X_test)\n\nfinal = pd.DataFrame()\nfinal[\"comment_id\"] = test[\"comment_id\"]\nfinal[\"score\"] = pred\nfinal.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:47:56.209679Z","iopub.execute_input":"2022-01-25T11:47:56.209962Z","iopub.status.idle":"2022-01-25T11:53:20.47942Z","shell.execute_reply.started":"2022-01-25T11:47:56.209908Z","shell.execute_reply":"2022-01-25T11:53:20.478702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T11:53:20.480803Z","iopub.execute_input":"2022-01-25T11:53:20.481079Z","iopub.status.idle":"2022-01-25T11:53:20.489371Z","shell.execute_reply.started":"2022-01-25T11:53:20.481043Z","shell.execute_reply":"2022-01-25T11:53:20.488684Z"},"trusted":true},"execution_count":null,"outputs":[]}]}