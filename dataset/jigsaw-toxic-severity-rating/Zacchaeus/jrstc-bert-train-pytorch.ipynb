{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\n# model_checkpoint = \"distilbert-base-uncased\"\nmodel_checkpoint = \"roberta-base\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-08T18:26:45.86254Z","iopub.execute_input":"2021-11-08T18:26:45.863312Z","iopub.status.idle":"2021-11-08T18:26:45.989212Z","shell.execute_reply.started":"2021-11-08T18:26:45.863274Z","shell.execute_reply":"2021-11-08T18:26:45.988225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jrstc-folds/jrstc_5folds.csv\")\ndf = df.drop_duplicates(subset=['less_toxic', 'more_toxic']).reset_index(drop=True)\ndf['text'] = ''\ndf['label'] = 0\nfor i in range(len(df)):\n    less = df.loc[i, 'less_toxic']\n    more = df.loc[i, 'more_toxic']\n    df.loc[i, 'label'] = i % 2\n    if i % 2 == 0:\n        df.loc[i, 'text'] = less + '</s>' + more\n    else:\n        df.loc[i, 'text'] = more + '</s>' + less\ndf = df.sample(frac=1).reset_index(drop=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-11-08T18:08:31.356555Z","iopub.execute_input":"2021-11-08T18:08:31.357216Z","iopub.status.idle":"2021-11-08T18:08:49.974928Z","shell.execute_reply.started":"2021-11-08T18:08:31.357181Z","shell.execute_reply":"2021-11-08T18:08:49.973801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T18:08:49.976896Z","iopub.execute_input":"2021-11-08T18:08:49.977371Z","iopub.status.idle":"2021-11-08T18:08:57.544499Z","shell.execute_reply.started":"2021-11-08T18:08:49.977328Z","shell.execute_reply":"2021-11-08T18:08:57.543515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JRSTCDataset(Dataset):\n    def __init__(self, tokenizer, df, fold, is_val=False):\n        self.tokenizer = tokenizer\n        if is_val:\n            self.texts = df.loc[df[\"kfold\"]==fold].text.tolist()\n            self.labels = df.loc[df[\"kfold\"]==fold].label.tolist()\n        else:\n            self.texts = df.loc[df[\"kfold\"]!=fold].text.tolist()\n            self.labels = df.loc[df[\"kfold\"]!=fold].label.tolist()\n        self.encodings = self.tokenizer(self.texts, truncation=True, padding=False)\n        \n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n    def __getitem__(self, idx):\n        return {'input_ids': torch.tensor(self.encodings[\"input_ids\"][idx]), \n                'attention_mask': torch.tensor(self.encodings[\"attention_mask\"][idx]), \n                'labels': torch.tensor(self.labels[idx])\n               }","metadata":{"execution":{"iopub.status.busy":"2021-11-08T18:30:06.58783Z","iopub.execute_input":"2021-11-08T18:30:06.588153Z","iopub.status.idle":"2021-11-08T18:30:06.604851Z","shell.execute_reply.started":"2021-11-08T18:30:06.58809Z","shell.execute_reply":"2021-11-08T18:30:06.603377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for fold in range(5):\nfor fold in range(1):\n    print('fold:', fold)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n    train_dataset = JRSTCDataset(tokenizer, df, fold, False)\n    val_dataset = JRSTCDataset(tokenizer, df, fold, True)\n    training_args = TrainingArguments(\n        output_dir=f'./fold{fold}',          # output directory\n        num_train_epochs=3,              # total number of training epochs\n        per_device_train_batch_size=16,  # batch size per device during training\n        per_device_eval_batch_size=64,   # batch size for evaluation\n        warmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\n        weight_decay=0.01,               # strength of weight decay\n        logging_dir=f'./fold{fold}',            # directory for storing logs\n        fp16=True,\n        report_to='none',\n        save_total_limit=1,\n        learning_rate=3e-5,\n        seed=42,\n        group_by_length=True,\n        save_strategy='steps',\n        save_steps=100,\n        evaluation_strategy='steps',\n        eval_steps=100,\n        logging_strategy='steps',\n        logging_steps=100\n    )\n\n    trainer = Trainer(\n        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n        args=training_args,                  # training arguments, defined above\n        train_dataset=train_dataset,         # training dataset\n        eval_dataset=val_dataset,             # evaluation dataset\n        tokenizer=tokenizer\n    )\n\n    trainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T18:30:32.537075Z","iopub.execute_input":"2021-11-08T18:30:32.537378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}