{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-09T08:55:59.753612Z","iopub.execute_input":"2021-11-09T08:55:59.754221Z","iopub.status.idle":"2021-11-09T08:55:59.765183Z","shell.execute_reply.started":"2021-11-09T08:55:59.754169Z","shell.execute_reply":"2021-11-09T08:55:59.76418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:crimson;\">Competition Info</span>**","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:limegreen;\">Some Basic EDA</span>**\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"sam_sub = pd.read_csv('../input/jigsaw-toxic-severity-rating/sample_submission.csv')\nval_sub = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\nscore_file = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T08:55:59.766761Z","iopub.execute_input":"2021-11-09T08:55:59.767033Z","iopub.status.idle":"2021-11-09T08:56:00.446516Z","shell.execute_reply.started":"2021-11-09T08:55:59.766986Z","shell.execute_reply":"2021-11-09T08:56:00.445801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('*'*40)\nprint(\"Sample Submission File\")\nprint('*'*40)\nprint(sam_sub.head())\nprint(sam_sub.describe())\nprint(sam_sub.columns)\nprint(sam_sub.dtypes)\nprint(sam_sub.shape)\n\nprint('*'*40)\nprint(\"Validation Submission File\")\nprint('*'*40)\nprint(val_sub.head())\nprint(val_sub.describe())\nprint(val_sub.columns)\nprint(val_sub.dtypes)\nprint(val_sub.shape)\n\nprint('*'*40)\nprint(\"Score File\")\nprint('*'*40)\nprint(score_file.head())\nprint(score_file.describe())\nprint(score_file.columns)\nprint(score_file.dtypes)\nprint(score_file.shape)\nprint('*'*40)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T08:56:00.448677Z","iopub.execute_input":"2021-11-09T08:56:00.449465Z","iopub.status.idle":"2021-11-09T08:56:00.509754Z","shell.execute_reply.started":"2021-11-09T08:56:00.449407Z","shell.execute_reply":"2021-11-09T08:56:00.508797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import nltk\nimport pycountry\nfrom nltk.stem import SnowballStemmer\n\n'''\ndef get_language(text):\n    tc = nltk.classify.textcat.TextCat() \n    guess = tc.guess_language(text)\n    try:\n        guess_name = pycountry.languages.get(alpha_3=guess).name\n    except:\n        guess_name='None'\n    return guess_name\n'''","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:45:49.473182Z","iopub.execute_input":"2021-11-09T09:45:49.473797Z","iopub.status.idle":"2021-11-09T09:45:49.480213Z","shell.execute_reply.started":"2021-11-09T09:45:49.473745Z","shell.execute_reply":"2021-11-09T09:45:49.47946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n'''\nscore_file[\"lang\"] = score_file[\"text\"].apply(get_language)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-11-09T10:04:37.099404Z","iopub.execute_input":"2021-11-09T10:04:37.099671Z","iopub.status.idle":"2021-11-09T10:04:38.319978Z","shell.execute_reply.started":"2021-11-09T10:04:37.099643Z","shell.execute_reply":"2021-11-09T10:04:38.319318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nlang_list = sorted(list(set(score_file[\"lang\"])))\ncounts = [list(score_file[\"lang\"]).count(cont) for cont in lang_list]\ndf = pd.DataFrame(np.transpose([lang_list, counts]))\ndf.columns = [\"Language\", \"Count\"]\ndf[\"Count\"] = df[\"Count\"].apply(int)\nfig = px.barh(df, y=\"Language\", x=\"Count\", title=\"Language of comments\", color=\"Language\", text=\"Count\")\n'''","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:45:10.043437Z","iopub.status.idle":"2021-11-09T09:45:10.044011Z","shell.execute_reply.started":"2021-11-09T09:45:10.043817Z","shell.execute_reply":"2021-11-09T09:45:10.043837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\n\nfrom nltk import tokenize\n\ntweets=score_file\n\nsid = SentimentIntensityAnalyzer()\n\ntweets['sentiment_compound_polarity']=tweets.text.apply(lambda x:sid.polarity_scores(x)['compound'])\ntweets['sentiment_neutral']=tweets.text.apply(lambda x:sid.polarity_scores(x)['neu'])\ntweets['sentiment_negative']=tweets.text.apply(lambda x:sid.polarity_scores(x)['neg'])\ntweets['sentiment_pos']=tweets.text.apply(lambda x:sid.polarity_scores(x)['pos'])\ntweets['sentiment_type']=''\ntweets.loc[tweets.sentiment_compound_polarity>0,'sentiment_type']='POSITIVE'\ntweets.loc[tweets.sentiment_compound_polarity==0,'sentiment_type']='NEUTRAL'\ntweets.loc[tweets.sentiment_compound_polarity<0,'sentiment_type']='NEGATIVE'\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T08:56:00.510928Z","iopub.execute_input":"2021-11-09T08:56:00.511294Z","iopub.status.idle":"2021-11-09T08:56:29.825878Z","shell.execute_reply.started":"2021-11-09T08:56:00.511262Z","shell.execute_reply":"2021-11-09T08:56:29.825008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntry:\n    from pywaffle import Waffle\nexcept:\n    !pip install pywaffle\n    from pywaffle import Waffle\n    \nimport random\n\n\nsentiment_type = tweets[\"sentiment_type\"].value_counts()\n\nfig = plt.figure(\n    FigureClass=Waffle,\n    rows=4,\n    columns=25,\n    values=sentiment_type,\n    title={'label': 'Sentiment Type', 'loc': 'center'},\n    labels=[\"{}({})\".format(a, b) for a, b in zip(sentiment_type.index, sentiment_type) ],\n    colors=[\"#feb308\", \"#070d0d\", \"#9b5fc0\"],\n    icons=['sun', 'cloud-showers-heavy', 'snowflake'],\n    legend={'loc': 'lower left', 'bbox_to_anchor': (0, -0.4), 'ncol': len(tweets.groupby(\"sentiment_type\")), 'framealpha': 0.2},\n    font_size=40, \n    figsize=(20, 8),  \n    icon_legend=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-09T10:12:41.629273Z","iopub.execute_input":"2021-11-09T10:12:41.629899Z","iopub.status.idle":"2021-11-09T10:12:42.401303Z","shell.execute_reply.started":"2021-11-09T10:12:41.629852Z","shell.execute_reply":"2021-11-09T10:12:42.400694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.sentiment_type.value_counts().plot(kind='bar',title=\"sentiment analysis\")","metadata":{"execution":{"iopub.status.busy":"2021-11-09T08:56:30.313335Z","iopub.execute_input":"2021-11-09T08:56:30.313625Z","iopub.status.idle":"2021-11-09T08:56:30.507265Z","shell.execute_reply.started":"2021-11-09T08:56:30.313592Z","shell.execute_reply":"2021-11-09T08:56:30.506639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sam_sub['score'] = tweets['sentiment_compound_polarity']","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:08:41.791393Z","iopub.execute_input":"2021-11-09T09:08:41.792349Z","iopub.status.idle":"2021-11-09T09:08:41.799123Z","shell.execute_reply.started":"2021-11-09T09:08:41.792287Z","shell.execute_reply":"2021-11-09T09:08:41.798433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:limegreen;\">Basic Sentiment Analysis Using NLTK</span>**\n","metadata":{}},{"cell_type":"code","source":"print('*'*40)\nprint(\"Sample Submission File\")\nprint('*'*40)\nprint(sam_sub.head())\nprint(sam_sub.describe())\nprint(sam_sub.columns)\nprint(sam_sub.dtypes)\nprint(sam_sub.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:09:01.742288Z","iopub.execute_input":"2021-11-09T09:09:01.742748Z","iopub.status.idle":"2021-11-09T09:09:01.766893Z","shell.execute_reply.started":"2021-11-09T09:09:01.7427Z","shell.execute_reply":"2021-11-09T09:09:01.766015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sam_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:10:32.897699Z","iopub.execute_input":"2021-11-09T09:10:32.898008Z","iopub.status.idle":"2021-11-09T09:10:32.929851Z","shell.execute_reply.started":"2021-11-09T09:10:32.897972Z","shell.execute_reply":"2021-11-09T09:10:32.929129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#fe46a5;\">Work In Progress... More to come</span>**\n","metadata":{}}]}