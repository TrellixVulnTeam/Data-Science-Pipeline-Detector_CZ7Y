{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing libs","metadata":{}},{"cell_type":"code","source":"# Basic Pydata Libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt     \nimport seaborn as sns\nimport html\nimport unicodedata\n\n# for reproducibility , to get the same results when evry your run\nnp.random.seed(2021) \n\n\n# sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.svm import LinearSVC, SVC\n\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# ML\nimport tensorflow as tf\nimport keras.backend as K\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Concatenate\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers.merge import concatenate\n\n\n\n#string\nimport string\nimport re\n\n#nlp\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk import ngrams\n\n\nstop_words = set(stopwords.words(\"english\"))\n\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nfrom wordcloud import WordCloud, STOPWORDS\n\n## warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina'","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:32:48.836006Z","iopub.execute_input":"2021-11-19T18:32:48.836503Z","iopub.status.idle":"2021-11-19T18:32:53.769498Z","shell.execute_reply.started":"2021-11-19T18:32:48.836369Z","shell.execute_reply":"2021-11-19T18:32:53.768574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing data","metadata":{}},{"cell_type":"code","source":"PATH = '/kaggle/input/jigsaw-toxic-severity-rating/'\ntrain = pd.read_csv(PATH + 'validation_data.csv')\ntest = pd.read_csv(PATH + 'comments_to_score.csv')\nsub = pd.read_csv(PATH + 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:32:53.771281Z","iopub.execute_input":"2021-11-19T18:32:53.771534Z","iopub.status.idle":"2021-11-19T18:32:54.031419Z","shell.execute_reply.started":"2021-11-19T18:32:53.7715Z","shell.execute_reply":"2021-11-19T18:32:54.030675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:32:54.032539Z","iopub.execute_input":"2021-11-19T18:32:54.032782Z","iopub.status.idle":"2021-11-19T18:32:54.047875Z","shell.execute_reply.started":"2021-11-19T18:32:54.03275Z","shell.execute_reply":"2021-11-19T18:32:54.046951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = train.shape[0]\nrows","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:32:54.050633Z","iopub.execute_input":"2021-11-19T18:32:54.050977Z","iopub.status.idle":"2021-11-19T18:32:54.058581Z","shell.execute_reply.started":"2021-11-19T18:32:54.050941Z","shell.execute_reply":"2021-11-19T18:32:54.05791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.less_toxic[0], train.more_toxic[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:32:54.059961Z","iopub.execute_input":"2021-11-19T18:32:54.060503Z","iopub.status.idle":"2021-11-19T18:32:54.066918Z","shell.execute_reply.started":"2021-11-19T18:32:54.060464Z","shell.execute_reply":"2021-11-19T18:32:54.066247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.less_toxic[rows-1], train.more_toxic[rows-1]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:32:54.068356Z","iopub.execute_input":"2021-11-19T18:32:54.068746Z","iopub.status.idle":"2021-11-19T18:32:54.07697Z","shell.execute_reply.started":"2021-11-19T18:32:54.068706Z","shell.execute_reply":"2021-11-19T18:32:54.076237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def remove_special_chars(text):\n    sequencePattern   = r\"(.)\\1\\1+\"\n    seqReplacePattern = r\"\\1\\1\"\n    text = re.sub(sequencePattern, seqReplacePattern, text)         # Replace 3 or more consecutive letters by 2 letter.\n    text = re.sub('<.*?>+', '', text)                               # remove tages\n    return text\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text, and by defult lemmatize nouns\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text  = remove_special_chars(text)\n    text  = remove_non_ascii(text)\n    text  = remove_punctuation(text)\n    text  = to_lowercase(text)\n    text  = replace_numbers(text)\n    words = text2words(text)\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:32:54.078384Z","iopub.execute_input":"2021-11-19T18:32:54.078815Z","iopub.status.idle":"2021-11-19T18:32:54.092539Z","shell.execute_reply.started":"2021-11-19T18:32:54.078777Z","shell.execute_reply":"2021-11-19T18:32:54.091727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a clean text for  less_toxic\ncleaned_lees_toxic = [normalize_text(sent) for sent in train['less_toxic']]\ntrain['cleaned_lees_toxic'] = cleaned_lees_toxic\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:32:54.095455Z","iopub.execute_input":"2021-11-19T18:32:54.095669Z","iopub.status.idle":"2021-11-19T18:33:27.385057Z","shell.execute_reply.started":"2021-11-19T18:32:54.095646Z","shell.execute_reply":"2021-11-19T18:33:27.38428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a clean text for  more_toxic\ncleaned_more_toxic = [normalize_text(sent) for sent in train['more_toxic']]\ntrain['cleaned_more_toxic'] = cleaned_more_toxic\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:33:27.386844Z","iopub.execute_input":"2021-11-19T18:33:27.387337Z","iopub.status.idle":"2021-11-19T18:34:01.337871Z","shell.execute_reply.started":"2021-11-19T18:33:27.387294Z","shell.execute_reply":"2021-11-19T18:34:01.33708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.cleaned_lees_toxic[0], train.cleaned_more_toxic[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.34443Z","iopub.execute_input":"2021-11-19T18:34:01.346568Z","iopub.status.idle":"2021-11-19T18:34:01.356239Z","shell.execute_reply.started":"2021-11-19T18:34:01.346525Z","shell.execute_reply":"2021-11-19T18:34:01.355426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.cleaned_lees_toxic[rows-1], train.cleaned_more_toxic[rows-1]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.357689Z","iopub.execute_input":"2021-11-19T18:34:01.357947Z","iopub.status.idle":"2021-11-19T18:34:01.367356Z","shell.execute_reply.started":"2021-11-19T18:34:01.357913Z","shell.execute_reply":"2021-11-19T18:34:01.366573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"**Here we will make two columns target \"score\" for each toxic level, and we indicate each one with `0 for less_toxic` and `1 for more_toxic`.**","metadata":{}},{"cell_type":"code","source":"less_toxic_score = [0] * rows\nmore_toxic_score = [1] * rows\ntrain['less_toxic_score'] = less_toxic_score\ntrain['more_toxic_score'] = more_toxic_score\n# drop the original toxic data\ntrain.drop(['less_toxic', 'more_toxic'], inplace= True, axis = 1)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.368742Z","iopub.execute_input":"2021-11-19T18:34:01.369285Z","iopub.status.idle":"2021-11-19T18:34:01.416649Z","shell.execute_reply.started":"2021-11-19T18:34:01.369246Z","shell.execute_reply":"2021-11-19T18:34:01.415881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will make a `toxic_data` column for cleaned_less_toxic and cleaned_more_toxic and make also a `target`  column which is indicate for each toxic score.**\n\n**Shuffel the `toxic_data` and `target` with the same random state for makes better in modeling.**","metadata":{}},{"cell_type":"code","source":"toxic_data = cleaned_lees_toxic + cleaned_more_toxic\ntarget = less_toxic_score + more_toxic_score","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.417846Z","iopub.execute_input":"2021-11-19T18:34:01.418333Z","iopub.status.idle":"2021-11-19T18:34:01.423816Z","shell.execute_reply.started":"2021-11-19T18:34:01.41829Z","shell.execute_reply":"2021-11-19T18:34:01.422588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_data[:5], target[:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.425619Z","iopub.execute_input":"2021-11-19T18:34:01.425966Z","iopub.status.idle":"2021-11-19T18:34:01.436454Z","shell.execute_reply.started":"2021-11-19T18:34:01.425928Z","shell.execute_reply":"2021-11-19T18:34:01.435724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_data[-5:], target[-5:]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.438248Z","iopub.execute_input":"2021-11-19T18:34:01.438813Z","iopub.status.idle":"2021-11-19T18:34:01.445522Z","shell.execute_reply.started":"2021-11-19T18:34:01.438775Z","shell.execute_reply":"2021-11-19T18:34:01.44464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Shuffel the data**","metadata":{}},{"cell_type":"code","source":"import random\n\na = ['a', 'b', 'c']\nb = [1, 2, 3]\n\nc = list(zip(a, b))\nprint(c)\n\nrandom.shuffle(c)\n\na, b = zip(*c)\n\nprint(a)\nprint(b)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.44691Z","iopub.execute_input":"2021-11-19T18:34:01.447186Z","iopub.status.idle":"2021-11-19T18:34:01.456Z","shell.execute_reply.started":"2021-11-19T18:34:01.447152Z","shell.execute_reply":"2021-11-19T18:34:01.455047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shuffled_data = list(zip(toxic_data, target))\nrandom.shuffle(shuffled_data)\ntoxic_data, target = zip(*shuffled_data)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.457588Z","iopub.execute_input":"2021-11-19T18:34:01.458099Z","iopub.status.idle":"2021-11-19T18:34:01.546029Z","shell.execute_reply.started":"2021-11-19T18:34:01.458062Z","shell.execute_reply":"2021-11-19T18:34:01.545236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_data[:5], target[:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.549263Z","iopub.execute_input":"2021-11-19T18:34:01.549472Z","iopub.status.idle":"2021-11-19T18:34:01.557035Z","shell.execute_reply.started":"2021-11-19T18:34:01.549447Z","shell.execute_reply":"2021-11-19T18:34:01.556132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_data[-5:], target[-5:]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.55863Z","iopub.execute_input":"2021-11-19T18:34:01.558925Z","iopub.status.idle":"2021-11-19T18:34:01.565311Z","shell.execute_reply.started":"2021-11-19T18:34:01.558889Z","shell.execute_reply":"2021-11-19T18:34:01.564226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Makes a new DataFrame for shuffedled data**","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame({'toxic_text': toxic_data,\n                  'target': target})\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.567485Z","iopub.execute_input":"2021-11-19T18:34:01.568111Z","iopub.status.idle":"2021-11-19T18:34:01.617445Z","shell.execute_reply.started":"2021-11-19T18:34:01.568071Z","shell.execute_reply":"2021-11-19T18:34:01.616692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.618846Z","iopub.execute_input":"2021-11-19T18:34:01.619122Z","iopub.status.idle":"2021-11-19T18:34:01.62723Z","shell.execute_reply.started":"2021-11-19T18:34:01.619087Z","shell.execute_reply":"2021-11-19T18:34:01.625842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data = df, x= 'target');","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.628592Z","iopub.execute_input":"2021-11-19T18:34:01.63008Z","iopub.status.idle":"2021-11-19T18:34:01.859078Z","shell.execute_reply.started":"2021-11-19T18:34:01.630042Z","shell.execute_reply":"2021-11-19T18:34:01.858353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Most frequent words\n","metadata":{}},{"cell_type":"code","source":"from collections import Counter","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.860214Z","iopub.execute_input":"2021-11-19T18:34:01.861078Z","iopub.status.idle":"2021-11-19T18:34:01.864837Z","shell.execute_reply.started":"2021-11-19T18:34:01.861037Z","shell.execute_reply":"2021-11-19T18:34:01.863884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def freq_words(text,score, num):\n    '''\n        take the whole data, and return data which is have # of words in each sentiment has been passed\n    '''\n    words = [word for sent in text['toxic_text'][text['target'] == float(score)] for word in sent.split()]    \n    freq_words = Counter(words)\n    freq_words_sorted = sorted(freq_words.items(), key=lambda pair: pair[1], reverse=True)\n    freq_words_df = pd.DataFrame(freq_words_sorted[:num], columns=['word', 'counts'])\n    return freq_words_df\n\ndef plot_freq(data, st, num):\n    '''\n        take the data, and st refeere to kind of sentiment\n    '''\n    plt.figure(figsize=(12, 6))\n    sns.barplot(data= data , x= 'counts', y= 'word')\n    plt.title(f'Top {num} words in {st}')\n    plt.show();\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.86627Z","iopub.execute_input":"2021-11-19T18:34:01.866823Z","iopub.status.idle":"2021-11-19T18:34:01.875809Z","shell.execute_reply.started":"2021-11-19T18:34:01.866783Z","shell.execute_reply":"2021-11-19T18:34:01.875154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Frequent words for each `less toxic data`","metadata":{}},{"cell_type":"code","source":"num = 30\nless_toxic_df = freq_words(df, 0, num)\nless_toxic_df.T","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:01.877189Z","iopub.execute_input":"2021-11-19T18:34:01.877465Z","iopub.status.idle":"2021-11-19T18:34:02.241464Z","shell.execute_reply.started":"2021-11-19T18:34:01.877428Z","shell.execute_reply":"2021-11-19T18:34:02.240567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_freq(less_toxic_df, 'less toxic', num)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:02.243053Z","iopub.execute_input":"2021-11-19T18:34:02.243338Z","iopub.status.idle":"2021-11-19T18:34:02.70591Z","shell.execute_reply.started":"2021-11-19T18:34:02.243301Z","shell.execute_reply":"2021-11-19T18:34:02.705209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Frequent words for each `more toxic data`","metadata":{}},{"cell_type":"code","source":"more_toxic_df = freq_words(df, 1, num)\nmore_toxic_df.T","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:02.70697Z","iopub.execute_input":"2021-11-19T18:34:02.707366Z","iopub.status.idle":"2021-11-19T18:34:03.061627Z","shell.execute_reply.started":"2021-11-19T18:34:02.707329Z","shell.execute_reply":"2021-11-19T18:34:03.060893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_freq(more_toxic_df, 'more toxic', num)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:03.065748Z","iopub.execute_input":"2021-11-19T18:34:03.065953Z","iopub.status.idle":"2021-11-19T18:34:03.522508Z","shell.execute_reply.started":"2021-11-19T18:34:03.065929Z","shell.execute_reply":"2021-11-19T18:34:03.521823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of top n-grams","metadata":{}},{"cell_type":"code","source":"def get_top_n_gram(corpus, score,  n_gram, top_n=None):\n    # list of splited senteces, which is just list of words\n    text = [word for sent in corpus['toxic_text'][corpus['target'] == float(score)] for word in sent.split()]    \n\n    grams = ngrams(text, n_gram)\n    grams = (' '.join(g) for g in grams)\n    num_of_grams = [words for words in grams]\n    freq_words = Counter(num_of_grams)\n    freq_words_sorted = sorted(freq_words.items(), key=lambda pair: pair[1], reverse=True)\n    freq_words_df = pd.DataFrame(freq_words_sorted[:top_n], columns=['word', 'counts'])\n    return freq_words_df[:top_n]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:03.523932Z","iopub.execute_input":"2021-11-19T18:34:03.524416Z","iopub.status.idle":"2021-11-19T18:34:03.532466Z","shell.execute_reply.started":"2021-11-19T18:34:03.524376Z","shell.execute_reply":"2021-11-19T18:34:03.531767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Two-grams for less toxic data\n","metadata":{}},{"cell_type":"code","source":"less_toxic_2_gram_df = get_top_n_gram(df, 0, 2, num)\nless_toxic_2_gram_df.T","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:03.533863Z","iopub.execute_input":"2021-11-19T18:34:03.534201Z","iopub.status.idle":"2021-11-19T18:34:04.713571Z","shell.execute_reply.started":"2021-11-19T18:34:03.534163Z","shell.execute_reply":"2021-11-19T18:34:04.712874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_freq(less_toxic_2_gram_df, 'less toxic', num)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:04.715035Z","iopub.execute_input":"2021-11-19T18:34:04.715512Z","iopub.status.idle":"2021-11-19T18:34:05.192688Z","shell.execute_reply.started":"2021-11-19T18:34:04.715473Z","shell.execute_reply":"2021-11-19T18:34:05.191932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Two-grams for more toxic data\n","metadata":{}},{"cell_type":"code","source":"more_toxic_2_gram_df = get_top_n_gram(df, 1, 2, num)\nmore_toxic_2_gram_df.T","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:05.193655Z","iopub.execute_input":"2021-11-19T18:34:05.193879Z","iopub.status.idle":"2021-11-19T18:34:06.364614Z","shell.execute_reply.started":"2021-11-19T18:34:05.193851Z","shell.execute_reply":"2021-11-19T18:34:06.363894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_freq(less_toxic_2_gram_df, 'more toxic', num)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:06.366014Z","iopub.execute_input":"2021-11-19T18:34:06.366287Z","iopub.status.idle":"2021-11-19T18:34:06.852967Z","shell.execute_reply.started":"2021-11-19T18:34:06.366251Z","shell.execute_reply":"2021-11-19T18:34:06.852212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explanation:\n\n**The data tells when we useing uni-gram, there's a huge different in words for each less and more toxic data, but when using bi-grams there's no different there!!!**\n\n**So in our TF-IDF we will pass the i for `ngram_range` in `TfidfVectorizer`**\n\n","metadata":{}},{"cell_type":"markdown","source":"## Word Cloud","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(cleaned_lees_toxic))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Less Toxic',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(cleaned_more_toxic))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('More Toxic',fontsize=40);","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:06.854312Z","iopub.execute_input":"2021-11-19T18:34:06.85472Z","iopub.status.idle":"2021-11-19T18:34:22.586517Z","shell.execute_reply.started":"2021-11-19T18:34:06.854678Z","shell.execute_reply":"2021-11-19T18:34:22.584076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:22.588099Z","iopub.execute_input":"2021-11-19T18:34:22.588613Z","iopub.status.idle":"2021-11-19T18:34:22.597912Z","shell.execute_reply.started":"2021-11-19T18:34:22.588573Z","shell.execute_reply":"2021-11-19T18:34:22.597152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['toxic_text'], df['target'], test_size = 0.2,  random_state=42)\nlen(X_train), len(y_train), len(X_test), len(y_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:22.599371Z","iopub.execute_input":"2021-11-19T18:34:22.599631Z","iopub.status.idle":"2021-11-19T18:34:22.625869Z","shell.execute_reply.started":"2021-11-19T18:34:22.599602Z","shell.execute_reply":"2021-11-19T18:34:22.625055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TF-IDF Vectorizer**","metadata":{}},{"cell_type":"code","source":"tf_idf = TfidfVectorizer(analyzer= 'word', max_features= 10000, ngram_range= (1, 1))\nX_train = tf_idf.fit_transform(X_train)\nX_test = tf_idf.transform(X_test)\nX_train.shape, X_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:22.626949Z","iopub.execute_input":"2021-11-19T18:34:22.627433Z","iopub.status.idle":"2021-11-19T18:34:25.432576Z","shell.execute_reply.started":"2021-11-19T18:34:22.627405Z","shell.execute_reply":"2021-11-19T18:34:25.431872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate Model Function:**","metadata":{}},{"cell_type":"code","source":"def model_Evaluate(model):\n    \n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n    \n    acc = accuracy_score(y_test, y_pred)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    \n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Non Toxic','Toxic']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n    return acc\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:25.433915Z","iopub.execute_input":"2021-11-19T18:34:25.434325Z","iopub.status.idle":"2021-11-19T18:34:25.443035Z","shell.execute_reply.started":"2021-11-19T18:34:25.434285Z","shell.execute_reply":"2021-11-19T18:34:25.442226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1- Logistic Regression","metadata":{}},{"cell_type":"code","source":"%%time\nlr = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\nlr.fit(X_train, y_train)\nlr_acc = model_Evaluate(lr)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:25.444316Z","iopub.execute_input":"2021-11-19T18:34:25.444642Z","iopub.status.idle":"2021-11-19T18:34:28.648482Z","shell.execute_reply.started":"2021-11-19T18:34:25.444606Z","shell.execute_reply":"2021-11-19T18:34:28.647723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2- LinearSVC","metadata":{}},{"cell_type":"code","source":"%%time\nSVCmodel = LinearSVC(C= 1, loss = 'hinge')\nSVCmodel.fit(X_train, y_train)\nSVC_acc = model_Evaluate(SVCmodel)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:28.649825Z","iopub.execute_input":"2021-11-19T18:34:28.650719Z","iopub.status.idle":"2021-11-19T18:34:31.315304Z","shell.execute_reply.started":"2021-11-19T18:34:28.650676Z","shell.execute_reply":"2021-11-19T18:34:31.31462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:31.316564Z","iopub.execute_input":"2021-11-19T18:34:31.316892Z","iopub.status.idle":"2021-11-19T18:34:31.326146Z","shell.execute_reply.started":"2021-11-19T18:34:31.316853Z","shell.execute_reply":"2021-11-19T18:34:31.325468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_sub = tf_idf.transform(test['text'])\npreds = lr.predict_proba(X_test_sub)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:31.327538Z","iopub.execute_input":"2021-11-19T18:34:31.328014Z","iopub.status.idle":"2021-11-19T18:34:32.026709Z","shell.execute_reply.started":"2021-11-19T18:34:31.327961Z","shell.execute_reply":"2021-11-19T18:34:32.025858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [int(p >= 0.5) for p in preds]\ntest['score'] = preds","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:32.028164Z","iopub.execute_input":"2021-11-19T18:34:32.028722Z","iopub.status.idle":"2021-11-19T18:34:32.043539Z","shell.execute_reply.started":"2021-11-19T18:34:32.028658Z","shell.execute_reply":"2021-11-19T18:34:32.042721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:32.044902Z","iopub.execute_input":"2021-11-19T18:34:32.045854Z","iopub.status.idle":"2021-11-19T18:34:32.060554Z","shell.execute_reply.started":"2021-11-19T18:34:32.045815Z","shell.execute_reply":"2021-11-19T18:34:32.059673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:32.062263Z","iopub.execute_input":"2021-11-19T18:34:32.062606Z","iopub.status.idle":"2021-11-19T18:34:32.208337Z","shell.execute_reply.started":"2021-11-19T18:34:32.062569Z","shell.execute_reply":"2021-11-19T18:34:32.20744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3- SVC\nIt takes a long time for get good results!","metadata":{}},{"cell_type":"code","source":"%%time\n# SVCmodel = LinearSVC(C= 1, loss = 'hinge')\nSVM = SVC(kernel = 'linear', gamma = 'auto', probability = True)\nSVM.fit(X_train, y_train)\nSVM_acc = model_Evaluate(SVM)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:32.676326Z","iopub.status.idle":"2021-11-19T18:34:32.677187Z","shell.execute_reply.started":"2021-11-19T18:34:32.676895Z","shell.execute_reply":"2021-11-19T18:34:32.67692Z"},"trusted":true},"execution_count":null,"outputs":[]}]}