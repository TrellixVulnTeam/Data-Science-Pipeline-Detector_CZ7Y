{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Librarires\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\n!pip install clean-text\nfrom tqdm import tqdm\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:49:12.322882Z","iopub.execute_input":"2021-12-22T16:49:12.323166Z","iopub.status.idle":"2021-12-22T16:49:25.720048Z","shell.execute_reply.started":"2021-12-22T16:49:12.323093Z","shell.execute_reply":"2021-12-22T16:49:25.719162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# collecting data from diffrent resources, removing duplicates and then merging them into a single dataframe\n\n# data from Jigsaw Multilingual Toxic Comment Classification\"\ndf1a = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv')\ndf1b = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv')\ndf1 = pd.concat([df1a,df1b])\ndf1 = df1.dropna()\ndf = df1[['comment_text','toxic']]\ndf['toxic'] = df['toxic'].apply(lambda x: int(x))\n# removing duplicates from data\ndf.drop_duplicates(subset =[\"comment_text\",'toxic'],keep = False, inplace = True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:49:25.723786Z","iopub.execute_input":"2021-12-22T16:49:25.724008Z","iopub.status.idle":"2021-12-22T16:49:36.099595Z","shell.execute_reply.started":"2021-12-22T16:49:25.723983Z","shell.execute_reply":"2021-12-22T16:49:36.098867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualizing Data Distribution\nfig = px.pie(values=[len(df[df['toxic']==0]),len(df[df['toxic']==1])], names=['Non-toxic','Toxic'], title='Distribution of Toxic and Non-toxic comments')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:49:36.100911Z","iopub.execute_input":"2021-12-22T16:49:36.101926Z","iopub.status.idle":"2021-12-22T16:49:36.907278Z","shell.execute_reply.started":"2021-12-22T16:49:36.101883Z","shell.execute_reply":"2021-12-22T16:49:36.906622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can clearly see that the data is unbalenced first we have balence it\ncounts =  df[df['toxic'] != 0].shape[0]\nnon_toxic = df[df['toxic']==0].sample(counts)\ntoxic = df[df['toxic']==1]\ndf = pd.concat([toxic,non_toxic])\nfig = px.pie(values=[len(df[df['toxic']==0]),len(df[df['toxic']==1])], names=['Non-toxic','Toxic'], title='Distribution of Toxic and Non-toxic comments')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:49:36.909456Z","iopub.execute_input":"2021-12-22T16:49:36.909954Z","iopub.status.idle":"2021-12-22T16:49:36.984863Z","shell.execute_reply.started":"2021-12-22T16:49:36.909917Z","shell.execute_reply":"2021-12-22T16:49:36.984032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we will create additional features from the given text which will help \n# us during the visualization of the model\n\n#1 Number of characters\ndf['no_of_char'] = df['comment_text'].progress_apply(lambda x:len(x))\n\n#2 Number of words\ndf['no_of_words'] = df['comment_text'].progress_apply(lambda x:len(x.split()))\n\n#3 Number of Capital Characters\ndf['no_of_cap_chars'] = df['comment_text'].progress_apply(lambda x:sum([1 for i in x if i.isupper()]))\n\n#4 Number of Capital words\ndf['no_of_cap_words'] = df['comment_text'].progress_apply(lambda x:sum([1 for i in x.split() if i.isupper()]))\n\n#5 Number of punctuation\ndef no_of_punc(text):\n    import string\n    punctuations=string.punctuation\n    d = 0\n    for i in text:\n        if i in punctuations:\n            d+=1\n    return d \n\ndf['no_of_punctuations'] = df['comment_text'].progress_apply(lambda x:no_of_punc(x))\n\n#6 number of stopwords\ndef count_stopwords(text):\n    stop_words = set(stopwords.words('english'))  \n    word_tokens = word_tokenize(text)\n    stopwords_x = [w for w in word_tokens if w in stop_words]\n    return len(stopwords_x)\ndf['no_of_stopwords'] = df['comment_text'].progress_apply(lambda x:count_stopwords(x))\n\n#7 number of unique words\ndf['no_of_unique_words'] = df['comment_text'].progress_apply(lambda x:len(set(x.split())))\n\n#8 avg word length\ndf['avg_word_length'] = round(df['no_of_char']/df['no_of_words'],3)\n\n#9 ratio of unique word and total words\ndf['unique_vs_words'] = round(df['no_of_unique_words']/df['no_of_words'],3)\n\n#10 ratio of stopwords and totalwords\ndf['stop_vs_words'] = round(df['no_of_stopwords']/df['no_of_words'],3)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:49:36.986361Z","iopub.execute_input":"2021-12-22T16:49:36.986607Z","iopub.status.idle":"2021-12-22T16:50:28.713474Z","shell.execute_reply.started":"2021-12-22T16:49:36.986575Z","shell.execute_reply":"2021-12-22T16:50:28.712743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# in this block we will do preprocessing on the text\n#lowering the text\ndf['comment_text'] = df['comment_text'].progress_apply(lambda x:x.lower())\n\n# removing punctuation\ndf['comment_text'] = df['comment_text'].progress_apply(lambda x:re.sub(\"[^-9A-Za-z ]\", \"\" , x))\n\n# tokenizeing \ndf['comment_text'] = df['comment_text'].progress_apply(lambda x:word_tokenize(x))\n\n# removing stopwords\nstop_words = set(stopwords.words('english'))\ndf['comment_text'] = df['comment_text'].progress_apply(lambda x:[i for i in x if i not in set(stopwords.words('english'))])\n\n# removing the epmty tokens\ndf = df[df['comment_text'].map(len)>0]\n\n# now cleaning text with help cleantext\nfrom cleantext import clean\n\ndef func90(s1):\n    s1 =\" \".join(s1)\n    clean_text = clean(s1, \n      fix_unicode=True, \n      to_ascii=True, \n      no_line_breaks=True,\n      no_urls=True, \n      no_numbers=True, \n      no_digits=True,  \n      no_currency_symbols=True, \n      no_punct=True, \n      replace_with_punct=\"\", \n      replace_with_url=\"\", \n      replace_with_number=\"\", \n      replace_with_digit=\"\", \n      replace_with_currency_symbol=\"\",\n      lang='en')\n    return word_tokenize(clean_text)\ndf['comment_text'] = df['comment_text'].progress_apply(lambda x:func90(x))\n\n# lemmatization\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\ndef func10(s):\n    text = [lemmatizer.lemmatize(i) for i in s]\n    return text\n\ndf['comment_text'] = df['comment_text'].progress_apply(lambda x:func10(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:50:28.714856Z","iopub.execute_input":"2021-12-22T16:50:28.715268Z","iopub.status.idle":"2021-12-22T16:57:09.905597Z","shell.execute_reply.started":"2021-12-22T16:50:28.715233Z","shell.execute_reply":"2021-12-22T16:57:09.90487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.area(df, y=\"no_of_char\", color=\"toxic\",title='Number of Characters in Comments (toxic/non-toxic)')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:09.906977Z","iopub.execute_input":"2021-12-22T16:57:09.907399Z","iopub.status.idle":"2021-12-22T16:57:10.189141Z","shell.execute_reply.started":"2021-12-22T16:57:09.907362Z","shell.execute_reply":"2021-12-22T16:57:10.188405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.area(df, y=\"no_of_words\", color=\"toxic\",title='Number of words in Comments (toxic/non-toxic)')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:10.190032Z","iopub.execute_input":"2021-12-22T16:57:10.19025Z","iopub.status.idle":"2021-12-22T16:57:10.808409Z","shell.execute_reply.started":"2021-12-22T16:57:10.190223Z","shell.execute_reply":"2021-12-22T16:57:10.807735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.area(df, y=\"no_of_cap_chars\", color=\"toxic\",title='Number of Capital Characters in Comments (toxic/non-toxic)')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:10.809872Z","iopub.execute_input":"2021-12-22T16:57:10.810365Z","iopub.status.idle":"2021-12-22T16:57:11.141267Z","shell.execute_reply.started":"2021-12-22T16:57:10.810329Z","shell.execute_reply":"2021-12-22T16:57:11.140661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.area(df, y=\"no_of_cap_words\", color=\"toxic\",title='Number of Capital Words in Comments (toxic/non-toxic)')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:11.144178Z","iopub.execute_input":"2021-12-22T16:57:11.144911Z","iopub.status.idle":"2021-12-22T16:57:11.395688Z","shell.execute_reply.started":"2021-12-22T16:57:11.144878Z","shell.execute_reply":"2021-12-22T16:57:11.394875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.area(df, y=\"no_of_punctuations\", color=\"toxic\",title='Number of Pucntuations in Comments (toxic/non-toxic)')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:11.397051Z","iopub.execute_input":"2021-12-22T16:57:11.3974Z","iopub.status.idle":"2021-12-22T16:57:11.659651Z","shell.execute_reply.started":"2021-12-22T16:57:11.397359Z","shell.execute_reply":"2021-12-22T16:57:11.658875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.area(df, y=\"no_of_stopwords\", color=\"toxic\",title='Number of StopWords in Comments (toxic/non-toxic)')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:11.661063Z","iopub.execute_input":"2021-12-22T16:57:11.661334Z","iopub.status.idle":"2021-12-22T16:57:11.91481Z","shell.execute_reply.started":"2021-12-22T16:57:11.661301Z","shell.execute_reply":"2021-12-22T16:57:11.913895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.area(df, y=\"no_of_unique_words\", color=\"toxic\",title='Number of Unique Words in Comments (toxic/non-toxic)')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:11.916394Z","iopub.execute_input":"2021-12-22T16:57:11.916719Z","iopub.status.idle":"2021-12-22T16:57:12.416059Z","shell.execute_reply.started":"2021-12-22T16:57:11.91668Z","shell.execute_reply":"2021-12-22T16:57:12.415246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.area(df, y=\"avg_word_length\", color=\"toxic\",title='Avg Words Length in Comments (toxic/non-toxic)')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:12.417491Z","iopub.execute_input":"2021-12-22T16:57:12.417797Z","iopub.status.idle":"2021-12-22T16:57:12.674582Z","shell.execute_reply.started":"2021-12-22T16:57:12.417746Z","shell.execute_reply":"2021-12-22T16:57:12.673979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.area(df, y=\"unique_vs_words\", color=\"toxic\",title='Unique vs StopWords in Comments (toxic/non-toxic)')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:12.675644Z","iopub.execute_input":"2021-12-22T16:57:12.676229Z","iopub.status.idle":"2021-12-22T16:57:12.933569Z","shell.execute_reply.started":"2021-12-22T16:57:12.67619Z","shell.execute_reply":"2021-12-22T16:57:12.932805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.area(df, y=\"stop_vs_words\", color=\"toxic\",title='StopWords vs Words in Comments (toxic/non-toxic)')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:12.934935Z","iopub.execute_input":"2021-12-22T16:57:12.935177Z","iopub.status.idle":"2021-12-22T16:57:13.200605Z","shell.execute_reply.started":"2021-12-22T16:57:12.935146Z","shell.execute_reply":"2021-12-22T16:57:13.199834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will make two models one simple ann and another one text cnn,\n# we wil train ann on feautres we created with help of text data\n# and text cnn will be trained on text data","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:13.202011Z","iopub.execute_input":"2021-12-22T16:57:13.202251Z","iopub.status.idle":"2021-12-22T16:57:13.205902Z","shell.execute_reply.started":"2021-12-22T16:57:13.202223Z","shell.execute_reply":"2021-12-22T16:57:13.205147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparing validation data given\n# seprating toxic data an giving them value\ndf2toxic = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndf2toxic['comment_text'] = df2toxic['more_toxic']\ndf2toxic = df2toxic.drop(['worker','less_toxic','more_toxic'],axis=1)\ndf2toxic['toxic'] = df2toxic['comment_text'].apply(lambda x:1)\n\n# seprating non toxic data and giving them their value\ndf2nontoxic = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndf2nontoxic['comment_text'] = df2nontoxic['less_toxic']\ndf2nontoxic = df2nontoxic.drop(['worker','less_toxic','more_toxic'],axis=1)\ndf2nontoxic['toxic'] = df2nontoxic['comment_text'].apply(lambda x:0)\n\n# merging the toxic and non toxic data\ndf2 = pd.concat([df2toxic,df2nontoxic])\n\n\n# we will do preprocessing like we did on train data\ndf2['comment_text'] = df2['comment_text'].progress_apply(lambda x:x.lower())\n# removing punctuation\ndf2['comment_text'] = df2['comment_text'].progress_apply(lambda x:re.sub(\"[^-9A-Za-z ]\", \"\" , x))\n# tokenizeing \ndf2['comment_text'] = df2['comment_text'].progress_apply(lambda x:word_tokenize(x))\n# removing stopwords\ndf2['comment_text'] = df2['comment_text'].progress_apply(lambda x:[i for i in x if i not in set(stopwords.words('english'))])\n\n# removing the epmty tokens\ndf2 = df2[df2['comment_text'].map(len)>0]\n# now cleaning text with help cleantext\ndf2['comment_text'] = df2['comment_text'].progress_apply(lambda x:func90(x))\n# lemmatization\ndf2['comment_text'] = df2['comment_text'].progress_apply(lambda x:func10(x))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:57:13.207424Z","iopub.execute_input":"2021-12-22T16:57:13.207693Z","iopub.status.idle":"2021-12-22T17:08:37.729713Z","shell.execute_reply.started":"2021-12-22T16:57:13.20766Z","shell.execute_reply":"2021-12-22T17:08:37.729004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train data\nx1 = df['comment_text']\ny1 = df['toxic']\n\n# valid data \nx2 = df2['comment_text']\ny2 = df2['toxic']\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=7000)\ntokenizer.fit_on_texts(x1)\nXcnn_train = tokenizer.texts_to_sequences(x1)\nXcnn_test = tokenizer.texts_to_sequences(x2)\nvocab_size = len(tokenizer.word_index) + 1 ","metadata":{"execution":{"iopub.status.busy":"2021-12-22T17:08:37.731133Z","iopub.execute_input":"2021-12-22T17:08:37.731549Z","iopub.status.idle":"2021-12-22T17:08:45.289687Z","shell.execute_reply.started":"2021-12-22T17:08:37.731513Z","shell.execute_reply":"2021-12-22T17:08:45.288932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nmaxlen = 100\nXcnn_train = pad_sequences(Xcnn_train, padding='post', maxlen=maxlen)\nXcnn_test = pad_sequences(Xcnn_test, padding='post', maxlen=maxlen)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T17:08:45.290859Z","iopub.execute_input":"2021-12-22T17:08:45.2927Z","iopub.status.idle":"2021-12-22T17:08:46.296434Z","shell.execute_reply.started":"2021-12-22T17:08:45.29267Z","shell.execute_reply":"2021-12-22T17:08:46.295672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding,Conv1D, GlobalMaxPooling1D,Dense,Dropout,LeakyReLU\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nembedding_dim = 200\nmodel = Sequential([\n    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n    Conv1D(128, 5),\n    LeakyReLU(alpha=0.05),\n    GlobalMaxPooling1D(),\n    Dense(10),\n    LeakyReLU(alpha=0.05),\n    Dense(1, activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2021-12-22T17:08:46.297759Z","iopub.execute_input":"2021-12-22T17:08:46.29803Z","iopub.status.idle":"2021-12-22T17:08:48.608301Z","shell.execute_reply.started":"2021-12-22T17:08:46.297997Z","shell.execute_reply":"2021-12-22T17:08:48.607549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-12-22T17:08:48.609814Z","iopub.execute_input":"2021-12-22T17:08:48.610099Z","iopub.status.idle":"2021-12-22T17:08:48.626541Z","shell.execute_reply.started":"2021-12-22T17:08:48.610049Z","shell.execute_reply":"2021-12-22T17:08:48.625904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary() ","metadata":{"execution":{"iopub.status.busy":"2021-12-22T17:08:48.627852Z","iopub.execute_input":"2021-12-22T17:08:48.628113Z","iopub.status.idle":"2021-12-22T17:08:48.638813Z","shell.execute_reply.started":"2021-12-22T17:08:48.628082Z","shell.execute_reply":"2021-12-22T17:08:48.637887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_perfomence = model.fit(Xcnn_train,\n    y=y1,\n    validation_data=(Xcnn_test,\n    y2),\n    epochs=15,\n    callbacks=EarlyStopping(patience=3, \n                   monitor='loss', \n                   restore_best_weights=True, \n                   mode='min', \n                   verbose=1)\n )","metadata":{"execution":{"iopub.status.busy":"2021-12-22T17:08:48.640571Z","iopub.execute_input":"2021-12-22T17:08:48.640866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_data(datah):\n    import plotly.express as px\n    fig = px.line(datah.history, y=['accuracy','val_accuracy'],title='Trainng & Validation accuracy')\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vis_data(model_perfomence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}