{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport gensim\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nnltk.download('stopwords')\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Flatten, Dropout, Dense, LSTM, Embedding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-11-23T17:24:20.7232Z","iopub.execute_input":"2021-11-23T17:24:20.724274Z","iopub.status.idle":"2021-11-23T17:24:42.920195Z","shell.execute_reply.started":"2021-11-23T17:24:20.724103Z","shell.execute_reply":"2021-11-23T17:24:42.919454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = '../input/'","metadata":{"execution":{"iopub.status.busy":"2021-11-23T17:24:42.921889Z","iopub.execute_input":"2021-11-23T17:24:42.922132Z","iopub.status.idle":"2021-11-23T17:24:42.925803Z","shell.execute_reply.started":"2021-11-23T17:24:42.922097Z","shell.execute_reply":"2021-11-23T17:24:42.92519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RidgeRegression Ensemble","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import rankdata\n\ndef ridge_cv (vec, X, y, X_test, folds, stratified ):\n    kf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=123)\n    val_scores = []\n    rmse_scores = []\n    X_less_toxics = []\n    X_more_toxics = []\n\n    preds = []\n    for fold, (train_index,val_index) in enumerate(kf.split(X,stratified)):\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]\n        model = Ridge()\n        model.fit(X_train, y_train)\n\n        rmse_score = mean_squared_error ( model.predict (X_val), y_val, squared = False) \n        rmse_scores.append (rmse_score)\n\n        X_less_toxic = vec.transform(df_val['less_toxic'])\n        X_more_toxic = vec.transform(df_val['more_toxic'])\n\n        p1 = model.predict(X_less_toxic)\n        p2 = model.predict(X_more_toxic)\n\n        X_less_toxics.append ( p1 )\n        X_more_toxics.append ( p2 )\n\n        # Validation Accuracy\n        val_acc = (p1< p2).mean()\n        val_scores.append(val_acc)\n\n        pred = model.predict (X_test)\n        preds.append (pred)\n\n        print(f\"FOLD:{fold}, rmse_fold:{rmse_score:.5f}, val_acc:{val_acc:.5f}\")\n\n    mean_val_acc = np.mean (val_scores)\n    mean_rmse_score = np.mean (rmse_scores)\n\n    p1 = np.mean ( np.vstack(X_less_toxics), axis=0 )\n    p2 = np.mean ( np.vstack(X_more_toxics), axis=0 )\n\n    val_acc = (p1< p2).mean()\n\n    print(f\"OOF: val_acc:{val_acc:.5f}, mean val_acc:{mean_val_acc:.5f}, mean rmse_score:{mean_rmse_score:.5f}\")\n    \n    preds = np.mean ( np.vstack(preds), axis=0 )\n    \n    return p1, p2, preds","metadata":{"execution":{"iopub.status.busy":"2021-11-23T17:24:42.927742Z","iopub.execute_input":"2021-11-23T17:24:42.928249Z","iopub.status.idle":"2021-11-23T17:24:42.943133Z","shell.execute_reply.started":"2021-11-23T17:24:42.928212Z","shell.execute_reply":"2021-11-23T17:24:42.942326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = pd.read_csv(base_path + 'jigsaw-toxic-severity-rating/validation_data.csv')\ndf_test = pd.read_csv(base_path + 'jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T17:24:42.944455Z","iopub.execute_input":"2021-11-23T17:24:42.944914Z","iopub.status.idle":"2021-11-23T17:24:43.208875Z","shell.execute_reply.started":"2021-11-23T17:24:42.944856Z","shell.execute_reply":"2021-11-23T17:24:43.208111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLDS = 5\ndef TfidfVec(df):\n    \n    vec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6) )\n    X = vec.fit_transform(df['text'])\n    y = df[\"y\"].values\n    X_test = vec.transform(df_test['text'])\n    \n    return vec, X, y, X_test","metadata":{"execution":{"iopub.status.busy":"2021-11-23T17:30:58.551186Z","iopub.execute_input":"2021-11-23T17:30:58.55149Z","iopub.status.idle":"2021-11-23T17:30:58.557845Z","shell.execute_reply.started":"2021-11-23T17:30:58.55146Z","shell.execute_reply":"2021-11-23T17:30:58.55605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jf_train_df = pd.read_csv(base_path + \"jigsaw-toxic-comment-classification-challenge/train.csv\")\nprint(f\"jf_train_df:{jf_train_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-23T17:24:43.219741Z","iopub.execute_input":"2021-11-23T17:24:43.220002Z","iopub.status.idle":"2021-11-23T17:24:44.10388Z","shell.execute_reply.started":"2021-11-23T17:24:43.219967Z","shell.execute_reply":"2021-11-23T17:24:44.103085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic = 1.0\nsevere_toxic = 2.0\nobscene = 1.0\nthreat = 1.0\ninsult = 1.0\nidentity_hate = 2.0\n\ndef create_train (df):\n    df['y'] = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].max(axis=1)\n    df['y'] = df[\"y\"]+df['severe_toxic']*severe_toxic\n    df['y'] = df[\"y\"]+df['obscene']*obscene\n    df['y'] = df[\"y\"]+df['threat']*threat\n    df['y'] = df[\"y\"]+df['insult']*insult\n    df['y'] = df[\"y\"]+df['identity_hate']*identity_hate\n    \n    \n    \n    df = df[['comment_text', 'y', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].rename(columns={'comment_text': 'text'})\n\n    #undersample non toxic comments  on Toxic Comment Classification Challenge\n    min_len = (df['y'] >= 1).sum()\n    df_y0_undersample = df[df['y'] == 0].sample(n=int(min_len*1.5),random_state=201)\n    df = pd.concat([df[df['y'] >= 1], df_y0_undersample])\n                                                \n    return df\n \njf_train_df = create_train (jf_train_df)\nprint(jf_train_df['y'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-11-23T17:24:44.107895Z","iopub.execute_input":"2021-11-23T17:24:44.111272Z","iopub.status.idle":"2021-11-23T17:24:44.226896Z","shell.execute_reply.started":"2021-11-23T17:24:44.111192Z","shell.execute_reply":"2021-11-23T17:24:44.225836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec, X, y, X_test = TfidfVec(jf_train_df)\nstratified = np.around ( y )\njf_p1, jf_p2, jf_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )","metadata":{"execution":{"iopub.status.busy":"2021-11-23T17:31:03.906486Z","iopub.execute_input":"2021-11-23T17:31:03.907019Z","iopub.status.idle":"2021-11-23T17:35:04.475469Z","shell.execute_reply.started":"2021-11-23T17:31:03.906982Z","shell.execute_reply":"2021-11-23T17:35:04.474674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"js_train_df = pd.read_csv(base_path + \"jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\nprint(f\"js_train_df:{js_train_df.shape}\")\njs_train_df = js_train_df.query (\"toxicity_annotator_count > 5\")\nprint(f\"juc_train_df:{js_train_df.shape}\")\n\njs_train_df['y'] = js_train_df[[ 'severe_toxicity', 'obscene', 'sexual_explicit','identity_attack', 'insult', 'threat']].sum(axis=1)\n\njs_train_df['y'] = js_train_df.apply(lambda row: row[\"target\"] if row[\"target\"] <= 0.5 else row[\"y\"] , axis=1)\njs_train_df = js_train_df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\nmin_len = (js_train_df['y'] > 0.5).sum()\ndf_y0_undersample = js_train_df[js_train_df['y'] <= 0.5].sample(n=int(min_len*1.5),random_state=201)\njs_train_df = pd.concat([js_train_df[js_train_df['y'] > 0.5], df_y0_undersample])\n\nprint(js_train_df['y'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-11-23T17:35:04.477311Z","iopub.execute_input":"2021-11-23T17:35:04.477789Z","iopub.status.idle":"2021-11-23T17:35:37.056762Z","shell.execute_reply.started":"2021-11-23T17:35:04.477746Z","shell.execute_reply":"2021-11-23T17:35:37.054322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec, X, y, X_test = TfidfVec(js_train_df)\n\nstratified = (np.around (y, decimals=1)*10).astype(int)\njs_p1, js_p2, js_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )","metadata":{"execution":{"iopub.status.busy":"2021-11-23T17:35:37.058239Z","iopub.execute_input":"2021-11-23T17:35:37.058877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rud_df = pd.read_csv(base_path + \"ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\nprint(f\"rudd_df:{rud_df.shape}\")\nrud_df['y'] = rud_df['offensiveness_score'].map(lambda x: 0.0 if x <=0 else x)\nrud_df = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\nmin_len = (rud_df['y'] < 0.5).sum()\nprint(rud_df['y'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec, X, y, X_test = TfidfVec(rud_df)\n\nstratified = (np.around ( y, decimals = 1  )*10).astype(int)\nrud_p1, rud_p2, rud_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# jigsaw-toxic-comment-classification-challenge Dataset","metadata":{}},{"cell_type":"code","source":"jf_train_df = pd.read_csv(base_path + 'jigsaw-toxic-comment-classification-challenge/train.csv')\njf_train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jf_train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jf_train_df['toxicity'] = (jf_train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0).astype(int)\njf_train_df = jf_train_df[['comment_text', 'toxicity']].rename(columns={'comment_text': 'text'})\njf_train_df.toxicity.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jf_train_df = jf_train_df[['text', 'toxicity']]\njf_train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Unintended Bias in Toxicity Classification Dataset","metadata":{}},{"cell_type":"code","source":"js_train_df = pd.read_csv(base_path + 'jigsaw-unintended-bias-in-toxicity-classification/train.csv')\njs_train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"js_train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"js_train_df['toxicity'] = (js_train_df[['target', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']].sum(axis=1) > 0).astype(int)\njs_train_df = js_train_df[['comment_text', 'toxicity']].rename(columns={'comment_text': 'text'})\njs_train_df.toxicity.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"js_train_df = js_train_df[['text', 'toxicity']]\njs_train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Ruddit: Norms of Offensiveness for English Reddit Comments Dataset","metadata":{}},{"cell_type":"code","source":"rud_df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\nrud_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rud_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rud_df['toxicity'] = rud_df['offensiveness_score'].map(lambda x: 0 if x <=0 else 1)\nrud_df = rud_df[['txt', 'toxicity']].rename(columns={'txt': 'text'})\nrud_df.toxicity.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rud_df = rud_df[['text', 'toxicity']]\nrud_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make one train data set","metadata":{}},{"cell_type":"code","source":"df = pd.concat([jf_train_df, js_train_df, rud_df])\ndf.toxicity.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sampling\nmin_len = (df['toxicity'] == 1).sum()\ndf_undersample = df[df['toxicity'] == 0].sample(n=min_len, random_state=201)\ndf = pd.concat([df_undersample, df[df['toxicity'] == 1]])\ndf = shuffle(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.toxicity.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.text = df.text.map(lambda x:x.replace('\\n', ' '))\ndf.text[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Pre-Processing","metadata":{}},{"cell_type":"code","source":"y = df.toxicity\nx = df.drop('toxicity', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = x.copy()\ntexts.reset_index(inplace=True, drop=True)\ntexts.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sys.getrecursionlimit())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.setrecursionlimit(6000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stemming","metadata":{}},{"cell_type":"code","source":"ps = PorterStemmer()\ncorpus = []\n\nfor i in tqdm(range(0, len(texts))):\n    cleaned = re.sub('[^a-zA-Z]', ' ', texts['text'][i])\n    cleaned = cleaned.lower().split()\n    \n    cleaned = [ps.stem(word) for word in cleaned if not word in stopwords.words('english')]\n    cleaned = ' '.join(cleaned)\n    corpus.append(cleaned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding","metadata":{}},{"cell_type":"code","source":"DIM = 100\n\nX = [d.split() for d in corpus]\nw2v_model = gensim.models.Word2Vec(sentences = X, vector_size = DIM, window = 10, min_count = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(w2v_model.wv.key_to_index.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.most_similar('toxic')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tokenizer.texts_to_sequences(X)\nX[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pad_sequences(X, padding='pre', maxlen=20)\nX[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1\nvocab = tokenizer.word_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_weights_matrix(model):\n    weights_matrix = np.zeros((vocab_size, DIM))\n    \n    for word, i in vocab.items():\n        weights_matrix[i] = model.wv[word]\n        \n    return weights_matrix\n\nembedding_vectors = get_weights_matrix(w2v_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling & Training","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(vocab_size, output_dim=DIM, weights=[embedding_vectors], input_length=20))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(64))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1, activation='linear'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='mean_squared_error', optimizer='adam', metrics='accuracy')\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n\nes = EarlyStopping(patience=3,\n                  monitor='loss',\n                  restore_best_weights=True,\n                  mode='min',\n                  verbose=1)\n\nhist = model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs=10, callbacks=es, batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n\n# visualize the models accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Processing predict value for Ensemble","metadata":{}},{"cell_type":"code","source":"# tokenizer for LSTM\nX_less_toxic = tokenizer.texts_to_sequences(df_val['less_toxic'])\nX_less_toxic = pad_sequences(X_less_toxic, maxlen=20)\nX_more_toxic = tokenizer.texts_to_sequences(df_val['more_toxic'])\nX_more_toxic = pad_sequences(X_more_toxic, maxlen=20)\nnew_text = tokenizer.texts_to_sequences(df_test.text)\nnew_text = pad_sequences(new_text, maxlen=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predict value to list for Ensemble\nlstm_p1 = model.predict(X_less_toxic)\nlstm_p2 = model.predict(X_more_toxic)\nlstm_preds = np.hstack(model.predict(new_text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"jf_max = max(jf_p1.max() , jf_p2.max())\njs_max = max(js_p1.max() , js_p2.max())\nrud_max = max(rud_p1.max() , rud_p2.max())\nlstm_max = max(lstm_p1.max(), lstm_p2.max())\n\n\np1 = jf_p1/jf_max + js_p1/js_max + rud_p1/rud_max + lstm_p1/lstm_max\np2 = jf_p2/jf_max + js_p2/js_max + rud_p2/rud_max + lstm_p2/lstm_max\n\nval_acc = (p1 < p2).mean()\nprint(f\"Ensemble: val_acc:{val_acc:.5f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = jf_preds/jf_max + js_preds/js_max + rud_preds/rud_max + lstm_preds/lstm_max\n## to enforce unique values on score\ndf_test['score'] = rankdata(score, method='ordinal')\n\ndf_test[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}