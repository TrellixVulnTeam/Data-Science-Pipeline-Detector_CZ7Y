{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jigsaw Toxicity: Word2Vec+TFIDF Training\n## Table of Contents\n* [1. Overview](#1.)\n* [2. Configuration](#2.)\n* [3. Setup](#3.)\n* [4. Import datasets](#4.)\n* [5. EDA & Preprocessing](#5.)\n    * [5.1 Select trainng data](#5.1)\n    * [5.2 Text Preprocessing Function](#5.2)\n    * [5.3 TF-IDF Vectorization](#5.3)\n    * [5.4 Word2Vec Vectorization](#5.4)\n    * [5.5 Train Validation Split](#5.5)\n    * [5.6 Create TensorFlow Dataset](#5.6)\n    * [5.7 Calculate Class weight](#5.7)\n* [6. Model Development](#6.)\n    * [6.1 FNet Encoder](#6.1)\n    * [6.2 Positional Embedding](#6.2)\n    * [6.3 Word2Vec FNet Model](#6.3)\n    * [6.4 TFIDF DNN Model](#6.4)\n    * [6.5 The Whole Model](#6.5)\n    * [6.6 Model Training](#6.6)\n    * [6.7 Evaluation](#6.7)\n* [7. Submission](#7.)\n* [8. References](#8.)","metadata":{"id":"QLQhXw_-L3bw"}},{"cell_type":"markdown","source":"<font color=\"red\" size=\"3\">If you found it useful and would like to back me up, just upvote.</font>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.\"></a>\n## 1. Overview\nIn my previous notebooks, I build Jigsaw Toxicity Model with [FNet](https://www.kaggle.com/lonnieqin/jigsaw-toxicity-training-with-fnet) using Word2Vec Vectorizatoin and [DNN](https://www.kaggle.com/lonnieqin/tf-idf-vectorization-with-keras) using TFIDF Vectorzation. How about combining this two Models together? I will try it in this notebook.\n\nThis Model is a binary classfication Model, ranking of toxicity can be calcualated via probability of binary classficiation.\n\nI use dataset from [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) and combine with more toxic data in this dataset for training.\n\nI will keep track of three Model: Model with Best Accuracy, Model with Best AUC, and Latest Model. So I can use them for inference and try different ensemble method to get a better score.","metadata":{"id":"3br121fsL3by"}},{"cell_type":"markdown","source":"<a id=\"2.\"></a>\n## 2. Configuration","metadata":{"id":"23-sSaFsL3bz"}},{"cell_type":"code","source":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    sequence_length = 100 # Length of sequence\n    batch_size = 1024\n    validation_split = 0.15\n    embed_dim = 256\n    latent_dim = 256\n    epochs = 50 # Number of Epochs to train\n    best_auc_model_path = \"model_best_auc.tf\"\n    best_acc_model_path = \"model_best_acc.tf\"\n    lastest_model_path = \"model_latest.tf\"\nconfig = Config()","metadata":{"id":"gmp8ivzzL3bz","execution":{"iopub.status.busy":"2021-11-16T18:16:11.231912Z","iopub.execute_input":"2021-11-16T18:16:11.23227Z","iopub.status.idle":"2021-11-16T18:16:11.249336Z","shell.execute_reply.started":"2021-11-16T18:16:11.232192Z","shell.execute_reply":"2021-11-16T18:16:11.248525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.\"></a>\n## 3. Setup","metadata":{"id":"98JWyaiDL3b0"}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport pathlib\nimport random\nimport string\nimport re\nimport sys\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import TweetTokenizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom scipy.stats import rankdata\nimport json","metadata":{"id":"oda23ThKL3b1","execution":{"iopub.status.busy":"2021-11-16T18:16:11.251122Z","iopub.execute_input":"2021-11-16T18:16:11.251604Z","iopub.status.idle":"2021-11-16T18:16:17.433208Z","shell.execute_reply.started":"2021-11-16T18:16:11.251564Z","shell.execute_reply":"2021-11-16T18:16:17.432346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.\"></a>\n## 4. Import datasets","metadata":{"id":"7czW02EaL3b1"}},{"cell_type":"code","source":"validation_data = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\")\nvalidation_data.head()","metadata":{"id":"PLs2vAotL3b2","outputId":"4f6891de-5362-40cf-b6ca-0af0a4220b3d","execution":{"iopub.status.busy":"2021-11-16T18:16:17.436149Z","iopub.execute_input":"2021-11-16T18:16:17.436421Z","iopub.status.idle":"2021-11-16T18:16:17.972124Z","shell.execute_reply.started":"2021-11-16T18:16:17.436386Z","shell.execute_reply":"2021-11-16T18:16:17.971448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:17.974337Z","iopub.execute_input":"2021-11-16T18:16:17.974832Z","iopub.status.idle":"2021-11-16T18:16:19.64214Z","shell.execute_reply.started":"2021-11-16T18:16:17.974792Z","shell.execute_reply":"2021-11-16T18:16:19.641442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.\"></a>\n## 5. EDA & Preprocessing","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.1\"></a>\n### 5.1 Select Traning Data","metadata":{}},{"cell_type":"markdown","source":"One of the way is to label `less_toxic` as 0 and `more_toxic` as 1, and FNet can get 0.749 score. I tried grouping the duplicated comment together and replace the label with average value, but got a worse 0.49 score instead. I also tried to convert the average value to a class value, but still can't learn any important information from it. So I am going to keep every variable we may use in the future to a data table.\n\n\nAnother way is to use external dataset from [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). Since there is a class imbalance problem, I also add more_toxic data from this dataset and label it as 1.","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"use_external_dataset = True\nif use_external_dataset:\n    train = train[[\"comment_text\", \"toxic\"]]\n    train.columns = [\"text\", \"label\"]\n    # Add More toxic data to mitigate class imbalance problem\n    train = train.append(pd.DataFrame({\"text\": validation_data[\"more_toxic\"], \"label\": [1] * len(validation_data)}))\nelse:\n    data = pd.DataFrame({\"text\": validation_data[\"less_toxic\"], \"label\": [0] * len(validation_data)})\n    data = data.append(pd.DataFrame({\"text\": validation_data[\"more_toxic\"], \"label\": [1] * len(validation_data)}))\n    text = data[\"text\"].unique()\n    grouped = data.groupby(\"text\")\n    label = list(grouped.mean()[\"label\"])\n    text_label_dict = dict({key: value for key, value in zip(text, label)})\n    index_label = sorted(grouped.mean()[\"label\"].unique())\n    data[\"average_value\"] = data[\"text\"].apply(lambda text: text_label_dict[text])\n    data[\"class\"] = data[\"average_value\"].apply(lambda value: index_label.index(value))\n    classes = sorted(data[\"class\"].unique())\n    print(\"Classes:\", classes)\n    train = data[[\"text\", \"label\"]]","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:19.643644Z","iopub.execute_input":"2021-11-16T18:16:19.644133Z","iopub.status.idle":"2021-11-16T18:16:19.678759Z","shell.execute_reply.started":"2021-11-16T18:16:19.644081Z","shell.execute_reply":"2021-11-16T18:16:19.677942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2\"></a>\n### 5.2 Text Preprocessing Function ","metadata":{}},{"cell_type":"code","source":"def custom_standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n    text = tf.strings.regex_replace(\n        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n    )\n    text = tf.strings.regex_replace(text, f\"[0-9]+\", \" \")\n    text = tf.strings.regex_replace(text, f\"[ ]+\", \" \")\n    text = tf.strings.strip(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:19.681553Z","iopub.execute_input":"2021-11-16T18:16:19.682112Z","iopub.status.idle":"2021-11-16T18:16:19.687778Z","shell.execute_reply.started":"2021-11-16T18:16:19.682074Z","shell.execute_reply":"2021-11-16T18:16:19.687065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.3\"></a>\n### 5.3 TF-IDF Vectorization","metadata":{}},{"cell_type":"code","source":"tfidf_vectozier = layers.TextVectorization(\n    standardize=custom_standardization, \n    max_tokens=config.vocab_size, \n    output_mode=\"tf-idf\", \n    ngrams=2\n)\nwith tf.device(\"CPU\"):\n    # A bug that prevents this from running on GPU for now.\n    tfidf_vectozier.adapt(list(train[\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:19.689036Z","iopub.execute_input":"2021-11-16T18:16:19.689432Z","iopub.status.idle":"2021-11-16T18:16:53.288309Z","shell.execute_reply.started":"2021-11-16T18:16:19.689395Z","shell.execute_reply":"2021-11-16T18:16:53.285875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.4\"></a>\n### 5.4 Word2Vec Vectorization","metadata":{}},{"cell_type":"code","source":"word2vec_vectozier = layers.TextVectorization(\n    standardize=custom_standardization, \n    max_tokens=config.vocab_size, \n    output_sequence_length=config.sequence_length\n)\nwith tf.device(\"CPU\"):\n    word2vec_vectozier.adapt(train[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:53.289529Z","iopub.status.idle":"2021-11-16T18:16:53.290128Z","shell.execute_reply.started":"2021-11-16T18:16:53.289875Z","shell.execute_reply":"2021-11-16T18:16:53.2899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.5\"></a>\n### 5.5 Train Validation Split","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train[\"text\"], train[\"label\"], test_size=config.validation_split)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:53.291312Z","iopub.status.idle":"2021-11-16T18:16:53.291917Z","shell.execute_reply.started":"2021-11-16T18:16:53.291659Z","shell.execute_reply":"2021-11-16T18:16:53.291685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_val.shape, y_val.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:53.293004Z","iopub.status.idle":"2021-11-16T18:16:53.293563Z","shell.execute_reply.started":"2021-11-16T18:16:53.293322Z","shell.execute_reply":"2021-11-16T18:16:53.293348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.6\"></a>\n### 5.6 Create TensorFlow Dataset","metadata":{}},{"cell_type":"code","source":"def make_dataset(X, y, batch_size, mode):\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    if mode == \"train\":\n       dataset = dataset.shuffle(256) \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.cache().prefetch(16).repeat(1)\n    return dataset","metadata":{"id":"KTzWxxsAL3b4","execution":{"iopub.status.busy":"2021-11-16T18:16:53.294649Z","iopub.status.idle":"2021-11-16T18:16:53.295272Z","shell.execute_reply.started":"2021-11-16T18:16:53.29498Z","shell.execute_reply":"2021-11-16T18:16:53.29501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = make_dataset(X_train, y_train, batch_size=config.batch_size, mode=\"train\")\nvalid_ds = make_dataset(X_val, y_val, batch_size=config.batch_size, mode=\"valid\")","metadata":{"id":"oAP4kRzzL3b4","execution":{"iopub.status.busy":"2021-11-16T18:16:53.296486Z","iopub.status.idle":"2021-11-16T18:16:53.29704Z","shell.execute_reply.started":"2021-11-16T18:16:53.296799Z","shell.execute_reply":"2021-11-16T18:16:53.296824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see what this data look like.","metadata":{}},{"cell_type":"code","source":"for batch in train_ds.take(1):\n    print(batch)","metadata":{"id":"BMjiEEa9L3b5","outputId":"598a1ca1-c02c-4be2-ec1e-d0632cda7d07","execution":{"iopub.status.busy":"2021-11-16T18:16:53.298144Z","iopub.status.idle":"2021-11-16T18:16:53.298699Z","shell.execute_reply.started":"2021-11-16T18:16:53.298453Z","shell.execute_reply":"2021-11-16T18:16:53.298478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.7\"></a>\n### 5.7  Calculate Class weight","metadata":{}},{"cell_type":"code","source":"class_weight =  1 / train[\"label\"].value_counts(normalize=True)\nclass_weight = dict(class_weight / class_weight.sum())\nclass_weight","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:53.299943Z","iopub.status.idle":"2021-11-16T18:16:53.300543Z","shell.execute_reply.started":"2021-11-16T18:16:53.300307Z","shell.execute_reply":"2021-11-16T18:16:53.300331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.\"></a>\n## 6. Model Development","metadata":{"id":"XSXAmMV0L3b6"}},{"cell_type":"markdown","source":"<a id=\"6.1\"></a>\n### 6.1 FNet Encoder","metadata":{"id":"n1qyTb9AL3b6"}},{"cell_type":"code","source":"class FNetEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, dropout_rate=0.1, **kwargs):\n        super(FNetEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n\n    def call(self, inputs):\n        # Casting the inputs to complex64\n        inp_complex = tf.cast(inputs, tf.complex64)\n        # Projecting the inputs to the frequency domain using FFT2D and\n        # extracting the real part of the output\n        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n        proj_input = self.layernorm_1(inputs + fft)\n        proj_output = self.dense_proj(proj_input)\n       \n        layer_norm = self.layernorm_2(proj_input + proj_output)\n        return layer_norm","metadata":{"id":"fRXW_RaML3b6","execution":{"iopub.status.busy":"2021-11-16T18:16:53.301779Z","iopub.status.idle":"2021-11-16T18:16:53.302416Z","shell.execute_reply.started":"2021-11-16T18:16:53.302157Z","shell.execute_reply":"2021-11-16T18:16:53.302184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2\"></a>\n### 6.2 Positional Embedding","metadata":{"id":"ISzHta1dL3b7"}},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","metadata":{"id":"kUiNiGv6L3b7","execution":{"iopub.status.busy":"2021-11-16T18:16:53.303464Z","iopub.status.idle":"2021-11-16T18:16:53.304011Z","shell.execute_reply.started":"2021-11-16T18:16:53.303779Z","shell.execute_reply":"2021-11-16T18:16:53.303804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.3\"></a>\n### 6.3 Word2Vec FNet Model","metadata":{"id":"2VUKFd9sL3b9"}},{"cell_type":"code","source":"def get_word2vec_model(config, inputs):\n    x = word2vec_vectozier(inputs)\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(x)\n    x = FNetEncoder(config.embed_dim, config.latent_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.5)(x)\n    for i in range(3):\n        x = layers.Dense(100, activation=\"relu\")(x)\n        x = layers.Dropout(0.3)(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:53.305069Z","iopub.status.idle":"2021-11-16T18:16:53.305627Z","shell.execute_reply.started":"2021-11-16T18:16:53.305385Z","shell.execute_reply":"2021-11-16T18:16:53.305409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.4\"></a>\n### 6.4 TFIDF DNN Model","metadata":{}},{"cell_type":"code","source":"def get_tfidf_model(config, inputs):\n    x = tfidf_vectozier(inputs)\n    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:53.306702Z","iopub.status.idle":"2021-11-16T18:16:53.307253Z","shell.execute_reply.started":"2021-11-16T18:16:53.307011Z","shell.execute_reply":"2021-11-16T18:16:53.307035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.5\"></a>\n### 6.5 The Whole Model","metadata":{}},{"cell_type":"code","source":"def get_model(config):\n    inputs = keras.Input(shape=(None, ), dtype=\"string\", name=\"inputs\")\n    word2vec_x = get_word2vec_model(config, inputs)\n    tfidf_x = get_tfidf_model(config, inputs)\n    x = layers.Concatenate()([word2vec_x, tfidf_x])\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, output, name=\"model\")\n    return model","metadata":{"id":"dp19cOEqL3b9","execution":{"iopub.status.busy":"2021-11-16T18:16:53.308344Z","iopub.status.idle":"2021-11-16T18:16:53.3089Z","shell.execute_reply.started":"2021-11-16T18:16:53.308661Z","shell.execute_reply":"2021-11-16T18:16:53.308687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(config)","metadata":{"id":"T-QhEpKoL3b-","execution":{"iopub.status.busy":"2021-11-16T18:16:53.309948Z","iopub.status.idle":"2021-11-16T18:16:53.310507Z","shell.execute_reply.started":"2021-11-16T18:16:53.310274Z","shell.execute_reply":"2021-11-16T18:16:53.310299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"ie_rklX2L3b-","outputId":"ba08f590-0cf9-4c61-a9df-0244391b2f05","execution":{"iopub.status.busy":"2021-11-16T18:16:53.311548Z","iopub.status.idle":"2021-11-16T18:16:53.312096Z","shell.execute_reply.started":"2021-11-16T18:16:53.311867Z","shell.execute_reply":"2021-11-16T18:16:53.311891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the Model.","metadata":{"id":"x_O7FfflL3b-"}},{"cell_type":"code","source":"keras.utils.plot_model(model, show_shapes=True)","metadata":{"id":"6Tm1nG0CL3b-","outputId":"0cd78731-3c68-4fdf-f8c7-9b0b2512013d","execution":{"iopub.status.busy":"2021-11-16T18:16:53.31317Z","iopub.status.idle":"2021-11-16T18:16:53.313718Z","shell.execute_reply.started":"2021-11-16T18:16:53.313476Z","shell.execute_reply":"2021-11-16T18:16:53.3135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.6\"></a>\n### 6.6 Model Training","metadata":{"id":"NFtSL6-3L3b-"}},{"cell_type":"code","source":"model.compile(\n    \"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.AUC()]\n)","metadata":{"id":"3wMy6AcIL3b_","execution":{"iopub.status.busy":"2021-11-16T18:16:53.314777Z","iopub.status.idle":"2021-11-16T18:16:53.315332Z","shell.execute_reply.started":"2021-11-16T18:16:53.31509Z","shell.execute_reply":"2021-11-16T18:16:53.315125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_checkpoint = keras.callbacks.ModelCheckpoint(config.best_acc_model_path, monitor=\"val_accuracy\",save_weights_only=True, save_best_only=True)\nauc_checkpoint = keras.callbacks.ModelCheckpoint(config.best_auc_model_path, monitor=\"val_auc\",save_weights_only=True, save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(patience=10)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(patience=5, min_delta=1e-4, min_lr=1e-6)\nmodel.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[acc_checkpoint, auc_checkpoint, reduce_lr], class_weight=class_weight)\nmodel.save_weights(config.lastest_model_path)","metadata":{"id":"YKjayqSSL3b_","outputId":"0a6677a7-60a0-4eda-e992-f0807e4d8f6e","execution":{"iopub.status.busy":"2021-11-16T18:16:53.316395Z","iopub.status.idle":"2021-11-16T18:16:53.316961Z","shell.execute_reply.started":"2021-11-16T18:16:53.316724Z","shell.execute_reply":"2021-11-16T18:16:53.316749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.7\"></a>\n### 6.7 Evaluation","metadata":{}},{"cell_type":"markdown","source":"#### Classification Report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_pred = np.array(model.predict(valid_ds) > 0.5, dtype=int)\ncls_report = classification_report(y_val, y_pred)\nprint(cls_report)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:53.318027Z","iopub.status.idle":"2021-11-16T18:16:53.318576Z","shell.execute_reply.started":"2021-11-16T18:16:53.318344Z","shell.execute_reply":"2021-11-16T18:16:53.318368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.\"></a>\n## 7. Submission","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv\")\ntest_ds = tf.data.Dataset.from_tensor_slices((test[\"text\"])).batch(config.batch_size).cache().prefetch(1)\nscores = []\nfor path in [config.best_acc_model_path, config.best_auc_model_path, config.lastest_model_path]:\n    model.load_weights(path)\n    score = model.predict(test_ds).reshape(-1)\n    scores.append(score)\nscore = np.mean(scores, axis=0)\nprint(score.shape)\nsample_submission[\"score\"] = rankdata(score, method='ordinal')\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T18:16:53.319843Z","iopub.status.idle":"2021-11-16T18:16:53.320434Z","shell.execute_reply.started":"2021-11-16T18:16:53.320204Z","shell.execute_reply":"2021-11-16T18:16:53.320228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<a id=\"8.\"></a>\n## 8. References\n- [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824v3)\n- [Attention Is All You Need](https://arxiv.org/abs/1706.03762v5)\n- [Text Generation using FNet](https://keras.io/examples/nlp/text_generation_fnet/)\n- [English-Spanish Translation: FNet](https://www.kaggle.com/lonnieqin/english-spanish-translation-fnet)","metadata":{"id":"uKmcuxr1L3cH"}}]}