{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jigsaw Toxicity Training with FNet\n## Table of Contents\n* [1. Overview](#1.)\n* [2. Configuration](#2.)\n* [3. Setup](#3.)\n* [4. Tools](#4.)\n* [5. Import datasets](#5.)\n* [6. EDA & Preprocessing](#6.)\n    * [6.1 Learn about Stemming](#6.1)\n    * [6.2 Learn about Lemmatisation](#6.2)\n    * [6.3 Select trainng data](#6.3)\n    * [6.4 Statistic info of Token length](#6.4)\n    * [6.5 Build a Tokenizer](#6.5)\n    * [6.6 Train Validation Split](#6.6)\n    * [6.7 Create TensorFlow Dataset](#6.7)\n    * [6.8 Calculate Class weight](#6.8)\n* [7. Model Development](#7.)\n    * [7.1 FNet Encoder](#7.1)\n    * [7.2 Positional Embedding](#7.2)\n    * [7.3 FNet Classification Model](#7.3)\n    * [7.4 Model Training](#7.4)\n* [8. Submission](#8.)\n* [9. References](#9.)","metadata":{"id":"QLQhXw_-L3bw"}},{"cell_type":"markdown","source":"<font color=\"red\" size=\"3\">If you found it useful and would like to back me up, just upvote.</font>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.\"></a>\n## 1. Overview\nIn this Notebook, I will develop a Jigsaw Toxicity Prediction Model using FNet from scratch.\nThe FNet Model was able to achieve 92-97% of BERT's accuracy while training 80% faster on GPUs and almost 70% faster on TPUs. So that we use use it to do quick experiment.\n\nI build this sample referring to [Text Generation using FNet](https://keras.io/examples/nlp/text_generation_fnet/), ranking of toxicity can be calcualated via probability of binary classficiation.\n\nI use dataset from [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) and combine with more toxic data in this dataset for training.\n\nApart from building Model using FNet, I also try to build a custom Tokenizer to vectorize texts.\n\nCurrently this notebook can get a 0.758 LB, not a very good score. Using a pretrained Model and better text preprocessing method could improve the LB score.","metadata":{"id":"3br121fsL3by"}},{"cell_type":"markdown","source":"<a id=\"2.\"></a>\n## 2. Configuration","metadata":{"id":"23-sSaFsL3bz"}},{"cell_type":"code","source":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    sequence_length = 100 # Length of sequence\n    batch_size = 1024\n    validation_split = 0.15\n    embed_dim = 256\n    latent_dim = 256\n    oov_token = \"<OOV>\" # Out of Word token\n    bos_token = \"<BOS>\" # Begin of sequence token\n    eos_token = \"<EOS>\" # End of Sequence token\n    epochs = 50 # Number of Epochs to train\n    model_path = \"model.h5\"\nconfig = Config()","metadata":{"id":"gmp8ivzzL3bz","execution":{"iopub.status.busy":"2021-11-13T17:36:43.534624Z","iopub.execute_input":"2021-11-13T17:36:43.535338Z","iopub.status.idle":"2021-11-13T17:36:43.540895Z","shell.execute_reply.started":"2021-11-13T17:36:43.535303Z","shell.execute_reply":"2021-11-13T17:36:43.539599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.\"></a>\n## 3. Setup","metadata":{"id":"98JWyaiDL3b0"}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport pathlib\nimport random\nimport string\nimport re\nimport sys\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import TweetTokenizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom scipy.stats import rankdata\nimport json","metadata":{"id":"oda23ThKL3b1","execution":{"iopub.status.busy":"2021-11-13T17:51:01.104697Z","iopub.execute_input":"2021-11-13T17:51:01.104975Z","iopub.status.idle":"2021-11-13T17:51:01.266626Z","shell.execute_reply.started":"2021-11-13T17:51:01.104944Z","shell.execute_reply":"2021-11-13T17:51:01.26585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.\"></a>\n## 4. Tools","metadata":{}},{"cell_type":"markdown","source":"### Tokenizer class\nThis class can help you build a vocabulary by fitting a sequence of text. It's similar to Tokenizer in TensorFlow, it can also support padding sequences and adding Begin-of-Sentence token and End-of-Sentence token at the same time. I build this class to have fun and it's more flexible to custimize in the future. It accepts 5 parameters: vocaulbary size, out of word token, Begin-of-Sentence token (can be null), End-of-Sentence token (can be null), max sequence length.\n\n`fit_transform` can build a vocuabury from a list of tokens like:\n```python\n[\n    [\"1\", \"2\", \"3\", \"4\", \"5\"],\n    [\"1\", \"2\", \"3\", \"4\", \"5\"]\n]\n```\nand return vectors like\n```python\n[\n    [1, 2, 3, 4, 5],\n    [1, 2, 3, 4, 5]\n]\n```\n\n`transform` method is similar to `fit_transform` without building Vocabulary.\n","metadata":{}},{"cell_type":"code","source":"class Tokenizer:\n    \n    def __init__(self, vocab_size = None, oov_token = None, bos_token = None, eos_token = None, max_length = 10000):\n        self.vocab_size = vocab_size\n        self.oov_token = oov_token\n        self.max_length = max_length\n        self.bos_token = bos_token\n        self.eos_token = eos_token\n        \n    stopwords = set([\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ])\n    \n    tweet_tokenizer = TweetTokenizer() \n    \n    stemmer = PorterStemmer()\n    \n    lemmatizer = WordNetLemmatizer()\n    \n    @staticmethod\n    def preprocess_string(text):\n        # Convert sentences to lowercase.\n        text = text.lower()\n        # Remove puntuations, but ? and ! are usually enmotional so I won't remove it.\n        text = re.sub(r'[\\n| |.|\\\"|,|:|\\(|\\)|#|\\{|\\}|\\*|\\/|\\$|\\—|~|;|=|\\[｜\\]|\\-]+', \" \", text)\n        # Remove Digits\n        text = re.sub(\"[0-9]+\", \" \", text)\n        text = re.sub(\"[ ]+\", \" \", text)\n        text = text.strip(\" \")\n        # Convert sentences to tokens\n        items = Tokenizer.tweet_tokenizer.tokenize(text)\n        # Remove stop words\n        new_items = []\n        for item in items:\n            if item not in Tokenizer.stopwords:\n                new_item = Tokenizer.lemmatizer.lemmatize(item)\n                new_item = Tokenizer.stemmer.stem(new_item)\n                new_items.append(new_item)\n        return new_items\n        \n    def fit_transform(self, texts):\n        current_index = 1\n        word_index = {self.oov_token: current_index}\n        if self.bos_token != None:\n            current_index += 1\n            word_index[self.bos_token] = current_index\n        if self.eos_token != None:\n            current_index += 1\n            word_index[self.eos_token] = current_index\n\n        word_count = {}\n        for i in range(len(texts)):\n            text = texts[i]\n            for item in text:\n                if item in word_count:\n                    word_count[item] += 1\n                else:\n                    word_count[item] = 1\n        word_count_df = pd.DataFrame({\"key\": word_count.keys(), \"count\": word_count.values()})\n        word_count_df.sort_values(by=\"count\", ascending=False, inplace=True)\n        self.word_count_df = word_count_df\n        vocab = list(word_index.keys())\n        vocab += list(word_count_df[\"key\"][0: self.vocab_size - len(word_index)])\n        vocab = set(vocab)\n        self.vocab = vocab\n        \n        sentences = []\n        offset = 1 if self.eos_token != None else 0\n        for i in range(len(texts)):\n            text = texts[i]\n            sentence = []\n            if self.bos_token != None:\n                sentence.append(word_index[self.bos_token])\n            for item in text:\n                if item in self.vocab:\n                    if item in word_index:\n                        sentence.append(word_index[item])\n                    else:\n                        current_index += 1\n                        word_index[item] = current_index\n                        sentence.append(word_index[item])\n                else:\n                    sentence.append(word_index[self.oov_token])\n            if len(sentence) <= self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(word_index[self.eos_token])\n                sentence += [0] * (self.max_length - len(sentence))\n            elif len(sentence) > self.max_length - offset:\n                sentence = sentence[:self.max_length - offset]\n                if self.eos_token != None:\n                    sentence.append(word_index[self.eos_token])\n            sentences.append(sentence)\n        self.word_index = word_index\n        self.index_word = dict({word_index[key]: key for key in word_index.keys()})\n        return sentences\n    \n    def save(self, path):\n        dic = {\n            \"vocab_size\": self.vocab_size,\n            \"oov_token\": self.oov_token,\n            \"max_length\":  self.max_length,\n            \"vocab\": list(self.vocab),\n            \"index_word\": self.index_word,\n            \"word_index\": self.word_index\n        }\n        if self.bos_token is not None:\n            dic[\"bos_token\"] = self.bos_token\n        if self.eos_token is not None:\n            dic[\"eos_token\"] = self.eos_token\n        res = json.dumps(dic)\n        with open(path, \"w+\") as f:\n            f.write(res)\n            \n    def load(self, path):\n        with open(path, \"r\") as f:\n            dic = json.load(f)\n        self.vocab_size = dic[\"vocab_size\"]\n        self.oov_token = dic[\"oov_token\"]\n        self.max_length = dic[\"max_length\"]\n        self.vocab = set(dic[\"vocab\"])\n        self.index_word = dic[\"index_word\"]\n        self.word_index = dic[\"word_index\"]\n        if \"bos_token\" in dic:\n            self.bos_token = dic[\"bos_token\"]\n        if \"eos_token\" in dic:\n            self.eos_token = dic[\"eos_token\"]\n            \n    def transform(self, texts):\n        sentences = []\n        offset = 1 if self.eos_token != None else 0\n        for i in range(len(texts)):\n            text = texts[i]\n            sentence = []\n            if self.bos_token != None:\n                sentence.append(self.word_index[self.bos_token])\n            for item in text:\n                if item in self.vocab:\n                    sentence.append(self.word_index[item])\n                else:\n                    sentence.append(self.word_index[self.oov_token])\n            if len(sentence) == self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n            elif len(sentence) < self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n                sentence += [0] * (self.max_length - len(sentence))\n            elif len(sentence) > self.max_length - offset:\n                sentence = sentence[:self.max_length - offset]\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n            sentences.append(sentence)\n        return sentences\n            ","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:38:42.792173Z","iopub.execute_input":"2021-11-13T17:38:42.792497Z","iopub.status.idle":"2021-11-13T17:38:42.83216Z","shell.execute_reply.started":"2021-11-13T17:38:42.792462Z","shell.execute_reply":"2021-11-13T17:38:42.831389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.\"></a>\n## 5. Import datasets","metadata":{"id":"7czW02EaL3b1"}},{"cell_type":"code","source":"validation_data = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\")\nvalidation_data.head()","metadata":{"id":"PLs2vAotL3b2","outputId":"4f6891de-5362-40cf-b6ca-0af0a4220b3d","execution":{"iopub.status.busy":"2021-11-13T17:38:48.229465Z","iopub.execute_input":"2021-11-13T17:38:48.230049Z","iopub.status.idle":"2021-11-13T17:38:48.484017Z","shell.execute_reply.started":"2021-11-13T17:38:48.230009Z","shell.execute_reply":"2021-11-13T17:38:48.483155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:38:50.761187Z","iopub.execute_input":"2021-11-13T17:38:50.761914Z","iopub.status.idle":"2021-11-13T17:38:51.786873Z","shell.execute_reply.started":"2021-11-13T17:38:50.761877Z","shell.execute_reply":"2021-11-13T17:38:51.786113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.\"></a>\n### 6. EDA & Preprocessing","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6.1\"></a>\n#### 6.1 Learn about Stemming\n\nStemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.","metadata":{}},{"cell_type":"code","source":"stemmer = PorterStemmer()\nprint(stemmer.stem(\"going\"))\nprint(stemmer.stem(\"dogs\"))\nprint(stemmer.stem(\"leaves\"))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:38:54.584843Z","iopub.execute_input":"2021-11-13T17:38:54.585723Z","iopub.status.idle":"2021-11-13T17:38:54.592954Z","shell.execute_reply.started":"2021-11-13T17:38:54.585669Z","shell.execute_reply":"2021-11-13T17:38:54.592114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2\"></a>\n### 6.2 Learn about Lemmatisation\n\nLemmatisation in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize(\"going\"))\nprint(lemmatizer.lemmatize(\"dogs\"))\nprint(lemmatizer.lemmatize(\"leaves\"))\nprint(stemmer.stem(\"leaf\"))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:38:56.813152Z","iopub.execute_input":"2021-11-13T17:38:56.814017Z","iopub.status.idle":"2021-11-13T17:38:56.821285Z","shell.execute_reply.started":"2021-11-13T17:38:56.813975Z","shell.execute_reply":"2021-11-13T17:38:56.820315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.3\"></a>\n### 6.3 Select Traning Data","metadata":{}},{"cell_type":"markdown","source":"One of the way is to label `less_toxic` as 0 and `more_toxic` as 1, and FNet can get 0.749 score. I tried grouping the duplicated comment together and replace the label with average value, but got a worse 0.49 score instead. I also tried to convert the average value to a class value, but still can't learn any important information from it. So I am going to keep every variable we may use in the future to a data table.\n\n\nAnother way is to use external dataset from [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). Since there is a class imbalance problem, I also add more_toxic data from this dataset and label it as 1.","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"sys.setrecursionlimit(100000)\nimport time\nbegin = time.time()\nuse_external_dataset = True\nif use_external_dataset:\n    train = train[[\"comment_text\", \"toxic\"]]\n    train.columns = [\"text\", \"label\"]\n    # Add More toxic data to mitigate class imbalance problem\n    train = train.append(pd.DataFrame({\"text\": validation_data[\"more_toxic\"], \"label\": [1] * len(validation_data)}))\nelse:\n    data = pd.DataFrame({\"text\": validation_data[\"less_toxic\"], \"label\": [0] * len(validation_data)})\n    data = data.append(pd.DataFrame({\"text\": validation_data[\"more_toxic\"], \"label\": [1] * len(validation_data)}))\n    text = data[\"text\"].unique()\n    grouped = data.groupby(\"text\")\n    label = list(grouped.mean()[\"label\"])\n    text_label_dict = dict({key: value for key, value in zip(text, label)})\n    index_label = sorted(grouped.mean()[\"label\"].unique())\n    data[\"average_value\"] = data[\"text\"].apply(lambda text: text_label_dict[text])\n    data[\"class\"] = data[\"average_value\"].apply(lambda value: index_label.index(value))\n    classes = sorted(data[\"class\"].unique())\n    print(\"Classes:\", classes)\n    train = data[[\"text\", \"label\"]]\ntokens = []\nlast_index = len(train) - 1\nfor i in range(len(train)):\n    tokens.append(Tokenizer.preprocess_string(train.iloc[i][\"text\"]))\n    if (i + 1) % 10000 == 0 or i == last_index:\n        current = time.time() - begin\n        print(\"%.2fs-%.2fs: %.2f%%\" % (current, current * len(train) / i, i / len(train) * 100))\ntrain[\"token\"] = tokens\ntrain[\"token_length\"] = train[\"token\"].apply(len)\ntrain = sklearn.utils.shuffle(train)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:38:59.232155Z","iopub.execute_input":"2021-11-13T17:38:59.23288Z","iopub.status.idle":"2021-11-13T17:41:04.344505Z","shell.execute_reply.started":"2021-11-13T17:38:59.232839Z","shell.execute_reply":"2021-11-13T17:41:04.343604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.4\"></a>\n### 6.4 Statistic info of Token length\nAverage Token length is 39. Most are under 100, so choosing 100 as sequence length is enough.","metadata":{}},{"cell_type":"code","source":"train[[\"token_length\"]].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:41:33.700428Z","iopub.execute_input":"2021-11-13T17:41:33.700707Z","iopub.status.idle":"2021-11-13T17:41:33.732125Z","shell.execute_reply.started":"2021-11-13T17:41:33.700679Z","shell.execute_reply":"2021-11-13T17:41:33.731272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"token_length\"][train[\"token_length\"] <= 100].hist()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:41:36.641084Z","iopub.execute_input":"2021-11-13T17:41:36.641846Z","iopub.status.idle":"2021-11-13T17:41:36.922892Z","shell.execute_reply.started":"2021-11-13T17:41:36.641802Z","shell.execute_reply":"2021-11-13T17:41:36.922113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"label\"].hist()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:41:39.346865Z","iopub.execute_input":"2021-11-13T17:41:39.347442Z","iopub.status.idle":"2021-11-13T17:41:39.606803Z","shell.execute_reply.started":"2021-11-13T17:41:39.347396Z","shell.execute_reply":"2021-11-13T17:41:39.60607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.5\"></a>\n### 6.5 Build a Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(\n    vocab_size=config.vocab_size, \n    oov_token=config.oov_token, \n    bos_token=config.bos_token,\n    eos_token=config.eos_token,\n    max_length=config.sequence_length\n)\nsequences = tokenizer.fit_transform(list(train[\"token\"]))\ntrain[\"sequence\"] = sequences\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:42:14.638847Z","iopub.execute_input":"2021-11-13T17:42:14.639138Z","iopub.status.idle":"2021-11-13T17:42:23.443331Z","shell.execute_reply.started":"2021-11-13T17:42:14.639107Z","shell.execute_reply":"2021-11-13T17:42:23.442563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of words:","metadata":{}},{"cell_type":"code","source":"len(tokenizer.index_word)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:42:26.797832Z","iopub.execute_input":"2021-11-13T17:42:26.79865Z","iopub.status.idle":"2021-11-13T17:42:26.807007Z","shell.execute_reply.started":"2021-11-13T17:42:26.798608Z","shell.execute_reply":"2021-11-13T17:42:26.806186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save the Tokenzier:","metadata":{}},{"cell_type":"code","source":"tokenizer.save(\"tokenizer.json\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:42:29.674637Z","iopub.execute_input":"2021-11-13T17:42:29.67492Z","iopub.status.idle":"2021-11-13T17:42:29.715454Z","shell.execute_reply.started":"2021-11-13T17:42:29.674888Z","shell.execute_reply":"2021-11-13T17:42:29.714462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the Tokenizer:","metadata":{}},{"cell_type":"code","source":"new_tokenizer = Tokenizer()\nnew_tokenizer.load(\"tokenizer.json\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:42:32.295152Z","iopub.execute_input":"2021-11-13T17:42:32.295762Z","iopub.status.idle":"2021-11-13T17:42:32.318771Z","shell.execute_reply.started":"2021-11-13T17:42:32.295726Z","shell.execute_reply":"2021-11-13T17:42:32.318019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of Words that seldom appear:","metadata":{}},{"cell_type":"code","source":"word_count_seldom_appear = {\"word_count\": [], \"num_words\": []}\nfor i in range(1, 10):\n    word_count_seldom_appear[\"word_count\"].append(i)\n    word_count_seldom_appear[\"num_words\"].append(len(tokenizer.word_count_df[tokenizer.word_count_df[\"count\"] <= i]))\nsns.barplot(x=\"word_count\", y=\"num_words\", data=pd.DataFrame(word_count_seldom_appear))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:52:49.764647Z","iopub.execute_input":"2021-11-13T17:52:49.764936Z","iopub.status.idle":"2021-11-13T17:52:49.838991Z","shell.execute_reply.started":"2021-11-13T17:52:49.764906Z","shell.execute_reply":"2021-11-13T17:52:49.838244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.6\"></a>\n### 6.6 Train Validation Split","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train[\"sequence\"], train[\"label\"], test_size=config.validation_split)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_val.shape, y_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.7\"></a>\n### 6.7 Create TensorFlow Dataset","metadata":{}},{"cell_type":"code","source":"def make_dataset(X, y, batch_size, mode):\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    if mode == \"train\":\n       dataset = dataset.shuffle(256) \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.cache().prefetch(16).repeat(1)\n    return dataset","metadata":{"id":"KTzWxxsAL3b4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = make_dataset(list(X_train), list(y_train), batch_size=config.batch_size, mode=\"train\")\nvalid_ds = make_dataset(list(X_val), list(y_val), batch_size=config.batch_size, mode=\"valid\")","metadata":{"id":"oAP4kRzzL3b4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see what this data look like.","metadata":{}},{"cell_type":"code","source":"for batch in train_ds.take(1):\n    print(batch)","metadata":{"id":"BMjiEEa9L3b5","outputId":"598a1ca1-c02c-4be2-ec1e-d0632cda7d07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.8\"></a>\n### 6.8  Calculate Class weight","metadata":{}},{"cell_type":"code","source":"class_weight =  dict(len(train) / train[\"label\"].value_counts())\nclass_weight","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.\"></a>\n## 7. Model Development","metadata":{"id":"XSXAmMV0L3b6"}},{"cell_type":"markdown","source":"<a id=\"7.1\"></a>\n### 7.1 FNet Encoder","metadata":{"id":"n1qyTb9AL3b6"}},{"cell_type":"code","source":"class FNetEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, dropout_rate=0.1, **kwargs):\n        super(FNetEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, inputs):\n        # Casting the inputs to complex64\n        inp_complex = tf.cast(inputs, tf.complex64)\n        # Projecting the inputs to the frequency domain using FFT2D and\n        # extracting the real part of the output\n        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n        proj_input = self.layernorm_1(inputs + fft)\n        proj_output = self.dense_proj(proj_input)\n       \n        layer_norm = self.layernorm_2(proj_input + proj_output)\n        output = self.dropout(layer_norm)\n        return output","metadata":{"id":"fRXW_RaML3b6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.2\"></a>\n### 7.2 Positional Embedding","metadata":{"id":"ISzHta1dL3b7"}},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","metadata":{"id":"kUiNiGv6L3b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.3\"></a>\n### 7.3 FNet Classification Model","metadata":{"id":"2VUKFd9sL3b9"}},{"cell_type":"code","source":"def get_fnet_classifier(config):\n    inputs = keras.Input(shape=(config.sequence_length), dtype=\"int64\", name=\"encoder_inputs\")\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(inputs)\n    x = FNetEncoder(config.embed_dim, config.latent_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.3)(x)\n    for i in range(3):\n        x = layers.Dense(100, activation=\"relu\")(x)\n        x = layers.Dropout(0.3)(x)\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    fnet = keras.Model(inputs, output, name=\"fnet\")\n    return fnet","metadata":{"id":"dp19cOEqL3b9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fnet = get_fnet_classifier(config)","metadata":{"id":"T-QhEpKoL3b-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fnet.summary()","metadata":{"id":"ie_rklX2L3b-","outputId":"ba08f590-0cf9-4c61-a9df-0244391b2f05","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the Model.","metadata":{"id":"x_O7FfflL3b-"}},{"cell_type":"code","source":"keras.utils.plot_model(fnet, show_shapes=True)","metadata":{"id":"6Tm1nG0CL3b-","outputId":"0cd78731-3c68-4fdf-f8c7-9b0b2512013d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<a id=\"7.4\"></a>\n### 7.4 Model Training","metadata":{"id":"NFtSL6-3L3b-"}},{"cell_type":"code","source":"fnet.compile(\n    \"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.AUC()]\n)","metadata":{"id":"3wMy6AcIL3b_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = keras.callbacks.ModelCheckpoint(config.model_path, monitor=\"val_accuracy\",save_weights_only=True, save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", patience=5, min_delta=1e-4, min_lr=1e-6)\nfnet.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[checkpoint, reduce_lr], class_weight=class_weight)\nfnet.save_weights(\"model_latest.h5\")","metadata":{"id":"YKjayqSSL3b_","outputId":"0a6677a7-60a0-4eda-e992-f0807e4d8f6e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fnet.load_weights(config.model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.5\"></a>\n### 7.5 Evaluation","metadata":{}},{"cell_type":"markdown","source":"### Classification Report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_pred = np.array(fnet.predict(valid_ds) > 0.5, dtype=int)\ncls_report = classification_report(y_val, y_pred)\nprint(cls_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8.\"></a>\n## 8. Submission","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv\")\ntest[\"text_preprocessed\"] = test[\"text\"].apply(Tokenizer.preprocess_string)\ntest_sequences = tokenizer.transform(list(test[\"text_preprocessed\"]))\nprint(test_sequences[0])\ntest_ds = tf.data.Dataset.from_tensor_slices((test_sequences)).batch(config.batch_size).prefetch(1)\nscore = fnet.predict(test_ds).reshape(-1)\nsample_submission[\"score\"] = rankdata(score, method='ordinal')\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<a id=\"9.\"></a>\n## 9. References\n- [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824v3)\n- [Attention Is All You Need](https://arxiv.org/abs/1706.03762v5)\n- [Text Generation using FNet](https://keras.io/examples/nlp/text_generation_fnet/)\n- [English-Spanish Translation: FNet](https://www.kaggle.com/lonnieqin/english-spanish-translation-fnet)","metadata":{"id":"uKmcuxr1L3cH"}}]}