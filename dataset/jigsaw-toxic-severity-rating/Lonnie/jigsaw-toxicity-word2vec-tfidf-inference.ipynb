{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jigsaw Toxicity Word2Vec+TFIDF Inference\n## Table of Contents\n* [1. Overview](#1.)\n* [2. Configuration](#2.)\n* [3. Setup](#3.)\n* [4. Model](#4.)\n    * [4.1 FNet Encoder](#4.1)\n    * [4.2 Positional Embedding](#4.2)\n    * [4.3 FNet Classification Model](#4.3)\n    * [4.4 Model Training](#4.4)\n* [5. Submission](#5.)\n* [6. References](#6.)","metadata":{"id":"QLQhXw_-L3bw"}},{"cell_type":"markdown","source":"<font color=\"red\" size=\"3\">If you found it useful and would like to back me up, just upvote.</font>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.\"></a>\n## 1. Overview\nThis is inference Notebook, for training notebook please see [here](https://www.kaggle.com/lonnieqin/jigsaw-toxicity-word2vec-tfidf-training).","metadata":{"id":"3br121fsL3by"}},{"cell_type":"markdown","source":"<a id=\"2.\"></a>\n## 2. Configuration","metadata":{"id":"23-sSaFsL3bz"}},{"cell_type":"code","source":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    sequence_length = 100 # Length of sequence\n    batch_size = 1024\n    embed_dim = 256\n    latent_dim = 256\n    base_model_path = \"../input/jigsaw-word2vec-tfidf-model/\"\n    best_auc_model_path = \"model_best_auc.tf\"\n    best_acc_model_path = \"model_best_acc.tf\"\n    lastest_model_path = \"model_latest.tf\"\nconfig = Config()","metadata":{"id":"gmp8ivzzL3bz","execution":{"iopub.status.busy":"2022-02-07T03:31:23.752564Z","iopub.execute_input":"2022-02-07T03:31:23.75318Z","iopub.status.idle":"2022-02-07T03:31:23.758482Z","shell.execute_reply.started":"2022-02-07T03:31:23.753124Z","shell.execute_reply":"2022-02-07T03:31:23.757549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.\"></a>\n## 3. Setup","metadata":{"id":"98JWyaiDL3b0"}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport pathlib\nimport random\nimport string\nimport re\nimport sys\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import TweetTokenizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom scipy.stats import rankdata\nimport json","metadata":{"id":"oda23ThKL3b1","execution":{"iopub.status.busy":"2022-02-07T03:31:06.512153Z","iopub.execute_input":"2022-02-07T03:31:06.512435Z","iopub.status.idle":"2022-02-07T03:31:12.681084Z","shell.execute_reply.started":"2022-02-07T03:31:06.512407Z","shell.execute_reply":"2022-02-07T03:31:12.680311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n    text = tf.strings.regex_replace(\n        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n    )\n    text = tf.strings.regex_replace(text, f\"[0-9]+\", \" \")\n    text = tf.strings.regex_replace(text, f\"[ ]+\", \" \")\n    text = tf.strings.strip(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:31:12.682832Z","iopub.execute_input":"2022-02-07T03:31:12.683093Z","iopub.status.idle":"2022-02-07T03:31:12.689101Z","shell.execute_reply.started":"2022-02-07T03:31:12.68306Z","shell.execute_reply":"2022-02-07T03:31:12.688196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_data = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\")\ntrain = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntrain = train[[\"comment_text\", \"toxic\"]]\ntrain.columns = [\"text\", \"label\"]\n# Add More toxic data to mitigate class imbalance problem\ntrain = train.append(pd.DataFrame({\"text\": validation_data[\"more_toxic\"], \"label\": [1] * len(validation_data)}))\ntfidf_vectozier = layers.TextVectorization(\n    standardize=custom_standardization, \n    max_tokens=config.vocab_size, \n    output_mode=\"tf-idf\", \n    ngrams=2\n)\nword2vec_vectozier = layers.TextVectorization(\n    standardize=custom_standardization, \n    max_tokens=config.vocab_size, \n    output_sequence_length=config.sequence_length\n)\nwith tf.device(\"CPU\"):\n    tfidf_vectozier.adapt(list(train[\"text\"]))\n    word2vec_vectozier.adapt(list(train[\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:31:27.623964Z","iopub.execute_input":"2022-02-07T03:31:27.624503Z","iopub.status.idle":"2022-02-07T03:32:40.041138Z","shell.execute_reply.started":"2022-02-07T03:31:27.624463Z","shell.execute_reply":"2022-02-07T03:32:40.040362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.\"></a>\n## 4. Model","metadata":{"id":"XSXAmMV0L3b6"}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n### 4.1 FNet Encoder","metadata":{"id":"n1qyTb9AL3b6"}},{"cell_type":"code","source":"class FNetEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, dropout_rate=0.1, **kwargs):\n        super(FNetEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        #self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, inputs):\n        # Casting the inputs to complex64\n        inp_complex = tf.cast(inputs, tf.complex64)\n        # Projecting the inputs to the frequency domain using FFT2D and\n        # extracting the real part of the output\n        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n        proj_input = self.layernorm_1(inputs + fft)\n        proj_output = self.dense_proj(proj_input)\n       \n        layer_norm = self.layernorm_2(proj_input + proj_output)\n        #output = self.dropout(layer_norm)\n        return layer_norm","metadata":{"id":"fRXW_RaML3b6","execution":{"iopub.status.busy":"2022-02-07T03:32:40.042985Z","iopub.execute_input":"2022-02-07T03:32:40.043268Z","iopub.status.idle":"2022-02-07T03:32:40.052098Z","shell.execute_reply.started":"2022-02-07T03:32:40.043231Z","shell.execute_reply":"2022-02-07T03:32:40.051281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n### 4.2 Positional Embedding","metadata":{"id":"ISzHta1dL3b7"}},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","metadata":{"id":"kUiNiGv6L3b7","execution":{"iopub.status.busy":"2022-02-07T03:32:40.053337Z","iopub.execute_input":"2022-02-07T03:32:40.053852Z","iopub.status.idle":"2022-02-07T03:32:40.065745Z","shell.execute_reply.started":"2022-02-07T03:32:40.053702Z","shell.execute_reply":"2022-02-07T03:32:40.064964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3\"></a>\n### 4.3 Classification Model","metadata":{"id":"2VUKFd9sL3b9"}},{"cell_type":"code","source":"def get_word2vec_model(config, inputs):\n    x = word2vec_vectozier(inputs)\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(x)\n    x = FNetEncoder(config.embed_dim, config.latent_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0)(x)\n    for i in range(3):\n        x = layers.Dense(100, activation=\"relu\")(x)\n        x = layers.Dropout(0)(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:32:40.068968Z","iopub.execute_input":"2022-02-07T03:32:40.069153Z","iopub.status.idle":"2022-02-07T03:32:40.078452Z","shell.execute_reply.started":"2022-02-07T03:32:40.069125Z","shell.execute_reply":"2022-02-07T03:32:40.077673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tfidf_model(config, inputs):\n    x = tfidf_vectozier(inputs)\n    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:32:40.07959Z","iopub.execute_input":"2022-02-07T03:32:40.080007Z","iopub.status.idle":"2022-02-07T03:32:40.087927Z","shell.execute_reply.started":"2022-02-07T03:32:40.079971Z","shell.execute_reply":"2022-02-07T03:32:40.087228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(config):\n    inputs = keras.Input(shape=(None, ), dtype=\"string\", name=\"inputs\")\n    word2vec_x = get_word2vec_model(config, inputs)\n    tfidf_x = get_tfidf_model(config, inputs)\n    x = layers.Concatenate()([word2vec_x, tfidf_x])\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, output, name=\"model\")\n    return model","metadata":{"id":"dp19cOEqL3b9","execution":{"iopub.status.busy":"2022-02-07T03:32:40.089321Z","iopub.execute_input":"2022-02-07T03:32:40.089563Z","iopub.status.idle":"2022-02-07T03:32:40.097391Z","shell.execute_reply.started":"2022-02-07T03:32:40.089532Z","shell.execute_reply":"2022-02-07T03:32:40.096675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(config)","metadata":{"id":"T-QhEpKoL3b-","execution":{"iopub.status.busy":"2022-02-07T03:32:40.09809Z","iopub.execute_input":"2022-02-07T03:32:40.098277Z","iopub.status.idle":"2022-02-07T03:32:40.479319Z","shell.execute_reply.started":"2022-02-07T03:32:40.098256Z","shell.execute_reply":"2022-02-07T03:32:40.478597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"ie_rklX2L3b-","outputId":"ba08f590-0cf9-4c61-a9df-0244391b2f05","execution":{"iopub.status.busy":"2022-02-07T03:32:40.480615Z","iopub.execute_input":"2022-02-07T03:32:40.480857Z","iopub.status.idle":"2022-02-07T03:32:40.494795Z","shell.execute_reply.started":"2022-02-07T03:32:40.480824Z","shell.execute_reply":"2022-02-07T03:32:40.494138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the Model.","metadata":{"id":"x_O7FfflL3b-"}},{"cell_type":"code","source":"keras.utils.plot_model(model, show_shapes=True)","metadata":{"id":"6Tm1nG0CL3b-","outputId":"0cd78731-3c68-4fdf-f8c7-9b0b2512013d","execution":{"iopub.status.busy":"2022-02-07T03:32:40.49578Z","iopub.execute_input":"2022-02-07T03:32:40.49602Z","iopub.status.idle":"2022-02-07T03:32:41.48059Z","shell.execute_reply.started":"2022-02-07T03:32:40.495987Z","shell.execute_reply":"2022-02-07T03:32:41.477369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation Accuracy","metadata":{}},{"cell_type":"code","source":"def inference(model, paths, data):\n    scores = []\n    for path in paths:\n        model.load_weights(config.base_model_path + path)\n        score = model.predict(data).reshape(-1)\n        scores.append(score)\n    return np.mean(scores, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:36:37.010675Z","iopub.execute_input":"2022-02-07T03:36:37.010937Z","iopub.status.idle":"2022-02-07T03:36:37.015896Z","shell.execute_reply.started":"2022-02-07T03:36:37.010906Z","shell.execute_reply":"2022-02-07T03:36:37.015171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:33:18.943034Z","iopub.execute_input":"2022-02-07T03:33:18.943676Z","iopub.status.idle":"2022-02-07T03:33:18.959348Z","shell.execute_reply.started":"2022-02-07T03:33:18.943628Z","shell.execute_reply":"2022-02-07T03:33:18.958487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = [config.best_acc_model_path, config.best_auc_model_path, config.lastest_model_path]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:36:12.075084Z","iopub.execute_input":"2022-02-07T03:36:12.075673Z","iopub.status.idle":"2022-02-07T03:36:12.079629Z","shell.execute_reply.started":"2022-02-07T03:36:12.075634Z","shell.execute_reply":"2022-02-07T03:36:12.078674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_pred = inference(model, paths, validation_data[\"less_toxic\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:36:41.039808Z","iopub.execute_input":"2022-02-07T03:36:41.04035Z","iopub.status.idle":"2022-02-07T03:37:04.127847Z","shell.execute_reply.started":"2022-02-07T03:36:41.040309Z","shell.execute_reply":"2022-02-07T03:37:04.127037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"more_pred = inference(model, paths, validation_data[\"more_toxic\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:37:04.129544Z","iopub.execute_input":"2022-02-07T03:37:04.129789Z","iopub.status.idle":"2022-02-07T03:37:19.573427Z","shell.execute_reply.started":"2022-02-07T03:37:04.129752Z","shell.execute_reply":"2022-02-07T03:37:19.572644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(less_pred < more_pred).mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:37:19.575184Z","iopub.execute_input":"2022-02-07T03:37:19.575426Z","iopub.status.idle":"2022-02-07T03:37:19.581132Z","shell.execute_reply.started":"2022-02-07T03:37:19.575392Z","shell.execute_reply":"2022-02-07T03:37:19.580363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.\"></a>\n## 5. Submission","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv\")\ntest_ds = tf.data.Dataset.from_tensor_slices((test[\"text\"])).batch(config.batch_size).cache().prefetch(1)\nscores = []\n#for path in [config.best_auc_model_path]:\n#for path in [config.best_acc_model_path, config.best_auc_model_path]:\nfor path in paths:\n    model.load_weights(config.base_model_path + path)\n    score = model.predict(test_ds).reshape(-1)\n    scores.append(score)\nscore = np.mean(scores, axis=0)\nprint(score.shape)\nsample_submission[\"score\"] = rankdata(score, method='ordinal')\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T17:19:04.543459Z","iopub.execute_input":"2021-11-16T17:19:04.544101Z","iopub.status.idle":"2021-11-16T17:19:08.104321Z","shell.execute_reply.started":"2021-11-16T17:19:04.544059Z","shell.execute_reply":"2021-11-16T17:19:08.103618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<a id=\"6.\"></a>\n## 6. References\n- [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824v3)\n- [Attention Is All You Need](https://arxiv.org/abs/1706.03762v5)\n- [Text Generation using FNet](https://keras.io/examples/nlp/text_generation_fnet/)\n- [English-Spanish Translation: FNet](https://www.kaggle.com/lonnieqin/english-spanish-translation-fnet)","metadata":{"id":"uKmcuxr1L3cH"}}]}