{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jigsaw Toxicity Inference with FNet\n## Table of Contents\n* [1. Configuration](#1.)\n* [2. Setup](#2.)\n* [3. Tokenzier](#3.)\n* [4. FNet Model](#4.)\n* [5. Submission](#5.)","metadata":{"id":"QLQhXw_-L3bw"}},{"cell_type":"markdown","source":"<font color=\"red\" size=\"3\">If you found it useful and would like to back me up, just upvote.</font>","metadata":{}},{"cell_type":"markdown","source":"This is inference notebook. For training notebook, visit [here](https://www.kaggle.com/lonnieqin/jigsaw-toxicity-prediction-with-fnet).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.\"></a>\n## 1. Configuration","metadata":{"id":"23-sSaFsL3bz"}},{"cell_type":"code","source":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    sequence_length = 100 # Length of sequence\n    batch_size = 1024\n    embed_dim = 256\n    latent_dim = 256\nconfig = Config()","metadata":{"id":"gmp8ivzzL3bz","execution":{"iopub.status.busy":"2021-11-13T12:06:19.386846Z","iopub.execute_input":"2021-11-13T12:06:19.387305Z","iopub.status.idle":"2021-11-13T12:06:19.408778Z","shell.execute_reply.started":"2021-11-13T12:06:19.387173Z","shell.execute_reply":"2021-11-13T12:06:19.407822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.\"></a>\n## 2. Setup","metadata":{"id":"98JWyaiDL3b0"}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport pathlib\nimport random\nimport string\nimport re\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import TweetTokenizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom scipy.stats import rankdata\nimport json\nimport sys\nsys.setrecursionlimit(100000)","metadata":{"id":"oda23ThKL3b1","execution":{"iopub.status.busy":"2021-11-13T12:06:19.411741Z","iopub.execute_input":"2021-11-13T12:06:19.412311Z","iopub.status.idle":"2021-11-13T12:06:26.531141Z","shell.execute_reply.started":"2021-11-13T12:06:19.41226Z","shell.execute_reply":"2021-11-13T12:06:26.53009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.\"></a>\n## 3. Tokenzier","metadata":{}},{"cell_type":"code","source":"class Tokenizer:\n    \n    stopwords = set([\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ])\n    \n    tweet_tokenizer = TweetTokenizer() \n    \n    stemmer = PorterStemmer()\n    \n    lemmatizer = WordNetLemmatizer()\n    \n    def __init__(self, vocab_size = None, oov_token = None, bos_token = None, eos_token = None, max_length = 10000):\n        self.vocab_size = vocab_size\n        self.oov_token = oov_token\n        self.max_length = max_length\n        self.bos_token = bos_token\n        self.eos_token = eos_token\n    \n    @staticmethod\n    def preprocess_string(text):\n        # Convert sentences to lowercase.\n        text = text.lower()\n        # Remove puntuations, but ? and ! are usually enmotional so I won't remove it.\n        text = re.sub(r'[\\n| |.|\\\"|,|:|\\(|\\)|#|\\'|\\{|\\}|\\*|\\/|\\$|\\—|~|;|=|\\[｜\\]|\\-]+', \" \", text)\n        # Remove Digits\n        text = re.sub(\"[0-9]+\", \" \", text)\n        text = re.sub(\"[ ]+\", \" \", text)\n        text = text.strip(\" \")\n        # Convert sentences to tokens\n        items = Tokenizer.tweet_tokenizer.tokenize(text)\n        # Remove stop words\n        new_items = []\n        for item in items:\n            if item not in Tokenizer.stopwords:\n                new_item = Tokenizer.lemmatizer.lemmatize(item)\n                new_item = Tokenizer.stemmer.stem(new_item)\n                new_items.append(new_item)\n        return new_items\n        \n    def fit_transform(self, texts):\n        current_index = 1\n        word_index = {self.oov_token: current_index}\n        if self.bos_token != None:\n            current_index += 1\n            word_index[self.bos_token] = current_index\n        if self.eos_token != None:\n            current_index += 1\n            word_index[self.eos_token] = current_index\n        word_count = {}\n        for i in range(len(texts)):\n            text = texts[i]\n            for item in text:\n                if item in word_count:\n                    word_count[item] += 1\n                else:\n                    word_count[item] = 1\n        word_count_df = pd.DataFrame({\"key\": word_count.keys(), \"count\": word_count.values()})\n        word_count_df.sort_values(by=\"count\", ascending=False, inplace=True)\n        vocab = list(word_index.keys())\n        vocab += list(word_count_df[\"key\"][0: self.vocab_size - len(word_index)])\n        vocab = set(vocab)\n        self.vocab = vocab\n        \n        sentences = []\n        offset = 1 if self.eos_token != None else 0\n        for i in range(len(texts)):\n            text = texts[i]\n            sentence = []\n            if self.bos_token != None:\n                sentence.append(word_index[self.bos_token])\n            for item in text:\n                if item in self.vocab:\n                    if item in word_index:\n                        sentence.append(word_index[item])\n                    else:\n                        current_index += 1\n                        word_index[item] = current_index\n                        sentence.append(word_index[item])\n                else:\n                    sentence.append(word_index[self.oov_token])\n            if len(sentence) <= self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(word_index[self.eos_token])\n                sentence += [0] * (self.max_length - len(sentence))\n            elif len(sentence) > self.max_length - offset:\n                sentence = sentence[:self.max_length - offset]\n                if self.eos_token != None:\n                    sentence.append(word_index[self.eos_token])\n            sentences.append(sentence)\n        self.word_index = word_index\n        self.index_word = dict({word_index[key]: key for key in word_index.keys()})\n        return sentences\n    \n    def save(self, path):\n        dic = {\n            \"vocab_size\": self.vocab_size,\n            \"oov_token\": self.oov_token,\n            \"max_length\":  self.max_length,\n            \"vocab\": list(self.vocab),\n            \"index_word\": self.index_word,\n            \"word_index\": self.word_index\n        }\n        \n        if self.bos_token is not None:\n            dic[\"bos_token\"] = self.bos_token\n            \n        if self.eos_token is not None:\n            dic[\"eos_token\"] = self.eos_token\n            \n        res = json.dumps(dic)\n        \n        with open(path, \"w+\") as f:\n            f.write(res)\n            \n    def load(self, path):\n        with open(path, \"r\") as f:\n            dic = json.load(f)\n        self.vocab_size = dic[\"vocab_size\"]\n        self.oov_token = dic[\"oov_token\"]\n        self.max_length = dic[\"max_length\"]\n        self.vocab = set(dic[\"vocab\"])\n        self.index_word = dic[\"index_word\"]\n        self.word_index = dic[\"word_index\"]\n        if \"bos_token\" in dic:\n            self.bos_token = dic[\"bos_token\"]\n        if \"eos_token\" in dic:\n            self.eos_token = dic[\"eos_token\"]\n            \n    def transform(self, texts):\n        sentences = []\n        offset = 1 if self.eos_token != None else 0\n        for i in range(len(texts)):\n            text = texts[i]\n            sentence = []\n            if self.bos_token != None:\n                sentence.append(self.word_index[self.bos_token])\n            for item in text:\n                if item in self.vocab:\n                    sentence.append(self.word_index[item])\n                else:\n                    sentence.append(self.word_index[self.oov_token])\n            if len(sentence) == self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n            elif len(sentence) < self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n                sentence += [0] * (self.max_length - len(sentence))\n            elif len(sentence) > self.max_length - offset:\n                sentence = sentence[:self.max_length - offset]\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n            sentences.append(sentence)\n        return sentences","metadata":{"execution":{"iopub.status.busy":"2021-11-13T12:08:36.65737Z","iopub.execute_input":"2021-11-13T12:08:36.657802Z","iopub.status.idle":"2021-11-13T12:08:36.703494Z","shell.execute_reply.started":"2021-11-13T12:08:36.657755Z","shell.execute_reply":"2021-11-13T12:08:36.702026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.load(\"../input/jigsaw-toxicity-fnet/tokenizer.json\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T12:08:49.188088Z","iopub.execute_input":"2021-11-13T12:08:49.188486Z","iopub.status.idle":"2021-11-13T12:08:49.218481Z","shell.execute_reply.started":"2021-11-13T12:08:49.188439Z","shell.execute_reply":"2021-11-13T12:08:49.217511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.\"></a>\n## 4. FNet Model","metadata":{}},{"cell_type":"markdown","source":"### 4.1 FNet Encoder","metadata":{}},{"cell_type":"code","source":"class FNetEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, dropout_rate=0.1, **kwargs):\n        super(FNetEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, inputs):\n        # Casting the inputs to complex64\n        inp_complex = tf.cast(inputs, tf.complex64)\n        # Projecting the inputs to the frequency domain using FFT2D and\n        # extracting the real part of the output\n        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n        proj_input = self.layernorm_1(inputs + fft)\n        proj_output = self.dense_proj(proj_input)\n       \n        layer_norm = self.layernorm_2(proj_input + proj_output)\n        output = self.dropout(layer_norm)\n        return output","metadata":{"id":"fRXW_RaML3b6","execution":{"iopub.status.busy":"2021-11-13T12:07:29.715015Z","iopub.execute_input":"2021-11-13T12:07:29.715339Z","iopub.status.idle":"2021-11-13T12:07:29.727218Z","shell.execute_reply.started":"2021-11-13T12:07:29.715308Z","shell.execute_reply":"2021-11-13T12:07:29.726033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n### 4.2 Positional Embedding","metadata":{"id":"ISzHta1dL3b7"}},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","metadata":{"id":"kUiNiGv6L3b7","execution":{"iopub.status.busy":"2021-11-13T12:07:31.931983Z","iopub.execute_input":"2021-11-13T12:07:31.932484Z","iopub.status.idle":"2021-11-13T12:07:31.943861Z","shell.execute_reply.started":"2021-11-13T12:07:31.932452Z","shell.execute_reply":"2021-11-13T12:07:31.942445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.\"></a>\n### 4.3 FNet Classification Model","metadata":{"id":"2VUKFd9sL3b9"}},{"cell_type":"code","source":"def get_fnet_classifier(config):\n    inputs = keras.Input(shape=(config.sequence_length), dtype=\"int64\", name=\"encoder_inputs\")\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(inputs)\n    x = FNetEncoder(config.embed_dim, config.latent_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.3)(x)\n    for i in range(3):\n        x = layers.Dense(100, activation=\"relu\")(x)\n        x = layers.Dropout(0.3)(x)\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    fnet = keras.Model(inputs, output, name=\"fnet\")\n    return fnet","metadata":{"id":"dp19cOEqL3b9","execution":{"iopub.status.busy":"2021-11-13T12:07:34.353833Z","iopub.execute_input":"2021-11-13T12:07:34.354304Z","iopub.status.idle":"2021-11-13T12:07:34.36347Z","shell.execute_reply.started":"2021-11-13T12:07:34.354269Z","shell.execute_reply":"2021-11-13T12:07:34.362045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fnet = get_fnet_classifier(config)","metadata":{"id":"T-QhEpKoL3b-","execution":{"iopub.status.busy":"2021-11-13T12:07:37.234207Z","iopub.execute_input":"2021-11-13T12:07:37.234602Z","iopub.status.idle":"2021-11-13T12:07:37.444398Z","shell.execute_reply.started":"2021-11-13T12:07:37.234573Z","shell.execute_reply":"2021-11-13T12:07:37.443476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fnet.load_weights(\"../input/jigsaw-toxicity-fnet/model_latest.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T12:07:39.569208Z","iopub.execute_input":"2021-11-13T12:07:39.569514Z","iopub.status.idle":"2021-11-13T12:07:40.321963Z","shell.execute_reply.started":"2021-11-13T12:07:39.569484Z","shell.execute_reply":"2021-11-13T12:07:40.320981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fnet.summary()","metadata":{"id":"ie_rklX2L3b-","outputId":"ba08f590-0cf9-4c61-a9df-0244391b2f05","execution":{"iopub.status.busy":"2021-11-13T12:07:42.130366Z","iopub.execute_input":"2021-11-13T12:07:42.130699Z","iopub.status.idle":"2021-11-13T12:07:42.155593Z","shell.execute_reply.started":"2021-11-13T12:07:42.130654Z","shell.execute_reply":"2021-11-13T12:07:42.154105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the Model.","metadata":{"id":"x_O7FfflL3b-"}},{"cell_type":"code","source":"keras.utils.plot_model(fnet, show_shapes=True)","metadata":{"id":"6Tm1nG0CL3b-","outputId":"0cd78731-3c68-4fdf-f8c7-9b0b2512013d","execution":{"iopub.status.busy":"2021-11-13T12:07:44.592595Z","iopub.execute_input":"2021-11-13T12:07:44.593528Z","iopub.status.idle":"2021-11-13T12:07:45.58984Z","shell.execute_reply.started":"2021-11-13T12:07:44.593482Z","shell.execute_reply":"2021-11-13T12:07:45.588738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.\"></a>\n## 5. Submission","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv\")\ntest[\"text_preprocess\"] = test[\"text\"].apply(Tokenizer.preprocess_string)\ntest_sequences = tokenizer.transform(list(test[\"text_preprocess\"]))\nprint(test_sequences[0])\ntest_ds = tf.data.Dataset.from_tensor_slices((test_sequences)).batch(config.batch_size).prefetch(1)\nscore = fnet.predict(test_ds).reshape(-1)\nsample_submission[\"score\"] = rankdata(score, method='ordinal')\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T12:08:54.381379Z","iopub.execute_input":"2021-11-13T12:08:54.381661Z","iopub.status.idle":"2021-11-13T12:09:18.683104Z","shell.execute_reply.started":"2021-11-13T12:08:54.38163Z","shell.execute_reply":"2021-11-13T12:09:18.682079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<a id=\"6.\"></a>\n## 6. References\n- [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824v3)\n- [Attention Is All You Need](https://arxiv.org/abs/1706.03762v5)\n- [Text Generation using FNet](https://keras.io/examples/nlp/text_generation_fnet/)\n- [English-Spanish Translation: FNet](https://www.kaggle.com/lonnieqin/english-spanish-translation-fnet)","metadata":{"id":"uKmcuxr1L3cH"}}]}