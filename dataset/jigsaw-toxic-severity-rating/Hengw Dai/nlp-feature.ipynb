{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install glove-python-binary","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:05:37.757769Z","iopub.execute_input":"2022-05-05T05:05:37.758099Z","iopub.status.idle":"2022-05-05T05:05:47.359813Z","shell.execute_reply.started":"2022-05-05T05:05:37.75801Z","shell.execute_reply":"2022-05-05T05:05:47.358999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install tokenizers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport gc\nimport pandas as pd\nimport nltk\nimport re\nfrom joblib import Parallel, delayed\nfrom gensim.models import FastText,Word2Vec\nfrom glove import Glove,Corpus\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import KMeans\nfrom scipy import sparse\nfrom bs4 import BeautifulSoup\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import (AutoModel,AutoModelForMaskedLM, \n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments,PreTrainedTokenizerFast,pipeline)\nfrom datasets import Dataset\nfrom sklearn.linear_model import Ridge\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-05T05:05:54.772042Z","iopub.execute_input":"2022-05-05T05:05:54.772539Z","iopub.status.idle":"2022-05-05T05:06:03.381592Z","shell.execute_reply.started":"2022-05-05T05:05:54.772497Z","shell.execute_reply":"2022-05-05T05:06:03.380885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def tokens2sentence(x):\n    return ' '.join(x)\n\ndef dummy_fun(doc):\n    return doc\n\ndef get_words_w2v_vec(model,sent):#使用word2vec获取整句话的vec\n    return np.array(list(map(lambda x:model.wv[x],sent)),dtype=float).mean(axis=0)\n\ndef get_words_glove_vec(model,sent):#使用glove获取整句话的vec\n    return np.array(list(map(lambda x:model.word_vectors[model.dictionary[x]],sent)),dtype=float).mean(axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:06:06.617766Z","iopub.execute_input":"2022-05-05T05:06:06.618325Z","iopub.status.idle":"2022-05-05T05:06:06.628446Z","shell.execute_reply.started":"2022-05-05T05:06:06.618286Z","shell.execute_reply":"2022-05-05T05:06:06.627029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport gc\nimport pandas as pd\nimport re\nimport torch\nfrom joblib import Parallel, delayed\nfrom gensim.models import FastText,Word2Vec\nfrom glove import Glove,Corpus\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import KMeans\nfrom scipy import sparse\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import (AutoModel,AutoModelForMaskedLM, \n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments,PreTrainedTokenizerFast,pipeline)\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.linear_model import Ridge\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm.auto import tqdm\n\ndef tokens2sentence(x):\n    return ' '.join(x)\n\ndef dummy_fun(doc):\n    return doc\n\ndef get_words_w2v_vec(model,sent):#使用word2vec获取整句话的vec\n    return np.array(list(map(lambda x:model.wv[x],sent)),dtype=float).mean(axis=0)\n\ndef get_words_glove_vec(model,sent):#使用glove获取整句话的vec\n    return np.array(list(map(lambda x:model.word_vectors[model.dictionary[x]],sent)),dtype=float).mean(axis=0)\n\ndef prepare_input(tokenizer,text):\n    inputs = tokenizer(text, \n                       max_length=128, \n                        return_tensors='pt',\n                        padding='max_length',\n                        truncation=True)\n    return inputs\nclass TrainDataset(Dataset):\n    def __init__(self, tokenizer, texts):\n        self.tokenizer = tokenizer\n        self.texts = texts.values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.tokenizer,  \n                               self.texts[item])\n        return inputs\n    \ndef model_infer(model,data,device):\n    preds = []\n    model.eval()\n    model.to(device)\n    for inputs in data:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(**inputs)\n        preds.append(y_preds['hidden_states'][0].detach().to('cpu').numpy().mean(1))\n    predictions = np.concatenate(preds,axis=0)\n    return predictions\n\nclass NLP_feature():\n    def __init__(self):\n        self.tokenizers = {}\n        self.embedded_texts = {}\n        self.model_name = 'prajjwal1/bert-tiny'\n        self.zero_shot_model = 'typeform/mobilebert-uncased-mnli' #joeddav/xlm-roberta-large-xnli\n        self.embeddings = {}\n        self.corpus_model = {}\n        self.encoders   = {}\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.candidate_labels = {}\n        self.pipline = None\n        self.n_clusters = 2\n        self.do_mlm = False\n        self.mlm_epochs = 2\n        self.emb_size   = 32\n        self.use_tokenizer = False\n        self.text_columns_def = None\n        self.y = None\n        self.task = None\n        self.embedding_mode = None\n    def fit(self,df,text_columns_def,use_tokenizer,embedding_mode,task,y=None,candidate_labels=None):\n        self.task = task\n        self.use_tokenizer=use_tokenizer\n        self.text_columns_def=text_columns_def\n        self.embedding_mode=embedding_mode\n        self.candidate_labels=candidate_labels\n        self.y = y\n        df = df.loc[:,text_columns_def]\n        for col in text_columns_def:\n            df[col] = df[col].apply(str)\n        if self.task == 'zero-shot-classification':\n            self.pipeline = pipeline(self.task,model=self.zero_shot_model )\n            return\n        ## 训练分词器，如果不使用，则默认使用空格分词\n        if self.use_tokenizer:\n            self.fit_tokenizers(df)\n        if self.embedding_mode != 'Bert':\n            for column in self.text_columns_def:\n                df[f'{column}_tokenized_ids'] = self.tokenize(df,column)\n#         return df\n        ## 训练embedding,初步确定五种： TFIDF、FastText、Word2Vec、Glove、 BertEmbedding,训练的embedding model数量与文本特征列数量相同,使用字典存储，索引为特征列名\n        self.fit_embeddings(df)\n        ## 根据task训练特征提取器,训练的encoder model数量与文本特征列数量相同,使用字典存储，索引为特征列名, 与每一列的embedding model也是一一对应\n        ## 目前支持：有监督回归数值特征、无监督KNN距离特征、无监督关键词离散特征(语义，情感，。。。）\n        ## 特殊任务：根据两两对比数据 生成 每个数据的具体得分，比如通过成对评论的恶意(替换成任何一种语义程度都可以)比较，生成单个评论的恶意程度\n        ## 注意： 关键词离散特征和特殊任务目前只支持使用深度模型，无法自选tokenizer和embedding\n        return self.fit_encoders(df,y)\n        ## 使用提取器处理文本数据生成新特征\n    \n    def fit_tokenizers(self,df):\n        raw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n        raw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=False)\n        raw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n        special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n        trainer = trainers.WordPieceTrainer(vocab_size=500, special_tokens=special_tokens)\n#         df = pd.concat([df_train_new[['comment_text']]])\n\n        def micro_tokenizer(text_df,name):\n            text = pd.concat([text_df])\n            dataset = Dataset.from_pandas(text)\n            def get_training_corpus():\n                for i in range(0, len(dataset), 1000):\n                    yield dataset[i : i + 1000][name]\n            raw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n            tokenizer = PreTrainedTokenizerFast(\n                tokenizer_object=raw_tokenizer,\n                unk_token=\"[UNK]\",\n                pad_token=\"[PAD]\",\n                cls_token=\"[CLS]\",\n                sep_token=\"[SEP]\",\n                mask_token=\"[MASK]\",\n            )\n            return tokenizer\n        \n        for column in self.text_columns_def:\n            print(f'Fitting column: {column} tokenizer')\n            if self.embedding_mode != 'Bert': \n                self.tokenizers.update({column:micro_tokenizer(df[[column]],column)})\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n                self.tokenizers.update({column:tokenizer})\n            \n    def tokenize(self,df,column):\n        if self.use_tokenizer:\n#             for column in self.text_columns_def:\n            print(f'Tokenizing column: {column}')\n            tokenizer = self.tokenizers[column]\n            return list(map(lambda x: self.tokenizers[column].convert_ids_to_tokens(x),\n                            self.tokenizers[column](df[column].to_list())['input_ids']))\n        else:\n            return [text.split(' ') for text in df[column].to_list()]\n    \n    def fit_embeddings(self,df):\n        if self.embedding_mode == 'TFIDF':\n            def micro_tfidf(text_df,name):\n                vectorizer = TfidfVectorizer(\n                    analyzer = 'word',\n                    tokenizer = dummy_fun,\n                    preprocessor = dummy_fun,\n                    token_pattern = None\n                )\n                self.embeddings.update({name:vectorizer.fit(text_df.to_list())})\n            for column in self.text_columns_def:\n                print(f'Fitting column: {column} tfidf embedding')\n                micro_tfidf(df[f'{column}_tokenized_ids'],column)\n                \n        elif self.embedding_mode == 'FastText':\n            def micro_fasttext(text_df,name):\n                model = FastText(vector_size=self.emb_size)\n#                 text_df = text_df.apply(tokens2sentence)\n                model.build_vocab(text_df)\n                model.train(\n                    text_df, epochs=model.epochs,\n                    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n                )\n                self.embeddings.update({name:model})\n            for column in self.text_columns_def:\n                print(f'Fitting column: {column} fasttext embedding')\n                micro_fasttext(df[f'{column}_tokenized_ids'],column)\n        elif self.embedding_mode == 'Word2Vec':\n            def micro_word2vec(text_df,name):\n                model = Word2Vec(vector_size=self.emb_size,min_count=-1)\n                model.build_vocab(text_df)\n#                 model.wv.key_to_index = self.tokenizers[name].vocab\n                model.train(\n                    text_df, epochs=model.epochs,\n                    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n                )\n                self.embeddings.update({name:model})\n            for column in self.text_columns_def:\n                print(f'Fitting column: {column} word2vec embedding')\n                micro_word2vec(df[f'{column}_tokenized_ids'],column)\n        elif self.embedding_mode == 'Glove':\n            def micro_glove(text_df,name):\n                corpus_model = Corpus()\n                corpus_model.fit(text_df,window=10,ignore_missing=False)\n                model = Glove(no_components=self.emb_size)\n                model.fit(corpus_model.matrix,epochs=10)\n                model.add_dictionary(corpus_model.dictionary)\n                self.embeddings.update({name:model})\n                self.corpus_model.update({name:corpus_model})\n            for column in self.text_columns_def:\n                print(f'Fitting column: {column} glove embedding')\n                micro_glove(df[f'{column}_tokenized_ids'],column)\n        elif self.embedding_mode == 'Bert':\n            def micro_bert(text_df,name):\n                text_df = text_df.apply(lambda x: x.replace('\\n',''))\n                text  = '\\n'.join(text_df.tolist())\n                with open('text.txt','w') as f:\n                    f.write(text)\n                model = AutoModelForMaskedLM.from_pretrained(self.model_name)\n                model.to(self.device)\n                model_path = f'./{name}_transformer'\n                train_dataset = LineByLineTextDataset(\n                    tokenizer=self.tokenizers[name],\n                    file_path=\"text.txt\",\n                    block_size=128)\n                data_collator = DataCollatorForLanguageModeling(\n                    tokenizer=self.tokenizers[name], mlm=True, mlm_probability=0.15)\n                training_args = TrainingArguments(\n                    output_dir=model_path,\n                    num_train_epochs=self.mlm_epochs,\n                    per_device_train_batch_size=8,\n                    per_device_eval_batch_size=8,\n                    evaluation_strategy= 'no',\n                    save_strategy='no',\n                    report_to = \"none\")\n                trainer = Trainer(\n                    model=model,\n                    args=training_args,\n                    data_collator=data_collator,\n                    train_dataset=train_dataset,\n                    eval_dataset=None)\n                if self.do_mlm:\n                    trainer.train()\n                trainer.save_model(model_path)\n                self.embeddings.update({name:model_path})\n            for column in self.text_columns_def:\n                print(f'Fitting column: {column} bert embedding')\n                micro_bert(df[column],column)\n        else:\n            raise NotImplementedError\n    def emb_text(self,raw_df,name):\n#             tokenizer = self.tokenizers[name]\n        raw_df[name] = raw_df[name].apply(str)\n        embedding = self.embeddings[name]\n        embedded_text = None\n        if self.embedding_mode == 'TFIDF':\n            embedded_text = embedding.transform(self.tokenize(raw_df,name))\n        elif self.embedding_mode == 'FastText':\n            embedded_text = sparse.csr_matrix(embedding.wv[\n                list(map(lambda x:tokens2sentence(x),self.tokenize(raw_df,name)))\n            ])\n        elif self.embedding_mode == 'Word2Vec':\n            text_df = self.tokenize(raw_df,name)\n            embedding.build_vocab(text_df)\n            embedding.train(\n                    text_df, epochs=embedding.epochs,\n                    total_examples=embedding.corpus_count, total_words=embedding.corpus_total_words,\n                )\n            embedded_text = sparse.csr_matrix(np.array(\n                list(map(lambda x:get_words_w2v_vec(embedding,x),text_df))\n            ))\n        elif self.embedding_mode == 'Glove':\n            text_df = self.tokenize(raw_df,name)\n            corpus_model = self.corpus_model[name]\n            corpus_model.fit(text_df,window=10,ignore_missing=False)\n            embedding.fit(corpus_model.matrix,epochs=10)\n            embedding.add_dictionary(corpus_model.dictionary)\n            embedded_text = sparse.csr_matrix(np.array(\n                list(map(lambda x:get_words_glove_vec(embedding,x),text_df))\n            ))\n        elif self.embedding_mode == 'Bert':\n            model = AutoModel.from_pretrained(self.embeddings[name],output_hidden_states=True)\n            encoded_input = TrainDataset(self.tokenizers[name],raw_df[name])\n            embedded_text = sparse.csr_matrix(model_infer(model,encoded_input,self.device))\n            # embedded_text = sparse.csr_matrix(model(**encoded_input)['hidden_states'][0].detach().to('cpu').numpy().mean(1))\n            # embedded_text = sparse.csr_matrix(model(**encoded_input)[0].detach().numpy().mean(1))\n        else:\n            raise NotImplementedError\n        return embedded_text\n    def fit_encoders(self,df,y=None):\n        if self.task == 'embedding':\n            res_dict={}\n            for column in self.text_columns_def:\n                print(f'Updating column: {column} embeddings output')\n                res_dict.update({column:self.emb_text(df,column)})\n            return res_dict\n#                 df[f\"{column}_meta_feature\"] = self.emb_text(df[column],column)\n\n        elif self.task == 'supervise' or self.task == 'unsupervise':\n            def micro_regressor(embedding_array,y):\n                regressor = Ridge(random_state=42, alpha=0.8)\n                regressor.fit(embedding_array, y)\n                return regressor\n            def micro_cluster(embedding_array):\n                cluster = KMeans(n_clusters=self.n_clusters)\n                cluster.fit(embedding_array)\n                return cluster\n            for column in self.text_columns_def:\n                print(f'Fitting column: {column} encoder')\n                encoders = []\n                folds = KFold(n_splits=5, shuffle=True, random_state=42)\n                for fold_n, (train_index, valid_index) in enumerate(folds.split(df)):\n                    if self.embedding_mode != 'Bert':\n                        trn =  df[f'{column}_tokenized_ids'][train_index]\n                        vld =  df[f'{column}_tokenized_ids'][valid_index]\n                    else:\n                        trn = TrainDataset(self.tokenizers[column],df[column][train_index])\n                        vld = TrainDataset(self.tokenizers[column],df[column][valid_index])\n                    if self.embedding_mode=='TFIDF':\n                        trn = self.embeddings[column].transform(trn)\n                        vld = self.embeddings[column].transform(vld)\n                    elif self.embedding_mode=='FastText':\n                        trn = sparse.csr_matrix(self.embeddings[column].wv[(trn.apply(tokens2sentence))])\n                        vld = self.embeddings[column].wv[(vld.apply(tokens2sentence))]\n                    elif self.embedding_mode=='Word2Vec':\n                        trn = sparse.csr_matrix(np.array(list(map(lambda x:get_words_w2v_vec(self.embeddings[column],x),trn))))\n                        vld = np.array(list(map(lambda x:get_words_w2v_vec(self.embeddings[column],x),vld)))\n                    elif self.embedding_mode=='Glove':\n                        trn = sparse.csr_matrix(np.array(list(map(lambda x:get_words_glove_vec(self.embeddings[column],x),trn))))\n                        vld = np.array(list(map(lambda x:get_words_glove_vec(self.embeddings[column],x),vld)))\n                    elif self.embedding_mode=='Bert':\n                        model = AutoModel.from_pretrained(self.embeddings[column],output_hidden_states=True)\n                        trn = sparse.csr_matrix(model_infer(model,trn,self.device))\n                        vld = sparse.csr_matrix(model_infer(model,vld,self.device))\n                        # trn = sparse.csr_matrix(model(**trn)['hidden_states'][0].detach().to('cpu').numpy().mean(1))\n                        # vld = sparse.csr_matrix(model(**vld)['hidden_states'][0].detach().to('cpu').numpy().mean(1))\n                    else:\n                        raise NotImplementedError\n                        \n                    if self.task == 'supervise': \n                        y_trn = y.iloc[train_index]\n                        encoders.append(micro_regressor(trn,y_trn))\n                    else:\n                        encoders.append(micro_cluster(trn))\n                    \n                    val = encoders[fold_n].predict(vld)\n                    df.loc[valid_index, f\"{column}_meta_feature\"] = val\n                self.encoders.update({column:encoders})\n                if self.embedding_mode != 'Bert':\n                    df = df.drop(columns=[f'{column}_tokenized_ids'])\n            return df.drop(columns=self.text_columns_def)\n    def transform(self,df):\n        if self.task == 'embedding' :\n            res_dict={}\n            for column in self.text_columns_def:\n                print(f'Updating column: {column} embeddings output')\n                res_dict.update({column:self.emb_text(df,column)})\n                # df[f'{column}_embedded'] = self.emb_text(df,column)\n            return res_dict\n        if self.task == 'supervise' or self.task == 'unsupervise':\n            for column in self.text_columns_def:\n                print(f'Transforming column: {column}')\n                embedded_text = self.emb_text(df,column)\n                meta_test = None\n                if self.task == 'supervise':\n                    for idx in range(5):\n                        encoder = self.encoders[column][idx]\n                        pred = (encoder.predict(embedded_text))/5\n                        if idx==0:\n                            meta_test = pred\n                        else:\n                            meta_test += pred\n#                     df[f'{column}_transformed'] = meta_test\n                else:\n                    for idx in range(5):\n                        encoder = self.encoders[column][idx]\n                        pred = np.eye(self.n_clusters)[encoder.predict(embedded_text)]\n                        if idx==0:\n                            meta_test = pred\n                        else:\n                            meta_test += pred\n                    meta_test = np.argmax(meta_test,axis=1)\n#                     meta_test = pd.DataFrame(np.eye(self.n_clusters)[np.argmax(meta_test,axis=1)])\n#                     for idx in range(self.n_clusters):\n#                         df[f'{column}_transformed_class{idx}'] =  meta_test[idx]\n                df[f'{column}_meta_feature'] = meta_test\n            return df\n        elif self.task == 'zero-shot-classification':\n            for column in self.text_columns_def:\n                pred_labels=[]\n                classifier = self.pipeline\n                candidate_labels = self.candidate_labels[column]\n                for text in df[column]:\n#                     classifier = self.pipline[]\n                    text = text[:128]\n                    results = classifier(text, candidate_labels)\n                    pred_labels.append(results['labels'][np.argmax(results['scores'])])\n                df[f'{column}_meta_feature'] = pred_labels\n            return df\n        else:\n            raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:06:09.516666Z","iopub.execute_input":"2022-05-05T05:06:09.516962Z","iopub.status.idle":"2022-05-05T05:06:09.598359Z","shell.execute_reply.started":"2022-05-05T05:06:09.516931Z","shell.execute_reply":"2022-05-05T05:06:09.597665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:06:41.914724Z","iopub.execute_input":"2022-05-05T05:06:41.914984Z","iopub.status.idle":"2022-05-05T05:06:41.970171Z","shell.execute_reply.started":"2022-05-05T05:06:41.914953Z","shell.execute_reply":"2022-05-05T05:06:41.969427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = NLP_feature()\n# nlp.tokenizers\nnlp.do_mlm = True\nnlp.emb_size=100\nnlp.n_clusters=5\ndf = nlp.fit(df_train,['text'],True,'Bert','supervise',df_train['target'],None)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:07:29.321543Z","iopub.execute_input":"2022-05-05T05:07:29.322241Z","iopub.status.idle":"2022-05-05T05:09:39.340814Z","shell.execute_reply.started":"2022-05-05T05:07:29.322194Z","shell.execute_reply":"2022-05-05T05:09:39.340063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['text_meta_feature'] = df['text_meta_feature']","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:10:07.353089Z","iopub.execute_input":"2022-05-05T05:10:07.353391Z","iopub.status.idle":"2022-05-05T05:10:07.358755Z","shell.execute_reply.started":"2022-05-05T05:10:07.35336Z","shell.execute_reply":"2022-05-05T05:10:07.358086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(nlp.tokenizers,'\\n',nlp.embeddings,'\\n',nlp.encoders)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:10:11.5851Z","iopub.execute_input":"2022-05-05T05:10:11.585585Z","iopub.status.idle":"2022-05-05T05:10:11.595122Z","shell.execute_reply.started":"2022-05-05T05:10:11.585548Z","shell.execute_reply":"2022-05-05T05:10:11.594207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = nlp.transform(df_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:10:26.08093Z","iopub.execute_input":"2022-05-05T05:10:26.08146Z","iopub.status.idle":"2022-05-05T05:10:33.452173Z","shell.execute_reply.started":"2022-05-05T05:10:26.081422Z","shell.execute_reply":"2022-05-05T05:10:33.451444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.to_csv('nlp_trn.csv',index=False)\ntest.to_csv('nlp_tst.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:11:38.833802Z","iopub.execute_input":"2022-05-05T05:11:38.83437Z","iopub.status.idle":"2022-05-05T05:11:38.921703Z","shell.execute_reply.started":"2022-05-05T05:11:38.83433Z","shell.execute_reply":"2022-05-05T05:11:38.921041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lab","metadata":{}},{"cell_type":"code","source":"model_name = 'prajjwal1/bert-tiny'\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_df = df_train['comment_text']\ntext_df = text_df.apply(lambda x: x.replace('\\n',''))\ntext  = '\\n'.join(text_df.tolist())\nwith open('text.txt','w') as f:\n    f.write(text)\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\", #mention train text file here\n    block_size=128)\n#                 valid_dataset = LineByLineTextDataset(\n#                     tokenizer=self.tokenizers[name],\n#                     file_path=\"text.txt\", #mention train text file here\n#                     block_size=128)\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\ntraining_args = TrainingArguments(\n                    output_dir='output', #select model path for checkpoint\n#                     overwrite_output_dir=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy= 'no',\n    save_strategy='no',\n    report_to = \"none\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=None)\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model('./output')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModel.from_pretrained(model_name,output_hidden_states=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_input = tokenizer(text_df.to_list(),max_length=256, return_tensors='pt',padding='max_length',truncation=True)\noutput = model(**encoded_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_input['input_ids'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sparse.csr_matrix(output['hidden_states'][0].detach().numpy().mean(1).shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Datasets","metadata":{}},{"cell_type":"code","source":"TRAIN_DATA_PATH = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\nVALID_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\"\nTEST_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_DATA_PATH)\ndf_valid = pd.read_csv(VALID_DATA_PATH)\ndf_test = pd.read_csv(TEST_DATA_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train=df_train.sample(n=10).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scoring training data","metadata":{}},{"cell_type":"code","source":"cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].mean(axis=1)\n\ndf_train['y'] = df_train['score']\n\n# min_len = (df_train['y'] > 0).sum()  # len of toxic comments\n# df_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=41)  # take non toxic comments\n# df_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\n# df_train_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer.convert_ids_to_tokens(tokenizer.encode('i love you'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dummy_fun(doc):\n    return doc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokenized_comments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = df_train_new['y']\ncomments = df_train_new['comment_text']\ndef id2token(tokenizer,ids):\n    return tokenizer.convert_ids_to_tokens(ids)\n    \ntokenized_comments = [tokenizer.convert_ids_to_tokens(ids) for ids in tokenizer(comments.to_list())['input_ids']]\n\nvectorizer = TfidfVectorizer(\n    analyzer = 'word',\n    tokenizer = dummy_fun,\n    preprocessor = dummy_fun,\n    token_pattern = None)\n\ncomments_tr = vectorizer.fit(tokenized_comments)\ncomments_tr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comments_tr.transform(['adwada','weqw']).toarray().tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.where((comments_tr.toarray()[0])!=0)\n# vectorizer.get_feature_names()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}