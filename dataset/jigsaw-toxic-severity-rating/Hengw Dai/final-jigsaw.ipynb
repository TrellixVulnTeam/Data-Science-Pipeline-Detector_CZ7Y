{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import gc\nimport os\nimport nltk\nimport time\nimport joblib\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport scipy.optimize as optimize\nfrom scipy import sparse\nfrom pprint import pprint\nfrom nltk.corpus import stopwords\nfrom IPython.display import display\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings; warnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:01.792429Z","iopub.execute_input":"2022-01-25T03:20:01.792777Z","iopub.status.idle":"2022-01-25T03:20:03.097834Z","shell.execute_reply.started":"2022-01-25T03:20:01.792677Z","shell.execute_reply":"2022-01-25T03:20:03.097097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def timer(func):\n    def wrapper(*args, **kws):\n        st = time.time()\n        res = func(*args, **kws)\n        et = time.time()\n        tt = (et-st)/60\n        print(f'Time taken is {tt:.2f} mins')\n        return res\n    return wrapper","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:03.101159Z","iopub.execute_input":"2022-01-25T03:20:03.101771Z","iopub.status.idle":"2022-01-25T03:20:03.108469Z","shell.execute_reply.started":"2022-01-25T03:20:03.101733Z","shell.execute_reply":"2022-01-25T03:20:03.107682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(data, col):\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)', r'\\1 \\2 \\3')\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}', r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'([*!?\\']+)', r' \\1 ')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b', r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B', r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}', ' ').str.strip()\n    data[col] = data[col].str.replace(r'[ ]{2,}', ' ').str.strip()\n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    return data\n\nstop = stopwords.words('english')\nlemmatizer = nltk.stem.WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:03.110019Z","iopub.execute_input":"2022-01-25T03:20:03.110427Z","iopub.status.idle":"2022-01-25T03:20:03.128888Z","shell.execute_reply.started":"2022-01-25T03:20:03.11039Z","shell.execute_reply":"2022-01-25T03:20:03.128072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def regular(pre):\n    return (pre-pre.min())/(pre.max()-pre.min())","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:03.130356Z","iopub.execute_input":"2022-01-25T03:20:03.131241Z","iopub.status.idle":"2022-01-25T03:20:03.135784Z","shell.execute_reply.started":"2022-01-25T03:20:03.131203Z","shell.execute_reply":"2022-01-25T03:20:03.135095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test data\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:03.138659Z","iopub.execute_input":"2022-01-25T03:20:03.138957Z","iopub.status.idle":"2022-01-25T03:20:03.238063Z","shell.execute_reply.started":"2022-01-25T03:20:03.138925Z","shell.execute_reply":"2022-01-25T03:20:03.237238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@timer\ndef infer(data_path_name, n_folds, clean_prm = False):\n    test_preds_arr = np.zeros((df_sub.shape[0], n_folds))\n    for fld in range(n_folds):\n        print(\"\\n\\n\")\n        print(f'Predicting model: {data_path_name}_fld{fld}.pkl')\n        pipeline = joblib.load(f'{data_path_name}_fld{fld}.pkl')\n        if clean_prm:\n            test_preds_arr[:,fld] = pipeline.predict(clean(df_sub,'text')['text'])\n        else:\n            test_preds_arr[:,fld] = pipeline.predict(df_sub['text'])\n    return regular(test_preds_arr.mean(axis = 1))","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:03.2393Z","iopub.execute_input":"2022-01-25T03:20:03.239573Z","iopub.status.idle":"2022-01-25T03:20:03.246606Z","shell.execute_reply.started":"2022-01-25T03:20:03.239535Z","shell.execute_reply":"2022-01-25T03:20:03.245865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Toxic Infer","metadata":{}},{"cell_type":"markdown","source":"# preds_1 = infer(\"df\", 7)\n\n# preds_2 = infer(\"df_clean\", 7,True)\n\npreds_1 = infer(\"../input/tfidf-model/df2_pkl/df2\",7)\npreds_2 = infer(\"../input/tfidf-model/dfu_pkl/dfu\", 7)\npreds_3 = infer(\"../input/extra-data-tfidf/dfe_pkl/dfe\", 7)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T10:07:21.28917Z","iopub.execute_input":"2022-01-12T10:07:21.289462Z","iopub.status.idle":"2022-01-12T10:10:35.222885Z","shell.execute_reply.started":"2022-01-12T10:07:21.28943Z","shell.execute_reply":"2022-01-12T10:10:35.222067Z"}}},{"cell_type":"markdown","source":"# Model Ensemble","metadata":{}},{"cell_type":"markdown","source":"# Attention-Large","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom torch.utils.data import TensorDataset, SequentialSampler, RandomSampler, DataLoader\n\n# Utils\nfrom tqdm import tqdm\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:03.248091Z","iopub.execute_input":"2022-01-25T03:20:03.248558Z","iopub.status.idle":"2022-01-25T03:20:09.703647Z","shell.execute_reply.started":"2022-01-25T03:20:03.248513Z","shell.execute_reply":"2022-01-25T03:20:09.702904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(42)\n\nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']        \n        \n        return {\n            'ids': torch.tensor(ids),\n            'mask': torch.tensor(mask)\n        }\n        \nclass AttentionHead(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass JigsawModel(nn.Module):\n    def __init__(self,model_name):\n        super(JigsawModel,self).__init__()\n        config = AutoConfig.from_pretrained(model_name)\n        config.update({\n            \"hidden_dropout_prob\": 0.3,\n            \"layer_norm_eps\": 1e-7,\n            \"output_hidden_states\": True\n            }) \n        self.h_size = config.hidden_size\n        self.transformer = AutoModel.from_pretrained(model_name, config = config)\n        self.head = AttentionHead(self.h_size*4)\n        self.linear = nn.Linear(self.h_size*2, 1)\n        self.linear_out = nn.Linear(self.h_size*8, 1)\n\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n       \n        all_hidden_states = torch.stack(transformer_out.hidden_states)\n        cat_over_last_layers = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n        )\n        \n        cls_pooling = cat_over_last_layers[:, 0]   \n        head_logits = self.head(cat_over_last_layers)\n        y_hat = self.linear_out(torch.cat([head_logits, cls_pooling], -1))\n        \n        return y_hat\n\n@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device)\n        mask = data['mask'].to(device)\n        \n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS\n\ndef inference(model_paths, dataloader, device, norm):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JigsawModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        \n        if norm:\n            preds = regular(preds)\n            \n        final_preds.append(preds)\n        \n        del model;\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:09.705002Z","iopub.execute_input":"2022-01-25T03:20:09.705258Z","iopub.status.idle":"2022-01-25T03:20:09.732413Z","shell.execute_reply.started":"2022-01-25T03:20:09.705224Z","shell.execute_reply":"2022-01-25T03:20:09.731709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom torch.utils.data import TensorDataset, SequentialSampler, RandomSampler, DataLoader\n\n# Utils\nfrom tqdm import tqdm\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:09.733646Z","iopub.execute_input":"2022-01-25T03:20:09.734484Z","iopub.status.idle":"2022-01-25T03:20:09.741853Z","shell.execute_reply.started":"2022-01-25T03:20:09.734446Z","shell.execute_reply":"2022-01-25T03:20:09.741024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Large","metadata":{}},{"cell_type":"markdown","source":"# Attention base","metadata":{}},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(model_name)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.2,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        weights = self.attention(last_layer_hidden_states)\n                \n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:09.743135Z","iopub.execute_input":"2022-01-25T03:20:09.743453Z","iopub.status.idle":"2022-01-25T03:20:09.751913Z","shell.execute_reply.started":"2022-01-25T03:20:09.743355Z","shell.execute_reply":"2022-01-25T03:20:09.7511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Large + tanh","metadata":{}},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self,model_name):\n        super(JigsawModel,self).__init__()\n        config = AutoConfig.from_pretrained(model_name)\n        config.update({\n            \"hidden_dropout_prob\": 0.,\n            \"layer_norm_eps\": 1e-7,\n            \"output_hidden_states\": True\n            }) \n        self.h_size = config.hidden_size\n        self.transformer = AutoModel.from_pretrained(model_name, config = config)\n        self.head = AttentionHead(self.h_size*4)\n        self.linear = nn.Linear(self.h_size*8, self.h_size // 2)\n        self.linear_out = nn.Linear(self.h_size // 2, 1)\n        self.tanh = nn.Tanh()\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n       \n        all_hidden_states = torch.stack(transformer_out.hidden_states)\n        cat_over_last_layers = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n        )\n        \n        cls_pooling = cat_over_last_layers[:, 0]   \n        head_logits = self.head(cat_over_last_layers)\n        logits = self.tanh(self.linear(torch.cat([head_logits, cls_pooling], -1)))\n        y_hat = self.linear_out(logits)\n        \n        return y_hat","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:09.753188Z","iopub.execute_input":"2022-01-25T03:20:09.753589Z","iopub.status.idle":"2022-01-25T03:20:09.764895Z","shell.execute_reply.started":"2022-01-25T03:20:09.75355Z","shell.execute_reply":"2022-01-25T03:20:09.764173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention large + dense","metadata":{}},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self,model_name):\n        super(JigsawModel,self).__init__()\n        config = AutoConfig.from_pretrained(model_name)\n        config.update({\n            \"hidden_dropout_prob\": 0.,\n            \"layer_norm_eps\": 1e-7,\n            \"output_hidden_states\": True\n            }) \n        self.h_size = config.hidden_size\n        self.transformer = AutoModel.from_pretrained(model_name, config = config)\n        self.head = AttentionHead(self.h_size*4)\n        self.drop = nn.Dropout(p=0.2)\n        self.layer_norm = nn.LayerNorm(self.h_size*8)\n        self.dense = nn.Sequential(\n            nn.Linear(self.h_size*8, self.h_size // 2),\n#             nn.LeakyReLU(negative_slope=0.01),\n            nn.Tanh(),\n            nn.Dropout(0.2),\n            nn.Linear(self.h_size // 2, 1)\n        )\n\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n       \n        all_hidden_states = torch.stack(transformer_out.hidden_states)\n        cat_over_last_layers = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n        )\n        \n        cls_pooling = cat_over_last_layers[:, 0]   \n        head_logits = self.head(cat_over_last_layers)\n        cat_logits = torch.cat([head_logits, cls_pooling], -1)\n        features = self.drop(self.layer_norm(cat_logits))\n        y_hat = self.dense(features)\n        \n        return y_hat","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:09.766153Z","iopub.execute_input":"2022-01-25T03:20:09.766462Z","iopub.status.idle":"2022-01-25T03:20:09.779142Z","shell.execute_reply.started":"2022-01-25T03:20:09.766428Z","shell.execute_reply":"2022-01-25T03:20:09.778385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    model_name = '../input/j-dblv3-29-67-att-dense-5f-02ldrop-0-5margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ntest_dataset = JigsawDataset(df_sub, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\nMODEL_PATHS = [\n    '../input/j-dblv3-29-67-att-dense-5f-02ldrop-0-5margin/Loss-Fold-0.bin',\n    '../input/j-dblv3-29-67-att-dense-5f-02ldrop-0-5margin/Loss-Fold-1.bin',\n    '../input/j-dblv3-29-67-att-dense-5f-02ldrop-0-5margin/Loss-Fold-2.bin',\n    '../input/j-dblv3-29-67-att-dense-5f-02ldrop-0-5margin/Loss-Fold-3.bin',\n    '../input/j-dblv3-29-67-att-dense-5f-02ldrop-0-5margin/Loss-Fold-4.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds45 = regular(preds)\npreds45","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:20:09.780611Z","iopub.execute_input":"2022-01-25T03:20:09.78113Z","iopub.status.idle":"2022-01-25T03:32:51.074922Z","shell.execute_reply.started":"2022-01-25T03:20:09.781089Z","shell.execute_reply":"2022-01-25T03:32:51.074188Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meanpooling base","metadata":{}},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        config = AutoConfig.from_pretrained(model_name)\n        config.update({\"output_hidden_states\":True,\n                      \"hidden_dropout_prob\": 0.})                       \n        self.model = AutoModel.from_pretrained(model_name,config=config)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        last_hidden_states = out[0]\n        feature = torch.mean(last_hidden_states, 1)\n        out = self.drop(feature)\n        outputs = self.fc(out)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:32:51.080799Z","iopub.execute_input":"2022-01-25T03:32:51.081308Z","iopub.status.idle":"2022-01-25T03:32:51.099925Z","shell.execute_reply.started":"2022-01-25T03:32:51.081267Z","shell.execute_reply":"2022-01-25T03:32:51.098293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    model_name = '../input/j-rbb-29-66-mpl-5f-02ldrop-0-5margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ntest_dataset = JigsawDataset(df_sub, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\nMODEL_PATHS = [\n    '../input/j-rbb-29-66-mpl-5f-02ldrop-0-5margin/Loss-Fold-0.bin',\n    '../input/j-rbb-29-66-mpl-5f-02ldrop-0-5margin/Loss-Fold-1.bin',\n    '../input/j-rbb-29-66-mpl-5f-02ldrop-0-5margin/Loss-Fold-2.bin',\n    '../input/j-rbb-29-66-mpl-5f-02ldrop-0-5margin/Loss-Fold-3.bin',\n    '../input/j-rbb-29-66-mpl-5f-02ldrop-0-5margin/Loss-Fold-4.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds42 = regular(preds)\npreds42","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:32:51.106165Z","iopub.execute_input":"2022-01-25T03:32:51.108477Z","iopub.status.idle":"2022-01-25T03:36:13.491113Z","shell.execute_reply.started":"2022-01-25T03:32:51.108439Z","shell.execute_reply":"2022-01-25T03:36:13.490304Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CONFIG = dict(\n    model_name = '../input/j-rbbo-29-83-mpl-5f-02ldrop-0margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ntest_dataset = JigsawDataset(df_sub, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\nMODEL_PATHS = [\n    '../input/j-rbbo-29-83-mpl-5f-02ldrop-0margin/Loss-Fold-0.bin',\n    '../input/j-rbbo-29-83-mpl-5f-02ldrop-0margin/Loss-Fold-1.bin',\n    '../input/j-rbbo-29-83-mpl-5f-02ldrop-0margin/Loss-Fold-2.bin',\n    '../input/j-rbbo-29-83-mpl-5f-02ldrop-0margin/Loss-Fold-3.bin',\n    '../input/j-rbbo-29-83-mpl-5f-02ldrop-0margin/Loss-Fold-4.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds43 = regular(preds)\npreds43","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:36:13.492612Z","iopub.execute_input":"2022-01-25T03:36:13.493013Z","iopub.status.idle":"2022-01-25T03:39:34.45176Z","shell.execute_reply.started":"2022-01-25T03:36:13.492952Z","shell.execute_reply":"2022-01-25T03:39:34.451061Z"}}},{"cell_type":"code","source":"CONFIG = dict(\n    model_name = '../input/j-rbbtu-2895-mpl-5f-02ldrop-0-5margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ntest_dataset = JigsawDataset(df_sub, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\nMODEL_PATHS = [\n    '../input/j-rbbtu-2895-mpl-5f-02ldrop-0-5margin/Loss-Fold-0.bin',\n    '../input/j-rbbtu-2895-mpl-5f-02ldrop-0-5margin/Loss-Fold-1.bin',\n    '../input/j-rbbtu-2895-mpl-5f-02ldrop-0-5margin/Loss-Fold-2.bin',\n    '../input/j-rbbtu-2895-mpl-5f-02ldrop-0-5margin/Loss-Fold-3.bin',\n    '../input/j-rbbtu-2895-mpl-5f-02ldrop-0-5margin/Loss-Fold-4.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds43 = regular(preds)\npreds43","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:39:34.453383Z","iopub.execute_input":"2022-01-25T03:39:34.453651Z","iopub.status.idle":"2022-01-25T03:43:05.140031Z","shell.execute_reply.started":"2022-01-25T03:39:34.453617Z","shell.execute_reply":"2022-01-25T03:43:05.139392Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    model_name = '../input/j-rbbtm-2886-mpl-5f-02ldrop-0-5margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ntest_dataset = JigsawDataset(df_sub, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\nMODEL_PATHS = [\n    '../input/j-rbbtm-2886-mpl-5f-02ldrop-0-5margin/Loss-Fold-0.bin',\n    '../input/j-rbbtm-2886-mpl-5f-02ldrop-0-5margin/Loss-Fold-1.bin',\n    '../input/j-rbbtm-2886-mpl-5f-02ldrop-0-5margin/Loss-Fold-2.bin',\n    '../input/j-rbbtm-2886-mpl-5f-02ldrop-0-5margin/Loss-Fold-3.bin',\n    '../input/j-rbbtm-2886-mpl-5f-02ldrop-0-5margin/Loss-Fold-4.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds44 = regular(preds)\npreds44","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:43:05.141585Z","iopub.execute_input":"2022-01-25T03:43:05.141851Z","iopub.status.idle":"2022-01-25T03:47:20.99935Z","shell.execute_reply.started":"2022-01-25T03:43:05.141814Z","shell.execute_reply":"2022-01-25T03:47:20.998583Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    model_name = '../input/j-rbbtm-2880-mpl-5f-02ldrop-1-0margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ntest_dataset = JigsawDataset(df_sub, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\nMODEL_PATHS = [\n    '../input/j-rbbtm-2880-mpl-5f-02ldrop-1-0margin/Loss-Fold-0.bin',\n    '../input/j-rbbtm-2880-mpl-5f-02ldrop-1-0margin/Loss-Fold-1.bin',\n    '../input/j-rbbtm-2880-mpl-5f-02ldrop-1-0margin/Loss-Fold-2.bin',\n    '../input/j-rbbtm-2880-mpl-5f-02ldrop-1-0margin/Loss-Fold-3.bin',\n    '../input/j-rbbtm-2880-mpl-5f-02ldrop-1-0margin/Loss-Fold-4.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds48 = regular(preds)\npreds48","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meanpooling base 4layer","metadata":{}},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        config = AutoConfig.from_pretrained(model_name)\n        config.update({\"output_hidden_states\":True,\n                      \"hidden_dropout_prob\": 0.,\n                      \"num_hidden_layers\":4})                       \n        self.model = AutoModel.from_pretrained(model_name,config=config)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        last_hidden_states = out[0]\n        feature = torch.mean(last_hidden_states, 1)\n        out = self.drop(feature)\n        outputs = self.fc(out)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:47:21.000748Z","iopub.execute_input":"2022-01-25T03:47:21.001026Z","iopub.status.idle":"2022-01-25T03:47:21.009709Z","shell.execute_reply.started":"2022-01-25T03:47:21.000989Z","shell.execute_reply":"2022-01-25T03:47:21.009012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    model_name = '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ntest_dataset = JigsawDataset(df_sub, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\nMODEL_PATHS = [\n    '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-0.bin',\n    '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-1.bin',\n    '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-2.bin',\n    '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-3.bin',\n    '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-4.bin',\n    '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-5.bin',\n    '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-6.bin',\n    '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-7.bin',\n    '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-8.bin',\n    '../input/j-rbbtu-2897-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-9.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds41 = regular(preds)\npreds41","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:47:21.010927Z","iopub.execute_input":"2022-01-25T03:47:21.011594Z","iopub.status.idle":"2022-01-25T03:50:24.436427Z","shell.execute_reply.started":"2022-01-25T03:47:21.011556Z","shell.execute_reply":"2022-01-25T03:50:24.435676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CONFIG = dict(\n    model_name = '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ntest_dataset = JigsawDataset(df_sub, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\nMODEL_PATHS = [\n    '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-0.bin',\n    '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-1.bin',\n    '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-2.bin',\n    '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-3.bin',\n    '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-4.bin',\n    '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-5.bin',\n    '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-6.bin',\n    '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-7.bin',\n    '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-8.bin',\n    '../input/j-btbtu-2970-mpl-10f-02ldrop-4layer-0-5margin/Loss-Fold-9.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds42 = regular(preds)\npreds42","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:50:24.437886Z","iopub.execute_input":"2022-01-25T03:50:24.438179Z","iopub.status.idle":"2022-01-25T03:53:18.32072Z","shell.execute_reply.started":"2022-01-25T03:50:24.438143Z","shell.execute_reply":"2022-01-25T03:53:18.320018Z"}}},{"cell_type":"markdown","source":"# Meanpooling base + dense","metadata":{}},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        config = AutoConfig.from_pretrained(model_name)\n        config.update({\"output_hidden_states\":True,\n                      \"hidden_dropout_prob\": 0.})                       \n        self.model = AutoModel.from_pretrained(model_name,config=config)\n        self.drop = nn.Dropout(p=0.2)\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n#         self.tanh = nn.Tanh()\n#         self.fc = nn.Linear(768, CONFIG['num_classes'])\n        self.dense = nn.Sequential(\n            nn.Linear(config.hidden_size, 256),\n#             nn.LeakyReLU(negative_slope=0.01),\n            nn.Tanh(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 1)\n        )\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        last_hidden_states = (out[0])\n        feature = torch.mean(last_hidden_states, 1)\n        out = self.drop(self.layer_norm(feature))\n        outputs = self.dense(out)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:53:18.322301Z","iopub.execute_input":"2022-01-25T03:53:18.322552Z","iopub.status.idle":"2022-01-25T03:53:18.331948Z","shell.execute_reply.started":"2022-01-25T03:53:18.322518Z","shell.execute_reply":"2022-01-25T03:53:18.331099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MeanPooling-large","metadata":{}},{"cell_type":"code","source":"def get_data_loaders(data, model_name, max_length, bs, inference_only=False):\n    \n    x_train = data['text'].tolist()\n    \n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    encoded_train = tokenizer.batch_encode_plus(\n        x_train, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=max_length, \n        return_tensors='pt'\n    )\n\n    if not inference_only:\n        y_train = data['y'].values\n        \n        dataset = TensorDataset(\n        encoded_train['input_ids'],\n        encoded_train['attention_mask'],\n        torch.tensor(y_train)\n        )\n        dataloader = DataLoader(\n        dataset,\n        batch_size=bs,\n        num_workers=2, shuffle=False, pin_memory=True\n        )\n\n        return dataloader\n    \n    else:\n        dataset = TensorDataset(\n        encoded_train['input_ids'],\n        encoded_train['attention_mask']\n        )\n        dataloader = DataLoader(\n        dataset,\n        batch_size=bs,\n        num_workers=2, shuffle=False, pin_memory=True\n        )\n\n        return dataloader\n    \n\ndef predict(model, data_loader, device):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n#     bar = tqdm(enumerate(data_loader), total=len(data_loader))\n    with torch.no_grad():\n        for batch in data_loader:\n        \n            batch = tuple(b.to(device) for b in batch)\n        \n            inputs = {'input_ids':      batch[0],\n                      'attention_mask': batch[1]\n                     }\n                        \n            pred = model(**inputs)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result\n\ndef inference(model_paths, dataloader, device, norm):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JigsawModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = predict(model, dataloader, CONFIG['device'])\n        \n        if norm:\n            preds = regular(preds)\n            \n        final_preds.append(preds)\n        \n        del model;\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds\n\nclass JigsawModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        config.update({\"hidden_dropout_prob\": 0.2})\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(config.hidden_size, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        logits = self.linear(mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:53:18.333543Z","iopub.execute_input":"2022-01-25T03:53:18.333819Z","iopub.status.idle":"2022-01-25T03:53:18.355309Z","shell.execute_reply.started":"2022-01-25T03:53:18.333782Z","shell.execute_reply":"2022-01-25T03:53:18.354675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meanpooling large + dense","metadata":{}},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        config.update({\"hidden_dropout_prob\": 0.})\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(config.hidden_size, 1)\n        self.drop = nn.Dropout(p=0.2)\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        self.dense = nn.Sequential(\n            nn.Linear(config.hidden_size, config.hidden_size // 2),\n#             nn.LeakyReLU(negative_slope=0.01),\n            nn.Tanh(),\n            nn.Dropout(0.2),\n            nn.Linear(config.hidden_size // 2, 1)\n        )\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        features = self.drop(self.layer_norm(mean_embeddings))\n        logits = self.dense(features)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        return preds","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:53:18.35661Z","iopub.execute_input":"2022-01-25T03:53:18.356919Z","iopub.status.idle":"2022-01-25T03:53:18.369792Z","shell.execute_reply.started":"2022-01-25T03:53:18.356884Z","shell.execute_reply":"2022-01-25T03:53:18.369086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CONFIG = dict(\n    model_name = '../input/j-rbbmt-2889-mpl-dense-5f-02ldrop-08gm-0-5margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\ntest_loader = get_data_loaders(df_sub, CONFIG['model_name'], CONFIG['max_length'], CONFIG['test_batch_size'], inference_only=True)\n\nMODEL_PATHS = [\n    '../input/j-rbbmt-2889-mpl-dense-5f-02ldrop-08gm-0-5margin/Loss-Fold-0.bin',\n    '../input/j-rbbmt-2889-mpl-dense-5f-02ldrop-08gm-0-5margin/Loss-Fold-1.bin',\n    '../input/j-rbbmt-2889-mpl-dense-5f-02ldrop-08gm-0-5margin/Loss-Fold-2.bin',\n    '../input/j-rbbmt-2889-mpl-dense-5f-02ldrop-08gm-0-5margin/Loss-Fold-3.bin',\n    '../input/j-rbbmt-2889-mpl-dense-5f-02ldrop-08gm-0-5margin/Loss-Fold-4.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds47 = regular(preds)\npreds47","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:53:18.37118Z","iopub.execute_input":"2022-01-25T03:53:18.371445Z","iopub.status.idle":"2022-01-25T03:57:47.099742Z","shell.execute_reply.started":"2022-01-25T03:53:18.371411Z","shell.execute_reply":"2022-01-25T03:57:47.098916Z"}}},{"cell_type":"code","source":"CONFIG = dict(\n    model_name = '../input/j-dblv3-29-39-mpl-dense-03gm-5f-02ldrop-0-5margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\ntest_loader = get_data_loaders(df_sub, CONFIG['model_name'], CONFIG['max_length'], CONFIG['test_batch_size'], inference_only=True)\n\nMODEL_PATHS = [\n    '../input/j-dblv3-29-39-mpl-dense-03gm-5f-02ldrop-0-5margin/Loss-Fold-0.bin',\n    '../input/j-dblv3-29-39-mpl-dense-03gm-5f-02ldrop-0-5margin/Loss-Fold-1.bin',\n    '../input/j-dblv3-29-39-mpl-dense-03gm-5f-02ldrop-0-5margin/Loss-Fold-2.bin',\n    '../input/j-dblv3-29-39-mpl-dense-03gm-5f-02ldrop-0-5margin/Loss-Fold-3.bin',\n    '../input/j-dblv3-29-39-mpl-dense-03gm-5f-02ldrop-0-5margin/Loss-Fold-4.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds46 = regular(preds)\npreds46","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:57:47.101181Z","iopub.execute_input":"2022-01-25T03:57:47.101713Z","iopub.status.idle":"2022-01-25T04:10:32.288167Z","shell.execute_reply.started":"2022-01-25T03:57:47.101673Z","shell.execute_reply":"2022-01-25T04:10:32.287402Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CONFIG = dict(\n    model_name = '../input/j-rblt-2937-mpl-dense-5f-02ldrop-0-5margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\ntest_loader = get_data_loaders(df_sub, CONFIG['model_name'], CONFIG['max_length'], CONFIG['test_batch_size'], inference_only=True)\n\nMODEL_PATHS = [\n    '../input/j-rblt-2937-mpl-dense-5f-02ldrop-0-5margin/Loss-Fold-0.bin',\n    '../input/j-rblt-2937-mpl-dense-5f-02ldrop-0-5margin/Loss-Fold-1.bin',\n    '../input/j-rblt-2937-mpl-dense-5f-02ldrop-0-5margin/Loss-Fold-2.bin',\n    '../input/j-rblt-2937-mpl-dense-5f-02ldrop-0-5margin/Loss-Fold-3.bin',\n    '../input/j-rblt-2937-mpl-dense-5f-02ldrop-0-5margin/Loss-Fold-4.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds410 = regular(preds)\npreds410","metadata":{"execution":{"iopub.status.busy":"2022-01-25T04:10:32.290715Z","iopub.execute_input":"2022-01-25T04:10:32.29098Z","iopub.status.idle":"2022-01-25T04:20:36.123533Z","shell.execute_reply.started":"2022-01-25T04:10:32.290946Z","shell.execute_reply":"2022-01-25T04:20:36.122789Z"}}},{"cell_type":"code","source":"CONFIG = dict(\n    model_name = '../input/j-rblt-n2937-mpl-dense-5f-02ldrop-0-5margin',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\ntest_loader = get_data_loaders(df_sub, CONFIG['model_name'], CONFIG['max_length'], CONFIG['test_batch_size'], inference_only=True)\n\nMODEL_PATHS = [\n    '../input/j-rblt-n2937-mpl-dense-5f-02ldrop-0-5margin/Loss-Fold-0.bin',\n    '../input/j-rblt-n2937-mpl-dense-5f-02ldrop-0-5margin/Loss-Fold-1.bin',\n    '../input/j-rblt-n2937-mpl-dense-5f-02ldrop-0-5margin/Loss-Fold-2.bin',\n    '../input/j-rblt-n2937-mpl-dense-5f-02ldrop-0-5margin/Loss-Fold-3.bin',\n    '../input/j-rblt-n2937-mpl-dense-5f-02ldrop-0-5margin/Loss-Fold-4.bin'\n]\npreds = inference(MODEL_PATHS, test_loader, CONFIG['device'],True)\npreds47 = regular(preds)\npreds47","metadata":{"execution":{"iopub.status.busy":"2022-01-25T04:20:36.126272Z","iopub.execute_input":"2022-01-25T04:20:36.126547Z","iopub.status.idle":"2022-01-25T04:31:00.499147Z","shell.execute_reply.started":"2022-01-25T04:20:36.126509Z","shell.execute_reply":"2022-01-25T04:31:00.498435Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# w1 = [0.02, 0.02, 0.06]\n# w2 = [0.2, 0.26]\n# w3 = [1]\nw4 = [-0.023694, 0.015 , 0.047388, 0.019 , -0.018  , 0.019 , 0.07898, 0.07898]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T04:31:00.504643Z","iopub.execute_input":"2022-01-25T04:31:00.506656Z","iopub.status.idle":"2022-01-25T04:31:00.512879Z","shell.execute_reply.started":"2022-01-25T04:31:00.506617Z","shell.execute_reply":"2022-01-25T04:31:00.512285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds1 = preds11*w1[0] + preds12*w1[1] + preds13*w1[2]\n# preds1 = regular(preds1)\n\n# preds2 = preds21*w2[0] + preds22*w2[1]\n# preds2 = regular(preds2)\n\n# preds3 = preds31*w3[0]\n# preds3 = regular(preds3)\n\npreds4 = preds41*w4[0] + preds42*w4[1] + preds43*w4[2] + preds44*w4[3] + preds45*w4[4] + preds46*w4[5] + preds47*w4[6] + preds48*w4[7]\npreds4 = regular(preds4)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T04:31:00.517238Z","iopub.execute_input":"2022-01-25T04:31:00.519655Z","iopub.status.idle":"2022-01-25T04:31:00.528464Z","shell.execute_reply.started":"2022-01-25T04:31:00.519593Z","shell.execute_reply":"2022-01-25T04:31:00.527762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# w0 = [ 0.16, -0.07 ,-0.01 , 0.54]\n# preds0 = preds1*w0[0] + preds2*w0[1] + preds3*w0[2] + preds4*w0[3]\n# preds0 = regular(preds0)\n\nimport matplotlib.pyplot as plt\nplt.hist(preds4,bins = 100)\nplt.show()\n\n# x = [ 0.06 ,0.08 ,0.04 ,0.38]\n\n# predsx = preds_1*x[0] + preds_2*x[1] + preds_3*x[2] + preds0*x[3]\n# predsx = regular(predsx)\n\n# plt.hist(predsx,bins = 100)\n# plt.show()\n\ndf_sub['score'] = preds4\ndf_sub['score'] = df_sub['score'].rank(method='first')\ndf_sub.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T04:31:00.531992Z","iopub.execute_input":"2022-01-25T04:31:00.543488Z","iopub.status.idle":"2022-01-25T04:31:01.336691Z","shell.execute_reply.started":"2022-01-25T04:31:00.543442Z","shell.execute_reply":"2022-01-25T04:31:01.335978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T04:31:01.33798Z","iopub.execute_input":"2022-01-25T04:31:01.338395Z","iopub.status.idle":"2022-01-25T04:31:01.37518Z","shell.execute_reply.started":"2022-01-25T04:31:01.338356Z","shell.execute_reply":"2022-01-25T04:31:01.374276Z"},"trusted":true},"execution_count":null,"outputs":[]}]}