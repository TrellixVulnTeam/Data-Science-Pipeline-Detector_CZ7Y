{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-06T06:44:32.333641Z","iopub.execute_input":"2022-02-06T06:44:32.334129Z","iopub.status.idle":"2022-02-06T06:44:32.434638Z","shell.execute_reply.started":"2022-02-06T06:44:32.334027Z","shell.execute_reply":"2022-02-06T06:44:32.433932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:44:32.436283Z","iopub.execute_input":"2022-02-06T06:44:32.436524Z","iopub.status.idle":"2022-02-06T06:44:32.439929Z","shell.execute_reply.started":"2022-02-06T06:44:32.436493Z","shell.execute_reply":"2022-02-06T06:44:32.439269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_left = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndf_left","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:44:32.441096Z","iopub.execute_input":"2022-02-06T06:44:32.441331Z","iopub.status.idle":"2022-02-06T06:44:34.13967Z","shell.execute_reply.started":"2022-02-06T06:44:32.441298Z","shell.execute_reply":"2022-02-06T06:44:34.139018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\n\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_left[category] = df_left[category] * cat_mtpl[category]\n    \ndf['score'] = (df_left[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(float)\ndf['score'] = df['score']/df['score'].max()\ndf['text'] = df_left['comment_text']","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:44:34.141014Z","iopub.execute_input":"2022-02-06T06:44:34.141282Z","iopub.status.idle":"2022-02-06T06:44:34.201529Z","shell.execute_reply.started":"2022-02-06T06:44:34.14122Z","shell.execute_reply":"2022-02-06T06:44:34.200814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:44:34.203833Z","iopub.execute_input":"2022-02-06T06:44:34.20409Z","iopub.status.idle":"2022-02-06T06:44:34.215171Z","shell.execute_reply.started":"2022-02-06T06:44:34.204057Z","shell.execute_reply":"2022-02-06T06:44:34.214557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv')\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:44:34.216659Z","iopub.execute_input":"2022-02-06T06:44:34.2171Z","iopub.status.idle":"2022-02-06T06:44:34.654646Z","shell.execute_reply.started":"2022-02-06T06:44:34.217067Z","shell.execute_reply":"2022-02-06T06:44:34.653995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\n\nstemmer = SnowballStemmer(language='english')\nlemmatizer = WordNetLemmatizer()\n\ndef drop_suffix(sent):\n    text = sent\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n    #text = ' '.join([word for word in text.split(' ') if word not in stop])\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:44:34.655956Z","iopub.execute_input":"2022-02-06T06:44:34.65619Z","iopub.status.idle":"2022-02-06T06:45:16.066841Z","shell.execute_reply.started":"2022-02-06T06:44:34.656158Z","shell.execute_reply":"2022-02-06T06:45:16.066107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef replaceURL(text):\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    return text\n\ndef replaceAbbrev(text):\n    text = text.replace(\"what's\", \"what is \").replace(\"\\'ve\", \" have \").replace(r\"can't\", \"cannot \").replace(r\"n't\", \" not \").replace(r\"i'm\", \"i am \").replace(r\"\\'re\", \" are \").replace(r\"\\'d\", \" would \").replace(r\"\\'ll\", \" will \").replace(r\"\\'scuse\", \" excuse \").replace(r\"\\'s\", \" \")\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:45:16.068029Z","iopub.execute_input":"2022-02-06T06:45:16.068747Z","iopub.status.idle":"2022-02-06T06:45:16.075303Z","shell.execute_reply.started":"2022-02-06T06:45:16.06871Z","shell.execute_reply":"2022-02-06T06:45:16.074474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeUnicode(text):\n    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r' ', text)       \n    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n    text=re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1',text)\n    text=re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1',text)\n    text=re.sub(r'[ ]{2,}',' ',text)\n    text = re.sub('@[^\\s]+','atUser',text)\n    text = re.sub(r'(fuckfuck)','fuck fuck ',text)\n    text = re.sub(r'(f+)( *)([u|*|_]+)( *)([c|*|_]+)( *)(k)+','fuck',text)\n    text = re.sub(r'(h+)(a+)(h+)(a+)','ha ha ',text)\n    text = re.sub(r'(s+ *h+ *[i|!]+ *t+)','shit',text)\n    text = re.sub(r'\\b(n+)(i+)(g+)(a+)\\b','nigga',text)\n    text = re.sub(r'\\b(n+)([i|!]+)(g+)(e+)(r+)\\b','nigger',text)\n    text = re.sub(r'\\b(d+)(o+)(u+)(c+)(h+)(e+)( *)(b+)(a+)(g+)\\b','douchebag',text)\n    text = re.sub(r'([a|@][$|s][s|$])','ass',text)\n    text = re.sub(r'(\\bfuk\\b)','fuck',text)\n    text = re.sub(r\"(^|\\W)\\d+\", \" \", text)\n    text = text.replace(\"5\",\"s\")\n    text = text.replace(\"1\",\"i\")\n    text = text.replace(\"0\",\"o\")\n    text=re.sub(r'([!])\\1\\1{2,}',r' mxm ',text)\n    text=re.sub(r'([?])\\1\\1{2,}',r' mqm ',text)\n    text=re.sub(r'([*])\\1\\1{2,}',r'*',text)\n    return text\n\n\nreplace_pun = {}\nseparators = set('\"%&\\'()+,-./:;<=>@[\\\\]^_`{|}~')\nfor punc in separators:\n    replace_pun[punc] = ' '\nreplace_pun['&']=' and '\n\ndef my_cleaner(s):\n    s = s.lower()\n    s=replaceURL(s)\n    s=removeUnicode(s)\n    s=replaceAbbrev(s)\n    \n    for punc in separators:\n        s = s.replace(punc,replace_pun[punc])               \n    return drop_suffix(s)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:45:16.076707Z","iopub.execute_input":"2022-02-06T06:45:16.077297Z","iopub.status.idle":"2022-02-06T06:45:16.091345Z","shell.execute_reply.started":"2022-02-06T06:45:16.07726Z","shell.execute_reply":"2022-02-06T06:45:16.090599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(my_cleaner)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:45:16.093642Z","iopub.execute_input":"2022-02-06T06:45:16.094354Z","iopub.status.idle":"2022-02-06T06:47:06.135842Z","shell.execute_reply.started":"2022-02-06T06:45:16.094318Z","shell.execute_reply":"2022-02-06T06:47:06.135065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:47:06.137139Z","iopub.execute_input":"2022-02-06T06:47:06.137402Z","iopub.status.idle":"2022-02-06T06:47:06.141547Z","shell.execute_reply.started":"2022-02-06T06:47:06.137369Z","shell.execute_reply":"2022-02-06T06:47:06.140887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe_test = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndataframe_test","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:47:06.142693Z","iopub.execute_input":"2022-02-06T06:47:06.143067Z","iopub.status.idle":"2022-02-06T06:47:06.245504Z","shell.execute_reply.started":"2022-02-06T06:47:06.143032Z","shell.execute_reply":"2022-02-06T06:47:06.244832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe_test = dataframe_test.drop('comment_id', axis = 1)\ndataframe_test['text'] = dataframe_test['text'].apply(my_cleaner)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:47:06.246668Z","iopub.execute_input":"2022-02-06T06:47:06.247015Z","iopub.status.idle":"2022-02-06T06:47:11.815911Z","shell.execute_reply.started":"2022-02-06T06:47:06.246984Z","shell.execute_reply":"2022-02-06T06:47:11.815164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv')\nout\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:47:11.820212Z","iopub.execute_input":"2022-02-06T06:47:11.820492Z","iopub.status.idle":"2022-02-06T06:47:11.843135Z","shell.execute_reply.started":"2022-02-06T06:47:11.820459Z","shell.execute_reply":"2022-02-06T06:47:11.842519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import io\nfrom tqdm import tqdm\nfrom itertools import islice\nimport numpy as np\n\ndef load_vectors(fname, limit):\n  fin = io.open(fname, 'r', encoding = 'utf-8', newline = '\\n', errors = 'ignore')\n  n, d = map(int, fin.readline().split())\n  data = {}\n  for line in tqdm(islice(fin, limit), total = limit):\n    tokens = line.rstrip().split(' ')\n    data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n  return data\n\nvecs = load_vectors('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec', 1300000)   \n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:47:11.84562Z","iopub.execute_input":"2022-02-06T06:47:11.845803Z","iopub.status.idle":"2022-02-06T06:49:31.045991Z","shell.execute_reply.started":"2022-02-06T06:47:11.845781Z","shell.execute_reply":"2022-02-06T06:49:31.045296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nnum_iters = 10\ny_pred_all = np.zeros((dataframe_test.shape[0], num_iters))\nfor i in range(num_iters):\n    df_test =  pd.concat([df[df['score']>0].sample(frac=0.4, random_state = i*i) , \n                        df[df['score']==0].sample(n=int(len(df[df['score']>0])*0.5) , \n                        random_state = i*i)], axis=0)\n    model = Ridge(alpha = 1)\n    tfidf =  TfidfVectorizer(min_df= 3, max_df=0.5,analyzer = 'char_wb', ngram_range = (3,5))\n    vec = tfidf.fit_transform(df_test['text'])\n    model.fit(vec, df_test['score'])\n    \n    vec_test = tfidf.transform(dataframe_test['text'])\n    sample_pred = model.predict(vec_test)\n    y_pred_all[:,i] = sample_pred\n\n    \ny_pred_tf_idf = np.mean(y_pred_all,axis=1)\nmx = y_pred_tf_idf.max()\nmn = y_pred_tf_idf.min()\ny_pred_tf_idf = (y_pred_tf_idf - mn) / (mx - mn)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:49:31.047109Z","iopub.execute_input":"2022-02-06T06:49:31.04755Z","iopub.status.idle":"2022-02-06T06:51:58.341753Z","shell.execute_reply.started":"2022-02-06T06:49:31.047512Z","shell.execute_reply":"2022-02-06T06:51:58.341017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zero = sum(vecs.values()) / len(vecs)\ndef text2vec(text):\n  words = text.split()\n  if len(words) == 0:\n        return zero\n  return sum(list(map(lambda w: np.array(list(vecs.get(w, zero))), words))) / len(words)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:51:58.342922Z","iopub.execute_input":"2022-02-06T06:51:58.343558Z","iopub.status.idle":"2022-02-06T06:51:59.771954Z","shell.execute_reply.started":"2022-02-06T06:51:58.343524Z","shell.execute_reply":"2022-02-06T06:51:59.771112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_iters_w2v = 9\ny_pred_w2v = np.zeros((dataframe_test.shape[0], num_iters_w2v))\nfor i in range(num_iters_w2v):\n    print(i)\n    df_test =  pd.concat([df[df['score']>0].sample(frac=0.4, random_state = i*i) , \n                        df[df['score']==0].sample(n=int(len(df[df['score']>0])*0.5) , \n                        random_state = i*i)], axis=0)\n    model = Ridge(alpha = 1)\n    X_train = list(map(lambda text: text2vec(text), df_test['text']))\n    model.fit(X_train, df_test['score'])\n    \n    X_test = list(map(lambda text: text2vec(text), dataframe_test['text']))\n    sample_pred = model.predict(X_test)\n    y_pred_w2v[:,i] = sample_pred","metadata":{"execution":{"iopub.status.busy":"2022-02-06T06:51:59.773107Z","iopub.execute_input":"2022-02-06T06:51:59.773356Z","iopub.status.idle":"2022-02-06T07:15:01.05236Z","shell.execute_reply.started":"2022-02-06T06:51:59.773323Z","shell.execute_reply":"2022-02-06T07:15:01.051474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_w2v = np.mean(y_pred_w2v,axis=1)\nmx = y_pred_w2v.max()\nmn = y_pred_w2v.min()\ny_pred_w2v = (y_pred_w2v - mn) / (mx - mn)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:15:01.05808Z","iopub.execute_input":"2022-02-06T07:15:01.060436Z","iopub.status.idle":"2022-02-06T07:15:01.068327Z","shell.execute_reply.started":"2022-02-06T07:15:01.060386Z","shell.execute_reply":"2022-02-06T07:15:01.067631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out['score'] = 0.83 * y_pred_tf_idf + 0.17 * y_pred_w2v","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:15:01.072622Z","iopub.execute_input":"2022-02-06T07:15:01.074985Z","iopub.status.idle":"2022-02-06T07:15:01.081677Z","shell.execute_reply.started":"2022-02-06T07:15:01.074939Z","shell.execute_reply":"2022-02-06T07:15:01.080742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out.to_csv('submission1.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:15:01.082958Z","iopub.execute_input":"2022-02-06T07:15:01.08334Z","iopub.status.idle":"2022-02-06T07:15:01.139522Z","shell.execute_reply.started":"2022-02-06T07:15:01.083305Z","shell.execute_reply":"2022-02-06T07:15:01.138763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport re \nimport scipy\nfrom scipy import sparse\n\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \n\nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge, Lasso, BayesianRidge\nfrom sklearn.svm import SVR\n\ndf_train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\ndf_train = df_train.rename(columns={'comment_text':'text'})\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\ntqdm.pandas()\ndf_train['text'] = df_train['text'].progress_apply(text_cleaning)\ndf = df_train.copy()\ndf['y'].value_counts(normalize=True)\nmin_len = (df['y'] >= 0.1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len * 2, random_state=42)\ndf = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\nvec = TfidfVectorizer(min_df= 3, max_df=0.8, analyzer = 'char_wb', ngram_range = (3,5))\nX = vec.fit_transform(df['text'])\nmodel = Ridge(alpha=0.5)\nmodel.fit(X, df['y'])\nl_model = Ridge(alpha=1.)\nl_model.fit(X, df['y'])\ns_model = Ridge(alpha=2.)\ns_model.fit(X, df['y'])\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ntqdm.pandas()\ndf_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\ndf_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)\nX_less_toxic = vec.transform(df_val['less_toxic'])\nX_more_toxic = vec.transform(df_val['more_toxic'])\np1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)\n# Validation Accuracy\nprint(f'val : {(p1 < p2).mean()}')\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ntqdm.pandas()\ndf_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\nX_test = vec.transform(df_sub['text'])\np3 = model.predict(X_test)\np4 = l_model.predict(X_test)\np5 = s_model.predict(X_test)\ndf_sub['score'] = (p3 + p4 + p5) / 3.\ndf_sub['score'] = (df_sub['score'] - df_sub['score'].min()) / (df_sub['score'].max() - df_sub['score'].min())\ndf_sub[['comment_id', 'score']].to_csv(\"submission2.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:15:01.140632Z","iopub.execute_input":"2022-02-06T07:15:01.140869Z","iopub.status.idle":"2022-02-06T07:18:01.740611Z","shell.execute_reply.started":"2022-02-06T07:15:01.140836Z","shell.execute_reply":"2022-02-06T07:18:01.739622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-pretrained-bert\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:18:01.745652Z","iopub.execute_input":"2022-02-06T07:18:01.748171Z","iopub.status.idle":"2022-02-06T07:20:31.521088Z","shell.execute_reply.started":"2022-02-06T07:18:01.748132Z","shell.execute_reply":"2022-02-06T07:20:31.5202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if True:\n    import os\n    import gc\n    import cv2\n    import copy\n    import time\n    import random\n\n    # For data manipulation\n    import numpy as np\n    import pandas as pd\n\n    # Pytorch Imports\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import Dataset, DataLoader\n\n    # For Transformer Models\n    from transformers import AutoTokenizer, AutoModel,AutoConfig\n\n    # Utils\n    from tqdm import tqdm\n\n    # For descriptive error messages\n    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\n\n    class Config:\n\n        model_name = '../input/roberta-base'\n\n        learning_rate = 3e-4\n        epochs = 3\n        train_bs =32\n        valid_bs = 64\n        test_bs = 128\n\n        seed = 2021\n        max_length = 128\n        min_lr = 1e-7\n        scheduler = 'CosineAnnealingLR'\n        T_max  = 500\n        weight_decay = 1e-6\n        max_grad_norm = 1.0\n        num_classes = 1\n        margin = 0.5\n        n_fold = 9\n        n_accululate = 1\n        device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        hidden_size =768\n        num_hidden_layers = 24\n\n        dropout = 0.1\n\n\n    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n\n    MODEL_PATHS = [\n        '../input/robertabase5fold2-linear-256/Loss-Fold-0.bin',\n        '../input/robertabase5fold2-linear-256/Loss-Fold-1.bin',\n        '../input/robertabase5fold2-linear-256/Loss-Fold-2.bin',\n        '../input/robertabase5fold2-linear-256/Loss-Fold-3.bin',\n        '../input/robertabase5fold2-linear-256/Loss-Fold-4.bin'\n    ]\n\n    def set_seed(seed = 42):\n        '''Sets the seed of the entire notebook so results are the same every time we run.\n        This is for REPRODUCIBILITY.'''\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Set a fixed value for the hash seed\n        os.environ['PYTHONHASHSEED'] = str(seed)\n\n    set_seed(Config.seed)\n\n\n    df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    df['text'] = df['text'].apply(my_cleaner)\n    df.head()\n\n\n    class JigsawDataset(Dataset):\n        def __init__(self, df, tokenizer, max_length):\n            self.df = df\n            self.max_len = max_length\n            self.tokenizer = tokenizer\n            self.text = df['text'].values\n\n        def __len__(self):\n            return len(self.df)\n\n        def __getitem__(self, index):\n            text = self.text[index]\n            inputs = self.tokenizer.encode_plus(\n                            text,\n                            truncation=True,\n                            add_special_tokens=True,\n                            max_length=self.max_len,\n                            padding='max_length'\n                        )\n\n            ids = inputs['input_ids']\n            mask = inputs['attention_mask']        \n\n            return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long)\n            }\n\n\n    test_dataset = JigsawDataset(df, tokenizer, max_length=Config.max_length)\n    test_loader = DataLoader(test_dataset, batch_size=Config.test_bs, num_workers=2, shuffle=False, pin_memory=True)\n\n    class JModel(nn.Module):\n        def __init__(self, checkpoint=Config.model_name, Config=Config):\n            super(JModel, self).__init__()\n            self.checkpoint = checkpoint\n            self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n            self.layer_norm = nn.LayerNorm(Config.hidden_size)\n            self.dropout = nn.Dropout(Config.dropout)\n            self.dense = nn.Sequential(\n                nn.Linear(Config.hidden_size, 256),\n                nn.LeakyReLU(negative_slope=0.01),\n                nn.Dropout(Config.dropout),\n                nn.Linear(256, 1)\n            )\n\n        def forward(self, input_ids, attention_mask):\n            _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n            pooled_output = self.layer_norm(pooled_output)\n            pooled_output = self.dropout(pooled_output)\n            preds = self.dense(pooled_output)\n            return preds        \n\n\n    @torch.no_grad()\n    def valid_fn(model, dataloader, device):\n        model.eval()\n\n        dataset_size = 0\n        running_loss = 0.0\n\n        PREDS = []\n\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n\n            outputs = model(ids, mask)\n            PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n\n        PREDS = np.concatenate(PREDS)\n        gc.collect()\n\n        return PREDS\n\n    def inference(model_paths, dataloader, device):\n        final_preds = []\n        for i, path in enumerate(model_paths):\n            model = JModel(Config.model_name)\n            model.to(Config.device)\n            model.load_state_dict(torch.load(path))\n\n            print(f\"Getting predictions for model {i+1}\")\n            preds = valid_fn(model, dataloader, device)\n            final_preds.append(preds)\n\n        final_preds = np.array(final_preds)\n        final_preds = np.mean(final_preds, axis=0)\n        return final_preds    \n\n    preds = inference(MODEL_PATHS, test_loader, Config.device)    \n    df['score'] = preds\n    df['score'] = df['score'].rank(method='first')\n    df.drop('text', axis=1, inplace=True)\n    df.to_csv(\"submission_bert.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:20:31.523159Z","iopub.execute_input":"2022-02-06T07:20:31.523469Z","iopub.status.idle":"2022-02-06T07:24:02.99645Z","shell.execute_reply.started":"2022-02-06T07:20:31.523426Z","shell.execute_reply":"2022-02-06T07:24:02.995663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LSTM, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom wordcloud import STOPWORDS\n\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:02.999623Z","iopub.execute_input":"2022-02-06T07:24:02.999826Z","iopub.status.idle":"2022-02-06T07:24:03.108862Z","shell.execute_reply.started":"2022-02-06T07:24:02.999802Z","shell.execute_reply":"2022-02-06T07:24:03.108196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/toxic-comments-train/train.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\nsample = pd.read_csv('../input/jigsaw-toxic-severity-rating/sample_submission.csv')\ntarget = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:03.11094Z","iopub.execute_input":"2022-02-06T07:24:03.111162Z","iopub.status.idle":"2022-02-06T07:24:05.160006Z","shell.execute_reply.started":"2022-02-06T07:24:03.111131Z","shell.execute_reply":"2022-02-06T07:24:05.159274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['y'] = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].sum(axis=1) > 0\ntrain.drop(['toxic','severe_toxic','obscene','threat','insult','identity_hate'], inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:05.161133Z","iopub.execute_input":"2022-02-06T07:24:05.162591Z","iopub.status.idle":"2022-02-06T07:24:05.183641Z","shell.execute_reply.started":"2022-02-06T07:24:05.162555Z","shell.execute_reply":"2022-02-06T07:24:05.182886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_of_toxic_comments =  train[train.y != 0].shape[0]\ncount_of_toxic_comments","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:05.184785Z","iopub.execute_input":"2022-02-06T07:24:05.18502Z","iopub.status.idle":"2022-02-06T07:24:05.195924Z","shell.execute_reply.started":"2022-02-06T07:24:05.184988Z","shell.execute_reply":"2022-02-06T07:24:05.195139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_toxic = train[train.y != 0]\ntrain_non_toxic = train[train.y == 0].sample(count_of_toxic_comments)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:05.197418Z","iopub.execute_input":"2022-02-06T07:24:05.19785Z","iopub.status.idle":"2022-02-06T07:24:05.219656Z","shell.execute_reply.started":"2022-02-06T07:24:05.197815Z","shell.execute_reply":"2022-02-06T07:24:05.218977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train_toxic, train_non_toxic])\ndf","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:05.22104Z","iopub.execute_input":"2022-02-06T07:24:05.2213Z","iopub.status.idle":"2022-02-06T07:24:05.239595Z","shell.execute_reply.started":"2022-02-06T07:24:05.221268Z","shell.execute_reply":"2022-02-06T07:24:05.238886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['word_count'] = df['comment_text'].apply(lambda x: len(str(x).split()))\n\ndf['unique_word_count'] = df['comment_text'].apply(lambda x: len(set(str(x).split())))\n\ndf['stop_word_count'] = df['comment_text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\ndf['mean_word_length'] = df['comment_text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\ndf['char_count'] = df['comment_text'].apply(lambda x: len(str(x)))\n\ndf['punctuation_count'] = df['comment_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:05.241073Z","iopub.execute_input":"2022-02-06T07:24:05.241893Z","iopub.status.idle":"2022-02-06T07:24:07.94651Z","shell.execute_reply.started":"2022-02-06T07:24:05.241855Z","shell.execute_reply":"2022-02-06T07:24:07.945739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'mean_word_length','char_count', 'punctuation_count']\nTOXIC_COMMENTS = df['y'] == 1\n\nfig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n\nfor i, feature in enumerate(METAFEATURES):\n    sns.distplot(df.loc[~TOXIC_COMMENTS][feature], label='Non Toxic', ax=axes[i][0], color='green')\n    sns.distplot(df.loc[TOXIC_COMMENTS][feature], label='Toxic', ax=axes[i][0], color='red')\n\n    sns.distplot(df[feature], label='Train', ax=axes[i][1])\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis='x', labelsize=12)\n        axes[i][j].tick_params(axis='y', labelsize=12)\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:07.947822Z","iopub.execute_input":"2022-02-06T07:24:07.948058Z","iopub.status.idle":"2022-02-06T07:24:14.540628Z","shell.execute_reply.started":"2022-02-06T07:24:07.948025Z","shell.execute_reply":"2022-02-06T07:24:14.53747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove stopwords & convert to lower case\ndf['comment_text'] = df['comment_text'].apply(lambda x: ' '.join([w for w in str(x).lower().split() if w not in STOPWORDS]))\n\n# Remove Punctuations\ndf[\"comment_text\"] = df['comment_text'].str.replace('[^\\w\\s]','')\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:14.541558Z","iopub.execute_input":"2022-02-06T07:24:14.541782Z","iopub.status.idle":"2022-02-06T07:24:15.299722Z","shell.execute_reply.started":"2022-02-06T07:24:14.541753Z","shell.execute_reply":"2022-02-06T07:24:15.29906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(['y'], axis=1)\ny = df['y']","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:15.301184Z","iopub.execute_input":"2022-02-06T07:24:15.301669Z","iopub.status.idle":"2022-02-06T07:24:15.309148Z","shell.execute_reply.started":"2022-02-06T07:24:15.301632Z","shell.execute_reply":"2022-02-06T07:24:15.308424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:15.310286Z","iopub.execute_input":"2022-02-06T07:24:15.310579Z","iopub.status.idle":"2022-02-06T07:24:15.350664Z","shell.execute_reply.started":"2022-02-06T07:24:15.310545Z","shell.execute_reply":"2022-02-06T07:24:15.350017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.comment_text.values\nX_test = X_test.comment_text.values","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:15.352223Z","iopub.execute_input":"2022-02-06T07:24:15.352414Z","iopub.status.idle":"2022-02-06T07:24:15.356918Z","shell.execute_reply.started":"2022-02-06T07:24:15.352392Z","shell.execute_reply":"2022-02-06T07:24:15.356301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OOV_TOKEN = '<OOV>'\nVOCAB_SIZE = 10000\nMAX_LEN = 100\nEMBEDDING_DIM = 100","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:15.358253Z","iopub.execute_input":"2022-02-06T07:24:15.358666Z","iopub.status.idle":"2022-02-06T07:24:15.366322Z","shell.execute_reply.started":"2022-02-06T07:24:15.35863Z","shell.execute_reply":"2022-02-06T07:24:15.365584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(\n    num_words=VOCAB_SIZE,\n    oov_token=OOV_TOKEN\n)\ntokenizer.fit_on_texts(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:15.367666Z","iopub.execute_input":"2022-02-06T07:24:15.368Z","iopub.status.idle":"2022-02-06T07:24:16.118002Z","shell.execute_reply.started":"2022-02-06T07:24:15.367964Z","shell.execute_reply":"2022-02-06T07:24:16.117263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_seq = tokenizer.texts_to_sequences(X_train)\ntrain_padded = pad_sequences(\n    train_seq, maxlen=MAX_LEN, dtype='int32', padding='post',\n    truncating='post'\n)\n\ntest_seq = tokenizer.texts_to_sequences(X_test)\ntest_padded = pad_sequences(\n    test_seq, maxlen=MAX_LEN, dtype='int32', padding='post',\n    truncating='post'\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:16.11933Z","iopub.execute_input":"2022-02-06T07:24:16.119579Z","iopub.status.idle":"2022-02-06T07:24:17.196975Z","shell.execute_reply.started":"2022-02-06T07:24:16.119547Z","shell.execute_reply":"2022-02-06T07:24:17.196274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n  Embedding(VOCAB_SIZE, EMBEDDING_DIM, name=\"embedding\"),\n    LSTM(64),\n    Dropout(0.2),\n  Dense(128, activation='relu'),\n    Dropout(0.1),\n  Dense(1,activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:17.198276Z","iopub.execute_input":"2022-02-06T07:24:17.198532Z","iopub.status.idle":"2022-02-06T07:24:17.841014Z","shell.execute_reply.started":"2022-02-06T07:24:17.198499Z","shell.execute_reply":"2022-02-06T07:24:17.840285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:17.846278Z","iopub.execute_input":"2022-02-06T07:24:17.846486Z","iopub.status.idle":"2022-02-06T07:24:17.860741Z","shell.execute_reply.started":"2022-02-06T07:24:17.84646Z","shell.execute_reply":"2022-02-06T07:24:17.860046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = EarlyStopping(patience=15, \n                   monitor='loss', \n                   restore_best_weights=True, \n                   mode='min', \n                   verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:17.861872Z","iopub.execute_input":"2022-02-06T07:24:17.863095Z","iopub.status.idle":"2022-02-06T07:24:17.868842Z","shell.execute_reply.started":"2022-02-06T07:24:17.863067Z","shell.execute_reply":"2022-02-06T07:24:17.868138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = model.fit(\n    train_padded,\n    y = y_train,\n    validation_data=(test_padded, y_test),\n    epochs=15,\n    callbacks=es\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:24:17.869999Z","iopub.execute_input":"2022-02-06T07:24:17.870576Z","iopub.status.idle":"2022-02-06T07:27:27.652314Z","shell.execute_reply.started":"2022-02-06T07:24:17.870541Z","shell.execute_reply":"2022-02-06T07:27:27.651544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_target = target\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:27:27.653833Z","iopub.execute_input":"2022-02-06T07:27:27.654097Z","iopub.status.idle":"2022-02-06T07:27:27.657921Z","shell.execute_reply.started":"2022-02-06T07:27:27.654061Z","shell.execute_reply":"2022-02-06T07:27:27.656902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_target['text'] = df_target['text'].apply(lambda x: ' '.join([w for w in str(x).lower().split() if w not in STOPWORDS]))\n\ndf_target[\"text\"] = df_target['text'].str.replace('[^\\w\\s]','')\ndf_target.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:27:27.659333Z","iopub.execute_input":"2022-02-06T07:27:27.659893Z","iopub.status.idle":"2022-02-06T07:27:27.878441Z","shell.execute_reply.started":"2022-02-06T07:27:27.659856Z","shell.execute_reply":"2022-02-06T07:27:27.877752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_seq = tokenizer.texts_to_sequences(df_target.text.values)\ntarget_padded = pad_sequences(\n    target_seq, maxlen=MAX_LEN, dtype='int32', padding='post',\n    truncating='post'\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:27:27.87984Z","iopub.execute_input":"2022-02-06T07:27:27.880099Z","iopub.status.idle":"2022-02-06T07:27:28.503791Z","shell.execute_reply.started":"2022-02-06T07:27:27.880067Z","shell.execute_reply":"2022-02-06T07:27:28.503014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = model.predict(target_padded)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:27:28.505113Z","iopub.execute_input":"2022-02-06T07:27:28.505397Z","iopub.status.idle":"2022-02-06T07:27:30.184455Z","shell.execute_reply.started":"2022-02-06T07:27:28.505363Z","shell.execute_reply":"2022-02-06T07:27:30.183693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:27:30.188984Z","iopub.execute_input":"2022-02-06T07:27:30.192373Z","iopub.status.idle":"2022-02-06T07:27:30.209964Z","shell.execute_reply.started":"2022-02-06T07:27:30.192332Z","shell.execute_reply":"2022-02-06T07:27:30.209148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = result.reshape(-1)\nresult = (result - result.min()) / (result.max() - result.min())","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:27:30.220706Z","iopub.execute_input":"2022-02-06T07:27:30.220943Z","iopub.status.idle":"2022-02-06T07:27:30.225352Z","shell.execute_reply.started":"2022-02-06T07:27:30.220914Z","shell.execute_reply":"2022-02-06T07:27:30.224732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"brt = pd.read_csv(\"submission_bert.csv\")['score']\nbrt = (brt - brt.min()) / (brt.max() - brt.min())","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:27:30.2273Z","iopub.execute_input":"2022-02-06T07:27:30.227581Z","iopub.status.idle":"2022-02-06T07:27:30.244374Z","shell.execute_reply.started":"2022-02-06T07:27:30.227543Z","shell.execute_reply":"2022-02-06T07:27:30.243719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target['score'] = result * 0.1 + brt * 0.9\naue = target['score']","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:27:30.245802Z","iopub.execute_input":"2022-02-06T07:27:30.246069Z","iopub.status.idle":"2022-02-06T07:27:30.256274Z","shell.execute_reply.started":"2022-02-06T07:27:30.246037Z","shell.execute_reply":"2022-02-06T07:27:30.255418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import rankdata\n#aue=rankdata(aue, method='ordinal')\ndata = pd.read_csv(\"./submission1.csv\")\ndata[\"score1\"] = data[\"score\"]\n#data[\"score1\"] = rankdata(data[\"score1\"], method = 'ordinal')\n\ndata[\"score2\"] = pd.read_csv(\"./submission2.csv\")[\"score\"]\n#data[\"score2\"] = rankdata( data[\"score2\"], method='ordinal')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:28:59.349008Z","iopub.execute_input":"2022-02-06T07:28:59.349315Z","iopub.status.idle":"2022-02-06T07:28:59.36607Z","shell.execute_reply.started":"2022-02-06T07:28:59.349284Z","shell.execute_reply":"2022-02-06T07:28:59.365396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['score'] = data['score1'] * 0.5 + data['score2'] * 0.92 + aue * 0.125","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:29:02.798837Z","iopub.execute_input":"2022-02-06T07:29:02.799605Z","iopub.status.idle":"2022-02-06T07:29:02.804858Z","shell.execute_reply.started":"2022-02-06T07:29:02.799565Z","shell.execute_reply":"2022-02-06T07:29:02.804159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = data","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:29:05.593161Z","iopub.execute_input":"2022-02-06T07:29:05.593451Z","iopub.status.idle":"2022-02-06T07:29:07.328093Z","shell.execute_reply.started":"2022-02-06T07:29:05.593409Z","shell.execute_reply":"2022-02-06T07:29:07.327339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[\"score\"] = rankdata( df_test[\"score\"], method='ordinal')\ndf_test","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:29:09.870701Z","iopub.execute_input":"2022-02-06T07:29:09.870967Z","iopub.status.idle":"2022-02-06T07:29:09.886577Z","shell.execute_reply.started":"2022-02-06T07:29:09.870929Z","shell.execute_reply":"2022-02-06T07:29:09.885903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[['comment_id','score']].to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:29:40.390214Z","iopub.execute_input":"2022-02-06T07:29:40.390814Z","iopub.status.idle":"2022-02-06T07:29:40.41337Z","shell.execute_reply.started":"2022-02-06T07:29:40.390776Z","shell.execute_reply":"2022-02-06T07:29:40.412649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"./submission.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:29:42.436084Z","iopub.execute_input":"2022-02-06T07:29:42.436709Z","iopub.status.idle":"2022-02-06T07:29:42.45194Z","shell.execute_reply.started":"2022-02-06T07:29:42.436671Z","shell.execute_reply":"2022-02-06T07:29:42.451136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}