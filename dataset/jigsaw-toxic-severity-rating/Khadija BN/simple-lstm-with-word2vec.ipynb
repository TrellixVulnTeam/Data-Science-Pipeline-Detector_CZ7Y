{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2 style=\"text-align:center;color:blue;\">Objectif :</h2>\n\n<h3 style=\"text-align:center;\">        In this competition we will be ranking comments in order of severity of toxicity. We are given a list of comments, and each comment should be scored according to their relative toxicity. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity.</h3>\n    \n  <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSblnNX1zqaG70dan0DywBXM1VP75dbjCYbkA&usqp=CAU\" width=\"400\"></img>","metadata":{"execution":{"iopub.status.busy":"2021-11-18T09:32:58.395551Z","iopub.execute_input":"2021-11-18T09:32:58.395803Z","iopub.status.idle":"2021-11-18T09:32:58.399634Z","shell.execute_reply.started":"2021-11-18T09:32:58.395771Z","shell.execute_reply":"2021-11-18T09:32:58.398876Z"}}},{"cell_type":"markdown","source":"<p style=\"background-color:orange; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Importing Libraries & Data</b></p> ","metadata":{}},{"cell_type":"code","source":"import sys\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport gensim\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nnltk.download('stopwords')\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Flatten, Dropout, Dense, LSTM, Embedding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-11-19T23:12:39.904843Z","iopub.execute_input":"2021-11-19T23:12:39.905187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndff.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"background-color:orange; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Quick EDA</b></p> ","metadata":{}},{"cell_type":"code","source":"dff.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff.severe_toxic.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff['toxicity'] = (dff[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\ndff = dff[['comment_text', 'toxicity']].rename(columns={'comment_text': 'text'})\ndff.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff.toxicity.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data is not balanced.","metadata":{}},{"cell_type":"code","source":"min_len = (dff['toxicity'] == 1).sum()\ndf_undersample = dff[dff['toxicity'] == 0].sample(n=min_len, random_state=201)\ndff = pd.concat([df_undersample, dff[dff['toxicity'] == 1]])\ndff = shuffle(dff)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff.text = dff.text.map(lambda x:x.replace('\\n', ' '))\ndff.text[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic = dff[dff['toxicity'] == 1]\nnot_toxic = dff[dff['toxicity'] == 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(width=1400, height=700, background_color='white').generate(' '.join(toxic.text.tolist()))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('The most 100 frequent words in the toxic comments', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(width=1400, height=700, background_color='white').generate(' '.join(not_toxic.text.tolist()))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('The most 100 frequent words in the normal comments', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"background-color:orange; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Text Pre-Processing</b></p> ","metadata":{}},{"cell_type":"markdown","source":"We get our X and y variables, then create a copy to work on it :","metadata":{}},{"cell_type":"code","source":"y = dff.toxicity\nx = dff.drop('toxicity', axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = x.copy()\ntexts.reset_index(inplace = True, drop = True)\ntexts.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to not get a RecursionError, we reset our recursionlimit to 6000.","metadata":{}},{"cell_type":"code","source":"print(sys.getrecursionlimit())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.setrecursionlimit(6000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When dealing with text, we should first do some cleaning and stemming :\n\n### What Is Stemming ?\n\nThe process of removing a part of a word, or reducing a word to its stem or root.\n\n### Example :\n\nLet’s assume we have a set of words — **send, sent and sending**. All three words are different tenses of the same root word **send**. So after we stem the words, we’ll have just the one word — send. ","metadata":{}},{"cell_type":"code","source":"ps = PorterStemmer()\ncorpus = []\n\nfor i in tqdm(range(0, len(texts))) :\n    cleaned = re.sub('[^a-zA-Z]', ' ', texts['text'][i])\n    cleaned = cleaned.lower().split()\n    \n    cleaned = [ps.stem(word) for word in cleaned if not word in stopwords.words('english')]\n    cleaned = ' '.join(cleaned)\n    corpus.append(cleaned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our model will not be able to deal with text, it should have numbers as input, that's why we do first word embedding.\n\n### What is Word Embedding ?\n\nWord embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n\n\nWord Embeddings are vector representations of a particular word. ","metadata":{}},{"cell_type":"code","source":"DIM = 100\n\nX = [d.split() for d in corpus]\nw2v_model = gensim.models.Word2Vec(sentences = X, vector_size = DIM, window = 10, min_count = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how many words in our vocabulary :","metadata":{}},{"cell_type":"code","source":"len(w2v_model.wv.key_to_index.keys()) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can find similar words to a specific one, let's try with the word 'toxic' :","metadata":{}},{"cell_type":"code","source":"w2v_model.wv.most_similar('toxic')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we tokenize the sentences and convert X into sequences of numbers :","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tokenizer.texts_to_sequences(X)\nX[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's convert all the sentences to have the same length which is 20 in our case :","metadata":{}},{"cell_type":"code","source":"X = pad_sequences(X, padding = 'pre', maxlen = 20)\nX[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will feed these vectors as initial weights to our model then recreate these vectors to get better accuracy :","metadata":{}},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1 \nvocab = tokenizer.word_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_weights_matrix(model) :\n    weights_matrix = np.zeros((vocab_size, DIM))\n    \n    for word, i in vocab.items() :\n        weights_matrix[i] = model.wv[word]\n        \n    return weights_matrix\n\n\nembedding_vectors = get_weights_matrix(w2v_model) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"background-color:orange; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Modeling & Training</b></p> ","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(vocab_size, output_dim = DIM, weights = [embedding_vectors], input_length = 20)) \nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(64))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1, activation = 'linear'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = 'accuracy')\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nes = EarlyStopping(patience=3, \n                   monitor='loss', \n                   restore_best_weights=True, \n                   mode='min', \n                   verbose=1)\n\n# train the model \nhist = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 15,\n                 callbacks=es, batch_size = 32, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n\n# visualize the models accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc = 'upper left')\nplt.show()  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"background-color:orange; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Submission</b></p> ","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_text = tokenizer.texts_to_sequences(sub.text)\nnew_text = pad_sequences(new_text, maxlen = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['score'] = model.predict(new_text) * 1000 \nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please If You Like This Notebook, Please don't Forget To Upvote It ;","metadata":{}}]}