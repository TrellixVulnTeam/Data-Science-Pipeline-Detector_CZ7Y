{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport json\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:23.119914Z","iopub.execute_input":"2022-01-16T04:43:23.120279Z","iopub.status.idle":"2022-01-16T04:43:23.938299Z","shell.execute_reply.started":"2022-01-16T04:43:23.120197Z","shell.execute_reply":"2022-01-16T04:43:23.937597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir1 = '/kaggle/input/jigsaw-toxic-comment-classification-challenge'\nbase_dir2 = '/kaggle/input/jigsaw-toxic-severity-rating'\nbase_dir3 = '/kaggle/input/ruddit-jigsaw-dataset/'\ndf = pd.read_csv(os.path.join(base_dir1,'train.csv'))\n\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df[category] = df[category] * cat_mtpl[category]\n\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(float)\ndf['y'] = df['y']/df['y'].max()\n\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:23.939917Z","iopub.execute_input":"2022-01-16T04:43:23.940164Z","iopub.status.idle":"2022-01-16T04:43:25.91226Z","shell.execute_reply.started":"2022-01-16T04:43:23.940128Z","shell.execute_reply":"2022-01-16T04:43:25.911474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 5\n# sample the traning data\ndef sample_train(df, fold):\n    frac_1 = 0.5\n    frac_1_factor = 1.2\n    tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = fold) , \n                        df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                        random_state = 0)], axis=0).sample(frac=1, random_state = fold)\n    return tmp_df","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:25.913655Z","iopub.execute_input":"2022-01-16T04:43:25.913928Z","iopub.status.idle":"2022-01-16T04:43:25.92029Z","shell.execute_reply.started":"2022-01-16T04:43:25.913888Z","shell.execute_reply":"2022-01-16T04:43:25.919594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fld in range(n_folds):\n    df_sub=sample_train(df,fld)\n    df_sub.to_csv(f'df_fld{fld}.csv', index=False)\n    print(f\"df_fld{fld}.csv\",\"created\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:25.92165Z","iopub.execute_input":"2022-01-16T04:43:25.922217Z","iopub.status.idle":"2022-01-16T04:43:27.759135Z","shell.execute_reply.started":"2022-01-16T04:43:25.922172Z","shell.execute_reply":"2022-01-16T04:43:27.758255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nimport transformers\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:27.763955Z","iopub.execute_input":"2022-01-16T04:43:27.764224Z","iopub.status.idle":"2022-01-16T04:43:34.236569Z","shell.execute_reply.started":"2022-01-16T04:43:27.76419Z","shell.execute_reply":"2022-01-16T04:43:34.235855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 32\nLEARNING_RATE = 1e-5\nWEIGHT_DECAY=1e-6\nEPS=1e-6\nDROPOUT=0.1\nTMAX=10\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained('../input/roberta-base')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:34.237714Z","iopub.execute_input":"2022-01-16T04:43:34.237952Z","iopub.status.idle":"2022-01-16T04:43:34.470629Z","shell.execute_reply.started":"2022-01-16T04:43:34.237919Z","shell.execute_reply":"2022-01-16T04:43:34.469889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ToxicData(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.text\n        self.targets = self.data.y\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:34.473615Z","iopub.execute_input":"2022-01-16T04:43:34.47382Z","iopub.status.idle":"2022-01-16T04:43:34.481988Z","shell.execute_reply.started":"2022-01-16T04:43:34.473795Z","shell.execute_reply":"2022-01-16T04:43:34.481364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\nsub_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': False,\n                'num_workers': 0\n                }","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:34.483038Z","iopub.execute_input":"2022-01-16T04:43:34.483658Z","iopub.status.idle":"2022-01-16T04:43:34.492285Z","shell.execute_reply.started":"2022-01-16T04:43:34.483613Z","shell.execute_reply":"2022-01-16T04:43:34.491579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(RobertaClass, self).__init__()\n        self.l1 = AutoModel.from_pretrained('../input/roberta-base')\n        self.layer_norm = torch.nn.LayerNorm(768)\n        self.dropout = torch.nn.Dropout(DROPOUT)\n        self.dense = torch.nn.Sequential(\n            torch.nn.Linear(768, 256),\n            torch.nn.LeakyReLU(negative_slope=0.01),\n            torch.nn.Dropout(DROPOUT),\n            torch.nn.Linear(256, 1)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooled_output = self.layer_norm(pooler)\n        pooled_output = self.dropout(pooled_output)\n        preds = self.dense(pooled_output)\n        return preds   ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:34.494651Z","iopub.execute_input":"2022-01-16T04:43:34.495075Z","iopub.status.idle":"2022-01-16T04:43:34.5025Z","shell.execute_reply.started":"2022-01-16T04:43:34.495021Z","shell.execute_reply":"2022-01-16T04:43:34.501768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_function = torch.nn.MSELoss()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:34.503951Z","iopub.execute_input":"2022-01-16T04:43:34.504402Z","iopub.status.idle":"2022-01-16T04:43:34.513008Z","shell.execute_reply.started":"2022-01-16T04:43:34.504365Z","shell.execute_reply":"2022-01-16T04:43:34.512304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model,epoch,training_loader,optimizer):\n    tr_loss = 0\n    nb_tr_steps = 0\n    model.train()\n    for _,data in tqdm(enumerate(training_loader, 0)):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n\n        outputs = model(ids, mask)\n        \n        targets = targets[:,None]\n        loss = loss_function(outputs, targets)\n        tr_loss += loss.item()\n        nb_tr_steps += 1\n        optimizer.zero_grad()\n        loss.backward()\n        # # When using GPU\n        optimizer.step()\n\n    epoch_loss = tr_loss/nb_tr_steps\n    print(f\"Training Loss Epoch {epoch}: {epoch_loss}\")\n\n    return ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:34.514229Z","iopub.execute_input":"2022-01-16T04:43:34.514663Z","iopub.status.idle":"2022-01-16T04:43:34.523107Z","shell.execute_reply.started":"2022-01-16T04:43:34.514619Z","shell.execute_reply":"2022-01-16T04:43:34.522465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid(model, epoch, testing_loader):\n    model.eval()\n    total = 0; tr_loss=0; nb_tr_steps=0\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(testing_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            targets=targets[:,None]\n            outputs = model(ids, mask)\n            loss = loss_function(outputs, targets)\n            tr_loss += loss.item()\n            nb_tr_steps += 1\n\n    epoch_loss = tr_loss/nb_tr_steps\n    print(f\"Validation Loss Epoch{epoch}: {epoch_loss}\")\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:34.525679Z","iopub.execute_input":"2022-01-16T04:43:34.526165Z","iopub.status.idle":"2022-01-16T04:43:34.534001Z","shell.execute_reply.started":"2022-01-16T04:43:34.526129Z","shell.execute_reply":"2022-01-16T04:43:34.533293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, testing_loader):\n    model.eval()\n    preds=None\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(testing_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            \n            outputs = model(ids, mask)\n            if(preds==None):\n                preds=outputs\n            else:\n                preds=torch.cat([preds,outputs],dim=0)\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:34.535107Z","iopub.execute_input":"2022-01-16T04:43:34.536005Z","iopub.status.idle":"2022-01-16T04:43:34.544708Z","shell.execute_reply.started":"2022-01-16T04:43:34.535968Z","shell.execute_reply":"2022-01-16T04:43:34.544029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test=pd.read_csv(os.path.join(base_dir2,\"comments_to_score.csv\"))\ndf_test=df_test.drop(['comment_id'],axis=1)\ndf_test['y']=0\ntest_preds_bert = np.zeros((df_test.shape[0], n_folds))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:34.547606Z","iopub.execute_input":"2022-01-16T04:43:34.547898Z","iopub.status.idle":"2022-01-16T04:43:34.661672Z","shell.execute_reply.started":"2022-01-16T04:43:34.547873Z","shell.execute_reply":"2022-01-16T04:43:34.660877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fld in range(n_folds):\n    df_all = pd.read_csv(f\"df_fld{fld}.csv\")\n    train_size = 0.9\n    train_data=df_all.sample(frac=train_size,random_state=200)\n    test_data=df_all.drop(train_data.index).reset_index(drop=True)\n    train_data = train_data.reset_index(drop=True)\n\n    print(\"FULL Dataset: {}\".format(df_all.shape))\n    print(\"TRAIN Dataset: {}\".format(train_data.shape))\n    print(\"TEST Dataset: {}\".format(test_data.shape))\n    print(\"SUBMISSION dataset: {}\".format(df_test.shape))\n\n    training_set = ToxicData(train_data, tokenizer, MAX_LEN)\n    testing_set = ToxicData(test_data, tokenizer, MAX_LEN)\n    submission_set = ToxicData(df_test,tokenizer,MAX_LEN)\n\n    training_loader = DataLoader(training_set, **train_params)\n    testing_loader = DataLoader(testing_set, **test_params)\n    submission_loader = DataLoader(submission_set,**sub_params)\n    \n    model = RobertaClass()\n    model.to(device)\n    \n    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY,eps=EPS)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TMAX)\n    EPOCHS = 10\n    best_score=12345\n    patience=1\n    cur_p=0\n    for epoch in range(EPOCHS):\n        train(model,epoch,training_loader,optimizer)\n        val_loss=valid(model,epoch,testing_loader)\n        if(val_loss<best_score):\n            best_score=val_loss\n            cur_p=0\n        else:\n            cur_p+=1\n        if(cur_p>patience):\n            break\n        scheduler.step()\n    preds=predict(model,submission_loader)\n    preds=preds.cpu().detach().numpy() \n    test_preds_bert[:,fld] = preds[:,0]\n    del model","metadata":{"execution":{"iopub.status.busy":"2022-01-16T04:43:34.665433Z","iopub.execute_input":"2022-01-16T04:43:34.665693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds_bert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv(os.path.join(base_dir2,\"sample_submission.csv\"))\nsubmission['score']=np.mean(test_preds_bert,axis=1)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}