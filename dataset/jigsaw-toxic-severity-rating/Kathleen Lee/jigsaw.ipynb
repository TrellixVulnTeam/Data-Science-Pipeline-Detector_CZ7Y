{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport re \nimport scipy\nfrom scipy import sparse\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge\nimport zipfile\nimport string\nimport nltk\nimport string\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer() ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-23T05:28:47.515146Z","iopub.execute_input":"2022-01-23T05:28:47.515753Z","iopub.status.idle":"2022-01-23T05:28:49.482703Z","shell.execute_reply.started":"2022-01-23T05:28:47.515627Z","shell.execute_reply":"2022-01-23T05:28:49.482041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"data collecting","metadata":{}},{"cell_type":"code","source":"train_csv_zip_path = '../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip'\nwith zipfile.ZipFile(train_csv_zip_path) as zf:\n    zf.extractall('./')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:28:49.484372Z","iopub.execute_input":"2022-01-23T05:28:49.484797Z","iopub.status.idle":"2022-01-23T05:28:51.304485Z","shell.execute_reply.started":"2022-01-23T05:28:49.484738Z","shell.execute_reply":"2022-01-23T05:28:51.303693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv_path = './train.csv'\nsample_sub_path = '../input/jigsaw-toxic-severity-rating/sample_submission.csv'\ncomments_to_score_path = '../input/jigsaw-toxic-severity-rating/comments_to_score.csv'\nval_path='../input/jigsaw-toxic-severity-rating/validation_data.csv'","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:28:51.306071Z","iopub.execute_input":"2022-01-23T05:28:51.306501Z","iopub.status.idle":"2022-01-23T05:28:51.309998Z","shell.execute_reply.started":"2022-01-23T05:28:51.306462Z","shell.execute_reply":"2022-01-23T05:28:51.309386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"./train.csv\")\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:28:51.311159Z","iopub.execute_input":"2022-01-23T05:28:51.311549Z","iopub.status.idle":"2022-01-23T05:28:52.542125Z","shell.execute_reply.started":"2022-01-23T05:28:51.311517Z","shell.execute_reply":"2022-01-23T05:28:52.541026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:28:52.545272Z","iopub.execute_input":"2022-01-23T05:28:52.5461Z","iopub.status.idle":"2022-01-23T05:28:52.570569Z","shell.execute_reply.started":"2022-01-23T05:28:52.54606Z","shell.execute_reply":"2022-01-23T05:28:52.569638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"data preprocessing\n\nfollow the instructions: https://medium.com/analytics-vidhya/text-cleaning-in-natural-language-processing-nlp-bea2c27035a6\n\nData Preprocessing must include the follows:\n\nRemoving HTML characters,ASCII\n\nConvert Text to Lowercase\n\nRemove Punctuation's\n\nRemove Stop words\n\nTokenization\n\nStemming vs Lemmatization\n","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n#replace the html characters with \" \"\n    text=re.sub('<.*?>', ' ', text)  \n#remove the punctuations\n    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n#consider only alphabets and numerics\n    text = re.sub('[^a-zA-Z]',' ',text)  \n#replace newline with space\n    text = re.sub(\"\\n\",\" \",text)\n#convert to lower case\n    text = text.lower()\n#split and join the words\n    text=' '.join(text.split())\n    return text\n\ndef stopwords(input_text, stop_words):\n    word_tokens = word_tokenize(input_text) \n    output_text = [w for w in word_tokens if not w in stop_words]\n    output = [] \n    for w in word_tokens: \n        if w not in stop_words:\n            output.append(w)\n            \n    text = ' '.join(output)\n    return text\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:28:52.572458Z","iopub.execute_input":"2022-01-23T05:28:52.57308Z","iopub.status.idle":"2022-01-23T05:28:52.583234Z","shell.execute_reply.started":"2022-01-23T05:28:52.573034Z","shell.execute_reply":"2022-01-23T05:28:52.582288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unrelevant_words = ['wiki','wikipedia','page']\n#Clean step 1, 2 and 3\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n#Clean Step 4\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n#Clean Step 5\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:28:52.585153Z","iopub.execute_input":"2022-01-23T05:28:52.58562Z","iopub.status.idle":"2022-01-23T05:31:03.537637Z","shell.execute_reply.started":"2022-01-23T05:28:52.585576Z","shell.execute_reply":"2022-01-23T05:31:03.536609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"reference:\n\nhttps://medium.com/analytics-vidhya/text-cleaning-in-natural-language-processing-nlp-bea2c27035a6","metadata":{}},{"cell_type":"code","source":"# Create a score that messure how much toxic is a comment\nrandom_score = {'obscene': 0.20, 'toxic': 0.40, 'threat': 0.6, \n            'insult': 0.65, 'severe_toxic': 0.9, 'identity_hate': 0.9}\n\nfor category in random_score:\n    df_train[category] = df_train[category] * random_score[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].mean(axis=1)\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_non_tox = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_non_tox])  # make new df\ndf_train_new.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:31:03.53918Z","iopub.execute_input":"2022-01-23T05:31:03.53951Z","iopub.status.idle":"2022-01-23T05:31:03.645127Z","shell.execute_reply.started":"2022-01-23T05:31:03.539466Z","shell.execute_reply":"2022-01-23T05:31:03.643818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_samples_toxic = len(df_train[df_train['score'] != 0])\nn_samples_normal = len(df_train) - n_samples_toxic\n\nidx_to_drop = df_train[df_train['score'] == 0].index[n_samples_toxic//5:]\ndf_train = df_train.drop(idx_to_drop)\n\nprint(f'Reduced number of neutral text samples from {n_samples_normal} to {n_samples_toxic//5}.')\nprint(f'Total number of training samples: {len(df_train)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:31:03.646802Z","iopub.execute_input":"2022-01-23T05:31:03.647128Z","iopub.status.idle":"2022-01-23T05:31:03.73405Z","shell.execute_reply.started":"2022-01-23T05:31:03.647091Z","shell.execute_reply":"2022-01-23T05:31:03.733418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tragets = pd.DataFrame(pd.unique(df_train['score'].values), columns=['target_value']).sort_values(by='target_value', ascending = True).reset_index(drop=True)\nTHRESHOLD = df_tragets['target_value'].quantile(q=0.2)\ndf_train['sentiment'] = df_train['score'].map(lambda x: 1 if x < THRESHOLD else 2 if x < THRESHOLD*2 else 3 if x < THRESHOLD*3 else 4 if x < THRESHOLD*4 else 5)\n\ndf_train = df_train[['comment_text','sentiment']].reset_index(drop=True)\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:31:03.735004Z","iopub.execute_input":"2022-01-23T05:31:03.735772Z","iopub.status.idle":"2022-01-23T05:31:03.786471Z","shell.execute_reply.started":"2022-01-23T05:31:03.735738Z","shell.execute_reply":"2022-01-23T05:31:03.785553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf_vect = TfidfVectorizer(analyzer='word',stop_words= 'english')\nX = tf_idf_vect.fit_transform(df_train['comment_text']).toarray()\nX","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:31:03.788559Z","iopub.execute_input":"2022-01-23T05:31:03.789081Z","iopub.status.idle":"2022-01-23T05:31:06.150338Z","shell.execute_reply.started":"2022-01-23T05:31:03.789043Z","shell.execute_reply":"2022-01-23T05:31:06.149408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(comments_to_score_path)\n\n#Clean step 1, 2 and 3\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n#Clean Step 4\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n#Clean Step 5\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\ndf_test.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:31:06.152021Z","iopub.execute_input":"2022-01-23T05:31:06.152356Z","iopub.status.idle":"2022-01-23T05:31:12.79731Z","shell.execute_reply.started":"2022-01-23T05:31:06.152312Z","shell.execute_reply":"2022-01-23T05:31:12.796354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf_vect = TfidfVectorizer(analyzer='word',stop_words= 'english')\nY = tf_idf_vect.fit_transform(df_test['text']).toarray()\nY","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:31:12.798605Z","iopub.execute_input":"2022-01-23T05:31:12.798871Z","iopub.status.idle":"2022-01-23T05:31:13.948246Z","shell.execute_reply.started":"2022-01-23T05:31:12.798838Z","shell.execute_reply":"2022-01-23T05:31:13.946957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score=[]\nfor i in range(len(df_train['sentiment'])): \n    score.append(df_train['sentiment'][i])","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:31:13.951331Z","iopub.execute_input":"2022-01-23T05:31:13.951616Z","iopub.status.idle":"2022-01-23T05:31:14.107998Z","shell.execute_reply.started":"2022-01-23T05:31:13.951584Z","shell.execute_reply":"2022-01-23T05:31:14.107177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define initial best params and MAE\\\nfrom numpy import arange\n\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    #'objective':'reg:linear',\n}\ngridsearch_params = [\n    (max_depth, min_child_weight, eta)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n    for eta in arange(0.1,1,0.1)\n]\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\ndtrain = xgb.DMatrix(X,score)\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight, eta in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}, eta={}\".format(\n                             max_depth,\n                             min_child_weight,\n                             eta ))    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    params['eta'] = eta\n # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        #num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=5\n    )    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight, eta)\n        print(\"Best params: {}, {}, {}, MAE: {}\".format(best_params[0], best_params[1],best_params[2], min_mae))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:31:14.109088Z","iopub.execute_input":"2022-01-23T05:31:14.110018Z"},"trusted":true},"execution_count":null,"outputs":[]}]}