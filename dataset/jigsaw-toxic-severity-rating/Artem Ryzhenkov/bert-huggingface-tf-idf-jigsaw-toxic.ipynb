{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport re\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gc\nimport scipy\nimport os\nimport nltk\nimport itertools\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom transformers import BertModel, BertTokenizerFast, BertConfig, BertForSequenceClassification, TrainingArguments, Trainer\n\nfrom datasets import Dataset, load_dataset, DatasetDict\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics, model_selection, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T11:14:52.990168Z","iopub.execute_input":"2022-02-06T11:14:52.990677Z","iopub.status.idle":"2022-02-06T11:15:03.024049Z","shell.execute_reply.started":"2022-02-06T11:14:52.990581Z","shell.execute_reply":"2022-02-06T11:15:03.023076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. Introduction.","metadata":{}},{"cell_type":"markdown","source":"In this notebook I'm going to create an ensemble of different models to implement pointwise ranking approach. \nIn other words, I'm going to find a regressor that predict toxicity for each document from the test dataset separately.\n\nHere I'm going to create the next pipeline (though there are no pipelines in this notebook):\n1. Split the test data into two groups (using some binary classifier toxic/nontoxic) and assign to each group some base toxicity score.\n2. Measure the toxicity of all test data (using some regressor) and add it to the base score.\n\n$$ \\textbf{FinalRegressor}(x) = baseScore \\cdot \\textbf{BinClassifier}(x) + \\textbf{SomeRegressor}(x)$$","metadata":{}},{"cell_type":"markdown","source":"## 0.1. Data for models.\n\n1. Jigsaw toxic comment classification challenge competition data.\n2. Ruddit data.","metadata":{}},{"cell_type":"markdown","source":"## 0.2. Steps.\n1. Fine-tune model built on top of the pretrained BERT tokenizer to solve the binary classification task (nontoxic/toxic). For this purpose I will use Hugginfface library (dataset, Trainer, etc.)\n2. Train ensemble of linear regressors (Ridge) on TF-IDF features (using differnet TfidfVectorizer parameters). \n3. Find the best $baseScore$ got at the first step using the competition validation data.\n","metadata":{}},{"cell_type":"markdown","source":"## 0.3. Text preprocessing methods.","metadata":{}},{"cell_type":"markdown","source":"Below I'm going to define all preprocessing methods that I found at some public notebooks and modified a bit (they are quite popular so I don't know their true origin).","metadata":{}},{"cell_type":"code","source":"RE_PATTERNS = {\n    ' fuck':\n        [\n            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k',\n            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*[ck]+[a-z@#\\$%\\^&\\*]*'\n            'f u u c',\n            '(f)(c|[^a-z ])(u|[^a-z ])(k)',\n            'feck ', ' fux ', 'f\\*\\*', \n            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck\\b', '\\bf ck\\b','\\bfuk\\b', 'wtf','fucck','f cking', 'fcking'\n        ],\n\n    ' ass ':\n        [\n            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n        ],\n\n    ' asshole ':\n        [\n            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'ass hole'\n        ],\n\n    ' bitch ':\n        [\n            'b[w]*i[t]*ch', 'b!tch',\n            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h','beetch'\n        ],\n\n    ' bastard ':\n        [\n            'ba[s|z]+t[e|a]+rd'\n        ],\n\n    ' transgender':\n        [\n            'trans gender'\n        ],\n\n    ' cock ':\n        [\n            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n        ],\n\n    ' dick ':\n        [\n            ' dick[^aeiou]', 'deek', 'd i c k','diick ', 'd+\\s?[\\*i1!-]+\\s?[\\*c-]+\\s?[\\*k-]+'\n        ],\n\n    ' suck ':\n        [\n            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n        ],\n\n    ' cunt ':\n        [\n             'c u n t', '\\sc+[ -]?[@u*]+[ -]*[n*-]{1,3}\\s?[t*-]'\n        ],\n\n    ' bullshit ':\n        [\n            'bullsh\\*t', 'bull\\$hit'\n        ],\n\n    ' homosexual':\n        [\n            'homo sexual','homosex'\n        ],\n\n\n    ' idiot ':\n        [\n            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots', 'i d i o t'\n        ],\n\n    ' dumb ':\n        [\n            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n        ],\n\n    ' shit ':\n        [\n            'shitty', 's[ -]?[h*][ -]*[i!*][ -]*t+', 'shite', '\\$hit\\b', 's h i t'\n        ],\n\n    ' shithole ':\n        [\n            'shythole','shit hole'\n        ],\n\n    ' retard ':\n        [\n            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n        ],\n\n    ' dumbass':\n        [\n            'dumb ass', 'dubass'\n        ],\n\n    ' asshead':\n        [\n            'butthead', 'ass head'\n        ],\n\n    ' sex ':\n        [\n            's3x',\n        ],\n\n\n    ' nigger ':\n        [\n            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r', '\\sn+[ -]?[i*]+[ -]*[g*-]{1,3}[ae*-]+\\s?[r*]?'\n        ],\n\n    ' shut the fuck up':\n        [\n            'stfu'\n        ],\n\n    ' pussy ':\n        [\n            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', '\\sp+[ -]?[u*]+[ -]*[$s*-]{1,3}\\s?[yi]'\n        ],\n\n    ' faggot ':\n        [\n            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n        ],\n\n    ' motherfucker':\n        [\n            ' motha ', ' motha f', ' mother f', 'motherucker', 'mother fucker'\n        ],\n\n    ' whore ':\n        [\n            'wh\\*\\*\\*', 'w h o r e', '\\sw+[ -]?[h*]+[ -]*[o*-]{1,3}\\s?[r*]+\\s?[e*]?'\n        ],\n    ' kill ':\n        [\n            '\\sk+[ -]?[!1i*]+[ -]*[1l*-]{1,3}'\n        ],\n    ' cocksucker ':\n        [\n            '\\sc+[ -]?[!o0*]+[ -]?[c*-]{1,3}[ -]?[*k-]+[ -]?[s*]+[u*]+[ -]?[a-z]*'\n        ]\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:15:03.02565Z","iopub.execute_input":"2022-02-06T11:15:03.025871Z","iopub.status.idle":"2022-02-06T11:15:03.044336Z","shell.execute_reply.started":"2022-02-06T11:15:03.025844Z","shell.execute_reply":"2022-02-06T11:15:03.043462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# these methods are from https://www.kaggle.com/andre112/0-826-hate-speech-ridgeregression-ensemble\n\ndef replace_abbrev(text):\n    text = re.sub(r\"what's\", \"what is \",text)    \n    text = re.sub(r\"\\'ve\", \" have \",text)\n    text = re.sub(r\"(\\w+)(n't)\", r\"\\1 not \",text)\n    text = re.sub(r\"i'm\", \"i am \",text)\n    text = re.sub(r\"\\'re\", \" are \",text)\n    text = re.sub(r\"\\'d\", \" would \",text)\n    text = re.sub(r\"\\'ll\", \" will \",text)\n    text = re.sub(r\"\\'scuse\", \" excuse \",text)\n    text = re.sub(r\"\\'s\", \" \",text)\n     # complete -ing\n    text = re.sub(r'(\\w+in)(\\')(\\s)', r'\\1g\\3', text)\n    return text\n\n\n\ndef replace_multi_punc(text):\n    text=re.sub(r'([.])\\1\\1{2,}',r' mpm ',text)\n    text=re.sub(r'([!])\\1\\1{2,}',r' mxm ',text)\n    text=re.sub(r'([?])\\1\\1{2,}',r' mqm ',text)\n    text=re.sub(r'([*])\\1\\1{2,}',r'*',text)\n    return text\n\ndef replace_url(text):\n    \"\"\" Replaces url address with \"url\" \"\"\"\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    return text\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:15:03.046238Z","iopub.execute_input":"2022-02-06T11:15:03.046611Z","iopub.status.idle":"2022-02-06T11:15:03.071886Z","shell.execute_reply.started":"2022-02-06T11:15:03.046573Z","shell.execute_reply":"2022-02-06T11:15:03.070864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n\n\ndef stem(text, stemmer=SnowballStemmer('english')):\n    return ' '.join([stemmer.stem(t) for t in text.split()])    \n\ndef lemm(text, lemmatizer=WordNetLemmatizer()):\n    tokens = nltk.tokenize.word_tokenize(text)                    \n    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n    return ' '.join(tokens)\n\ndef clean(data, stem_on=False, lemm_on=True) -> str:\n    data = data.lower()\n    data = data.strip(\"\\\" \")\n    \n    data = replace_abbrev(data)\n    # remove User:\n    data = re.sub(r\"(u|U)ser:[a-zA-Z\\d]{3,15}\", 'stmsr', data)\n    # remove Date\n    data = re.sub(r\"([\\d]{1,2}\\s([jJ]an|[fF]eb|[mM]ar|[aA]pr|[mM]ay|[jJ]un|[jJ]ul|[aA]ug|[sS]ep|[oO]ct|[nN]ov|[dD]ec),?[a-z]{0,6},?\\s[\\d]{4}(\\s?\\([a-zA-Z]{3}\\))?)\", 'dttm', data)\n    # remove time\n    data = re.sub(r'[\\d]{2}:[\\d]{2}','dttm', data)\n    \n    data = replace_url(data)\n    # Clean some punctutations\n    data = re.sub('\\n', ' ', data)\n    # Remove ip address\n    data = re.sub(r'(([0-9]+\\.){2,}[0-9]+)','stmip', data)\n    \n    # Replace repeating characters more than 3 times to length of 3\n    data = replace_multi_punc(data)\n    # patterns with repeating characters \n    data = re.sub(r'([a-z])\\1{2,}\\b',r'\\1\\1', data)\n    data = re.sub(r'([a-z])\\1\\1{2,}\\B',r'\\1\\1\\1', data)\n            \n    \n    for target, patterns in RE_PATTERNS.items():\n        for pat in patterns:\n            data = re.sub(pat, target, data)\n        \n    data = emoji_pattern.sub(r'', data)\n    # remove all special characters\n    data = re.sub(r\"[^a-z.!?\\']\", \" \", data)\n    \n    # remove extra spaces\n    data = re.sub('\\s+', ' ', data)\n    \n    # stem\n    if stem_on:\n        data = stem(data)\n    \n    if lemm_on:\n        data = lemm(data)\n        \n    return data\n\n\ndef simple_clean(data, lemm_on):\n    data = data.lower()\n    data = data.strip(\"\\\" \")\n    \n    data = replace_abbrev(data)\n    # remove User:\n    data = re.sub(r\"(u|U)ser:[a-zA-Z\\d]{3,15}\", 'stmsr', data)\n    # remove all special characters\n    data = re.sub(r\"[^a-z.!?\\']\", \" \", data)\n    \n    data = emoji_pattern.sub(r'', data)\n    # remove extra spaces\n    data = re.sub('\\s+', ' ', data)\n    if lemm_on:\n        data = lemm(data)\n        \n    return data\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:15:03.074013Z","iopub.execute_input":"2022-02-06T11:15:03.074418Z","iopub.status.idle":"2022-02-06T11:15:03.097859Z","shell.execute_reply.started":"2022-02-06T11:15:03.074372Z","shell.execute_reply":"2022-02-06T11:15:03.097184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. BERT Binary Classifier.","metadata":{}},{"cell_type":"code","source":"BERT_NAME = '../input/d/xhlulu/huggingface-bert/bert-base-uncased'\nMAX_LEN = 400\nBATCH_SIZE = 8\nVAL_SPLIT = 0.2\nN_EPOCH = 14 * 3\nDEVICE = 'cuda'\nLEARNING_RATE = 5e-4","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T16:10:58.48631Z","iopub.execute_input":"2022-02-06T16:10:58.486597Z","iopub.status.idle":"2022-02-06T16:10:58.492312Z","shell.execute_reply.started":"2022-02-06T16:10:58.486565Z","shell.execute_reply":"2022-02-06T16:10:58.491379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1. Data preparations.","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained(BERT_NAME)\n#tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_NAME)\n\ndef get_tokenize_func(column_name):\n    def tokenize_func(examples):\n        return tokenizer(examples[column_name], \n                         padding=\"max_length\", \n                         truncation=True, \n                         add_special_tokens=True,\n                         max_length=MAX_LEN,\n                         return_token_type_ids=False,\n                         return_attention_mask=True)\n    return tokenize_func\n\ndef get_clean_column_func(column_name):\n    def func(ds):\n        ds[column_name] = clean(ds[column_name])\n        return ds\n    return func","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:15:03.111822Z","iopub.execute_input":"2022-02-06T11:15:03.112592Z","iopub.status.idle":"2022-02-06T11:15:03.220715Z","shell.execute_reply.started":"2022-02-06T11:15:03.112548Z","shell.execute_reply":"2022-02-06T11:15:03.219834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.1. Prepare the contest's test data.","metadata":{}},{"cell_type":"code","source":"contest_test_ds = load_dataset('csv', data_files='../input/jigsaw-toxic-severity-rating/comments_to_score.csv') \ncontest_test_ds = contest_test_ds.map(get_clean_column_func('text'))\ncontest_test_ds = contest_test_ds.map(get_tokenize_func(\"text\"), batched=False)\n\ncontest_test_ds['train'].set_format(type='torch', columns=['input_ids', 'attention_mask'])\ncontest_test_dataloader = torch.utils.data.DataLoader(contest_test_ds['train'], batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:15:03.221872Z","iopub.execute_input":"2022-02-06T11:15:03.22214Z","iopub.status.idle":"2022-02-06T11:15:32.402963Z","shell.execute_reply.started":"2022-02-06T11:15:03.222095Z","shell.execute_reply":"2022-02-06T11:15:32.402344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.2. Prepare train data (jigsaw-toxic classification).","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndf = df.rename(columns={\"comment_text\": \"comment\"})\ndf = df.drop(columns=['id'])\ndf.reset_index(drop=True, inplace=True)\n\ndf['comment'] = df['comment'].progress_apply(clean)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:15:32.404376Z","iopub.execute_input":"2022-02-06T11:15:32.40479Z","iopub.status.idle":"2022-02-06T11:22:10.922089Z","shell.execute_reply.started":"2022-02-06T11:15:32.404759Z","shell.execute_reply":"2022-02-06T11:22:10.921203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_all(row):\n    toxicity = row[1:].sum()\n    if toxicity > 0:\n        return 0\n    else:\n        return 1\n\ndf['nontoxic'] = df.progress_apply(add_all, axis='columns')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:22:10.923568Z","iopub.execute_input":"2022-02-06T11:22:10.924428Z","iopub.status.idle":"2022-02-06T11:22:35.325526Z","shell.execute_reply.started":"2022-02-06T11:22:10.924384Z","shell.execute_reply":"2022-02-06T11:22:35.324618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lens = np.array(list(map(lambda x: len(x), df['comment'])))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:22:35.329357Z","iopub.execute_input":"2022-02-06T11:22:35.329677Z","iopub.status.idle":"2022-02-06T11:22:35.43705Z","shell.execute_reply.started":"2022-02-06T11:22:35.329634Z","shell.execute_reply":"2022-02-06T11:22:35.436219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = pd.concat([df[np.logical_and(df['nontoxic'] == 1, lens > 45)].sample((df['nontoxic'] == 0).sum()), df[df['nontoxic'] == 0]])\nnew_df = new_df.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:22:35.438476Z","iopub.execute_input":"2022-02-06T11:22:35.438698Z","iopub.status.idle":"2022-02-06T11:22:35.486551Z","shell.execute_reply.started":"2022-02-06T11:22:35.43867Z","shell.execute_reply":"2022-02-06T11:22:35.485748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gather_labels(row):\n    return 1 - row\n\nnew_df['labels'] = new_df[['nontoxic']].progress_apply(gather_labels, axis=1)\nnew_df = new_df.drop(columns=['identity_hate', 'obscene', 'toxic', 'insult', 'threat', 'severe_toxic', 'nontoxic'])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:22:35.487634Z","iopub.execute_input":"2022-02-06T11:22:35.487836Z","iopub.status.idle":"2022-02-06T11:22:40.976301Z","shell.execute_reply.started":"2022-02-06T11:22:35.48781Z","shell.execute_reply":"2022-02-06T11:22:40.975503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_ds = Dataset.from_pandas(new_df)\nclassification_ds = classification_ds.train_test_split(VAL_SPLIT)\nclassification_ds = classification_ds.map(get_tokenize_func(\"comment\"), batched=True)\nclassification_ds.set_format(type='torch', columns=['attention_mask', 'input_ids', 'labels'])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:22:40.977517Z","iopub.execute_input":"2022-02-06T11:22:40.977736Z","iopub.status.idle":"2022-02-06T11:22:48.622664Z","shell.execute_reply.started":"2022-02-06T11:22:40.977709Z","shell.execute_reply":"2022-02-06T11:22:48.62214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3. Contest validatation data.","metadata":{}},{"cell_type":"code","source":"contest_val_ds = load_dataset('csv', data_files='/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv') \ncontest_val_ds = contest_val_ds.map(get_clean_column_func('less_toxic'))\ncontest_val_ds = contest_val_ds.map(get_clean_column_func('more_toxic'))\n\ncontest_val_ds = contest_val_ds.map(get_tokenize_func(\"less_toxic\"), batched=False)\ncontest_val_ds = contest_val_ds.map(lambda example: {'less_attention_mask': example['attention_mask'], 'less_input_ids': example['input_ids']}, \n                                     remove_columns=['attention_mask', 'input_ids'])\n\ncontest_val_ds = contest_val_ds.map(get_tokenize_func(\"more_toxic\"), batched=False)\ncontest_val_ds = contest_val_ds.map(lambda example: {'more_attention_mask': example['attention_mask'], 'more_input_ids': example['input_ids']}, \n                                     remove_columns=['attention_mask', 'input_ids'])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:22:48.623543Z","iopub.execute_input":"2022-02-06T11:22:48.624272Z","iopub.status.idle":"2022-02-06T11:27:52.303344Z","shell.execute_reply.started":"2022-02-06T11:22:48.624235Z","shell.execute_reply":"2022-02-06T11:27:52.302384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contest_val_ds['train'].set_format(type='torch', columns=['less_input_ids', 'less_attention_mask', 'more_input_ids', 'more_attention_mask'])\ncontest_val_dataloader = torch.utils.data.DataLoader(contest_val_ds['train'], batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:27:52.304795Z","iopub.execute_input":"2022-02-06T11:27:52.30504Z","iopub.status.idle":"2022-02-06T11:27:52.312091Z","shell.execute_reply.started":"2022-02-06T11:27:52.305011Z","shell.execute_reply":"2022-02-06T11:27:52.310415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. BERT binary classifier training.","metadata":{}},{"cell_type":"code","source":"configuration = BertConfig(classifier_dropout=0.003)\n#configuration = BertConfig()\nconfiguration.num_labels = 2\nmodel = BertForSequenceClassification.from_pretrained(BERT_NAME, config=configuration)\n\n# freezing all layers but the classifier\nfor name, param in model.named_parameters():\n    if 'classifier'  not in name:\n        param.requires_grad = False\n        \ntraining_args = TrainingArguments(\"test_trainer\",learning_rate=LEARNING_RATE, num_train_epochs=N_EPOCH, evaluation_strategy='epoch', save_strategy='epoch', load_best_model_at_end=True, per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE, save_total_limit=1)\ntrainer = Trainer(model=model, args=training_args, train_dataset=classification_ds['train'], eval_dataset=classification_ds['test'])\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:27:52.313588Z","iopub.execute_input":"2022-02-06T11:27:52.313808Z","iopub.status.idle":"2022-02-06T11:30:50.107351Z","shell.execute_reply.started":"2022-02-06T11:27:52.313781Z","shell.execute_reply":"2022-02-06T11:30:50.105271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.108427Z","iopub.status.idle":"2022-02-06T11:30:50.108767Z","shell.execute_reply.started":"2022-02-06T11:30:50.108592Z","shell.execute_reply":"2022-02-06T11:30:50.10861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I'm going to get nontoxic/toxic classes for the validation and test data.","metadata":{}},{"cell_type":"code","source":"def validate_model(model, dataloader):\n    loss = 0\n    model.eval()\n    model.to(DEVICE)\n    less_classes = []\n    more_classes = []\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n            less_classes += list(model(data['less_input_ids'].to(DEVICE), data['less_attention_mask'].to(DEVICE)).logits.cpu().detach().numpy())\n            more_classes += list(model(data['more_input_ids'].to(DEVICE), data['more_attention_mask'].to(DEVICE)).logits.cpu().detach().numpy())\n    return np.array([score.argmax() for score in less_classes], dtype=np.float), np.array([score.argmax() for score in more_classes], dtype=np.float)\n\nbert_val_scores = validate_model(model, contest_val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.109867Z","iopub.status.idle":"2022-02-06T11:30:50.110194Z","shell.execute_reply.started":"2022-02-06T11:30:50.110014Z","shell.execute_reply":"2022-02-06T11:30:50.110031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nmodel.to(DEVICE)\nall_scores = []\nwith torch.no_grad():\n    for _, data in tqdm(enumerate(contest_test_dataloader), total=len(contest_test_dataloader)):\n        all_scores += list(model(data['input_ids'].to(DEVICE), data['attention_mask'].to(DEVICE)).logits.cpu().detach().numpy())\nbert_test_scores = np.array([score.argmax() for score in all_scores])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.111777Z","iopub.status.idle":"2022-02-06T11:30:50.11207Z","shell.execute_reply.started":"2022-02-06T11:30:50.111917Z","shell.execute_reply":"2022-02-06T11:30:50.111934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Linear regressors on TF-IDF documents features.","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Data preparations.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndf = df.rename(columns={\"comment_text\": \"comment\"})\ndf = df.drop(columns=['id'])\ndf.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.113076Z","iopub.status.idle":"2022-02-06T11:30:50.113427Z","shell.execute_reply.started":"2022-02-06T11:30:50.113268Z","shell.execute_reply":"2022-02-06T11:30:50.113287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_all(row):\n    toxicity = row[1:].sum()\n    if toxicity > 0:\n        return 0\n    else:\n        return 1\n\ndf['nontoxic'] = df.apply(add_all, axis='columns')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.114378Z","iopub.status.idle":"2022-02-06T11:30:50.114671Z","shell.execute_reply.started":"2022-02-06T11:30:50.114514Z","shell.execute_reply":"2022-02-06T11:30:50.114529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lens = np.array(list(map(lambda x: len(x), df['comment'])))\nnew_clf_df = pd.concat([df[np.logical_and(df['nontoxic'] == 1, lens > 25)].sample((df['nontoxic'] == 0).sum(), random_state=42), df[df['nontoxic'] == 0]])\nnew_clf_df = new_clf_df.sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.115682Z","iopub.status.idle":"2022-02-06T11:30:50.115976Z","shell.execute_reply.started":"2022-02-06T11:30:50.115824Z","shell.execute_reply":"2022-02-06T11:30:50.11584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_clf_df['comment'] = new_clf_df['comment'].apply(clean)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.116864Z","iopub.status.idle":"2022-02-06T11:30:50.117183Z","shell.execute_reply.started":"2022-02-06T11:30:50.116997Z","shell.execute_reply":"2022-02-06T11:30:50.117013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = ['obscene', 'threat', 'insult', 'identity_hate', 'toxic', 'severe_toxic']\nlabel_weights = np.array([0.06, 0.09, 0.13, 0.15, 0.45, 0.25])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.118288Z","iopub.status.idle":"2022-02-06T11:30:50.119614Z","shell.execute_reply.started":"2022-02-06T11:30:50.119439Z","shell.execute_reply":"2022-02-06T11:30:50.119459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gather_labels(row):\n    return np.dot(label_weights, np.array(row.values))\n\nnew_clf_df['score'] = new_clf_df[labels].apply(gather_labels, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.120494Z","iopub.status.idle":"2022-02-06T11:30:50.121005Z","shell.execute_reply.started":"2022-02-06T11:30:50.120823Z","shell.execute_reply":"2022-02-06T11:30:50.120842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_clf_df = new_clf_df.drop(columns=labels)\nnew_clf_df = new_clf_df.drop(columns=['nontoxic'])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.12243Z","iopub.status.idle":"2022-02-06T11:30:50.122929Z","shell.execute_reply.started":"2022-02-06T11:30:50.122745Z","shell.execute_reply":"2022-02-06T11:30:50.122765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\ndf = df[df['txt'] != '[deleted]']\ndf = df.drop(columns=['post_id', 'comment_id', 'url'])\ndf.reset_index(drop=True, inplace=True)\ndf = df.rename(columns={\"txt\": \"comment\", \"offensiveness_score\" : \"score\"})\ndf[\"comment\"] = df[\"comment\"].apply(clean)\n\ndf['score'] = df['score'].apply(lambda x: x if x > 0 else 0 )\nruddit_df = df\nruddit_df","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.123853Z","iopub.status.idle":"2022-02-06T11:30:50.124201Z","shell.execute_reply.started":"2022-02-06T11:30:50.124005Z","shell.execute_reply":"2022-02-06T11:30:50.124028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\n\nval_df['less_toxic_clean'] = val_df['less_toxic'].apply(clean)\nval_df['more_toxic_clean'] = val_df['more_toxic'].apply(clean)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.125037Z","iopub.status.idle":"2022-02-06T11:30:50.125372Z","shell.execute_reply.started":"2022-02-06T11:30:50.125202Z","shell.execute_reply":"2022-02-06T11:30:50.125225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2. Training.","metadata":{}},{"cell_type":"code","source":"def train_model(train_df, tfidf_vec):\n    train_tfidf = tfidf_vec.transform(train_df['comment'].values.tolist())\n    train_y = train_df[\"score\"].values\n    model = linear_model.Ridge(alpha=0.01)\n    model.fit(train_tfidf, train_y)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.127128Z","iopub.status.idle":"2022-02-06T11:30:50.127633Z","shell.execute_reply.started":"2022-02-06T11:30:50.127449Z","shell.execute_reply":"2022-02-06T11:30:50.127471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, tfidf_vec, val_df):\n    comment1 = val_df['less_toxic_clean'].values\n    comment2 = val_df['more_toxic_clean'].values\n\n    comm1 = tfidf_vec.transform(comment1)\n    comm2 = tfidf_vec.transform(comment2)\n\n    pred1 = np.array(model.predict(comm1))\n    pred2 = np.array(model.predict(comm2))\n\n    t = sorted(np.abs(pred1 - pred2))\n    score_diffs = np.array(t[1:]) - np.array(t[:-1])\n    val_df2 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\n\n    updated_pred1 = pred1 \n    updated_pred2 = pred2 \n    \n    return updated_pred1, updated_pred2","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.128844Z","iopub.status.idle":"2022-02-06T11:30:50.129206Z","shell.execute_reply.started":"2022-02-06T11:30:50.129004Z","shell.execute_reply":"2022-02-06T11:30:50.129026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframes = [new_clf_df, ruddit_df]\nparams = [{'analyzer': 'char_wb', 'ngram_range': (3, 4), 'sublinear_tf': True}, {'analyzer': 'word', 'ngram_range': (1, 2), 'sublinear_tf': True}]\nmodels = []\nval_preds = []\nvecs = []","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.130558Z","iopub.status.idle":"2022-02-06T11:30:50.1314Z","shell.execute_reply.started":"2022-02-06T11:30:50.131183Z","shell.execute_reply":"2022-02-06T11:30:50.131211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in dataframes:\n    for param in params:\n        tfidf_vec = TfidfVectorizer(stop_words='english', **param)\n        tfidf_vec.fit_transform(df['comment'].values.tolist())\n        model = train_model(df, tfidf_vec)\n        models.append(model)\n        vecs.append(tfidf_vec)\n        val_preds.append(validate(model, tfidf_vec, val_df))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.13253Z","iopub.status.idle":"2022-02-06T11:30:50.132877Z","shell.execute_reply.started":"2022-02-06T11:30:50.132698Z","shell.execute_reply":"2022-02-06T11:30:50.132722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. BaseScore and regressors' weights tuning.","metadata":{}},{"cell_type":"code","source":"def vote(val_preds, model_weights, toxic_classes, base_score):\n    res = np.stack([t[0]  + base_score * toxic_classes[0] < t[1] + base_score * toxic_classes[1] for t in val_preds], axis=1)\n    threshold = len(val_preds) / 2\n    \n    votes = 0\n    for r in res:\n        if np.dot(r, np.array(model_weights)) >= np.dot(np.ones(len(r)), np.array(model_weights)) / 2:\n            votes += 1\n            \n    return votes / len(res)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.134047Z","iopub.status.idle":"2022-02-06T11:30:50.1344Z","shell.execute_reply.started":"2022-02-06T11:30:50.13423Z","shell.execute_reply":"2022-02-06T11:30:50.134252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_scores = np.linspace(0.15, 1, 3)\nbest_base_score = 0\nmax_val_score = 0\nbest_regr_weights = [0] * len(val_preds)\nfor base_score in base_scores:\n    for model_weights in list(itertools.product(np.linspace(0.1, 1, 3), repeat=len(val_preds))):\n        val_score = vote(val_preds, model_weights, bert_val_scores, base_score)\n        if val_score > max_val_score:\n            max_val_score = val_score\n            best_base_score = base_score\n            best_regr_weights = model_weights","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.135581Z","iopub.status.idle":"2022-02-06T11:30:50.136299Z","shell.execute_reply.started":"2022-02-06T11:30:50.136078Z","shell.execute_reply":"2022-02-06T11:30:50.136101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Validation score: \", max_val_score, \"\\nBase score: \", best_base_score, \"\\nModel weights: \", best_regr_weights)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.137349Z","iopub.status.idle":"2022-02-06T11:30:50.138045Z","shell.execute_reply.started":"2022-02-06T11:30:50.137812Z","shell.execute_reply":"2022-02-06T11:30:50.137844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Final submission.","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n\nsub['text'] = sub['text'].apply(clean)\nsub['score'] = np.zeros(len(sub))\nfor i in range(len(models)):\n    comms = vecs[i].transform(sub['text'].values)\n    sub['score'] += np.array(models[i].predict(comms)) * best_regr_weights[i]\n\nsub['score'] = sub['score'] / np.dot(np.ones(len(models)), np.array(best_regr_weights))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.139433Z","iopub.status.idle":"2022-02-06T11:30:50.139752Z","shell.execute_reply.started":"2022-02-06T11:30:50.139584Z","shell.execute_reply":"2022-02-06T11:30:50.139607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['score'] += bert_test_scores * best_base_score","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.140888Z","iopub.status.idle":"2022-02-06T11:30:50.141237Z","shell.execute_reply.started":"2022-02-06T11:30:50.141035Z","shell.execute_reply":"2022-02-06T11:30:50.141057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['score']  = scipy.stats.rankdata(sub['score'], method='ordinal')\nsub[['comment_id', 'score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:30:50.142606Z","iopub.status.idle":"2022-02-06T11:30:50.142917Z","shell.execute_reply.started":"2022-02-06T11:30:50.142752Z","shell.execute_reply":"2022-02-06T11:30:50.142775Z"},"trusted":true},"execution_count":null,"outputs":[]}]}