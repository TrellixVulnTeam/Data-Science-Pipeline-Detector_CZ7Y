{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### I have first trained DistilBert seperately and then used the trained model to predict the toxicity.\n\n### The notebook for DistilBert is [here](https://www.kaggle.com/devkhant24/distilbert-for-jigsaw-comment) and the trained model can be found [here](https://www.kaggle.com/devkhant24/distilbert-jigsaw-comments).","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:44:01.681974Z","iopub.execute_input":"2021-11-20T11:44:01.682245Z","iopub.status.idle":"2021-11-20T11:44:01.686959Z","shell.execute_reply.started":"2021-11-20T11:44:01.682215Z","shell.execute_reply":"2021-11-20T11:44:01.684796Z"},"editable":false}},{"cell_type":"code","source":"# Importing libraries\n\nimport math\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport re\nimport unidecode\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\nimport tensorflow as tf\nfrom transformers import DistilBertTokenizerFast, TFDistilBertModel\nfrom transformers import Trainer, TrainingArguments\nfrom tokenizers import BertWordPieceTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"execution":{"iopub.status.busy":"2021-12-13T12:07:09.440451Z","iopub.execute_input":"2021-12-13T12:07:09.441145Z","iopub.status.idle":"2021-12-13T12:07:09.446818Z","shell.execute_reply.started":"2021-12-13T12:07:09.441109Z","shell.execute_reply":"2021-12-13T12:07:09.446138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining constants\n\nMax_length = 512\nmodel_name = \"../input/distilbertbaseuncased\"\nBatch_size = 8\nAUTO = tf.data.experimental.AUTOTUNE\n\ntrain_prev_comp = \"../input/toxic-comment/jigsaw-toxic-comment-train.csv\"\ntest_cur_comp = \"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\n\n\ndef seed_everything():\n    np.random.seed(123)\n    random.seed(123)\n    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n    os.environ['PYTHONHASHSEED'] = str(123)\n\nseed_everything()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-12-13T12:07:11.653801Z","iopub.execute_input":"2021-12-13T12:07:11.65432Z","iopub.status.idle":"2021-12-13T12:07:11.661118Z","shell.execute_reply.started":"2021-12-13T12:07:11.654281Z","shell.execute_reply":"2021-12-13T12:07:11.659506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:,1]\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)\n    \n    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n    model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-12-13T12:07:14.866248Z","iopub.execute_input":"2021-12-13T12:07:14.866542Z","iopub.status.idle":"2021-12-13T12:07:14.873937Z","shell.execute_reply.started":"2021-12-13T12:07:14.866509Z","shell.execute_reply":"2021-12-13T12:07:14.873175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(length = maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size]\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for cleaning comments\n\ndef clean_data(sent):\n    sent = sent.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n    soup = BeautifulSoup(sent, \"html.parser\")\n    sent = soup.get_text(separator=\" \")\n    remove_https = re.sub(r'http\\S+', '', sent)\n    sent = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n    sent = unidecode.unidecode(sent)\n    sent = sent.lower()\n    sent = re.sub(r\"[^a-zA-Z0-9:$-,()%.?!]+\", ' ', sent) \n    sent = re.sub(r\"[:$-,()%.?!]+\", ' ',sent)\n    stoplist = stopwords.words(\"english\")\n    sent = [word for word in word_tokenize(sent) if word not in stoplist]\n    sent = \" \".join(sent)\n    \n    return sent","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:17:43.736145Z","iopub.execute_input":"2021-11-20T12:17:43.736461Z","iopub.status.idle":"2021-11-20T12:17:43.747929Z","shell.execute_reply.started":"2021-11-20T12:17:43.73643Z","shell.execute_reply":"2021-11-20T12:17:43.746861Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading train file from previous competition\n\ndf = pd.read_csv(train_prev_comp)\n\n\ndf[\"y\"] = (df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].sum(axis=1) > 0).astype(int)\ndf.drop([\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"], axis=1, inplace = True)\ndf.head()","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seeing that dataset is imbalanced\n\ndf[\"y\"].value_counts()","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Balacing dataset\n\nX = np.array(df[\"comment_text\"].values)\nX = X.reshape(-1,1)\ny = np.array(df[\"y\"].values)\nrus = RandomUnderSampler(random_state=0)\nx, y = rus.fit_resample(X, y)\n\nx = x.flatten()\ndf = pd.DataFrame()\ndf[\"text\"] = x\ndf[\"target\"] = y\n\n\n# Now its balanced\n\ndf[\"target\"].value_counts()","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating column clean_text for cleaned comments\n\ndf[\"text\"] = df[\"text\"].map(clean_data)\n\nx = list(df[\"text\"])\ny = list(df[\"target\"])\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2)","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing Tokenizer\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\ntokenizer.save_pretrained('.')\n\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating encoding for train and validation\ntrain_encodings = fast_encode(xtrain, fast_tokenizer, maxlen = Max_length)\nval_encodings = fast_encode(xtest, fast_tokenizer, maxlen = Max_length)","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_encodings, ytrain))\n    .repeat()\n    .shuffle(1024)\n    .batch(Batch_size)\n    .prefetch(AUTO)\n)\n\nval_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_encodings, ytest))\n    .repeat()\n    .shuffle(1024)\n    .batch(Batch_size)\n    .prefetch(AUTO)\n)","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_layer = TFDistilBertModel.from_pretrained(model_name)\n\nmodel = build_model(transformer_layer, max_len = Max_length)\nmodel.summary()","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_steps = len(xtrain)\n\npredictor = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=val_dataset,\n    epochs=3\n)","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing Bert Tokenizer and Model\ntokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n\nmodel = RobertaForSequenceClassification.from_pretrained(model_name).to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:17:47.036002Z","iopub.execute_input":"2021-11-20T12:17:47.036878Z","iopub.status.idle":"2021-11-20T12:18:01.244979Z","shell.execute_reply.started":"2021-11-20T12:17:47.036837Z","shell.execute_reply":"2021-11-20T12:18:01.243871Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get predicitions\ndef get_prediction(text):\n    \n    text = clean_data(text)\n    inputs = tokenizer(text, truncation = True, padding = True, max_length = Max_length, return_tensors = \"pt\").to(\"cuda\")\n    output = model(**inputs)\n    probs = output[0].softmax(1)\n    return probs[:,1].item()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:18:01.247204Z","iopub.execute_input":"2021-11-20T12:18:01.247535Z","iopub.status.idle":"2021-11-20T12:18:01.254705Z","shell.execute_reply.started":"2021-11-20T12:18:01.247501Z","shell.execute_reply":"2021-11-20T12:18:01.253477Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading given test dataset \n# Storing predicted values in score column\n\ntest = pd.read_csv(file_path)\n\ntest[\"score\"] = test[\"text\"].map(get_prediction)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:18:01.256343Z","iopub.execute_input":"2021-11-20T12:18:01.256905Z","iopub.status.idle":"2021-11-20T12:20:26.286515Z","shell.execute_reply.started":"2021-11-20T12:18:01.256853Z","shell.execute_reply":"2021-11-20T12:20:26.285564Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making submission file\n\nfinal = pd.DataFrame()\nfinal[\"comment_id\"] = test[\"comment_id\"]\nfinal[\"score\"] = test[\"score\"]\nfinal.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:20:26.288872Z","iopub.execute_input":"2021-11-20T12:20:26.289103Z","iopub.status.idle":"2021-11-20T12:20:26.331196Z","shell.execute_reply.started":"2021-11-20T12:20:26.289067Z","shell.execute_reply":"2021-11-20T12:20:26.33024Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:20:26.332568Z","iopub.execute_input":"2021-11-20T12:20:26.333001Z","iopub.status.idle":"2021-11-20T12:20:26.352984Z","shell.execute_reply.started":"2021-11-20T12:20:26.332953Z","shell.execute_reply":"2021-11-20T12:20:26.35182Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}