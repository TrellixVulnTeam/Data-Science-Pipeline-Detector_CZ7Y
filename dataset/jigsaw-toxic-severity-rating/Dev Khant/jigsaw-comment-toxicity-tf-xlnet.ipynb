{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing libraries\n\nimport math\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport re\nimport unidecode\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-09T11:42:36.789005Z","iopub.execute_input":"2022-01-09T11:42:36.789359Z","iopub.status.idle":"2022-01-09T11:42:45.782155Z","shell.execute_reply.started":"2022-01-09T11:42:36.789274Z","shell.execute_reply":"2022-01-09T11:42:45.781204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try:\n#     # TPU detection. No parameters necessary if TPU_NAME environment variable is set (always set in Kaggle)\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n#     strategy = tf.distribute.get_strategy()\n\n# print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T11:42:45.783985Z","iopub.execute_input":"2022-01-09T11:42:45.78422Z","iopub.status.idle":"2022-01-09T11:42:51.924075Z","shell.execute_reply.started":"2022-01-09T11:42:45.784193Z","shell.execute_reply":"2022-01-09T11:42:51.92336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining constants\n\nAUTO = tf.data.experimental.AUTOTUNE\n\nmodel_name = \"../input/roberta-base\"\nMax_len = 25\nBatch_size = 8\n\n\ndata_prev_comp = \"../input/toxic-comment/jigsaw-toxic-comment-train.csv\"\ndata_cur_comp = \"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\n\n\ndef seed_everything():\n    np.random.seed(123)\n    random.seed(123)\n    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n    os.environ['PYTHONHASHSEED'] = str(123)\n\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T12:00:50.409953Z","iopub.execute_input":"2022-01-09T12:00:50.410635Z","iopub.status.idle":"2022-01-09T12:00:50.420166Z","shell.execute_reply.started":"2022-01-09T12:00:50.410586Z","shell.execute_reply":"2022-01-09T12:00:50.419085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for cleaning comments\n\ndef clean_data(sent):\n    sent = sent.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n    soup = BeautifulSoup(sent, \"html.parser\")\n    sent = soup.get_text(separator=\" \")\n    remove_https = re.sub(r'http\\S+', '', sent)\n    sent = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n    sent = unidecode.unidecode(sent)\n    sent = sent.lower()\n    sent = re.sub(r\"[^a-zA-Z0-9:$-,()%.?!]+\", ' ', sent) \n    sent = re.sub(r\"[:$-,()%.?!]+\", ' ',sent)\n    stoplist = stopwords.words(\"english\")\n    sent = [word for word in word_tokenize(sent) if word not in stoplist]\n    sent = \" \".join(sent)\n    \n    return sent","metadata":{"execution":{"iopub.status.busy":"2022-01-09T11:42:51.933938Z","iopub.execute_input":"2022-01-09T11:42:51.934154Z","iopub.status.idle":"2022-01-09T11:42:51.944077Z","shell.execute_reply.started":"2022-01-09T11:42:51.934129Z","shell.execute_reply":"2022-01-09T11:42:51.943362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading train file from previous competition\n\ndf = pd.read_csv(data_prev_comp)\ny_features = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\ndf[\"severe_toxic\"] = df[\"severe_toxic\"] * 2\ndf[\"y\"] = (df[y_features].sum(axis=1)).astype(int)\ndf[\"y\"] = df[\"y\"] / df[\"y\"].max()\ndf.drop(y_features, axis=1, inplace = True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T11:42:51.94525Z","iopub.execute_input":"2022-01-09T11:42:51.945708Z","iopub.status.idle":"2022-01-09T11:42:55.025179Z","shell.execute_reply.started":"2022-01-09T11:42:51.945673Z","shell.execute_reply":"2022-01-09T11:42:55.024218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seeing that dataset is imbalanced\n\ndf[\"y\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T11:42:55.026599Z","iopub.execute_input":"2022-01-09T11:42:55.026894Z","iopub.status.idle":"2022-01-09T11:42:55.039518Z","shell.execute_reply.started":"2022-01-09T11:42:55.026856Z","shell.execute_reply":"2022-01-09T11:42:55.03863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating column clean_text for cleaned comments\n\ndf[\"comment_text\"] = df[\"comment_text\"].map(clean_data)\n\nxtrain, xtest, ytrain, ytest = train_test_split(df[\"comment_text\"], df[\"y\"], test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T11:42:55.040614Z","iopub.execute_input":"2022-01-09T11:42:55.04083Z","iopub.status.idle":"2022-01-09T11:47:26.365107Z","shell.execute_reply.started":"2022-01-09T11:42:55.040804Z","shell.execute_reply":"2022-01-09T11:47:26.364193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for creating word encodings\n\ndef roberta_encode(texts, tokenizer):\n    ct = len(texts)\n    input_ids = np.ones((ct, Max_len), dtype='int32')\n    attention_mask = np.zeros((ct, Max_len), dtype='int32')\n\n    for k, text in enumerate(texts):\n        encoded_text = tokenizer.tokenize(text)\n        \n        # Truncate and convert tokens to numerical IDs\n        enc_text = tokenizer.convert_tokens_to_ids(encoded_text[:(Max_len-2)])\n        \n        input_length = len(enc_text) + 2\n        input_length = input_length if input_length < Max_len else Max_len\n        \n        # Add tokens [CLS] and [SEP] at the beginning and the end\n        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n        \n        # Set to 1s in the attention input\n        attention_mask[k,:input_length] = 1\n        \n    return{\n        'input_words_ids': input_ids,\n        'input_mask': attention_mask,\n    }","metadata":{"execution":{"iopub.status.busy":"2022-01-09T11:47:26.366453Z","iopub.execute_input":"2022-01-09T11:47:26.366967Z","iopub.status.idle":"2022-01-09T11:47:26.375242Z","shell.execute_reply.started":"2022-01-09T11:47:26.366935Z","shell.execute_reply":"2022-01-09T11:47:26.374328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing Tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\n# Creating encodings for train and validation data\nX_train = roberta_encode(xtrain, tokenizer)\nX_validation = roberta_encode(xtest, tokenizer)\n\nY_train = np.asarray(ytrain, dtype=\"int32\")\nY_validation = np.asarray(ytest, dtype=\"int32\")","metadata":{"execution":{"iopub.status.busy":"2022-01-09T11:47:26.376852Z","iopub.execute_input":"2022-01-09T11:47:26.377111Z","iopub.status.idle":"2022-01-09T11:48:33.789741Z","shell.execute_reply.started":"2022-01-09T11:47:26.377085Z","shell.execute_reply":"2022-01-09T11:48:33.788982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for building the Roberta Model\n\ndef build_model():\n    input_words_ids = tf.keras.Input(shape=(Max_len, ), dtype = tf.int32, name=\"input_words_ids\")\n    input_mask = tf.keras.Input(shape=(Max_len, ), dtype = tf.int32, name=\"input_mask\")\n\n    roberta_model = TFAutoModel.from_pretrained(model_name)\n    x = roberta_model(input_words_ids, attention_mask = input_mask)\n#         x = tf.keras.layers.Dropout(0.2)(x[0])\n#         x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(768, activation='relu')(x[1])\n    x = tf.keras.layers.Dense(1)(x)\n\n    model = tf.keras.Model(inputs = [input_words_ids, input_mask], outputs = x)\n\n    model.compile(\n        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),\n        loss = \"BinaryCrossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-09T12:00:54.828881Z","iopub.execute_input":"2022-01-09T12:00:54.829314Z","iopub.status.idle":"2022-01-09T12:00:54.838052Z","shell.execute_reply.started":"2022-01-09T12:00:54.829269Z","shell.execute_reply":"2022-01-09T12:00:54.837396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\n\nmodel = build_model()\n\nhistory = model.fit(\n            X_train,\n            Y_train,\n            epochs = 1,\n            batch_size = Batch_size,\n            validation_data = (X_validation, Y_validation)\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T12:00:57.388973Z","iopub.execute_input":"2022-01-09T12:00:57.389272Z","iopub.status.idle":"2022-01-09T12:03:09.713052Z","shell.execute_reply.started":"2022-01-09T12:00:57.389237Z","shell.execute_reply":"2022-01-09T12:03:09.711611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the test data\n\ntest = pd.read_csv(data_cur_comp)\n\ntest[\"text\"] = test[\"text\"].map(clean_data)\nX_test = roberta_encode(test[\"text\"], tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T12:03:26.501795Z","iopub.execute_input":"2022-01-09T12:03:26.50216Z","iopub.status.idle":"2022-01-09T12:03:28.982057Z","shell.execute_reply.started":"2022-01-09T12:03:26.502125Z","shell.execute_reply":"2022-01-09T12:03:28.981182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making prediction and creating submission file\n\npred = model.predict(X_test)\n\nfinal = pd.DataFrame()\nfinal[\"comment_id\"] = test[\"comment_id\"]\nfinal[\"score\"] = pred\nfinal.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T12:03:13.584972Z","iopub.execute_input":"2022-01-09T12:03:13.585277Z","iopub.status.idle":"2022-01-09T12:03:26.500313Z","shell.execute_reply.started":"2022-01-09T12:03:13.585243Z","shell.execute_reply":"2022-01-09T12:03:26.499638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T12:03:31.229881Z","iopub.execute_input":"2022-01-09T12:03:31.23037Z","iopub.status.idle":"2022-01-09T12:03:31.239244Z","shell.execute_reply.started":"2022-01-09T12:03:31.230319Z","shell.execute_reply":"2022-01-09T12:03:31.238604Z"},"trusted":true},"execution_count":null,"outputs":[]}]}