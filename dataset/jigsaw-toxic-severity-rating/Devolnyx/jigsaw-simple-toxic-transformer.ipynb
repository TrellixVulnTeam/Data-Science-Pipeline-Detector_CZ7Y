{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Multi-label classification with simple keras transformer\nTraining set is based on the data from previous competitions that are available in this dataset: https://www.kaggle.com/julian3833/jigsaw-toxic-comment-classification-challenge\n<br>The target is a sum of initial toxic labels from the dataset and ranges from 0 to 6. Final score calculated as dot product of labels and their probabilities for each comment.\n<br>The model obtained from the 'Keras Code examples' section.\n\nCredits:\n* https://www.kaggle.com/steubk/jrsotc-ridgeregression \n* https://www.kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768\n* https://www.kaggle.com/devkhant24/jigsaw-comment-toxicity-gru/\n* https://keras.io/examples/nlp/text_classification_with_transformer/","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nimport re\nimport unidecode\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom scipy.stats import rankdata\nfrom sklearn.model_selection import train_test_split\n\nseed = 42\ntrain_set = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ntrain_set.head(3)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-20T15:36:11.213397Z","iopub.execute_input":"2021-11-20T15:36:11.213604Z","iopub.status.idle":"2021-11-20T15:36:19.303429Z","shell.execute_reply.started":"2021-11-20T15:36:11.213578Z","shell.execute_reply":"2021-11-20T15:36:19.3027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for cleaning comments\ndef clean_data(data):\n    final = []\n    for sent in data:\n        sent = sent.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n        soup = BeautifulSoup(sent, \"html.parser\")\n        sent = soup.get_text(separator=\" \")\n        remove_https = re.sub(r'http\\S+', '', sent)\n        sent = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n        sent = unidecode.unidecode(sent)\n        sent = sent.lower()\n        sent = re.sub(r\"[^a-zA-Z0-9:$-,()%.?!]+\", ' ', sent) \n        sent = re.sub(r\"[:$-,()%.?!]+\", ' ',sent)\n        stoplist = stopwords.words(\"english\")\n        sent = [word for word in word_tokenize(sent) if word not in stoplist]\n        sent = \" \".join(sent)\n        final.append(sent)\n    \n    return final","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:36:19.305209Z","iopub.execute_input":"2021-11-20T15:36:19.305461Z","iopub.status.idle":"2021-11-20T15:36:19.313447Z","shell.execute_reply.started":"2021-11-20T15:36:19.305427Z","shell.execute_reply":"2021-11-20T15:36:19.312787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The most of the comments in loaded dataset are non-toxic. Only 'n' of them are used for training.","metadata":{}},{"cell_type":"code","source":"train_set['toxicity'] = train_set.drop(['id', 'comment_text'], axis=1).sum(axis=1)\ntrain_set.toxicity.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:36:19.314983Z","iopub.execute_input":"2021-11-20T15:36:19.315605Z","iopub.status.idle":"2021-11-20T15:36:19.336391Z","shell.execute_reply.started":"2021-11-20T15:36:19.315569Z","shell.execute_reply":"2021-11-20T15:36:19.335688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use only 'n' non-toxic comments\nn = 15000\n\nnontoxic_sample = train_set[train_set.toxicity==0].sample(n, random_state = seed)\ntrain = pd.concat([train_set[train_set.toxicity!=0], nontoxic_sample]).sort_index()\ntrain = train[['comment_text', 'toxicity']]\ntrain.toxicity.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:36:19.338293Z","iopub.execute_input":"2021-11-20T15:36:19.338539Z","iopub.status.idle":"2021-11-20T15:36:19.386985Z","shell.execute_reply.started":"2021-11-20T15:36:19.338507Z","shell.execute_reply":"2021-11-20T15:36:19.386154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['comment_text'] = clean_data(train.comment_text)\ntest['text'] = clean_data(test.text)\n\nmax_sequence_len = 250\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train.comment_text)\ntotal_words = len(tokenizer.word_index)+1\n\nX = tokenizer.texts_to_sequences(train.comment_text)\nX = pad_sequences(X, maxlen = max_sequence_len, padding='pre')\n\nx_test = tokenizer.texts_to_sequences(test.text)\nx_test = pad_sequences(x_test, maxlen = max_sequence_len)\n\ny = train.toxicity.astype(np.int8)#.clip(0,4)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:36:19.388441Z","iopub.execute_input":"2021-11-20T15:36:19.388707Z","iopub.status.idle":"2021-11-20T15:37:05.666622Z","shell.execute_reply.started":"2021-11-20T15:36:19.388672Z","shell.execute_reply":"2021-11-20T15:37:05.6659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_toxic_tokenizer = Tokenizer()\nnon_toxic_tokenizer.fit_on_texts(train[train.toxicity==0].comment_text)\nnon_toxic_count = non_toxic_tokenizer.word_counts\nnon_toxic_count = sorted(dict(non_toxic_count).items(), key=lambda tup: tup[1], reverse=True)\n\ntoxic_tokenizer = Tokenizer()\ntoxic_tokenizer.fit_on_texts(train[train.toxicity>0].comment_text)\ntoxic_count = toxic_tokenizer.word_counts\ntoxic_count = sorted(dict(toxic_count).items(), key=lambda tup: tup[1], reverse=True)[:200]\n\nall_words = pd.DataFrame(toxic_count).merge(pd.DataFrame(non_toxic_count), how='left', on=0)\nall_words = all_words.dropna().reset_index()\n\nall_words['ratio'] = np.log(all_words['1_x'] / all_words['1_y'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-20T15:21:44.36376Z","iopub.execute_input":"2021-11-20T15:21:44.364115Z","iopub.status.idle":"2021-11-20T15:21:45.817647Z","shell.execute_reply.started":"2021-11-20T15:21:44.364058Z","shell.execute_reply":"2021-11-20T15:21:45.816595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.scatter(all_words.iloc[:100], x=\"1_y\", y=\"1_x\", text=0, log_x=True, log_y=True, color=\"ratio\", color_continuous_scale='Portland',\n                labels={\n                     \"1_x\": \"Number of word's appearance in toxic comments\",\n                     \"1_y\": \"Number of word's appearance in non-toxic comments\"})\n\nfig.update_traces(textposition='top center')\n\nfig.update_layout(\n    height=800,\n    title_text='100 most frequent words in toxic comments',\ncoloraxis_showscale=False)\n\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-20T15:21:45.819063Z","iopub.execute_input":"2021-11-20T15:21:45.819851Z","iopub.status.idle":"2021-11-20T15:21:49.241602Z","shell.execute_reply.started":"2021-11-20T15:21:45.819807Z","shell.execute_reply":"2021-11-20T15:21:49.240408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 256  # Embedding size for each token.\nnum_heads = 4  # Number of attention heads\nff_dim = 384 #  Hidden layer size in feedforward network.\nbatch_size = 128  # Batch size.\nclasses = len(y.unique())\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embedding_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n    \nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embedding_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:41:01.360299Z","iopub.execute_input":"2021-11-20T15:41:01.361104Z","iopub.status.idle":"2021-11-20T15:41:01.376105Z","shell.execute_reply.started":"2021-11-20T15:41:01.361063Z","shell.execute_reply":"2021-11-20T15:41:01.37532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = layers.Input(shape=(max_sequence_len,))\nembedding_layer = TokenAndPositionEmbedding( max_sequence_len, total_words, embedding_dim)\nx = embedding_layer(inputs)\nx = TransformerBlock(embedding_dim, num_heads, ff_dim)(x)\nx = TransformerBlock(embedding_dim, num_heads, ff_dim)(x)\nx = layers.Flatten()(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(256, activation=\"relu\")(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(classes, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate = 7e-5), metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:41:26.540535Z","iopub.execute_input":"2021-11-20T15:41:26.54081Z","iopub.status.idle":"2021-11-20T15:41:26.796235Z","shell.execute_reply.started":"2021-11-20T15:41:26.540777Z","shell.execute_reply":"2021-11-20T15:41:26.795502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:41:28.302799Z","iopub.execute_input":"2021-11-20T15:41:28.303493Z","iopub.status.idle":"2021-11-20T15:41:28.319441Z","shell.execute_reply.started":"2021-11-20T15:41:28.303458Z","shell.execute_reply":"2021-11-20T15:41:28.318526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:41:30.479283Z","iopub.execute_input":"2021-11-20T15:41:30.479538Z","iopub.status.idle":"2021-11-20T15:41:30.675461Z","shell.execute_reply.started":"2021-11-20T15:41:30.47951Z","shell.execute_reply":"2021-11-20T15:41:30.67457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(seed)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=seed)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience = 7,restore_best_weights = True)\nmodel.fit(X_train, y_train, validation_data = (X_val, y_val),\n    epochs = 40, \n    batch_size = batch_size, \n    shuffle = True,\n    callbacks = [early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:41:39.714599Z","iopub.execute_input":"2021-11-20T15:41:39.715013Z","iopub.status.idle":"2021-11-20T15:55:10.034031Z","shell.execute_reply.started":"2021-11-20T15:41:39.714976Z","shell.execute_reply":"2021-11-20T15:55:10.033344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### validate\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n\ndf_val['less_toxic'] = clean_data(df_val['less_toxic'])\ndf_val['more_toxic'] = clean_data(df_val['more_toxic'])\n\nX_less_toxic = tokenizer.texts_to_sequences(df_val['less_toxic'])\nX_more_toxic = tokenizer.texts_to_sequences(df_val['more_toxic'])\n\nX_less_toxic = pad_sequences(X_less_toxic, maxlen = max_sequence_len)\nX_more_toxic = pad_sequences(X_more_toxic, maxlen = max_sequence_len)\n\np1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)\n\np1 = (np.linspace(0,classes-1,classes) * p1).sum(axis=1)\np2 = (np.linspace(0,classes-1,classes) * p2).sum(axis=1)\n\n# Validation Accuracy\n(p1< p2).mean()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:55:10.03562Z","iopub.execute_input":"2021-11-20T15:55:10.035898Z","iopub.status.idle":"2021-11-20T15:57:55.707359Z","shell.execute_reply.started":"2021-11-20T15:55:10.03585Z","shell.execute_reply":"2021-11-20T15:57:55.706652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = tokenizer.texts_to_sequences(test.text)\nx_test = pad_sequences(x_test, maxlen = max_sequence_len)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:33:14.707017Z","iopub.execute_input":"2021-11-20T15:33:14.708856Z","iopub.status.idle":"2021-11-20T15:33:15.116872Z","shell.execute_reply.started":"2021-11-20T15:33:14.708808Z","shell.execute_reply":"2021-11-20T15:33:15.115924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(x_test)\npreds = (np.linspace(0,classes-1,classes) * preds).sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:33:15.118191Z","iopub.execute_input":"2021-11-20T15:33:15.118565Z","iopub.status.idle":"2021-11-20T15:33:18.829357Z","shell.execute_reply.started":"2021-11-20T15:33:15.118524Z","shell.execute_reply":"2021-11-20T15:33:18.828354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making submission file\n\nfinal = pd.DataFrame()\nfinal[\"comment_id\"] = test[\"comment_id\"]\nfinal[\"score\"] = rankdata(preds)\nfinal.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:33:18.830971Z","iopub.execute_input":"2021-11-20T15:33:18.831296Z","iopub.status.idle":"2021-11-20T15:33:18.878131Z","shell.execute_reply.started":"2021-11-20T15:33:18.83125Z","shell.execute_reply":"2021-11-20T15:33:18.877068Z"},"trusted":true},"execution_count":null,"outputs":[]}]}