{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sb","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:40:20.533008Z","iopub.execute_input":"2022-02-08T21:40:20.533599Z","iopub.status.idle":"2022-02-08T21:40:21.756386Z","shell.execute_reply.started":"2022-02-08T21:40:20.533479Z","shell.execute_reply":"2022-02-08T21:40:21.755324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Try to overfit CV pls !**    Upvote if you fork/liked it, Thanks !","metadata":{}},{"cell_type":"markdown","source":"Credit : Tokenizer training + TFIDF + RIDGE [LB 0.860] by Adriano Passos\nhttps://www.kaggle.com/coldfir3/tokenizer-training-tfidf-ridge-lb-0-860","metadata":{}},{"cell_type":"code","source":"import optuna","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:40:21.758717Z","iopub.execute_input":"2022-02-08T21:40:21.759427Z","iopub.status.idle":"2022-02-08T21:40:22.767927Z","shell.execute_reply.started":"2022-02-08T21:40:21.759375Z","shell.execute_reply":"2022-02-08T21:40:22.766966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\n\nTRAIN_DATA_PATH = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\nVALID_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\"\nTEST_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\n\ndf_train2 = pd.read_csv(TRAIN_DATA_PATH)\ndf_valid2 = pd.read_csv(VALID_DATA_PATH)\ndf_test2 = pd.read_csv(TEST_DATA_PATH)\n\n\n#optuna Version\nfrom optuna.integration import LightGBMPruningCallback\nimport optuna\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgbm\nfrom sklearn.metrics import mean_squared_error\ndef objective(trial):\n    # Parameters Network\n    cat_mtpl = {\n\n        \"obscene\": trial.suggest_float(\"obscene\", 0.01, 2),\n        \"toxic\": trial.suggest_float(\"toxic\", 0.01, 2),\n        \"threat\": trial.suggest_float(\"threat\", 0.01, 2),\n        \"insult\": trial.suggest_float(\"insult\", 0.01, 2),\n        \"severe_toxic\": trial.suggest_float(\"severe_toxic\", 0.01, 2),\n        'identity_hate': trial.suggest_float(\"identity_hate\", 0.01, 2),\n    }\n    \n    for category in cat_mtpl:\n        df_train2[category] = df_train2[category] * cat_mtpl[category]\n\n    df_train2['score'] = df_train2.loc[:, 'toxic':'identity_hate'].mean(axis=1)\n\n    df_train2['y'] = df_train2['score']\n\n    min_len = (df_train2['y'] > 0).sum()  # len of toxic comments\n    df_y0_undersample = df_train2[df_train2['y'] == 0].sample(n=min_len, random_state=41)  # take non toxic comments\n    df_train_new = pd.concat([df_train2[df_train2['y'] > 0], df_y0_undersample])  # make new df\n    from tokenizers import (\n        decoders,\n        models,\n        normalizers,\n        pre_tokenizers,\n        processors,\n        trainers,\n        Tokenizer,\n    )\n    raw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n    raw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n    raw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n    special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n    trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n    from datasets import Dataset\n\n    dataset = Dataset.from_pandas(df_train_new[['comment_text']])\n\n    def get_training_corpus():\n        for i in range(0, len(dataset), 1000):\n            yield dataset[i : i + 1000][\"comment_text\"]\n\n    raw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n\n    from transformers import PreTrainedTokenizerFast\n\n    tokenizer = PreTrainedTokenizerFast(\n        tokenizer_object=raw_tokenizer,\n        unk_token=\"[UNK]\",\n        pad_token=\"[PAD]\",\n        cls_token=\"[CLS]\",\n        sep_token=\"[SEP]\",\n        mask_token=\"[MASK]\",\n    )\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.linear_model import Ridge\n\n    def dummy_fun(doc):\n        return doc\n\n    labels = df_train_new['y']\n    comments = df_train_new['comment_text']\n    tokenized_comments = tokenizer(comments.to_list())['input_ids']\n\n    vectorizer = TfidfVectorizer(\n        analyzer = 'word',\n        tokenizer = dummy_fun,\n        preprocessor = dummy_fun,\n        token_pattern = None)\n\n    comments_tr = vectorizer.fit_transform(tokenized_comments)\n\n    regressor = Ridge(random_state=42, alpha=0.8)\n    regressor.fit(comments_tr, labels)\n\n    less_toxic_comments = df_valid2['less_toxic']\n    more_toxic_comments = df_valid2['more_toxic']\n\n    less_toxic_comments = tokenizer(less_toxic_comments.to_list())['input_ids']\n    more_toxic_comments = tokenizer(more_toxic_comments.to_list())['input_ids']\n\n    less_toxic = vectorizer.transform(less_toxic_comments)\n    more_toxic = vectorizer.transform(more_toxic_comments)\n\n    # make predictions\n    y_pred_less = regressor.predict(less_toxic)\n    y_pred_more = regressor.predict(more_toxic)\n\n    print(f'val : {(y_pred_less < y_pred_more).mean()}')\n\n    return (y_pred_less < y_pred_more).mean()\n\nstudy = optuna.create_study(direction=\"maximize\", study_name=\"Best weight Finder\")\nfunc = lambda trial: objective(trial)\nstudy.optimize(func, n_trials=50)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:40:22.769618Z","iopub.execute_input":"2022-02-08T21:40:22.769927Z","iopub.status.idle":"2022-02-08T21:45:23.003706Z","shell.execute_reply.started":"2022-02-08T21:40:22.769883Z","shell.execute_reply":"2022-02-08T21:45:22.999217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finally, after 500 trial research, you will get best weight!\nBest trial: {'obscene': 1.2537639596777044, 'toxic': 1.284033992994945, 'threat': 0.3220098913216658, 'insult': 1.4594296267520046, 'severe_toxic': 0.873410475008243, 'identity_hate': 0.7399081551266039}","metadata":{}},{"cell_type":"code","source":"#Result\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:45:23.004923Z","iopub.status.idle":"2022-02-08T21:45:23.005366Z","shell.execute_reply.started":"2022-02-08T21:45:23.005113Z","shell.execute_reply":"2022-02-08T21:45:23.005142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# If you want to get visualization result, maybe you should use colab","metadata":{}},{"cell_type":"code","source":"#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\n# optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:45:23.113543Z","iopub.status.idle":"2022-02-08T21:45:23.1139Z","shell.execute_reply.started":"2022-02-08T21:45:23.113712Z","shell.execute_reply":"2022-02-08T21:45:23.11373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_parallel_coordinate: interactively visualizes the hyperparameters and scores\n# optuna.visualization.plot_parallel_coordinate(study)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:45:23.114827Z","iopub.status.idle":"2022-02-08T21:45:23.115229Z","shell.execute_reply.started":"2022-02-08T21:45:23.115019Z","shell.execute_reply":"2022-02-08T21:45:23.115041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Don'use Scaling trick, since that it will overfit LB score(5% data)","metadata":{}},{"cell_type":"code","source":"# df_test = data\n# for i in range(0, 500):\n#     df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.25\n# for i in range(801, 1200):\n#     df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.35\n# for i in range(1701, 2300):\n#     df_test['score'].iloc[i] = df_test['score'].iloc[i] * 0.9\n# for i in range(2501, 2980):\n#     df_test['score'].iloc[i] = df_test['score'].iloc[i] * 0.90    \n# for i in range(3001, 4000):\n#     df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.35    \n# for i in range(4001, 4500):\n#     df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.4   \n# for i in range(4501, 4940):\n#     df_test['score'].iloc[i] = df_test['score'].iloc[i] * 0.95\n# for i in range(5501, 5980):\n#     df_test['score'].iloc[i] = df_test['score'].iloc[i] * 0.9\n# for i in range(6001, 6500):\n#     df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.4\n# for i in range(7001, 7536):\n#     df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.38  ","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:45:23.117613Z","iopub.status.idle":"2022-02-08T21:45:23.117985Z","shell.execute_reply.started":"2022-02-08T21:45:23.117791Z","shell.execute_reply":"2022-02-08T21:45:23.117817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test[\"score\"] = rankdata( df_test[\"score\"], method='ordinal')\n# df_test[\"score\"].to_csv('./submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:45:23.119455Z","iopub.status.idle":"2022-02-08T21:45:23.119956Z","shell.execute_reply.started":"2022-02-08T21:45:23.119719Z","shell.execute_reply":"2022-02-08T21:45:23.119739Z"},"trusted":true},"execution_count":null,"outputs":[]}]}