{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## uHack Sentiments 2.0: Decode Code Words","metadata":{}},{"cell_type":"markdown","source":"The challenge here is to analyze and deep dive into the natural language text (reviews) and bucket them based on their topics of discussion. Furthermore, analyzing the overall sentiment will also help the business to make tangible decisions.\n\nThe data set provided to you has a mix of customer reviews for products across categories and retailers. We would like you to model on the data to bucket the future reviews in their respective topics (Note: A review can talk about multiple topics) and Overall polarity (positive/negative sentiment)","metadata":{}},{"cell_type":"markdown","source":"Data:\n\nTopics (Components, Delivery and Customer Support, Design and Aesthetics, Dimensions, Features, Functionality, Installation, Material, Price, Quality and Usability)\n\nPolarity (Positive/Negative)\n\nNote: The target variables are all encoded in the train dataset for convenience. Please submit the test results in the similar encoded fashion for us to evaluate your results.","metadata":{}},{"cell_type":"markdown","source":"### import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os,gc,re\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### data","metadata":{}},{"cell_type":"code","source":"train  = pd.read_csv(\"../input/ugam-hack-data/train.csv\",index_col='Id')\npd.set_option('display.max_colwidth', None)\ntrain.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test  = pd.read_csv(\"../input/ugam-hack-data/test.csv\",index_col='Id')\ntest.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.iloc[:,1:].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories = list(train.columns[1:].values)\nsns.set(font_scale = 1.2)\nplt.figure(figsize=(25,8))\nax= sns.barplot(x=categories, y=train.iloc[:,1:].sum().values)\nplt.title(\"Reviews in each category\", fontsize=24)\nplt.ylabel('Number of Reviews', fontsize=18)\nplt.xlabel('Comment Type ', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = train.iloc[:,1:].sum().values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 10, label, ha='center', va='bottom', fontsize=11)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Counting the number of comments having multiple labels","metadata":{}},{"cell_type":"code","source":"rowSums =train.iloc[:,1:].sum(axis=1)\nmultiLabel_counts = rowSums.value_counts()\nmultiLabel_counts = multiLabel_counts.iloc[1:]\nsns.set(font_scale = 1.2)\nplt.figure(figsize=(15,4))\nax = sns.barplot(x=multiLabel_counts.index, y=multiLabel_counts.values)\nplt.title(\"Comments having multiple labels \")\nplt.ylabel('Number of comments', fontsize=14)\nplt.xlabel('Number of labels', fontsize=15)\n#adding the text labels\nrects = ax.patches\nlabels = multiLabel_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del ax,rects, labels\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data processing","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\nimport sys\nimport warnings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n    \ndef cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(sentence))\n    return cleantext\n\ndef cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\n\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Review'] = train['Review'].str.lower()\ntrain['Review'] = train['Review'].apply(cleanHtml)\ntrain['Review'] = train['Review'].apply(cleanPunc)\ntrain['Review'] = train['Review'].apply(keepAlpha)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Review'] = test['Review'].str.lower()\ntest['Review'] = test['Review'].apply(cleanHtml)\ntest['Review'] = test['Review'].apply(cleanPunc)\ntest['Review'] = test['Review'].apply(keepAlpha)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_whitespace(text):\n    text = text.strip()\n    return text\n\ntrain['Review'] = train['Review'].apply(clean_whitespace)\ntest['Review'] = test['Review'].apply(clean_whitespace)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Ratings']= train[train.columns[1:]].values.tolist()\ntrain.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train[['Review','Ratings']].copy()\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"!pip install -q transformers","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertModel, BertConfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sections of config\n\n# Defining some key variables that will be used later on in the training\nMAX_LEN = 96\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 9\nLEARNING_RATE = 1e-05\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.Review = dataframe.Review\n        self.targets = self.data.Ratings\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.Review)\n\n    def __getitem__(self, index):\n        Review = str(self.Review[index])\n        Review = \" \".join(Review.split())\n\n        inputs = self.tokenizer.encode_plus(\n            Review,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the dataset and dataloader for the neural network\n\ntrain_size = 0.8\ntrain_dataset= data.sample(frac=train_size,random_state=14)\ntest_dataset=  data.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\n\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n\nclass BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n        self.l2 = torch.nn.Dropout(0.3)\n        self.l3 = torch.nn.Linear(768, 12)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids,return_dict=False)\n        output_2 = self.l2(output_1)\n        output = self.l3(output_2)\n        return output\n\nmodel = BERTClass()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Setting up the device for GPU usage\n\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss function\ndef loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer\noptimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# fine tuning","metadata":{}},{"cell_type":"code","source":"def train(epoch):\n    model.train()\n    for _,training_data in enumerate(training_loader, 0):\n        ids = training_data['ids'].to(device, dtype = torch.long)\n        mask = training_data['mask'].to(device, dtype = torch.long)\n        token_type_ids = training_data['token_type_ids'].to(device, dtype = torch.long)\n        targets = training_data['targets'].to(device, dtype = torch.float)\n        \n        outputs = model(ids, mask, token_type_ids)\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        if _%5000==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    train(epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(),'../output')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SubmissionDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.Review = dataframe.Review\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.Review)\n\n    def __getitem__(self, index):\n        Review = str(self.Review[index])\n        Review = \" \".join(Review.split())\n\n        inputs = self.tokenizer.encode_plus(\n            Review,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_data = SubmissionDataset(test,tokenizer, MAX_LEN)\nsub_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_params = {'batch_size': 8,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\nsub_loader = DataLoader(sub_data, **sub_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _,data in enumerate(sub_loader,0):\n    ids = data['ids'].to(device, dtype = torch.long)\n    mask = data['mask'].to(device, dtype = torch.long)\n    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\noutputs = model(ids, mask, token_type_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}