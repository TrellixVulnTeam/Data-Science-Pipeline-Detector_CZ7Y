{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport pickle\nimport joblib\nimport string\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Utils\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom scipy import sparse\n\n# ML Imports\nfrom xgboost import XGBRegressor\n\n# NLP Imports\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nnltk.download('wordnet')\nimport itertools\nfrom string import ascii_lowercase\nimport fasttext as ft\nfrom bs4 import BeautifulSoup\nfrom gensim.models import KeyedVectors, FastText\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AdamW, AutoConfig\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T21:46:39.390014Z","iopub.execute_input":"2022-02-07T21:46:39.390647Z","iopub.status.idle":"2022-02-07T21:47:02.53404Z","shell.execute_reply.started":"2022-02-07T21:46:39.39055Z","shell.execute_reply":"2022-02-07T21:47:02.533278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PRIVATE = False\nVAL = False\nn_folds =7","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.535288Z","iopub.execute_input":"2022-02-07T21:47:02.53553Z","iopub.status.idle":"2022-02-07T21:47:02.539451Z","shell.execute_reply.started":"2022-02-07T21:47:02.535495Z","shell.execute_reply":"2022-02-07T21:47:02.53871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    def create_folds(data, num_splits):\n        num_bins = int(np.floor(1 + np.log2(len(data))))\n        data.loc[:, \"bins\"] = pd.cut(\n            data[\"y\"], bins=num_bins, labels=False\n        )\n\n        data[\"fold\"] = -1\n        kf = StratifiedKFold(n_splits=num_splits)\n        for f, (t_, v_) in enumerate(kf.split(X=data, y=data['bins'].values)):\n            data.loc[v_, 'fold'] = f\n        data = data.drop(\"bins\", axis=1)\n        return data","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.541059Z","iopub.execute_input":"2022-02-07T21:47:02.54158Z","iopub.status.idle":"2022-02-07T21:47:02.553038Z","shell.execute_reply.started":"2022-02-07T21:47:02.541523Z","shell.execute_reply":"2022-02-07T21:47:02.552359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = pd.read_csv(\"../input/js-cleaned-validation-data/Validation_data_clean_no_duplicates.csv\")\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.556919Z","iopub.execute_input":"2022-02-07T21:47:02.557394Z","iopub.status.idle":"2022-02-07T21:47:02.564373Z","shell.execute_reply.started":"2022-02-07T21:47:02.557358Z","shell.execute_reply":"2022-02-07T21:47:02.563653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if df_sub.shape[0] == 7537:\n    PRIVATE = False","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.565893Z","iopub.execute_input":"2022-02-07T21:47:02.566464Z","iopub.status.idle":"2022-02-07T21:47:02.572724Z","shell.execute_reply.started":"2022-02-07T21:47:02.566425Z","shell.execute_reply":"2022-02-07T21:47:02.571746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RE_PATTERNS = {\n    ' american ':\n        [\n            'amerikan'\n        ],\n\n    ' adolf ':\n        [\n            'adolf'\n        ],\n\n\n    ' hitler ':\n        [\n            'hitler'\n        ],\n\n    ' fuck':\n        [\n            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n            'feck ', ' fux ', 'f\\*\\*', \n            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck','fuk', 'wtf','fucck','f cking'\n        ],\n\n    ' ass ':\n        [\n            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n        ],\n\n    ' asshole ':\n        [\n            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'ass hole'\n        ],\n\n    ' bitch ':\n        [\n            'b[w]*i[t]*ch', 'b!tch',\n            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h','beetch'\n        ],\n\n    ' bastard ':\n        [\n            'ba[s|z]+t[e|a]+rd'\n        ],\n\n    ' transgender':\n        [\n            'transgender','trans gender'\n        ],\n\n    ' gay ':\n        [\n            'gay'\n        ],\n\n    ' cock ':\n        [\n            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n        ],\n\n    ' dick ':\n        [\n            ' dick[^aeiou]', 'deek', 'd i c k','diick '\n        ],\n\n    ' suck ':\n        [\n            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n        ],\n\n    ' cunt ':\n        [\n            'cunt', 'c u n t'\n        ],\n\n    ' bullshit ':\n        [\n            'bullsh\\*t', 'bull\\$hit','bs'\n        ],\n\n    ' homosexual':\n        [\n            'homo sexual','homosex'\n        ],\n\n    ' jerk ':\n        [\n            'jerk'\n        ],\n\n    ' idiot ':\n        [\n            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots', 'i d i o t'\n        ],\n\n    ' dumb ':\n        [\n            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n        ],\n\n    ' shit ':\n        [\n            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n        ],\n\n    ' shithole ':\n        [\n            'shythole','shit hole'\n        ],\n\n    ' retard ':\n        [\n            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n        ],\n\n    ' rape ':\n        [\n            ' raped'\n        ],\n\n    ' dumbass':\n        [\n            'dumb ass', 'dubass'\n        ],\n\n    ' asshead':\n        [\n            'butthead', 'ass head'\n        ],\n\n    ' sex ':\n        [\n            's3x', 'sexuality',\n        ],\n\n\n    ' nigger ':\n        [\n            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n        ],\n\n    ' shut the fuck up':\n        [\n            'stfu'\n        ],\n\n    ' pussy ':\n        [\n            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n        ],\n\n    ' faggot ':\n        [\n            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n        ],\n\n    ' motherfucker':\n        [\n            ' motha ', ' motha f', ' mother f', 'motherucker', 'mother fucker'\n        ],\n\n    ' whore ':\n        [\n            'wh\\*\\*\\*', 'w h o r e'\n        ],\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.574671Z","iopub.execute_input":"2022-02-07T21:47:02.575095Z","iopub.status.idle":"2022-02-07T21:47:02.584867Z","shell.execute_reply.started":"2022-02-07T21:47:02.575057Z","shell.execute_reply":"2022-02-07T21:47:02.584045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Text Normalization\n\ndef clean_text(text, remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n    \n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip()\n    if is_lower:\n        text=text.lower()\n\n    if remove_patterns_text:\n        for target, patterns in RE_PATTERNS.items():\n          for pat in patterns:\n            text=str(text).replace(pat, target)\n\n    if remove_repeat_text:\n        text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n    \n    text = str(text).replace(\"\\n\", \" \")\n    text = re.sub(r'[^\\w\\s]',' ',text)\n    text = re.sub('[0-9]',\"\",text)\n    text = re.sub(\" +\", \" \", text)\n    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n    return text \n\ndef clean(text):\n        text = text.replace(r\"what's\", \"what is \")\n        text = text.replace(r\"\\'ve\", \" have \")\n        text = text.replace(r\"can't\", \"cannot \")\n        text = text.replace(r\"n't\", \" not \")\n        text = text.replace(r\"i'm\", \"i am \")\n        text = text.replace(r\"\\'re\", \" are \")\n        text = text.replace(r\"\\'d\", \" would \")\n        text = text.replace(r\"\\'ll\", \" will \")\n        text = text.replace(r\"\\'scuse\", \" excuse \")\n        text = text.replace(r\"\\'s\", \" \")\n        text = text.replace('\\n', ' \\n ')\n        text = text.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)', r'\\1 \\2 \\3')\n        text = text.replace(r'([*!?\\'])\\1\\1{2,}', r'\\1\\1\\1')\n        text = text.replace(r'([*!?\\']+)', r' \\1 ')\n        text = text.replace(r'([a-zA-Z])\\1{2,}\\b', r'\\1\\1')\n        text = text.replace(r'([a-zA-Z])\\1\\1{2,}\\B', r'\\1\\1\\1')\n        text = text.replace(r'[ ]{2,}', ' ').strip()\n        text = text.replace(r'[ ]{2,}', ' ').strip()\n        return text","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.58636Z","iopub.execute_input":"2022-02-07T21:47:02.587Z","iopub.status.idle":"2022-02-07T21:47:02.597267Z","shell.execute_reply.started":"2022-02-07T21:47:02.586938Z","shell.execute_reply":"2022-02-07T21:47:02.596456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer= WordNetLemmatizer()\ndef lemma(text, lemmatization=True):\n  output=''\n  if lemmatization:\n    text=text.split(' ')\n    for word in text:\n      word1 = lemmatizer.lemmatize(word, pos = \"n\") #noun \n      word2 = lemmatizer.lemmatize(word1, pos = \"v\") #verb\n      word3 = lemmatizer.lemmatize(word2, pos = \"a\") #adjective\n      word4 = lemmatizer.lemmatize(word3, pos = \"r\") #adverb\n      output=output + \" \" + word4\n  else:\n    output=text\n  \n  return str(output.strip())","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.598816Z","iopub.execute_input":"2022-02-07T21:47:02.599398Z","iopub.status.idle":"2022-02-07T21:47:02.609061Z","shell.execute_reply.started":"2022-02-07T21:47:02.599359Z","shell.execute_reply":"2022-02-07T21:47:02.608276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Stopwords Removal\n\nstopword_list=[]\ndef iter_all_strings():\n    for size in itertools.count(1):\n        for s in itertools.product(ascii_lowercase, repeat=size):\n            yield \"\".join(s)\n\ndual_alpha_list=[]\nfor s in iter_all_strings():\n    dual_alpha_list.append(s)\n    if s == 'zz':\n        break\n\ndual_alpha_list.remove('i')\ndual_alpha_list.remove('a')\ndual_alpha_list.remove('am')\ndual_alpha_list.remove('an')\ndual_alpha_list.remove('as')\ndual_alpha_list.remove('at')\ndual_alpha_list.remove('be')\ndual_alpha_list.remove('by')\ndual_alpha_list.remove('do')\ndual_alpha_list.remove('go')\ndual_alpha_list.remove('he')\ndual_alpha_list.remove('hi')\ndual_alpha_list.remove('if')\ndual_alpha_list.remove('is')\ndual_alpha_list.remove('in')\ndual_alpha_list.remove('me')\ndual_alpha_list.remove('my')\ndual_alpha_list.remove('no')\ndual_alpha_list.remove('of')\ndual_alpha_list.remove('on')\ndual_alpha_list.remove('or')\ndual_alpha_list.remove('ok')\ndual_alpha_list.remove('so')\ndual_alpha_list.remove('to')\ndual_alpha_list.remove('up')\ndual_alpha_list.remove('us')\ndual_alpha_list.remove('we')\n\n\nfor letter in dual_alpha_list:\n    stopword_list.append(letter)\n    \npotential_stopwords=['editor', 'reference', 'thank', 'work','find', 'good', 'know', 'like', 'look', 'thing', 'want', 'time', 'list', 'section','wikipedia', 'doe', 'add','new', 'try', 'think', 'write','use', 'user', 'way', 'page']\n\n#Adding above retrived words into the stopwords list.\nfor word in tqdm(potential_stopwords):\n    stopword_list.append(word)\n    \ndef remove_stopwords(text, remove_stop=True):\n  output = \"\"\n  if remove_stop:\n    text=text.split(\" \")\n    for word in text:\n      if word not in stopword_list:\n        output=output + \" \" + word\n  else :\n    output=text\n\n  return str(output.strip())","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.610475Z","iopub.execute_input":"2022-02-07T21:47:02.610959Z","iopub.status.idle":"2022-02-07T21:47:02.618228Z","shell.execute_reply.started":"2022-02-07T21:47:02.610921Z","shell.execute_reply":"2022-02-07T21:47:02.617396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    df_jigsaw = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n\n    df_jigsaw['severe_toxic'] = df_jigsaw.severe_toxic * 2\n    df_jigsaw['y'] = (df_jigsaw[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis = 1)).astype(int)\n    df_jigsaw['y'] = df_jigsaw['y'] / df_jigsaw['y'].max()\n    df_jigsaw = df_jigsaw[['comment_text', 'y']].rename(columns = {'comment_text': 'text'})\n    df_jigsaw[\"text\"] = df_jigsaw[\"text\"].progress_apply(clean_text)\n    df_jigsaw['text'] = df_jigsaw[\"text\"].progress_apply(clean)\n    df_jigsaw[\"text\"] = df_jigsaw[\"text\"].progress_apply(lemma)\n    df_jigsaw[\"text\"] = df_jigsaw[\"text\"].progress_apply(remove_stopwords)\n    df_jigsaw.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.620021Z","iopub.execute_input":"2022-02-07T21:47:02.620512Z","iopub.status.idle":"2022-02-07T21:47:02.628522Z","shell.execute_reply.started":"2022-02-07T21:47:02.620474Z","shell.execute_reply":"2022-02-07T21:47:02.627828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    if VAL:\n        df_val[\"less_toxic\"] = df_val[\"less_toxic\"].progress_apply(clean_text)\n        df_val['less_toxic'] = df_val[\"less_toxic\"].progress_apply(clean)\n        df_val[\"less_toxic\"] = df_val[\"less_toxic\"].progress_apply(lemma)\n        df_val[\"less_toxic\"] = df_val[\"less_toxic\"].progress_apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.630928Z","iopub.execute_input":"2022-02-07T21:47:02.631847Z","iopub.status.idle":"2022-02-07T21:47:02.638127Z","shell.execute_reply.started":"2022-02-07T21:47:02.631811Z","shell.execute_reply":"2022-02-07T21:47:02.636898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    if VAL:\n        df_val[\"more_toxic\"] = df_val[\"more_toxic\"].progress_apply(clean_text)\n        df_val['more_toxic'] = df_val[\"more_toxic\"].progress_apply(clean)\n        df_val[\"more_toxic\"] = df_val[\"more_toxic\"].progress_apply(lemma)\n        df_val[\"more_toxic\"] = df_val[\"more_toxic\"].progress_apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.639806Z","iopub.execute_input":"2022-02-07T21:47:02.640069Z","iopub.status.idle":"2022-02-07T21:47:02.645003Z","shell.execute_reply.started":"2022-02-07T21:47:02.640034Z","shell.execute_reply":"2022-02-07T21:47:02.644036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    df_sub[\"text\"] = df_sub[\"text\"].progress_apply(clean_text)\n    df_sub['text'] = df_sub[\"text\"].progress_apply(clean)\n    df_sub[\"text\"] = df_sub[\"text\"].progress_apply(lemma)\n    df_sub[\"text\"] = df_sub[\"text\"].progress_apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.650426Z","iopub.execute_input":"2022-02-07T21:47:02.650681Z","iopub.status.idle":"2022-02-07T21:47:02.655364Z","shell.execute_reply.started":"2022-02-07T21:47:02.650647Z","shell.execute_reply":"2022-02-07T21:47:02.65454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    df_jigsaw = create_folds(df_jigsaw, n_folds)\n    df_jigsaw.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.656979Z","iopub.execute_input":"2022-02-07T21:47:02.657969Z","iopub.status.idle":"2022-02-07T21:47:02.662336Z","shell.execute_reply.started":"2022-02-07T21:47:02.657929Z","shell.execute_reply":"2022-02-07T21:47:02.661434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    df_ruddit = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\n    df_ruddit = df_ruddit[['txt', 'offensiveness_score']].rename(columns = {'txt': 'text', 'offensiveness_score': 'y'})\n    df_ruddit['y'] = (df_ruddit['y'] - df_ruddit.y.min()) / (df_ruddit.y.max() - df_ruddit.y.min())\n    df_ruddit = create_folds(df_ruddit, n_folds)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.664097Z","iopub.execute_input":"2022-02-07T21:47:02.664642Z","iopub.status.idle":"2022-02-07T21:47:02.668396Z","shell.execute_reply.started":"2022-02-07T21:47:02.664606Z","shell.execute_reply":"2022-02-07T21:47:02.667652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    val_preds_jigsaw1 = np.zeros((df_val.shape[0], n_folds))\n    val_preds_jigsaw2 = np.zeros((df_val.shape[0], n_folds))\n    test_preds_jigsaw = np.zeros((df_sub.shape[0], n_folds))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.669659Z","iopub.execute_input":"2022-02-07T21:47:02.670535Z","iopub.status.idle":"2022-02-07T21:47:02.677405Z","shell.execute_reply.started":"2022-02-07T21:47:02.670487Z","shell.execute_reply":"2022-02-07T21:47:02.67645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    for fld in tqdm(range(n_folds)):\n        df = df_jigsaw.loc[df_jigsaw.fold == fld]\n        features = FeatureUnion([(\"vect3\", TfidfVectorizer(min_df = 3, max_df = 0.5, analyzer = 'char_wb', ngram_range = (3, 5)))])\n        pipeline = Pipeline([(\"features\", features), (\"clf\", Ridge())])\n        pipeline.fit(df['text'], df['y'])\n        val_preds_jigsaw1[:, fld] = pipeline.predict(df_val['less_toxic'])\n        val_preds_jigsaw2[:, fld] = pipeline.predict(df_val['more_toxic'])\n        test_preds_jigsaw[:, fld] = pipeline.predict(df_sub['text'])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.67881Z","iopub.execute_input":"2022-02-07T21:47:02.679807Z","iopub.status.idle":"2022-02-07T21:47:02.684188Z","shell.execute_reply.started":"2022-02-07T21:47:02.679748Z","shell.execute_reply":"2022-02-07T21:47:02.683371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    del df, pipeline\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.685584Z","iopub.execute_input":"2022-02-07T21:47:02.686567Z","iopub.status.idle":"2022-02-07T21:47:02.690847Z","shell.execute_reply.started":"2022-02-07T21:47:02.686344Z","shell.execute_reply":"2022-02-07T21:47:02.689925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    val_preds_ruddit1 = np.zeros((df_val.shape[0], n_folds))\n    val_preds_ruddit2 = np.zeros((df_val.shape[0], n_folds))\n    test_preds_ruddit = np.zeros((df_sub.shape[0], n_folds))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.692271Z","iopub.execute_input":"2022-02-07T21:47:02.692732Z","iopub.status.idle":"2022-02-07T21:47:02.697346Z","shell.execute_reply.started":"2022-02-07T21:47:02.692657Z","shell.execute_reply":"2022-02-07T21:47:02.696623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    for fld in tqdm(range(n_folds)):\n        df = df_ruddit.loc[df_ruddit.fold == fld]\n        features = FeatureUnion([(\"vect3\", TfidfVectorizer(min_df = 3, max_df = 0.5, analyzer = 'char_wb', ngram_range = (3, 5)))])\n        pipeline = Pipeline([(\"features\", features), (\"clf\", Ridge())])\n        pipeline.fit(df['text'], df['y'])\n        val_preds_ruddit1[:, fld] = pipeline.predict(df_val['less_toxic'])\n        val_preds_ruddit2[:, fld] = pipeline.predict(df_val['more_toxic'])\n        test_preds_ruddit[:, fld] = pipeline.predict(df_sub['text'])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.698796Z","iopub.execute_input":"2022-02-07T21:47:02.699334Z","iopub.status.idle":"2022-02-07T21:47:02.705076Z","shell.execute_reply.started":"2022-02-07T21:47:02.699237Z","shell.execute_reply":"2022-02-07T21:47:02.704158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    del df, pipeline\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.706623Z","iopub.execute_input":"2022-02-07T21:47:02.707513Z","iopub.status.idle":"2022-02-07T21:47:02.712223Z","shell.execute_reply.started":"2022-02-07T21:47:02.707472Z","shell.execute_reply":"2022-02-07T21:47:02.711368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    p1 = val_preds_jigsaw1.mean(axis = 1)\n    p2 = val_preds_jigsaw2.mean(axis = 1)\n    p3 = val_preds_ruddit1.mean(axis = 1)\n    p4 = val_preds_ruddit2.mean(axis = 1)\n    # p5 = val_preds_arr1c.mean(axis = 1)\n    # p6 = val_preds_arr2c.mean(axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.713761Z","iopub.execute_input":"2022-02-07T21:47:02.714258Z","iopub.status.idle":"2022-02-07T21:47:02.721565Z","shell.execute_reply.started":"2022-02-07T21:47:02.714221Z","shell.execute_reply":"2022-02-07T21:47:02.720564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    wts_acc = []\n    for i in range(1, 100, 1):\n        w1 = i / 100\n        w2 = (100 - i) / 100\n        p1_wt = w1 * p1 + w2 * p3\n        p2_wt = w1 * p2 + w2 * p4\n        wts_acc.append((w1, w2, np.round((p1_wt < p2_wt).mean() * 100, 2)))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.723202Z","iopub.execute_input":"2022-02-07T21:47:02.723837Z","iopub.status.idle":"2022-02-07T21:47:02.728538Z","shell.execute_reply.started":"2022-02-07T21:47:02.7238Z","shell.execute_reply":"2022-02-07T21:47:02.727346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    w1, w2, score = sorted(wts_acc, key = lambda x: x[2], reverse = True)[0]\n    print(score)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.730217Z","iopub.execute_input":"2022-02-07T21:47:02.730825Z","iopub.status.idle":"2022-02-07T21:47:02.73567Z","shell.execute_reply.started":"2022-02-07T21:47:02.730761Z","shell.execute_reply":"2022-02-07T21:47:02.734794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    print(w1)\n    print(w2)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.737455Z","iopub.execute_input":"2022-02-07T21:47:02.738081Z","iopub.status.idle":"2022-02-07T21:47:02.743199Z","shell.execute_reply.started":"2022-02-07T21:47:02.738039Z","shell.execute_reply":"2022-02-07T21:47:02.741969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    p1_wt = w1 * p1 + w2 * p3\n    p2_wt = w1 * p2 + w2 * p4\n    df_val['p1'] = p1_wt\n    df_val['p2'] = p2_wt\n    df_val['diff'] = np.abs(p2_wt - p1_wt)\n    df_val['correct'] = (p1_wt < p2_wt).astype('int')\n    df_sub['score'] = w1 * test_preds_jigsaw.mean(axis = 1) + w2 * test_preds_ruddit.mean(axis = 1)\n    df_sub.to_csv(\"submission_tfidf.csv\", index = None)\n    # df_sub['score'].count() - df_sub['score'].nunique()\n\n    # same_score = df_sub['score'].value_counts().reset_index()[:10]\n    # df_sub[df_sub['score'].isin(same_score['index'].tolist())]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.745247Z","iopub.execute_input":"2022-02-07T21:47:02.745534Z","iopub.status.idle":"2022-02-07T21:47:02.752476Z","shell.execute_reply.started":"2022-02-07T21:47:02.7455Z","shell.execute_reply":"2022-02-07T21:47:02.75144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *DEEP LEARNING* - Ranking Loss","metadata":{}},{"cell_type":"code","source":"CONFIG = dict(\n    seed = 42,\n    config_name = '../input/js-mpnet/mpnet-base/mpnet-base/config.json',\n    model_name = '../input/js-mpnet/mpnet-base/mpnet-base',\n    test_batch_size = 32,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    debug= False,\n    size = 768\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.754351Z","iopub.execute_input":"2022-02-07T21:47:02.754909Z","iopub.status.idle":"2022-02-07T21:47:02.866299Z","shell.execute_reply.started":"2022-02-07T21:47:02.75487Z","shell.execute_reply":"2022-02-07T21:47:02.865463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df = pd.read_csv(\"../input/js-cleaned-validation-data/Validation_data_clean_no_duplicates.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:02.867635Z","iopub.execute_input":"2022-02-07T21:47:02.867931Z","iopub.status.idle":"2022-02-07T21:47:03.063041Z","shell.execute_reply.started":"2022-02-07T21:47:02.867893Z","shell.execute_reply":"2022-02-07T21:47:03.062114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df2 = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.064795Z","iopub.execute_input":"2022-02-07T21:47:03.065171Z","iopub.status.idle":"2022-02-07T21:47:03.119325Z","shell.execute_reply.started":"2022-02-07T21:47:03.065126Z","shell.execute_reply":"2022-02-07T21:47:03.118551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.120569Z","iopub.execute_input":"2022-02-07T21:47:03.120952Z","iopub.status.idle":"2022-02-07T21:47:03.129287Z","shell.execute_reply.started":"2022-02-07T21:47:03.12091Z","shell.execute_reply":"2022-02-07T21:47:03.128425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dict = {\n    \"roberta-base\":{\n        \"model_name\": \"../input/js-roberta/roberta-base/roberta-base\",\n        \"config_name\": \"../input/js-roberta/roberta-base/roberta-base/config.json\",\n        \"model_paths\": [f\"../input/js-roberta/JS_roberta-base_RL/models/roberta-base_{fold}_model.bin\" for fold in range(5)],\n        \"version\": 0,\n        \"size\": 768\n    },\n    \"roberta-base2\":{\n        \"model_name\": \"../input/js-roberta/roberta-base/roberta-base\",\n        \"config_name\": \"../input/js-roberta/roberta-base/roberta-base/config.json\",\n        \"model_paths\": [f\"../input/js-roberta/JS_roBERTa-base-cat-multi/JS_roBERTa-base-cat-multi/roberta-base_toxic_{fold}_model.bin\" for fold in range(5)],\n        \"version\": 0,\n        \"size\": 768    \n    },\n    \"deberta-base\":{\n        \"model_name\": \"../input/js-deberta/deberta-base/deberta-base\",\n        \"config_name\": \"../input/js-deberta/deberta-base/deberta-base/config.json\",\n        \"model_paths\": [f\"../input/js-deberta/JS_deberta-base_RL/models/deberta-base_{fold}_model.bin\" for fold in range(5)],\n        \"version\": 1,\n        \"size\": 768\n    },\n    \"deberta-base2\":{\n        \"model_name\": \"../input/js-deberta/deberta-base/deberta-base\",\n        \"config_name\": \"../input/js-deberta/deberta-base/deberta-base/config.json\",\n        \"model_paths\": [f\"../input/js-deberta/JS_deberta-base-multi-cat/JS_deberta-base-multi-cat/deberta-v3-base_toxic_{fold}_model.bin\" for fold in range(5)],\n        \"version\": 2,\n        \"size\": 768\n    },\n    \"mpnet-base\":{\n        \"model_name\": \"../input/js-mpnet/mpnet-base/mpnet-base\",\n        \"config_name\": \"../input/js-mpnet/mpnet-base/mpnet-base/config.json\",\n        \"model_paths\": [f\"../input/js-mpnet/JS_mpnet-base_RL (1)/models/mpnet-base_{fold}_model.bin\" for fold in range(5)],\n        \"version\": 0,\n        \"size\": 768\n    },\n    \"mpnetv2-base\":{\n        \"model_name\": \"../input/js-mpnet/all-mpnet-base-v2/all-mpnet-base-v2\",\n        \"config_name\": \"../input/js-mpnet/all-mpnet-base-v2/all-mpnet-base-v2/config.json\",\n        \"model_paths\": [f\"../input/js-mpnet/JS_mpnetv2-base_RL/models/all-mpnet-base-v2_{fold}_model.bin\" for fold in range(5)],\n        \"version\": 0,\n        \"size\": 768\n    },\n    \"mpnetv2-base2\":{\n        \"model_name\": \"../input/js-mpnet/all-mpnet-base-v2/all-mpnet-base-v2\",\n        \"config_name\": \"../input/js-mpnet/all-mpnet-base-v2/all-mpnet-base-v2/config.json\",\n        \"model_paths\": [f\"../input/js-mpnet/JS_p0 (2)/content/models/all-mpnet-base-v2_toxic_{fold}_model.bin\" for fold in range(5)],\n        \"version\": 0,\n        \"size\": 768\n    },\n    \"albert-base\":{\n        \"model_name\": \"../input/js-albert/albert-base-v2/albert-base-v2\",\n        \"config_name\": \"../input/js-albert/albert-base-v2/albert-base-v2/config.json\",\n        \"model_paths\": [f\"../input/js-albert/JS_albert-base_RL/models/albert-base-v2_{fold}_model.bin\" for fold in range(5)],\n        \"version\": 0,\n        \"size\": 768\n    },    \n}","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.13159Z","iopub.execute_input":"2022-02-07T21:47:03.132194Z","iopub.status.idle":"2022-02-07T21:47:03.143622Z","shell.execute_reply.started":"2022-02-07T21:47:03.132147Z","shell.execute_reply":"2022-02-07T21:47:03.142844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, col):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = self.df[col].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']        \n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.145045Z","iopub.execute_input":"2022-02-07T21:47:03.145442Z","iopub.status.idle":"2022-02-07T21:47:03.156594Z","shell.execute_reply.started":"2022-02-07T21:47:03.145354Z","shell.execute_reply":"2022-02-07T21:47:03.155895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self, config_name):\n        super(JigsawModel, self).__init__()\n        config = AutoConfig.from_pretrained(config_name)\n        self.model = AutoModel.from_config(config = config)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc1 = nn.Linear(CONFIG['size'],256)\n        self.fc2 = nn.Linear(256,CONFIG['num_classes'])\n\n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc2(self.fc1(out))\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.159683Z","iopub.execute_input":"2022-02-07T21:47:03.159985Z","iopub.status.idle":"2022-02-07T21:47:03.169646Z","shell.execute_reply.started":"2022-02-07T21:47:03.159946Z","shell.execute_reply":"2022-02-07T21:47:03.168919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass JigsawModel2(nn.Module):\n    def __init__(self, model_name, config_path=None, pretrained=False):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(model_name, output_hidden_states=True)\n        self.model = AutoModel.from_config(config=self.config)\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        self.dropout = nn.Dropout(0.4)\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self, ids, mask):\n        x = self.model(input_ids=ids,attention_mask=mask)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x\n\nclass JigsawModel3(nn.Module):\n    def __init__(self, model_name, config_path=None, pretrained=False):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(model_name, output_hidden_states=True)\n        self.model = AutoModel.from_config(config=self.config)\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        self.dropout = nn.Dropout(0.4)\n        self.fc1 = nn.Linear(self.config.hidden_size,256)\n        self.fc2 = nn.Linear(256,1)\n\n    def forward(self, ids, mask):\n        x = self.model(input_ids=ids,attention_mask=mask)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.fc1(x)\n        x = torch.sigmoid(self.fc2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.171191Z","iopub.execute_input":"2022-02-07T21:47:03.17147Z","iopub.status.idle":"2022-02-07T21:47:03.188353Z","shell.execute_reply.started":"2022-02-07T21:47:03.171435Z","shell.execute_reply":"2022-02-07T21:47:03.187606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        PREDS.append(outputs.sigmoid().view(-1).cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    del model\n    gc.collect()\n    \n    return PREDS","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.189696Z","iopub.execute_input":"2022-02-07T21:47:03.190173Z","iopub.status.idle":"2022-02-07T21:47:03.199386Z","shell.execute_reply.started":"2022-02-07T21:47:03.190134Z","shell.execute_reply":"2022-02-07T21:47:03.19858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(model_paths, dataloader, device, version):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        if version == 1:\n            model = JigsawModel2(CONFIG['config_name'])\n        elif version == 0:\n            model = JigsawModel(CONFIG[\"config_name\"])\n        elif version ==2:\n            model = JigsawModel3(CONFIG[\"config_name\"])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path, map_location = device))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.200447Z","iopub.execute_input":"2022-02-07T21:47:03.201903Z","iopub.status.idle":"2022-02-07T21:47:03.211607Z","shell.execute_reply.started":"2022-02-07T21:47:03.201864Z","shell.execute_reply":"2022-02-07T21:47:03.210821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_multimodel(model_dict, device, col = 'text', df = sub_df2):\n    \n    pred =pd.DataFrame()\n    pred \n    for key, item in model_dict.items():\n        print(key)\n        CONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(item['model_name']) \n        CONFIG[\"config_name\"] = item[\"config_name\"]\n        CONFIG[\"size\"] = item[\"size\"]\n        MODEL_PATHS = item[\"model_paths\"]\n        version = item[\"version\"]\n        test_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'], col = col)\n        test_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                                 num_workers=2, shuffle=False, pin_memory=True)\n        preds = inference(MODEL_PATHS, test_loader, device, version)\n        pred[key] = preds   \n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.213033Z","iopub.execute_input":"2022-02-07T21:47:03.213837Z","iopub.status.idle":"2022-02-07T21:47:03.222458Z","shell.execute_reply.started":"2022-02-07T21:47:03.213799Z","shell.execute_reply":"2022-02-07T21:47:03.221563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    if VAL:\n        preds_more_toxic_ = inference_multimodel(model_dict, CONFIG['device'], col = \"more_toxic\", df = val_df)\n        preds_less_toxic_ = inference_multimodel(model_dict, CONFIG['device'], col = \"less_toxic\", df = val_df)\n        p1 = preds_more_toxic_[list(model_dict.keys())].mean(axis = 1)\n        p2 = preds_less_toxic_[list(model_dict.keys())].mean(axis = 1)\n        print(f\"Accuracy: {np.round((p2 < p1).mean() * 100, 2)}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.224154Z","iopub.execute_input":"2022-02-07T21:47:03.224754Z","iopub.status.idle":"2022-02-07T21:47:03.232586Z","shell.execute_reply.started":"2022-02-07T21:47:03.22465Z","shell.execute_reply":"2022-02-07T21:47:03.231913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    preds = inference_multimodel(model_dict, CONFIG['device'], col = \"text\", df = sub_df2)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:48:12.310678Z","iopub.execute_input":"2022-02-07T21:48:12.311627Z","iopub.status.idle":"2022-02-07T21:49:23.608632Z","shell.execute_reply.started":"2022-02-07T21:48:12.311584Z","shell.execute_reply":"2022-02-07T21:49:23.607636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    preds[\"score\"] = preds[list(model_dict.keys())].mean(axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.246709Z","iopub.execute_input":"2022-02-07T21:47:03.247159Z","iopub.status.idle":"2022-02-07T21:47:03.251809Z","shell.execute_reply.started":"2022-02-07T21:47:03.247122Z","shell.execute_reply":"2022-02-07T21:47:03.251009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    sub_df2[\"score\"] = preds[\"score\"]\n    sub_df2.to_csv(\"submission_DL.csv\", index=None)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.253012Z","iopub.execute_input":"2022-02-07T21:47:03.254054Z","iopub.status.idle":"2022-02-07T21:47:03.25947Z","shell.execute_reply.started":"2022-02-07T21:47:03.253962Z","shell.execute_reply":"2022-02-07T21:47:03.258434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *FASTTEXT*","metadata":{}},{"cell_type":"code","source":"N_MODELS = 4\nEXTRA_DIM = 256\nALPHA_STEP_SIZE = 0.5","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.261029Z","iopub.execute_input":"2022-02-07T21:47:03.261879Z","iopub.status.idle":"2022-02-07T21:47:03.266244Z","shell.execute_reply.started":"2022-02-07T21:47:03.261841Z","shell.execute_reply":"2022-02-07T21:47:03.265237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning_fasttext(text):\n    template = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = template.sub(r'', text)\n    soup = BeautifulSoup(text, 'lxml')\n    only_text = soup.get_text()\n    text = only_text\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"\n                               u\"\\U0001F300-\\U0001F5FF\"\n                               u\"\\U0001F680-\\U0001F6FF\"\n                               u\"\\U0001F1E0-\\U0001F1FF\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags = re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text)\n    text = re.sub(' +', ' ', text)\n    text = text.strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.267749Z","iopub.execute_input":"2022-02-07T21:47:03.268381Z","iopub.status.idle":"2022-02-07T21:47:03.273717Z","shell.execute_reply.started":"2022-02-07T21:47:03.268344Z","shell.execute_reply":"2022-02-07T21:47:03.272687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    df = pd.read_csv('../input/jigsaw-regression-based-data/train_data_version3.csv')\n    df = df.dropna(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.275251Z","iopub.execute_input":"2022-02-07T21:47:03.27648Z","iopub.status.idle":"2022-02-07T21:47:03.281524Z","shell.execute_reply.started":"2022-02-07T21:47:03.27644Z","shell.execute_reply":"2022-02-07T21:47:03.280555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    df[\"text\"] = df[\"text\"].progress_apply(text_cleaning_fasttext)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.283013Z","iopub.execute_input":"2022-02-07T21:47:03.283668Z","iopub.status.idle":"2022-02-07T21:47:03.288058Z","shell.execute_reply.started":"2022-02-07T21:47:03.283629Z","shell.execute_reply":"2022-02-07T21:47:03.287157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    vec = TfidfVectorizer(min_df = 3, max_df = 0.5, analyzer = 'char_wb', ngram_range = (3, 5), max_features = 46000)\n    vec.fit(df['text'])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.289588Z","iopub.execute_input":"2022-02-07T21:47:03.290412Z","iopub.status.idle":"2022-02-07T21:47:03.294705Z","shell.execute_reply.started":"2022-02-07T21:47:03.290317Z","shell.execute_reply":"2022-02-07T21:47:03.293678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    fmodel = FastText.load('../input/jigsaw-regression-based-data/FastText-jigsaw-256D/Jigsaw-Fasttext-Word-Embeddings-256D.bin')","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.296177Z","iopub.execute_input":"2022-02-07T21:47:03.297721Z","iopub.status.idle":"2022-02-07T21:47:03.307045Z","shell.execute_reply.started":"2022-02-07T21:47:03.297692Z","shell.execute_reply":"2022-02-07T21:47:03.304838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def splitter(text): \n    return [word for word in text.split(' ')]\n\ndef vectorizer(text):\n    tokens = splitter(text)\n    x1 = vec.transform([text]).toarray()\n    x2 = np.mean(fmodel.wv[tokens], axis = 0).reshape(1, -1)\n    x = np.concatenate([x1, x2], axis = -1).astype(np.float16)\n    del x1\n    del x2\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.308375Z","iopub.execute_input":"2022-02-07T21:47:03.308685Z","iopub.status.idle":"2022-02-07T21:47:03.31352Z","shell.execute_reply.started":"2022-02-07T21:47:03.308646Z","shell.execute_reply":"2022-02-07T21:47:03.312509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    X_np = np.array([vectorizer(text) for text in df.text]).reshape(-1, (len(vec.vocabulary_) + EXTRA_DIM))\n    X = sparse.csr_matrix(X_np)\n    del X_np","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.315159Z","iopub.execute_input":"2022-02-07T21:47:03.315685Z","iopub.status.idle":"2022-02-07T21:47:03.320677Z","shell.execute_reply.started":"2022-02-07T21:47:03.315628Z","shell.execute_reply":"2022-02-07T21:47:03.319835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RidgeEnsemble():\n    def __init__(self, n_models = 4, alpha_step_size = 0.5): \n\n        self.models = [Ridge(alpha = alpha) for alpha in [alpha_step_size * i for i in range(1, n_models + 1)]]\n\n    def fit(self, X, y): \n\n        self.models = [model.fit(X, y) for model in self.models]\n\n    def predict(self, X): \n\n        return np.mean(np.concatenate([np.expand_dims(model.predict(X), axis = 0) for model in self.models], axis = 0), axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.322503Z","iopub.execute_input":"2022-02-07T21:47:03.323236Z","iopub.status.idle":"2022-02-07T21:47:03.328043Z","shell.execute_reply.started":"2022-02-07T21:47:03.323189Z","shell.execute_reply":"2022-02-07T21:47:03.327036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    model = RidgeEnsemble()\n    model.fit(X, df['y'])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.329716Z","iopub.execute_input":"2022-02-07T21:47:03.330459Z","iopub.status.idle":"2022-02-07T21:47:03.336807Z","shell.execute_reply.started":"2022-02-07T21:47:03.330212Z","shell.execute_reply":"2022-02-07T21:47:03.335981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    if VAL:\n        val_df = pd.read_csv(\"../input/js-cleaned-validation-data/Validation_data_clean_no_duplicates.csv\")\n\n        val_df[\"more_toxic\"] = val_df[\"more_toxic\"].progress_apply(text_cleaning_fasttext)\n        val_df[\"less_toxic\"] = val_df[\"less_toxic\"].progress_apply(text_cleaning_fasttext)\n\n        X_less_toxic_temp = []\n        for text in val_df.less_toxic: \n            X_less_toxic_temp.append(vectorizer(text))\n        X_less_toxic_temp = np.array(X_less_toxic_temp).reshape(-1, (len(vec.vocabulary_) + EXTRA_DIM))\n        X_less_toxic = sparse.csr_matrix(X_less_toxic_temp)\n        del X_less_toxic_temp\n\n        X_more_toxic_temp = []\n        for text in val_df.more_toxic: \n            X_more_toxic_temp.append(vectorizer(text))\n        X_more_toxic_temp = np.array(X_more_toxic_temp).reshape(-1, (len(vec.vocabulary_) + EXTRA_DIM))\n        X_more_toxic = sparse.csr_matrix(X_more_toxic_temp)\n        del X_more_toxic_temp\n\n        preds_more_toxic = model.predict(X_more_toxic)\n        preds_less_toxic = model.predict(X_less_toxic)\n\n        print(f\"Accuracy: {np.round((preds_less_toxic < preds_more_toxic).mean() * 100, 2)}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.340578Z","iopub.execute_input":"2022-02-07T21:47:03.341138Z","iopub.status.idle":"2022-02-07T21:47:03.3468Z","shell.execute_reply.started":"2022-02-07T21:47:03.341075Z","shell.execute_reply":"2022-02-07T21:47:03.345899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    df_sub3 = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    df_sub3['text'] = df_sub3['text'].progress_apply(text_cleaning_fasttext)\n    X_sub_temp = []\n    for text in df_sub3.text: \n        X_sub_temp.append(vectorizer(text))\n    X_sub_temp = np.array(X_sub_temp).reshape(-1, (len(vec.vocabulary_) + 256))\n    X_test = sparse.csr_matrix(X_sub_temp)\n    del X_sub_temp\n\n    df_sub3['score'] = model.predict(X_test)\n    df_sub3['score'] = df_sub3['score']\n    df_sub3[['comment_id', 'score']].to_csv(\"submission_fasttext.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.348526Z","iopub.execute_input":"2022-02-07T21:47:03.349277Z","iopub.status.idle":"2022-02-07T21:47:03.353988Z","shell.execute_reply.started":"2022-02-07T21:47:03.349236Z","shell.execute_reply":"2022-02-07T21:47:03.353007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    if VAL:\n        p1 = p2_wt\n        p2 = p1_wt\n        p3 = preds_more_toxic_[list(model_dict.keys())].mean(axis = 1)\n        p4 = preds_less_toxic_[list(model_dict.keys())].mean(axis = 1)\n        p5 = preds_more_toxic\n        p6 = preds_less_toxic\n        wts_acc_ = []\n        for i in range(1, 100, 1):\n            for j in range(i,100, 1):\n                w1 = i / 100\n                w2 = j / 100\n                w3 = (100 - i - j) / 100\n                if (w3>0):\n                    p1_wt = w1 * p1 + w2 * p3 + w3 * p5\n                    p2_wt = w1 * p2 + w2 * p4 + w3 * p6\n                    wts_acc_.append((w1, w2, w3, np.round((p2_wt < p1_wt).mean() * 100, 2)))\n        w1_, w2_, w3_, score = sorted(wts_acc_, key = lambda x: x[3], reverse = True)[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.355683Z","iopub.execute_input":"2022-02-07T21:47:03.356595Z","iopub.status.idle":"2022-02-07T21:47:03.363237Z","shell.execute_reply.started":"2022-02-07T21:47:03.356558Z","shell.execute_reply":"2022-02-07T21:47:03.362068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1_ = 0.7\nw2_ = 0.15\nw3_ = 0.15","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.36518Z","iopub.execute_input":"2022-02-07T21:47:03.365754Z","iopub.status.idle":"2022-02-07T21:47:03.370729Z","shell.execute_reply.started":"2022-02-07T21:47:03.365712Z","shell.execute_reply":"2022-02-07T21:47:03.369701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    sub1 = pd.read_csv(\"./submission_tfidf.csv\")\n    sub2 = pd.read_csv(\"./submission_DL.csv\")\n    sub3 = pd.read_csv(\"./submission_fasttext.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.372767Z","iopub.execute_input":"2022-02-07T21:47:03.37342Z","iopub.status.idle":"2022-02-07T21:47:03.38093Z","shell.execute_reply.started":"2022-02-07T21:47:03.373377Z","shell.execute_reply":"2022-02-07T21:47:03.379843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    sub[\"score\"] = sub1[\"score\"]*w1_ + sub2[\"score\"]*w2_ + sub3[\"score\"]*w3_\n    sub[[\"comment_id\", \"score\"]].to_csv(\"submission.csv\", index = None)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.382836Z","iopub.execute_input":"2022-02-07T21:47:03.383367Z","iopub.status.idle":"2022-02-07T21:47:03.389958Z","shell.execute_reply.started":"2022-02-07T21:47:03.383328Z","shell.execute_reply":"2022-02-07T21:47:03.38902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not PRIVATE:\n    sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    sub[\"score\"] = 0\n    sub[[\"comment_id\", \"score\"]].to_csv(\"submission.csv\", index = None)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:47:03.391932Z","iopub.execute_input":"2022-02-07T21:47:03.392568Z","iopub.status.idle":"2022-02-07T21:47:03.46086Z","shell.execute_reply.started":"2022-02-07T21:47:03.392528Z","shell.execute_reply":"2022-02-07T21:47:03.4601Z"},"trusted":true},"execution_count":null,"outputs":[]}]}