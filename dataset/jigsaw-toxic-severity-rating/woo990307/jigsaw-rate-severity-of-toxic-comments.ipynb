{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-12T12:35:08.399091Z","iopub.execute_input":"2021-12-12T12:35:08.399362Z","iopub.status.idle":"2021-12-12T12:35:08.430045Z","shell.execute_reply.started":"2021-12-12T12:35:08.399331Z","shell.execute_reply":"2021-12-12T12:35:08.429348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Required Libraries\nimport os  # operating system library\nimport gc  # Garbage Collector - module provides the ability to disable the collector, tune the collection frequency, and set debugging options\nimport copy  # The assignment operation does not copy the object, it only creates a reference to the object. \n# For mutable collections, or for collections containing mutable items, a copy is often needed so that it can be modified without changing the original. \n# This module provides general (shallow and deep) copy operations.\n\nimport time  # time library\nimport random  # library for working with random values\n\nimport string  # common string operations\n\n# For data manipulation\nimport pandas as pd  # data analysis library\nimport numpy as np  # library linear algebra, Fourier transform and random numbers\n\n# Pytorch Imports\nimport torch  #  a Tensor library like NumPy, with strong GPU support\nimport torch.nn as nn  # a neural networks library deeply integrated with autograd designed for maximum flexibility\nimport torch.optim as optim  # Pytorch also has a package with various optimization algorithms.\n# We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter.\n# This will let us replace our previous manually coded optimization step\n\nfrom torch.optim import lr_scheduler  # provides several methods to adjust the learning rate based on the number of epochs.\nfrom torch.utils.data import Dataset, DataLoader  # DataLoader and other utility functions for convenience\n# DataLoader is responsible for managing batches. You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches. \n# PyTorch has an abstract Dataset class. A Dataset can be anything that has a __len__ function (called by Python’s standard len function) and a __getitem__ function \n# as a way of indexing into it\n\n\n# Utils\nfrom tqdm import tqdm  # tqdm derives from the Arabic word taqaddum  which can mean \"progress,\" and is an abbreviation for \"I love you so much\" in Spanish \n# (te quiero demasiado).  this library show a smart progress meter - just wrap any iterable with tqdm(iterable)\nfrom collections import defaultdict  # Usually, a Python dictionary throws a KeyError if you try to get an item with a key that is not currently in the dictionary. \n# The defaultdict in contrast will simply create any items that you try to access (provided of course they do not exist yet). \n\n\n# Sklearn Imports\n# sklearn - а set of python modules for machine learning and data mining\nfrom sklearn.metrics import mean_squared_error  # Mean squared error regression loss\nfrom sklearn.model_selection import StratifiedKFold, KFold  # Stratified K-Folds cross-validator.\n# K-Folds cross-validator. Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).\n# Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AdamW  \n#  In many cases, the architecture you want to use can be guessed from the name or the path of the \n# pretrained model you are supplying to the from_pretrained() method. AutoClasses are here to do this job for you so that you automatically retrieve the \n# relevant model given the name/path to the pretrained weights/config/vocabulary.\n# Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\n# ANSI escape character sequences have long been used to produce colored terminal text and cursor positioning on Unix and Macs. \n# Colorama makes this work on Windows, too, by wrapping stdout, stripping ANSI sequences it finds (which would appear as gobbledygook in the output), \n# and converting them into the appropriate win32 calls to modify the state of the terminal. On other platforms, Colorama does nothing.\n\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\n# Suppress warnings\nimport warnings  # Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program\n\nwarnings.filterwarnings(\"ignore\")  # This is the base class of all warning category classes. It is a subclass of Exception. \n# The warnings filter controls whether warnings are ignored, displayed, or turned into errors (raising an exception) \n# \"ignore\" - never print matching warnings\n# For descriptive error messages\n\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n# The beautiful thing of PyTorch's immediate execution model is that you can actually debug your programs. Sometimes, however, the asynchronous nature \n# of CUDA execution makes it hard. Here is a little trick to debug your programs. When you run a PyTorch program using CUDA operations, \n# the program usually doesn't wait until the computation finishes but continues to throw instructions at the GPU until it actually needs \n# a result (e.g. to evaluate using .item() or .cpu() or printing).\n# While this behaviour is key to the blazing performance of PyTorch programs, there is a downside: When a cuda operation fails, your program \n# has long gone on to do other stuff. The usual symptom is that you get a very non-descript error at a more or less random place somewhere \n# after the instruction that triggered the error.\n# One option in debugging is to move things to CPU. But often, we use libraries or have complex things where that isn't an option. So what now? If we could only get a good traceback, we should find the problem in no time.\n# This is how to get a good traceback:You can launch the program with the environment variable CUDA_LAUNCH_BLOCKING set to 1.\n\n\nfrom transformers import logging  # lib for logging\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:35:10.786877Z","iopub.execute_input":"2021-12-12T12:35:10.787139Z","iopub.status.idle":"2021-12-12T12:35:10.797841Z","shell.execute_reply.started":"2021-12-12T12:35:10.78711Z","shell.execute_reply":"2021-12-12T12:35:10.797108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Configuration\ndef id_generator(size=12, chars=string.ascii_lowercase + string.digits):  # this function Takes random choices from 12 ascii etters and digits\n    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))  # returns the resulting 12 character string\n \n# In Python, string ascii_lowercase will give the lowercase letters ‘abcdefghijklmnopqrstuvwxyz’.\nHASH_NAME = id_generator(size=12)  # will create a test variable and check how our generation function works\nprint(HASH_NAME)  # display the result","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:35:18.15218Z","iopub.execute_input":"2021-12-12T12:35:18.152462Z","iopub.status.idle":"2021-12-12T12:35:18.161454Z","shell.execute_reply.started":"2021-12-12T12:35:18.15243Z","shell.execute_reply":"2021-12-12T12:35:18.160408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## define the configuration of our model\n#CONFIG = {\"seed\": 42, # 2021->42\n#          \"epochs\": 3, # 3->5->10->7->3\n#          \"model_name\": \"roberta-base\",\n#          \"train_batch_size\": 32,\n#          \"valid_batch_size\": 64,\n#          \"max_length\": 128,\n#          \"learning_rate\": 1e-4,\n#          \"scheduler\": 'CosineAnnealingLR',\n#          \"min_lr\": 1e-6,\n#          \"T_max\": 500,\n#          \"weight_decay\": 1e-6,\n#          \"n_fold\": 10, #5->10->5->10\n#          \"n_accumulate\": 1,\n#          \"num_classes\": 1,\n#          \"margin\": 0.5,\n#          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n#          \"hash_name\": HASH_NAME\n#          }\n#\n## 10folds * 3epochs * 13 minuts = 390 minuts = 6,5 hours  on GPU kaggle\n## if Using GPU: Tesla K80 google colab it will turn out many times faster = 1,5 часа\n#\n#CONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n#CONFIG['group'] = f'{HASH_NAME}-Baseline'","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:37:05.019034Z","iopub.execute_input":"2021-12-12T12:37:05.01962Z","iopub.status.idle":"2021-12-12T12:37:05.024068Z","shell.execute_reply.started":"2021-12-12T12:37:05.019576Z","shell.execute_reply":"2021-12-12T12:37:05.02321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Set Seed for Reproducivility\n## Sets the seed of the entire notebook so results are the same every time we run.\n## This is for REPRODUCIBILITY\n#def set_seed(seed=42): \n#    np.random.seed(seed)\n#    torch.manual_seed(seed)\n#    torch.cuda.manual_seed(seed)\n#    # When running on the CuDNN backend, two further options must be set\n#    torch.backends.cudnn.deterministic = True\n#    torch.backends.cudnn.benchmark = False\n#    # Set a fixed value for the hash seed\n#    os.environ['PYTHONHASHSEED'] = str(seed)\n#    \n#set_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:37:07.688672Z","iopub.execute_input":"2021-12-12T12:37:07.689239Z","iopub.status.idle":"2021-12-12T12:37:07.69662Z","shell.execute_reply.started":"2021-12-12T12:37:07.689203Z","shell.execute_reply":"2021-12-12T12:37:07.695844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Read the Data\n#df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")  # create a variable dataframe containing data from the original competition data file  \n#print(df.shape)  # display statistics for in this file\n#df.head()  # display the first 5 rows of the dataframe table","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:37:25.898797Z","iopub.execute_input":"2021-12-12T12:37:25.899072Z","iopub.status.idle":"2021-12-12T12:37:25.902791Z","shell.execute_reply.started":"2021-12-12T12:37:25.899041Z","shell.execute_reply":"2021-12-12T12:37:25.901797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create Folds\n#skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])  # set the parameters for splitting our dataframe into data for training and testing\n#\n#for fold, ( _, val_) in enumerate(skf.split(X=df, y=df.worker)):  # dataframe splitting\n#    df.loc[val_ , \"kfold\"] = int(fold)\n#    \n#df[\"kfold\"] = df[\"kfold\"].astype(int)  # add one more column of folder number to the original dataframe\n#df.head()  # display the first 5 rows of the dataframe table","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:37:36.176947Z","iopub.execute_input":"2021-12-12T12:37:36.177444Z","iopub.status.idle":"2021-12-12T12:37:36.181223Z","shell.execute_reply.started":"2021-12-12T12:37:36.177407Z","shell.execute_reply":"2021-12-12T12:37:36.180491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Dataset Class\n#class JigsawDataset(Dataset):  # create a JigsawDataset class\n#    def __init__(self, df, tokenizer, max_length):  # initialization of the class at the input of the dataframe, tokenizer, max_length\n#        # set the class attributes\n#        # the __init__ function is run once when instantiating the Dataset object.\n#        self.df = df\n#        self.max_len = max_length\n#        self.tokenizer = tokenizer\n#        self.more_toxic = df['more_toxic'].values\n#        self.less_toxic = df['less_toxic'].values\n#        \n#    def __len__(self):  # the __len__ function returns the number of samples in our dataset.\n#        return len(self.df)\n#    \n#    def __getitem__(self, index):  # __getitem__ function loads and returns a sample from the dataset at the given index index. Based on the index, \n#        # it identifies the data location on disk, encodes the data, and sets the target = 1. Finally it returns tensors of the ids, mask, target. \n#        # Remember that we are using the roberta model here which does not have token_type_ids. \n#        \n#        more_toxic = self.more_toxic[index]\n#        less_toxic = self.less_toxic[index]\n#        inputs_more_toxic = self.tokenizer.encode_plus(\n#                                more_toxic,\n#                                truncation=True,\n#                                add_special_tokens=True,\n#                                max_length=self.max_len,\n#                                padding='max_length'\n#                            )\n#        inputs_less_toxic = self.tokenizer.encode_plus(\n#                                less_toxic,\n#                                truncation=True,\n#                                add_special_tokens=True,\n#                                max_length=self.max_len,\n#                                padding='max_length'\n#                            )\n#        target = 1\n#        # Since no Target variable is present in the dataset (remember we explicitly specified Target = 1 in the dataset class), we use the Margin Ranking Loss\n#        # It creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch Tensors, and a label 1D mini-batch tensor y (containing 1 or -1).\n#        # If y = 1, then it assumed the first input should be ranked higher (should have a larger value) than the second input, and vice-versa for y = -1.\n#        # For the same reason, we set Target = 1 earlier.\n#        \n#        more_toxic_ids = inputs_more_toxic['input_ids']\n#        more_toxic_mask = inputs_more_toxic['attention_mask']\n#        \n#        less_toxic_ids = inputs_less_toxic['input_ids']\n#        less_toxic_mask = inputs_less_toxic['attention_mask']\n#        \n#        \n#        return {  # returns the obtained values\n#            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n#            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n#            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n#            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n#           'target': torch.tensor(target, dtype=torch.long)\n#        }","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:38:27.024443Z","iopub.execute_input":"2021-12-12T12:38:27.024778Z","iopub.status.idle":"2021-12-12T12:38:27.031029Z","shell.execute_reply.started":"2021-12-12T12:38:27.024738Z","shell.execute_reply":"2021-12-12T12:38:27.030094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create Model\n#class JigsawModel(nn.Module):  # create a JigsawModel class\n#    def __init__(self, model_name):  # initialization of the class at the input of the dataframe, tokenizer, max_length\n#        # set the class attributes\n#        super(JigsawModel, self).__init__()\n#        self.model = AutoModel.from_pretrained(model_name)\n#        self.drop = nn.Dropout(p=0.2)\n#        self.fc = nn.Linear(768, CONFIG['num_classes'])\n#        \n#    def forward(self, ids, mask):        \n#        out = self.model(input_ids=ids,attention_mask=mask,\n#                         output_hidden_states=False)\n#        out = self.drop(out[1])\n#        outputs = self.fc(out)\n#        \n#        return outputs  # returns the obtained values","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:38:42.021595Z","iopub.execute_input":"2021-12-12T12:38:42.022354Z","iopub.status.idle":"2021-12-12T12:38:42.028444Z","shell.execute_reply.started":"2021-12-12T12:38:42.02231Z","shell.execute_reply":"2021-12-12T12:38:42.027433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Loss Function\n#def criterion(outputs1, outputs2, targets):  # Creates a criterion that measures the loss\n#    return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:38:46.96123Z","iopub.execute_input":"2021-12-12T12:38:46.961805Z","iopub.status.idle":"2021-12-12T12:38:46.966252Z","shell.execute_reply.started":"2021-12-12T12:38:46.961766Z","shell.execute_reply":"2021-12-12T12:38:46.965147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Function\n#def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):  # one epoch training function\n#    model.train()\n#    \n#    dataset_size = 0\n#    running_loss = 0.0\n#    \n#    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n#    \n#    for step, data in bar:\n#        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n#        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n#        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n#        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n#        targets = data['target'].to(device, dtype=torch.long)\n#        \n#        batch_size = more_toxic_ids.size(0)\n#\n#        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n#        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n#        \n#        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n#        loss = loss / CONFIG['n_accumulate']\n#        loss.backward()\n#    \n#        if (step + 1) % CONFIG['n_accumulate'] == 0:\n#            optimizer.step()\n#\n#            # zero the parameter gradients\n#            optimizer.zero_grad()\n#\n#            if scheduler is not None:\n#                scheduler.step()\n#                \n#        running_loss += (loss.item() * batch_size)\n#        dataset_size += batch_size\n#        \n#        epoch_loss = running_loss / dataset_size\n#        \n#        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n#                        LR=optimizer.param_groups[0]['lr'])\n#    gc.collect()\n#    \n#    return epoch_loss  # returns the result of the training function for one epoch","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:39:27.459752Z","iopub.execute_input":"2021-12-12T12:39:27.460248Z","iopub.status.idle":"2021-12-12T12:39:27.465153Z","shell.execute_reply.started":"2021-12-12T12:39:27.460206Z","shell.execute_reply":"2021-12-12T12:39:27.464417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Validation Function\n#@torch.no_grad()\n#def valid_one_epoch(model, dataloader, device, epoch):  # one epoch check function\n#    model.eval()\n#    \n#    dataset_size = 0\n#    running_loss = 0.0\n#    \n#    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n#    for step, data in bar:        \n#        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n#        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n#        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n#        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n#        targets = data['target'].to(device, dtype=torch.long)\n#        \n#        batch_size = more_toxic_ids.size(0)\n#\n#        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n#        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n#        \n#        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n#        \n#        running_loss += (loss.item() * batch_size)\n#        dataset_size += batch_size\n#        \n#        epoch_loss = running_loss / dataset_size\n#        \n#        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n#                        LR=optimizer.param_groups[0]['lr'])   \n#    \n#    gc.collect()\n#    \n#    return epoch_loss  # returns the result of the check function for one epoch","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:39:57.314263Z","iopub.execute_input":"2021-12-12T12:39:57.314521Z","iopub.status.idle":"2021-12-12T12:39:57.319722Z","shell.execute_reply.started":"2021-12-12T12:39:57.314492Z","shell.execute_reply":"2021-12-12T12:39:57.318548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run Training\n#def run_training(model, optimizer, scheduler, device, num_epochs, fold):  # general training function\n#    # To automatically log gradients\n#    #wandb.watch(model, log_freq=100)\n#    \n#    if torch.cuda.is_available():\n#        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n#    \n#    start = time.time()\n#    best_model_wts = copy.deepcopy(model.state_dict())\n#    best_epoch_loss = np.inf\n#    history = defaultdict(list)\n#    \n#    for epoch in range(1, num_epochs + 1): \n#        gc.collect()\n#        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n#                                           dataloader=train_loader, \n#                                           device=CONFIG['device'], epoch=epoch)\n#        \n#        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n#                                         epoch=epoch)\n#    \n#        history['Train Loss'].append(train_epoch_loss)\n#        history['Valid Loss'].append(val_epoch_loss)\n#        \n#        # Log the metrics\n#        #wandb.log({\"Train Loss\": train_epoch_loss})\n#        #wandb.log({\"Valid Loss\": val_epoch_loss})\n#        \n#        # deep copy the model\n#        if val_epoch_loss <= best_epoch_loss:\n#            print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n#            best_epoch_loss = val_epoch_loss\n#            #run.summary[\"Best Loss\"] = best_epoch_loss\n#            best_model_wts = copy.deepcopy(model.state_dict())\n#            PATH = f\"Loss-Fold-{fold}.bin\"\n#            torch.save(model.state_dict(), PATH)\n#            # Save a model file from the current directory\n#            print(f\"Model Saved{sr_}\")\n#            \n#        print()\n#    \n#    end = time.time()\n#    time_elapsed = end - start\n#    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n#        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n#    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n#    \n#    # load best model weights\n#    model.load_state_dict(best_model_wts)\n#    \n#    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:40:46.912484Z","iopub.execute_input":"2021-12-12T12:40:46.912789Z","iopub.status.idle":"2021-12-12T12:40:46.919949Z","shell.execute_reply.started":"2021-12-12T12:40:46.912746Z","shell.execute_reply":"2021-12-12T12:40:46.91717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def prepare_loaders(fold):\n#    df_train = df[df.kfold != fold].reset_index(drop=True)\n#    df_valid = df[df.kfold == fold].reset_index(drop=True)\n#    \n#    train_dataset = JigsawDataset(df_train, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n#    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n#\n#    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n#                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n#    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n#                              num_workers=2, shuffle=False, pin_memory=True)\n#    \n#    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:41:13.292345Z","iopub.execute_input":"2021-12-12T12:41:13.292645Z","iopub.status.idle":"2021-12-12T12:41:13.297084Z","shell.execute_reply.started":"2021-12-12T12:41:13.292612Z","shell.execute_reply":"2021-12-12T12:41:13.296099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def fetch_scheduler(optimizer):\n#    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n#        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n#                                                   eta_min=CONFIG['min_lr'])\n#    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n#        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n#                                                             eta_min=CONFIG['min_lr'])\n#    elif CONFIG['scheduler'] == None:\n#        return None\n#        \n#    return scheduler","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:41:22.812595Z","iopub.execute_input":"2021-12-12T12:41:22.813315Z","iopub.status.idle":"2021-12-12T12:41:22.817561Z","shell.execute_reply.started":"2021-12-12T12:41:22.813277Z","shell.execute_reply":"2021-12-12T12:41:22.8165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training\n#for fold in range(0, CONFIG['n_fold']):\n#    print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n#    \"\"\"\"\"\"run = wandb.init(project='Jigsaw', \n#                     config=CONFIG,\n#                     job_type='Train',\n#                     group=CONFIG['group'],\n#                     tags=['roberta-base', f'{HASH_NAME}', 'margin-loss'],\n#                     name=f'{HASH_NAME}-fold-{fold}',\n#                     anonymous='must')\"\"\"\"\"\"\n#    \n#    # Create Dataloaders\n#    train_loader, valid_loader = prepare_loaders(fold=fold)\n#    \n#    model = JigsawModel(CONFIG['model_name'])\n#    model.to(CONFIG['device'])\n#    \n#    # Define Optimizer and Scheduler\n#    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n#    scheduler = fetch_scheduler(optimizer)\n#    \n#    model, history = run_training(model, optimizer, scheduler,\n#                                  device=CONFIG['device'],\n#                                  num_epochs=CONFIG['epochs'],\n#                                  fold=fold)\n#    \n#    #run.finish()\n#    \n#    del model, history, train_loader, valid_loader\n#    _ = gc.collect()\n#    print()\n#print('Сompleted training and saving models!')","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:41:55.733702Z","iopub.execute_input":"2021-12-12T12:41:55.733986Z","iopub.status.idle":"2021-12-12T12:41:55.738635Z","shell.execute_reply.started":"2021-12-12T12:41:55.733953Z","shell.execute_reply":"2021-12-12T12:41:55.73799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## save tokenizer for turn-off internet\n#tkn = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n#tkn.save_pretrained('roberta-base-tokenizer')","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:42:07.129206Z","iopub.execute_input":"2021-12-12T12:42:07.129498Z","iopub.status.idle":"2021-12-12T12:42:07.136167Z","shell.execute_reply.started":"2021-12-12T12:42:07.129463Z","shell.execute_reply":"2021-12-12T12:42:07.132768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nCONFIG = dict(\n    seed = 42,\n    model_name = \"../input/roberta-base\", # '../input/roberta-base'\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\n#CONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name']) # for internet\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained('../input/robertabasetokenizer')\n\n\nMODEL_PATHS = [\n    '../input/jigsaw-toxic-comments-roberta-model/Loss-Fold-0.bin',\n    '../input/jigsaw-toxic-comments-roberta-model/Loss-Fold-1.bin',\n    '../input/jigsaw-toxic-comments-roberta-model/Loss-Fold-2.bin',\n    '../input/jigsaw-toxic-comments-roberta-model/Loss-Fold-3.bin',\n    '../input/jigsaw-toxic-comments-roberta-model/Loss-Fold-4.bin',\n    '../input/jigsaw-toxic-comments-roberta-model/Loss-Fold-5.bin',\n    '../input/jigsaw-toxic-comments-roberta-model/Loss-Fold-6.bin',\n    '../input/jigsaw-toxic-comments-roberta-model/Loss-Fold-7.bin',\n    '../input/jigsaw-toxic-comments-roberta-model/Loss-Fold-8.bin',\n    '../input/jigsaw-toxic-comments-roberta-model/Loss-Fold-9.bin'\n]\n\n\n\ndef set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    \nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']        \n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }    \n\n    \nclass JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs\n    \n@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS\n\n\ndef inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JigsawModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        #model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))  # for cpu\n        model.load_state_dict(torch.load(path))  # for gpu\n       \n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds\n\n\nset_seed(CONFIG['seed'])\ndf = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf.head()\n\ntest_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\npreds1 = inference(MODEL_PATHS, test_loader, CONFIG['device'])","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:42:13.427368Z","iopub.execute_input":"2021-12-12T12:42:13.427619Z","iopub.status.idle":"2021-12-12T12:47:50.622146Z","shell.execute_reply.started":"2021-12-12T12:42:13.427589Z","shell.execute_reply":"2021-12-12T12:47:50.619572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(preds1)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:48:09.40723Z","iopub.execute_input":"2021-12-12T12:48:09.407529Z","iopub.status.idle":"2021-12-12T12:48:09.414518Z","shell.execute_reply.started":"2021-12-12T12:48:09.407496Z","shell.execute_reply":"2021-12-12T12:48:09.413491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:48:17.996527Z","iopub.execute_input":"2021-12-12T12:48:17.997454Z","iopub.status.idle":"2021-12-12T12:48:18.01552Z","shell.execute_reply.started":"2021-12-12T12:48:17.997388Z","shell.execute_reply":"2021-12-12T12:48:18.01459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['score'] = preds1","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:48:21.84609Z","iopub.execute_input":"2021-12-12T12:48:21.846874Z","iopub.status.idle":"2021-12-12T12:48:21.857096Z","shell.execute_reply.started":"2021-12-12T12:48:21.846822Z","shell.execute_reply":"2021-12-12T12:48:21.856364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:48:25.631639Z","iopub.execute_input":"2021-12-12T12:48:25.632296Z","iopub.status.idle":"2021-12-12T12:48:25.641675Z","shell.execute_reply.started":"2021-12-12T12:48:25.632255Z","shell.execute_reply":"2021-12-12T12:48:25.640666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:48:35.516533Z","iopub.execute_input":"2021-12-12T12:48:35.516814Z","iopub.status.idle":"2021-12-12T12:48:35.553806Z","shell.execute_reply.started":"2021-12-12T12:48:35.516781Z","shell.execute_reply":"2021-12-12T12:48:35.553018Z"},"trusted":true},"execution_count":null,"outputs":[]}]}