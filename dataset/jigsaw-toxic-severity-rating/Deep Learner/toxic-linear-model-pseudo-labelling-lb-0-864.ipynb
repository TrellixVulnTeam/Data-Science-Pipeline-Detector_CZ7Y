{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Toxic Linear Public: LB 0.864\n---\nImprovements over the previous single model public SOTA (LB: 0.860):  \n- Adding pseudo labels by a deep model trained on validation_data.csv\n- Adding positive toxic(y>0) comments from test labels\n- Limiting the max features to prevent overfitting\n\nI only have 5 submissions a day, so I cannot perform as many experiments as I wish. I believe that the score for this notebook is a local optima and a much higher LB score is possible with the same dataset and a different set of hyperparameters.  \n\nI am requesting the Kaggle philanthropists in this competition to use some of their submissions to perform different experiments and comment the results in this notebook. Feel free to use some of my ideas below. \n\n### Version 5: 0.864","metadata":{}},{"cell_type":"code","source":"\"\"\"\nEXPERIMENTS\n-----------\n- 0.10 pseudo label weight: 0.861\n- 0.025 psuedo label weight: 0.864\n- Adding old test positives: 0.860 to 0.862\n- Changed max features from 90k to 50k: 0.864\n- Add word feature: 0.862 to 0.843\n- Changed Weights: 0.860 (Slight Change), 0.846 (Bigger Change)\n- Add 2019 competition toxic data: 0.862 to 0.795\n- Rearranging score with papi and roberta lb 833: 0.838 to 0.824\n- Rearranging score with papi+roberta 833+ensemble 864: 0.821\n- Adding 10k word features with 50k ngram features: 0.864 to 0.850\n\nTODO EXPERIMENTS \n----------------\n- Normalize the pseudo labels with mean 0 and variance 1 before adding?\n- What is the effect of random seed when selecting negatives? What is the variation, mean and the max?\n- How to balance regularization while adding more features?\n- What is the effect of casing (lowercase/uppercase)?\n- How does overlap with valid in the training set effect the score?\n- Best way to ensemble multiple rankings? \n- Effect of alpha regularization in Ridge \n- Effect of undersampling, min_df, adding word features, lowercase, etc.\n- Effect of different weights and the difference between them. \n- What is the effect of cleaning (no clean vs clean vs very clean)?\n- Linearly increasing weights? (obscene: 0.10, toxic: 0.20, ..., severe_toxic: 0.60) ?\n\n\nTODO FURTHERMORE\n----------------\n- Add bert embeddings along with tfidf features\n- Rearranging rankings with a cross encoder\n- Using an unbiased model to detect false positives \n    - comments with words like gay and jew have high scores irrespective of context\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:28:34.828655Z","iopub.execute_input":"2022-01-03T16:28:34.829012Z","iopub.status.idle":"2022-01-03T16:28:34.837184Z","shell.execute_reply.started":"2022-01-03T16:28:34.828973Z","shell.execute_reply":"2022-01-03T16:28:34.836167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notebook Imports & Setup\nfrom collections import Counter, defaultdict\nfrom functools import partial\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom time import time\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport joblib\nimport re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport sklearn.pipeline","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:53:24.460056Z","iopub.execute_input":"2022-01-03T15:53:24.461087Z","iopub.status.idle":"2022-01-03T15:53:25.40559Z","shell.execute_reply.started":"2022-01-03T15:53:24.461022Z","shell.execute_reply":"2022-01-03T15:53:25.404647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:53:25.407948Z","iopub.execute_input":"2022-01-03T15:53:25.40821Z","iopub.status.idle":"2022-01-03T15:53:25.813835Z","shell.execute_reply.started":"2022-01-03T15:53:25.408179Z","shell.execute_reply":"2022-01-03T15:53:25.812726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def optimize_weights(cols, trials=300000): \n    for col in cols: \n        dict_map = sub.set_index('text')[col].to_dict()\n        valid[f'L{col}'] = valid.less_toxic.map(dict_map)\n        valid[f'M{col}'] = valid.more_toxic.map(dict_map)\n    best_weights = [1] * len(cols)\n    best_acc = 0\n    less_dict = {p: valid[f'L{p}'].values for p in cols}\n    more_dict = {p: valid[f'M{p}'].values for p in cols}\n\n    for _ in tqdm(range(trials)):\n        less, more = np.zeros(len(valid)), np.zeros(len(valid))\n        weights = [random.random() for _ in range(len(cols))]\n        for p, wt in zip(cols, weights): \n            less += wt * less_dict[p]\n            more += wt * more_dict[p]\n\n        acc = (more > less).sum() / len(valid)\n        if acc > best_acc: \n            print('acc improved from', best_acc , 'to ', acc)\n            best_acc = acc\n            best_weights = weights\n\n    print('Best Linear Accuracy: ', best_acc)\n    print('Best Weights: ', best_weights)\n    return np.array(best_weights)\n\ndef get_features(pipeline): \n    print('Total features: ', len(pipeline['features'].get_feature_names()))\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n    feature_df = pd.DataFrame(feature_wts, columns = ['feat','val']).T\n    return feature_df.T","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:53:25.815651Z","iopub.execute_input":"2022-01-03T15:53:25.815938Z","iopub.status.idle":"2022-01-03T15:53:25.830717Z","shell.execute_reply.started":"2022-01-03T15:53:25.815905Z","shell.execute_reply":"2022-01-03T15:53:25.829831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURE_WTS = {\n    'severe_toxic': 1.5, 'identity_hate': 1.5, 'threat': 1.5, \n    'insult': 0.64, 'toxic': 0.32, 'obscene': 0.16, \n}\nPSEUDO_LABEL_WEIGHT = 0.033\n\nFEATURES = list(FEATURE_WTS.keys())\nFEATURES","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:57:38.746506Z","iopub.execute_input":"2022-01-03T15:57:38.74685Z","iopub.status.idle":"2022-01-03T15:57:38.756081Z","shell.execute_reply.started":"2022-01-03T15:57:38.7468Z","shell.execute_reply":"2022-01-03T15:57:38.754875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"old_train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\nold_train['y'] = 0\nfor feat, wt in FEATURE_WTS.items(): \n    old_train.y += wt*old_train[feat]\nold_train.y = old_train.y/old_train.y.max()\n    \npos = old_train[old_train.y>0]\nneg = old_train[old_train.y==0].sample(len(pos), random_state=201)\nold_train = pd.concat([pos, neg])\nold_train","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:56:16.414523Z","iopub.execute_input":"2022-01-03T15:56:16.414841Z","iopub.status.idle":"2022-01-03T15:56:17.493262Z","shell.execute_reply.started":"2022-01-03T15:56:16.414796Z","shell.execute_reply":"2022-01-03T15:56:17.492253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_old_test(): \n    df_test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv')\n    df_test_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv')\n    df = pd.merge(df_test, df_test_labels, how='left', on = 'id')\n    df = df.replace(-1, np.nan).dropna()\n    return df\n\nold_test = read_old_test()\nold_test['y'] = 0\nfor feat, wt in FEATURE_WTS.items(): \n    old_test.y += wt * old_test[feat]\nold_test.y = old_test.y / old_test.y.max()\nold_test_pos = old_test[old_test.y>0]\n\ntrain = pd.concat([old_train, old_test_pos])","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:56:17.49572Z","iopub.execute_input":"2022-01-03T15:56:17.496065Z","iopub.status.idle":"2022-01-03T15:56:18.93543Z","shell.execute_reply.started":"2022-01-03T15:56:17.496024Z","shell.execute_reply":"2022-01-03T15:56:18.934398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add Pseudo Labels\npseudo = pd.read_csv('../input/toxic-public-dataframes/old_pseudo_label.csv')\ncomment_to_pseudo_label = pseudo.set_index('comment_text').to_dict()['score']\ntrain['soft_pseudo_label_score'] = train.comment_text.map(comment_to_pseudo_label)\n\ntrain.y = train.y + PSEUDO_LABEL_WEIGHT*train.soft_pseudo_label_score\ntrain.sort_values(by='y')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:58:33.31439Z","iopub.execute_input":"2022-01-03T15:58:33.315001Z","iopub.status.idle":"2022-01-03T15:58:39.386461Z","shell.execute_reply.started":"2022-01-03T15:58:33.314958Z","shell.execute_reply":"2022-01-03T15:58:39.385433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overlap with valid\nvalid = pd.read_csv('../input/toxic-public-dataframes/valid.csv')\nin_valid = train.comment_text.isin(valid.more_toxic)|train.comment_text.isin(valid.less_toxic)\ntrain[in_valid]","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:36:40.0222Z","iopub.execute_input":"2022-01-03T16:36:40.023303Z","iopub.status.idle":"2022-01-03T16:36:40.201844Z","shell.execute_reply.started":"2022-01-03T16:36:40.023261Z","shell.execute_reply":"2022-01-03T16:36:40.200878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\ntrain.comment_text = train.comment_text.progress_apply(text_cleaning)\ntrain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.linear_model\nimport sklearn.pipeline\nimport joblib\n\nfeatures = sklearn.pipeline.FeatureUnion([\n    ('vec', TfidfVectorizer(\n        min_df=3, max_df=0.5, \n        analyzer='char_wb', ngram_range = (3,5), \n        lowercase=True, max_features=50000,\n    )), \n    # ('vec2', TfidfVectorizer(\n    #     min_df=3, max_df=0.75, \n    #     analyzer='word', \n    #     lowercase=False, max_features=10000,\n    # ))\n])\npipeline = sklearn.pipeline.Pipeline([\n    ('features', features), \n    ('clf', sklearn.linear_model.Ridge(alpha=0.5)), \n])\npipeline.fit(train.comment_text, train.y)\n\njoblib.dump(pipeline, 'pipeline.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:53:43.93223Z","iopub.execute_input":"2022-01-03T15:53:43.932953Z","iopub.status.idle":"2022-01-03T15:54:20.318766Z","shell.execute_reply.started":"2022-01-03T15:53:43.932915Z","shell.execute_reply":"2022-01-03T15:54:20.317894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyze Predictions \ndisplay(get_features(pipeline))\n\ntrain['y_pred'] = pipeline.predict(train.comment_text)\nprint('Train RMSE:', (((train.y_pred-train.y)**2).mean())**0.5)\ntrain['delta'] = abs(train.y_pred - train.y)\ntrain.sort_values(by='delta')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:03:53.556291Z","iopub.execute_input":"2022-01-03T16:03:53.5576Z","iopub.status.idle":"2022-01-03T16:04:27.234736Z","shell.execute_reply.started":"2022-01-03T16:03:53.557551Z","shell.execute_reply":"2022-01-03T16:04:27.23376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsub = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\nsub['comment_text'] = sub.text\nsub.text = sub.text.progress_apply(text_cleaning)\nsub['score'] = pipeline.predict(sub.text)\nsub","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:05:52.647032Z","iopub.execute_input":"2022-01-03T16:05:52.647491Z","iopub.status.idle":"2022-01-03T16:06:02.056974Z","shell.execute_reply.started":"2022-01-03T16:05:52.647459Z","shell.execute_reply":"2022-01-03T16:06:02.056057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[['comment_id', 'score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:06:07.869004Z","iopub.execute_input":"2022-01-03T16:06:07.869352Z","iopub.status.idle":"2022-01-03T16:06:07.890545Z","shell.execute_reply.started":"2022-01-03T16:06:07.869315Z","shell.execute_reply":"2022-01-03T16:06:07.889511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm.auto import tqdm\n# import random \n\n# papi = pd.read_csv('../input/toxic-pseudo-labels/perspective_api_lb771.csv')\n# roberta = pd.read_csv('../input/toxic-pseudo-labels/robertab_lb833.csv')\n# ensemble = pd.read_csv('../input/toxic-pseudo-labels/public_ensemble_lb864.csv')\n\n# papi_cid_to_score = papi.set_index('comment_id').to_dict()['score']\n# roberta_cid_to_score = roberta.set_index('comment_id').to_dict()['score']\n# ensemble_cid_to_score = ensemble.set_index('comment_id').to_dict()['score']\n# comment_id_to_idx = {cid: idx for idx, cid in enumerate(sub.comment_id)}\n\n# comment_ids = list(set(roberta.comment_id.values) & set(sub.comment_id.values))\n# sub_scores = list(sub.score.values)\n# num_swaps = 0\n# for i in tqdm(range(100000000)): \n#     cid1, cid2 = random.choice(comment_ids), random.choice(comment_ids)\n#     i1, i2 = comment_id_to_idx[cid1], comment_id_to_idx[cid2]\n#     if sub_scores[i1] > sub_scores[i2]: \n#         continue\n#     if (roberta_cid_to_score[cid1] > roberta_cid_to_score[cid2]) \\\n#     and (papi_cid_to_score[cid1] > papi_cid_to_score[cid2]) \\\n#     and (ensemble_cid_to_score[cid1] > ensemble_cid_to_score[cid2]): \n#         num_swaps += 1\n#         sub_scores[i1], sub_scores[i2] = sub_scores[i2], sub_scores[i1]\n\n# print('Number of swaps: ', num_swaps)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:54:29.972Z","iopub.status.idle":"2022-01-03T15:54:29.972834Z","shell.execute_reply.started":"2022-01-03T15:54:29.972598Z","shell.execute_reply":"2022-01-03T15:54:29.972623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub['org_score'] = sub.score.rank(method='first')\n# sub['score'] = sub_scores\n# sub.score = sub.score.rank(method='first')\n# sub\n\n# # sub['swap_scores'] = sub_scores\n# # sub.swap_scores = sub.swap_scores.rank(method='first')\n# # sub.score = sub.score.rank(method='first')\n\n# # sub","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:54:29.973756Z","iopub.status.idle":"2022-01-03T15:54:29.9747Z","shell.execute_reply.started":"2022-01-03T15:54:29.974449Z","shell.execute_reply":"2022-01-03T15:54:29.97447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:54:29.975657Z","iopub.status.idle":"2022-01-03T15:54:29.976284Z","shell.execute_reply.started":"2022-01-03T15:54:29.97608Z","shell.execute_reply":"2022-01-03T15:54:29.976105Z"},"trusted":true},"execution_count":null,"outputs":[]}]}