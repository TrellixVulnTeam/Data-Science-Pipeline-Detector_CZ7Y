{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"SUBMISSION = True\nSKIP = 16\n\nFAST_SUBMISSION = True","metadata":{"execution":{"iopub.status.busy":"2022-02-07T19:30:52.921945Z","iopub.execute_input":"2022-02-07T19:30:52.922246Z","iopub.status.idle":"2022-02-07T19:30:52.94899Z","shell.execute_reply.started":"2022-02-07T19:30:52.922155Z","shell.execute_reply":"2022-02-07T19:30:52.948369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Installations & Downloads\n!pip install -U --no-build-isolation --no-deps /kaggle/input/transformers-master/ -qq\n!pip install --no-index --find-links /kaggle/input/hf-datasets/wheels datasets -qq\n\n# Notebook Imports & Setup\nfrom functools import partial\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport tensorflow as tf\nfrom time import time\nimport pandas as pd\nimport numpy as np\nimport random \nimport joblib\nimport re\n\nimport tensorflow_addons as tfa\nimport tensorflow as tf\n\nimport transformers\nimport tokenizers\nimport datasets\n\n# Enable Mixed Precision and JIT Compilation\ndef _enable_mixed_precision(): \n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n\n_enable_mixed_precision()\ntf.config.optimizer.set_jit(True)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T19:30:52.951756Z","iopub.execute_input":"2022-02-07T19:30:52.95344Z","iopub.status.idle":"2022-02-07T19:31:49.529817Z","shell.execute_reply.started":"2022-02-07T19:30:52.953412Z","shell.execute_reply":"2022-02-07T19:31:49.528338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BACKBONES_DIR = Path('/kaggle/input/toxic-internet-deep-model-backbones')\nWEIGHTS_DIR = Path('/kaggle/input/toxic-internet-ensemble-model-weights')\nif SUBMISSION: \n    test = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n    test['comment_text'] = test.text\nelse: \n    test = pd.read_csv('../input/toxic-dataframes/test_comments.csv')\n    test['text'] = test.comment_text\n\n# old = pd.read_csv('../input/toxic-dataframes/old_pseudo_label.csv')\n# valid = pd.read_csv('../input/toxic-dataframes/valid.csv')\n# #valid = valid[valid.fold==3]\n# valid = valid[valid.more_toxic.isin(old.comment_text) & valid.less_toxic.isin(old.comment_text)]\n# # # valid = pd.concat([valid, pd.read_csv('../input/toxic-dataframes/manual_pair_labels (1).csv')])\n# comments = np.unique(np.concatenate([valid.more_toxic.values, valid.less_toxic.values]))\n# test = pd.DataFrame({'comment_text': comments, 'text': comments})\n# test['comment_id'] = 0\n\nif len(test) < 10000: \n    SKIP = SKIP * 4\nif FAST_SUBMISSION and len(test) < 10000: \n    test = test.sample(1000)\n\npipeline = joblib.load('/kaggle/input/toxic-dataframes/pipeline_lb864.pkl')\n%time test['tfidf_score'] = pipeline.predict(test.text)\ntest['tfidf_score'] = test.tfidf_score.rank(method='first')\nTFIDF_SCORES = test.tfidf_score.values\n\nrobertal_tokenizer = transformers.AutoTokenizer.from_pretrained(BACKBONES_DIR/'roberta_large')\nwith tf.device('/device:GPU:0'): \n    robertal_backbone = transformers.TFAutoModel.from_pretrained(BACKBONES_DIR/'roberta_large')","metadata":{"execution":{"iopub.status.busy":"2022-02-07T19:31:49.531505Z","iopub.execute_input":"2022-02-07T19:31:49.531772Z","iopub.status.idle":"2022-02-07T19:32:16.195576Z","shell.execute_reply.started":"2022-02-07T19:31:49.531736Z","shell.execute_reply":"2022-02-07T19:32:16.194788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Type 1: Single Backbone w. Late Interaction Layer","metadata":{}},{"cell_type":"code","source":"def convert_dataset_to_test_ds(dataset): \n    'Processed huggingface dataset to tensorflow dataset'\n    dataset.set_format(type='numpy')\n    input_ids_ds = tf.data.Dataset.from_tensor_slices(dataset['input_ids'].astype(np.int32))\n    attention_mask_ds = tf.data.Dataset.from_tensor_slices(dataset['attention_mask'].astype(np.int32))\n    ds = tf.data.Dataset.zip((input_ids_ds, attention_mask_ds))\n    ds = tf.data.Dataset.zip((ds, ds))\n    return ds\n\ndef tokenize(examples): \n    return tokenizer(\n        examples['comment_text'], \n        max_length=MAX_SEQ_LEN, \n        padding='max_length', \n        truncation=True, \n    )\n\ndef df_to_test_ds(df):\n    raw_dataset = datasets.Dataset.from_pandas(df)\n    processed_dataset = raw_dataset.map(tokenize, batched=True)\n    test_ds = convert_dataset_to_test_ds(processed_dataset)\n    return test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE) \n\ndef load_tokenizer_and_backbone(backbone_name): \n    if backbone_name == 'roberta_large': \n        return robertal_tokenizer, robertal_backbone\n    folder = BACKBONES_DIR / backbone_name\n    print('Loading tokenizer and backbone from', folder)\n    tokenizer = transformers.AutoTokenizer.from_pretrained(str(folder))\n    with tf.device('/device:GPU:0'): \n        backbone = transformers.TFAutoModel.from_pretrained(str(folder))\n    return tokenizer, backbone\n\ndef build_hidden_layer(hidden_layer_units):\n    if hidden_layer_units is None: \n        return lambda x: x\n    hidden_layers = []\n    for unit in hidden_layer_units: \n        hidden_layers.append(tf.keras.layers.Dense(unit, activation=tfa.activations.mish))\n    return tf.keras.Sequential(hidden_layers, name='hidden_layer')\n\ndef build_model(backbone): \n    input_ids = tf.keras.Input((MAX_SEQ_LEN,), dtype=tf.int32)\n    attention_mask = tf.keras.Input((MAX_SEQ_LEN,), dtype=tf.int32)\n\n    backbone_outputs = backbone(\n        input_ids=input_ids, \n        attention_mask=attention_mask, \n        return_dict=True,\n    )\n    x = backbone_outputs.pooler_output\n    hidden_layer = build_hidden_layer(HIDDEN_LAYERS)\n    x = hidden_layer(x)\n    \n    score_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n    model = tf.keras.Model([input_ids, attention_mask],  outputs=score_layer(x))\n    return model\n\n\ndef build_interaction_model():\n    x_dim = 1024\n    A_x = tf.keras.Input((x_dim,), dtype=tf.float32)\n    B_x = tf.keras.Input((x_dim,), dtype=tf.float32)\n    if CONCAT_ABS_DIFF:\n        abs_diff = tf.math.abs(A_x-B_x)\n        x = tf.concat([A_x, B_x, abs_diff], axis=-1)\n    else: \n        x = tf.concat([A_x, B_x], axis=-1)\n    inter_hidden_layer = build_hidden_layer(INTER_HIDDEN_LAYERS)\n    x = inter_hidden_layer(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    return tf.keras.Model([A_x, B_x], outputs=x)\n\ndef temp_model(backbone): \n    input_ids = tf.keras.Input((MAX_SEQ_LEN,), dtype=tf.int32)\n    attention_mask = tf.keras.Input((MAX_SEQ_LEN,), dtype=tf.int32)\n    x = backbone(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n    return tf.keras.Model([input_ids, attention_mask], outputs=x)\n\ndef get_pooler_output(backbone, ds): \n    return np.squeeze(temp_model(backbone).predict(ds, verbose=1).astype(np.float32))\n\ndef build_test_ds(batch_i, num_compares, B_x):\n    start_i = batch_i * INTR_BATCH_SIZE \n    A_x = np.concatenate([\n        np.stack([sub_x[i] for _ in range(num_compares)])\n        for i in range(start_i, start_i+INTR_BATCH_SIZE)\n    ])\n    A_ds = tf.data.Dataset.from_tensor_slices(A_x)\n    B_ds = tf.data.Dataset.from_tensor_slices(B_x)\n    ds = tf.data.Dataset.zip((A_ds, B_ds))\n    ds = tf.data.Dataset.zip((ds, A_ds))\n    return ds.batch(4096*4).prefetch(tf.data.AUTOTUNE)\n\ndef split_for_each_i(array, num_compares): \n    return np.split(array, np.arange(num_compares, len(array), num_compares))\n\ndef compute_scores(sub_x, sub):\n    B_x = sub_x[np.arange(START, len(sub), SKIP)]\n    num_compares = len(B_x)\n    B_x = np.row_stack([B_x for _ in range(INTR_BATCH_SIZE)])\n    \n    sub_scores = []\n    for batch_i in tqdm(range(len(sub)//INTR_BATCH_SIZE)): \n        test_ds = build_test_ds(batch_i, num_compares, B_x)\n        scores = scorer.predict(test_ds, verbose=0)\n        scores = np.squeeze(scores.astype(np.float32))\n        scores_all = split_for_each_i(scores, num_compares)\n        for score_arr in scores_all:\n            thresh = 0.50\n            num_wins = (score_arr>thresh).sum()\n            sub_scores.append(num_wins+2*score_arr.mean())\n    \n    num_additions = len(sub)-len(sub_scores)\n    print(f'Adding {num_additions} random values')\n    sub_scores = sub_scores + [sum(sub_scores)/len(sub)+random.random() for _ in range(num_additions)]\n    sub['score'] = sub_scores\n    sub[['comment_id', 'score']].to_csv('submission.csv', index=False)\n    sub.tfidf_score = sub.tfidf_score.rank(method='first')\n    sub.score = sub.score.rank(method='first')\n    display(sub.sort_values(by='score'))\n#     display(sub.sort_values(by='tfidf_score'))\n#     display(test.sample(frac=1.))\n    return sub.score.values","metadata":{"execution":{"iopub.status.busy":"2022-02-07T19:32:16.197828Z","iopub.execute_input":"2022-02-07T19:32:16.198351Z","iopub.status.idle":"2022-02-07T19:32:16.226755Z","shell.execute_reply.started":"2022-02-07T19:32:16.198312Z","shell.execute_reply":"2022-02-07T19:32:16.225825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INTER_HIDDEN_LAYERS = [1024, 256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 256, 512\nSTART, INTR_BATCH_SIZE = 0, 128\n\nBACKBONE_WT = 'backbonelb844_loss1402_acc772.h5'\nINTR_WT = 'inter_model_loss1402_acc772_hi1024.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL1_LB863_SCORES: \\n')    \nMODEL1_LB863_SCORES = compute_scores(sub_x, test)\n\n\n\nINTER_HIDDEN_LAYERS = [256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 256, 512\nSTART, INTR_BATCH_SIZE = 1, 128\n\nBACKBONE_WT = 'backbonelb839_loss152_acc778.h5'\nINTR_WT = 'inter_model_loss152_acc778.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\n\nprint('MODEL2_LB872_SCORES: \\n')    \nMODEL2_LB872_SCORES = compute_scores(sub_x, test)\n\nSTART = SKIP-1 \nMODEL2_LB872_SCORES += compute_scores(sub_x, test)\nSTART = SKIP-2\nMODEL2_LB872_SCORES += compute_scores(sub_x, test)\nSTART = SKIP-3\nMODEL2_LB872_SCORES += compute_scores(sub_x, test)\nMODEL2_LB872_SCORES = MODEL2_LB872_SCORES / 4\n\nINTER_HIDDEN_LAYERS = [1024, 256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 256, 512\nSTART, INTR_BATCH_SIZE = 2, 128\n\nBACKBONE_WT = 'backbonelb844_loss1488_acc776.h5'\nINTR_WT = 'inter_model_loss1488_acc776.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL3_SCORES: \\n')    \nMODEL3_SCORES = compute_scores(sub_x, test)\n\n\n\nINTER_HIDDEN_LAYERS = [256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 256, 512\nSTART, INTR_BATCH_SIZE = 3, 128\n\nBACKBONE_WT = 'backbonelb838_loss1544_acc765.h5'\nINTR_WT = 'inter_model_loss1544_acc765.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL4_LB852_SCORES: \\n')    \nMODEL4_LB852_SCORES = compute_scores(sub_x, test)\n\n\n\nINTER_HIDDEN_LAYERS = [256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 192, 512\nSTART, INTR_BATCH_SIZE = 4, 128\n\nBACKBONE_WT = 'backbonelb8392_loss1503_acc782.h5'\nINTR_WT = 'inter_model_loss1503_acc782.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL5_SCORES: \\n')    \nMODEL5_SCORES = compute_scores(sub_x, test)\n\n\n\nINTER_HIDDEN_LAYERS = [1024, 256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 256, 512\nSTART, INTR_BATCH_SIZE = 5, 128\n\nBACKBONE_WT = 'backbonelb844freeze_loss128_acc75.h5'\nINTR_WT = 'inter_model_loss128_acc75.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL6_SCORES: \\n')    \nMODEL6_SCORES = compute_scores(sub_x, test)\n\n\n\nINTER_HIDDEN_LAYERS = [1024, 256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 192, 512\nSTART, INTR_BATCH_SIZE = 6, 128\n\nBACKBONE_WT = 'backboneVanillaFreezelb837_loss1406_acc80.h5'\nINTR_WT = 'inter_model_loss1406_acc80.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL7_SCORES: \\n')    \nMODEL7_SCORES = compute_scores(sub_x, test)\n\n\nINTER_HIDDEN_LAYERS = [1024, 256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 256, 512\nSTART, INTR_BATCH_SIZE = 7, 128\n\nBACKBONE_WT = 'backboneseq256_loss152_acc77.h5'\nINTR_WT = 'inter_model_loss152_acc77.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL8_SCORES: \\n')    \nMODEL8_SCORES = compute_scores(sub_x, test)\n\n\n\nINTER_HIDDEN_LAYERS = [256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 256, 512\nSTART, INTR_BATCH_SIZE = 7, 128\n\nBACKBONE_WT = 'backbone_loss1098_acc798_trloss176.h5'\nINTR_WT = 'inter_model_loss1098_acc798_trloss176.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL9_SCORES: \\n')    \nMODEL9_SCORES = compute_scores(sub_x, test)\n\n\nINTER_HIDDEN_LAYERS = [1024, 256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 256, 512\nSTART, INTR_BATCH_SIZE = 8, 128\n\nBACKBONE_WT = 'backbone_loss1044_acc86_any10_both100.h5'\nINTR_WT = 'inter_model_loss1044_acc86_any10_both100.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL11_SCORES: \\n')\nMODEL11_SCORES = compute_scores(sub_x, test)\n\n\n\n\nINTER_HIDDEN_LAYERS = [1024, 256, 64, 16, 4]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 256, 512\nSTART, INTR_BATCH_SIZE = 9, 128\n\nBACKBONE_WT = 'backbone_loss1344_acc77.h5'\nINTR_WT = 'inter_model_loss1344_acc77.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL12_SCORES: \\n')\nMODEL12_SCORES = compute_scores(sub_x, test)\n\n\n\n\nINTER_HIDDEN_LAYERS = [512, 128, 32, 8]\nCONCAT_ABS_DIFF = True\n\nMAX_SEQ_LEN, BATCH_SIZE = 256, 512\nSTART, INTR_BATCH_SIZE = 0, 128\n\nSTART = 0\nSKIP = SKIP // 4\nINTR_BATCH_SIZE = INTR_BATCH_SIZE // 4\n\nBACKBONE_WT = 'backbone_loss20_acc100_trloss21.h5'\nINTR_WT = 'inter_model_loss20_acc100_trloss21.h5'\n\nWEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\nBACKBONE = 'roberta_large'\n\nwith tf.device('/device:GPU:0'): \n    tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n    backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n    sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n    scorer = build_interaction_model()\n    scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n\nprint('MODEL10_SCORES_LB862: \\n')    \nMODEL10_SCORES_LB862 = compute_scores(sub_x, test)\nMODEL10_SCORES_LB862 = MODEL10_SCORES_LB862 / 4","metadata":{"execution":{"iopub.status.busy":"2022-02-07T19:32:16.2277Z","iopub.execute_input":"2022-02-07T19:32:16.227882Z","iopub.status.idle":"2022-02-07T20:16:50.472126Z","shell.execute_reply.started":"2022-02-07T19:32:16.227859Z","shell.execute_reply":"2022-02-07T20:16:50.471065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SCORES = [\n#     MODEL1_LB863_SCORES, MODEL2_LB872_SCORES, MODEL3_SCORES, MODEL5_SCORES, \n#     MODEL4_LB852_SCORES, MODEL6_SCORES, MODEL7_SCORES, MODEL8_SCORES, MODEL9_SCORES, \n#     MODEL10_SCORES_LB862, MODEL11_SCORES, MODEL12_SCORES, \n# ]\n# less_toxic_comments = valid.less_toxic.values\n# more_toxic_comments = valid.more_toxic.values\n# test_comments = test.comment_text.values\n\n# BEST_ACC = 0\n# for i in tqdm(range(300000)): \n#     weights = [random.random()-0.01*random.random() for _ in SCORES]\n#     final_score = np.array([score*wt for score, wt in zip(SCORES, weights)]).sum(axis=0)\n#     comment_to_score = {test_comment: score for test_comment, score in zip(test_comments, final_score)}\n    \n#     correct, wrong = 0, 0\n#     for less_toxic, more_toxic in zip(less_toxic_comments, more_toxic_comments): \n#         if comment_to_score[more_toxic] > comment_to_score[less_toxic]: \n#             correct += 1\n#         else: \n#             wrong += 1\n#     acc = correct / (correct+wrong)\n#     if acc > BEST_ACC: \n#         BEST_ACC = acc\n#         print(f'new best accuracy ({i}): ', BEST_ACC)\n#         print('weights: ', weights)\n#         print('*'*100)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:16:50.47376Z","iopub.execute_input":"2022-02-07T20:16:50.474094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 6: Good at recognizing low toxicity comments\n# Model 9: Very Very bad at low toxicity comments\n# Model 10: Horrible predictions but somehow decent score of 862?\n\ntest.score = \\\n0.05064488852494361 * MODEL1_LB863_SCORES + \\\n0.9130546627400447 * MODEL2_LB872_SCORES + \\\n0.0034512375086494518 * MODEL3_SCORES + \\\n0.18933604708602006 * MODEL5_SCORES + \\\n0.4050676922435245 * MODEL4_LB852_SCORES + \\\n0.04471655339978278 * MODEL6_SCORES + \\\n0.03217086659636254 * MODEL7_SCORES + \\\n0.04340479591574183 * MODEL8_SCORES + \\\n0.132781900698542 * MODEL9_SCORES + \\\n0.2518645874220956 * MODEL10_SCORES_LB862 + \\\n0.32597758670120247 * MODEL11_SCORES + \\\n0.4428370339374257 * MODEL12_SCORES\n\n\ntest.score = test.score.rank(method='first')\ntest[['comment_id', 'score']].to_csv('/kaggle/working/submission.csv', index=False)\n\ndisplay(test.sort_values(by='score'))\ndisplay(test.sort_values(by='tfidf_score'))\ndisplay(test.sample(frac=1.))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LB: 873\n# test.score = \\\n# MODEL_1_SCORES + \\\n# 4*MODEL_2_SCORES + \\\n# 2*MODEL_3_SCORES + \\\n# MODEL_5_SCORES/2 + \\\n# MODEL_6_SCORES/2 + \\\n# MODEL_7_SCORES + \\\n# MODEL_8_SCORES_LB844/4 + \\\n# MODEL_9_SCORES/4\n\n# INTER_HIDDEN_LAYERS = [1024, 256, 64, 16, 4]\n# CONCAT_ABS_DIFF = True\n\n# MAX_SEQ_LEN, BATCH_SIZE = 256, 512\n# START, INTR_BATCH_SIZE = 4, 256\n\n# BACKBONE_WT = 'backboneseq256_loss15_acc78.h5'\n# INTR_WT = 'inter_model_loss15_acc78.h5'\n\n# WEIGHTS_DIR = Path('../input/toxic-internet-ensemble-model-weights')\n# BACKBONE = 'roberta_large'\n\n# with tf.device('/device:GPU:0'): \n#     tokenizer, backbone = load_tokenizer_and_backbone(BACKBONE)\n#     backbone.load_weights(WEIGHTS_DIR/BACKBONE_WT)\n#     sub_x = get_pooler_output(backbone, df_to_test_ds(test))\n#     scorer = build_interaction_model()\n#     scorer.load_weights(WEIGHTS_DIR/INTR_WT)\n    \n# MODEL8_SCORES_LB844 = compute_scores(sub_x, test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}