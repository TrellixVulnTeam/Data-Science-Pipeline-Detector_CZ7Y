{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ‚ò£Ô∏è Jigsaw - Early Ensemble\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n\n# If it is interesting, you like it, or you fork...\n# üôåüôè Please, _DO_ upvote !! üôèüôå\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n---\n\n\n## Simple weighted sum of the following 7 public models:\n\nIt got a much higher LB than I was expecting. It might be overfitting badly so... be aware of that...\n\n|Number |Model| Author/s| LB |\n|--|--|--|--:|\n|1 |[[0.816] Jigsaw Inference](https://www.kaggle.com/debarshichanda/0-816-jigsaw-inference)| [Debarshi Chanda](https://www.kaggle.com/debarshichanda)|`0.816`|\n|2 |[JRSoTC - RidgeRegression (ensemble of 3)](https://www.kaggle.com/adityasharma01/jrsotc-ridgeregression-ensemble-of-3)|[steubk](https://www.kaggle.com/steubk) / [Aditya Sharma](https://www.kaggle.com/adityasharma01/) |`0.825`|\n|3 | [Pytorch RoBERTa Ranking Baseline JRSTC [Infer]](https://www.kaggle.com/manabendrarout/pytorch-roberta-ranking-baseline-jrstc-infer) | [Manav](https://www.kaggle.com/manabendrarout)|`0.807` |\n|4 | [JRSTC \\| INFER \\| LB : 0.806 üéÉ](https://www.kaggle.com/kishalmandal/jrstc-infer-lb-0-806)|[Kishal](https://www.kaggle.com/kishalmandal)|`0.806`|\n|5 |[‚ò£Ô∏è Jigsaw - ü§ó HF hub out-of-the-box models](https://www.kaggle.com/julian3833/jigsaw-hf-hub-out-of-the-box-models)|[dataista0 (Juli√°n Peller)](https://www.kaggle.com/julian3833/) |`0.782`|\n|6 |[‚ò£Ô∏è Jigsaw - Incredibly Simple Naive Bayes [0.768]](https://www.kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768)|[dataista0 (Juli√°n Peller)](https://www.kaggle.com/julian3833/)|`0.768`|\n|7 |[*#&@ the Benchmark [0.81+] - TFIDF - Ridge](https://www.kaggle.com/samarthagarwal23/the-benchmark-0-81-tfidf-ridge)|[Samarth Agarwal](https://www.kaggle.com/samarthagarwal23) |`0.812`|\n\n---\n\n## Changelog\n\n|Best|Version | Description | LB |\n|--|-- | -- | --: |\n||V1 | Model 1 + Model 2. Linear weighting of MinMax-ed predictions | `0.823` |\n||[**V2**](https://www.kaggle.com/julian3833/jigsaw-early-ensemble-lb-0-829?scriptVersionId=80257580) | Model 1 + Model 2. Rank as sum of ranks | `0.828` |\n|| [**V4**](https://www.kaggle.com/julian3833/jigsaw-early-ensemble-lb-0-829?scriptVersionId=80260646) | Model 1 + 0.5 * Model 2. Combination of ranks (as V2) | `0.825` |\n|| [**V5**](https://www.kaggle.com/julian3833/jigsaw-early-ensemble-lb-0-829?scriptVersionId=80272370) | 7 models. Weighted combination of ranks. Weights=`[1, 1, 1, 1, 0.5, 0.5, 1]` | `0.829` |\n|_Best_| [**V7**](https://www.kaggle.com/julian3833/jigsaw-early-ensemble-lb-0-836?scriptVersionId=80279395) | 7 models (as V5). Weights=`[1.25, 1.5, 1.25, 1, 0.25, 0.25, 1]` | `0.836` |\n|_Best_| [**V8**](https://www.kaggle.com/julian3833/jigsaw-early-ensemble-lb-0-836?scriptVersionId=80279726) / [**Current**](https://www.kaggle.com/julian3833/jigsaw-early-ensemble-lb-0-836) | w=`[0.39, 0.39, 0.06, 0.06, 0.02, 0.02, 0.06]` | `0.836` |\n","metadata":{}},{"cell_type":"markdown","source":"# Model 1: [[0.816] Jigsaw Inference](https://www.kaggle.com/debarshichanda/0-816-jigsaw-inference)\n`LB=0.816`","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel\n\n# Utils\nfrom tqdm import tqdm\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nCONFIG = dict(\n    seed = 42,\n    model_name = '../input/roberta-base',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\nMODEL_PATHS = [\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-0.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-1.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-2.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-3.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-4.bin'\n]\n\ndef set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    \nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']        \n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }    \n\n    \nclass JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs\n    \n@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS\n\n\ndef inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JigsawModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds\n\n\nset_seed(CONFIG['seed'])\ndf = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf.head()\n\ntest_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\npreds1 = inference(MODEL_PATHS, test_loader, CONFIG['device'])","metadata":{"execution":{"iopub.status.busy":"2021-11-20T01:30:32.665364Z","iopub.execute_input":"2021-11-20T01:30:32.665661Z","iopub.status.idle":"2021-11-20T01:34:09.510304Z","shell.execute_reply.started":"2021-11-20T01:30:32.665585Z","shell.execute_reply":"2021-11-20T01:34:09.509496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds1[:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T01:34:09.512193Z","iopub.execute_input":"2021-11-20T01:34:09.512752Z","iopub.status.idle":"2021-11-20T01:34:09.520474Z","shell.execute_reply.started":"2021-11-20T01:34:09.512692Z","shell.execute_reply":"2021-11-20T01:34:09.519753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 2: [JRSoTC - RidgeRegression (ensemble of 3)](https://www.kaggle.com/adityasharma01/jrsotc-ridgeregression-ensemble-of-3)\n`LB=0.825`","metadata":{}},{"cell_type":"code","source":"%%time\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import rankdata\n\ndef ridge_cv (vec, X, y, X_test, folds, stratified ):\n    kf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=12)\n    val_scores = []\n    rmse_scores = []\n    X_less_toxics = []\n    X_more_toxics = []\n\n    preds = []\n    for fold, (train_index,val_index) in enumerate(kf.split(X,stratified)):\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]\n        model = Ridge()\n        model.fit(X_train, y_train)\n\n        rmse_score = mean_squared_error ( model.predict (X_val), y_val, squared = False) \n        rmse_scores.append (rmse_score)\n\n        X_less_toxic = vec.transform(df_val['less_toxic'])\n        X_more_toxic = vec.transform(df_val['more_toxic'])\n\n        p1 = model.predict(X_less_toxic)\n        p2 = model.predict(X_more_toxic)\n\n        X_less_toxics.append ( p1 )\n        X_more_toxics.append ( p2 )\n\n        # Validation Accuracy\n        val_acc = (p1< p2).mean()\n        val_scores.append(val_acc)\n\n        pred = model.predict (X_test)\n        preds.append (pred)\n\n        print(f\"FOLD:{fold}, rmse_fold:{rmse_score:.5f}, val_acc:{val_acc:.5f}\")\n\n    mean_val_acc = np.mean (val_scores)\n    mean_rmse_score = np.mean (rmse_scores)\n\n    p1 = np.mean ( np.vstack(X_less_toxics), axis=0 )\n    p2 = np.mean ( np.vstack(X_more_toxics), axis=0 )\n\n    val_acc = (p1< p2).mean()\n\n    print(f\"OOF: val_acc:{val_acc:.5f}, mean val_acc:{mean_val_acc:.5f}, mean rmse_score:{mean_rmse_score:.5f}\")\n    \n    preds = np.mean ( np.vstack(preds), axis=0 )\n    \n    return p1, p2, preds\n\ntoxic = 1.0\nsevere_toxic = 2.0\nobscene = 1.0\nthreat = 1.0\ninsult = 1.0\nidentity_hate = 2.0\n\ndef create_train (df):\n    df['y'] = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].max(axis=1)\n    df['y'] = df[\"y\"]+df['severe_toxic']*severe_toxic\n    df['y'] = df[\"y\"]+df['obscene']*obscene\n    df['y'] = df[\"y\"]+df['threat']*threat\n    df['y'] = df[\"y\"]+df['insult']*insult\n    df['y'] = df[\"y\"]+df['identity_hate']*identity_hate\n    \n    \n    \n    df = df[['comment_text', 'y', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].rename(columns={'comment_text': 'text'})\n\n    #undersample non toxic comments  on Toxic Comment Classification Challenge\n    min_len = (df['y'] >= 1).sum()\n    df_y0_undersample = df[df['y'] == 0].sample(n=int(min_len*1.5),random_state=201)\n    df = pd.concat([df[df['y'] >= 1], df_y0_undersample])\n                                                \n    return df\n \n\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ndf_test = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\n\n\n\njc_train_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\nprint(f\"jc_train_df:{jc_train_df.shape}\")\n\njc_train_df = create_train(jc_train_df)\n                           \ndf = jc_train_df\nprint(df['y'].value_counts())\n\n\nFOLDS = 7\n\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6) )\nX = vec.fit_transform(df['text'])\ny = df[\"y\"].values\nX_test = vec.transform(df_test['text'])\n\nstratified = np.around ( y )\njc_p1, jc_p2, jc_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )\n\n\njuc_train_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\nprint(f\"juc_train_df:{juc_train_df.shape}\")\njuc_train_df = juc_train_df.query (\"toxicity_annotator_count > 5\")\nprint(f\"juc_train_df:{juc_train_df.shape}\")\n\njuc_train_df['y'] = juc_train_df[[ 'severe_toxicity', 'obscene', 'sexual_explicit','identity_attack', 'insult', 'threat']].sum(axis=1)\n\njuc_train_df['y'] = juc_train_df.apply(lambda row: row[\"target\"] if row[\"target\"] <= 0.5 else row[\"y\"] , axis=1)\njuc_train_df = juc_train_df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\nmin_len = (juc_train_df['y'] > 0.5).sum()\ndf_y0_undersample = juc_train_df[juc_train_df['y'] <= 0.5].sample(n=int(min_len*1.5),random_state=201)\njuc_train_df = pd.concat([juc_train_df[juc_train_df['y'] > 0.5], df_y0_undersample])\n\ndf = juc_train_df\nprint(df['y'].value_counts())\n\nFOLDS = 7\n\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6) )\nX = vec.fit_transform(df['text'])\ny = df[\"y\"].values\nX_test = vec.transform(df_test['text'])\n\nstratified = (np.around ( y, decimals = 1  )*10).astype(int)\njuc_p1, juc_p2, juc_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )\n\n\n\n\n\nrud_df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\nprint(f\"rud_df:{rud_df.shape}\")\nrud_df['y'] = rud_df['offensiveness_score'].map(lambda x: 0.0 if x <=0 else x)\nrud_df = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\nmin_len = (rud_df['y'] < 0.5).sum()\nprint(rud_df['y'].value_counts())\n\nFOLDS = 7\ndf = rud_df\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6) )\nX = vec.fit_transform(df['text'])\ny = df[\"y\"].values\nX_test = vec.transform(df_test['text'])\n\nstratified = (np.around ( y, decimals = 1  )*10).astype(int)\nrud_p1, rud_p2, rud_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )\n\njc_max = max(jc_p1.max() , jc_p2.max())\njuc_max = max(juc_p1.max() , juc_p2.max())\nrud_max = max(rud_p1.max() , rud_p2.max())\n\n\np1 = jc_p1/jc_max + juc_p1/juc_max + rud_p1/rud_max\np2 = jc_p2/jc_max + juc_p2/juc_max + rud_p2/rud_max\n\nval_acc = (p1< p2).mean()\nprint(f\"Ensemble: val_acc:{val_acc:.5f}\")\n\npreds2 = jc_preds/jc_max + juc_preds/juc_max + rud_preds/rud_max  ","metadata":{"execution":{"iopub.status.busy":"2021-11-20T01:34:37.721405Z","iopub.execute_input":"2021-11-20T01:34:37.721658Z","iopub.status.idle":"2021-11-20T01:57:11.017198Z","shell.execute_reply.started":"2021-11-20T01:34:37.721629Z","shell.execute_reply":"2021-11-20T01:57:11.016394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds2[:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T01:57:11.018831Z","iopub.execute_input":"2021-11-20T01:57:11.019158Z","iopub.status.idle":"2021-11-20T01:57:11.025772Z","shell.execute_reply.started":"2021-11-20T01:57:11.019119Z","shell.execute_reply":"2021-11-20T01:57:11.024962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 3: [Pytorch RoBERTa Ranking Baseline JRSTC [Infer]](https://www.kaggle.com/manabendrarout/pytorch-roberta-ranking-baseline-jrstc-infer)\n`LB=0.807`","metadata":{}},{"cell_type":"code","source":"%%time\n# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport random\nimport gc\nimport glob\npd.set_option('display.max_columns', None)\nnp.seterr(divide='ignore', invalid='ignore')\ngc.enable()\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import OneCycleLR\n# NLP\nfrom transformers import AutoTokenizer, AutoModel\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')\n\ndata_dir = '../input/jigsaw-toxic-severity-rating'\nmodels_dir = '../input/jrstc-models/roberta_base'\ntest_file_path = os.path.join(data_dir, 'comments_to_score.csv')\nprint(f'Train file: {test_file_path}')\n\ntest_df = pd.read_csv(test_file_path)\n\n# Text Cleaning\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\ntqdm.pandas()\ntest_df['text'] = test_df['text'].progress_apply(text_cleaning)\n\ntest_df.sample(10)\n\n# CFG\n\nparams = {\n    'device': device,\n    'debug': False,\n    'checkpoint': '../input/roberta-base',\n    'output_logits': 768,\n    'max_len': 256,\n    'batch_size': 32,\n    'dropout': 0.2,\n    'num_workers': 2\n}\n\nif params['debug']:\n    train_df = train_df.sample(frac=0.01)\n    print('Reduced training Data Size for Debugging purposes')\n\n# Dataset\n\nclass BERTDataset:\n    def __init__(self, text, max_len=params['max_len'], checkpoint=params['checkpoint']):\n        self.text = text\n        self.max_len = max_len\n        self.checkpoint = checkpoint\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.num_examples = len(self.text)\n\n    def __len__(self):\n        return self.num_examples\n\n    def __getitem__(self, idx):\n        text = str(self.text[idx])\n\n        tokenized_text = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n\n        ids = tokenized_text['input_ids']\n        mask = tokenized_text['attention_mask']\n        token_type_ids = tokenized_text['token_type_ids']\n\n        return {'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)}\n\n# NLP Model\n\nclass ToxicityModel(nn.Module):\n    def __init__(self, checkpoint=params['checkpoint'], params=params):\n        super(ToxicityModel, self).__init__()\n        self.checkpoint = checkpoint\n        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.layer_norm = nn.LayerNorm(params['output_logits'])\n        self.dropout = nn.Dropout(params['dropout'])\n        self.dense = nn.Sequential(\n            nn.Linear(params['output_logits'], 256),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.Dropout(params['dropout']),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        _, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n        pooled_output = self.layer_norm(pooled_output)\n        pooled_output = self.dropout(pooled_output)\n        preds = self.dense(pooled_output)\n        return preds\n\n# Prediction\n\npredictions_nn = None\nfor model_name in glob.glob(models_dir + '/*.pth'):\n    model = ToxicityModel()\n    model.load_state_dict(torch.load(model_name))\n    model = model.to(params['device'])\n    model.eval()\n\n    test_dataset = BERTDataset(\n        text = test_df['text'].values\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n    )\n\n    temp_preds = None\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=f'Predicting. '):\n            ids= batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            predictions = model(ids, token_type_ids, mask).to('cpu').numpy()\n            \n            if temp_preds is None:\n                temp_preds = predictions\n            else:\n                temp_preds = np.vstack((temp_preds, predictions))\n\n    if predictions_nn is None:\n        predictions_nn = temp_preds\n    else:\n        predictions_nn += temp_preds\n        \npredictions_nn /= (len(glob.glob(models_dir + '/*.pth')))\n\npreds3 = predictions_nn\npreds3 = preds3.squeeze(-1)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:16:23.458838Z","iopub.execute_input":"2021-11-20T02:16:23.459163Z","iopub.status.idle":"2021-11-20T02:22:49.933632Z","shell.execute_reply.started":"2021-11-20T02:16:23.459126Z","shell.execute_reply":"2021-11-20T02:22:49.932721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds3[:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:24:46.966237Z","iopub.execute_input":"2021-11-20T02:24:46.966547Z","iopub.status.idle":"2021-11-20T02:24:46.973398Z","shell.execute_reply.started":"2021-11-20T02:24:46.966516Z","shell.execute_reply":"2021-11-20T02:24:46.972591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 4: [JRSTC | INFER | LB : 0.806 üéÉ](https://www.kaggle.com/kishalmandal/jrstc-infer-lb-0-806)\n`LB=0.806`","metadata":{}},{"cell_type":"code","source":"%%time\nimport os\nimport gc\nimport copy\nimport time\nimport random\nimport string\n\nimport nltk\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nimport re\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\n\nclass Config:\n    num_classes=1\n    epochs=10\n    margin=0.5\n    model_name = '../input/robertalarge'\n    batch_size = 8\n    lr = 1e-4\n    weight_decay=0.01\n    scheduler = 'CosineAnnealingLR'\n    max_length = 196\n    accumulation_step = 1\n    patience = 1\n\nclass ToxicDataset(Dataset):\n    def __init__(self, comments, tokenizer, max_length):\n        self.comment = comments\n        self.tokenizer = tokenizer\n        self.max_len = max_length\n        \n    def __len__(self):\n        return len(self.comment)\n    \n    def __getitem__(self, idx):\n\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                self.comment[idx],\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n\n        \n        input_ids = inputs_more_toxic['input_ids']\n        attention_mask = inputs_more_toxic['attention_mask']\n\n       \n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n        }\n       \n\nclass ToxicModel(nn.Module):\n    def __init__(self, model_name, args):\n        super(ToxicModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.output = nn.Linear(1024, self.args.num_classes)\n    \n        \n    def forward(self, toxic_ids, toxic_mask):\n        \n        out = self.model(\n            input_ids=toxic_ids,\n            attention_mask=toxic_mask,\n            output_hidden_states=False\n        )\n        \n        out = self.dropout(out[1])\n        outputs = self.output(out)\n\n        return outputs\n        \n\ndef get_predictions(model, dataloader):\n    model.eval()\n    \n    PREDS=[]\n    with torch.no_grad():\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:        \n            input_ids = data['input_ids'].cuda()\n            attention_mask = data['attention_mask'].cuda()\n\n            outputs = model(input_ids, attention_mask)\n\n            PREDS.append(outputs.view(-1).cpu().detach().numpy())\n\n            bar.set_postfix(Stage='Inference')  \n        \n        PREDS = np.hstack(PREDS)\n        gc.collect()\n\n        return PREDS\n\ndf = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\n\nargs = Config()\n\ntokenizer = AutoTokenizer.from_pretrained(args.model_name)\n\ndef washing_machine(comments):\n    corpus=[]\n    for i in tqdm(range(len(comments))):\n        comment = re.sub('[^a-zA-Z]', ' ', comments[i])\n        comment = comment.lower()\n        comment = comment.split()\n        stemmer = SnowballStemmer('english')\n        lemmatizer = WordNetLemmatizer()\n        all_stopwords = stopwords.words('english')\n        comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n        comment = [lemmatizer.lemmatize(word) for word in comment]\n        comment = ' '.join(comment)\n        corpus.append(comment)\n\n    return corpus\n\n\n\n\ndef inference(dataloader):\n    final_preds = []\n    args = Config()\n    base_path='../input/large-jigsaw-kishal-v1/'\n    for fold in range(1):\n        model = ToxicModel(args.model_name, args)\n        model = model.cuda()\n        path = base_path + f'model_fold_0.bin'\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {fold+1}\")\n        preds = get_predictions(model, dataloader)\n        final_preds.append(preds)\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds\n\n# Prediction and submission\n\nsub = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n\nsub.head(1)\n\nsub_dataset = ToxicDataset(washing_machine(sub['text'].values), tokenizer, max_length=args.max_length)\nsub_loader = DataLoader(sub_dataset, batch_size=2*args.batch_size,\n                        num_workers=2, shuffle=False, pin_memory=True)\n\npreds4 = inference(sub_loader)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:44:19.214482Z","iopub.execute_input":"2021-11-20T02:44:19.214771Z","iopub.status.idle":"2021-11-20T02:47:45.151545Z","shell.execute_reply.started":"2021-11-20T02:44:19.214736Z","shell.execute_reply":"2021-11-20T02:47:45.150654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds4[:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:47:45.154372Z","iopub.execute_input":"2021-11-20T02:47:45.154595Z","iopub.status.idle":"2021-11-20T02:47:45.160838Z","shell.execute_reply.started":"2021-11-20T02:47:45.154564Z","shell.execute_reply":"2021-11-20T02:47:45.159977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 5: [‚ò£Ô∏è Jigsaw - ü§ó HF hub out-of-the-box models](https://www.kaggle.com/julian3833/jigsaw-hf-hub-out-of-the-box-models)\n`LB=0.782`","metadata":{}},{"cell_type":"code","source":"%%time\nimport os; os.environ['TOKENIZERS_PARALLELISM'] = 'false'\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nclass Dataset:\n    \"\"\"\n    For comments_to_score.csv (the submission), get only one comment per row\n    \"\"\"\n    def __init__(self, text, tokenizer, max_len):\n        self.text = text\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long)\n        }\n    \ndef generate_predictions(model_path, max_len, is_multioutput):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    \n    dataset = Dataset(text=df.text.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=2, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n\n    for data in data_loader:\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output = model(**data)\n            \n            if is_multioutput:\n                # Sum the logits for all the toxic labels\n                # One strategy out of various possible\n                output = output.logits.sum(dim=1)\n            else:\n                # Classifier. Get logits for \"toxic\"\n                output = output.logits[:, 1]\n            \n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n    \n    torch.cuda.empty_cache()\n    return np.array(final_output)\n\npreds_bert = generate_predictions(\"../input/toxic-bert\", max_len=192, is_multioutput=True)\npreds_rob1 = generate_predictions(\"../input/roberta-base-toxicity\", max_len=192, is_multioutput=False)\npreds_rob2 = generate_predictions(\"../input/roberta-toxicity-classifier\", max_len=192, is_multioutput=False)\n\n\n\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf_sub[\"score_bert\"] = preds_bert\ndf_sub[\"score_rob1\"] = preds_rob1\ndf_sub[\"score_rob2\"] = preds_rob2\ndf_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]] = MinMaxScaler().fit_transform(df_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]])\n\npreds5 = df_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]].sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:27:29.895797Z","iopub.execute_input":"2021-11-20T02:27:29.896244Z","iopub.status.idle":"2021-11-20T02:30:45.613838Z","shell.execute_reply.started":"2021-11-20T02:27:29.896206Z","shell.execute_reply":"2021-11-20T02:30:45.612954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds5[:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:30:45.616158Z","iopub.execute_input":"2021-11-20T02:30:45.616449Z","iopub.status.idle":"2021-11-20T02:30:45.625255Z","shell.execute_reply.started":"2021-11-20T02:30:45.616409Z","shell.execute_reply":"2021-11-20T02:30:45.624289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 6: [‚ò£Ô∏è Jigsaw - Incredibly Simple Naive Bayes [0.768]](https://www.kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768)\n`LB=0.768`","metadata":{}},{"cell_type":"code","source":"%%time\nimport pandas as pd\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndf = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\nmin_len = (df['y'] == 1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\ndf = pd.concat([df[df['y'] == 1], df_y0_undersample])\nvec = TfidfVectorizer()\nX = vec.fit_transform(df['text'])\nmodel = MultinomialNB()\nmodel.fit(X, df['y'])\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nX_test = vec.transform(df_sub['text'])\npreds6 = model.predict_proba(X_test)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:31:58.428134Z","iopub.execute_input":"2021-11-20T02:31:58.42848Z","iopub.status.idle":"2021-11-20T02:32:14.727665Z","shell.execute_reply.started":"2021-11-20T02:31:58.428442Z","shell.execute_reply":"2021-11-20T02:32:14.726876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds6[:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:32:14.729358Z","iopub.execute_input":"2021-11-20T02:32:14.730254Z","iopub.status.idle":"2021-11-20T02:32:14.737974Z","shell.execute_reply.started":"2021-11-20T02:32:14.730196Z","shell.execute_reply":"2021-11-20T02:32:14.737067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 7: [*#&@ the Benchmark [0.81+] - TFIDF - Ridge](https://www.kaggle.com/samarthagarwal23/the-benchmark-0-81-tfidf-ridge)\n`LB=0.812`","metadata":{}},{"cell_type":"code","source":"%%time\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\nimport scipy\npd.options.display.max_colwidth=300\n\ndf = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n\n# Give more weight to severe toxic \ndf['severe_toxic'] = df.severe_toxic * 2\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n\n\ndf = pd.concat([df[df.y>0] , \n                df[df.y==0].sample(int(len(df[df.y>0])*1.5)) ], axis=0).sample(frac=1)\n\npipeline = Pipeline(\n    [\n        (\"vect\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        (\"clf\", Ridge()),\n    ]\n)\npipeline.fit(df['text'], df['y'])\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\npreds7 = pipeline.predict(df_sub['text'])\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:38:52.52995Z","iopub.execute_input":"2021-11-20T02:38:52.530638Z","iopub.status.idle":"2021-11-20T02:39:37.182781Z","shell.execute_reply.started":"2021-11-20T02:38:52.530583Z","shell.execute_reply":"2021-11-20T02:39:37.181907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds7[:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:40:27.161478Z","iopub.execute_input":"2021-11-20T02:40:27.16176Z","iopub.status.idle":"2021-11-20T02:40:27.167954Z","shell.execute_reply.started":"2021-11-20T02:40:27.161724Z","shell.execute_reply":"2021-11-20T02:40:27.167171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf['model1'] = preds1\ndf['model2'] = preds2\ndf['model3'] = preds3\ndf['model4'] = preds4\ndf['model5'] = preds5\ndf['model6'] = preds6\ndf['model7'] = preds7\n\ncols = [c for c in df.columns if c.startswith('model')]\n\n# Put all predictions in the same scale. \n# Make all the distances between predictions uniform\n#df[cols] = df[cols].rank(method='first').astype(int)\n# Make all the distances not uniform\ndf[cols] = MinMaxScaler().fit_transform(df[cols])\n\n# Weights of each model\nweights = {\n    'model1': 0.44, # 0.816\n    'model2': 0.45, # 0.825\n    'model3': 0.05, # 0.807\n    'model4': 0.045, # 0.806\n    'model5': 0.005, # 0.782\n    'model6': 0.005, # 0.768\n    'model7': 0.005  # 0.812\n}\n\n# A weighted sum determines the final position\n# It is the same as an average in the end\ndf['score'] = pd.DataFrame([df[c] * weights[c] for c in cols]).T.sum(axis=1).rank(method='first').astype(int)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:49:35.976923Z","iopub.execute_input":"2021-11-20T02:49:35.977417Z","iopub.status.idle":"2021-11-20T02:49:36.143935Z","shell.execute_reply.started":"2021-11-20T02:49:35.977379Z","shell.execute_reply":"2021-11-20T02:49:36.143186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sort_values(\"score\", ascending=True).head(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:53:38.319374Z","iopub.execute_input":"2021-11-20T02:53:38.319635Z","iopub.status.idle":"2021-11-20T02:53:38.334062Z","shell.execute_reply.started":"2021-11-20T02:53:38.319605Z","shell.execute_reply":"2021-11-20T02:53:38.333178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sort_values(\"score\", ascending=True).tail(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:53:43.422141Z","iopub.execute_input":"2021-11-20T02:53:43.422426Z","iopub.status.idle":"2021-11-20T02:53:43.439928Z","shell.execute_reply.started":"2021-11-20T02:53:43.422395Z","shell.execute_reply":"2021-11-20T02:53:43.439242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sort_values(\"score\", ascending=True).sample(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:53:57.58985Z","iopub.execute_input":"2021-11-20T02:53:57.590333Z","iopub.status.idle":"2021-11-20T02:53:57.60736Z","shell.execute_reply.started":"2021-11-20T02:53:57.590293Z","shell.execute_reply":"2021-11-20T02:53:57.606678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Correlation","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.set(rc={'figure.figsize':(10,8)})\nsns.heatmap(df.drop(\"comment_id\", axis=1).corr(), annot=True, fmt='.2f', cmap='Greens')","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:54:22.536117Z","iopub.execute_input":"2021-11-20T02:54:22.536397Z","iopub.status.idle":"2021-11-20T02:54:23.191292Z","shell.execute_reply.started":"2021-11-20T02:54:22.536365Z","shell.execute_reply":"2021-11-20T02:54:23.190543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:54:25.562148Z","iopub.execute_input":"2021-11-20T02:54:25.563039Z","iopub.status.idle":"2021-11-20T02:54:25.575329Z","shell.execute_reply.started":"2021-11-20T02:54:25.562995Z","shell.execute_reply":"2021-11-20T02:54:25.573894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:54:30.94689Z","iopub.execute_input":"2021-11-20T02:54:30.947185Z","iopub.status.idle":"2021-11-20T02:54:30.972743Z","shell.execute_reply.started":"2021-11-20T02:54:30.947153Z","shell.execute_reply":"2021-11-20T02:54:30.972052Z"},"trusted":true},"execution_count":null,"outputs":[]}]}