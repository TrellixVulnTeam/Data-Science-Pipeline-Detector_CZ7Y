{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ☣️ Jigsaw - HuggingFace Hub Baselines\n\nIn this notebook I will explore, without fine-tuning, various models from the huggingface hub.\n\nI am bringing them to kaggle as datasets:\n\n* [toxic-bert](https://www.kaggle.com/julian3833/toxic-bert)\n* [roberta-base-toxicity](https://www.kaggle.com/julian3833/roberta-base-toxicity)\n* [roberta-toxicity-classifier](https://www.kaggle.com/julian3833/roberta-toxicity-classifier)\n\n\n\n\n\n|Version | Model | Validation (first 5000 samples) | LB |\n|---| ---   | ---: | --- |\n|[V1](https://www.kaggle.com/julian3833/jigsaw-huggingface-hub-baselines?scriptVersionId=79545636) | [toxic-bert](https://www.kaggle.com/julian3833/toxic-bert) | `0.71` | `0.758` |\n|[V2](https://www.kaggle.com/julian3833/jigsaw-huggingface-hub-baselines?scriptVersionId=79547125) | [roberta-base-toxicity](https://www.kaggle.com/julian3833/roberta-base-toxicity) | `0.66` | `0.751` |\n|[V3](https://www.kaggle.com/julian3833/jigsaw-huggingface-hub-baselines?scriptVersionId=79547879) | [roberta-toxicity-classifier](https://www.kaggle.com/julian3833/roberta-toxicity-classifier) | `0.69` | `0.768` |\n|[V4](https://www.kaggle.com/julian3833/jigsaw-huggingface-hub-baselines) | Ensemble of the previous 3 | `--` | `0.782` |\n\n\n# Please, _DO_ upvote if you find this useful or interesting!","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os; os.environ['TOKENIZERS_PARALLELISM'] = 'false'\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-11-13T05:08:09.252284Z","iopub.execute_input":"2021-11-13T05:08:09.252666Z","iopub.status.idle":"2021-11-13T05:08:15.699333Z","shell.execute_reply.started":"2021-11-13T05:08:09.252547Z","shell.execute_reply":"2021-11-13T05:08:15.698607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test and Validation Dataset","metadata":{}},{"cell_type":"code","source":"class Dataset:\n    \"\"\"\n    For comments_to_score.csv (the submission), get only one comment per row\n    \"\"\"\n    def __init__(self, text, tokenizer, max_len):\n        self.text = text\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long)\n        }\n    \n    \nclass ValidationDataset:\n    \"\"\"\n    Goes through validation_data.csv, Loading and tokenizing both less_toxic and more_toxic\n    \n    Inspired by: https://www.kaggle.com/debarshichanda/pytorch-w-b-jigsaw-starter\n    \"\"\"\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def tokenize(self, text):\n        return self.tokenizer(text, max_length=self.max_len, \n                              padding=\"max_length\", truncation=True)\n    \n    def __getitem__(self, i):\n        more_toxic = self.df['more_toxic'].iloc[i]\n        less_toxic = self.df['less_toxic'].iloc[i]\n        \n        less_inputs = self.tokenize(less_toxic)\n        more_inputs = self.tokenize(more_toxic)\n\n        return {\n            \"less_input_ids\": torch.tensor(less_inputs[\"input_ids\"], dtype=torch.long),\n            \"less_attention_mask\": torch.tensor(less_inputs[\"attention_mask\"], dtype=torch.long),\n            \"more_input_ids\": torch.tensor(more_inputs[\"input_ids\"], dtype=torch.long),\n            \"more_attention_mask\": torch.tensor(more_inputs[\"attention_mask\"], dtype=torch.long),\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-13T05:08:15.707285Z","iopub.execute_input":"2021-11-13T05:08:15.70767Z","iopub.status.idle":"2021-11-13T05:08:15.723453Z","shell.execute_reply.started":"2021-11-13T05:08:15.707632Z","shell.execute_reply":"2021-11-13T05:08:15.722657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation\n","metadata":{}},{"cell_type":"code","source":"def validate(model_path, max_len, is_multioutput):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\", nrows=VALIDATION_SIZE)\n    \n    dataset = ValidationDataset(df=df, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=64, num_workers=2, pin_memory=True, shuffle=False\n    )\n\n    n_samples = len(dataset)\n    hits = 0\n    \n    for data in data_loader:\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n                \n            less_output = model(input_ids=data['less_input_ids'], \n                                attention_mask=data['less_attention_mask'])\n            \n            more_output = model(input_ids=data['more_input_ids'], \n                                attention_mask=data['more_attention_mask'])\n            \n            if is_multioutput:\n                # Sum the logits of the 6 toxic labels\n                less_score = less_output.logits.sum(dim=1)\n                more_score = more_output.logits.sum(dim=1)\n                hits += (less_score < more_score).sum().item()\n            else:\n                less_score = less_output.logits[:, 1]\n                more_score = more_output.logits[:, 1]\n                hits += (less_score < more_score).sum().item()\n            \n            \n    \n    \n    accuracy = hits / n_samples\n    print(f\"Validation Accuracy: {accuracy:4.2f}\")\n    \n    torch.cuda.empty_cache()\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2021-11-13T05:08:15.725937Z","iopub.execute_input":"2021-11-13T05:08:15.726277Z","iopub.status.idle":"2021-11-13T05:08:15.739317Z","shell.execute_reply.started":"2021-11-13T05:08:15.726241Z","shell.execute_reply":"2021-11-13T05:08:15.73845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validate(model_chk, max_length, is_multioutput)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T05:08:15.740744Z","iopub.execute_input":"2021-11-13T05:08:15.741244Z","iopub.status.idle":"2021-11-13T05:08:15.752018Z","shell.execute_reply.started":"2021-11-13T05:08:15.741162Z","shell.execute_reply":"2021-11-13T05:08:15.751282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\n\nAdapted from [AutoNLP for toxic ratings ;)](https://www.kaggle.com/abhishek/autonlp-for-toxic-ratings) by Abhishek.","metadata":{}},{"cell_type":"code","source":"def generate_predictions(model_path, max_len, is_multioutput):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    \n    dataset = Dataset(text=df.text.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=2, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n\n    for data in data_loader:\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output = model(**data)\n            \n            if is_multioutput:\n                # Sum the logits for all the toxic labels\n                # One strategy out of various possible\n                output = output.logits.sum(dim=1)\n            else:\n                # Classifier. Get logits for \"toxic\"\n                output = output.logits[:, 1]\n            \n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n    \n    torch.cuda.empty_cache()\n    return np.array(final_output)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T05:08:15.753795Z","iopub.execute_input":"2021-11-13T05:08:15.75426Z","iopub.status.idle":"2021-11-13T05:08:15.765118Z","shell.execute_reply.started":"2021-11-13T05:08:15.754223Z","shell.execute_reply":"2021-11-13T05:08:15.764207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds1 = generate_predictions(\"../input/toxic-bert\", max_len=192, is_multioutput=True)\npreds2 = generate_predictions(\"../input/roberta-base-toxicity\", max_len=192, is_multioutput=False)\npreds3 = generate_predictions(\"../input/roberta-toxicity-classifier\", max_len=192, is_multioutput=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble\n\nLinear ensemble of the three models.\n\n\nSince their scales are off I first MinMaxScale the results (per model), and then I sum the scores","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf_sub[\"score_bert\"] = preds1\ndf_sub[\"score_rob1\"] = preds2\ndf_sub[\"score_rob2\"] = preds3\n\ndf_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]] = MinMaxScaler().fit_transform(df_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]])\n\ndf_sub[\"score\"] = df_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]].sum(axis=1)\ndf_sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T05:11:01.605233Z","iopub.execute_input":"2021-11-13T05:11:01.605501Z","iopub.status.idle":"2021-11-13T05:11:02.15218Z","shell.execute_reply.started":"2021-11-13T05:11:01.605464Z","shell.execute_reply":"2021-11-13T05:11:02.151398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# View some results","metadata":{}},{"cell_type":"code","source":"pd.set_option(\"display.max_colwidth\", 500)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T05:11:02.153481Z","iopub.execute_input":"2021-11-13T05:11:02.153739Z","iopub.status.idle":"2021-11-13T05:11:02.159059Z","shell.execute_reply.started":"2021-11-13T05:11:02.153705Z","shell.execute_reply":"2021-11-13T05:11:02.157897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.sort_values(\"score\").head(3)[['score', 'text']]","metadata":{"execution":{"iopub.status.busy":"2021-11-13T05:11:02.161648Z","iopub.execute_input":"2021-11-13T05:11:02.162421Z","iopub.status.idle":"2021-11-13T05:11:02.182947Z","shell.execute_reply.started":"2021-11-13T05:11:02.162361Z","shell.execute_reply":"2021-11-13T05:11:02.182031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.sort_values(\"score\").tail(3)[['score', 'text']]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T05:11:02.184312Z","iopub.execute_input":"2021-11-13T05:11:02.184672Z","iopub.status.idle":"2021-11-13T05:11:02.19907Z","shell.execute_reply.started":"2021-11-13T05:11:02.184631Z","shell.execute_reply":"2021-11-13T05:11:02.197874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":" # Tie-break, if any\ndf_sub['score'] = df_sub['score'].rank(method='first')\n\ndf_sub = df_sub[[\"comment_id\", \"score\"]]\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T05:11:02.200507Z","iopub.execute_input":"2021-11-13T05:11:02.20102Z","iopub.status.idle":"2021-11-13T05:11:02.238691Z","shell.execute_reply.started":"2021-11-13T05:11:02.200979Z","shell.execute_reply":"2021-11-13T05:11:02.23782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Please, _DO_ upvote if you find this useful or interesting!","metadata":{}}]}