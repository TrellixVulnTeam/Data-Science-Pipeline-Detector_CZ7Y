{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport gc\nimport scipy\nimport numpy as np\nimport pandas as pd\nfrom copy import deepcopy\nfrom string import printable\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# Sklearn\nfrom sklearn.linear_model import Ridge, ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Transformers\nfrom transformers import BertTokenizer\nfrom transformers import TFBertModel, BertModel\n\n# Keras\nfrom keras import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.layers import Input, Dropout, Dense\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:36.156232Z","iopub.execute_input":"2021-12-01T13:11:36.156542Z","iopub.status.idle":"2021-12-01T13:11:38.939441Z","shell.execute_reply.started":"2021-12-01T13:11:36.156459Z","shell.execute_reply":"2021-12-01T13:11:38.938719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constants I'll be using in the notebook\nRANDOM_STATE = 201\nSTOPWORDS = set(STOPWORDS)\nBERT_DIR = '../input/huggingface-bert/bert-base-uncased'\n\nplt.style.use('ggplot')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:38.941361Z","iopub.execute_input":"2021-12-01T13:11:38.941893Z","iopub.status.idle":"2021-12-01T13:11:38.946758Z","shell.execute_reply.started":"2021-12-01T13:11:38.941854Z","shell.execute_reply":"2021-12-01T13:11:38.946113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n### What We Know So Far\nI've read some [Discussions](https://www.kaggle.com/c/jigsaw-toxic-severity-rating/discussion) and [Notebooks](https://www.kaggle.com/c/jigsaw-toxic-severity-rating/code) for this and previous competitions that helped me understand more about the problem I'm tackling. Here is what I know:\n- We don't have a proper validation set to use and must come up with a creative validation technique.\n- There is something off about the \"*validation_data.csv*\" as discussed [here](https://www.kaggle.com/c/jigsaw-toxic-severity-rating/discussion/287350) and [here](https://www.kaggle.com/c/jigsaw-toxic-severity-rating/discussion/287140) (we also notice that later on) which results in best validation score maxing out to **82.4%**! This is because it seems ***some comments are incorrectly tagged*** (annotator are humans after all) and ***the validation set contains duplicate (less_toxic, more_toxic) pairs*** (different annotator have different opinions).\n- There is no original training data for this competition and we should rely on the datasets mentioned in the competition overview and others out there.\n- The objective is to rank the given comments in \"*comments_to_score.csv*\" based on their toxicity.\n- Some of our validation data exist in the training data!\n\n### Strategy\nThe choice of data is important. For that matter I've decided to use [jigsaw-multilingual-toxic-comment-classification](https://www.kaggle.com/julian3833/jigsaw-multilingual-toxic-comment-classification) and [ruddit jigsaw dataset](https://www.kaggle.com/rajkumarl/ruddit-jigsaw-dataset). My primary dataset is the first one which has labeled comments with multiple toxicity types. How we choose to combine them into an overall toxicity level is extremely important.","metadata":{}},{"cell_type":"markdown","source":"# Utility Scripts\n## Weights\nHere I have defined a dictionary that will map toxicity types to their corresponding weights. These weights are one of the most important parameters in the entire notebook:","metadata":{}},{"cell_type":"code","source":"# Toxicity weights - These weights are later used to combine all toxicity types into one\ntoxicity_weights = {\n    'toxic': 1,\n    'severe_toxic': 2,\n    'obscene': 1,\n    'threat': 1,\n    'insult': 1,\n    'identity_hate': 2,\n    'sexual_explicit': 1\n}\n\ntoxicity_types = list(toxicity_weights.keys())","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:38.948087Z","iopub.execute_input":"2021-12-01T13:11:38.948345Z","iopub.status.idle":"2021-12-01T13:11:38.957969Z","shell.execute_reply.started":"2021-12-01T13:11:38.948311Z","shell.execute_reply":"2021-12-01T13:11:38.957312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text-Cleaning Methods\nAs newer language models and techniques come into play, text-cleaning is becoming less and less a necessity and more like an optional feature. But let's not forget that text-cleaning can still be of great importance in many models and scenarios. I have defined a number of functions that will help clean parts of our texts and have later on used a few I believed to be helpful.","metadata":{}},{"cell_type":"code","source":"HTML_TAG_PATTERN = r\"<.*?>\"\nEMAIL_PATTERN = r'(?:[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\\\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])'\nURL_PATTERN = r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n\ndef remove_html_tags(string: str, replace_with: str = '') -> str:\n    return re.sub(pattern = HTML_TAG_PATTERN, repl = replace_with, string = string)\n\ndef remove_special_characters(string: str) -> str:\n    return ''.join(filter(lambda x: x in printable, string))\n\ndef remove_urls(string: str, replace_with: str = '') -> str:\n    return re.sub(pattern = URL_PATTERN, repl = replace_with, string = string)\n\ndef remove_emails(string: str, replace_with: str = '') -> str:\n    return re.sub(EMAIL_PATTERN, replace_with, string)\n\ndef remove_IPs(text):\n    return re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', text)        # 71.228.77.211\n\ndef remove_times(text):\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{1,2} [a-zA-Z]+,? \\d{4} \\(UTC\\)', '', text)    # 04:09, 11 Jul, 2003  \n    text = re.sub(r'\\d{1,2}:\\d{2},? [a-zA-Z]+ \\d{1,2},? \\d{4} \\(UTC\\)', '', text)    # 16:47, Jul 23, 2004\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{4} [a-zA-Z]+ \\d{1,2},? \\(UTC\\)', '', text)    # 22:07, 2004 Dec 30\n    text = re.sub(r'\\d{1,2} [a-zA-Z]+ \\d{4},? \\d{1,2}:\\d{2} \\(UTC\\)', '', text)      # 29 June 2005 22:08\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{1,2} [a-zA-Z]+,?', '', text)                  # 21:31, 6 April\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{1,2},?', '', text)                            # 17:52, 12\n    text = re.sub(r'\\d{1,2}:\\d{1,2}-\\d{1,2}-\\d{1,2}', '', text)                      # 01:05-09-09    \n    text = re.sub(r'\\d{1,2}:\\d{2}', '', text)                                        # 17:52, 12\n    text = re.sub(r'\\d{1,2} [a-zA-Z]+,? \\d{4}', '', text)                            # 4 May, 2006\n    \n    \n    text = re.sub(r'\\(UTC\\)', '', text)                                              # (UTC)\n    return text\n\ndef shorten_consecutive_repetitions(text):\n    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n    \n    # Add space around repeated characters\n    text = re.sub(r'[ ]{2,}',' ', text).strip()\n    text = re.sub(r'([*!?]+)',r' \\1 ', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:38.961428Z","iopub.execute_input":"2021-12-01T13:11:38.961952Z","iopub.status.idle":"2021-12-01T13:11:38.974683Z","shell.execute_reply.started":"2021-12-01T13:11:38.961914Z","shell.execute_reply":"2021-12-01T13:11:38.973956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):            \n    \n    text = shorten_consecutive_repetitions(text)  # Remove consecutive repeated characters (threshold = 3)\n    text = remove_special_characters(text)        # Remove non-ascci characters\n    text = remove_html_tags(text)                 # Remove HTML tags (not their contents)\n    text = remove_emails(text)                    # Remove Email Addresses\n    text = remove_urls(text)                      # Remove URLs\n    text = remove_times(text)                     # Remove times (refer to the definition for more info)\n    text = remove_IPs(text)                       # Remove IP Addresses\n    \n    text = text.replace(':', ' : ')               # Add space before and after :\n    text = re.sub(r'[|=\\n\"_\\-/~]', ' ', text)           # Remove some stuff\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:38.975838Z","iopub.execute_input":"2021-12-01T13:11:38.976711Z","iopub.status.idle":"2021-12-01T13:11:38.98495Z","shell.execute_reply.started":"2021-12-01T13:11:38.976672Z","shell.execute_reply":"2021-12-01T13:11:38.984233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit, Validate and Predict\n#### Validation Strategy\nWe don't have the straight-forward validation data as we normally have, so we I will be using the \"*validation.csv*\" which has two columns: *less_toxic* and *more_toxic*. We do prediction on each column and compare them pair-wise to see how many we got correct. The average number is our validation score.\n\nI will also be using *RMSE* along side *Accuracy*. This is because *Accuracy* can be a little misleading here due to the fact that our data is heavily biased (read more [here](https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/))","metadata":{}},{"cell_type":"code","source":"# Calculate RMSE and Accuracy metrics\ndef validate(pipe, X_val, y_val):\n    ''' Pipe must have been fitted before being passed to this function '''\n    \n    # RMSE\n    rmse = mean_squared_error(pipe.predict(X_val), y_val, squared = False) \n\n    # Accuracy\n    lt_pred = pipe.predict(val_df['less_toxic'])\n    mt_pred = pipe.predict(val_df['more_toxic'])\n    accuracy = (lt_pred < mt_pred).mean()\n    \n    return lt_pred, mt_pred, accuracy, rmse","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:38.986651Z","iopub.execute_input":"2021-12-01T13:11:38.987298Z","iopub.status.idle":"2021-12-01T13:11:38.994736Z","shell.execute_reply.started":"2021-12-01T13:11:38.987259Z","shell.execute_reply":"2021-12-01T13:11:38.994069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training and Predicting in batches\nThis part is a bit tricky. I use folds instead of letting the model see all the data at once. In each fold I'll select a subset of data and use it to train the model, validate it and make a prediction based on the selected subset of data. Then return the prediction results from validation and the prediction itself.\n\nI'll later use the validation predictions to calculate optimal ensemble weights for the final prediction.","metadata":{}},{"cell_type":"code","source":"def fit_validate_predict(pipe, X, y, folds = 5, verbose = True):\n    \n    # Created folds\n    skf = KFold(\n        n_splits = folds,\n        shuffle = True, # Default is False\n        random_state = RANDOM_STATE\n    )\n    accuracies, rmses = np.zeros(folds), np.zeros(folds)\n    lt_preds, mt_preds = np.zeros((val_df.shape[0], folds)), np.zeros((val_df.shape[0], folds))\n    preds = np.zeros((test_df.shape[0], folds))\n\n    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n        \n        # Split the data into train and test sets\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]\n            \n        # Train the pipeline\n        pipe.fit(X_train, y_train)\n        \n        # Validate the pipeline with test_df['text'] and y_val\n        lt_pred, mt_pred, accuracy, rmse = validate(pipe, X_val, y_val)\n        accuracies[fold], rmses[fold] = accuracy, rmse\n        lt_preds[:, fold], mt_preds[:, fold] = lt_pred, mt_pred\n        \n        # Make predictions\n        preds[:, fold] = pipe.predict(test_df['text'])\n        \n        if verbose:\n            print(f\"FOLD #{fold + 1}) Accuracy: {accuracy.round(4)}, RMSE: {rmse.round(4)}\")    \n    \n    return lt_preds, mt_preds, preds, accuracies, rmses","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:38.99617Z","iopub.execute_input":"2021-12-01T13:11:38.996707Z","iopub.status.idle":"2021-12-01T13:11:39.007299Z","shell.execute_reply.started":"2021-12-01T13:11:38.996671Z","shell.execute_reply":"2021-12-01T13:11:39.006628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizations\nSince I'll (probably) be using multiple datasets in this notebook and run pretty much the same analysis over them, I'll define a few methods to avoid code duplication:","metadata":{}},{"cell_type":"code","source":"# Plots number of values for each toxicity level in the given dataframe\ndef plot_toxic_types_dist(df):    \n    fig = plt.figure(figsize = (20, 5))\n    plt.title('Toxicity Categories Count')\n    plt.bar([type for type in toxicity_types if type in jtc_df.columns], [df[type].value_counts()[1] for type in toxicity_types if type in df.columns], label = 'Number of occurrences')\n    plt.legend()\n    plt.show()\n\n# Plots the didtribution of values in toxicity columns of the given dataframe\ndef plot_toxicity_dist(df):\n    toxicity_values = df['toxicity'].value_counts()\n    \n    plt.figure(figsize = (20, 5))\n    plt.title('Toxicity Level Distribution')\n    plt.bar(toxicity_values.keys(), toxicity_values.values, color = 'g')\n    plt.show()\n\n# Plots the wordcloud for each toxicity level of the given data frame (Stopwords are removed)\ndef plot_wordcloud(df):\n    wordcloud = WordCloud(stopwords = STOPWORDS)\n    fig, ax = plt.subplots(3, 2, figsize = (20, 10))\n\n    i = 0\n    for row in ax:\n        for col in row:        \n            wordcloud.generate(' '.join(df.loc[df[toxicity_types[i]] != 0, 'text'].tolist()))\n            col.set_title(toxicity_types[i])        \n            col.imshow(wordcloud)        \n            col.axis(\"off\")\n            i += 1\n    plt.tight_layout(pad = 0)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:39.009141Z","iopub.execute_input":"2021-12-01T13:11:39.009918Z","iopub.status.idle":"2021-12-01T13:11:39.021452Z","shell.execute_reply.started":"2021-12-01T13:11:39.00989Z","shell.execute_reply":"2021-12-01T13:11:39.020803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Jigsaw Rate Severity of Toxic Comments\nThis is our original dataset for the competition. The columns are:\n- *comment_to_score.csv*: The dataset that is used for the final predictions.\n- *validation_data.csv*: The dataset that is used to validate the models.\n- *sample_submission.csv*: A sample submission file.","metadata":{}},{"cell_type":"code","source":"val_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ntest_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n\nprint(f'test_df\\n- Shape: {test_df.shape}\\n- Columns: {list(test_df.columns)}')\nprint(f'- Duplicates: {test_df.duplicated(subset = \"text\").sum()}\\n')\n\nprint(f'val_df\\n- Shape: {val_df.shape}\\n- Columns: {list(val_df.columns)}')\nprint(f'- Duplicates: {val_df.duplicated(subset = [\"less_toxic\", \"more_toxic\"]).sum()}!')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:39.022828Z","iopub.execute_input":"2021-12-01T13:11:39.023253Z","iopub.status.idle":"2021-12-01T13:11:39.343568Z","shell.execute_reply.started":"2021-12-01T13:11:39.02322Z","shell.execute_reply":"2021-12-01T13:11:39.342809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Duplicates in Validation Set\nAs mentioned before, the validation data contains duplicates. We could remove them using the following code:","metadata":{}},{"cell_type":"code","source":"# # Get the dupicate items\n# vals_duplicate_df = val_df[['less_toxic', 'more_toxic']]\n\n# # Drop the duplicate paires except the first occurrence (Remove the worker column as well)\n# val_df = vals_duplicate_df.loc[~vals_duplicate_df.duplicated(keep = 'first')]\n\n# print(f\"- New shape: {val_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:39.344865Z","iopub.execute_input":"2021-12-01T13:11:39.345577Z","iopub.status.idle":"2021-12-01T13:11:39.349453Z","shell.execute_reply.started":"2021-12-01T13:11:39.345539Z","shell.execute_reply":"2021-12-01T13:11:39.34871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text-Cleaning","metadata":{}},{"cell_type":"code","source":"# val_df\nval_df['less_toxic'] = val_df['less_toxic'].apply(clean_text)\nval_df['more_toxic'] = val_df['more_toxic'].apply(clean_text)\nprint('- Validation set cleaned.')\n\n# test_df\ntest_df['text'] = test_df['text'].apply(clean_text)\nprint('- Test set cleaned.')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:11:39.350884Z","iopub.execute_input":"2021-12-01T13:11:39.351679Z","iopub.status.idle":"2021-12-01T13:12:16.112903Z","shell.execute_reply.started":"2021-12-01T13:11:39.351627Z","shell.execute_reply":"2021-12-01T13:12:16.112139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# jigsaw toxic comment classification challenge\nThe \"*jigsaw-toxic-comment-train.csv*\" contains data from *train.csv* and *test.csv* of the *jigsaw-toxic-comment-classification-challenge* competition combined. (The test data and their corresponding labels have been merged)\n\n**NOTE #1**: I will be changing the columns names to match the original dataset columns' names.","metadata":{}},{"cell_type":"code","source":"jtc_df = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv').rename(\n    columns = {\n        'id': 'comment_id',\n        'comment_text': 'text'\n    }\n)\n\nprint(f'jtc_df\\n- Shape: {jtc_df.shape}')\nprint(f'- Columns: {list(jtc_df.columns)}')\nprint(f'- Duplicates: {jtc_df.duplicated(\"text\").sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:12:16.114427Z","iopub.execute_input":"2021-12-01T13:12:16.11494Z","iopub.status.idle":"2021-12-01T13:12:18.7782Z","shell.execute_reply.started":"2021-12-01T13:12:16.114902Z","shell.execute_reply":"2021-12-01T13:12:18.77742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Combining Toxicity Types\nHere I'll create a new column \"*toxicity*\" that is the weighted sum of all toxicity types.","metadata":{}},{"cell_type":"code","source":"# Combine all toxicity levels into one with the same weights set\njtc_df['toxicity'] = sum([jtc_df[type] * coef for type, coef in toxicity_weights.items() if type in jtc_df])\n\n# Standardize toxicity (converts to continues values)\n# jtc_df['toxicity'] = jtc_df['toxicity'] / jtc_df['toxicity'].max()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:12:18.781584Z","iopub.execute_input":"2021-12-01T13:12:18.781808Z","iopub.status.idle":"2021-12-01T13:12:18.793898Z","shell.execute_reply.started":"2021-12-01T13:12:18.781782Z","shell.execute_reply":"2021-12-01T13:12:18.793112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Downsampling & Text-Cleaning\nOur data is heavily unblanaced ([why is that bad?](https://machinelearningmastery.com/what-is-imbalanced-classification/)) and must be fixed. There are a few tricks we can pull off:\n- The weights can be adjusted in a way to try balance out the data (Not recommended - We have enough data for downsampling, don't sacrifice the weights to balance the data!)\n- Downsampling can drop the portion of data from the problematic side (Most effective)","metadata":{}},{"cell_type":"code","source":"# Downsample\njtc_df = pd.concat([\n    jtc_df[jtc_df['toxicity'] <= 0].sample(n = int((jtc_df['toxicity'] > 0).sum() * 1.5),random_state = RANDOM_STATE),\n    jtc_df[jtc_df['toxicity'] > 0]\n])\nprint(f\"- New shape: {jtc_df.shape}\")\n\n# Clean\njtc_df['text'] = jtc_df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:12:18.795485Z","iopub.execute_input":"2021-12-01T13:12:18.795758Z","iopub.status.idle":"2021-12-01T13:12:45.55194Z","shell.execute_reply.started":"2021-12-01T13:12:18.795723Z","shell.execute_reply":"2021-12-01T13:12:45.551221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis\nExplore fruther the datast using the following functions:","metadata":{}},{"cell_type":"code","source":"# plot_toxic_types_dist(jtc_df)\n# plot_toxicity_dist(jtc_df)\n# plot_wordcloud(jtc_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:12:45.553749Z","iopub.execute_input":"2021-12-01T13:12:45.554007Z","iopub.status.idle":"2021-12-01T13:12:45.559102Z","shell.execute_reply.started":"2021-12-01T13:12:45.553973Z","shell.execute_reply":"2021-12-01T13:12:45.558301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ruddit jigsaw dataset\nThird dataset used is the [ruddit-jigsaw-dataset](https://www.kaggle.com/rajkumarl/ruddit-jigsaw-dataset) and spesificly the \"*ruddit_with_text.csv*\". There are a few things worth paying attention:\n- Deleted comments are marked as *[deleted]*. Do we keep them? If comment is deleted by the user then it won't have any useful information, but if it's deleted by the community, that would raise a question: why?\n- I also shifted the toxicity scores to be between 0 and 1 (*toxicity* column of the previous dataset was also between 0 and 1)\n\n**NOTE #1**: The *offensiveness_score* is probably different that *toxicity*, but I will rename the column to match the other dataframes.","metadata":{}},{"cell_type":"code","source":"# Select only the columns we need\nrjd_df = pd.read_csv('../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv').rename(\n    columns = {\n        'txt': 'text',\n        'offensiveness_score': 'toxicity'\n    }\n)[['comment_id', 'text', 'toxicity']]\n\n# Change scale\nrjd_df['toxicity'] = (rjd_df['toxicity'] - rjd_df['toxicity'].min()) / (rjd_df['toxicity'].max() - rjd_df['toxicity'].min()) \n\nprint(f'rjd_df\\n- Shape: {jtc_df.shape}')\nprint(f'- Columns: {list(jtc_df.columns)}')\nprint(f'- Duplicates: {jtc_df.duplicated(\"text\").sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:12:45.560524Z","iopub.execute_input":"2021-12-01T13:12:45.560836Z","iopub.status.idle":"2021-12-01T13:12:45.679984Z","shell.execute_reply.started":"2021-12-01T13:12:45.560797Z","shell.execute_reply":"2021-12-01T13:12:45.679257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Invalid Entries & Text-Cleaning\nLooking at the below histogram, I believe the comments are removed by the authors themselves not the community (there is not much pattern for the deleted comments) and I will remove them entirely.","metadata":{}},{"cell_type":"code","source":"# Get duplicates texts\nduplicates = rjd_df['text'].duplicated(keep = 'first')\n\n# Plot distribution of toxicity scores for deleted texts\nplt.figure(figsize = (10, 5))\nplt.hist(rjd_df.loc[duplicates, 'toxicity'])\nplt.show()\n\n# Drop the deleted comments\nrjd_df = rjd_df.loc[rjd_df['text'] != '[deleted]']\nprint(f\"- New shape: {val_df.shape}\")\n\n# Text Cleaing\nrjd_df['text'] = rjd_df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:12:45.681279Z","iopub.execute_input":"2021-12-01T13:12:45.681699Z","iopub.status.idle":"2021-12-01T13:12:47.26374Z","shell.execute_reply.started":"2021-12-01T13:12:45.681649Z","shell.execute_reply":"2021-12-01T13:12:47.26292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble: Ridge() & TfidfVectorizer()\n## Creating the Pipeline","metadata":{}},{"cell_type":"code","source":"features = FeatureUnion([\n    ('vect', TfidfVectorizer(analyzer = 'char_wb', max_df = 0.5, min_df = 3, ngram_range = (3, 5))),\n])\n\n# Define pipeline\npipe = Pipeline([\n    (\"features\", features),\n    ('ridge', Ridge(random_state = RANDOM_STATE))\n])","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:12:47.265203Z","iopub.execute_input":"2021-12-01T13:12:47.265454Z","iopub.status.idle":"2021-12-01T13:12:47.270903Z","shell.execute_reply.started":"2021-12-01T13:12:47.265419Z","shell.execute_reply":"2021-12-01T13:12:47.270232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TfidfVectorizer & Ridge","metadata":{}},{"cell_type":"code","source":"# multilingual data\njtc_lt_preds, jtc_mt_preds, jtc_preds, jtc_accuracies, jtc_rmses = fit_validate_predict(\n    pipe = deepcopy(pipe),            # Don't train on the original pipeline\n    X = np.array(jtc_df['text']),\n    y = np.array(jtc_df['toxicity']),\n    folds = 5,\n)\n\nprint(f\"\\n- Avg Accuracy: {jtc_accuracies.round(4).mean()}\\n- Avg RMSE: {jtc_rmses.round(4).mean()}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:12:47.272672Z","iopub.execute_input":"2021-12-01T13:12:47.27332Z","iopub.status.idle":"2021-12-01T13:20:34.544902Z","shell.execute_reply.started":"2021-12-01T13:12:47.273281Z","shell.execute_reply":"2021-12-01T13:20:34.543193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ruddit data\nrjd_lt_preds, rjd_mt_preds, rjd_preds, rjd_accuracies, rjd_rmses = fit_validate_predict(\n    pipe = deepcopy(pipe),            # Don't train on the original pipeline\n    X = np.array(rjd_df['text']),\n    y = np.array(rjd_df['toxicity']),\n    folds = 5,\n)\n\nprint(f\"\\n- Avg Accuracy: {rjd_accuracies.round(4).mean()}\\n- Avg RMSE: {rjd_rmses.round(4).mean()}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:20:34.546288Z","iopub.execute_input":"2021-12-01T13:20:34.546624Z","iopub.status.idle":"2021-12-01T13:24:43.915195Z","shell.execute_reply.started":"2021-12-01T13:20:34.546585Z","shell.execute_reply":"2021-12-01T13:24:43.914409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT Model","metadata":{}},{"cell_type":"code","source":"# Sequence length for BERT\nSEQ_LEN = 60\nLEARNING_RATE = 1e-5\n\n# Loading transformers modules\nBERT_tokenizer = BertTokenizer.from_pretrained(BERT_DIR)\nBERT_base = TFBertModel.from_pretrained(BERT_DIR)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:24:43.916428Z","iopub.execute_input":"2021-12-01T13:24:43.917818Z","iopub.status.idle":"2021-12-01T13:24:56.725753Z","shell.execute_reply.started":"2021-12-01T13:24:43.917775Z","shell.execute_reply":"2021-12-01T13:24:56.724176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encoding texts for BERT","metadata":{}},{"cell_type":"code","source":"# Function that encodes txts into IDs and Masks for BERT to understnad\ndef get_BERT_input(text, max_length) :\n    input_ids = []\n    attention_masks = []\n\n    def breakdown(x):\n        encoded = BERT_tokenizer.encode_plus(\n            x,\n            add_special_tokens = True,\n            max_length = max_length,\n            pad_to_max_length = True,\n            return_attention_mask = True,\n            truncation = True   # Truncate to max_length\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    \n    text.apply(breakdown)\n    return [np.array(input_ids), np.array(attention_masks)]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:24:56.727154Z","iopub.execute_input":"2021-12-01T13:24:56.727394Z","iopub.status.idle":"2021-12-01T13:24:56.734815Z","shell.execute_reply.started":"2021-12-01T13:24:56.727359Z","shell.execute_reply":"2021-12-01T13:24:56.732933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jtc_bert_input = get_BERT_input(jtc_df['text'], SEQ_LEN)\nprint('- jtc_df Done.')\n\nval_less_toxic_bert_input = get_BERT_input(val_df['less_toxic'], SEQ_LEN)\nval_more_toxic_bert_input = get_BERT_input(val_df['more_toxic'], SEQ_LEN)\nprint('- val_df Done.')\n\ntest_bert_input = get_BERT_input(test_df['text'], SEQ_LEN)\nprint('- test_df Done.')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:24:56.73619Z","iopub.execute_input":"2021-12-01T13:24:56.736765Z","iopub.status.idle":"2021-12-01T13:29:36.666163Z","shell.execute_reply.started":"2021-12-01T13:24:56.73673Z","shell.execute_reply":"2021-12-01T13:29:36.665355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining BERT Model Architecture","metadata":{}},{"cell_type":"code","source":"def get_BERT_model(model_layer, seq_len, learning_rate = 1e-5):\n    \n    # Create input IDs and Masks\n    input_ids = Input(shape = (seq_len,), dtype = 'int32', name = 'input_ids')\n    input_attention_mask = Input(shape = (seq_len,), dtype = 'int32', name = 'attention_mask')\n    \n    # Build the model\n    output = model_layer(input_ids, attention_mask = input_attention_mask)[1]\n    output = Dropout(0.2)(output)    \n    output = Dense(units = 1, activation = 'sigmoid')(output)\n    \n    # Add input and output layers\n    model = Model(inputs = [input_ids, input_attention_mask], outputs = output)\n    \n    # Compile the model\n    model.compile(Adam(lr = learning_rate), loss = 'mse', metrics = [])    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:29:36.66756Z","iopub.execute_input":"2021-12-01T13:29:36.667859Z","iopub.status.idle":"2021-12-01T13:29:36.675284Z","shell.execute_reply.started":"2021-12-01T13:29:36.667821Z","shell.execute_reply":"2021-12-01T13:29:36.674595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_BERT_model(BERT_base, SEQ_LEN, LEARNING_RATE)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:29:36.676785Z","iopub.execute_input":"2021-12-01T13:29:36.677296Z","iopub.status.idle":"2021-12-01T13:29:42.68619Z","shell.execute_reply.started":"2021-12-01T13:29:36.677257Z","shell.execute_reply":"2021-12-01T13:29:42.685361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training BERT Model","metadata":{}},{"cell_type":"code","source":"EPOCHS = 10\nBATCH_SIZE = 32\nVALIDATION_SPLIT = 0.2\n\n# Checkpoints\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True,\n    save_weights_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\nearly_stop = EarlyStopping(\n    restore_best_weights = True,\n    patience = 2,\n)\n\n# Training the model\nhistory = model.fit(\n    jtc_bert_input,\n    jtc_df['toxicity'],\n    epochs = EPOCHS,\n    batch_size = BATCH_SIZE,\n    verbose = 1,\n    validation_split = VALIDATION_SPLIT,\n    callbacks = [reduce_lr, checkpoint, early_stop]\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:34:18.174973Z","iopub.execute_input":"2021-12-01T13:34:18.175802Z","iopub.status.idle":"2021-12-01T13:40:40.193593Z","shell.execute_reply.started":"2021-12-01T13:34:18.175757Z","shell.execute_reply":"2021-12-01T13:40:40.192787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Validation & Prediction for BERT Model\nThe *loss* and *val_loss* aren't enough metrics for us. To compare its performance against **Ridge()** I'll also validate it using *val_df*.","metadata":{}},{"cell_type":"code","source":"# Load best weights\nmodel.load_weights('model.h5')\n\n# Make predictions for validation set\nbert_lt_preds = model.predict(val_less_toxic_bert_input)\nbert_mt_preds = model.predict(val_more_toxic_bert_input)\n\nprint(f'- Val Accuracy: {(bert_lt_preds < bert_mt_preds).mean()}')\nprint(f'- Avg Loss: {np.array(history.history[\"loss\"]).mean()}')\nprint(f'- Avg Val Loss: {np.array(history.history[\"val_loss\"]).mean()}')\n\n# Make prediction for test set\nbert_preds = model.predict(test_bert_input)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:40:40.195856Z","iopub.execute_input":"2021-12-01T13:40:40.196134Z","iopub.status.idle":"2021-12-01T13:43:43.191223Z","shell.execute_reply.started":"2021-12-01T13:40:40.196096Z","shell.execute_reply":"2021-12-01T13:43:43.190455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble Modeling\n### Finding optimal weights\n\nI have used *scipy.optimizer* to find the optimal weights (See [documentations](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brute.html)) by brute forcing all possible combinations.\n\n**NOTE**: When added a new dataset, append the corresponding *lt_preds* and *mt_preds* to the end of *params* list.","metadata":{}},{"cell_type":"code","source":"params = (\n    [jtc_lt_preds.mean(axis = 1), rjd_lt_preds.mean(axis = 1), bert_lt_preds.mean(axis = 1)],\n    [jtc_mt_preds.mean(axis = 1), rjd_mt_preds.mean(axis = 1), bert_mt_preds.mean(axis = 1)]\n)\n\ndef func(x, *params):\n    lt = sum([x[i] * params[0][i] for i in range(len(x))])\n    mt = sum([x[i] * params[1][i] for i in range(len(x))])\n    return -1 * ((lt < mt).mean())\n\nresbrute = scipy.optimize.brute(\n    func,\n    ranges = ([slice(0, 1, 0.1) for _ in range(len(params[0]))]),\n    args = params,\n    full_output = True,\n    finish = None\n)\n\nprint(f'- Optimal weights: {resbrute[0]}\\n- Global Minimum: {resbrute[1] * -1}')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:49:33.501966Z","iopub.execute_input":"2021-12-01T13:49:33.50225Z","iopub.status.idle":"2021-12-01T13:49:33.807816Z","shell.execute_reply.started":"2021-12-01T13:49:33.502219Z","shell.execute_reply":"2021-12-01T13:49:33.807091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate Final Predictions\nWe have the optimal weights and we have the predictions. So the final prediction can be calculated using the two.\n\n**NOTE**: When added a new dataset, append the corresponding *_preds* to the end of *preds* list.","metadata":{}},{"cell_type":"code","source":"preds = [\n    jtc_preds.mean(axis = 1),\n    rjd_preds.mean(axis = 1),\n    bert_preds.mean(axis = 1),\n]\n\n# Multiply predictions and their corresponding weighs, then sum them up\ny_pred = np.array([preds[i] * resbrute[0][i] for i in range(len(preds))]).sum(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:50:02.037225Z","iopub.execute_input":"2021-12-01T13:50:02.037536Z","iopub.status.idle":"2021-12-01T13:50:02.04604Z","shell.execute_reply.started":"2021-12-01T13:50:02.037504Z","shell.execute_reply":"2021-12-01T13:50:02.04515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Submission\n**NOTE**: The predictions are ranked to get rid of any ties.","metadata":{}},{"cell_type":"code","source":"# Remove ties\ny_pred = scipy.stats.rankdata(y_pred, method = 'ordinal')\n\n# Create submission file\nsubmission_df = pd.DataFrame(data = {\n    'comment_id': test_df['comment_id'],\n    'score': y_pred\n}).to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T13:50:09.476156Z","iopub.execute_input":"2021-12-01T13:50:09.476679Z","iopub.status.idle":"2021-12-01T13:50:09.501462Z","shell.execute_reply.started":"2021-12-01T13:50:09.476624Z","shell.execute_reply":"2021-12-01T13:50:09.500723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}