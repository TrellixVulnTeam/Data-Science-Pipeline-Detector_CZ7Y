{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Inference kernel\n\n### This is the Inference kernel to : [Toxic Trainer | FIT | Multi Label :)](https://www.kaggle.com/kishalmandal/toxic-trainer-fit-multi-label/edit/run/79270347)","metadata":{}},{"cell_type":"code","source":"\nimport gc\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport transformers\n\nfrom transformers import AdamW, AutoTokenizer, AutoModel\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nimport time\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nimport re\nfrom nltk.corpus import stopwords","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-16T16:04:50.562887Z","iopub.execute_input":"2021-11-16T16:04:50.563379Z","iopub.status.idle":"2021-11-16T16:04:53.330679Z","shell.execute_reply.started":"2021-11-16T16:04:50.563296Z","shell.execute_reply":"2021-11-16T16:04:53.32994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    model_name = '../input/roberta-base'\n    batch_size = 64\n    lr = 1e-4\n    weight_decay = 0.01\n    scheduler = 'CosineAnnealingLR'\n    early_stopping_epochs = 1\n    epochs = 20\n    max_length = 128","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:04:53.332137Z","iopub.execute_input":"2021-11-16T16:04:53.332419Z","iopub.status.idle":"2021-11-16T16:04:53.337262Z","shell.execute_reply.started":"2021-11-16T16:04:53.332381Z","shell.execute_reply":"2021-11-16T16:04:53.336457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ToxicDataset:\n    def __init__(self, comments, tokenizer, max_len=196):\n        self.comments = comments\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.comments)\n    \n    def __getitem__(self, idx):\n        \n        tokenized = self.tokenizer.encode_plus(\n            self.comments[idx],\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n        \n        input_ids = tokenized['input_ids']\n        attention_mask = tokenized['attention_mask']\n        \n\n        return {\n            'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask' : torch.tensor(attention_mask, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:04:53.338424Z","iopub.execute_input":"2021-11-16T16:04:53.338871Z","iopub.status.idle":"2021-11-16T16:04:53.349381Z","shell.execute_reply.started":"2021-11-16T16:04:53.338829Z","shell.execute_reply":"2021-11-16T16:04:53.348765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ToxicModel(nn.Module):\n    def __init__(self, args):\n        super(ToxicModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(self.args.model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.toxic = nn.Linear(768, 1)\n        self.stoxic = nn.Linear(768, 1)\n        self.obs = nn.Linear(768, 1)\n        self.threat = nn.Linear(768, 1)\n        self.insult = nn.Linear(768, 1)\n        self.id_hate = nn.Linear(768, 1)\n    \n        \n    def forward(self, input_ids, attention_mask):\n        \n        out = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=False\n        )\n        \n        out = self.dropout(out[1])\n        \n        toxic = self.toxic(out)\n        stoxic = self.stoxic(out)\n        obs = self.obs(out)\n        threat = self.threat(out)\n        insult = self.insult(out)\n        id_hate = self.id_hate(out)\n\n        return torch.cat([toxic, stoxic, obs, threat, insult, id_hate], dim=-1)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:04:53.3526Z","iopub.execute_input":"2021-11-16T16:04:53.352841Z","iopub.status.idle":"2021-11-16T16:04:53.363452Z","shell.execute_reply.started":"2021-11-16T16:04:53.352783Z","shell.execute_reply":"2021-11-16T16:04:53.361251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(args, dataloader, model):\n    model.eval()\n    all_outputs=[]\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    with torch.no_grad():\n        for step, data in bar:\n            batch_size = args.batch_size\n\n            input_ids = data['input_ids'].cuda()\n            attention_mask = data['attention_mask'].cuda()\n            outputs = model(input_ids, attention_mask)\n            outputs = outputs.cpu().detach().numpy()\n            outputs = [sum(output) for output in outputs]\n            all_outputs.append(outputs)\n\n            bar.set_postfix(Stage='Inference') \n    return np.hstack(all_outputs)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:04:53.364511Z","iopub.execute_input":"2021-11-16T16:04:53.365189Z","iopub.status.idle":"2021-11-16T16:04:53.375449Z","shell.execute_reply.started":"2021-11-16T16:04:53.365162Z","shell.execute_reply":"2021-11-16T16:04:53.374751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def washing_machine(comments):\n    corpus=[]\n    for i in tqdm(range(len(comments))):\n        comment = re.sub('[^a-zA-Z]', ' ', comments[i])\n        comment = comment.lower()\n        comment = comment.split()\n        stemmer = SnowballStemmer('english')\n        lemmatizer = WordNetLemmatizer()\n        all_stopwords = stopwords.words('english')\n        comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n        comment = [lemmatizer.lemmatize(word) for word in comment]\n        comment = ' '.join(comment)\n        corpus.append(comment)\n\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:04:53.377808Z","iopub.execute_input":"2021-11-16T16:04:53.378023Z","iopub.status.idle":"2021-11-16T16:04:53.386326Z","shell.execute_reply.started":"2021-11-16T16:04:53.377999Z","shell.execute_reply":"2021-11-16T16:04:53.385646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:41:12.819216Z","iopub.status.idle":"2021-11-12T06:41:12.822893Z","shell.execute_reply.started":"2021-11-12T06:41:12.822624Z","shell.execute_reply":"2021-11-12T06:41:12.822663Z"}}},{"cell_type":"code","source":"def inference(data):\n    args=Config()\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    base_path= ['../input/2-folds-test/', '../input/robbaseuc/']\n    \n    dataset = ToxicDataset(data, tokenizer)\n    dataloader = DataLoader(dataset, batch_size=16*args.batch_size)\n    \n    final_preds = []\n    \n    \n    ### 2 folds test ###\n    num_folds = 5\n    \n    for fold in range(num_folds):\n        model = ToxicModel(args)\n        model = model.cuda()\n        path = base_path[0] + f'model_fold_{fold}.bin'\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {fold+1}\")\n        preds = get_predictions(args, dataloader, model)\n        final_preds.append(np.vstack(preds))\n        del model\n        gc.collect()\n        \n    ### roberta base un cleaned ###\n    num_folds = 2\n    for fold in range(num_folds):\n        model = ToxicModel(args)\n        model = model.cuda()\n        path = base_path[1] + f'model_fold_{fold}.bin'\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {fold+1}\")\n        preds = get_predictions(args, dataloader, model)\n        final_preds.append(np.vstack(preds))\n        del model\n        gc.collect()\n        \n    return np.hstack(sum(final_preds)/7)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:04:53.387892Z","iopub.execute_input":"2021-11-16T16:04:53.388473Z","iopub.status.idle":"2021-11-16T16:04:53.40034Z","shell.execute_reply.started":"2021-11-16T16:04:53.388433Z","shell.execute_reply":"2021-11-16T16:04:53.39967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"def show_validation():\n\n    df = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\n#     com1 = washing_machine(df['less_toxic'].values)\n#     com2 = washing_machine(df['more_toxic'].values)\n    com1 = df['less_toxic'].values\n    com2 = df['more_toxic'].values\n    pred1 = inference(com1)\n    pred2 = inference(com2)\n    score=[]\n    for o1,o2 in zip(pred1, pred2):\n        if o1<o2:\n            score.append(1)\n        else:\n            score.append(0)\n\n    mean_score = np.mean(score)\n    print('-'*50)\n    print('Validation Score :',mean_score)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:04:53.402868Z","iopub.execute_input":"2021-11-16T16:04:53.403431Z","iopub.status.idle":"2021-11-16T16:04:53.410358Z","shell.execute_reply.started":"2021-11-16T16:04:53.403394Z","shell.execute_reply":"2021-11-16T16:04:53.409621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_validation()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:04:53.411533Z","iopub.execute_input":"2021-11-16T16:04:53.412002Z","iopub.status.idle":"2021-11-16T16:49:38.72172Z","shell.execute_reply.started":"2021-11-16T16:04:53.411895Z","shell.execute_reply":"2021-11-16T16:49:38.720974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and submission","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:41:12.840048Z","iopub.status.idle":"2021-11-12T06:41:12.840655Z","shell.execute_reply.started":"2021-11-12T06:41:12.840414Z","shell.execute_reply":"2021-11-12T06:41:12.840437Z"}}},{"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:49:38.722999Z","iopub.execute_input":"2021-11-16T16:49:38.72351Z","iopub.status.idle":"2021-11-16T16:49:38.769643Z","shell.execute_reply.started":"2021-11-16T16:49:38.723469Z","shell.execute_reply":"2021-11-16T16:49:38.768946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:49:38.770904Z","iopub.execute_input":"2021-11-16T16:49:38.77158Z","iopub.status.idle":"2021-11-16T16:49:38.783983Z","shell.execute_reply.started":"2021-11-16T16:49:38.77154Z","shell.execute_reply":"2021-11-16T16:49:38.783297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments = washing_machine(df['text'].values)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:49:38.785422Z","iopub.execute_input":"2021-11-16T16:49:38.785919Z","iopub.status.idle":"2021-11-16T16:49:49.974164Z","shell.execute_reply.started":"2021-11-16T16:49:38.785882Z","shell.execute_reply":"2021-11-16T16:49:49.973413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = inference(comments)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:49:49.977102Z","iopub.execute_input":"2021-11-16T16:49:49.977315Z","iopub.status.idle":"2021-11-16T16:55:26.327881Z","shell.execute_reply.started":"2021-11-16T16:49:49.977281Z","shell.execute_reply":"2021-11-16T16:55:26.327181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['score'] = pred","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:55:26.329416Z","iopub.execute_input":"2021-11-16T16:55:26.330006Z","iopub.status.idle":"2021-11-16T16:55:26.334498Z","shell.execute_reply.started":"2021-11-16T16:55:26.329965Z","shell.execute_reply":"2021-11-16T16:55:26.333769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['comment_id', 'score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:55:26.335579Z","iopub.execute_input":"2021-11-16T16:55:26.336469Z","iopub.status.idle":"2021-11-16T16:55:26.373903Z","shell.execute_reply.started":"2021-11-16T16:55:26.336431Z","shell.execute_reply":"2021-11-16T16:55:26.373286Z"},"trusted":true},"execution_count":null,"outputs":[]}]}