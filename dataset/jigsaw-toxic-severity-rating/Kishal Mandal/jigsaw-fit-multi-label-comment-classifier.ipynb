{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using Toxic Comment Classification\n\nHere I have used the multi-label Toxic Comment Classification Dataset (by Jigsaw).\n\nI have used Roberta-base to predict 6 different outputs each belonging to one label (toxic, severe_toxic, etc...)\n\nI have not passed the output logits through any sigmoid function. Just used the outputs for these 6 labels and taken a linear average of them.\n\nThat's what I have used as the score for comparison.\n\n#### I have used:\n\n* BCELogitsLoss as the loss funtion : It already applies a sigmoid function on the output before calculating the loss\n* Early stooping with a patience of 1 : You can modify according to your need\n* Used CosineAnnealingLR scheduler : It changes the LR per step following a cosine funtion.\n\n","metadata":{}},{"cell_type":"markdown","source":"## The final layer of the model looks like this:\n\n ![NN](https://user-images.githubusercontent.com/74188336/141213710-3a1b7473-8436-4683-841e-64d87789f47e.png)","metadata":{}},{"cell_type":"markdown","source":"It has 6 output heads giving outputs to the 6 different labels to determine.\n\nThis 6 different heads are attached on top the roberta-base (for now) will implement roberta-large too.","metadata":{}},{"cell_type":"markdown","source":"### Please <span style=\"color:red\">UPVOTE</span> If it Helps :)\n\nModel used : RoBERTa-base (RoBERTa-large is way too big and takes a whole lot of time training O.o)\n\n#### The Inference Kernel : [INFER | ToxicComments | ðŸŒŸ](https://www.kaggle.com/kishalmandal/inference-comments/edit)\n\n**I have trained only 2 folds till now and got 0.798 on the LB (made public)**","metadata":{}},{"cell_type":"code","source":"!pip install tez -q","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:41.718985Z","iopub.execute_input":"2021-11-16T11:44:41.719535Z","iopub.status.idle":"2021-11-16T11:44:49.775601Z","shell.execute_reply.started":"2021-11-16T11:44:41.719436Z","shell.execute_reply":"2021-11-16T11:44:49.774698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport transformers\nimport tez\nfrom transformers import AdamW, AutoTokenizer, AutoModel\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nimport time\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-16T11:44:49.777707Z","iopub.execute_input":"2021-11-16T11:44:49.777987Z","iopub.status.idle":"2021-11-16T11:44:53.193868Z","shell.execute_reply.started":"2021-11-16T11:44:49.777949Z","shell.execute_reply":"2021-11-16T11:44:53.19314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    model_name = 'roberta-base'\n    batch_size = 96\n    lr = 1e-4\n    weight_decay = 0.01\n    scheduler = 'CosineAnnealingLR'\n    early_stopping_epochs = 1\n    epochs = 20\n    max_length = 128","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:53.195809Z","iopub.execute_input":"2021-11-16T11:44:53.196353Z","iopub.status.idle":"2021-11-16T11:44:53.202237Z","shell.execute_reply.started":"2021-11-16T11:44:53.196312Z","shell.execute_reply":"2021-11-16T11:44:53.201448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class ToxicDataset:\n    def __init__(self, data, tokenizer, max_len):\n        self.comments = data['comment_text'].values\n        self.tokenizer = tokenizer\n        self.targets = data[[\n            'toxic', 'severe_toxic', 'obscene',\n            'threat','insult', 'identity_hate']].values\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.comments)\n    \n    def __getitem__(self, idx):\n        \n        tokenized = self.tokenizer.encode_plus(\n            self.comments[idx],\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n        \n        input_ids = tokenized['input_ids']\n        attention_mask = tokenized['attention_mask']\n        \n        toxic, severe_toxic, obscene, threat, insult, identity_hate = self.targets[idx]\n\n        return {\n            'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask' : torch.tensor(attention_mask, dtype=torch.long),\n            'toxic' : torch.tensor(toxic, dtype=torch.float),\n            'severe_toxic' : torch.tensor(severe_toxic, dtype=torch.float),\n            'obscene' : torch.tensor(obscene, dtype=torch.float),\n            'threat' : torch.tensor(threat, dtype=torch.float),\n            'insult' : torch.tensor(insult, dtype=torch.float),\n            'identity_hate' : torch.tensor(identity_hate, dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:53.203507Z","iopub.execute_input":"2021-11-16T11:44:53.203942Z","iopub.status.idle":"2021-11-16T11:44:53.216382Z","shell.execute_reply.started":"2021-11-16T11:44:53.203903Z","shell.execute_reply":"2021-11-16T11:44:53.215649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Model","metadata":{}},{"cell_type":"code","source":"class ToxicModel(nn.Module):\n    def __init__(self, args, model_name):\n        super(ToxicModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(self.args.model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.toxic = nn.Linear(768, 1)\n        self.stoxic = nn.Linear(768, 1)\n        self.obs = nn.Linear(768, 1)\n        self.threat = nn.Linear(768, 1)\n        self.insult = nn.Linear(768, 1)\n        self.id_hate = nn.Linear(768, 1)\n    \n        \n    def forward(self, input_ids, attention_mask):\n        \n        out = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=False\n        )\n        \n        out = self.dropout(out[1])\n        \n        toxic = self.toxic(out)\n        stoxic = self.stoxic(out)\n        obs = self.obs(out)\n        threat = self.threat(out)\n        insult = self.insult(out)\n        id_hate = self.id_hate(out)\n\n        return [toxic, stoxic, obs, threat, insult, id_hate]\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:53.218827Z","iopub.execute_input":"2021-11-16T11:44:53.219584Z","iopub.status.idle":"2021-11-16T11:44:53.230071Z","shell.execute_reply.started":"2021-11-16T11:44:53.219516Z","shell.execute_reply":"2021-11-16T11:44:53.229364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function","metadata":{}},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    o1, o2, o3, o4, o5, o6 = outputs\n    t1, t2, t3, t4, t5, t6 = targets\n    l1 = nn.BCEWithLogitsLoss()(o1, t1.view(-1,1))\n    l2 = nn.BCEWithLogitsLoss()(o2, t2.view(-1,1))\n    l3 = nn.BCEWithLogitsLoss()(o3, t3.view(-1,1))\n    l4 = nn.BCEWithLogitsLoss()(o4, t4.view(-1,1))\n    l5 = nn.BCEWithLogitsLoss()(o5, t5.view(-1,1))\n    l6 = nn.BCEWithLogitsLoss()(o6, t6.view(-1,1))\n    \n    total_loss = (l1+l2+l3+l4+l5+l6)/6\n    \n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:53.23135Z","iopub.execute_input":"2021-11-16T11:44:53.232365Z","iopub.status.idle":"2021-11-16T11:44:53.242324Z","shell.execute_reply.started":"2021-11-16T11:44:53.232334Z","shell.execute_reply":"2021-11-16T11:44:53.241547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def metrics(outputs, targets):\n    auc_scores=[]\n    for o, t in zip(outputs, targets):\n        o = o.cpu().detach().numpy()\n        t = t.cpu().detach().numpy()\n        auc = roc_auc_score(o, t)\n        auc_scores(auc)\n\n    return np.mean(auc_scores)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:53.245709Z","iopub.execute_input":"2021-11-16T11:44:53.246251Z","iopub.status.idle":"2021-11-16T11:44:53.253375Z","shell.execute_reply.started":"2021-11-16T11:44:53.246206Z","shell.execute_reply":"2021-11-16T11:44:53.252586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training epoch","metadata":{}},{"cell_type":"code","source":"def train_epoch(args, dataloader, model, optimizer, scheduler, epoch):\n    model.train()\n    epoch_loss = 0.0\n    running_loss = 0.0\n    dataset_size=0\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        optimizer.zero_grad()\n        \n        input_ids = data['input_ids'].cuda()\n        attention_mask = data['attention_mask'].cuda()\n        toxic = data['toxic'].cuda()\n        severe_toxic = data['severe_toxic'].cuda()\n        obscene = data['obscene'].cuda()\n        threat = data['threat'].cuda()\n        insult = data['insult'].cuda()\n        identity_hate = data['identity_hate'].cuda()\n        \n        batch_size = args.batch_size\n        \n        targets = (toxic, severe_toxic, obscene, threat, insult, identity_hate)\n        outputs = model(input_ids, attention_mask)\n        \n        \n        loss = loss_fn(outputs, targets)\n\n        \n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:53.255649Z","iopub.execute_input":"2021-11-16T11:44:53.257907Z","iopub.status.idle":"2021-11-16T11:44:53.26794Z","shell.execute_reply.started":"2021-11-16T11:44:53.257873Z","shell.execute_reply":"2021-11-16T11:44:53.267133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Epoch","metadata":{}},{"cell_type":"code","source":"def validation(args, dataloader, model):\n    model.eval()\n    epoch_loss = 0.0\n    running_loss = 0.0\n    dataset_size=0\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    with torch.no_grad():\n        for step, data in bar:\n            batch_size = args.batch_size\n\n            input_ids = data['input_ids'].cuda()\n            attention_mask = data['attention_mask'].cuda()\n            toxic = data['toxic'].cuda()\n            severe_toxic = data['severe_toxic'].cuda()\n            obscene = data['obscene'].cuda()\n            threat = data['threat'].cuda()\n            insult = data['insult'].cuda()\n            identity_hate = data['identity_hate'].cuda()\n\n            targets = (toxic, severe_toxic, obscene, threat, insult, identity_hate)\n\n            outputs = model(input_ids, attention_mask)\n\n            loss = loss_fn(outputs, targets)\n\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n\n            epoch_loss = running_loss / dataset_size\n\n            bar.set_postfix(Valid_Loss=epoch_loss,\n                            Stage='Validation') \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:53.270244Z","iopub.execute_input":"2021-11-16T11:44:53.270901Z","iopub.status.idle":"2021-11-16T11:44:53.280702Z","shell.execute_reply.started":"2021-11-16T11:44:53.270862Z","shell.execute_reply":"2021-11-16T11:44:53.279914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer","metadata":{}},{"cell_type":"code","source":"def get_optimizer(args, params):\n    opt = AdamW(params, lr=args.lr, weight_decay=args.weight_decay)\n    return opt","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:53.281873Z","iopub.execute_input":"2021-11-16T11:44:53.282515Z","iopub.status.idle":"2021-11-16T11:44:53.293031Z","shell.execute_reply.started":"2021-11-16T11:44:53.282432Z","shell.execute_reply":"2021-11-16T11:44:53.29223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scheduler","metadata":{}},{"cell_type":"code","source":"def get_scheduler(args, optimizer):\n    if args.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=500, \n                                                   eta_min=1e-6)\n    else:\n        schduler = None\n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:53.294379Z","iopub.execute_input":"2021-11-16T11:44:53.29487Z","iopub.status.idle":"2021-11-16T11:44:53.302036Z","shell.execute_reply.started":"2021-11-16T11:44:53.294832Z","shell.execute_reply":"2021-11-16T11:44:53.301364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and validation Loop","metadata":{}},{"cell_type":"code","source":"def run(data, fold, args=None, save_model=False):\n    print('-'*50)\n    print(f'Fold : {fold}')\n    print('-'*50)\n    \n    if args is None:\n        args = Config()\n        \n    start = time.time()\n    model = ToxicModel(args, args.model_name)\n    model = model.cuda()\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    \n    optimizer = get_optimizer(args, model.parameters())\n    scheduler = get_scheduler(args, optimizer)\n    \n    train = data[data['kfold']!=fold]\n    valid = data[data['kfold']==fold]\n    \n    train_dataset = ToxicDataset(train, tokenizer, args.max_length)\n    valid_dataset = ToxicDataset(valid, tokenizer, args.max_length)\n    \n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n    valid_loader = DataLoader(valid_dataset, batch_size=2*args.batch_size)\n    \n    best_val_loss = np.inf\n    patience_counter = 0\n\n    for epoch in range(args.epochs):\n        train_loss = train_epoch(args, train_loader, model, optimizer, scheduler, epoch)\n        \n        valid_loss = validation(args, valid_loader, model)\n        \n        if valid_loss <= best_val_loss:\n            print(f\"Validation Loss Improved ({best_val_loss} ---> {valid_loss})\")\n            best_val_loss = valid_loss\n            \n            if save_model:\n                PATH = f\"model_fold_{fold}.bin\"\n                torch.save(model.state_dict(), PATH)\n                print(f\"----------Model Saved----------\")\n        \n        else:\n            patience_counter += 1\n            print(f'Early stopping counter {patience_counter} of {args.early_stopping_epochs}')\n            if patience_counter == args.early_stopping_epochs:\n                print('*************** Early Stopping ***************')\n                break\n    \n    end = time.time()\n    time_elapsed = end-start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_val_loss))\n    \n    del model, train_loader, valid_loader\n    gc.collect()\n    return best_val_loss","metadata":{"execution":{"iopub.status.busy":"2021-11-16T12:24:46.063675Z","iopub.execute_input":"2021-11-16T12:24:46.064363Z","iopub.status.idle":"2021-11-16T12:24:46.077702Z","shell.execute_reply.started":"2021-11-16T12:24:46.064314Z","shell.execute_reply":"2021-11-16T12:24:46.076913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read the data","metadata":{}},{"cell_type":"markdown","source":"I have used 5 folds that I have cleaned using this notebook: [Multi-Label Stratified K-fold | Toxic Comments](https://www.kaggle.com/kishalmandal/multi-label-stratified-k-fold-toxic-comments)\n\nThe data is the same data used in the toxicity classification challenge by Jigsaw","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/multi-label-stratified-k-fold-toxic-comments/5folds.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:53.319789Z","iopub.execute_input":"2021-11-16T11:44:53.320345Z","iopub.status.idle":"2021-11-16T11:44:54.853095Z","shell.execute_reply.started":"2021-11-16T11:44:53.320304Z","shell.execute_reply":"2021-11-16T11:44:54.852316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:54.856014Z","iopub.execute_input":"2021-11-16T11:44:54.856324Z","iopub.status.idle":"2021-11-16T11:44:54.936583Z","shell.execute_reply.started":"2021-11-16T11:44:54.856288Z","shell.execute_reply":"2021-11-16T11:44:54.93584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:44:54.938064Z","iopub.execute_input":"2021-11-16T11:44:54.938492Z","iopub.status.idle":"2021-11-16T11:44:55.006307Z","shell.execute_reply.started":"2021-11-16T11:44:54.938453Z","shell.execute_reply":"2021-11-16T11:44:55.005604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna","metadata":{}},{"cell_type":"code","source":"# df_trial = df[:100]","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:45:02.160634Z","iopub.execute_input":"2021-11-16T11:45:02.161669Z","iopub.status.idle":"2021-11-16T11:45:02.166391Z","shell.execute_reply.started":"2021-11-16T11:45:02.16161Z","shell.execute_reply":"2021-11-16T11:45:02.165593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def objective(trial):\n#     args = Config()\n#     args.epochs=1\n#     args.lr = trial.suggest_uniform('lr',1e-6, 1e-3)\n#     all_losses = []\n#     for fold in range(5):\n#         temp_loss = run(df_trial, fold, args=args)\n#         all_losses.append(temp_loss)\n    \n#     return np.mean(all_losses)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:45:02.340051Z","iopub.execute_input":"2021-11-16T11:45:02.342277Z","iopub.status.idle":"2021-11-16T11:45:02.348487Z","shell.execute_reply.started":"2021-11-16T11:45:02.342228Z","shell.execute_reply":"2021-11-16T11:45:02.347743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=10)\n\n# print('Best Trial:')\n# trial_ = study.best_trial\n# print(trial_.values)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:45:02.592835Z","iopub.execute_input":"2021-11-16T11:45:02.593399Z","iopub.status.idle":"2021-11-16T12:03:45.561459Z","shell.execute_reply.started":"2021-11-16T11:45:02.593362Z","shell.execute_reply":"2021-11-16T12:03:45.560735Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trial_.params['lr']","metadata":{"execution":{"iopub.status.busy":"2021-11-16T12:17:13.852397Z","iopub.execute_input":"2021-11-16T12:17:13.852886Z","iopub.status.idle":"2021-11-16T12:17:13.858556Z","shell.execute_reply.started":"2021-11-16T12:17:13.852848Z","shell.execute_reply":"2021-11-16T12:17:13.857867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run training","metadata":{}},{"cell_type":"code","source":"args=Config()\nargs.lr = 0.0005149849355804644\nrun(df, fold=0, save_model=True, args=args)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T12:24:50.403448Z","iopub.execute_input":"2021-11-16T12:24:50.403798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}