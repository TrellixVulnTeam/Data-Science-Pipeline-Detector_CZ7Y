{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-09T11:09:04.583526Z","iopub.execute_input":"2021-11-09T11:09:04.58411Z","iopub.status.idle":"2021-11-09T11:09:05.505526Z","shell.execute_reply.started":"2021-11-09T11:09:04.584067Z","shell.execute_reply":"2021-11-09T11:09:05.504889Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![nlp](https://user-images.githubusercontent.com/74188336/140924469-f7292676-d422-4871-98d3-59ce395e6e07.jpeg)","metadata":{}},{"cell_type":"markdown","source":"# Lets import NLTK","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:09:05.506836Z","iopub.execute_input":"2021-11-09T11:09:05.507178Z","iopub.status.idle":"2021-11-09T11:09:06.213578Z","shell.execute_reply.started":"2021-11-09T11:09:05.507147Z","shell.execute_reply":"2021-11-09T11:09:06.212908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What is stemming?\n\nStemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n\nStem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis). So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Example of stemming","metadata":{}},{"cell_type":"markdown","source":"![stemming](https://user-images.githubusercontent.com/74188336/140915632-b20d6c02-86c6-41bb-a2fb-6f8abc8d19bd.png)","metadata":{}},{"cell_type":"markdown","source":"# Let's See the action of a stemmer\n\nWe will be using SnwoballStemmer for Stemming from the NLTK library.\n\nYou can also use other stememr like PorterStemmer, etc.","metadata":{}},{"cell_type":"markdown","source":"# Import SnowballStemmer","metadata":{}},{"cell_type":"code","source":"from nltk.stem import SnowballStemmer","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:09:06.214522Z","iopub.execute_input":"2021-11-09T11:09:06.21526Z","iopub.status.idle":"2021-11-09T11:09:06.21864Z","shell.execute_reply.started":"2021-11-09T11:09:06.215223Z","shell.execute_reply":"2021-11-09T11:09:06.218002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Available Languages.\n\nStememr are not available for all languages. \nSo, Let us check out the languages which are available.","metadata":{}},{"cell_type":"code","source":"SnowballStemmer.languages","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:10:36.723524Z","iopub.execute_input":"2021-11-09T11:10:36.723842Z","iopub.status.idle":"2021-11-09T11:10:36.730119Z","shell.execute_reply.started":"2021-11-09T11:10:36.723809Z","shell.execute_reply":"2021-11-09T11:10:36.729421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmer = SnowballStemmer('english')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:22:39.443581Z","iopub.execute_input":"2021-11-09T11:22:39.443871Z","iopub.status.idle":"2021-11-09T11:22:39.447975Z","shell.execute_reply.started":"2021-11-09T11:22:39.443841Z","shell.execute_reply":"2021-11-09T11:22:39.446677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let us check out how good it is.\n\nWe will take the words in the image show above.\n","metadata":{}},{"cell_type":"code","source":"stemmer.stem('playing')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:22:40.354726Z","iopub.execute_input":"2021-11-09T11:22:40.355207Z","iopub.status.idle":"2021-11-09T11:22:40.36115Z","shell.execute_reply.started":"2021-11-09T11:22:40.35516Z","shell.execute_reply":"2021-11-09T11:22:40.360586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmer.stem('plays')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:22:54.813285Z","iopub.execute_input":"2021-11-09T11:22:54.813587Z","iopub.status.idle":"2021-11-09T11:22:54.819079Z","shell.execute_reply.started":"2021-11-09T11:22:54.813553Z","shell.execute_reply":"2021-11-09T11:22:54.818255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmer.stem('played')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:23:00.330847Z","iopub.execute_input":"2021-11-09T11:23:00.331507Z","iopub.status.idle":"2021-11-09T11:23:00.336318Z","shell.execute_reply.started":"2021-11-09T11:23:00.331466Z","shell.execute_reply":"2021-11-09T11:23:00.335428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Great!! It performed perfectly","metadata":{}},{"cell_type":"markdown","source":"### For stopwords!!!\n\nStopwords are the most frequent words in a sentence.\n\nThose include 'have', 'is', 'are' etc.\n\nBut sometimes, stemming the stopwords drive the meaning of a sentence in a completely different direction. Depending on the problem statement we can either ignore stemming the stopwords or stem them. But according to this problem statement, since the severity of a toxic comment depends on vulgar keywords, we might not be using stopwords at all :)\n\nBut still let us see how a stemmer ignore stopwords. It's pretty simple :)","metadata":{}},{"cell_type":"code","source":"stemmer_ignore_stopwords = SnowballStemmer('english', ignore_stopwords=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:30:29.141122Z","iopub.execute_input":"2021-11-09T11:30:29.141431Z","iopub.status.idle":"2021-11-09T11:30:29.148822Z","shell.execute_reply.started":"2021-11-09T11:30:29.141392Z","shell.execute_reply":"2021-11-09T11:30:29.148171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stemming with the generalized stemmer","metadata":{}},{"cell_type":"code","source":"stemmer.stem('having')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:31:11.393917Z","iopub.execute_input":"2021-11-09T11:31:11.394224Z","iopub.status.idle":"2021-11-09T11:31:11.399432Z","shell.execute_reply.started":"2021-11-09T11:31:11.39419Z","shell.execute_reply":"2021-11-09T11:31:11.398788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output is stemmed perfectly","metadata":{}},{"cell_type":"markdown","source":"#### Now lets use the stemmer that ignore stopwords","metadata":{}},{"cell_type":"code","source":"stemmer_ignore_stopwords.stem('having')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:32:01.300198Z","iopub.execute_input":"2021-11-09T11:32:01.300726Z","iopub.status.idle":"2021-11-09T11:32:01.305665Z","shell.execute_reply.started":"2021-11-09T11:32:01.300679Z","shell.execute_reply":"2021-11-09T11:32:01.304914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great our problem is now solved !!! :D","metadata":{}},{"cell_type":"markdown","source":"# What is Lemmatization?\n\nLemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n\nFor example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words.\n\n\nWe will be using **WordNetLemmatizer** for Lemmatizing","metadata":{}},{"cell_type":"markdown","source":"#### Let's download the 'wordnet' corpora","metadata":{}},{"cell_type":"code","source":"nltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:42:40.642423Z","iopub.execute_input":"2021-11-09T11:42:40.642833Z","iopub.status.idle":"2021-11-09T11:42:40.713295Z","shell.execute_reply.started":"2021-11-09T11:42:40.642786Z","shell.execute_reply":"2021-11-09T11:42:40.712633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:43:17.160203Z","iopub.execute_input":"2021-11-09T11:43:17.160457Z","iopub.status.idle":"2021-11-09T11:43:17.163903Z","shell.execute_reply.started":"2021-11-09T11:43:17.160431Z","shell.execute_reply":"2021-11-09T11:43:17.163258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:44:42.728692Z","iopub.execute_input":"2021-11-09T11:44:42.728957Z","iopub.status.idle":"2021-11-09T11:44:42.732284Z","shell.execute_reply.started":"2021-11-09T11:44:42.72893Z","shell.execute_reply":"2021-11-09T11:44:42.731688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer.lemmatize('feet')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:47:57.692755Z","iopub.execute_input":"2021-11-09T11:47:57.693052Z","iopub.status.idle":"2021-11-09T11:47:57.699284Z","shell.execute_reply.started":"2021-11-09T11:47:57.693018Z","shell.execute_reply":"2021-11-09T11:47:57.698331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now lets clean the data by removing unnecessary symbols and stopwords","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/toxic-comments/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:52:29.577883Z","iopub.execute_input":"2021-11-09T11:52:29.578728Z","iopub.status.idle":"2021-11-09T11:52:30.90838Z","shell.execute_reply.started":"2021-11-09T11:52:29.578685Z","shell.execute_reply":"2021-11-09T11:52:30.907744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:52:33.557242Z","iopub.execute_input":"2021-11-09T11:52:33.557568Z","iopub.status.idle":"2021-11-09T11:52:33.576913Z","shell.execute_reply.started":"2021-11-09T11:52:33.557512Z","shell.execute_reply":"2021-11-09T11:52:33.576141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['comment_text'].values[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:53:04.797343Z","iopub.execute_input":"2021-11-09T11:53:04.798089Z","iopub.status.idle":"2021-11-09T11:53:04.806836Z","shell.execute_reply.started":"2021-11-09T11:53:04.798051Z","shell.execute_reply":"2021-11-09T11:53:04.806012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Printing shows that there are not unnecessary symbols","metadata":{}},{"cell_type":"code","source":"df['comment_text'].values[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:53:14.991303Z","iopub.execute_input":"2021-11-09T11:53:14.991794Z","iopub.status.idle":"2021-11-09T11:53:14.997323Z","shell.execute_reply.started":"2021-11-09T11:53:14.99176Z","shell.execute_reply":"2021-11-09T11:53:14.996565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### But considering the array, we can see there are some escape sequences like '\\n'. \n\n#### Also, we won't be needing any digits neither will be needing those stopwords.","metadata":{}},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\n\n# creating a corpus with all the comments\ncorpus = []\ndf = df[:100]\nfor i in tqdm(range(len(df))):\n    comment = re.sub('[^a-zA-Z]', ' ', df['comment_text'][i])\n    comment = comment.lower()\n    comment = comment.split()\n    stemmer = SnowballStemmer('english')\n    lemmatizer = WordNetLemmatizer()\n    all_stopwords = stopwords.words('english')\n    comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n    comment = [lemmatizer.lemmatize(word) for word in comment]\n    comment = ' '.join(comment)\n    corpus.append(comment)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:59:29.96636Z","iopub.execute_input":"2021-11-09T11:59:29.967226Z","iopub.status.idle":"2021-11-09T11:59:30.117164Z","shell.execute_reply.started":"2021-11-09T11:59:29.967177Z","shell.execute_reply":"2021-11-09T11:59:30.116194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-09T11:59:49.95038Z","iopub.execute_input":"2021-11-09T11:59:49.950823Z","iopub.status.idle":"2021-11-09T11:59:49.956515Z","shell.execute_reply.started":"2021-11-09T11:59:49.950791Z","shell.execute_reply":"2021-11-09T11:59:49.955728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['comment_text'].values[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:00:05.134908Z","iopub.execute_input":"2021-11-09T12:00:05.13548Z","iopub.status.idle":"2021-11-09T12:00:05.14142Z","shell.execute_reply.started":"2021-11-09T12:00:05.135435Z","shell.execute_reply":"2021-11-09T12:00:05.140728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see how much the sentence has changed","metadata":{}},{"cell_type":"markdown","source":"# But why do we use Stemming and Lemmatizing and remove stopwords?\n\n#### Well to answer that, let us see the number of unique words before and after the process","metadata":{}},{"cell_type":"markdown","source":"I have taken only the first 100 samples of the dataset","metadata":{}},{"cell_type":"code","source":"len(set(np.hstack([sentence.split() for sentence in df['comment_text'].values])))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:03:18.575127Z","iopub.execute_input":"2021-11-09T12:03:18.575374Z","iopub.status.idle":"2021-11-09T12:03:18.603638Z","shell.execute_reply.started":"2021-11-09T12:03:18.57535Z","shell.execute_reply":"2021-11-09T12:03:18.602884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(set(np.hstack([sentence.split() for sentence in corpus])))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:03:32.038786Z","iopub.execute_input":"2021-11-09T12:03:32.03932Z","iopub.status.idle":"2021-11-09T12:03:32.049596Z","shell.execute_reply.started":"2021-11-09T12:03:32.039286Z","shell.execute_reply":"2021-11-09T12:03:32.048604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wow Almost Half!!!!\n\nThat will be enough to answer the question.\n\nThis reduces the number of unique words and hence reduces the dimensionality of the problem.\n\nBut **Not Always**. <br>\nSome problems require the use of stopwords. For example excluding not from a sentence changes the sentiment of that sentence.","metadata":{}},{"cell_type":"markdown","source":"### Let's see a bad case of removing stopwords","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:06:55.41903Z","iopub.execute_input":"2021-11-09T12:06:55.419289Z","iopub.status.idle":"2021-11-09T12:06:55.423211Z","shell.execute_reply.started":"2021-11-09T12:06:55.419263Z","shell.execute_reply":"2021-11-09T12:06:55.42224Z"}}},{"cell_type":"code","source":"comment = 'The food is not good'\ncomment = re.sub('[^a-zA-Z]', ' ', comment)\ncomment = comment.lower()\ncomment = comment.split()\nstemmer = SnowballStemmer('english')\nlemmatizer = WordNetLemmatizer()\nall_stopwords = stopwords.words('english')\ncomment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\ncomment = [lemmatizer.lemmatize(word) for word in comment]\ncomment = ' '.join(comment)\ncomment","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:07:52.315299Z","iopub.execute_input":"2021-11-09T12:07:52.316214Z","iopub.status.idle":"2021-11-09T12:07:52.325107Z","shell.execute_reply.started":"2021-11-09T12:07:52.31617Z","shell.execute_reply":"2021-11-09T12:07:52.324259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Complete opposite! :(\nThe original sentence was of negative sentiment : **'The food is not good'**\n\nThe output : **'food good'**","metadata":{}},{"cell_type":"markdown","source":"# Lets clean out data and save it as a Dataset for others to use :)","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/toxic-comments/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:10:50.053204Z","iopub.execute_input":"2021-11-09T12:10:50.054083Z","iopub.status.idle":"2021-11-09T12:10:51.24042Z","shell.execute_reply.started":"2021-11-09T12:10:50.054029Z","shell.execute_reply":"2021-11-09T12:10:51.239453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Washing machine\ncorpus=[]\nfor i in tqdm(range(len(df))):\n    comment = re.sub('[^a-zA-Z]', ' ', df['comment_text'][i])\n    comment = comment.lower()\n    comment = comment.split()\n    stemmer = SnowballStemmer('english')\n    lemmatizer = WordNetLemmatizer()\n    all_stopwords = stopwords.words('english')\n    comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n    comment = [lemmatizer.lemmatize(word) for word in comment]\n    comment = ' '.join(comment)\n    corpus.append(comment)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:15:08.021591Z","iopub.execute_input":"2021-11-09T12:15:08.022497Z","iopub.status.idle":"2021-11-09T12:18:02.437981Z","shell.execute_reply.started":"2021-11-09T12:15:08.022443Z","shell.execute_reply":"2021-11-09T12:18:02.437066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['cleaned_comment'] = corpus","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:18:02.439387Z","iopub.execute_input":"2021-11-09T12:18:02.43971Z","iopub.status.idle":"2021-11-09T12:18:02.47646Z","shell.execute_reply.started":"2021-11-09T12:18:02.439678Z","shell.execute_reply":"2021-11-09T12:18:02.475618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:21:49.01516Z","iopub.execute_input":"2021-11-09T12:21:49.015447Z","iopub.status.idle":"2021-11-09T12:21:49.029986Z","shell.execute_reply.started":"2021-11-09T12:21:49.015418Z","shell.execute_reply":"2021-11-09T12:21:49.028987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### lets create a csv of cleaned comments","metadata":{}},{"cell_type":"code","source":"df.to_csv('cleaned_train.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### function for generalization","metadata":{}},{"cell_type":"code","source":"def washing_machine(df):\n    corpus=[]\n    for i in tqdm(range(len(df))):\n        comment = re.sub('[^a-zA-Z]', ' ', df['comment_text'][i])\n        comment = comment.lower()\n        comment = comment.split()\n        stemmer = SnowballStemmer('english')\n        lemmatizer = WordNetLemmatizer()\n        all_stopwords = stopwords.words('english')\n        comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n        comment = [lemmatizer.lemmatize(word) for word in comment]\n        comment = ' '.join(comment)\n        corpus.append(comment)\n    df['cleaned_comment'] = corpus\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:21:02.640603Z","iopub.execute_input":"2021-11-09T12:21:02.640877Z","iopub.status.idle":"2021-11-09T12:21:02.647871Z","shell.execute_reply.started":"2021-11-09T12:21:02.640841Z","shell.execute_reply":"2021-11-09T12:21:02.6469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaned_df = washing_machine(df)","metadata":{},"execution_count":null,"outputs":[]}]}