{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\nh1,h2 {\n    text-align: center;\n    background-color: pink;\n    padding: 20px;\n    margin: 0;\n    color: white;\n    font-family: ariel;\n    border-radius: 80px\n}\n\nh3 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: gold;\n}\n\nbody, p {\n    font-family: ariel;\n    font-size: 15px;\n    color: charcoal;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n\nh4 {\n    padding: 0px;\n    margin: 0;\n    font-family: ariel;\n    color: purple;\n}\n</style>\n\"\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-19T21:24:00.154422Z","iopub.execute_input":"2021-11-19T21:24:00.154819Z","iopub.status.idle":"2021-11-19T21:24:00.163295Z","shell.execute_reply.started":"2021-11-19T21:24:00.154784Z","shell.execute_reply":"2021-11-19T21:24:00.162661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Detailed EDA and using TF-IDF with Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"### (: <span style=\"color:green\">WELCOME</span> :)\n\n![jig](https://user-images.githubusercontent.com/74188336/142691516-4b0fee38-6c8b-4204-8b1f-c1d8d1144161.jpeg)\n\n### Overview:\n\nThis notebook is to visualise the text data to see and identify some patterns in the text data which might help us in differentiating between `less_toxic` and `more_toxic` comments.\n\nI have used my custom designed dataset made from the previous Jigsaw Toxic Comment Classification Challenge. The making of the dataset has been detailed in this discussion.\n\nI wont be training any Deep Learning Model in this notebook. But if you want to train and take reference you can use any of my notebooks:\n\nI have used different techniques:\n* Proposed by @debarshichanda: Using RankingLoss Function to train a transformer.  [ðŸŽƒ BERT | FIT | ES and Visualisation ðŸ“ˆ](https://www.kaggle.com/kishalmandal/bert-fit-es-and-visualisation): This notebook can be used to train that model and [JRSTC | BERT | INFER ðŸŽƒ](https://www.kaggle.com/kishalmandal/jrstc-bert-infer) is the inference kernel. Just need to change the model name. No need to change the hidden_nodes everytime you switch between base and large models (since it used `nn.LazyLinear()` layer as the transformer head)\n\n* Used the Toxic Comment Classification Dataset and trained a multi-headed (6-heads) to classify the comments in those 6 categories. Then summed up the output probabilities and used those probabilities to rank the comments. You can find the training kernel [here](https://www.kaggle.com/kishalmandal/jigsaw-fit-multi-label-comment-classifier) and the inference kernel [here](https://www.kaggle.com/kishalmandal/infer-toxiccomments).\n\nNow back to this notebook :)\n\nThis notebook is focused on the different visualisation, some important plots (including uni-grams, bi-grams, tri-grams), some box-plots, distribution plots. A detailed discussion on TF-IDF vectorizer. Then using TF-IDF vectorizer to train a Logistic Regression model (5 folds) and make a submission.\n\n\n## Please <span style=\"color:white\">upvote</span>  if it helps you or if you like it :) It's free to <span style=\"color:white\">upvote</span> :) <span style=\"color:white\">:)</span> :)","metadata":{}},{"cell_type":"markdown","source":"# : <span style=\"color:white\">Contents</span> :\n\n### 1. Some basic pre-processing    \n### 2. n-grams (with visualizations) \n### 3. WordClouds ðŸ˜ƒ\n### 4. Box Plots \n### 5. Distribution plots\n### 6. Trying another approach\n### 7. TF-<span style=\"color:purple\">IDF</span>\n### 8. Training using Logistic Regression Model\n### 9. Visualization of `more-toxic` words\n### 10. Calculating score on `validation_data.csv`\n### 11. <span style=\"color:purple\">Maximum</span> score that can be obtained on `validation_data.csv`\n### 12. Submission time ðŸŽƒ","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import STOPWORDS, WordCloud, ImageColorGenerator\nfrom collections import defaultdict\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom PIL import Image\nimport seaborn as sns\nimport string\nimport plotly.figure_factory as ff\nimport random\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:24.27959Z","iopub.execute_input":"2021-11-19T21:17:24.280022Z","iopub.status.idle":"2021-11-19T21:17:27.394896Z","shell.execute_reply.started":"2021-11-19T21:17:24.279965Z","shell.execute_reply":"2021-11-19T21:17:27.394021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/toxic-comments-train/training_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:27.396135Z","iopub.execute_input":"2021-11-19T21:17:27.396366Z","iopub.status.idle":"2021-11-19T21:17:28.72461Z","shell.execute_reply.started":"2021-11-19T21:17:27.396338Z","shell.execute_reply":"2021-11-19T21:17:28.723678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:28.725727Z","iopub.execute_input":"2021-11-19T21:17:28.725948Z","iopub.status.idle":"2021-11-19T21:17:28.744919Z","shell.execute_reply.started":"2021-11-19T21:17:28.725924Z","shell.execute_reply":"2021-11-19T21:17:28.744027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Some basic pre-processing:\n\nThough it says some basic pre-processing, here I have just converted into lower case and removed some extra spaces.... :D\n\nAnd currently working of correcting mis-spelled words and some shortened words...\n\nNeed to increase my vocab...never heard of these `toxic` words :D","metadata":{}},{"cell_type":"markdown","source":"### 1.1. Converting to lowercase and removing extra spaces","metadata":{}},{"cell_type":"code","source":"df[\"less_toxic\"] = df['less_toxic'].apply(\n    lambda x: ' '.join([w for w in str(x).lower().split()])\n)\n\ndf[\"more_toxic\"] = df['more_toxic'].apply(\n    lambda x: ' '.join([w for w in str(x).lower().split()])\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:28.747114Z","iopub.execute_input":"2021-11-19T21:17:28.747424Z","iopub.status.idle":"2021-11-19T21:17:30.18794Z","shell.execute_reply.started":"2021-11-19T21:17:28.747392Z","shell.execute_reply":"2021-11-19T21:17:30.187335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:30.189076Z","iopub.execute_input":"2021-11-19T21:17:30.189802Z","iopub.status.idle":"2021-11-19T21:17:30.199405Z","shell.execute_reply.started":"2021-11-19T21:17:30.189758Z","shell.execute_reply":"2021-11-19T21:17:30.198853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2. Working on shortened and correcting mispelled words","metadata":{}},{"cell_type":"code","source":"df[\"less_toxic\"] = df[\"less_toxic\"].str.replace('fk', 'fuck')\ndf[\"less_toxic\"] = df[\"less_toxic\"].str.replace('fuk', 'fuck')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:30.200398Z","iopub.execute_input":"2021-11-19T21:17:30.201067Z","iopub.status.idle":"2021-11-19T21:17:30.412948Z","shell.execute_reply.started":"2021-11-19T21:17:30.201028Z","shell.execute_reply":"2021-11-19T21:17:30.412103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:30.414288Z","iopub.execute_input":"2021-11-19T21:17:30.414516Z","iopub.status.idle":"2021-11-19T21:17:30.424655Z","shell.execute_reply.started":"2021-11-19T21:17:30.414489Z","shell.execute_reply":"2021-11-19T21:17:30.423807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. n-grams\n\n### What are n-grams?\n\nIn the fields of computational linguistics and probability, an n-gram (sometimes also called Q-gram) is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.","metadata":{}},{"cell_type":"markdown","source":"#### **n-gram generator**","metadata":{}},{"cell_type":"code","source":"def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:30.425829Z","iopub.execute_input":"2021-11-19T21:17:30.426083Z","iopub.status.idle":"2021-11-19T21:17:30.436023Z","shell.execute_reply.started":"2021-11-19T21:17:30.426055Z","shell.execute_reply":"2021-11-19T21:17:30.435046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### N number of `n-grams` to visualize","metadata":{}},{"cell_type":"code","source":"N = 20","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:30.437222Z","iopub.execute_input":"2021-11-19T21:17:30.437652Z","iopub.status.idle":"2021-11-19T21:17:30.448334Z","shell.execute_reply.started":"2021-11-19T21:17:30.43762Z","shell.execute_reply":"2021-11-19T21:17:30.447416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1. uni-grams\n\nUnigrams are single words in a sentence. It's the smallest unit of word measurement.\n\nfor e.g. \n\nsentence : `'Hello I am the Leader of the Nazis'`\n\nThe unigrams are: `Hello`, `I`, `am`, `the`, `Leader`, `of`, `the`, `Nazis`","metadata":{}},{"cell_type":"code","source":"less_toxic_unigrams = defaultdict(int)\nfor tweet in df['less_toxic']:\n    for word in generate_ngrams(tweet, 1):\n        less_toxic_unigrams[word] += 1\n        \ndf_less_toxic_unigrams = pd.DataFrame(sorted(less_toxic_unigrams.items(), key=lambda x: x[1])[::-1])\n\nunigrams_less_100 = df_less_toxic_unigrams[:N]\n\nmore_toxic_unigrams = defaultdict(int)\nfor tweet in df['more_toxic']:\n    for word in generate_ngrams(tweet, 1):\n        more_toxic_unigrams[word] += 1\n        \ndf_more_toxic_unigrams = pd.DataFrame(sorted(more_toxic_unigrams.items(), key=lambda x: x[1])[::-1])\n\nunigrams_more_100 = df_more_toxic_unigrams[:N]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:30.44957Z","iopub.execute_input":"2021-11-19T21:17:30.450038Z","iopub.status.idle":"2021-11-19T21:17:35.243776Z","shell.execute_reply.started":"2021-11-19T21:17:30.449996Z","shell.execute_reply":"2021-11-19T21:17:35.243196Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(18, N//2), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=unigrams_less_100[0], x=unigrams_less_100[1], ax=axes[0], color='green')\nsns.barplot(y=unigrams_more_100[0], x=unigrams_more_100[1], ax=axes[1], color='red')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common unigrams in less_toxic comments', fontsize=15)\naxes[1].set_title(f'Top {N} most common unigrams in more_toxic comments', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:35.245116Z","iopub.execute_input":"2021-11-19T21:17:35.245972Z","iopub.status.idle":"2021-11-19T21:17:36.067917Z","shell.execute_reply.started":"2021-11-19T21:17:35.245925Z","shell.execute_reply":"2021-11-19T21:17:36.06701Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. bi-grams\n\nBi-grams are two words zipped together. If we iterate through each word in a sentence, then the pair of that word and the next word is called a bi-gram.\n\nLet's take the previous sentence as example:\n\nsentence : `'Hello I am the Leader of the Nazis'`\n\nThe bi-igrams are: `Hello I`, `I am`, `am the`, `the Leader`, `Leader of`, `of the`, `the Nazis`","metadata":{}},{"cell_type":"code","source":"less_toxic_bigrams = defaultdict(int)\nfor tweet in df['less_toxic']:\n    for word in generate_ngrams(tweet, 2):\n        less_toxic_bigrams[word] += 1\n        \ndf_less_toxic_bigrams = pd.DataFrame(sorted(less_toxic_bigrams.items(), key=lambda x: x[1])[::-1])\n\nbigrams_less_100 = df_less_toxic_bigrams[:N]\n\nmore_toxic_bigrams = defaultdict(int)\nfor tweet in df['more_toxic']:\n    for word in generate_ngrams(tweet, 2):\n        more_toxic_bigrams[word] += 1\n        \ndf_more_toxic_bigrams = pd.DataFrame(sorted(more_toxic_bigrams.items(), key=lambda x: x[1])[::-1])\n\nbigrams_more_100 = df_more_toxic_bigrams[:N]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:36.069178Z","iopub.execute_input":"2021-11-19T21:17:36.069412Z","iopub.status.idle":"2021-11-19T21:17:42.054501Z","shell.execute_reply.started":"2021-11-19T21:17:36.069384Z","shell.execute_reply":"2021-11-19T21:17:42.05341Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(18, N//2), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=bigrams_less_100[0], x=bigrams_less_100[1], ax=axes[0], color='green')\nsns.barplot(y=bigrams_more_100[0], x=bigrams_more_100[1], ax=axes[1], color='red')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common bigrams in less_toxic comments', fontsize=15)\naxes[1].set_title(f'Top {N} most common bigrams in more_toxic comments', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:42.059279Z","iopub.execute_input":"2021-11-19T21:17:42.059556Z","iopub.status.idle":"2021-11-19T21:17:43.054872Z","shell.execute_reply.started":"2021-11-19T21:17:42.059525Z","shell.execute_reply":"2021-11-19T21:17:43.053945Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3. Tri-Grams\n\nSimilarly, the tri-grams would be 3 consecutive words in a sentence. \n\nsentence : `'Hello I am the Leader of the Nazis'`\n\nThe tri-grams are: `Hello I am`, `I am the`, `am the Leader`, `the Leader of`, `Leader of the`, `of the Nazis`\n\nSimilarly we can go on calculating n-grams :)","metadata":{}},{"cell_type":"code","source":"less_toxic_trigrams = defaultdict(int)\nfor tweet in df['less_toxic']:\n    for word in generate_ngrams(tweet, 3):\n        less_toxic_trigrams[word] += 1\n        \ndf_less_toxic_trigrams = pd.DataFrame(sorted(less_toxic_trigrams.items(), key=lambda x: x[1])[::-1])\n\ntrigrams_less_100 = df_less_toxic_trigrams[:N]\n\nmore_toxic_trigrams = defaultdict(int)\nfor tweet in df['more_toxic']:\n    for word in generate_ngrams(tweet, 3):\n        more_toxic_trigrams[word] += 1\n        \ndf_more_toxic_trigrams = pd.DataFrame(sorted(more_toxic_trigrams.items(), key=lambda x: x[1])[::-1])\n\ntrigrams_more_100 = df_more_toxic_trigrams[:N]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:43.056257Z","iopub.execute_input":"2021-11-19T21:17:43.056489Z","iopub.status.idle":"2021-11-19T21:17:49.152168Z","shell.execute_reply.started":"2021-11-19T21:17:43.056462Z","shell.execute_reply":"2021-11-19T21:17:49.151379Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(30, N//2), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=trigrams_less_100[0], x=trigrams_less_100[1], ax=axes[0], color='green')\nsns.barplot(y=trigrams_more_100[0], x=trigrams_more_100[1], ax=axes[1], color='red')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common trigrams in less_toxic comments', fontsize=35)\naxes[1].set_title(f'Top {N} most common trigrams in more_toxic comments', fontsize=35)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:17:49.153205Z","iopub.execute_input":"2021-11-19T21:17:49.15342Z","iopub.status.idle":"2021-11-19T21:17:50.401057Z","shell.execute_reply.started":"2021-11-19T21:17:49.153394Z","shell.execute_reply":"2021-11-19T21:17:50.40019Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. WordClouds","metadata":{}},{"cell_type":"markdown","source":"\n\nWordCloud is a great way of visualizing the occurances of the most common words. The font size of the words depend on the occurance of that particular word in the whole corpus.","metadata":{}},{"cell_type":"markdown","source":"### 3.1 `less_toxic` comments WordCloud visualization\n\nLet's visualise the the word cloud of the `less_toxic` corpus. You can also project the wordcloud on a mask as well as select any colormap. ","metadata":{}},{"cell_type":"code","source":"import requests\nfrom io import BytesIO\ntry:\n    url=\"https://user-images.githubusercontent.com/74188336/142692890-641ebc21-2e47-4556-9d37-1c0b9e1a0587.jpeg\"\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n\n    text = ' '.join(df['less_toxic'].values)\n    mask = np.array(img)\n    wordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\", mask=mask, colormap='BuGn').generate(text.lower())\n    plt.figure(figsize=(15,15))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\nexcept Exception as e:\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:24:34.767634Z","iopub.execute_input":"2021-11-19T21:24:34.768837Z","iopub.status.idle":"2021-11-19T21:24:47.919632Z","shell.execute_reply.started":"2021-11-19T21:24:34.76878Z","shell.execute_reply":"2021-11-19T21:24:47.918662Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. `more_toxic` comments WordCloud visualization\n\nNow let's visualise the `more_toxic` comments....\n\nOh my.... even the wordcloud was able to determine the comments were toxic... See for yourself ðŸ˜±","metadata":{}},{"cell_type":"code","source":"\ntry:\n    text = ' '.join(df['more_toxic'].values)\n    url=\"https://user-images.githubusercontent.com/74188336/142692894-c17240e4-1101-4591-9d10-71793e460816.jpeg\"\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n\n    mask = np.array(img)\n    wordcloud = WordCloud(max_font_size=50, max_words=2000, background_color=\"white\", mask=mask, contour_width=0, contour_color='grey', colormap='Reds').generate(text.lower())\n    plt.figure(figsize=(15,15))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\nexcept Exception as e:\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:22.587477Z","iopub.execute_input":"2021-11-19T21:25:22.588527Z","iopub.status.idle":"2021-11-19T21:25:41.398016Z","shell.execute_reply.started":"2021-11-19T21:25:22.588471Z","shell.execute_reply":"2021-11-19T21:25:41.396881Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_toxic_words = [len(sentence.split(' ')) for sentence in df['less_toxic'].values]\nmore_toxic_words = [len(sentence.split(' ')) for sentence in df['more_toxic'].values]\n\nless_toxic_chars = [len(sentence) for sentence in df['less_toxic'].values]\nmore_toxic_chars = [len(sentence) for sentence in df['more_toxic'].values]\n\nless_toxic_punct = [len([char for char in sentence if char in string.punctuation]) for sentence in df['less_toxic'].values]\nmore_toxic_punct = [len([char for char in sentence if char in string.punctuation]) for sentence in df['more_toxic'].values]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:41.399809Z","iopub.execute_input":"2021-11-19T21:25:41.400106Z","iopub.status.idle":"2021-11-19T21:25:47.072626Z","shell.execute_reply.started":"2021-11-19T21:25:41.40007Z","shell.execute_reply":"2021-11-19T21:25:47.071736Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Box Plots\n\nLet's do some box plots for the number in the `less_toxic` and `more_toxic` comments to see the IQR. This will help us to determine the `mex_len` we will be using to pad/truncate the comments in our DeepNLP model (using BERT/RoBERTa)\n\nLink to the notebook :\n\nSimilarly, let's do :\n* Box plot for the number of characters\n* Box plot for the number of punctuations.\n\nThis punctuation part plays a dual role :(\n\nSometimes some punctuations are important and sometimes unecessary..\n\nFor e.g.: `What!!!!!???!?!?!?!`\n\nHere the punctuations emphasize the toxicity of the sentence.\n\nSometimes unecessary punctuations just lengthen the sentence which might result in truncation of the important part...\n\nFor e.g.: `\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"What!!!>??!L{\">\">`","metadata":{}},{"cell_type":"markdown","source":"### 4.1. Box plot of Number of words","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Box(y=less_toxic_words, name = 'less_toxic',))\nfig.add_trace(go.Box(y=more_toxic_words, name = 'more_toxic'))\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:47.073772Z","iopub.execute_input":"2021-11-19T21:25:47.074009Z","iopub.status.idle":"2021-11-19T21:25:48.329049Z","shell.execute_reply.started":"2021-11-19T21:25:47.073966Z","shell.execute_reply":"2021-11-19T21:25:48.328071Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2. Box Plot of Number of characters","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Box(y=less_toxic_chars, name = 'less_toxic',))\nfig.add_trace(go.Box(y=more_toxic_chars, name = 'more_toxic'))\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:48.332266Z","iopub.execute_input":"2021-11-19T21:25:48.332917Z","iopub.status.idle":"2021-11-19T21:25:49.481707Z","shell.execute_reply.started":"2021-11-19T21:25:48.332863Z","shell.execute_reply":"2021-11-19T21:25:49.48032Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3. Box Plot of Number of Punctuations","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Box(y=less_toxic_punct, name = 'less_toxic',))\nfig.add_trace(go.Box(y=more_toxic_punct, name = 'more_toxic'))\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:49.483502Z","iopub.execute_input":"2021-11-19T21:25:49.483804Z","iopub.status.idle":"2021-11-19T21:25:50.649014Z","shell.execute_reply.started":"2021-11-19T21:25:49.483769Z","shell.execute_reply":"2021-11-19T21:25:50.648039Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Distribution Plots\n\nLet us also observe the distribution of the number of words, number of characters and number of punctuations just as we did for the box-plots. The box-plots give an idea of the distribution of the values, but still let's observe the distribution for further clarification about the values.\n\nI will be taking a 10% random sample to plot because the original dataset has 100,000 comment pairs (just to make it faster)","metadata":{}},{"cell_type":"markdown","source":"### 5.1. Distribution of the number of words\n\n**Note: The value in the y-axis doesn't show the actual count but rather the fraction of count.**","metadata":{}},{"cell_type":"code","source":"less_toxic_words_plot = random.sample(less_toxic_words, 10000)\nmore_toxic_words_plot = random.sample(more_toxic_words, 10000)\nhist_data = [less_toxic_words_plot, more_toxic_words_plot]\n\nX = ['less_toxic', 'more_toxic']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, X, show_hist=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:50.65044Z","iopub.execute_input":"2021-11-19T21:25:50.650711Z","iopub.status.idle":"2021-11-19T21:25:51.147628Z","shell.execute_reply.started":"2021-11-19T21:25:50.650678Z","shell.execute_reply":"2021-11-19T21:25:51.146778Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2. Distribution of number of characters","metadata":{}},{"cell_type":"code","source":"import plotly.figure_factory as ff\nimport numpy as np\nimport random\n\nless_toxic_chars_plot = random.sample(less_toxic_chars, 10000)\nmore_toxic_chars_plot = random.sample(more_toxic_chars, 10000)\nhist_data = [less_toxic_chars_plot, more_toxic_chars_plot]\n\nX = ['less_toxic', 'more_toxic']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, X, show_hist=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:51.148892Z","iopub.execute_input":"2021-11-19T21:25:51.149168Z","iopub.status.idle":"2021-11-19T21:25:51.554396Z","shell.execute_reply.started":"2021-11-19T21:25:51.149129Z","shell.execute_reply":"2021-11-19T21:25:51.553694Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3. Distribution of the number of puncutation","metadata":{}},{"cell_type":"code","source":"import plotly.figure_factory as ff\nimport numpy as np\nimport random\n\nless_toxic_punct_plot = random.sample(less_toxic_punct, 10000)\nmore_toxic_punct_plot = random.sample(more_toxic_punct, 10000)\nhist_data = [less_toxic_punct_plot, more_toxic_punct_plot]\n\nX = ['less_toxic', 'more_toxic']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, X, show_hist=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:51.555437Z","iopub.execute_input":"2021-11-19T21:25:51.556155Z","iopub.status.idle":"2021-11-19T21:25:51.924249Z","shell.execute_reply.started":"2021-11-19T21:25:51.556118Z","shell.execute_reply":"2021-11-19T21:25:51.923447Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Trying another approach cuz I keep running out of GPU :)\n\nI have labelled the `less_toxic` comments as `0` and the `more_toxic` as `1`. This is not exactly the correct process. This dataset was purposely made to train of ranking loss function. But I thought why not give it a try :)\n\nIt will be giving very close results as both categories have similar kind of sentences. In this approach I have also visualised the weights of the positive (`more_toxic`) words and the negative (`less_toxic`) words. I know I have reversed it :)\n\nPlease bear with it :D","metadata":{}},{"cell_type":"code","source":"less_toxic_df = pd.DataFrame()\nless_toxic_df['comment'] = df['less_toxic']\nless_toxic_df['target'] = len(df['less_toxic'])*[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:51.925483Z","iopub.execute_input":"2021-11-19T21:25:51.926292Z","iopub.status.idle":"2021-11-19T21:25:51.97423Z","shell.execute_reply.started":"2021-11-19T21:25:51.926247Z","shell.execute_reply":"2021-11-19T21:25:51.973252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"more_toxic_df = pd.DataFrame()\nmore_toxic_df['comment'] = df['more_toxic']\nmore_toxic_df['target'] = len(df['more_toxic'])*[1]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:51.97672Z","iopub.execute_input":"2021-11-19T21:25:51.977096Z","iopub.status.idle":"2021-11-19T21:25:52.016768Z","shell.execute_reply.started":"2021-11-19T21:25:51.977064Z","shell.execute_reply":"2021-11-19T21:25:52.015815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = pd.concat([less_toxic_df, more_toxic_df])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:52.018009Z","iopub.execute_input":"2021-11-19T21:25:52.018265Z","iopub.status.idle":"2021-11-19T21:25:52.029685Z","shell.execute_reply.started":"2021-11-19T21:25:52.018234Z","shell.execute_reply":"2021-11-19T21:25:52.029015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = final_df.sample(frac=0.1, random_state=2).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:52.030882Z","iopub.execute_input":"2021-11-19T21:25:52.031277Z","iopub.status.idle":"2021-11-19T21:25:52.05631Z","shell.execute_reply.started":"2021-11-19T21:25:52.031244Z","shell.execute_reply":"2021-11-19T21:25:52.055429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-19T21:25:52.05768Z","iopub.execute_input":"2021-11-19T21:25:52.057933Z","iopub.status.idle":"2021-11-19T21:25:52.082361Z","shell.execute_reply.started":"2021-11-19T21:25:52.057902Z","shell.execute_reply":"2021-11-19T21:25:52.081324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. TF-IDF\n\n### 7.1. Learning about the tf-idf vectorizer.\n\n**tf : Term Frequency**\n\n\n$$tf(t,d)= \\frac{\\text{count of the word t in d}}{\\text{total number of words in d}}$$\n\n\n**df : Document Frequency**\n\nThis measures the importance of documents in a whole set of the corpus. This is very similar to TF but the only difference is that TF is the frequency counter for a term t in document d, whereas DF is the count of occurrences of term t in the document set N. In other words, DF is the number of documents in which the word is present. We consider one occurrence if the term is present in the document at least once, we do not need to know the number of times the term is present.\n\n$$df(t) = \\text{occurrence of t in N documents}$$\n\nTo keep this also in a range, we normalize by dividing by the total number of documents. Our main goal is to know the informativeness of a term, and DF is the exact inverse of it. that is why we inverse the DF\n\n\n**idf : inverse document frequency**\n\nIDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because they are present in almost all of the documents, and N/df will give a very low value to that word). This finally gives what we want, a relative weightage.\n\n$$\\text{df}(t) = \\frac{\\text{N}}{\\text{df}}$$\n\n\nNow there are few other problems with the IDF, when we have a large corpus size say N=10000, the IDF value explodes. So to dampen the effect we take the log of IDF\n\n$$\\text{idf}(t) = \\log\\frac{N}{df + 1}$$\n\nFinally, by taking a multiplicative value of TF and IDF, we get the TF-IDF score. \n\n$$\\text{tf-idf}(t, d) = \\text{tf(t, d)}\\times\\log\\frac{N}{df + 1}$$\n\nPreviously thought of using the count vectorizer, but TF-IDF vectorizer automatically drops the weights of the words that doesn't contribute to the classification. That's why we won't need to remove the stopwords.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(final_df, test_size=0.2, random_state=2)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:52.084084Z","iopub.execute_input":"2021-11-19T21:25:52.084603Z","iopub.status.idle":"2021-11-19T21:25:52.277455Z","shell.execute_reply.started":"2021-11-19T21:25:52.084558Z","shell.execute_reply":"2021-11-19T21:25:52.276527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.2. Vectorizing using `TfidfVectorizer`","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit_transform(train_df['comment'].values.tolist() + test_df['comment'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['comment'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['comment'].values.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:52.278806Z","iopub.execute_input":"2021-11-19T21:25:52.279342Z","iopub.status.idle":"2021-11-19T21:25:57.409251Z","shell.execute_reply.started":"2021-11-19T21:25:52.27929Z","shell.execute_reply":"2021-11-19T21:25:57.408425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Training using Logistic Regression Model\n\nI have used simple linear regression model, since it is the fastest and will help in determining the weights of the words that contribute to the classification. \n\nI have plotted the words with their weights below.\n\nAlso, here I have used accuracy_score as the metric, which is not at all the correct metric :( but the way I designed this problem does the word :D\n\nI have printed the OOF accuracy scores and also the accuracy score on the whole test set after this","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics, model_selection, linear_model\n\ntrain_y = train_df[\"target\"].values\n\ndef runModel(train_X, train_y, test_X, test_y, test_X2):\n    model = linear_model.LogisticRegression(C=5., solver='sag')\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict(test_X)#[:,1]\n    pred_test_y2 = model.predict(test_X2)#[:,1]\n    return pred_test_y, pred_test_y2, model\n\nprint(\"Building model.\")\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0]])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n\nall_models=[]\nfold=0\nfor dev_index, val_index in kf.split(train_df):\n    \n    print('-'*50)\n    print('Fold :', fold)\n    print('-'*50)\n\n\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    print('Training..')\n    pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_tfidf)\n    \n    all_models.append(model)\n    \n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index] = pred_val_y\n    cv_scores.append(metrics.accuracy_score(val_y, pred_val_y))\n    print(f'Accuracy Score for fold {fold}:', metrics.accuracy_score(val_y, pred_val_y))\n    fold+=1","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:25:57.410576Z","iopub.execute_input":"2021-11-19T21:25:57.411086Z","iopub.status.idle":"2021-11-19T21:26:00.126433Z","shell.execute_reply.started":"2021-11-19T21:25:57.411041Z","shell.execute_reply":"2021-11-19T21:26:00.125465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [round(y) for y in pred_full_test/5]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:00.127824Z","iopub.execute_input":"2021-11-19T21:26:00.128452Z","iopub.status.idle":"2021-11-19T21:26:00.141974Z","shell.execute_reply.started":"2021-11-19T21:26:00.128403Z","shell.execute_reply":"2021-11-19T21:26:00.141288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy on test set:', metrics.accuracy_score(test_df.target, preds))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:00.143146Z","iopub.execute_input":"2021-11-19T21:26:00.14377Z","iopub.status.idle":"2021-11-19T21:26:00.151021Z","shell.execute_reply.started":"2021-11-19T21:26:00.143736Z","shell.execute_reply":"2021-11-19T21:26:00.150085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy on the test set isn't bad. But the results will change when we will predict the probailities :( since the all the sentences are toxic........\n\nEven the models don't like toxicity -_-","metadata":{}},{"cell_type":"markdown","source":"# 9. Visualisation of `more_toxic` words\n\nThe weights determine the **severity** of toxicity. The words in `red` are **lesser** toxic words and the words in `green` are **more** toxic words.","metadata":{}},{"cell_type":"code","source":"import eli5\neli5.show_weights(model, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:00.152635Z","iopub.execute_input":"2021-11-19T21:26:00.153203Z","iopub.status.idle":"2021-11-19T21:26:07.164928Z","shell.execute_reply.started":"2021-11-19T21:26:00.153153Z","shell.execute_reply":"2021-11-19T21:26:07.164047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Calculating Score on Validation Data","metadata":{}},{"cell_type":"code","source":"valdf = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:07.166223Z","iopub.execute_input":"2021-11-19T21:26:07.16647Z","iopub.status.idle":"2021-11-19T21:26:07.710339Z","shell.execute_reply.started":"2021-11-19T21:26:07.16644Z","shell.execute_reply":"2021-11-19T21:26:07.709342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comment1 = valdf['less_toxic'].values\ncomment2 = valdf['more_toxic'].values","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:07.71182Z","iopub.execute_input":"2021-11-19T21:26:07.712297Z","iopub.status.idle":"2021-11-19T21:26:07.717204Z","shell.execute_reply.started":"2021-11-19T21:26:07.712265Z","shell.execute_reply":"2021-11-19T21:26:07.716058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comm1 = tfidf_vec.transform(comment1)\ncomm2 = tfidf_vec.transform(comment2)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:07.718651Z","iopub.execute_input":"2021-11-19T21:26:07.719063Z","iopub.status.idle":"2021-11-19T21:26:13.896449Z","shell.execute_reply.started":"2021-11-19T21:26:07.719019Z","shell.execute_reply":"2021-11-19T21:26:13.895496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1 = []\npred2 = []\nfor model in all_models:\n    pred1.append(np.array(model.predict_proba(comm1)[:,1]))\n    pred2.append(np.array(model.predict_proba(comm2)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:13.897673Z","iopub.execute_input":"2021-11-19T21:26:13.897938Z","iopub.status.idle":"2021-11-19T21:26:13.964396Z","shell.execute_reply.started":"2021-11-19T21:26:13.897908Z","shell.execute_reply":"2021-11-19T21:26:13.962894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1 = sum(pred1)/5\npred2 = sum(pred2)/5","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:13.965691Z","iopub.execute_input":"2021-11-19T21:26:13.965968Z","iopub.status.idle":"2021-11-19T21:26:13.97418Z","shell.execute_reply.started":"2021-11-19T21:26:13.965937Z","shell.execute_reply":"2021-11-19T21:26:13.973203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:30:09.730141Z","iopub.execute_input":"2021-11-19T21:30:09.730617Z","iopub.status.idle":"2021-11-19T21:30:09.73808Z","shell.execute_reply.started":"2021-11-19T21:30:09.730583Z","shell.execute_reply":"2021-11-19T21:30:09.736799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_score=[]\nfor s1, s2 in zip(pred1, pred2):\n    if s1<s2:\n        val_score.append(1)\n    else:\n        val_score.append(0)\n        \nprint('Validation Score :',np.mean(val_score))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:13.976084Z","iopub.execute_input":"2021-11-19T21:26:13.97681Z","iopub.status.idle":"2021-11-19T21:26:14.001825Z","shell.execute_reply.started":"2021-11-19T21:26:13.976759Z","shell.execute_reply":"2021-11-19T21:26:14.000334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Maximum Score that can be obtained on `validation_data.csv`\n\nThe maximum score that can be obtained on `validation_data.csv` can be calculated by the following code in the cell below:\n\nYou can find about this discussion [here](https://www.kaggle.com/c/jigsaw-toxic-severity-rating/discussion/287350) and the code was proposed by [yuval reina](https://www.kaggle.com/yuval6967) ","metadata":{}},{"cell_type":"code","source":"gp1=valdf.groupby(['less_toxic','more_toxic']).worker.count().reset_index()\ngp2=gp1.copy()\ngp2['less_toxic']=gp1['more_toxic']\ngp2['more_toxic']=gp1['less_toxic']\nmrg=gp1.merge(gp2,how='outer',on=['less_toxic','more_toxic']).fillna(0)\nmrg['sum']=mrg.worker_x+mrg.worker_y\nmrg['max']=mrg[['worker_x','worker_y']].max(1)\nprint('Maximum Score :', mrg['max'].sum()/mrg['sum'].sum())","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:14.004053Z","iopub.execute_input":"2021-11-19T21:26:14.004716Z","iopub.status.idle":"2021-11-19T21:26:14.171967Z","shell.execute_reply.started":"2021-11-19T21:26:14.004647Z","shell.execute_reply":"2021-11-19T21:26:14.170502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Validation Score isn't great compared to transformers or Naive-Bayes Model. But can't exactly tell it's bad because the maximum score that can be obtained from the validation set is : **0.824**","metadata":{}},{"cell_type":"code","source":"print('Effective accuracy:', np.round(np.mean(val_score)/0.824*100, 2),'%')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:26:14.173379Z","iopub.execute_input":"2021-11-19T21:26:14.173653Z","iopub.status.idle":"2021-11-19T21:26:14.182143Z","shell.execute_reply.started":"2021-11-19T21:26:14.173621Z","shell.execute_reply":"2021-11-19T21:26:14.181051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Not bad..\n\nWithout any pre-processing and just using Logistic Regression gets about an effective 75% accuracy.. :D\nSeems like TF-IDF works great. Let's make a submission now :)","metadata":{}},{"cell_type":"markdown","source":"# 12. Submission Time ðŸŽƒ","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n\ncomms = tfidf_vec.transform(sub['text'].values)\nsub_preds = []\nfor model in all_models:\n    sub_preds.append(np.array(model.predict_proba(comms)[:,1]))\n\nsub['score'] = sum(sub_preds)\nsub[['comment_id', 'score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:30:50.187492Z","iopub.execute_input":"2021-11-19T21:30:50.187817Z","iopub.status.idle":"2021-11-19T21:30:51.222182Z","shell.execute_reply.started":"2021-11-19T21:30:50.187788Z","shell.execute_reply":"2021-11-19T21:30:51.220473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[['comment_id', 'score']].head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:31:18.788867Z","iopub.execute_input":"2021-11-19T21:31:18.789358Z","iopub.status.idle":"2021-11-19T21:31:18.801309Z","shell.execute_reply.started":"2021-11-19T21:31:18.789326Z","shell.execute_reply":"2021-11-19T21:31:18.799799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (: <span style=\"color:white\">Thank you for reading</span> :)\n\n# <span style=\"color:white\">o.O</span> Please DO <span style=\"color:white\">UPVOTE</span> if you find it useful <span style=\"color:white\">O.o</span>","metadata":{}},{"cell_type":"markdown","source":"### References:\n\n* [Simple Exploration Notebook - QIQC](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) \n* [Just some simple EDA](https://www.kaggle.com/tunguz/just-some-simple-eda)\n* [NLP with Disaster Tweets - EDA, Cleaning and BERT](https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)","metadata":{"_kg_hide-output":true}}]}