{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.options.mode.chained_assignment = None\nimport seaborn as sns\n#sns.set(style = 'white')\nimport nltk\n#import string\nfrom textblob import TextBlob\nimport string\nimport re\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_colwidth', 100)\n\n\nimport transformers\nfrom transformers import TFDistilBertForSequenceClassification\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom pylab import rcParams\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\nimport tensorflow as tf\nimport datasets \n\nRANDOM_SEED = 331\nnp.random.seed(RANDOM_SEED)\n\n# Evaluation\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n\n#https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-09T21:25:17.461782Z","iopub.execute_input":"2022-01-09T21:25:17.462317Z","iopub.status.idle":"2022-01-09T21:25:27.831056Z","shell.execute_reply.started":"2022-01-09T21:25:17.462216Z","shell.execute_reply":"2022-01-09T21:25:27.830068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df=pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T21:25:29.583802Z","iopub.execute_input":"2022-01-09T21:25:29.586852Z","iopub.status.idle":"2022-01-09T21:25:29.797838Z","shell.execute_reply.started":"2022-01-09T21:25:29.586776Z","shell.execute_reply":"2022-01-09T21:25:29.796766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T21:25:31.671859Z","iopub.execute_input":"2022-01-09T21:25:31.672686Z","iopub.status.idle":"2022-01-09T21:25:31.684277Z","shell.execute_reply.started":"2022-01-09T21:25:31.672647Z","shell.execute_reply":"2022-01-09T21:25:31.682748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punc = string.punctuation\n\ndef text_cleaning(text):\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n\n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\nimport nltk\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndef clean(data, col):\n    \n    data[col] = data[col].str.replace('https?://\\S+|www\\.\\S+', ' social medium ', regex=True)      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\", regex=False) \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\", regex=False)\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \", regex=False)\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \", regex=False)\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you', regex=True)\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace('\\s+', ' ', regex=True)  # will remove more than one whitespace character\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1', regex=True) # 2 or more characters are replaced by 2 characters\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\"[:|♣|'|§|♠|*|/|?|=|%|&|-|#|•|~|^|>|<|►|_]\", '', regex=True)\n    \n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \", regex=False)\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'s\", \" \", regex=False)\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', regex=True)\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', regex=True)    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ', regex=True)    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    tqdm.pandas()\n    data[col] = data[col].progress_apply(text_cleaning)\n    data[col] = data[col].apply(lambda x: x.lower())\n    return data\n\ntest_df=clean(test_df,'text')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T21:25:32.401713Z","iopub.execute_input":"2022-01-09T21:25:32.402007Z","iopub.status.idle":"2022-01-09T21:25:40.051693Z","shell.execute_reply.started":"2022-01-09T21:25:32.401972Z","shell.execute_reply":"2022-01-09T21:25:40.050718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T21:25:40.053673Z","iopub.execute_input":"2022-01-09T21:25:40.054147Z","iopub.status.idle":"2022-01-09T21:25:40.0614Z","shell.execute_reply.started":"2022-01-09T21:25:40.054105Z","shell.execute_reply":"2022-01-09T21:25:40.060397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_toxic = TFDistilBertForSequenceClassification.from_pretrained('../input/my-model/toxic')\nmodel_severe_toxic = TFDistilBertForSequenceClassification.from_pretrained('../input/my-model/severe_toxic')\nmodel_threat = TFDistilBertForSequenceClassification.from_pretrained('../input/my-model/model_threat')\nmodel_obscene = TFDistilBertForSequenceClassification.from_pretrained('../input/my-model/model_obscene')\nmodel_insult = TFDistilBertForSequenceClassification.from_pretrained('../input/my-model/model_insult')\nmodel_identity_hate = TFDistilBertForSequenceClassification.from_pretrained('../input/my-model/model_identity_hate')\nmodel_regression = TFDistilBertForSequenceClassification.from_pretrained('../input/my-model/model_regression')\n\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"../input/tokenizer\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T21:25:43.83432Z","iopub.execute_input":"2022-01-09T21:25:43.834661Z","iopub.status.idle":"2022-01-09T21:26:12.531085Z","shell.execute_reply.started":"2022-01-09T21:25:43.834629Z","shell.execute_reply":"2022-01-09T21:26:12.529962Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\nbatch_size=200\ndef tokenize_test_function(example):\n    return tokenizer(example[\"text\"], truncation=True)\ntest_ds = datasets.Dataset.from_pandas(test_df)\ntokenized_test_ds = test_ds.map(tokenize_test_function, batched=True)\ntf_test_ds = tokenized_test_ds.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=batch_size,\n)\n\ntoxic = tf.math.sigmoid(model_toxic.predict(tf_test_ds).logits)\nsevere_toxic = tf.math.sigmoid(model_severe_toxic.predict(tf_test_ds).logits)\nobscene = tf.math.sigmoid(model_obscene.predict(tf_test_ds).logits)\ninsult = tf.math.sigmoid(model_insult.predict(tf_test_ds).logits)\nidentity_hate = tf.math.sigmoid(model_identity_hate.predict(tf_test_ds).logits)\nregression = model_regression.predict(tf_test_ds).logits\nthreat = tf.math.sigmoid(model_threat.predict(tf_test_ds).logits)\n\n\ntest_df['toxic'] = toxic[:, 0]\ntest_df['severe_toxic'] = severe_toxic[:, 0]\ntest_df['obscene'] = obscene[:, 0]\ntest_df['insult'] = insult[:, 0]\ntest_df['identity_hate'] = identity_hate[:, 0]\ntest_df['regression'] = regression[:, 0]\ntest_df['threat'] = threat[:, 0]\n\n#submission_df = test_df[['comment_id', 'score']]\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T21:26:27.848565Z","iopub.execute_input":"2022-01-09T21:26:27.848863Z","iopub.status.idle":"2022-01-09T21:36:00.29431Z","shell.execute_reply.started":"2022-01-09T21:26:27.848833Z","shell.execute_reply":"2022-01-09T21:36:00.293372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d={'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, 'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\ntest_df['score']=0\nfor k in d:\n    test_df['score'] = test_df.score + d[k]*test_df[k]\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T21:36:50.805658Z","iopub.execute_input":"2022-01-09T21:36:50.805939Z","iopub.status.idle":"2022-01-09T21:36:50.819643Z","shell.execute_reply.started":"2022-01-09T21:36:50.80591Z","shell.execute_reply":"2022-01-09T21:36:50.818468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['score']= test_df['score']*0.6 + test_df['regression']*(5.5/200)*0.9","metadata":{"execution":{"iopub.status.busy":"2022-01-09T21:38:32.280132Z","iopub.execute_input":"2022-01-09T21:38:32.280439Z","iopub.status.idle":"2022-01-09T21:38:32.289421Z","shell.execute_reply.started":"2022-01-09T21:38:32.280408Z","shell.execute_reply":"2022-01-09T21:38:32.288172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = test_df[['comment_id', 'score']]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T21:38:47.054517Z","iopub.execute_input":"2022-01-09T21:38:47.054803Z","iopub.status.idle":"2022-01-09T21:38:47.066767Z","shell.execute_reply.started":"2022-01-09T21:38:47.054772Z","shell.execute_reply":"2022-01-09T21:38:47.065729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T21:38:47.899711Z","iopub.execute_input":"2022-01-09T21:38:47.900325Z","iopub.status.idle":"2022-01-09T21:38:47.937747Z","shell.execute_reply.started":"2022-01-09T21:38:47.900288Z","shell.execute_reply":"2022-01-09T21:38:47.936836Z"},"trusted":true},"execution_count":null,"outputs":[]}]}