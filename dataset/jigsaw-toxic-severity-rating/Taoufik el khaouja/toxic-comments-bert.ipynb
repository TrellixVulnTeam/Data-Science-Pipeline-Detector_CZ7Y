{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classification of toxic comments","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.options.mode.chained_assignment = None\nimport seaborn as sns\n#sns.set(style = 'white')\nimport nltk\n#import string\nfrom textblob import TextBlob\nimport string\nimport re\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_colwidth', 100)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T13:46:00.479763Z","iopub.execute_input":"2022-01-19T13:46:00.480228Z","iopub.status.idle":"2022-01-19T13:46:02.4289Z","shell.execute_reply.started":"2022-01-19T13:46:00.480114Z","shell.execute_reply":"2022-01-19T13:46:02.427894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification,\\\n                              AdamW, get_linear_schedule_with_warmup, DistilBertConfig\nimport torch\nfrom pylab import rcParams\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\nimport tensorflow as tf\nimport datasets \n\nRANDOM_SEED = 331\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\n# Evaluation\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n\n#https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/","metadata":{"execution":{"iopub.status.busy":"2022-01-19T13:46:02.43028Z","iopub.execute_input":"2022-01-19T13:46:02.430486Z","iopub.status.idle":"2022-01-19T13:46:10.561576Z","shell.execute_reply.started":"2022-01-19T13:46:02.430462Z","shell.execute_reply":"2022-01-19T13:46:10.560731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df=pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ndf=pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\n#test_df=pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T13:46:10.563058Z","iopub.execute_input":"2022-01-19T13:46:10.56327Z","iopub.status.idle":"2022-01-19T13:46:12.579752Z","shell.execute_reply.started":"2022-01-19T13:46:10.563246Z","shell.execute_reply":"2022-01-19T13:46:12.578947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T13:47:03.72233Z","iopub.execute_input":"2022-01-19T13:47:03.722646Z","iopub.status.idle":"2022-01-19T13:47:03.740013Z","shell.execute_reply.started":"2022-01-19T13:47:03.722615Z","shell.execute_reply":"2022-01-19T13:47:03.738775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-19T13:49:15.944798Z","iopub.execute_input":"2022-01-19T13:49:15.945062Z","iopub.status.idle":"2022-01-19T13:49:15.952104Z","shell.execute_reply.started":"2022-01-19T13:49:15.945028Z","shell.execute_reply":"2022-01-19T13:49:15.95092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is six targets to be predicted: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nWe can create one multi-label-classification model or six binary-classification models.","metadata":{}},{"cell_type":"markdown","source":"# Target analysis","metadata":{}},{"cell_type":"markdown","source":"Here we will study the ditribution of the data on each target","metadata":{}},{"cell_type":"code","source":"df.toxic.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T13:51:43.809341Z","iopub.execute_input":"2022-01-19T13:51:43.809653Z","iopub.status.idle":"2022-01-19T13:51:43.824763Z","shell.execute_reply.started":"2022-01-19T13:51:43.809618Z","shell.execute_reply":"2022-01-19T13:51:43.824028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize=(20, 7))\n\nsns.histplot(data=df, x=\"toxic\", ax=axs[0, 0])\nsns.histplot(data=df, x=\"severe_toxic\",ax=axs[0, 1])\nsns.histplot(data=df, x=\"obscene\", ax=axs[0, 2])\nsns.histplot(data=df, x=\"threat\", ax=axs[1, 0])\nsns.histplot(data=df, x=\"insult\", ax=axs[1, 1])\nsns.histplot(data=df, x=\"identity_hate\", ax=axs[1, 2])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:17:29.800512Z","iopub.execute_input":"2022-01-19T10:17:29.800829Z","iopub.status.idle":"2022-01-19T10:17:30.877189Z","shell.execute_reply.started":"2022-01-19T10:17:29.800792Z","shell.execute_reply":"2022-01-19T10:17:30.876415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df0=pd.concat([df[(df.toxic==1)],\n              df[(df.toxic==0)].sample(15294)\n             ])\n\nfig, axs = plt.subplots(2, 3, figsize=(20, 7))\n\nsns.histplot(data=df0, x=\"toxic\", ax=axs[0, 0])\nsns.histplot(data=df0, x=\"severe_toxic\",ax=axs[0, 1])\nsns.histplot(data=df0, x=\"obscene\", ax=axs[0, 2])\nsns.histplot(data=df0, x=\"threat\", ax=axs[1, 0])\nsns.histplot(data=df0, x=\"insult\", ax=axs[1, 1])\nsns.histplot(data=df0, x=\"identity_hate\", ax=axs[1, 2])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:18:36.810636Z","iopub.execute_input":"2022-01-19T10:18:36.811573Z","iopub.status.idle":"2022-01-19T10:18:38.224701Z","shell.execute_reply.started":"2022-01-19T10:18:36.811526Z","shell.execute_reply":"2022-01-19T10:18:38.223588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text analysis","metadata":{}},{"cell_type":"code","source":"from nltk import word_tokenize\n\ndf['len_tokenized_sents'] = df.apply(lambda row: len(word_tokenize(row['comment_text'])), axis=1)\nsns.histplot(data=df, x=\"len_tokenized_sents\")","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:22:58.977177Z","iopub.execute_input":"2022-01-19T10:22:58.977446Z","iopub.status.idle":"2022-01-19T10:24:47.508352Z","shell.execute_reply.started":"2022-01-19T10:22:58.977416Z","shell.execute_reply":"2022-01-19T10:24:47.507521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"comment_text\"][np.argmax(df['len_tokenized_sents'])]","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:26:12.606444Z","iopub.execute_input":"2022-01-19T10:26:12.607434Z","iopub.status.idle":"2022-01-19T10:26:12.613932Z","shell.execute_reply.started":"2022-01-19T10:26:12.607381Z","shell.execute_reply":"2022-01-19T10:26:12.613233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we need a to preprocess the text","metadata":{}},{"cell_type":"markdown","source":"# Text preprocessing","metadata":{}},{"cell_type":"code","source":"def text_cleaning(text):\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n\n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\ndef clean(data, col):\n    \n    data[col] = data[col].str.replace('https?://\\S+|www\\.\\S+', ' social medium ', regex=True)      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\", regex=False) \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\", regex=False)\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \", regex=False)\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \", regex=False)\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you', regex=True)\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace('\\s+', ' ', regex=True)  # will remove more than one whitespace character\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1', regex=True) # 2 or more characters are replaced by 2 characters\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\"[:|♣|'|§|♠|*|/|?|=|%|&|-|#|•|~|^|>|<|►|_]\", '', regex=True)\n    \n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \", regex=False)\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'s\", \" \", regex=False)\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', regex=True)\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', regex=True)    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ', regex=True)    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    tqdm.pandas()\n    data[col] = data[col].progress_apply(text_cleaning)\n    data[col] = data[col].apply(lambda x: x.lower())\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:59:07.120876Z","iopub.execute_input":"2022-01-19T11:59:07.121148Z","iopub.status.idle":"2022-01-19T11:59:07.177788Z","shell.execute_reply.started":"2022-01-19T11:59:07.121116Z","shell.execute_reply":"2022-01-19T11:59:07.176824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_df=clean(test_df,'text')\ndf=clean(df, 'comment_text')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:59:17.262766Z","iopub.execute_input":"2022-01-19T11:59:17.263405Z","iopub.status.idle":"2022-01-19T12:01:35.57581Z","shell.execute_reply.started":"2022-01-19T11:59:17.263362Z","shell.execute_reply":"2022-01-19T12:01:35.575045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['len_tokenized_sents'] = df.apply(lambda row: len(word_tokenize(row['comment_text'])), axis=1)\nsns.histplot(data=df, x=\"len_tokenized_sents\")","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:33:20.372975Z","iopub.execute_input":"2022-01-19T10:33:20.37395Z","iopub.status.idle":"2022-01-19T10:34:14.971637Z","shell.execute_reply.started":"2022-01-19T10:33:20.373897Z","shell.execute_reply":"2022-01-19T10:34:14.97066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"comment_text\"][np.argmax(df['len_tokenized_sents'])]","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:34:43.969244Z","iopub.execute_input":"2022-01-19T10:34:43.969524Z","iopub.status.idle":"2022-01-19T10:34:43.97626Z","shell.execute_reply.started":"2022-01-19T10:34:43.969494Z","shell.execute_reply":"2022-01-19T10:34:43.975414Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df[df['len_tokenized_sents']>200])/len(df)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:38:42.744662Z","iopub.execute_input":"2022-01-19T10:38:42.74511Z","iopub.status.idle":"2022-01-19T10:38:42.75706Z","shell.execute_reply.started":"2022-01-19T10:38:42.745059Z","shell.execute_reply":"2022-01-19T10:38:42.756281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# prepare data for training","metadata":{}},{"cell_type":"code","source":"df=pd.concat([df[(df.toxic==1)],\n              df[(df.toxic==0)].sample(15294)\n             ])\ndf['target']=df['toxic']\n\ndf.reset_index(inplace=True)\ndf_train, df_valid = train_test_split(df, test_size=0.1, random_state=42)\ndf_valid, df_test = train_test_split(df_valid, test_size=0.5, random_state=42)\n\ntrain_ds = datasets.Dataset.from_pandas(df_train[['comment_text','target']])\nvalid_ds = datasets.Dataset.from_pandas(df_valid[['comment_text','target']])\n\ncheckpoint = \"distilbert-base-uncased\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n\ndef tokenize_function(example):\n    return tokenizer(example[\"comment_text\"], truncation=True)\n\ntokenized_train_ds = train_ds.map(tokenize_function, batched=True)\ntokenized_valid_ds = valid_ds.map(tokenize_function, batched=True)\n\nbatch_size = 16\n\ndata_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n\ntf_train_ds = tokenized_train_ds.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    label_cols=[\"target\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=batch_size,\n)\n\ntf_valid_ds = tokenized_valid_ds.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    label_cols=[\"target\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=batch_size,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T12:01:41.32272Z","iopub.execute_input":"2022-01-19T12:01:41.322985Z","iopub.status.idle":"2022-01-19T12:01:55.018662Z","shell.execute_reply.started":"2022-01-19T12:01:41.322955Z","shell.execute_reply":"2022-01-19T12:01:55.017887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP model, BERT","metadata":{}},{"cell_type":"code","source":"from transformers import TFAutoModelForSequenceClassification\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T14:30:05.569213Z","iopub.execute_input":"2022-01-09T14:30:05.569457Z","iopub.status.idle":"2022-01-09T14:30:20.641795Z","shell.execute_reply.started":"2022-01-09T14:30:05.569425Z","shell.execute_reply":"2022-01-09T14:30:20.641073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 2\nnum_train_steps = len(tf_train_ds) * num_epochs\n\nlr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n)\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T14:30:20.643058Z","iopub.execute_input":"2022-01-09T14:30:20.643907Z","iopub.status.idle":"2022-01-09T14:30:20.671203Z","shell.execute_reply.started":"2022-01-09T14:30:20.643869Z","shell.execute_reply":"2022-01-09T14:30:20.670557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit_history = model.fit(tf_train_ds,\n                        epochs=1,\n                        validation_data=tf_valid_ds,\n                        verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T14:30:20.673854Z","iopub.execute_input":"2022-01-09T14:30:20.67412Z","iopub.status.idle":"2022-01-09T14:31:53.415166Z","shell.execute_reply.started":"2022-01-09T14:30:20.674093Z","shell.execute_reply":"2022-01-09T14:31:53.414323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"./model_identity_hate\")","metadata":{"execution":{"iopub.status.busy":"2022-01-09T14:32:54.244244Z","iopub.execute_input":"2022-01-09T14:32:54.244526Z","iopub.status.idle":"2022-01-09T14:32:54.778849Z","shell.execute_reply.started":"2022-01-09T14:32:54.244495Z","shell.execute_reply":"2022-01-09T14:32:54.778117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test model","metadata":{}},{"cell_type":"code","source":"from transformers import TFDistilBertForSequenceClassification\n\nmodel = TFDistilBertForSequenceClassification.from_pretrained('../input/d/taoufikelkhaouja/my-model/toxic')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T12:02:01.880518Z","iopub.execute_input":"2022-01-19T12:02:01.880775Z","iopub.status.idle":"2022-01-19T12:02:07.200195Z","shell.execute_reply.started":"2022-01-19T12:02:01.880746Z","shell.execute_reply":"2022-01-19T12:02:07.199411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = datasets.Dataset.from_pandas(df_test)\ntokenized_test_ds = test_ds.map(tokenize_function, batched=True)\ntf_test_ds = tokenized_test_ds.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=batch_size,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T12:02:11.538517Z","iopub.execute_input":"2022-01-19T12:02:11.539378Z","iopub.status.idle":"2022-01-19T12:02:12.188958Z","shell.execute_reply.started":"2022-01-19T12:02:11.539332Z","shell.execute_reply":"2022-01-19T12:02:12.188307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_result = model.predict(tf_test_ds)\nresult = raw_result.logits","metadata":{"execution":{"iopub.status.busy":"2022-01-19T12:02:13.955743Z","iopub.execute_input":"2022-01-19T12:02:13.955998Z","iopub.status.idle":"2022-01-19T12:02:28.102334Z","shell.execute_reply.started":"2022-01-19T12:02:13.95597Z","shell.execute_reply":"2022-01-19T12:02:28.101502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['score'] = tf.math.sigmoid(result)[:, 0]\ndf_test['score'] = df_test['score'].apply(lambda x: 0 if x<0.5 else 1)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T12:03:31.917578Z","iopub.execute_input":"2022-01-19T12:03:31.917848Z","iopub.status.idle":"2022-01-19T12:03:31.929179Z","shell.execute_reply.started":"2022-01-19T12:03:31.917818Z","shell.execute_reply":"2022-01-19T12:03:31.928244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(df_test['target'],df_test['score'])","metadata":{"execution":{"iopub.status.busy":"2022-01-19T12:03:35.04286Z","iopub.execute_input":"2022-01-19T12:03:35.043137Z","iopub.status.idle":"2022-01-19T12:03:35.056248Z","shell.execute_reply.started":"2022-01-19T12:03:35.043106Z","shell.execute_reply":"2022-01-19T12:03:35.055367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# hhh","metadata":{}},{"cell_type":"code","source":"def tokenize_test_function(example):\n    return tokenizer(example[\"text\"], truncation=True)\ntest_ds = datasets.Dataset.from_pandas(test_df)\ntokenized_test_ds = test_ds.map(tokenize_test_function, batched=True)\ntf_test_ds = tokenized_test_ds.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=batch_size,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T10:15:45.969262Z","iopub.execute_input":"2022-01-09T10:15:45.969524Z","iopub.status.idle":"2022-01-09T10:15:47.770187Z","shell.execute_reply.started":"2022-01-09T10:15:45.969495Z","shell.execute_reply":"2022-01-09T10:15:47.769538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_result = model.predict(tf_test_ds)\nresult = raw_result.logits\ntest_df['pred'] = tf.math.sigmoid(result)[:, 0]\ntest_df['pred'] = test_df['score'].apply(lambda x: 0 if x<0.5 else 1)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T10:16:03.60343Z","iopub.execute_input":"2022-01-09T10:16:03.60404Z","iopub.status.idle":"2022-01-09T10:17:26.780154Z","shell.execute_reply.started":"2022-01-09T10:16:03.604002Z","shell.execute_reply":"2022-01-09T10:17:26.779389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.math.sigmoid(result)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T10:19:25.618649Z","iopub.execute_input":"2022-01-09T10:19:25.619426Z","iopub.status.idle":"2022-01-09T10:19:25.629827Z","shell.execute_reply.started":"2022-01-09T10:19:25.619383Z","shell.execute_reply":"2022-01-09T10:19:25.628783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['score'] = tf.math.sigmoid(result)[:, 0]\ntest_df['score'] = test_df['score'].apply(lambda x: 0 if x<0.5 else 1)\nsubmission_df = test_df[['comment_id', 'score']]\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T10:20:47.134374Z","iopub.execute_input":"2022-01-09T10:20:47.134684Z","iopub.status.idle":"2022-01-09T10:20:47.151647Z","shell.execute_reply.started":"2022-01-09T10:20:47.134651Z","shell.execute_reply":"2022-01-09T10:20:47.150542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['score'].sort_values()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T10:21:06.703562Z","iopub.execute_input":"2022-01-09T10:21:06.70413Z","iopub.status.idle":"2022-01-09T10:21:06.713591Z","shell.execute_reply.started":"2022-01-09T10:21:06.704093Z","shell.execute_reply":"2022-01-09T10:21:06.712915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.sort_values('score')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T10:21:31.954528Z","iopub.execute_input":"2022-01-09T10:21:31.95508Z","iopub.status.idle":"2022-01-09T10:21:31.977365Z","shell.execute_reply.started":"2022-01-09T10:21:31.955043Z","shell.execute_reply":"2022-01-09T10:21:31.976648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission1.csv\", index=False)\nsub=submission_df\nsub.to_csv('submitions.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T13:00:18.015746Z","iopub.execute_input":"2022-01-02T13:00:18.016011Z","iopub.status.idle":"2022-01-02T13:00:18.067995Z","shell.execute_reply.started":"2022-01-02T13:00:18.015982Z","shell.execute_reply":"2022-01-02T13:00:18.067328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighting(target):\n    if target<0.1:\n        return 1/1.25\n    elif target<0.2:\n        return 1/0.15\n    elif target<0.3:\n        return 1/0.1\n    elif target<0.4:\n        return 1/0.07\n    else:\n        return 0\n\ndf['weights']=df['target'].apply(weighting)\ndf.weights=df.weights/df.weights.sum()\n\ndf_0=df[df.target<=0.4]\ndf_1=df[df.target>0.4]\ndf_0=df_0.sample(n=700000, weights=df_0.weights)\ndf=pd.concat([df_0,df_1])\ndel df_0\ndel df_1\n\nfor i in range(100):\n    l=len(df[(df.target<(i+1)/100) & (df.target>=(i)/100)])\n    df=pd.concat([df[(df.target>=(i+1)/100) | (df.target<(i)/100)],\n                  df[(df.target<(i+1)/100) & (df.target>=(i)/100)].sample(n=min(l,5000))\n                  ])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T13:29:46.776742Z","iopub.execute_input":"2022-01-02T13:29:46.777223Z","iopub.status.idle":"2022-01-02T13:30:16.033428Z","shell.execute_reply.started":"2022-01-02T13:29:46.777178Z","shell.execute_reply":"2022-01-02T13:30:16.03266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.concat([df[(df.toxic==1)&(df.identity_hate==1)],\n              df[(df.toxic==1)&(df.threat==0)].sample(2000),\n              df[(df.toxic==0)&(df.threat==0)].sample(300),\n              df[(df.toxic==0)&(df.threat==1)],\n             ])\n\ndf['target']=df.identity_hate\n\n#d={'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, 'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n#df['target']=0\n#for k in d:\n#    df['target'] = df.target + d[k]*df[k]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T14:29:15.6368Z","iopub.execute_input":"2022-01-09T14:29:15.637339Z","iopub.status.idle":"2022-01-09T14:29:15.684119Z","shell.execute_reply.started":"2022-01-09T14:29:15.637299Z","shell.execute_reply":"2022-01-09T14:29:15.683323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d={'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, 'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\ndf['target']=0\nfor k in d:\n    df['target'] = df.target + d[k]*df[k]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T11:21:56.465739Z","iopub.execute_input":"2022-01-09T11:21:56.46604Z","iopub.status.idle":"2022-01-09T11:21:56.490407Z","shell.execute_reply.started":"2022-01-09T11:21:56.466008Z","shell.execute_reply":"2022-01-09T11:21:56.489522Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d={'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, 'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\ndf['target']=0\nfor k in d:\n    df['target'] = df.target + d[k]*df[k]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T11:21:56.465739Z","iopub.execute_input":"2022-01-09T11:21:56.46604Z","iopub.status.idle":"2022-01-09T11:21:56.490407Z","shell.execute_reply.started":"2022-01-09T11:21:56.466008Z","shell.execute_reply":"2022-01-09T11:21:56.489522Z"},"trusted":true},"execution_count":null,"outputs":[]}]}