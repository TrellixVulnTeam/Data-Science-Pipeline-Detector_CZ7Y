{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport copy\nimport pickle\nimport random\nfrom tqdm.auto import tqdm\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom gensim.models import KeyedVectors, FastText\n\n#torch packages\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n#transformer packages\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import AdamW\nfrom transformers import get_scheduler\nfrom transformers import AutoTokenizer, AutoModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-16T11:58:30.029927Z","iopub.execute_input":"2022-01-16T11:58:30.031009Z","iopub.status.idle":"2022-01-16T11:58:30.040577Z","shell.execute_reply.started":"2022-01-16T11:58:30.030921Z","shell.execute_reply":"2022-01-16T11:58:30.03992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=5080):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nclass JigsawTestDataset(Dataset):\n    def __init__(\n        self, df, max_length, tokenizer = None, use_tfidf=False, \n        tfidf_matrix=None, use_sentence_embedding=False, embed_matrix=None\n    ):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n        self.use_tfidf = use_tfidf\n        self.use_sentence_embedding = use_sentence_embedding\n        if use_tfidf:\n            self.tfidf_matrix = tfidf_matrix\n        elif use_sentence_embedding:\n            self.embed_matrix = embed_matrix\n            \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs_text = self.tokenizer.encode_plus(\n                                text,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        text_ids = inputs_text['input_ids']\n        text_mask = inputs_text['attention_mask']\n        \n        if self.use_tfidf:\n            tfidf = self.tfidf_matrix[index]\n            return {\n                'text_ids': torch.tensor(text_ids, dtype=torch.long),\n                'text_mask': torch.tensor(text_mask, dtype=torch.long),\n                'tfidf': torch.tensor(tfidf, dtype=torch.long)\n            }\n        elif use_sentence_embedding:\n            sent_embed = self.embed_matrix[index]\n            return {\n                'text_ids': torch.tensor(text_ids, dtype=torch.long),\n                'text_mask': torch.tensor(text_mask, dtype=torch.long),\n                'sent_embed': torch.tensor(sent_embed, dtype=torch.long)\n            }\n        else:\n            return {\n                'text_ids': torch.tensor(text_ids, dtype=torch.long),\n                'text_mask': torch.tensor(text_mask, dtype=torch.long)\n            }","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:58:30.877896Z","iopub.execute_input":"2022-01-16T11:58:30.87877Z","iopub.status.idle":"2022-01-16T11:58:30.896779Z","shell.execute_reply.started":"2022-01-16T11:58:30.878716Z","shell.execute_reply":"2022-01-16T11:58:30.895689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NN(nn.Module):\n    def __init__(\n        self, bert_drop_out, HID_DIM=768, tfidf_len=0, use_tfidf=False, \n        use_sentence_embedding=False, embed_len=0\n    ):\n        super().__init__()\n        if use_tfidf:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768+tfidf_len, 1)\n            )\n        elif use_sentence_embedding:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768+embed_len, 1)      \n            )\n        else:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768, 1)\n            )\n            \n    def forward(self, x):\n        score = self.net(x)\n        return score\n                \nclass JigsawModel(nn.Module):\n    def __init__(self, BERT, NN):\n        super(JigsawModel, self).__init__()\n        self.bert = BERT\n        self.fc = NN\n        \n    def forward(\n        self, ids, mask, tfidf_vec=None, use_tfidf=False, \n        sent_embed=None, use_sentence_embedding=False\n    ):        \n        out = self.bert(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        if use_tfidf:\n            fc_in = torch.cat(\n                (out[\"pooler_output\"], tfidf_vec), dim=1\n            )\n        elif use_sentence_embedding:\n            fc_in = torch.cat(\n                (out[\"pooler_output\"], sent_embed), dim=1\n            )        \n        else:\n            fc_in = out[\"pooler_output\"]\n        outputs = self.fc(fc_in)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:58:31.516659Z","iopub.execute_input":"2022-01-16T11:58:31.516948Z","iopub.status.idle":"2022-01-16T11:58:31.531112Z","shell.execute_reply.started":"2022-01-16T11:58:31.516916Z","shell.execute_reply":"2022-01-16T11:58:31.53034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_corpus(df_test):\n    return df_test[\"text\"].to_list()\n\ndef tokenize_test_by_bert_tokenizer(bert_tokenizer, corpus):\n    corpus_tokenized = [\n        bert_tokenizer.tokenize(sentence) for sentence in corpus\n    ]\n    return corpus_tokenized\n\ndef testCorpus2tfidf(tfidf_transfomer, corpus_tokenized):\n    tfidf_matrix_sparse = tfidf_transfomer.transform(corpus_tokenized)\n    tfidf_matrix = tfidf_matrix_sparse.toarray()\n    return tfidf_matrix\n\ndef identity_tokenizer(text):\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:58:32.093418Z","iopub.execute_input":"2022-01-16T11:58:32.09436Z","iopub.status.idle":"2022-01-16T11:58:32.100808Z","shell.execute_reply.started":"2022-01-16T11:58:32.094281Z","shell.execute_reply":"2022-01-16T11:58:32.099968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(5080)\ndata_test = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nPRETRAINED_MODEL_NAME = \"../input/transformers/roberta-base\"\nbert_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:58:32.709863Z","iopub.execute_input":"2022-01-16T11:58:32.710279Z","iopub.status.idle":"2022-01-16T11:58:32.93345Z","shell.execute_reply.started":"2022-01-16T11:58:32.710247Z","shell.execute_reply":"2022-01-16T11:58:32.932569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TFIDF_PATH = \"../input/temp-model/tfidf_roberta_obj (1).pickle\"\nwith open(TFIDF_PATH, 'rb') as f:\n    tfidf_transformer = pickle.load(f)\n    \ntest_corpus = create_test_corpus(data_test)\ncorpus_tokenized = tokenize_test_by_bert_tokenizer(bert_tokenizer, test_corpus)\ntfidf_matrix = testCorpus2tfidf(tfidf_transformer, corpus_tokenized)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:58:42.007909Z","iopub.execute_input":"2022-01-16T11:58:42.008184Z","iopub.status.idle":"2022-01-16T11:58:48.536954Z","shell.execute_reply.started":"2022-01-16T11:58:42.008156Z","shell.execute_reply":"2022-01-16T11:58:48.536101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token2idx = tfidf_transformer.vocabulary_\ntoken_list = list(\n    tfidf_transformer.vocabulary_.keys()\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:58:48.538965Z","iopub.execute_input":"2022-01-16T11:58:48.539282Z","iopub.status.idle":"2022-01-16T11:58:48.545066Z","shell.execute_reply.started":"2022-01-16T11:58:48.539241Z","shell.execute_reply":"2022-01-16T11:58:48.544078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:58:48.546591Z","iopub.execute_input":"2022-01-16T11:58:48.546911Z","iopub.status.idle":"2022-01-16T11:58:48.560528Z","shell.execute_reply.started":"2022-01-16T11:58:48.54687Z","shell.execute_reply":"2022-01-16T11:58:48.559853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build sentence embedding","metadata":{}},{"cell_type":"code","source":"# fmodel = FastText.load(\n#     '../input/jigsaw-regression-based-data/FastText-jigsaw-256D/Jigsaw-Fasttext-Word-Embeddings-256D.bin'\n# )","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:58:48.562003Z","iopub.execute_input":"2022-01-16T11:58:48.562361Z","iopub.status.idle":"2022-01-16T11:59:20.430966Z","shell.execute_reply.started":"2022-01-16T11:58:48.562333Z","shell.execute_reply":"2022-01-16T11:59:20.430161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# w2v_embed_dim = 256\n# w2v = np.zeros(\n#     (len(token2idx), w2v_embed_dim)\n# )\n# for tok in token_list:\n#     token_idx = token2idx[tok]\n#     w2v[token_idx] = fmodel.wv[tok]","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:59:20.431984Z","iopub.execute_input":"2022-01-16T11:59:20.432181Z","iopub.status.idle":"2022-01-16T11:59:22.100331Z","shell.execute_reply.started":"2022-01-16T11:59:20.432156Z","shell.execute_reply":"2022-01-16T11:59:22.099422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence_embedding = np.dot(\n#     tfidf_matrix, w2v\n# )","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:23:22.689991Z","iopub.execute_input":"2022-01-16T10:23:22.690203Z","iopub.status.idle":"2022-01-16T10:23:25.665569Z","shell.execute_reply.started":"2022-01-16T10:23:22.690177Z","shell.execute_reply":"2022-01-16T10:23:25.664603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence_embedding.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:23:25.668312Z","iopub.execute_input":"2022-01-16T10:23:25.668996Z","iopub.status.idle":"2022-01-16T10:23:25.676791Z","shell.execute_reply.started":"2022-01-16T10:23:25.668943Z","shell.execute_reply":"2022-01-16T10:23:25.675982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and Dataloader","metadata":{}},{"cell_type":"code","source":"batch_size = 16\nmax_token_length = 128\nuse_tfidf = True\nuse_sentence_embedding = False\nif use_tfidf:\n    tfidf_len = tfidf_matrix.shape[1]\n    test_dataset = JigsawTestDataset(\n        data_test, max_length=max_token_length, tokenizer=bert_tokenizer,\n        use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n    )\nelif use_sentence_embedding:\n    embed_len = sentence_embedding.shape[1]\n    test_dataset = JigsawTestDataset(\n        data_test, max_length=max_token_length, tokenizer=bert_tokenizer,\n        use_sentence_embedding=use_sentence_embedding, embed_matrix=sentence_embedding\n    )\nelse:\n    test_dataset = JigsawTestDataset(\n        data_test, max_length=max_token_length, tokenizer=bert_tokenizer\n    )\n\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False,\n    num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:59:22.102124Z","iopub.execute_input":"2022-01-16T11:59:22.102352Z","iopub.status.idle":"2022-01-16T11:59:22.115387Z","shell.execute_reply.started":"2022-01-16T11:59:22.102317Z","shell.execute_reply":"2022-01-16T11:59:22.114266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR = 1e-5\nEPOCH = 10\nHID_DIM = 768\nMARGIN = 0.5\nDATE = \"0115\"\nLR = 1e-5\nWD = 0\nBDR = 0.2\n\nMODEL_PATH = f\"../input/temp-model/0116_roberta_LR_0.0001_WD_1e-06_BDR_0.3.pth\"\n\nbert = AutoModel.from_pretrained(PRETRAINED_MODEL_NAME).to(device)\nif use_tfidf:\n    dnn = NN(\n        BDR, HID_DIM, \n        tfidf_len, use_tfidf\n    ).to(device)\nelif use_sentence_embedding:\n    dnn = NN(BDR, HID_DIM, \n             embed_len=embed_len, use_sentence_embedding=True\n            ).to(device)   \nelse:\n    dnn = NN(\n        BDR, HID_DIM\n    ).to(device)\n\nmodel = JigsawModel(bert, dnn).to(device)\nif torch.cuda.is_available():\n    checkpoint = torch.load(MODEL_PATH)\nelse:\n    checkpoint = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\nmodel.bert.load_state_dict(checkpoint[\"BERT\"])\nmodel.fc.load_state_dict(checkpoint[\"NN\"])\n# bert.load_state_dict(checkpoint[\"BERT\"])\n# dnn.load_state_dict(checkpoint[\"NN\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:59:22.117094Z","iopub.execute_input":"2022-01-16T11:59:22.118035Z","iopub.status.idle":"2022-01-16T11:59:32.277381Z","shell.execute_reply.started":"2022-01-16T11:59:22.117986Z","shell.execute_reply":"2022-01-16T11:59:32.27652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check training set accuracy \n## (Please annotate the following sections before submit to competition)","metadata":{}},{"cell_type":"code","source":"# class JigsawDataset(Dataset):\n#     def __init__(self, df, tokenizer, max_length, use_tfidf=False, tfidf_matrix=None):\n#         self.df = df\n#         self.max_len = max_length\n#         self.tokenizer = tokenizer\n#         self.more_toxic = df['more_toxic'].values\n#         self.less_toxic = df['less_toxic'].values\n#         self.use_tfidf = use_tfidf\n#         if use_tfidf:\n#             self.more_toxic_tfidf_idx = df['more_toxic_tfidf_idx'].values\n#             self.less_toxic_tfidf_idx = df['less_toxic_tfidf_idx'].values\n#             self.tfidf_matrix = tfidf_matrix\n\n#     def __len__(self):\n#         return len(self.df)\n    \n#     def __getitem__(self, index):\n#         more_toxic = self.more_toxic[index]\n#         less_toxic = self.less_toxic[index]\n#         inputs_more_toxic = self.tokenizer.encode_plus(\n#                                 more_toxic,\n#                                 truncation=True,\n#                                 add_special_tokens=True,\n#                                 max_length=self.max_len,\n#                                 padding='max_length'\n#                             )\n#         inputs_less_toxic = self.tokenizer.encode_plus(\n#                                 less_toxic,\n#                                 truncation=True,\n#                                 add_special_tokens=True,\n#                                 max_length=self.max_len,\n#                                 padding='max_length'\n#                             )\n#         target = 1\n        \n#         more_toxic_ids = inputs_more_toxic['input_ids']\n#         more_toxic_mask = inputs_more_toxic['attention_mask']        \n#         less_toxic_ids = inputs_less_toxic['input_ids']\n#         less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n#         if self.use_tfidf:\n#             more_toxic_tfidf_idx = self.more_toxic_tfidf_idx[index]\n#             less_toxic_tfidf_idx = self.less_toxic_tfidf_idx[index]\n#             more_toxic_tfidf = self.tfidf_matrix[more_toxic_tfidf_idx]\n#             less_toxic_tfidf = self.tfidf_matrix[less_toxic_tfidf_idx]\n#             return {\n#                 'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n#                 'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n#                 'more_toxic_tfidf': torch.tensor(more_toxic_tfidf, dtype=torch.long),\n#                 'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n#                 'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n#                 'less_toxic_tfidf': torch.tensor(less_toxic_tfidf, dtype=torch.long),\n#                 'target': torch.tensor(target, dtype=torch.long)\n#             }\n#         else:\n#             return {\n#                 'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n#                 'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n#                 'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n#                 'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n#                 'target': torch.tensor(target, dtype=torch.long)\n#             }","metadata":{"execution":{"iopub.status.busy":"2022-01-09T02:40:27.542632Z","iopub.execute_input":"2022-01-09T02:40:27.542912Z","iopub.status.idle":"2022-01-09T02:40:27.55826Z","shell.execute_reply.started":"2022-01-09T02:40:27.542878Z","shell.execute_reply":"2022-01-09T02:40:27.557402Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def validate_all_combine(\n#     model, criterion, \n#     valid_loader, device, use_tfidf=False\n# ):\n#     epoch_loss = 0\n#     y_preds = []\n    \n#     model.eval()\n#     with torch.no_grad():\n#         for data in valid_loader:\n#             more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n#             more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n#             less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n#             less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n#             targets = data['target'].to(device, dtype=torch.long)\n            \n#             if use_tfidf:\n#                 more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n#                 less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n#                 more_out = model(more_toxic_ids, more_toxic_mask, more_toxic_tfidf, use_tfidf=True)\n#                 less_out = model(less_toxic_ids, less_toxic_mask, less_toxic_tfidf, use_tfidf=True)\n#             else:\n#                 more_out = model(more_toxic_ids, more_toxic_mask)\n#                 less_out = model(less_toxic_ids, less_toxic_mask)\n            \n#             loss = criterion(more_out, less_out, targets)\n\n#             epoch_loss += loss.item()\n#             for i in range(len(data['more_toxic_ids'])):\n#                 y_preds.append([less_out[i].item(), more_out[i].item()])\n#         df_score = pd.DataFrame(y_preds,columns=['less','more'])\n#         accuracy = validate_accuracy(df_score)\n#     return df_score, accuracy, (epoch_loss / len(valid_loader))\n\n# def validate_accuracy(df_score):\n#     return len(df_score[df_score['less'] < df_score['more']]) / len(df_score)\n\n# def return_wrong_text(df_score, df_valid):\n#     df_score_text = pd.concat((df_valid.reset_index().drop('index',axis=1),df_score),axis=1)\n#     return df_score_text[df_score_text['less'] > df_score_text['more']]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T02:40:28.070583Z","iopub.execute_input":"2022-01-09T02:40:28.070841Z","iopub.status.idle":"2022-01-09T02:40:28.083822Z","shell.execute_reply.started":"2022-01-09T02:40:28.070811Z","shell.execute_reply":"2022-01-09T02:40:28.082955Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def remove_duplicates(df, used_col):\n#     \"\"\"Combine `less_toxic` text and `more_toxic` text,\n#     then remove duplicate pair of comments while keeping the last pair\n#     \"\"\"\n#     df[\"combine\"] = df[\"less_toxic\"] + df[\"more_toxic\"]\n#     df = df.drop_duplicates(subset=used_col, keep=\"last\")\n#     return df","metadata":{"execution":{"iopub.status.busy":"2022-01-09T02:40:40.004758Z","iopub.execute_input":"2022-01-09T02:40:40.005036Z","iopub.status.idle":"2022-01-09T02:40:40.009747Z","shell.execute_reply.started":"2022-01-09T02:40:40.005006Z","shell.execute_reply":"2022-01-09T02:40:40.008917Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_train = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n# #data_train_remove_duplicates = remove_duplicates(data_train, \"combine\")\n\n# train_dataset = JigsawDataset(\n#         data_train, tokenizer=bert_tokenizer, \n#         max_length=max_token_length, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n#     )\n\n# train_loader = DataLoader(\n#         train_dataset, batch_size=batch_size, shuffle=True,\n#         num_workers=2)\n    \n# criterion = nn.MarginRankingLoss(margin=MARGIN)\n\n# df_score, valid_acc, valid_loss = validate_all_combine(\n#     model, criterion, all_loader, \n#     device, use_tfidf\n# )\n\n# print(f\"Recheck: Accuracy = {valid_acc}, loss = {valid_loss}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-09T02:48:35.50283Z","iopub.execute_input":"2022-01-09T02:48:35.503119Z","iopub.status.idle":"2022-01-09T02:48:35.783957Z","shell.execute_reply.started":"2022-01-09T02:48:35.503085Z","shell.execute_reply":"2022-01-09T02:48:35.782998Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def predict_combine(\n    model, test_loader, device, \n    use_tfidf=False, use_sentence_embedding=False\n):\n    predict = []\n    with torch.no_grad():\n        model.eval()\n        for data in test_loader:\n            text_ids = data['text_ids'].to(device, dtype = torch.long)\n            text_mask = data['text_mask'].to(device, dtype = torch.long)\n            if use_tfidf:\n                text_tfidf = data['tfidf'].to(device, dtype = torch.long)\n                score = model(text_ids, text_mask, text_tfidf, use_tfidf)\n            elif use_sentence_embedding:\n                text_sent_embed = data[\"sent_embed\"].to(device, dtype = torch.long)\n                score = model(\n                    text_ids, text_mask, \n                    sent_embed=text_sent_embed, use_sentence_embedding=use_sentence_embedding\n                )\n            else:\n                score = model(text_ids, text_mask)\n            score = score.view(-1).cpu().detach().numpy()\n            for pred in score:\n                predict.append(pred)\n    return predict","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:59:32.27899Z","iopub.execute_input":"2022-01-16T11:59:32.279388Z","iopub.status.idle":"2022-01-16T11:59:32.290123Z","shell.execute_reply.started":"2022-01-16T11:59:32.279344Z","shell.execute_reply":"2022-01-16T11:59:32.289159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = predict_combine(\n    model, test_loader, device, \n    use_tfidf=use_tfidf, use_sentence_embedding=use_sentence_embedding\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T11:59:32.292602Z","iopub.execute_input":"2022-01-16T11:59:32.293229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = np.array(predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total Predictiions: {predict.shape[0]}\")\nprint(f\"Total Unique Predictions: {np.unique(predict).shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:25:04.904448Z","iopub.execute_input":"2022-01-09T13:25:04.904857Z","iopub.status.idle":"2022-01-09T13:25:04.91874Z","shell.execute_reply.started":"2022-01-09T13:25:04.904821Z","shell.execute_reply":"2022-01-09T13:25:04.918051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:25:48.75114Z","iopub.execute_input":"2022-01-09T13:25:48.751916Z","iopub.status.idle":"2022-01-09T13:25:48.803078Z","shell.execute_reply.started":"2022-01-09T13:25:48.751863Z","shell.execute_reply":"2022-01-09T13:25:48.80227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['score'] = predict\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:25:50.496195Z","iopub.execute_input":"2022-01-09T13:25:50.496445Z","iopub.status.idle":"2022-01-09T13:25:50.507009Z","shell.execute_reply.started":"2022-01-09T13:25:50.496415Z","shell.execute_reply":"2022-01-09T13:25:50.506319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['score'] = df['score'].rank(method='first')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:25:52.311286Z","iopub.execute_input":"2022-01-09T13:25:52.311821Z","iopub.status.idle":"2022-01-09T13:25:52.327325Z","shell.execute_reply.started":"2022-01-09T13:25:52.311783Z","shell.execute_reply":"2022-01-09T13:25:52.326654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('text', axis=1, inplace=True)\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:25:53.46004Z","iopub.execute_input":"2022-01-09T13:25:53.460809Z","iopub.status.idle":"2022-01-09T13:25:53.486721Z","shell.execute_reply.started":"2022-01-09T13:25:53.46076Z","shell.execute_reply":"2022-01-09T13:25:53.486046Z"},"trusted":true},"execution_count":null,"outputs":[]}]}