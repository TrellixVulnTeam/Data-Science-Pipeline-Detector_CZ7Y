{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nimport os\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import callbacks, models, layers\nimport matplotlib.pyplot as plt\n\n# tokenization\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:03:01.440781Z","iopub.execute_input":"2022-01-18T04:03:01.441219Z","iopub.status.idle":"2022-01-18T04:03:10.493384Z","shell.execute_reply.started":"2022-01-18T04:03:01.441109Z","shell.execute_reply":"2022-01-18T04:03:10.492355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_WORDS = 25_000\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:03:10.495303Z","iopub.execute_input":"2022-01-18T04:03:10.495575Z","iopub.status.idle":"2022-01-18T04:03:30.545451Z","shell.execute_reply.started":"2022-01-18T04:03:10.495542Z","shell.execute_reply":"2022-01-18T04:03:30.544524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create train data\n\nThe competition was multioutput\n\nWe turn it into a binary toxic/ no-toxic classification","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:03:30.546825Z","iopub.execute_input":"2022-01-18T04:03:30.547108Z","iopub.status.idle":"2022-01-18T04:03:32.95421Z","shell.execute_reply.started":"2022-01-18T04:03:30.547076Z","shell.execute_reply":"2022-01-18T04:03:32.95336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Undersample\n\nThe dataset is very unbalanced. Here we undersample the majority class. Other strategies might work better.","metadata":{}},{"cell_type":"code","source":"min_len = (df['y'] == 1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\ndf = pd.concat([df[df['y'] == 1], df_y0_undersample])\ndf['y'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:03:32.956049Z","iopub.execute_input":"2022-01-18T04:03:32.956312Z","iopub.status.idle":"2022-01-18T04:03:33.004674Z","shell.execute_reply.started":"2022-01-18T04:03:32.956281Z","shell.execute_reply":"2022-01-18T04:03:33.003703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# transform the data","metadata":{}},{"cell_type":"code","source":"# stop_words = stopwords.words(\"english\")\n# # lemmatizer = nltk.stem.WordNetLemmatizer()\n\n# # def lemmatize_text(text):\n# #     return [lemmatizer.lemmatize(w) for w in text]\n\n# def clean(comment):\n#     clean_html = BeautifulSoup(comment).get_text()\n#     clean_non_letters = re.sub(\"[^a-zA-Z]\", \" \", clean_html)\n#     cleaned_lowercase = clean_non_letters.lower()\n#     words = cleaned_lowercase.split()\n#     cleaned_words = [w for w in words if w not in stop_words]\n#     return \" \".join(cleaned_words)\n\n# df['cleaned'] = df['text'].apply(clean)\n# df","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:03:33.006007Z","iopub.execute_input":"2022-01-18T04:03:33.006215Z","iopub.status.idle":"2022-01-18T04:03:46.633009Z","shell.execute_reply.started":"2022-01-18T04:03:33.006191Z","shell.execute_reply":"2022-01-18T04:03:46.631862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = Tokenizer(num_words=MAX_WORDS)\n# tokenizer.fit_on_texts(df.cleaned)\n# total_words = len(tokenizer.word_index) + 1\n# total_words","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:06:48.217388Z","iopub.execute_input":"2022-01-18T04:06:48.217719Z","iopub.status.idle":"2022-01-18T04:06:49.576356Z","shell.execute_reply.started":"2022-01-18T04:06:48.21768Z","shell.execute_reply":"2022-01-18T04:06:49.57536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sequences = tokenizer.texts_to_sequences(df.cleaned)\n# max_sequence_len = max([len(x) for x in sequences])\n# padded_sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n# labels = np.array(df.y)\n# X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=0)\n# print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:07:14.161575Z","iopub.execute_input":"2022-01-18T04:07:14.161952Z","iopub.status.idle":"2022-01-18T04:07:16.002465Z","shell.execute_reply.started":"2022-01-18T04:07:14.161914Z","shell.execute_reply":"2022-01-18T04:07:16.001412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec = TfidfVectorizer()\nX = vec.fit_transform(df.text)\ny = df.y\nprint(X.shape, y.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(X_train.shape, y_train.shape, X_val.shape, y_val.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_dataset(data, labels):\n    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n    dataset = dataset.cache().shuffle(X_train.shape[0] + 1).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset\ntrain_ds = to_dataset(X_train, y_train)\nval_ds = to_dataset(X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:07:23.119156Z","iopub.execute_input":"2022-01-18T04:07:23.119641Z","iopub.status.idle":"2022-01-18T04:07:23.538498Z","shell.execute_reply.started":"2022-01-18T04:07:23.119608Z","shell.execute_reply":"2022-01-18T04:07:23.537078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model","metadata":{}},{"cell_type":"code","source":"LSTM_SIZE = 4\n\ndef lstm_model():\n    model = models.Sequential()\n    model.add(layers.Embedding(total_words, 2, input_length=max_sequence_len - 1))\n    model.add(layers.LSTM(LSTM_SIZE))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n    return model, f'lstm_{LSTM_SIZE}'","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:07:30.012339Z","iopub.execute_input":"2022-01-18T04:07:30.013576Z","iopub.status.idle":"2022-01-18T04:07:30.022381Z","shell.execute_reply.started":"2022-01-18T04:07:30.013501Z","shell.execute_reply":"2022-01-18T04:07:30.021233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenizer_train(model, name):\n    reducer = callbacks.ReduceLROnPlateau(monior='val_loss', factor=0.5, patience=3, mode='min', cooldown=1)\n    stopper = callbacks.EarlyStopping(monitor='val_loss', patience=6, mode='min', restore_best_weights=True)\n    hist = model.fit(train_ds,\n              epochs=100,\n              verbose=1,\n              callbacks=[stopper, reducer],\n              validation_data=val_ds)\n    results = model.evaluate(val_ds)\n#     model.save(f'/kaggle/working/{name}')\n    print(f\"results: {results}, type: {type(results)}\")\n    return hist","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:07:31.930477Z","iopub.execute_input":"2022-01-18T04:07:31.931873Z","iopub.status.idle":"2022-01-18T04:07:31.939805Z","shell.execute_reply.started":"2022-01-18T04:07:31.931817Z","shell.execute_reply":"2022-01-18T04:07:31.938947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, name = lstm_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:07:33.022336Z","iopub.execute_input":"2022-01-18T04:07:33.023036Z","iopub.status.idle":"2022-01-18T04:07:33.475Z","shell.execute_reply.started":"2022-01-18T04:07:33.022984Z","shell.execute_reply":"2022-01-18T04:07:33.473945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"hist = tokenizer_train(model, name)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:07:35.178357Z","iopub.execute_input":"2022-01-18T04:07:35.178625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(3, 1, figsize=(8,8), tight_layout=True)\n    \naxs[0].plot(hist.history['loss'])\naxs[0].plot(hist.history['val_loss'])\naxs[0].set_title('binary_crossentropy Loss')\naxs[0].set_ylabel('Loss')\naxs[0].set_xlabel('Epoch')\naxs[0].legend(['train', 'val'], loc='upper right')\n\naxs[1].plot(hist.history['binary_accuracy'])\naxs[1].plot(hist.history['val_binary_accuracy'])\naxs[1].set_title('binary_accuracy Metric')\naxs[1].set_ylabel('Error')\naxs[1].set_xlabel('Epoch')\naxs[1].legend(['train', 'val'], loc='upper left')\n\naxs[2].plot(hist.history['lr'])\naxs[2].set_title('Learining Rate')\naxs[2].set_ylabel('LR')\naxs[2].set_xlabel('Epoch')\nplt.savefig(f'/kaggle/working/{name}_graphs.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate","metadata":{}},{"cell_type":"code","source":"# df_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n# X_less_toxic = tokenizer.texts_to_sequences(df_val['less_toxic'].apply(clean))\n# X_less_toxic = np.array(pad_sequences(X_less_toxic, maxlen=max_sequence_len, padding='pre'))\n\n# X_more_toxic = tokenizer.texts_to_sequences(df_val['more_toxic'].apply(clean))\n# X_more_toxic = np.array(pad_sequences(X_more_toxic, maxlen=max_sequence_len, padding='pre'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\nX_less_toxic = vec.transform(df_val['less_toxic'])\nX_more_toxic = vec.transform(df_val['more_toxic'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation Accuracy\n(p1 < p2).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n# X_test = tokenizer.texts_to_sequences(df_sub['text'].apply(clean))\n# X_test = np.array(pad_sequences(X_test, maxlen=max_sequence_len, padding='pre'))\n# p3 = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nX_test = vec.transform(df_sub['text'])\np3 = model.predict_proba(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score'] = p3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 9 comments will fail if compared one with the other\ndf_sub['score'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}