{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Esta es una primer prueba a entender el problema para el equipo de text mining. \n\nConsiderar que todo lo redactado puede tener errores de redaccion, tipograficos, etc. porque le estoy prestando cero atencion a eso. Solo intento desglozar algunas partes del problema.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-26T12:32:56.251276Z","iopub.execute_input":"2022-05-26T12:32:56.25159Z","iopub.status.idle":"2022-05-26T12:32:56.274408Z","shell.execute_reply.started":"2022-05-26T12:32:56.25156Z","shell.execute_reply":"2022-05-26T12:32:56.273684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una de las cosas mas dificiles de esta competencia es el propio dataset. El cual no existe y tenemos que construirlo nosotros de manera \"creativa\" usando otros data sets anteriores de competencias similares. \n\nA priori parece lo que lo mas inteligente es usar dos datasets (Del segundo no estoy tan convencido aun) Que son  [jigsaw-multilingual-toxic-comment-classification](http://../input/jigsaw-toxic-comment-classification-challenge) y [ruddit jigsaw dataset](http://../input/ruddit-jigsaw-dataset).\n\nA priori voy a usar el primero para crear un dataset y ver si finalmente necesitamos algo del segundo. Super importante entender que como creemos este dataset va a impactar de manera directa en que tan buenas sean finalmente nuestras decisiones. ","metadata":{}},{"cell_type":"markdown","source":"**Creamos data set**","metadata":{}},{"cell_type":"code","source":"#El data set a usar tiene filas con comentarios y columnas con categorias binarias. \n# Toxico, muy toxico, obseno, amenaza, insulto, discrimacion hacia la identidad, contenido sexual explicito\n\n#ponderador\nponderador_de_toxicidad = {\n    'toxic': 1,\n    'severe_toxic': 2,\n    'obscene': 1,\n    'threat': 1,\n    'insult': 1,\n    'identity_hate': 2,\n    'sexual_explicit': 1\n}\n\ntoxicity_types = list(ponderador_de_toxicidad.keys())\ntoxicity_types","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:32:56.610054Z","iopub.execute_input":"2022-05-26T12:32:56.610387Z","iopub.status.idle":"2022-05-26T12:32:56.619168Z","shell.execute_reply.started":"2022-05-26T12:32:56.610353Z","shell.execute_reply":"2022-05-26T12:32:56.618226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Como paso siguiente deberiamos limpiar el data set**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport pandas as pd\nimport numpy as np\n\npd.set_option('display.max_colwidth',300)\n# Importo el dataset\ndf = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv', encoding='utf-8')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:32:57.024051Z","iopub.execute_input":"2022-05-26T12:32:57.024384Z","iopub.status.idle":"2022-05-26T12:32:58.762021Z","shell.execute_reply.started":"2022-05-26T12:32:57.024348Z","shell.execute_reply":"2022-05-26T12:32:58.761051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:32:58.763765Z","iopub.execute_input":"2022-05-26T12:32:58.764019Z","iopub.status.idle":"2022-05-26T12:32:58.780556Z","shell.execute_reply.started":"2022-05-26T12:32:58.763988Z","shell.execute_reply":"2022-05-26T12:32:58.779013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef get_top_n_words(corpus, n=10):\n    cv = CountVectorizer()\n    corpus_matrix = cv.fit_transform(corpus)\n    corpus_matrix = pd.DataFrame.sparse.from_spmatrix(corpus_matrix, columns=cv.get_feature_names())\n    aux = corpus_matrix.sum()\n    aux=aux.to_frame('count')\n    aux.sort_values(by='count',ascending=False,inplace=True)\n    aux=aux.head(n)\n    return aux","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:32:58.781946Z","iopub.execute_input":"2022-05-26T12:32:58.782187Z","iopub.status.idle":"2022-05-26T12:32:58.789249Z","shell.execute_reply.started":"2022-05-26T12:32:58.782157Z","shell.execute_reply":"2022-05-26T12:32:58.788518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cantidad de tokens en todo el corpus\nimport plotly.express as px\n\nmost_importants = get_top_n_words(df.comment_text, n=500)\nprint(most_importants.head(10))\npx.bar(most_importants, x=most_importants.index, y='count')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:32:58.791209Z","iopub.execute_input":"2022-05-26T12:32:58.791661Z","iopub.status.idle":"2022-05-26T12:33:46.616376Z","shell.execute_reply.started":"2022-05-26T12:32:58.791626Z","shell.execute_reply":"2022-05-26T12:33:46.615327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_toxic = df[(df.toxic == 1) | (df.severe_toxic == 1) ]\ndf_toxic","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:33:46.617958Z","iopub.execute_input":"2022-05-26T12:33:46.618217Z","iopub.status.idle":"2022-05-26T12:33:46.647059Z","shell.execute_reply.started":"2022-05-26T12:33:46.618186Z","shell.execute_reply":"2022-05-26T12:33:46.646368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cantidad de tokens en todo el corpus\nmost_importants = get_top_n_words(df_toxic.comment_text, n=500)\nprint(most_importants.head(10))\npx.bar(most_importants, x=most_importants.index, y='count')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:33:46.648373Z","iopub.execute_input":"2022-05-26T12:33:46.648911Z","iopub.status.idle":"2022-05-26T12:33:52.645468Z","shell.execute_reply.started":"2022-05-26T12:33:46.648873Z","shell.execute_reply":"2022-05-26T12:33:52.644825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Proceso NLP**","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nstop_words = set( stopwords.words('english'))\nstop_words.update(string.punctuation)\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nfrom nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer('english')\n\n# Pre procesamiento basico, agregar las etapas que considere necesario\ndef pre_procesamiento_texto(text):\n  # Armo los tokens para procesar los datos\n  tokens = word_tokenize(text)\n\n  # Elimino las stopwords\n  tokens = [t.lower() for t in tokens if t.lower() not in stop_words]\n\n  return tokens","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:33:52.646669Z","iopub.execute_input":"2022-05-26T12:33:52.647741Z","iopub.status.idle":"2022-05-26T12:33:52.657956Z","shell.execute_reply.started":"2022-05-26T12:33:52.6477Z","shell.execute_reply":"2022-05-26T12:33:52.65691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:33:52.659554Z","iopub.execute_input":"2022-05-26T12:33:52.659943Z","iopub.status.idle":"2022-05-26T12:33:52.67086Z","shell.execute_reply.started":"2022-05-26T12:33:52.659907Z","shell.execute_reply":"2022-05-26T12:33:52.670225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tengo que crear el dataset limpio. Es decir solo con una columna de \"resultado\" de si es toxico o no para poder seguir probando","metadata":{}},{"cell_type":"code","source":"# Combine all toxicity levels into one with the same weights set\ndf['toxicity'] = sum([df[type] * coef for type, coef in ponderador_de_toxicidad.items() if type in df])\n\n# Standardize toxicity (converts to continues values)\ndf['toxicity'] = df['toxicity'] / df['toxicity'].max()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:33:52.672483Z","iopub.execute_input":"2022-05-26T12:33:52.673002Z","iopub.status.idle":"2022-05-26T12:33:52.695368Z","shell.execute_reply.started":"2022-05-26T12:33:52.672954Z","shell.execute_reply":"2022-05-26T12:33:52.694368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downsample\ndf = pd.concat([\n    df[df['toxicity'] <= 0].sample(n = int((df['toxicity'] > 0).sum() * 1.5),random_state = 201),\n    df[df['toxicity'] > 0]\n])\nprint(f\"- New shape: {df.shape}\")\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:33:52.699636Z","iopub.execute_input":"2022-05-26T12:33:52.699997Z","iopub.status.idle":"2022-05-26T12:33:52.780615Z","shell.execute_reply.started":"2022-05-26T12:33:52.699944Z","shell.execute_reply":"2022-05-26T12:33:52.779669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vectorizacion de resultados BOW, tambien se puede probar TF-IDF\ncv = CountVectorizer(tokenizer=pre_procesamiento_texto, max_df=0.2, min_df=0.03)\nX_transform_cv = cv.fit_transform(df.comment_text) # Armo matriz para entrenar CV\n\n# TF-IDF \ntf = TfidfVectorizer(tokenizer=pre_procesamiento_texto, max_df=0.2, min_df=0.03)\nX_transform_tf = tf.fit_transform(df.comment_text) # Armo matriz para entrenar tf\nprint(X_transform_cv.shape)\nprint(X_transform_tf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:33:52.781829Z","iopub.execute_input":"2022-05-26T12:33:52.782058Z","iopub.status.idle":"2022-05-26T12:36:00.587768Z","shell.execute_reply.started":"2022-05-26T12:33:52.78203Z","shell.execute_reply":"2022-05-26T12:36:00.586501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Separado X,y en train y test, dos veces porque cada proceso de NLP es distinto\n\nX_traincv, X_testcv, y_train, y_test = train_test_split(X_transform_cv, df.toxicity,test_size=.30)\nX_traintf, X_testtf, y_train, y_test = train_test_split(X_transform_tf, df.toxicity,test_size=.30)\ny_train = y_train.astype('float32')\ny_test = y_test.astype('float32')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:36:00.58961Z","iopub.execute_input":"2022-05-26T12:36:00.590973Z","iopub.status.idle":"2022-05-26T12:36:00.619475Z","shell.execute_reply.started":"2022-05-26T12:36:00.590912Z","shell.execute_reply":"2022-05-26T12:36:00.618351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transform = pd.DataFrame.sparse.from_spmatrix(X_traincv, columns=cv.get_feature_names())\n","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:36:00.620959Z","iopub.execute_input":"2022-05-26T12:36:00.621321Z","iopub.status.idle":"2022-05-26T12:36:00.6329Z","shell.execute_reply.started":"2022-05-26T12:36:00.621285Z","shell.execute_reply":"2022-05-26T12:36:00.63155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aux = df_transform.sum().to_frame('count')\naux.sort_values(by='count',ascending=False,inplace=True)\naux=aux.head(100)\npx.bar(aux, x=aux.index, y='count')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:36:00.634402Z","iopub.execute_input":"2022-05-26T12:36:00.635317Z","iopub.status.idle":"2022-05-26T12:36:00.722024Z","shell.execute_reply.started":"2022-05-26T12:36:00.635235Z","shell.execute_reply":"2022-05-26T12:36:00.72104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Primer modelo**","metadata":{}},{"cell_type":"code","source":"\n\n## CV\n\n# Entreno CV - regresión lineal\nfrom sklearn.linear_model import LinearRegression\nlearner1 = LinearRegression()\nlearner1.fit(X_traincv, y_train)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:36:00.723559Z","iopub.execute_input":"2022-05-26T12:36:00.724559Z","iopub.status.idle":"2022-05-26T12:36:00.891157Z","shell.execute_reply.started":"2022-05-26T12:36:00.724507Z","shell.execute_reply":"2022-05-26T12:36:00.889988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\"\"\nLightlg\nimport lightgbm as lgb\nlearner5 = lgb.LGBMRegressor(n_estimators=1000, max_depth=10)\nlearner5.fit(X_traintf, y_train)\n\"\"\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:36:00.893174Z","iopub.execute_input":"2022-05-26T12:36:00.893955Z","iopub.status.idle":"2022-05-26T12:36:00.903715Z","shell.execute_reply.started":"2022-05-26T12:36:00.893897Z","shell.execute_reply":"2022-05-26T12:36:00.902674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## CV\n\n#Entreno CV - Random Forest\nfrom sklearn.ensemble import RandomForestRegressor\nlearner2 = RandomForestRegressor(n_estimators = 100, random_state = 0)\nlearner2.fit(X_traincv, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:36:00.906455Z","iopub.execute_input":"2022-05-26T12:36:00.907513Z","iopub.status.idle":"2022-05-26T12:39:54.349461Z","shell.execute_reply.started":"2022-05-26T12:36:00.907457Z","shell.execute_reply":"2022-05-26T12:39:54.347364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TF\n\n# Entreno TF - Regresion\nfrom sklearn.linear_model import LinearRegression\nlearner3 = LinearRegression()\nlearner3.fit(X_traintf, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.350652Z","iopub.status.idle":"2022-05-26T12:39:54.35102Z","shell.execute_reply.started":"2022-05-26T12:39:54.350829Z","shell.execute_reply":"2022-05-26T12:39:54.350851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TF\n\n# Entreno TF - Random Forest\nfrom sklearn.linear_model import LinearRegression\nlearner4 = RandomForestRegressor(n_estimators = 100, random_state = 0)\nlearner4.fit(X_traintf, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.352419Z","iopub.status.idle":"2022-05-26T12:39:54.352741Z","shell.execute_reply.started":"2022-05-26T12:39:54.352574Z","shell.execute_reply":"2022-05-26T12:39:54.352591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hago predicciones sobre Test\n\n## Para CV Regresion\ny_pred1 = learner1.predict(X_testcv)\n\n## Para CV RF\ny_pred2 = learner2.predict(X_testcv)\n\n## Para CV Regresion\ny_pred3 = learner3.predict(X_testcv)\n\n## Para CV RF\ny_pred4 = learner4.predict(X_testcv)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.354344Z","iopub.status.idle":"2022-05-26T12:39:54.354673Z","shell.execute_reply.started":"2022-05-26T12:39:54.354503Z","shell.execute_reply":"2022-05-26T12:39:54.35452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RMSE (Root Mean Square Error)\n\nfrom sklearn.metrics import mean_squared_error \n\nrmse1 = float(format(np.sqrt(mean_squared_error(y_test, y_pred1)),'.3f'))\nprint(\"\\nRMSE - CV Reg Mult:\\n\",rmse1)\nrmse2 = float(format(np.sqrt(mean_squared_error(y_test, y_pred2)),'.3f'))\nprint(\"\\nRMSE - CV Random Forest:\\n\",rmse2)\nrmse3 = float(format(np.sqrt(mean_squared_error(y_test, y_pred3)),'.3f'))\nprint(\"\\nRMSE - TF Reg Mult:\\n\",rmse3)\nrmse4 = float(format(np.sqrt(mean_squared_error(y_test, y_pred4)),'.3f'))\nprint(\"\\nRMSE - TF Random Forest:\\n\",rmse4)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.355649Z","iopub.status.idle":"2022-05-26T12:39:54.355956Z","shell.execute_reply.started":"2022-05-26T12:39:54.355794Z","shell.execute_reply":"2022-05-26T12:39:54.355811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Bert\n\nimport tensorflow as tf\nimport tensorflow_hub as hub","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.356909Z","iopub.status.idle":"2022-05-26T12:39:54.357201Z","shell.execute_reply.started":"2022-05-26T12:39:54.357045Z","shell.execute_reply":"2022-05-26T12:39:54.357061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = df\ndf2.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.358449Z","iopub.status.idle":"2022-05-26T12:39:54.358843Z","shell.execute_reply.started":"2022-05-26T12:39:54.3586Z","shell.execute_reply":"2022-05-26T12:39:54.358617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.rename(columns = {'toxicity':'Category', 'comment_text':'Message'}, inplace = True)\ndf2.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.360849Z","iopub.status.idle":"2022-05-26T12:39:54.361203Z","shell.execute_reply.started":"2022-05-26T12:39:54.361013Z","shell.execute_reply":"2022-05-26T12:39:54.361038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.drop(['id','toxic', 'severe_toxic', 'obscene','threat','insult','identity_hate'], axis = 'columns', inplace=True)\ndf2.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.362372Z","iopub.status.idle":"2022-05-26T12:39:54.362672Z","shell.execute_reply.started":"2022-05-26T12:39:54.362514Z","shell.execute_reply":"2022-05-26T12:39:54.36253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df2['Message'],df2['Category'], stratify=df['Category'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.364196Z","iopub.status.idle":"2022-05-26T12:39:54.364895Z","shell.execute_reply.started":"2022-05-26T12:39:54.364647Z","shell.execute_reply":"2022-05-26T12:39:54.364673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow-text","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.366306Z","iopub.status.idle":"2022-05-26T12:39:54.366815Z","shell.execute_reply.started":"2022-05-26T12:39:54.366578Z","shell.execute_reply":"2022-05-26T12:39:54.366606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_text as text","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.367675Z","iopub.status.idle":"2022-05-26T12:39:54.368212Z","shell.execute_reply.started":"2022-05-26T12:39:54.368Z","shell.execute_reply":"2022-05-26T12:39:54.368026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\nbert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.369621Z","iopub.status.idle":"2022-05-26T12:39:54.369942Z","shell.execute_reply.started":"2022-05-26T12:39:54.369771Z","shell.execute_reply":"2022-05-26T12:39:54.369794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sentence_embeding(sentences):\n    preprocessed_text = bert_preprocess(sentences)\n    return bert_encoder(preprocessed_text)['pooled_output']\n\nget_sentence_embeding([\n    \"500$ discount. hurry up\", \n    \"Bhavin, are you up for a volleybal game tomorrow?\"]\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.371319Z","iopub.status.idle":"2022-05-26T12:39:54.371787Z","shell.execute_reply.started":"2022-05-26T12:39:54.371584Z","shell.execute_reply":"2022-05-26T12:39:54.371611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bert layers\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers\nl = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\nl = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n\n# Use inputs and outputs to construct a final model\nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.372623Z","iopub.status.idle":"2022-05-26T12:39:54.373302Z","shell.execute_reply.started":"2022-05-26T12:39:54.373067Z","shell.execute_reply":"2022-05-26T12:39:54.37309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.37493Z","iopub.status.idle":"2022-05-26T12:39:54.375566Z","shell.execute_reply.started":"2022-05-26T12:39:54.375313Z","shell.execute_reply":"2022-05-26T12:39:54.375342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.376909Z","iopub.status.idle":"2022-05-26T12:39:54.377492Z","shell.execute_reply.started":"2022-05-26T12:39:54.377066Z","shell.execute_reply":"2022-05-26T12:39:54.377088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, epochs=1, batch_size = 32)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.378681Z","iopub.status.idle":"2022-05-26T12:39:54.379012Z","shell.execute_reply.started":"2022-05-26T12:39:54.378838Z","shell.execute_reply":"2022-05-26T12:39:54.378861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predicted = model.predict(X_test)\ny_predicted = y_predicted.flatten()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:39:54.3809Z","iopub.status.idle":"2022-05-26T12:39:54.381461Z","shell.execute_reply.started":"2022-05-26T12:39:54.381237Z","shell.execute_reply":"2022-05-26T12:39:54.381282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}