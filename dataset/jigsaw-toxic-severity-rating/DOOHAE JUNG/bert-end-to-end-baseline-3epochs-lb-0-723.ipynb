{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT only end-to-end Baseline for Jigsaw Toxic Classification\n\nHello, I'm a new Kaggler and this is the very first competition I participate in Kaggle  \nSaw many of great Kagglers utilize sklearn based architecture like TF-IDF, Ridge Regression which are really really awesome and well-performing!!  \nAnd I wrote just a simple baseline notebook for the freshman like me! (BERT architecture only ver.)  \nAppreciate any comment or advice except for TOXIC one LOLðŸ¤—  \n\nReferences :\n- https://www.kaggle.com/debarshichanda/pytorch-w-b-jigsaw-starter\n- https://www.kaggle.com/chryzal/jigsaw-ensemble-0-864\n- https://www.kaggle.com/readoc/toxic-linear-model-pseudo-labelling-lb-0-864  \n\nPlease Upvote for the above links which are much greater than mine :)\n","metadata":{}},{"cell_type":"markdown","source":"### Approach Summary\n1. Train: BERT model with pseudo-labeled \"jigsaw-toxic-comment-classification-challenge\" data\n2. Valid: test accuracy with our \"jigsaw-toxic-severity-rating\" data after every one-epoch-training done\n3. Test: make submission with the model which got best accuracy in validation part","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport sys\nimport gc\nimport re\nimport time\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModel, AutoTokenizer, logging\n# Ignore model init warning\nlogging.set_verbosity_error()\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nfrom IPython.display import display\nfrom copy import deepcopy\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Ignore tokenizers warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-11T07:45:56.372183Z","iopub.execute_input":"2022-01-11T07:45:56.372821Z","iopub.status.idle":"2022-01-11T07:45:56.379568Z","shell.execute_reply.started":"2022-01-11T07:45:56.372781Z","shell.execute_reply":"2022-01-11T07:45:56.378813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Original Dataset","metadata":{}},{"cell_type":"code","source":"valid_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\nvalid_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:46:25.032365Z","iopub.execute_input":"2022-01-11T07:46:25.03263Z","iopub.status.idle":"2022-01-11T07:46:25.625987Z","shell.execute_reply.started":"2022-01-11T07:46:25.032601Z","shell.execute_reply":"2022-01-11T07:46:25.625261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:46:27.794639Z","iopub.execute_input":"2022-01-11T07:46:27.79522Z","iopub.status.idle":"2022-01-11T07:46:27.801261Z","shell.execute_reply.started":"2022-01-11T07:46:27.795185Z","shell.execute_reply":"2022-01-11T07:46:27.800441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(data):\n\n    # Clean some punctutations\n    data = re.sub('\\n', ' ', data)\n    # Remove ip address\n    data = re.sub(r'(([0-9]+\\.){2,}[0-9]+)',' ', data)\n    \n    data = re.sub(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', data)\n    # Replace repeating characters more than 3 times to length of 3\n    data = re.sub(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', data)\n    # patterns with repeating characters \n    data = re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', data)\n    data = re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', data)\n\n    # Add space around repeating characters\n    data = re.sub(' +', ' ', data)\n    \n    # Ex) I didn ' t -> I didn't\n    data = re.sub(\" ' \", \"'\", data)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:46:28.081846Z","iopub.execute_input":"2022-01-11T07:46:28.082123Z","iopub.status.idle":"2022-01-11T07:46:28.089268Z","shell.execute_reply.started":"2022-01-11T07:46:28.082095Z","shell.execute_reply":"2022-01-11T07:46:28.088334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df[\"less_toxic\"] = valid_df[\"less_toxic\"].apply(clean)\nvalid_df[\"more_toxic\"] = valid_df[\"more_toxic\"].apply(clean)\nvalid_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:46:28.388126Z","iopub.execute_input":"2022-01-11T07:46:28.38874Z","iopub.status.idle":"2022-01-11T07:46:53.702182Z","shell.execute_reply.started":"2022-01-11T07:46:28.388693Z","shell.execute_reply":"2022-01-11T07:46:53.701522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nsubmission_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/sample_submission.csv\")\ndisplay(comments_df.head())\ndisplay(submission_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:46:53.703873Z","iopub.execute_input":"2022-01-11T07:46:53.704133Z","iopub.status.idle":"2022-01-11T07:46:53.843037Z","shell.execute_reply.started":"2022-01-11T07:46:53.704097Z","shell.execute_reply":"2022-01-11T07:46:53.842381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Jigsaw Toxic Comment Classification Data","metadata":{}},{"cell_type":"code","source":"jigsaw_train_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\njigsaw_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:46:53.844438Z","iopub.execute_input":"2022-01-11T07:46:53.844907Z","iopub.status.idle":"2022-01-11T07:46:55.883405Z","shell.execute_reply.started":"2022-01-11T07:46:53.844869Z","shell.execute_reply":"2022-01-11T07:46:55.882723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\nfor feature in features:\n    print(\"*\"*20, feature.upper(), \"*\"*20)\n    display(jigsaw_train_df[jigsaw_train_df[feature]==1][[\"comment_text\", feature]].sample(5))","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:46:55.885331Z","iopub.execute_input":"2022-01-11T07:46:55.88567Z","iopub.status.idle":"2022-01-11T07:46:55.952023Z","shell.execute_reply.started":"2022-01-11T07:46:55.88562Z","shell.execute_reply":"2022-01-11T07:46:55.951338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jigsaw_label = deepcopy(jigsaw_train_df)\n\nFEATURE_WTS = {\n    'severe_toxic': 3, 'identity_hate': 1.5, 'threat': 1.5, \n    'insult': 0.64, 'toxic': 2, 'obscene': 0.16, \n}\n\nFEATURES = list(FEATURE_WTS.keys())\n\njigsaw_label['label'] = 0\nfor feat, wt in FEATURE_WTS.items(): \n    jigsaw_label.label += wt*jigsaw_label[feat]\njigsaw_label.label = jigsaw_label.label/jigsaw_label.label.max()\n    \npos = jigsaw_label[jigsaw_label.label>0]\nneg = jigsaw_label[jigsaw_label.label==0].sample(len(pos), random_state=201)\njigsaw_label = pd.concat([pos, neg])\njigsaw_label = jigsaw_label[[\"comment_text\", \"label\"]]\njigsaw_label","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:46:55.953356Z","iopub.execute_input":"2022-01-11T07:46:55.953596Z","iopub.status.idle":"2022-01-11T07:46:56.020736Z","shell.execute_reply.started":"2022-01-11T07:46:55.953562Z","shell.execute_reply":"2022-01-11T07:46:56.020013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jigsaw_label[\"comment_text\"] = jigsaw_label[\"comment_text\"].apply(clean)\njigsaw_label","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:46:56.023712Z","iopub.execute_input":"2022-01-11T07:46:56.023927Z","iopub.status.idle":"2022-01-11T07:47:11.482794Z","shell.execute_reply.started":"2022-01-11T07:46:56.023902Z","shell.execute_reply":"2022-01-11T07:47:11.481209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Datasets & DataLoaders","metadata":{}},{"cell_type":"code","source":"# Define Train Dataset Class\nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, text_col=\"comment_text\", is_test=False):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        if not is_test:\n            self.labels = [torch.tensor(label, dtype=torch.float) for label in tqdm(df['label'].values)]\n        self.texts = [\n            self.tokenizer.encode_plus(\n                                text,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n            for text in tqdm(df[text_col].values)]\n        \n        self.input_ids = [torch.tensor(text[\"input_ids\"], dtype=torch.long) for text in tqdm(self.texts)]\n        self.attention_masks = [torch.tensor(text[\"attention_mask\"], dtype=torch.long) for text in tqdm(self.texts)]\n\n        \n    def __len__(self):\n        return len(self.df)\n    \n    \n    def __getitem__(self, index):\n        if self.is_test:\n            return {\n                \"text_ids\": self.input_ids[index],\n                \"text_mask\": self.attention_masks[index]\n            }\n        \n        label = self.labels[index]\n        return {\n            \"text_ids\": self.input_ids[index],\n            \"text_mask\": self.attention_masks[index],\n            \"label\": label\n        }\n","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:47:11.484169Z","iopub.execute_input":"2022-01-11T07:47:11.484575Z","iopub.status.idle":"2022-01-11T07:47:11.494741Z","shell.execute_reply.started":"2022-01-11T07:47:11.484536Z","shell.execute_reply":"2022-01-11T07:47:11.493697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"../input/roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:47:11.496437Z","iopub.execute_input":"2022-01-11T07:47:11.496815Z","iopub.status.idle":"2022-01-11T07:47:11.692599Z","shell.execute_reply.started":"2022-01-11T07:47:11.496778Z","shell.execute_reply":"2022-01-11T07:47:11.691861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = JigsawDataset(\n    df=jigsaw_label,\n    tokenizer=tokenizer,\n    max_length=tokenizer.max_model_input_sizes[model_name.split(\"/\")[-1]]\n)\n\nvalid_less_dataset = JigsawDataset(\n    df=valid_df,\n    tokenizer=tokenizer,\n    max_length=tokenizer.max_model_input_sizes[model_name.split(\"/\")[-1]],\n    text_col=\"less_toxic\",\n    is_test=True\n)\n\nvalid_more_dataset = JigsawDataset(\n    df=valid_df,\n    tokenizer=tokenizer,\n    max_length=tokenizer.max_model_input_sizes[model_name.split(\"/\")[-1]],\n    text_col=\"more_toxic\",\n    is_test=True\n)\n\ntest_dataset = JigsawDataset(\n    df=comments_df,\n    tokenizer=tokenizer,\n    max_length=tokenizer.max_model_input_sizes[model_name.split(\"/\")[-1]],\n    text_col=\"text\",\n    is_test=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:47:11.693776Z","iopub.execute_input":"2022-01-11T07:47:11.694027Z","iopub.status.idle":"2022-01-11T07:48:09.930396Z","shell.execute_reply.started":"2022-01-11T07:47:11.693993Z","shell.execute_reply":"2022-01-11T07:48:09.929722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=True\n)\n\nvalid_less_dataloader = DataLoader(\n    valid_less_dataset,\n    batch_size=16,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\n\nvalid_more_dataloader = DataLoader(\n    valid_more_dataset,\n    batch_size=16,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=16,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:48:09.932716Z","iopub.execute_input":"2022-01-11T07:48:09.933221Z","iopub.status.idle":"2022-01-11T07:48:09.941038Z","shell.execute_reply.started":"2022-01-11T07:48:09.93318Z","shell.execute_reply":"2022-01-11T07:48:09.940367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Model Class","metadata":{}},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, 1)\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,\n                         attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:48:09.942202Z","iopub.execute_input":"2022-01-11T07:48:09.943006Z","iopub.status.idle":"2022-01-11T07:48:09.952121Z","shell.execute_reply.started":"2022-01-11T07:48:09.94297Z","shell.execute_reply":"2022-01-11T07:48:09.951424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set-up for Training","metadata":{}},{"cell_type":"code","source":"learning_rate = 1e-5\nepochs = 2\n\nmodel = JigsawModel(model_name).to(device)\ncriterion = nn.L1Loss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n                                                max_lr=learning_rate, \n                                                steps_per_epoch=len(train_dataloader),\n                                                epochs=epochs,\n                                                pct_start=0.05\n                                               )","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:48:09.95322Z","iopub.execute_input":"2022-01-11T07:48:09.953464Z","iopub.status.idle":"2022-01-11T07:48:22.741433Z","shell.execute_reply.started":"2022-01-11T07:48:09.953432Z","shell.execute_reply":"2022-01-11T07:48:22.740707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions for Training","metadata":{}},{"cell_type":"code","source":"def train(model, optimizer, scheduler, dataloader, device):\n    \n    model.train()\n    dataset_size = 0\n    running_loss = 0.0\n\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        text_ids = data['text_ids'].to(device, dtype = torch.long)\n        text_mask = data['text_mask'].to(device, dtype = torch.long)\n        targets = data['label'].to(device, dtype=torch.float)\n\n        batch_size = text_ids.size(0)\n\n        outputs = model(text_ids, text_mask)\n\n        loss = criterion(outputs, targets)\n\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        if scheduler is not None:\n            scheduler.step()\n\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n\n        epoch_loss = running_loss / dataset_size\n\n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n        \n    return epoch_loss\n            \n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:48:22.742794Z","iopub.execute_input":"2022-01-11T07:48:22.74307Z","iopub.status.idle":"2022-01-11T07:48:22.771229Z","shell.execute_reply.started":"2022-01-11T07:48:22.743035Z","shell.execute_reply":"2022-01-11T07:48:22.770423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid(model, loader, device):\n    \n    all_labels = []\n    model.eval()\n    for data in tqdm(loader):\n        \n        text_ids = data['text_ids'].to(device, dtype = torch.long)\n        text_mask = data['text_mask'].to(device, dtype = torch.long)\n\n        batch_size = text_ids.size(0)\n\n        outputs = model(text_ids, text_mask).view(-1)\n        all_labels.extend(list(outputs.detach().cpu().numpy()))\n        \n    return np.array(all_labels)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:48:22.772515Z","iopub.execute_input":"2022-01-11T07:48:22.772889Z","iopub.status.idle":"2022-01-11T07:48:22.781693Z","shell.execute_reply.started":"2022-01-11T07:48:22.772811Z","shell.execute_reply":"2022-01-11T07:48:22.780784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main Training Part","metadata":{}},{"cell_type":"code","source":"# Training Part\nbest_acc = 0\nall_preds = []\nfor epoch in tqdm(range(epochs)):\n    \n    epoch_loss = train(\n        model = model,\n        optimizer = optimizer,\n        scheduler = scheduler,\n        dataloader = train_dataloader,\n        device = device\n    )\n    \n    less_labels = valid(model, valid_less_dataloader, device)\n    more_labels = valid(model, valid_more_dataloader, device)\n    preds = more_labels - less_labels\n    accuracy = len(preds[preds > 0])/len(preds)\n\n    if accuracy > best_acc:\n        print(f\"Best Accuracy Updated: {accuracy:.4f}\")\n        print(f\"Outdated Best Acc.: {best_acc:.4f}\")\n        best_acc = accuracy\n        best_submission = valid(model, test_dataloader, device)\n        if os.path.exists(\"checkpoint.pth\"):\n            os.remove(\"checkpoint.pth\")\n        torch.save(model, \"checkpoint.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-01-11T07:48:22.78267Z","iopub.execute_input":"2022-01-11T07:48:22.783474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit Your Best Validated Result","metadata":{}},{"cell_type":"code","source":"submission_df['score'] = best_submission\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}