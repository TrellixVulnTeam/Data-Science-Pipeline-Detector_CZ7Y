{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport nltk\nimport copy\nimport scipy\nimport tensorflow\nimport numpy as np \nimport pandas as pd \nfrom tqdm.auto import tqdm \nfrom bs4 import BeautifulSoup \nfrom scipy.sparse import vstack\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom sklearn.pipeline import Pipeline\nfrom tensorflow.keras import activations\nfrom sklearn.naive_bayes import MultinomialNB\nfrom tensorflow.keras.layers import LSTM, GRU\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow. keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, Input, Bidirectional, TimeDistributed, Dense, Conv1D, MaxPool1D, concatenate, \\\n    Dropout, add, InputSpec, PReLU, Flatten, multiply, Reshape, Permute, BatchNormalization,LeakyReLU","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-15T16:39:41.462609Z","iopub.execute_input":"2022-01-15T16:39:41.463193Z","iopub.status.idle":"2022-01-15T16:39:41.472521Z","shell.execute_reply.started":"2022-01-15T16:39:41.463154Z","shell.execute_reply":"2022-01-15T16:39:41.47168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Load","metadata":{}},{"cell_type":"code","source":"data_train=pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndata_val=pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:39:43.505736Z","iopub.execute_input":"2022-01-15T16:39:43.506384Z","iopub.status.idle":"2022-01-15T16:39:44.563187Z","shell.execute_reply.started":"2022-01-15T16:39:43.506248Z","shell.execute_reply":"2022-01-15T16:39:44.562475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Give Weight to different toxicity level","metadata":{}},{"cell_type":"code","source":"weight_toxic={'toxic':1.2,'severe_toxic':1.3,'obscene':0.5,'threat':0.4,'insult':0.3,'identity_hate':0.5}\ndata=copy.deepcopy(data_train)\ndata['weight_rate']=0\nfor col,val in weight_toxic.items():\n    data['weight_rate']+=val*data[col]","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:40:38.040195Z","iopub.execute_input":"2022-01-15T16:40:38.040776Z","iopub.status.idle":"2022-01-15T16:40:38.063081Z","shell.execute_reply.started":"2022-01-15T16:40:38.040735Z","shell.execute_reply":"2022-01-15T16:40:38.062392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Balance through down_scaling\ndata_insulting=data[data['weight_rate']>0]\ndata_val_0=data[data['weight_rate']==0].sample(len(data_insulting)+5000)\ndata_under_sampled=pd.concat([data_insulting,data_val_0])\ndata=data_under_sampled","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:40:39.15962Z","iopub.execute_input":"2022-01-15T16:40:39.160093Z","iopub.status.idle":"2022-01-15T16:40:39.196994Z","shell.execute_reply.started":"2022-01-15T16:40:39.160055Z","shell.execute_reply":"2022-01-15T16:40:39.196249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data['rate']=data['toxic']+data['severe_toxic']+data['obscene']+data['threat']+data['insult']+data['identity_hate']\n# data.rate.unique()\n# print(data.rate.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:40:40.968566Z","iopub.execute_input":"2022-01-15T16:40:40.969239Z","iopub.status.idle":"2022-01-15T16:40:40.973768Z","shell.execute_reply.started":"2022-01-15T16:40:40.9692Z","shell.execute_reply":"2022-01-15T16:40:40.97287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data['rate']=data.rate.apply(lambda x:1 if x>0 else 0)\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:40:41.304169Z","iopub.execute_input":"2022-01-15T16:40:41.304391Z","iopub.status.idle":"2022-01-15T16:40:41.307924Z","shell.execute_reply.started":"2022-01-15T16:40:41.304365Z","shell.execute_reply":"2022-01-15T16:40:41.307168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"#Cleaning Template copied from-\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\ndef clean(data, col):\n    \n    data[col] = data[col].str.replace('https?://\\S+|www\\.\\S+', ' social medium ', regex=True)      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\", regex=False) \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\", regex=False)\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \", regex=False)\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \", regex=False)\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you', regex=True)\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace('\\s+', ' ', regex=True)\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1', regex=True) \n    data[col] = data[col].str.replace(\"[:|♣|'|§|♠|*|/|?|=|%|&|-|#|•|~|^|>|<|►|_]\", '', regex=True)\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \", regex=False)\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'s\", \" \", regex=False)\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', regex=True)\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', regex=True)    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ', regex=True)    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    tqdm.pandas()\n    data[col] = data[col].progress_apply(text_cleaning)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:40:42.794556Z","iopub.execute_input":"2022-01-15T16:40:42.794856Z","iopub.status.idle":"2022-01-15T16:40:42.84892Z","shell.execute_reply.started":"2022-01-15T16:40:42.794822Z","shell.execute_reply":"2022-01-15T16:40:42.848061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean the data","metadata":{}},{"cell_type":"code","source":"data_clean = clean(data,'comment_text')      #Training data\ndata_val=clean(data_val,'less_toxic')        #Validation data\ndata_val=clean(data_val,'more_toxic')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:40:45.067921Z","iopub.execute_input":"2022-01-15T16:40:45.068186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_clean.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tf-Idf+Ridge","metadata":{}},{"cell_type":"code","source":"# pipeline = Pipeline(\n#     [\n#         (\"vect\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n#         (\"clf\", Ridge()),\n#     ]\n# )\n# pipeline.fit(data_clean['comment_text'],data_clean['rate'])\n# p1 = pipeline.predict(val_less_toxic['less_toxic'])\n# p2 = pipeline.predict(val_more_toxic['more_toxic'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM+Embedding Layer","metadata":{}},{"cell_type":"code","source":"#To vectorize the text(we can also use keras functions for this)\ndef text_to_vector(df,col_name,w2v):\n    w2v={}\n    text=list(df[col_name])\n    rate=list(df['weight_rate'])\n    count=1\n    if len(w2v)==0:\n        for txt in text:\n            for word in txt.split(' '):\n                if word not in w2v:\n                    w2v[word]=count\n                    count+=1\n    X=np.zeros((len(text),30),dtype='float32')\n    Y=np.zeros((len(text),1),dtype='float32')\n    for i,txt in enumerate(text):\n        j=0\n        Y[i][0]=rate[i]\n        for word in txt.split(' '):\n            if word in w2v and j<30:\n                X[i][j]=w2v[word]\n                j+=1\n                \n    return X,Y,w2v ","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:07:48.396338Z","iopub.execute_input":"2022-01-15T16:07:48.396598Z","iopub.status.idle":"2022-01-15T16:07:48.40452Z","shell.execute_reply.started":"2022-01-15T16:07:48.396568Z","shell.execute_reply":"2022-01-15T16:07:48.403816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,y_train,w2v=text_to_vector(data_clean,col_name='comment_text',w2v={})\nprint(x_train.shape,y_train.shape,len(w2v))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:07:49.176058Z","iopub.execute_input":"2022-01-15T16:07:49.176602Z","iopub.status.idle":"2022-01-15T16:07:49.96916Z","shell.execute_reply.started":"2022-01-15T16:07:49.176562Z","shell.execute_reply":"2022-01-15T16:07:49.96822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def lstm_model(V1, V2, K, sequence_length):\n#     inputs = Input(shape=(sequence_length,))\n#     l0 = Embedding(V1, K, input_length=sequence_length,mask_zero=True)(inputs)\n#     l0 = BatchNormalization()(l0)\n#     l0 = LSTM(356)(l0)\n#     l0 = BatchNormalization()(l0)\n#     l0=Dense(20)(l0)\n#     l0=LeakyReLU()(l0)\n#     l0 = BatchNormalization()(l0)\n#     out = Dense(V2, activation='linear')(l0)\n#     model = Model(inputs=inputs, outputs=out)\n#     opt=tensorflow.keras.optimizers.Adam(learning_rate=0.004)\n# #     opt=tensorflow.keras.optimizers.SGD(learning_rate=0.001)\n#     model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n#     return model\n\n# model= lstm_model(len(w2v)+1, 1, 300, 30)\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:17:27.146768Z","iopub.execute_input":"2022-01-15T16:17:27.147244Z","iopub.status.idle":"2022-01-15T16:17:28.097113Z","shell.execute_reply.started":"2022-01-15T16:17:27.147206Z","shell.execute_reply":"2022-01-15T16:17:28.096393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lstm_model(V1, V2, K, sequence_length):\n    inputs = Input(shape=(sequence_length,))\n    l0 = Embedding(V1, K, input_length=sequence_length,mask_zero=True)(inputs)\n    l0 = BatchNormalization()(l0)\n    l0 = Bidirectional(LSTM(256))(l0)\n    l0 = BatchNormalization()(l0)\n    l0=Dense(10)(l0)\n    l0=LeakyReLU()(l0)\n    l0 = BatchNormalization()(l0)\n    out = Dense(V2, activation='linear')(l0)\n    model = Model(inputs=inputs, outputs=out)\n    opt=tensorflow.keras.optimizers.Adam(learning_rate=0.004)\n#     opt=tensorflow.keras.optimizers.SGD(learning_rate=0.001)\n    model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n    return model\n\nmodel= lstm_model(len(w2v)+1, 1, 200, 30)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:34:25.718168Z","iopub.execute_input":"2022-01-15T16:34:25.718443Z","iopub.status.idle":"2022-01-15T16:34:27.204624Z","shell.execute_reply.started":"2022-01-15T16:34:25.718413Z","shell.execute_reply":"2022-01-15T16:34:27.203936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,epochs=10)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:34:29.979746Z","iopub.execute_input":"2022-01-15T16:34:29.980206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text2vect(df,col):\n    X=np.zeros((len(list(df[col])),30),dtype='float32')\n    for i,txt in enumerate(list(df[col])):\n        j=0\n        for word in txt.split(' '):\n            if word in w2v and j<30:\n                X[i][j]=w2v[word]\n                j+=1\n    return X\n\n\ndef validation_result(df,col1,col2):\n    less_toxic=text2vect(df,col1)\n    more_toxic=text2vect(df,col2)\n    match=0\n    if len(less_toxic)==len(more_toxic):\n        \n        pred_less_toxic=model.predict(less_toxic)\n        pred_more_toxic=model.predict(more_toxic)\n        \n        for i in tqdm(range(len(pred_less_toxic))):\n            if pred_less_toxic[i]<=pred_more_toxic[i]:\n                match+=1\n        return match/len(pred_less_toxic)\n    return 'length not same'\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:32:57.806076Z","iopub.execute_input":"2022-01-15T16:32:57.806332Z","iopub.status.idle":"2022-01-15T16:32:57.817082Z","shell.execute_reply.started":"2022-01-15T16:32:57.806303Z","shell.execute_reply":"2022-01-15T16:32:57.816365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Number Check","metadata":{}},{"cell_type":"code","source":"print(validation_result(data_val,'less_toxic','more_toxic'))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:32:59.369049Z","iopub.execute_input":"2022-01-15T16:32:59.369322Z","iopub.status.idle":"2022-01-15T16:33:09.631734Z","shell.execute_reply.started":"2022-01-15T16:32:59.369291Z","shell.execute_reply":"2022-01-15T16:33:09.630923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission Part","metadata":{}},{"cell_type":"code","source":"df_sub=pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndf_sub=clean(df_sub,'text')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:33:16.78757Z","iopub.execute_input":"2022-01-15T16:33:16.788197Z","iopub.status.idle":"2022-01-15T16:33:23.755491Z","shell.execute_reply.started":"2022-01-15T16:33:16.788161Z","shell.execute_reply":"2022-01-15T16:33:23.754826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score=model.predict(text2vect(df_sub,'text'))\nscore.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:33:30.075569Z","iopub.execute_input":"2022-01-15T16:33:30.076194Z","iopub.status.idle":"2022-01-15T16:33:31.190311Z","shell.execute_reply.started":"2022-01-15T16:33:30.076152Z","shell.execute_reply":"2022-01-15T16:33:31.189509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score']=score\ndf_sub['score'] = df_sub['score'].rank(method='first')\ndf_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T16:33:37.482386Z","iopub.execute_input":"2022-01-15T16:33:37.483035Z","iopub.status.idle":"2022-01-15T16:33:37.516075Z","shell.execute_reply.started":"2022-01-15T16:33:37.482986Z","shell.execute_reply":"2022-01-15T16:33:37.515423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}