{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Private Score 0.80048**\n\n**Public Score 0.86644**","metadata":{"papermill":{"duration":0.017247,"end_time":"2022-02-12T02:32:04.503227","exception":false,"start_time":"2022-02-12T02:32:04.48598","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.execute_input":"2022-02-12T02:32:04.547763Z","iopub.status.busy":"2022-02-12T02:32:04.546854Z","iopub.status.idle":"2022-02-12T02:32:15.971883Z","shell.execute_reply":"2022-02-12T02:32:15.972353Z"},"papermill":{"duration":11.452878,"end_time":"2022-02-12T02:32:15.972765","exception":false,"start_time":"2022-02-12T02:32:04.519887","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/sklearn01/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nimport pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport sentencepiece","metadata":{"execution":{"iopub.execute_input":"2022-02-12T02:32:16.02229Z","iopub.status.busy":"2022-02-12T02:32:16.021462Z","iopub.status.idle":"2022-02-12T02:32:29.637223Z","shell.execute_reply":"2022-02-12T02:32:29.636712Z","shell.execute_reply.started":"2022-02-06T03:42:56.979761Z"},"papermill":{"duration":13.642675,"end_time":"2022-02-12T02:32:29.63736","exception":false,"start_time":"2022-02-12T02:32:15.994685","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding=\"utf-8\"\nglove_big = {}\nwith open(\"../input/glove-840b-300d/glove.840B.300d.txt\", \"rb\") as infile:\n    for line in infile:\n        parts = line.split()\n        word = parts[0].decode(encoding)\n        nums=np.array(parts[1:], dtype=np.float32)\n        glove_big[word] = nums\n\nclass MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        if len(word2vec)>0:\n            self.dim=len(word2vec[next(iter(glove_big))])\n        else:\n            self.dim=0\n            \n    def fit(self, X ,y):\n        return self \n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])","metadata":{"execution":{"iopub.execute_input":"2022-02-12T02:32:29.694768Z","iopub.status.busy":"2022-02-12T02:32:29.694127Z","iopub.status.idle":"2022-02-12T02:36:05.716434Z","shell.execute_reply":"2022-02-12T02:36:05.715916Z"},"papermill":{"duration":216.054574,"end_time":"2022-02-12T02:36:05.716605","exception":false,"start_time":"2022-02-12T02:32:29.662031","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport transformers\n\nfrom transformers import RobertaTokenizer\n\n\nclass BertSequenceVectorizer:\n    def __init__(self, model_name=\"bert-base-uncased\", max_len=128):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model_name = model_name\n        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n        self.bert_model = transformers.RobertaModel.from_pretrained(self.model_name)\n        self.bert_model = self.bert_model.to(self.device)\n        self.max_len = max_len\n\n    def vectorize(self, sentence: str) -> np.array:\n        inp = self.tokenizer.encode(sentence)\n        len_inp = len(inp)\n\n        if len_inp >= self.max_len:\n            inputs = inp[:self.max_len]\n            masks = [1] * self.max_len\n        else:\n            inputs = inp + [0] * (self.max_len - len_inp)\n            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n\n        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n\n        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n        seq_out = bert_out['last_hidden_state']\n#         seq_out = self.bert_model(inputs_tensor, masks_tensor)[0]\n#         pooled_out = self.bert_model(inputs_tensor, masks_tensor)[1]\n\n        if torch.cuda.is_available():    \n            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n        else:\n            return seq_out[0][0].detach().numpy()\n        ","metadata":{"execution":{"iopub.execute_input":"2022-02-12T02:36:05.775572Z","iopub.status.busy":"2022-02-12T02:36:05.7749Z","iopub.status.idle":"2022-02-12T02:36:07.491523Z","shell.execute_reply":"2022-02-12T02:36:07.490978Z","shell.execute_reply.started":"2022-02-06T03:43:30.420285Z"},"papermill":{"duration":1.751033,"end_time":"2022-02-12T02:36:07.491671","exception":false,"start_time":"2022-02-12T02:36:05.740638","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import LinearSVR\nfrom sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer,CountVectorizer\nimport scipy\nfrom scipy.stats import rankdata\nimport nltk\n\nstop_words = nltk.corpus.stopwords.words('english')\n\njr = pd.read_csv(\"../input/jigsaw-regression-based-data/train_data_version2.csv\")\njr.shape\ndf = jr[['text', 'y']]\ndf = df.drop_duplicates()\nBSV = BertSequenceVectorizer(model_name=\"../input/unbiasedtoxicroberta/unbiased-toxic-roberta\", max_len=512)\nX1 = np.stack(df[\"text\"].map(lambda x: BSV.vectorize(x).reshape(-1)).values)\nvec2 = TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=1, ngram_range=(2, 5), stop_words=stop_words)\nX2 = vec2.fit_transform(df['text'])\nX = scipy.sparse.hstack([X1, X2])\ndel X2\nz = df[\"y\"].values\ny=np.around ( z ,decimals = 2)\nmodel1=LinearSVR(max_iter=1000000)\nmodel1.fit(X, y)\nmodel2=Ridge(alpha=0.5)\nmodel2.fit(X, y)\nmodel3=Ridge(alpha=1.0)\nmodel3.fit(X, y)\n\nVALID_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\"\ndf_valid2 = pd.read_csv(VALID_DATA_PATH)\nless_toxic1 = np.stack(df_valid2['less_toxic'].map(lambda x: BSV.vectorize(x).reshape(-1)).values)\nmore_toxic1 = np.stack(df_valid2['more_toxic'].map(lambda x: BSV.vectorize(x).reshape(-1)).values)\nless_toxic2 = vec2.transform(df_valid2['less_toxic'])\nmore_toxic2 = vec2.transform(df_valid2['more_toxic'])\nless_toxic = scipy.sparse.hstack([less_toxic1, less_toxic2])\nmore_toxic = scipy.sparse.hstack([more_toxic1, more_toxic2])\ndel less_toxic2,more_toxic2\ny_pred_less = model1.predict(less_toxic)\ny_pred_more = model1.predict(more_toxic)\nprint(f'val : {(y_pred_less < y_pred_more).mean()}')\ny_pred_less2 = model2.predict(less_toxic)\ny_pred_more2 = model2.predict(more_toxic)\nprint(f'val : {(y_pred_less2 < y_pred_more2).mean()}')\ny_pred_less3 = model3.predict(less_toxic)\ny_pred_more3 = model3.predict(more_toxic)\nprint(f'val : {(y_pred_less3 < y_pred_more3).mean()}')\n\ndf_test = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ntest1 = np.stack(df_test['text'].map(lambda x: BSV.vectorize(x).reshape(-1)).values)\ntest2 = vec2.transform(df_test['text'])\ntest = scipy.sparse.hstack([test1, test2])\ndel test2\njr_preds=model1.predict(test)\ndf_test['score1']=rankdata( jr_preds, method='ordinal') \njr_preds11=model2.predict(test)\ndf_test['score11']=rankdata( jr_preds11, method='ordinal') \njr_preds111=model3.predict(test)\ndf_test['score111']=rankdata( jr_preds111, method='ordinal') \ndf_test['score1']=(df_test['score1']+df_test['score11']+df_test['score111'])/3\n\nrud_df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\nrud_df.loc[((rud_df[\"txt\"]==\"[deleted]\")|(rud_df[\"txt\"]==\"[removed]\")),\"txt\"] = np.nan\nrud_df=rud_df.dropna(how=\"any\")\n#print(f\"rud_df:{rud_df.shape}\")\nrud_df['y'] = rud_df[\"offensiveness_score\"] \ndf = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\nX = np.stack(df[\"text\"].map(lambda x: BSV.vectorize(x).reshape(-1)).values)\nvec2 = TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=3, ngram_range=(3, 4), stop_words=stop_words)\nX2 = vec2.fit_transform(df['text'])\nX = scipy.sparse.hstack([X, X2])\ndel X2\nz = df[\"y\"].values\ny=np.around ( z ,decimals = 1)\nmodel1=Ridge(alpha=0.1)\nmodel1.fit(X, y)\nmodel2=Ridge(alpha=0.5)\nmodel2.fit(X, y)\nmodel3=LinearSVR(max_iter=1000000)\nmodel3.fit(X, y)\n\nVALID_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\"\ndf_valid2 = pd.read_csv(VALID_DATA_PATH)\nless_toxic2 = vec2.transform(df_valid2['less_toxic'])\nmore_toxic2 = vec2.transform(df_valid2['more_toxic'])\nless_toxic = scipy.sparse.hstack([less_toxic1, less_toxic2])\nmore_toxic = scipy.sparse.hstack([more_toxic1, more_toxic2])\ndel less_toxic2,more_toxic2\ny_pred_less = model1.predict(less_toxic)\ny_pred_more = model1.predict(more_toxic)\nprint(f'val : {(y_pred_less < y_pred_more).mean()}')\ny_pred_less2 = model2.predict(less_toxic)\ny_pred_more2 = model2.predict(more_toxic)\nprint(f'val : {(y_pred_less2 < y_pred_more2).mean()}')\ny_pred_less3 = model3.predict(less_toxic)\ny_pred_more3 = model3.predict(more_toxic)\nprint(f'val : {(y_pred_less3 < y_pred_more3).mean()}')\n\ntest2 = vec2.transform(df_test['text'])\ntest = scipy.sparse.hstack([test1, test2])\ndel test2\nrud_preds=model1.predict(test)\ndf_test['score2']=rankdata( rud_preds, method='ordinal')\nrud_preds22=model2.predict(test)\ndf_test['score22']=rankdata( rud_preds22, method='ordinal')\nrud_preds222=model3.predict(test)\ndf_test['score222']=rankdata( rud_preds222, method='ordinal')\ndf_test['score2']=(df_test['score2']+df_test['score22']+df_test['score222'])/3\ndf_test['score']=df_test['score1']+df_test['score2']\ndf_test['score']=rankdata( df_test['score'], method='ordinal')\ndf_test[['comment_id', 'score']].to_csv(\"submission1.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2022-02-12T02:36:07.56567Z","iopub.status.busy":"2022-02-12T02:36:07.555603Z","iopub.status.idle":"2022-02-12T07:31:19.120597Z","shell.execute_reply":"2022-02-12T07:31:19.120085Z"},"papermill":{"duration":17711.604809,"end_time":"2022-02-12T07:31:19.120753","exception":false,"start_time":"2022-02-12T02:36:07.515944","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Sub2 : 0.860](https://www.kaggle.com/coldfir3/tokenizer-training-tfidf-ridge-lb-0-860)","metadata":{"papermill":{"duration":0.025775,"end_time":"2022-02-12T07:31:19.171944","exception":false,"start_time":"2022-02-12T07:31:19.146169","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Credit : [Tokenizer training + TFIDF + RIDGE [LB 0.860]](https://www.kaggle.com/coldfir3/tokenizer-training-tfidf-ridge-lb-0-860) by [Adriano Passos](https://www.kaggle.com/coldfir3)","metadata":{"papermill":{"duration":0.025746,"end_time":"2022-02-12T07:31:19.223813","exception":false,"start_time":"2022-02-12T07:31:19.198067","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\n\nTRAIN_DATA_PATH = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\nVALID_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\"\nTEST_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\n\ndf_train2 = pd.read_csv(TRAIN_DATA_PATH)\ndf_valid2 = pd.read_csv(VALID_DATA_PATH)\ndf_test2 = pd.read_csv(TEST_DATA_PATH)\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_train2[category] = df_train2[category] * cat_mtpl[category]\n\ndf_train2['score'] = df_train2.loc[:, 'toxic':'identity_hate'].mean(axis=1)\ndf_train2['y'] = df_train2['score']\n\nmin_len = (df_train2['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train2[df_train2['y'] == 0].sample(n=min_len, random_state=41)  # take non toxic comments\ndf_train_new = pd.concat([df_train2[df_train2['y'] > 0], df_y0_undersample])  # make new df\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nraw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\nraw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\nraw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\nfrom datasets import Dataset\n\ndataset = Dataset.from_pandas(df_train_new[['comment_text']])\n\ndef get_training_corpus():\n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"comment_text\"]\n\nraw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n\nfrom transformers import PreTrainedTokenizerFast\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\n\ndef dummy_fun(doc):\n    return doc\n\nlabels = df_train_new['y']\ncomments = df_train_new['comment_text']\ntokenized_comments = tokenizer(comments.to_list())['input_ids']\n\nvectorizer = TfidfVectorizer(\n    analyzer = 'word',\n    tokenizer = dummy_fun,\n    preprocessor = dummy_fun,\n    token_pattern = None)\n\ncomments_tr = vectorizer.fit_transform(tokenized_comments)\n\nregressor = Ridge(random_state=42, alpha=0.8)\nregressor.fit(comments_tr, labels)\nregressor2 = Ridge(random_state=42, alpha=0.6)\nregressor2.fit(comments_tr, labels)\nregressor3 = Ridge(random_state=42, alpha=1.0)\nregressor3.fit(comments_tr, labels)\n\nless_toxic_comments = df_valid2['less_toxic']\nmore_toxic_comments = df_valid2['more_toxic']\n\nless_toxic_comments = tokenizer(less_toxic_comments.to_list())['input_ids']\nmore_toxic_comments = tokenizer(more_toxic_comments.to_list())['input_ids']\n\nless_toxic = vectorizer.transform(less_toxic_comments)\nmore_toxic = vectorizer.transform(more_toxic_comments)\n\n# make predictions\ny_pred_less = regressor.predict(less_toxic)\ny_pred_more = regressor.predict(more_toxic)\ny_pred_less2 = regressor2.predict(less_toxic)\ny_pred_more2 = regressor2.predict(more_toxic)\ny_pred_less3 = regressor3.predict(less_toxic)\ny_pred_more3 = regressor3.predict(more_toxic)\n\nprint(f'val : {(y_pred_less < y_pred_more).mean()}')\nprint(f'val : {(y_pred_less2 < y_pred_more2).mean()}')\nprint(f'val : {(y_pred_less3 < y_pred_more3).mean()}')\ntexts = df_test2['text']\ntexts = tokenizer(texts.to_list())['input_ids']\ntexts = vectorizer.transform(texts)\n\ndf_test2['prediction'] = regressor.predict(texts)\ndf_test2['prediction2'] = regressor2.predict(texts)\ndf_test2['prediction3'] = regressor3.predict(texts)\ndf_test2 = df_test2[['comment_id','prediction','prediction2','prediction3']]\n\ndf_test2['score'] = (df_test2['prediction'] + df_test2['prediction2'] + df_test2['prediction3'])/3\ndf_test2 = df_test2[['comment_id','score']]\n\ndf_test2.to_csv('./submission2.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2022-02-12T07:31:19.297769Z","iopub.status.busy":"2022-02-12T07:31:19.296943Z","iopub.status.idle":"2022-02-12T07:32:00.849916Z","shell.execute_reply":"2022-02-12T07:32:00.85037Z","shell.execute_reply.started":"2022-01-29T06:41:39.35981Z"},"papermill":{"duration":41.600695,"end_time":"2022-02-12T07:32:00.850602","exception":false,"start_time":"2022-02-12T07:31:19.249907","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Sub3 : 0.858](https://www.kaggle.com/tenffe/rapids-tfidf-linear-model-ensemble/notebook)","metadata":{"papermill":{"duration":0.026475,"end_time":"2022-02-12T07:32:00.904343","exception":false,"start_time":"2022-02-12T07:32:00.877868","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Credit : [[RAPIDS] TFIDF_linear_model_ensemble](https://www.kaggle.com/tenffe/rapids-tfidf-linear-model-ensemble/notebook) by [zhangxin](https://www.kaggle.com/tenffe)","metadata":{"papermill":{"duration":0.026243,"end_time":"2022-02-12T07:32:00.958031","exception":false,"start_time":"2022-02-12T07:32:00.931788","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n\nimport re \nimport scipy\nfrom scipy import sparse\n\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \n\nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge, Lasso, BayesianRidge\nfrom sklearn.svm import SVR\n\ndf_train = pd.read_csv(\"../input/jigsawtoxicdataset05/jigsaw-toxic-comment-train.csv\")\n# df_train = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201, replace=True)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\ndf_train = df_train_new.rename(columns={'comment_text':'text'})\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    #remove multiple toxic words\n#     text = re.sub(r\"#ofc\", \" of fuckin course \",text)\n#     text = re.sub(r\"fggt\", \" faggot \",text)\n#     text = re.sub(r\"cuntbag\", \" cunt bag \",text)\n#     text = re.sub(r\"fartchina\", \" fart china \",text)    \n#     text = re.sub(r\"youi\", \" you i \",text)\n#     text = re.sub(r\"cunti\", \" cunt i \",text)\n#     text = re.sub(r\"sucki\", \" suck i \",text)\n#     text = re.sub(r\"pagedelete\", \" page delete \",text)\n#     text = re.sub(r\"offuck\", \" of fuck \",text)\n#     text = re.sub(r\"centraliststupid\", \" central ist stupid \",text)\n#     text = re.sub(r\"hitleri\", \" hitler i \",text)\n#     text = re.sub(r\"f u c k\", \" fuck \",text)\n#     text = re.sub(r\"bunksteve\", \" bunk steve \",text)\n#     text = re.sub(r\"sexsex\", \" sex \",text)\n#     text = re.sub(r\"youbollocks\", \" you bollocks \",text)\n#     text = re.sub(r\"mothjer\", \" mother \",text)\n#     text = re.sub(r\"cuntfranks\", \" cunt \",text)\n#     text = re.sub(r\"ullmann\", \" jewish \",text)\n#     text = re.sub(r\"mr.\", \" mister \",text)\n#     text = re.sub(r\"aidsaids\", \" aids \",text)\n#     text = re.sub(r\"njgw\", \" nigger \",text)\n#     text = re.sub(r\"administrator\", \" admin \",text)\n#     text = re.sub(r\"gamaliel\", \" jewish \",text)\n#     text = re.sub(r\"rvv\", \" vanadalism \",text)\n#     text = re.sub(r\"admins\", \" admin \",text)\n#     text = re.sub(r\"pensnsnniensnsn\", \" penis \",text)\n#     text = re.sub(r\"pneis\", \" penis \",text)\n#     text = re.sub(r\"pennnis\", \" penis \",text)\n#     text = re.sub(r\"pov.\", \" point of view \",text)\n#     text = re.sub(r\"vandalising\", \" vandalism \",text)\n#     text = re.sub(r\"cock\", \" dick \",text)\n#     text = re.sub(r\"youi\", \" you \",text)\n#     text = re.sub(r\"afd\", \" all fucking day \",text)\n#     text = re.sub(r\"sockpuppets\", \" sockpuppetry \",text)\n#     text = re.sub(r\"iiprick\", \" iprick \",text)\n#     text = re.sub(r\"penisi\", \" penis \",text)\n#     text = re.sub(r\"loil\", \" laughing out insanely loud \",text)\n#     text = re.sub(r\"ilol\", \" i lol \",text)\n#     text = re.sub(r'\\b[uU]\\b', 'you',text)\n##\n#     text = re.sub(r'(fuckfuck)','fuck fuck ',text)\n#     text = re.sub(r'(f+)( *)([u|*]+)( *)([c|*]+)( *)(k)+','fuck',text)\n#     text = re.sub(r'(s+ *h+ *i+ *t+)','shit',text)\n#     text = re.sub(r'([a|@][$|s][s|$])','ass',text)\n#     text = re.sub(r'(\\bfuk\\b)','fuck',text)\n#     text = re.sub(r'(\\bpennnis\\b)','penis',text)\n#     text = re.sub(r'(\\bfggt\\b)','faggot',text)\n#     text = re.sub(r'(\\bfag\\b)','faggot',text)\n\n#     #replaces abreviations\n#     text = re.sub(r\"what's\", \"what is \",text)    \n#     text = re.sub(r\"\\'ve\", \" have \",text)\n#     text = re.sub(r\"can't\", \"cannot \",text)\n#     text = re.sub(r\"n't\", \" not \",text)\n#     text = re.sub(r\"i'm\", \"i am \",text)\n#     text = re.sub(r\"\\'re\", \" are \",text)\n#     text = re.sub(r\"\\'ll\", \" will \",text)\n#     text = re.sub(r\"\\'scuse\", \" excuse \",text)\n\n#     #remove unicode strings\n#     \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n#     text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r' ', text)       \n#     text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n\n#     \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n#     text = re.sub('@[^\\s]+','atUser',text)\n        \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\ntqdm.pandas()\ndf_train['text'] = df_train['text'].progress_apply(text_cleaning)\ndf = df_train.copy()\ndf['y'].value_counts(normalize=True)\nmin_len = (df['y'] >= 0.1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len * 2, random_state=402, replace=True)\ndf = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=1, ngram_range=(2, 5))\nX = vec.fit_transform(df['text'])\nvec2 = MeanEmbeddingVectorizer(glove_big)\nvec2.fit(df['text'],df['y'])\nX2 = vec2.transform(df['text'])\nX = scipy.sparse.hstack([X, X2])\ndel X2\nmodel = LinearSVR(max_iter=1000000)\nmodel.fit(X, df['y'])\nl_model = Ridge(alpha=1.0)\nl_model.fit(X, df['y'])\ns_model = Ridge(alpha=2.)\ns_model.fit(X, df['y'])\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ntqdm.pandas()\ndf_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\ndf_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)\nX_less_toxic = vec.transform(df_val['less_toxic'])\nX_more_toxic = vec.transform(df_val['more_toxic'])\nX_less_toxic2 = vec2.transform(df_val['less_toxic'])\nX_more_toxic2 = vec2.transform(df_val['more_toxic'])\nX_less_toxic = scipy.sparse.hstack([X_less_toxic, X_less_toxic2])\nX_more_toxic = scipy.sparse.hstack([X_more_toxic, X_more_toxic2])\ndel X_less_toxic2,X_more_toxic2\np1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)\nprint(f'val : {(p1 < p2).mean()}')\np3 = l_model.predict(X_less_toxic)\np4 = l_model.predict(X_more_toxic)\nprint(f'val : {(p3 < p4).mean()}')\np5 = s_model.predict(X_less_toxic)\np6 = s_model.predict(X_more_toxic)\nprint(f'val : {(p5 < p6).mean()}')\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ntqdm.pandas()\ndf_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\nX_test = vec.transform(df_sub['text'])\nX_test2 = vec2.transform(df_sub['text'])\nX_test = scipy.sparse.hstack([X_test, X_test2])\ndel X_test2\np3 = model.predict(X_test)\np4 = l_model.predict(X_test)\np5 = s_model.predict(X_test)\ndf_sub['score'] = (p3 + p4 + p5) / 3.\ndf_sub['score'] = df_sub['score']\ndf_sub[['comment_id', 'score']].to_csv(\"submission3.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2022-02-12T07:32:01.042857Z","iopub.status.busy":"2022-02-12T07:32:01.042119Z","iopub.status.idle":"2022-02-12T07:55:18.792927Z","shell.execute_reply":"2022-02-12T07:55:18.79436Z","shell.execute_reply.started":"2022-01-29T06:41:39.362076Z"},"papermill":{"duration":1397.810262,"end_time":"2022-02-12T07:55:18.794605","exception":false,"start_time":"2022-02-12T07:32:00.984343","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport re \nimport scipy\nfrom scipy import sparse\n\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \n\nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge, Lasso, BayesianRidge\nfrom sklearn.svm import SVR\n\ndf_train = pd.read_csv(\"../input/jigsaw-regression-based-data/train_data_version2.csv\")\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf_train = df_train.drop_duplicates()\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    #remove multiple toxic words\n#     text = re.sub(r\"#ofc\", \" of fuckin course \",text)\n#     text = re.sub(r\"fggt\", \" faggot \",text)\n#     text = re.sub(r\"cuntbag\", \" cunt bag \",text)\n#     text = re.sub(r\"fartchina\", \" fart china \",text)    \n#     text = re.sub(r\"youi\", \" you i \",text)\n#     text = re.sub(r\"cunti\", \" cunt i \",text)\n#     text = re.sub(r\"sucki\", \" suck i \",text)\n#     text = re.sub(r\"pagedelete\", \" page delete \",text)\n#     text = re.sub(r\"offuck\", \" of fuck \",text)\n#     text = re.sub(r\"centraliststupid\", \" central ist stupid \",text)\n#     text = re.sub(r\"hitleri\", \" hitler i \",text)\n#     text = re.sub(r\"f u c k\", \" fuck \",text)\n#     text = re.sub(r\"bunksteve\", \" bunk steve \",text)\n#     text = re.sub(r\"sexsex\", \" sex \",text)\n#     text = re.sub(r\"youbollocks\", \" you bollocks \",text)\n#     text = re.sub(r\"mothjer\", \" mother \",text)\n#     text = re.sub(r\"cuntfranks\", \" cunt \",text)\n#     text = re.sub(r\"ullmann\", \" jewish \",text)\n#     text = re.sub(r\"mr.\", \" mister \",text)\n#     text = re.sub(r\"aidsaids\", \" aids \",text)\n#     text = re.sub(r\"njgw\", \" nigger \",text)\n#     text = re.sub(r\"administrator\", \" admin \",text)\n#     text = re.sub(r\"gamaliel\", \" jewish \",text)\n#     text = re.sub(r\"rvv\", \" vanadalism \",text)\n#     text = re.sub(r\"admins\", \" admin \",text)\n#     text = re.sub(r\"pensnsnniensnsn\", \" penis \",text)\n#     text = re.sub(r\"pneis\", \" penis \",text)\n#     text = re.sub(r\"pennnis\", \" penis \",text)\n#     text = re.sub(r\"pov.\", \" point of view \",text)\n#     text = re.sub(r\"vandalising\", \" vandalism \",text)\n#     text = re.sub(r\"cock\", \" dick \",text)\n#     text = re.sub(r\"youi\", \" you \",text)\n#     text = re.sub(r\"afd\", \" all fucking day \",text)\n#     text = re.sub(r\"sockpuppets\", \" sockpuppetry \",text)\n#     text = re.sub(r\"iiprick\", \" iprick \",text)\n#     text = re.sub(r\"penisi\", \" penis \",text)\n#     text = re.sub(r\"loil\", \" laughing out insanely loud \",text)\n#     text = re.sub(r\"ilol\", \" i lol \",text)\n#     text = re.sub(r'\\b[uU]\\b', 'you',text)\n##\n#     text = re.sub(r'(fuckfuck)','fuck fuck ',text)\n#     text = re.sub(r'(f+)( *)([u|*]+)( *)([c|*]+)( *)(k)+','fuck',text)\n#     text = re.sub(r'(s+ *h+ *i+ *t+)','shit',text)\n#     text = re.sub(r'([a|@][$|s][s|$])','ass',text)\n#     text = re.sub(r'(\\bfuk\\b)','fuck',text)\n#     text = re.sub(r'(\\bpennnis\\b)','penis',text)\n#     text = re.sub(r'(\\bfggt\\b)','faggot',text)\n#     text = re.sub(r'(\\bfag\\b)','faggot',text)\n\n#     #replaces abreviations\n#     text = re.sub(r\"what's\", \"what is \",text)    \n#     text = re.sub(r\"\\'ve\", \" have \",text)\n#     text = re.sub(r\"can't\", \"cannot \",text)\n#     text = re.sub(r\"n't\", \" not \",text)\n#     text = re.sub(r\"i'm\", \"i am \",text)\n#     text = re.sub(r\"\\'re\", \" are \",text)\n#     text = re.sub(r\"\\'ll\", \" will \",text)\n#     text = re.sub(r\"\\'scuse\", \" excuse \",text)\n\n#     #remove unicode strings\n#     \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n#     text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r' ', text)       \n#     text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n\n#     \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n#     text = re.sub('@[^\\s]+','atUser',text)\n        \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\ntqdm.pandas()\ndf_train['text'] = df_train['text'].progress_apply(text_cleaning)\ndf = df_train.copy()\ndf['y'].value_counts(normalize=True)\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=1, ngram_range=(2, 5))\nX = vec.fit_transform(df['text'])\nvec2 = HashingVectorizer(analyzer='char_wb', ngram_range=(2, 5))\nX2 = vec2.fit_transform(df['text'])\nX = scipy.sparse.hstack([X, X2])\ndel X2\nmodel = LinearSVR(max_iter=1000000)\nmodel.fit(X, df['y'])\nl_model = Ridge(alpha=1.0)\nl_model.fit(X, df['y'])\ns_model = Ridge(alpha=2.)\ns_model.fit(X, df['y'])\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ntqdm.pandas()\ndf_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\ndf_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)\nX_less_toxic = vec.transform(df_val['less_toxic'])\nX_more_toxic = vec.transform(df_val['more_toxic'])\nX_less_toxic2 = vec2.transform(df_val['less_toxic'])\nX_more_toxic2 = vec2.transform(df_val['more_toxic'])\nX_less_toxic = scipy.sparse.hstack([X_less_toxic, X_less_toxic2])\nX_more_toxic = scipy.sparse.hstack([X_more_toxic, X_more_toxic2])\ndel X_less_toxic2,X_more_toxic2\np1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)\nprint(f'val : {(p1 < p2).mean()}')\np3 = l_model.predict(X_less_toxic)\np4 = l_model.predict(X_more_toxic)\nprint(f'val : {(p3 < p4).mean()}')\np5 = s_model.predict(X_less_toxic)\np6 = s_model.predict(X_more_toxic)\nprint(f'val : {(p5 < p6).mean()}')\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ntqdm.pandas()\ndf_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\nX_test = vec.transform(df_sub['text'])\nX_test2 = vec2.transform(df_sub['text'])\nX_test = scipy.sparse.hstack([X_test, X_test2])\ndel X_test2\np3 = model.predict(X_test)\np4 = l_model.predict(X_test)\np5 = s_model.predict(X_test)\ndf_sub['score'] = (p3 + p4 + p5) / 3.\ndf_sub['score'] = df_sub['score']\ndf_sub[['comment_id', 'score']].to_csv(\"submission33.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2022-02-12T07:55:18.926301Z","iopub.status.busy":"2022-02-12T07:55:18.925292Z","iopub.status.idle":"2022-02-12T08:02:03.968906Z","shell.execute_reply":"2022-02-12T08:02:03.96837Z","shell.execute_reply.started":"2022-01-29T06:41:39.364181Z"},"papermill":{"duration":405.109912,"end_time":"2022-02-12T08:02:03.969051","exception":false,"start_time":"2022-02-12T07:55:18.859139","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"./submission1.csv\",index_col=\"comment_id\")\ndata[\"score1\"] = data[\"score\"]\n\ndata[\"score2\"] = pd.read_csv(\"./submission2.csv\",index_col=\"comment_id\")[\"score\"]\ndata[\"score2\"] = rankdata( data[\"score2\"], method='ordinal')\n\ndata[\"score3\"] = pd.read_csv(\"./submission3.csv\",index_col=\"comment_id\")[\"score\"]\ndata[\"score3\"] = rankdata( data[\"score3\"], method='ordinal')\n\ndata[\"score33\"] = pd.read_csv(\"./submission33.csv\",index_col=\"comment_id\")[\"score\"]\ndata[\"score33\"] = rankdata( data[\"score33\"], method='ordinal')\n\ndata[\"score\"] = .75*data[\"score1\"] + .55*data[\"score2\"] + (0.5*data[\"score3\"]+0.5*data[\"score33\"])*.75","metadata":{"execution":{"iopub.execute_input":"2022-02-12T08:02:04.041489Z","iopub.status.busy":"2022-02-12T08:02:04.040694Z","iopub.status.idle":"2022-02-12T08:02:04.073859Z","shell.execute_reply":"2022-02-12T08:02:04.073278Z","shell.execute_reply.started":"2022-01-29T06:41:39.366096Z"},"papermill":{"duration":0.072822,"end_time":"2022-02-12T08:02:04.073997","exception":false,"start_time":"2022-02-12T08:02:04.001175","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"score\"] = rankdata( data[\"score\"], method='ordinal')\ndata.head()","metadata":{"execution":{"iopub.execute_input":"2022-02-12T08:02:04.14255Z","iopub.status.busy":"2022-02-12T08:02:04.141746Z","iopub.status.idle":"2022-02-12T08:02:04.154868Z","shell.execute_reply":"2022-02-12T08:02:04.154314Z","shell.execute_reply.started":"2022-01-29T06:41:39.367988Z"},"papermill":{"duration":0.049313,"end_time":"2022-02-12T08:02:04.154984","exception":false,"start_time":"2022-02-12T08:02:04.105671","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = data\ndf_test[\"score\"].to_csv('./submission.csv')","metadata":{"execution":{"iopub.execute_input":"2022-02-12T08:02:04.225518Z","iopub.status.busy":"2022-02-12T08:02:04.224734Z","iopub.status.idle":"2022-02-12T08:02:04.247826Z","shell.execute_reply":"2022-02-12T08:02:04.24726Z","shell.execute_reply.started":"2022-01-29T06:41:39.371687Z"},"papermill":{"duration":0.061047,"end_time":"2022-02-12T08:02:04.247971","exception":false,"start_time":"2022-02-12T08:02:04.186924","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"./submission.csv\")","metadata":{"execution":{"iopub.execute_input":"2022-02-12T08:02:04.31817Z","iopub.status.busy":"2022-02-12T08:02:04.317364Z","iopub.status.idle":"2022-02-12T08:02:04.331406Z","shell.execute_reply":"2022-02-12T08:02:04.331909Z","shell.execute_reply.started":"2022-01-29T06:41:39.373544Z"},"papermill":{"duration":0.051118,"end_time":"2022-02-12T08:02:04.332047","exception":false,"start_time":"2022-02-12T08:02:04.280929","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}