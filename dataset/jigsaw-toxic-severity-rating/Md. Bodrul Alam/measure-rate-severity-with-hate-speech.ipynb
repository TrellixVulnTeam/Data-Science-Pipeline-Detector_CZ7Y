{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import rankdata","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:21:19.005925Z","iopub.execute_input":"2022-02-06T16:21:19.00659Z","iopub.status.idle":"2022-02-06T16:21:19.013338Z","shell.execute_reply.started":"2022-02-06T16:21:19.006541Z","shell.execute_reply":"2022-02-06T16:21:19.012202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"validation = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\nhspeech = pd.read_csv('../input/measuring-hate-speech/measuring_hate_speech.csv')\nsubmission = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:21:19.015273Z","iopub.execute_input":"2022-02-06T16:21:19.015781Z","iopub.status.idle":"2022-02-06T16:21:22.955713Z","shell.execute_reply.started":"2022-02-06T16:21:19.015725Z","shell.execute_reply":"2022-02-06T16:21:22.954711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hspeech.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:21:22.957071Z","iopub.execute_input":"2022-02-06T16:21:22.957334Z","iopub.status.idle":"2022-02-06T16:21:22.988696Z","shell.execute_reply.started":"2022-02-06T16:21:22.957301Z","shell.execute_reply":"2022-02-06T16:21:22.987685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get mean scores for each comment_id\nscores_dict = hspeech.groupby('comment_id')['hate_speech_score'].apply(np.mean).to_dict()\n\n# drop duplicate comment_ids\nhspeech = hspeech.drop_duplicates(subset='comment_id')\nhspeech['hate_speech_score'] = hspeech['comment_id'].map(scores_dict)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:21:22.990116Z","iopub.execute_input":"2022-02-06T16:21:22.990357Z","iopub.status.idle":"2022-02-06T16:21:25.576945Z","shell.execute_reply.started":"2022-02-06T16:21:22.990331Z","shell.execute_reply":"2022-02-06T16:21:25.575843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plots","metadata":{}},{"cell_type":"markdown","source":"## Plot Hate Speech Scores Histogram","metadata":{}},{"cell_type":"code","source":"hspeech['hate_speech_score'].plot.hist(bins=100, title='Hate speech scores')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:21:25.579317Z","iopub.execute_input":"2022-02-06T16:21:25.57968Z","iopub.status.idle":"2022-02-06T16:21:26.001036Z","shell.execute_reply.started":"2022-02-06T16:21:25.579648Z","shell.execute_reply":"2022-02-06T16:21:26.000025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compare text size with competition validation data","metadata":{}},{"cell_type":"code","source":"hspeech['text'].apply(lambda x: len(x.split())).plot.hist(bins=100)\npd.concat([\n    validation['less_toxic'],\n    validation['more_toxic']\n]).to_frame('text')['text'].apply(lambda x: len(x.split())).plot.hist(bins=100, alpha=0.5, figsize=(14, 7), title='Text size') #blue validation - orange hspeech data","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:21:26.002577Z","iopub.execute_input":"2022-02-06T16:21:26.002845Z","iopub.status.idle":"2022-02-06T16:21:27.045495Z","shell.execute_reply.started":"2022-02-06T16:21:26.002816Z","shell.execute_reply":"2022-02-06T16:21:27.044548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text cleaning","metadata":{}},{"cell_type":"markdown","source":"The text is already quite clean but some extra pre-processing is added:\n* Remove URL with 'url'\n* Remove unicode strings\n* Remove numbers\n* Lemmatization","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n\nwordnet_lemmatizer = WordNetLemmatizer()\ndef replaceURL(text):\n    \"\"\" Replaces url address with \"url\" \"\"\"\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    return text\n\ndef replaceAbbrev(text):\n    text = re.sub(r\"what's\", \"what is \",text)    \n    text = re.sub(r\"\\'ve\", \" have \",text)\n    text = re.sub(r\"can't\", \"cannot \",text)\n    text = re.sub(r\"n't\", \" not \",text)\n    text = re.sub(r\"i'm\", \"i am \",text)\n    text = re.sub(r\"\\'re\", \" are \",text)\n    text = re.sub(r\"\\'d\", \" would \",text)\n    text = re.sub(r\"\\'ll\", \" will \",text)\n    text = re.sub(r\"\\'scuse\", \" excuse \",text)\n    text = re.sub(r\"\\'s\", \" \",text)\n    return text\n\ndef removeUnicode(text):\n    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r' ', text)       \n    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n    return text\ndef removeRepeatPattern(text):\n    text=re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1',text)\n    text=re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1',text)\n    text=re.sub(r'[ ]{2,}',' ',text)\n    return text\n\ndef replaceAtUser(text):\n    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n    text = re.sub('@[^\\s]+','atUser',text)\n    return text\n\ndef replaceMultiToxicWords(text):\n    text = re.sub(r'(fuckfuck)','fuck fuck ',text)\n    text = re.sub(r'(f+)( *)([u|*|_]+)( *)([c|*|_]+)( *)(k)+','fuck',text)\n    text = re.sub(r'(h+)(a+)(h+)(a+)','ha ha ',text)\n    text = re.sub(r'(s+ *h+ *[i|!]+ *t+)','shit',text)\n    text = re.sub(r'\\b(n+)(i+)(g+)(a+)\\b','nigga',text)\n    text = re.sub(r'\\b(n+)([i|!]+)(g+)(e+)(r+)\\b','nigger',text)\n    text = re.sub(r'\\b(d+)(o+)(u+)(c+)(h+)(e+)( *)(b+)(a+)(g+)\\b','douchebag',text)\n    text = re.sub(r'([a|@][$|s][s|$])','ass',text)\n    text = re.sub(r'(\\bfuk\\b)','fuck',text)\n    return text\n\ndef removeNumbers(text):\n    \"\"\" Removes integers \"\"\"\n    text = re.sub(r\"(^|\\W)\\d+\", \" \", text)\n    text = re.sub(\"5\",\"s\",text)\n    text = re.sub(\"1\",\"i\",text)\n    text = re.sub(\"0\",\"o\",text)\n    return text\n                  \ndef replaceMultiPunc(text):\n    text=re.sub(r'([!])\\1\\1{2,}',r' mxm ',text)\n    text=re.sub(r'([?])\\1\\1{2,}',r' mqm ',text)\n    text=re.sub(r'([*])\\1\\1{2,}',r'*',text)\n    return text\n\n\nreplace_pun = {}\nseparators = set('\"%&\\'()+,-./:;<=>@[\\\\]^_`{|}~')\nfor punc in separators:\n    replace_pun[punc] = ' '\nreplace_pun['&']=' and '\n\ndef my_cleaner(s):\n    #s = s.lower()\n    s=replaceURL(s)\n    s=removeUnicode(s)\n    s=removeNumbers(s)\n    s=replaceAbbrev(s)\n    s=replaceMultiToxicWords(s)\n    s=replaceMultiPunc(s)\n    s=removeRepeatPattern(s)\n    \n    for punc in separators:\n        s= s.replace(punc,replace_pun[punc])                   # remove & replace punctuations\n    tokens = nltk.tokenize.word_tokenize(s)                    # split a string into words (tokens)\n    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n    return ' '.join(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:21:27.047043Z","iopub.execute_input":"2022-02-06T16:21:27.047312Z","iopub.status.idle":"2022-02-06T16:21:27.070756Z","shell.execute_reply.started":"2022-02-06T16:21:27.04728Z","shell.execute_reply":"2022-02-06T16:21:27.069532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clean all texts (train data, validation, and submission)","metadata":{}},{"cell_type":"code","source":"hspeech['text'].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:21:27.072425Z","iopub.execute_input":"2022-02-06T16:21:27.072798Z","iopub.status.idle":"2022-02-06T16:21:27.093587Z","shell.execute_reply.started":"2022-02-06T16:21:27.072754Z","shell.execute_reply":"2022-02-06T16:21:27.092361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hspeech['text_clean'] = hspeech['text'].apply(my_cleaner)\nvalidation['less_toxic'] = validation['less_toxic'].apply(my_cleaner)\nvalidation['more_toxic'] = validation['more_toxic'].apply(my_cleaner)\nsubmission['text'] = submission['text'].apply(my_cleaner)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:59:54.325645Z","iopub.execute_input":"2022-02-06T16:59:54.326006Z","iopub.status.idle":"2022-02-06T17:02:06.125596Z","shell.execute_reply.started":"2022-02-06T16:59:54.325967Z","shell.execute_reply":"2022-02-06T17:02:06.124738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hspeech['text_clean']","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:16:27.979154Z","iopub.execute_input":"2022-02-06T17:16:27.980341Z","iopub.status.idle":"2022-02-06T17:16:27.991686Z","shell.execute_reply.started":"2022-02-06T17:16:27.980301Z","shell.execute_reply":"2022-02-06T17:16:27.990905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorize the text with TF-IDF","metadata":{}},{"cell_type":"code","source":"vec = TfidfVectorizer(lowercase=True, stop_words=['english'], analyzer='char_wb', ngram_range = (3,5))\nX = vec.fit_transform(hspeech['text_clean'])\nx_less_toxic =  vec.transform(validation['less_toxic'])\nx_more_toxic = vec.transform(validation['more_toxic'])\nx_test = vec.transform(submission['text'])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:02:06.127633Z","iopub.execute_input":"2022-02-06T17:02:06.127966Z","iopub.status.idle":"2022-02-06T17:03:08.764297Z","shell.execute_reply.started":"2022-02-06T17:02:06.127921Z","shell.execute_reply":"2022-02-06T17:03:08.763036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build 3 Ridge Models and build an Ensemble","metadata":{}},{"cell_type":"markdown","source":"Build three  ridges models with varying regularization parameters","metadata":{}},{"cell_type":"code","source":"model_1 = Ridge(alpha=0.5)\nmodel_1.fit(X, hspeech['hate_speech_score'])\nprint(f'Model 1 validation accuracy score:  {(model_1.predict(x_less_toxic) < model_1.predict(x_more_toxic)).mean()}')\n\nmodel_2 = Ridge(alpha=1)\nmodel_2.fit(X, hspeech['hate_speech_score'])\nprint(f'Model 2 validation accuracy score:  {(model_2.predict(x_less_toxic) < model_2.predict(x_more_toxic)).mean()}')\n\n\nmodel_3 = Ridge(alpha=2)\nmodel_3.fit(X, hspeech['hate_speech_score'])\nprint(f'Model 3 validation accuracy score: {(model_3.predict(x_less_toxic) < model_3.predict(x_more_toxic)).mean()}')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:25:26.905427Z","iopub.status.idle":"2022-02-06T16:25:26.906129Z","shell.execute_reply.started":"2022-02-06T16:25:26.905904Z","shell.execute_reply":"2022-02-06T16:25:26.905934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Average the model scores and submit","metadata":{}},{"cell_type":"code","source":"p1 = model_1.predict(x_test)\np2 = model_2.predict(x_test)\np3 = model_3.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:25:26.907552Z","iopub.status.idle":"2022-02-06T16:25:26.90789Z","shell.execute_reply.started":"2022-02-06T16:25:26.907722Z","shell.execute_reply":"2022-02-06T16:25:26.90774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['score'] = (p1 + p2 + p3) / 3","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:25:26.909541Z","iopub.status.idle":"2022-02-06T16:25:26.909896Z","shell.execute_reply.started":"2022-02-06T16:25:26.909725Z","shell.execute_reply":"2022-02-06T16:25:26.909744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy\nfrom scipy.stats import rankdata\n\ntest = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ntest['score'] = rankdata(submission['score'].values, method='ordinal')\ntest[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:25:26.911284Z","iopub.status.idle":"2022-02-06T16:25:26.911648Z","shell.execute_reply.started":"2022-02-06T16:25:26.911444Z","shell.execute_reply":"2022-02-06T16:25:26.911478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.sort_values('score', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:25:26.912909Z","iopub.status.idle":"2022-02-06T16:25:26.913264Z","shell.execute_reply.started":"2022-02-06T16:25:26.913093Z","shell.execute_reply":"2022-02-06T16:25:26.913111Z"},"trusted":true},"execution_count":null,"outputs":[]}]}