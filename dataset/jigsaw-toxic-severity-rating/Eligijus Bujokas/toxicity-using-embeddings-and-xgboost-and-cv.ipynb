{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Package imports ","metadata":{}},{"cell_type":"code","source":"# Data wrangling \nimport pandas as pd \n\n# Fastetext embeddings\nimport fasttext\n\n# Importing regex \nimport re \n\n# Array math \nimport numpy as np\n\n# OS traversal\nimport os \n\n# Zip files\nimport zipfile\n\n# Plotting \nimport matplotlib.pyplot as plt\n\n# Itteration tracking\nfrom tqdm import tqdm\n\n# Machine learning \nimport xgboost as xgb\n\n# Time tracking \nimport time\n\n# Data scalers\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Hp parameter search \nfrom sklearn.model_selection import ParameterGrid","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:20:31.306323Z","iopub.execute_input":"2022-01-30T09:20:31.307038Z","iopub.status.idle":"2022-01-30T09:20:32.180325Z","shell.execute_reply.started":"2022-01-30T09:20:31.306937Z","shell.execute_reply":"2022-01-30T09:20:32.179637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data reading ","metadata":{}},{"cell_type":"markdown","source":"In order to create a good classifier to evaluate the toxicity of a comment, we need to gather as much prior labeled observations as possible. Luckaly, there are numerous data sources that provide a label for toxicity in one way or another. In this notebook, I shall use data from: \n\n* https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data \n* This competition's data\n\nThe goal is to create a classifier $f$:\n\n$$f: \\mathbb{X} \\rightarrow \\mathbb{Y}$$\n\nWhere \n\n$\\mathbb{X}$ - comment (text)\n\n$\\mathbb{Y}$ - toxicity score ($\\in$ $\\mathbb{R}$)","metadata":{}},{"cell_type":"code","source":"# Defining the input directory\n_input_dir = '/kaggle/input/jigsaw-toxic-severity-rating/'\n\n# Defining the path to the input file \n_input_file = os.path.join(_input_dir, 'comments_to_score.csv')\n_val_file = os.path.join(_input_dir, 'validation_data.csv')\n\n# Reading the data file\nd = pd.read_csv(_input_file)\n\n# Reading the validation data \ndval = pd.read_csv(_val_file)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:20:32.181943Z","iopub.execute_input":"2022-01-30T09:20:32.182643Z","iopub.status.idle":"2022-01-30T09:20:32.732373Z","shell.execute_reply.started":"2022-01-30T09:20:32.182598Z","shell.execute_reply":"2022-01-30T09:20:32.731661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of comments:\\n{d.shape[0]}\\nColumns:\\n{d.columns.tolist()}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:20:32.735776Z","iopub.execute_input":"2022-01-30T09:20:32.735973Z","iopub.status.idle":"2022-01-30T09:20:32.744175Z","shell.execute_reply.started":"2022-01-30T09:20:32.735947Z","shell.execute_reply":"2022-01-30T09:20:32.743465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eyeballing some data \nprint(d.sample(1)['text'].tolist())","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:20:32.746735Z","iopub.execute_input":"2022-01-30T09:20:32.747893Z","iopub.status.idle":"2022-01-30T09:20:32.760925Z","shell.execute_reply.started":"2022-01-30T09:20:32.747854Z","shell.execute_reply":"2022-01-30T09:20:32.760084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inspecting the validation data set ","metadata":{}},{"cell_type":"code","source":"print(f\"Number of observations in validation set:\\n{dval.shape[0]}\")\nprint(f\"Sample of data:\\n{dval.sample(10)}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:20:32.763994Z","iopub.execute_input":"2022-01-30T09:20:32.764299Z","iopub.status.idle":"2022-01-30T09:20:32.775377Z","shell.execute_reply.started":"2022-01-30T09:20:32.764264Z","shell.execute_reply":"2022-01-30T09:20:32.774493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Additional data \n\n### Jigsaw toxicity classification challenge","metadata":{}},{"cell_type":"code","source":"# Path to data \n_aux_path = '/kaggle/input/jigsaw-toxic-comment-classification-challenge/'\n_aux_file_path = os.path.join(_aux_path, 'train.csv')\n\n# Reading the data \n_d_jigsaw = pd.read_csv(_aux_file_path)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:20:32.776864Z","iopub.execute_input":"2022-01-30T09:20:32.777145Z","iopub.status.idle":"2022-01-30T09:20:34.420475Z","shell.execute_reply.started":"2022-01-30T09:20:32.777107Z","shell.execute_reply":"2022-01-30T09:20:34.419712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape of data:\\n{_d_jigsaw.shape}\\nColumns:\\n{_d_jigsaw.columns.tolist()}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:20:34.422074Z","iopub.execute_input":"2022-01-30T09:20:34.422639Z","iopub.status.idle":"2022-01-30T09:20:34.428552Z","shell.execute_reply.started":"2022-01-30T09:20:34.422599Z","shell.execute_reply":"2022-01-30T09:20:34.427655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(_d_jigsaw.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:20:34.429913Z","iopub.execute_input":"2022-01-30T09:20:34.430797Z","iopub.status.idle":"2022-01-30T09:20:34.442247Z","shell.execute_reply.started":"2022-01-30T09:20:34.430757Z","shell.execute_reply":"2022-01-30T09:20:34.441597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the fasttext embeddings \n\nThe embeddings can be dowloaded using kaggle's **add data** feature. The link to embeddings is: https://www.kaggle.com/kambarakun/fasttext-pretrained-word-vectors-english","metadata":{}},{"cell_type":"code","source":"# Reading the embeddings \nembeddings = fasttext.load_model('/kaggle/input/fasttext-pretrained-word-vectors-english/wiki.en.bin')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:20:34.443132Z","iopub.execute_input":"2022-01-30T09:20:34.443308Z","iopub.status.idle":"2022-01-30T09:22:10.565014Z","shell.execute_reply.started":"2022-01-30T09:20:34.443285Z","shell.execute_reply":"2022-01-30T09:22:10.56392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text cleaning\n\nThe text cleaning function will be used throughout all the data sources for consistency. ","metadata":{}},{"cell_type":"code","source":"def clean_text(text: str) -> str:\n    \"\"\"\n    Function to clean the text for embedding creation\n    \"\"\"\n    # Lowering \n    text = text.lower()\n    \n    # Leaving only the english letters and numerics\n    text = text.replace('\\n', ' ')\n\n    # Removing the punctuations\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n\n    # Removing the special characters\n    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n\n    # Removing more than 1 whitespaces\n    text = re.sub('\\s+', ' ', text)\n\n    return text\n\n# Applying the fucntion to the texts\nd['clean_text'] = [clean_text(x) for x in d['text']]\ndval['clean_less_toxic'] = [clean_text(x) for x in dval['less_toxic']]\ndval['clean_more_toxic'] = [clean_text(x) for x in dval['more_toxic']]","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:22:10.566974Z","iopub.execute_input":"2022-01-30T09:22:10.567583Z","iopub.status.idle":"2022-01-30T09:22:16.823637Z","shell.execute_reply.started":"2022-01-30T09:22:10.567545Z","shell.execute_reply":"2022-01-30T09:22:16.822909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model for jigsaw classification challenge \n\n## Creating the Y variable \n\nIn this competition, it is important to measure the scale of toxicity. It is importnat to distinguish between low toxicity and high toxicity. \n\nTo encompass that logic, from the collumns \n* toxic\n* severe_toxic\n* obscene\n* threat\n* insult\n* identity_hate\n\nI will create a column $Y$ that is the sum of all of the above columns. The higher the sum - the higher the toxicity. ","metadata":{}},{"cell_type":"code","source":"toxicity_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n# Ensuring correct types \n_d_jigsaw['Y'] = _d_jigsaw.apply(lambda x: x[toxicity_columns].sum(), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:22:16.82502Z","iopub.execute_input":"2022-01-30T09:22:16.825282Z","iopub.status.idle":"2022-01-30T09:23:36.704754Z","shell.execute_reply.started":"2022-01-30T09:22:16.825247Z","shell.execute_reply":"2022-01-30T09:23:36.703973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspecting the distribution of the Y \nagg = _d_jigsaw.groupby(\"Y\", as_index=False).size()\nagg['share_in_data'] = agg['size'] / agg['size'].sum()\n\nplt.bar(x=agg['Y'], height=agg['share_in_data'])\nplt.title(f\"Total data points: {agg['size'].sum()}\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:23:36.706116Z","iopub.execute_input":"2022-01-30T09:23:36.706366Z","iopub.status.idle":"2022-01-30T09:23:36.924543Z","shell.execute_reply.started":"2022-01-30T09:23:36.706334Z","shell.execute_reply":"2022-01-30T09:23:36.923866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = _d_jigsaw['Y'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:23:36.92581Z","iopub.execute_input":"2022-01-30T09:23:36.926209Z","iopub.status.idle":"2022-01-30T09:23:36.933695Z","shell.execute_reply.started":"2022-01-30T09:23:36.926169Z","shell.execute_reply":"2022-01-30T09:23:36.932798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the X matrix for training\n\nThe term embedding means to convert text into a number vector. In our case, the vector will have 300 coordinates: \n\n$$f(text) \\rightarrow \\mathbb{R}^{300} = [x_{1}, x_{2}, ..., x_{300}]$$\n\nThe unit of text is a comment. Thus, no matter the number of words in the comment, the strategy is to somehow represent the text as a vector of 300 coordinates.\n\nThe basic strategy is the following:\n\n* Get the embeddings of each of the words in the description.\n* Average each coordinate of the gotten word embeddings. \n\nFor example, if a comment has $k$ words, then the initial all word embedding matrix is $\\mathbb{M}_{kx300}$. To get the final embedding of the comment, we will average column wise the matrix. Thus, each coordinate of the final embedding $y$ is: \n\n$$y = [\\dfrac{1}{k} \\sum_{i=1}^{k}M[i, 1], \\dfrac{1}{k} \\sum_{i=1}^{k}M[i, 2],..., \\dfrac{1}{k} \\sum_{i=1}^{k}M[i, k]]$$","metadata":{}},{"cell_type":"code","source":"# Cleaning the descriptions \n_d_jigsaw['comment_text_clean'] = [clean_text(x) for x in tqdm(_d_jigsaw['comment_text'], desc='Cleaning comments', total=len(_d_jigsaw))]\n\n# Getting the mean embedding for each comment (X matrix for models) \nX = [embeddings.get_sentence_vector(text) for text in tqdm(_d_jigsaw['comment_text_clean'], desc='Creating embeddings', total=len(_d_jigsaw))]\n\n# Converting to an array that is digestable for ML frameworks\nX = np.array(X).reshape(-1, 300)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:23:36.936826Z","iopub.execute_input":"2022-01-30T09:23:36.937156Z","iopub.status.idle":"2022-01-30T09:24:31.345219Z","shell.execute_reply.started":"2022-01-30T09:23:36.937119Z","shell.execute_reply":"2022-01-30T09:24:31.344472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grid search for best hyper parameters\n\nWe will treat this as a regression problem. \n\nThe ML algorithm of choice is **xgboost.** \n\nWe will test out the model based on the provided validation set. \n\nThe strategy to evaluate the model is the following: \n\n* Get the score for less toxic \n* Get the score for more toxic \n* Calculate the number of correct predictions","metadata":{}},{"cell_type":"code","source":"# Creating the embeddings for less_toxic comments \nless_toxic_embedding = [embeddings.get_sentence_vector(text) for text in dval['clean_less_toxic']]\nmore_toxic_embedding = [embeddings.get_sentence_vector(text) for text in dval['clean_more_toxic']]\n\n# Reshaping for model \nless_toxic_embedding = np.array(less_toxic_embedding).reshape(-1, 300)\nmore_toxic_embedding = np.array(more_toxic_embedding).reshape(-1, 300)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:24:31.347728Z","iopub.execute_input":"2022-01-30T09:24:31.348Z","iopub.status.idle":"2022-01-30T09:24:44.174639Z","shell.execute_reply.started":"2022-01-30T09:24:31.347962Z","shell.execute_reply":"2022-01-30T09:24:44.173899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the embeddings for all the sentences \nsubmission_matrix = [embeddings.get_sentence_vector(text) for text in d['clean_text']]\n\n# Reshaping \nsubmission_matrix = np.array(submission_matrix).reshape(-1, 300)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:24:44.176145Z","iopub.execute_input":"2022-01-30T09:24:44.176389Z","iopub.status.idle":"2022-01-30T09:24:46.049876Z","shell.execute_reply.started":"2022-01-30T09:24:44.176355Z","shell.execute_reply":"2022-01-30T09:24:46.048951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a list of hyperparameters \nhp_dict = {\n    \"objective\": ['reg:squarederror'],\n    \"tree_method\": ['gpu_hist'],\n    \"max_depth\": [4, 5, 6, 7, 8],\n    'n_estimators': [200, 400, 600, 800]\n}\n\n# Creating the hp grid \nhp_grid = ParameterGrid(hp_dict)\n\n# Max score tracker  \nmax_score = 0\n\n# Best hp dictionary \nbest_hp = {}\n\n# Initiating the \"best\" score list \nscore = []\n\nfor hp in hp_grid: \n\n    # Initiating the empty model\n    reg = xgb.XGBRegressor(**hp)\n\n    # Fitting on data \n    reg.fit(X, Y)\n    \n    # Predicting \n    less_toxic_hat = reg.predict(less_toxic_embedding)\n    more_toxic_hat = reg.predict(more_toxic_embedding)\n\n    # Calculating how many entries are larger in more toxic set\n    # than in less toxic set \n    current_score = np.sum([less_toxic_hat[i] < more_toxic_hat[i] for i in range(len(less_toxic_hat))])\n    current_score = current_score / len(less_toxic_hat)\n\n    # Checking if this is the highest auc \n    if current_score > max_score:\n        max_score = current_score \n        best_hp = hp \n        \n        print(f\"New best hp parameters found:\\n{best_hp}\\nBest score: {round(max_score, 3)}\")\n        \n        # Applying the best found model\n        score = reg.predict(submission_matrix)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:24:46.051367Z","iopub.execute_input":"2022-01-30T09:24:46.051625Z","iopub.status.idle":"2022-01-30T09:28:46.468576Z","shell.execute_reply.started":"2022-01-30T09:24:46.05159Z","shell.execute_reply":"2022-01-30T09:28:46.467774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the final submission ","metadata":{}},{"cell_type":"code","source":"# Saving to the dataframe \nd['score'] = score\n\n# Sorting by distance \nd.sort_values(by='score', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:28:46.469928Z","iopub.execute_input":"2022-01-30T09:28:46.470189Z","iopub.status.idle":"2022-01-30T09:28:46.480131Z","shell.execute_reply.started":"2022-01-30T09:28:46.470156Z","shell.execute_reply":"2022-01-30T09:28:46.479315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final submission file ","metadata":{}},{"cell_type":"code","source":"# Most \"light\" comments\nd.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:28:46.481682Z","iopub.execute_input":"2022-01-30T09:28:46.482156Z","iopub.status.idle":"2022-01-30T09:28:46.497996Z","shell.execute_reply.started":"2022-01-30T09:28:46.482117Z","shell.execute_reply":"2022-01-30T09:28:46.497297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Most \"severe\" comments\nd.tail(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:28:46.499163Z","iopub.execute_input":"2022-01-30T09:28:46.499359Z","iopub.status.idle":"2022-01-30T09:28:46.514055Z","shell.execute_reply.started":"2022-01-30T09:28:46.499335Z","shell.execute_reply":"2022-01-30T09:28:46.513385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:28:46.515081Z","iopub.execute_input":"2022-01-30T09:28:46.516561Z","iopub.status.idle":"2022-01-30T09:28:46.548072Z","shell.execute_reply.started":"2022-01-30T09:28:46.51652Z","shell.execute_reply":"2022-01-30T09:28:46.54746Z"},"trusted":true},"execution_count":null,"outputs":[]}]}