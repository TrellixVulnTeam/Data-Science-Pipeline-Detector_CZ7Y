{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-06T09:36:11.427853Z","iopub.execute_input":"2022-02-06T09:36:11.42809Z","iopub.status.idle":"2022-02-06T09:36:11.444915Z","shell.execute_reply.started":"2022-02-06T09:36:11.428064Z","shell.execute_reply":"2022-02-06T09:36:11.444236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\nfrom datasets import Dataset\nfrom transformers import PreTrainedTokenizerFast\nfrom tokenizers import (\n    models,\n    normalizers,\n    pre_tokenizers,\n    trainers,\n    Tokenizer,\n)\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:36:33.467103Z","iopub.execute_input":"2022-02-06T09:36:33.467315Z","iopub.status.idle":"2022-02-06T09:36:36.368136Z","shell.execute_reply.started":"2022-02-06T09:36:33.467291Z","shell.execute_reply":"2022-02-06T09:36:36.36746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_SPLITS = 5\nEARLY_STOPPING_ROUNDS = 200\nSEED = 42\n\nCLASSIFY_DATA = \"/kaggle/input/comment-classify/train.csv\"\nRUDDIT_DATA = \"/kaggle/input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\"\nTEST_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\n\nRANK_MODEL_PATH = '/kaggle/input/classic-toxic-ensemble/save_small_model'\ndf_test = pd.read_csv(TEST_DATA_PATH)\n\nparams = {'max_iter': 1096, 'tol': 0.013287415929166577, 'alpha': 0.4270990730406918, 'solver': 'auto'}","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:36:36.36936Z","iopub.execute_input":"2022-02-06T09:36:36.369576Z","iopub.status.idle":"2022-02-06T09:36:36.516901Z","shell.execute_reply.started":"2022-02-06T09:36:36.369554Z","shell.execute_reply":"2022-02-06T09:36:36.516393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dummy_fun(doc):\n    return doc\n\nvectorizer = TfidfVectorizer(\n    analyzer = 'word',\n    tokenizer = dummy_fun,\n    preprocessor = dummy_fun,\n    token_pattern = None)\n\n\ndef cross_validate_sub(\n    model,\n    train_data,\n    test_data\n):\n    train_oof = np.zeros(len(train_data))\n    predictions_test = np.zeros((len(test_data), N_SPLITS))\n\n    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n    for fold, (train_idx, valid_idx) in tqdm(enumerate(kf.split(train_data))):\n        train_fold, valid_fold = train_data.iloc[train_idx], train_data.iloc[valid_idx]\n        \n        y_train = train_fold['Y']\n        y_valid = valid_fold['Y']\n\n        X_train = train_fold['X']\n        X_valid = valid_fold['X']\n\n        X_train_transformed = vectorizer.fit_transform(X_train)\n\n        model.fit(\n            X_train_transformed, \n            y_train\n        )\n\n        X_valid_transformed = vectorizer.transform(X_valid)\n\n        temp_oof = model.predict(X_valid_transformed)\n        train_oof[valid_idx] = temp_oof\n\n        print(f'Fold {fold} RMSE: ', mean_squared_error(y_valid, temp_oof))\n\n        test_data_transform = vectorizer.transform(test_data)\n        predictions_test[:, fold] = model.predict(test_data_transform)\n\n    print(f'OOF AUC: ', mean_squared_error(train_data['Y'], train_oof))\n    \n    return train_oof, predictions_test, model\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:37:30.848944Z","iopub.execute_input":"2022-02-06T09:37:30.849166Z","iopub.status.idle":"2022-02-06T09:37:30.857385Z","shell.execute_reply.started":"2022-02-06T09:37:30.849144Z","shell.execute_reply":"2022-02-06T09:37:30.856693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ruddit_data():\n    df_ruddit = pd.read_csv(RUDDIT_DATA)\n    df_ruddit = df_ruddit[['txt', 'offensiveness_score']].rename(columns = {'txt': 'text', 'offensiveness_score': 'y'})\n    df_ruddit['y'] = (df_ruddit['y'] - df_ruddit.y.min()) / (df_ruddit.y.max() - df_ruddit.y.min())\n    df_ruddit = df_ruddit[df_ruddit['text']!='[deleted]'] \n    df_ruddit = df_ruddit.drop_duplicates()\n    df_ruddit = df_ruddit.dropna()    \n\n    raw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n    raw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n    raw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n    special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n    trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n\n    dataset = Dataset.from_pandas(df_ruddit[['text']])\n\n    def get_training_corpus():\n        for i in range(0, len(dataset), 1000):\n            yield dataset[i : i + 1000][\"text\"]\n\n    raw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n\n    tokenizer = PreTrainedTokenizerFast(\n        tokenizer_object=raw_tokenizer,\n        unk_token=\"[UNK]\",\n        pad_token=\"[PAD]\",\n        cls_token=\"[CLS]\",\n        sep_token=\"[SEP]\",\n        mask_token=\"[MASK]\",\n    )\n\n    labels = df_ruddit['y']\n    comments = df_ruddit['text']\n\n    return comments, labels, tokenizer\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:37:31.837119Z","iopub.execute_input":"2022-02-06T09:37:31.837901Z","iopub.status.idle":"2022-02-06T09:37:31.846643Z","shell.execute_reply.started":"2022-02-06T09:37:31.837842Z","shell.execute_reply":"2022-02-06T09:37:31.846239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_classify():\n    df_train = pd.read_csv(CLASSIFY_DATA)\n    cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n                'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n    for category in cat_mtpl:\n        df_train[category] = df_train[category] * cat_mtpl[category]\n    df_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].mean(axis=1)\n    df_train['y'] = df_train['score']\n    min_len = (df_train['y'] > 0).sum()  # len of toxic comments\n    df_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=SEED)  # take non toxic comments\n    df_train_final = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\n\n    raw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n    raw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n    raw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n    special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n    trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n\n    dataset = Dataset.from_pandas(df_train_final[['comment_text']])\n\n    def get_training_corpus():\n        for i in range(0, len(dataset), 1000):\n            yield dataset[i : i + 1000][\"comment_text\"]\n\n    raw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n\n    tokenizer = PreTrainedTokenizerFast(\n        tokenizer_object=raw_tokenizer,\n        unk_token=\"[UNK]\",\n        pad_token=\"[PAD]\",\n        cls_token=\"[CLS]\",\n        sep_token=\"[SEP]\",\n        mask_token=\"[MASK]\",\n    )\n\n    labels = df_train_final['y']\n    comments = df_train_final['comment_text']\n\n    return comments, labels, tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:37:32.814144Z","iopub.execute_input":"2022-02-06T09:37:32.814604Z","iopub.status.idle":"2022-02-06T09:37:32.823967Z","shell.execute_reply.started":"2022-02-06T09:37:32.814581Z","shell.execute_reply":"2022-02-06T09:37:32.823229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments, labels, tokenizer = get_ruddit_data()\ntokenized_comments = tokenizer(comments.to_list())['input_ids']\n\ntrain_data = pd.DataFrame()\n\ntrain_data['X'] = tokenized_comments\ntrain_data['Y'] = labels.tolist()\n\n#Create Model\nmodel = Ridge(**params)\n\n_, output_ruddit, _ = cross_validate_sub(\n        model,\n        train_data,\n        tokenizer(df_test['text'].to_list())['input_ids']\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:37:39.920656Z","iopub.execute_input":"2022-02-06T09:37:39.92089Z","iopub.status.idle":"2022-02-06T09:37:44.504936Z","shell.execute_reply.started":"2022-02-06T09:37:39.920867Z","shell.execute_reply":"2022-02-06T09:37:44.504445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments, labels, tokenizer = get_data_classify()\ntokenized_comments = tokenizer(comments.to_list())['input_ids']\n\ntrain_data = pd.DataFrame()\n\ntrain_data['X'] = tokenized_comments\ntrain_data['Y'] = labels.tolist()\n\n#Create Model\nmodel = Ridge(**params)\n\n_, output_classify, _ = cross_validate_sub(\n        model,\n        train_data,\n        tokenizer(df_test['text'].to_list())['input_ids']\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:37:48.17878Z","iopub.execute_input":"2022-02-06T09:37:48.179231Z","iopub.status.idle":"2022-02-06T09:38:08.555046Z","shell.execute_reply.started":"2022-02-06T09:37:48.179192Z","shell.execute_reply":"2022-02-06T09:38:08.554158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scoring_data = np.concatenate((np.array(output_ruddit), np.array(output_classify)), axis = 1)\nranking_input = torch.tensor(scoring_data, dtype=torch.float)\n\nclass BinaryClassification(nn.Module):\n    def __init__(self, hidden_layers):\n        super(BinaryClassification, self).__init__()\n        self.layer_1 = nn.Linear(hidden_layers, 1)\n        self.relu = nn.ReLU()\n    def forward(self, inputs):\n        x = self.relu(inputs)\n        x = self.layer_1(x)\n        return x\n\nmodel = BinaryClassification(10)\nmodel.load_state_dict(torch.load(RANK_MODEL_PATH))\nmodel.eval()\n\nfinal_output = model(torch.tensor(ranking_input, dtype=torch.float)).view(-1).detach().numpy().tolist()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:38:08.557369Z","iopub.execute_input":"2022-02-06T09:38:08.557926Z","iopub.status.idle":"2022-02-06T09:38:08.62985Z","shell.execute_reply.started":"2022-02-06T09:38:08.557902Z","shell.execute_reply":"2022-02-06T09:38:08.628429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\n\nsubmission['comment_id'] = df_test['comment_id']\nsubmission['score'] = final_output\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:39:13.347756Z","iopub.execute_input":"2022-02-06T09:39:13.348365Z","iopub.status.idle":"2022-02-06T09:39:13.374958Z","shell.execute_reply.started":"2022-02-06T09:39:13.348339Z","shell.execute_reply":"2022-02-06T09:39:13.37416Z"},"trusted":true},"execution_count":null,"outputs":[]}]}