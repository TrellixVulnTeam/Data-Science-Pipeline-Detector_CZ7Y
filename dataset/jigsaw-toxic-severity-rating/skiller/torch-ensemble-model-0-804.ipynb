{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T13:05:32.428437Z","iopub.execute_input":"2022-02-07T13:05:32.428679Z","iopub.status.idle":"2022-02-07T13:05:32.48146Z","shell.execute_reply.started":"2022-02-07T13:05:32.428606Z","shell.execute_reply":"2022-02-07T13:05:32.480802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport re\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport torch\nimport torch.nn as nn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATH = \"/kaggle/input/toxic-model-files/model_files/model_files_fold/model\"\nTOKENIZER_PATH = \"/kaggle/input/toxic-model-files/model_files/model_files_fold/tokenizer\"\nMAX_LEN = 512\nBATCH_SIZE = 16\nDEVICE = 'cuda'\nCLASSIFY_MODEL_PATH = '/kaggle/input/toxic-model-files/model_files/model_files_fold/best_model_classify'\nRUDDIT_MODEL_PATH = '/kaggle/input/toxic-model-files/model_files/model_files_fold/best_model_ruddit'\nRANK_MODEL_PATH = '/kaggle/input/toxic-model-files/final_model_output_2'\nTEST_FILE_PATH = '/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv'\nFOLDS = 5\n\ndevice = torch.device(DEVICE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Reader","metadata":{}},{"cell_type":"code","source":"class BatchEnabler:\n    def __init__(self, data):\n        self.tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n        self.text_encodings = self.tokenizer(data['text'].to_list(), truncation=True, padding='max_length', max_length=MAX_LEN)\n        self.comment_id_list = data['comment_id'].to_list()\n    def __getitem__(self, idx):\n        return {\n            'input_ids' : torch.tensor(self.text_encodings.input_ids[idx], dtype=torch.long),\n            'attention_mask' : torch.tensor(self.text_encodings.attention_mask[idx], dtype=torch.long),\n            'comment_id' : self.comment_id_list[idx],\n        }\n    def __len__(self):\n        return len(self.comment_id_list)\n\ndf_file = pd.read_csv(TEST_FILE_PATH)\nbatch_enabled = BatchEnabler(df_file)\ndata_loader = DataLoader(batch_enabled, \n                        batch_size = BATCH_SIZE)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define The Model Architecture","metadata":{}},{"cell_type":"code","source":"class ToxicScoreModel(nn.Module):\n    def __init__(self):\n        super(ToxicScoreModel, self).__init__()\n        self.bert_layer = AutoModel.from_pretrained(MODEL_PATH)\n        self.bert_drop_1 = nn.Dropout(0.2)\n        self.bert_drop_2 = nn.Dropout(0.2)\n        self.l0_mean = nn.Linear(768, 128)\n        self.l0_max = nn.Linear(768, 128)\n        self.l1 = nn.Linear(256, 1)\n    def forward(self, ids, mask):\n        bert_out_text = self.bert_layer(\n            input_ids = ids,\n            attention_mask = mask\n            )\n        output = self.bert_drop_1(bert_out_text['last_hidden_state'].mean(axis=1))\n        output_mean = self.l0_mean(output)\n        output = self.bert_drop_1(bert_out_text['last_hidden_state'].max(axis=1)[0])\n        output_max = self.l0_max(output)\n        output = self.bert_drop_2(torch.cat((output_max, output_mean), 1))\n        output = self.l1(output)\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RUDDIT MODEL SCORE INFERENCE","metadata":{}},{"cell_type":"code","source":"final_toxic_ruddit = np.zeros((len(df_file), FOLDS))\n\nfor fold in range(FOLDS):\n    model = ToxicScoreModel()\n    model.load_state_dict(torch.load(f\"{RUDDIT_MODEL_PATH}_{fold}\"))\n    model.to(device)\n    model.eval()\n\n    toxic_ruddit = []\n    for _, dataset in tqdm(enumerate(data_loader), total=len(data_loader)): \n        input_ids = dataset['input_ids'].to(device, dtype = torch.long)\n        attention_mask = dataset['attention_mask'].to(device, dtype = torch.long)\n        with torch.no_grad():\n            model_output = model(\n                input_ids, \n                attention_mask, \n            )\n        toxic_ruddit.extend(model_output.view(-1).detach().cpu().numpy().tolist())\n\n    del model\n    final_toxic_ruddit[:, fold] = np.array(toxic_ruddit)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CLASSIFY MODEL SCORE INFERENCE","metadata":{}},{"cell_type":"code","source":"final_toxic_classify = np.zeros((len(df_file), FOLDS))\n\nfor fold in range(FOLDS):\n    model = ToxicScoreModel()\n    model.load_state_dict(torch.load(f\"{CLASSIFY_MODEL_PATH}_{fold}\"))\n    model.to(device)\n    model.eval()\n\n    toxic_classify = []\n    for _, dataset in tqdm(enumerate(data_loader), total=len(data_loader)): \n        input_ids = dataset['input_ids'].to(device, dtype = torch.long)\n        attention_mask = dataset['attention_mask'].to(device, dtype = torch.long)\n        with torch.no_grad():\n            model_output = model(\n                input_ids, \n                attention_mask, \n            )\n        toxic_classify.extend(model_output.view(-1).detach().cpu().numpy().tolist())\n\n    del model\n    final_toxic_classify[:, fold] = np.array(toxic_classify)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FINAL ENSEMBLE LAYER","metadata":{}},{"cell_type":"code","source":"final_toxic_classify = final_toxic_classify.mean(axis=1)\nfinal_toxic_ruddit = final_toxic_ruddit.mean(axis=1)\n\nranking_input = np.concatenate((final_toxic_classify.reshape(-1,1), final_toxic_ruddit.reshape(-1,1)), axis = 1)\n\nclass BinaryClassification(nn.Module):\n    def __init__(self, hidden_input):\n        super(BinaryClassification, self).__init__()\n        self.layer_1 = nn.Linear(hidden_input, 1)\n        self.relu = nn.ReLU()\n    def forward(self, inputs):\n        x = self.relu(inputs)\n        x = self.layer_1(x)\n        return x\n\nmodel = BinaryClassification(2)\nmodel.load_state_dict(torch.load(RANK_MODEL_PATH))\nmodel.eval()\n\nfinal_output = model(torch.tensor(ranking_input, dtype=torch.float)).view(-1).detach().numpy().tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission File","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame()\n\nsubmission['comment_id'] = df_file['comment_id']\nsubmission['score'] = final_output\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}