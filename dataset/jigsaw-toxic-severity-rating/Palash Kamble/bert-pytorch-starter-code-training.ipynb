{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, BertModel, logging\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_lightning.metrics.functional import accuracy\nfrom collections import defaultdict\n\nlogging.set_verbosity_error()","metadata":{"_uuid":"c7614e06-7c95-4626-8dc4-f0812fa1ab17","_cell_guid":"6fde8561-100d-487a-b116-c1974199ffb2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-30T15:15:54.163453Z","iopub.execute_input":"2022-01-30T15:15:54.163813Z","iopub.status.idle":"2022-01-30T15:15:54.171288Z","shell.execute_reply.started":"2022-01-30T15:15:54.163777Z","shell.execute_reply":"2022-01-30T15:15:54.170616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset path\n\nIn this notebook, I have added dataset from \nhttps://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data","metadata":{}},{"cell_type":"code","source":"train_csv_path = '../input/d/julian3833/jigsaw-toxic-comment-classification-challenge/train.csv'\nsample_sub_path = '../input/jigsaw-toxic-severity-rating/sample_submission.csv'\ntest_csv_path = '../input/jigsaw-toxic-severity-rating/comments_to_score.csv'","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:15:54.174735Z","iopub.execute_input":"2022-01-30T15:15:54.174931Z","iopub.status.idle":"2022-01-30T15:15:54.179768Z","shell.execute_reply.started":"2022-01-30T15:15:54.174907Z","shell.execute_reply":"2022-01-30T15:15:54.179054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_csv_path)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:15:54.18097Z","iopub.execute_input":"2022-01-30T15:15:54.181351Z","iopub.status.idle":"2022-01-30T15:15:56.078139Z","shell.execute_reply.started":"2022-01-30T15:15:54.181249Z","shell.execute_reply":"2022-01-30T15:15:56.07745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(test_csv_path)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:15:56.64139Z","iopub.execute_input":"2022-01-30T15:15:56.642192Z","iopub.status.idle":"2022-01-30T15:15:56.745084Z","shell.execute_reply.started":"2022-01-30T15:15:56.642147Z","shell.execute_reply":"2022-01-30T15:15:56.744423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub_df = pd.read_csv(sample_sub_path)\nsample_sub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:15:56.837848Z","iopub.execute_input":"2022-01-30T15:15:56.83816Z","iopub.status.idle":"2022-01-30T15:15:56.855151Z","shell.execute_reply.started":"2022-01-30T15:15:56.83813Z","shell.execute_reply":"2022-01-30T15:15:56.854496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking NaN","metadata":{}},{"cell_type":"code","source":"train_df.comment_text.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:15:57.219839Z","iopub.execute_input":"2022-01-30T15:15:57.220386Z","iopub.status.idle":"2022-01-30T15:15:57.2535Z","shell.execute_reply.started":"2022-01-30T15:15:57.22035Z","shell.execute_reply":"2022-01-30T15:15:57.25268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.text.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:15:57.439736Z","iopub.execute_input":"2022-01-30T15:15:57.440118Z","iopub.status.idle":"2022-01-30T15:15:57.447235Z","shell.execute_reply.started":"2022-01-30T15:15:57.440088Z","shell.execute_reply":"2022-01-30T15:15:57.446358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking for imbalance in the training dataset","metadata":{}},{"cell_type":"code","source":"def add_all(row):\n    toxicity = row[2:].sum()\n    if toxicity > 0:\n        return 1\n    else:\n        return 0\n\ntrain_df['toxic_nontoxic'] = train_df.apply(add_all, axis='columns')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:15:57.824709Z","iopub.execute_input":"2022-01-30T15:15:57.825231Z","iopub.status.idle":"2022-01-30T15:16:20.261041Z","shell.execute_reply.started":"2022-01-30T15:15:57.825195Z","shell.execute_reply":"2022-01-30T15:16:20.260342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(train_df.toxic_nontoxic)\nplt.xlabel('Toxic vs Non toxic')\nax.set_xticklabels(['Non Toxic', 'Toxic'])","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:20.262785Z","iopub.execute_input":"2022-01-30T15:16:20.263012Z","iopub.status.idle":"2022-01-30T15:16:20.466099Z","shell.execute_reply.started":"2022-01-30T15:16:20.262979Z","shell.execute_reply":"2022-01-30T15:16:20.465437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.toxic_nontoxic.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:20.467407Z","iopub.execute_input":"2022-01-30T15:16:20.467788Z","iopub.status.idle":"2022-01-30T15:16:20.476439Z","shell.execute_reply.started":"2022-01-30T15:16:20.467753Z","shell.execute_reply":"2022-01-30T15:16:20.475751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the dataset is highly imbalanced. Around 16k comments are toxic. We will sample 16k non toxic comments to create new balanced training dataset.\n","metadata":{}},{"cell_type":"code","source":"toxic_train_df = train_df[train_df.toxic_nontoxic == 1]\nnontoxic_train_df = train_df[train_df.toxic_nontoxic == 0]\ntoxic_train_df.shape, nontoxic_train_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:20.478972Z","iopub.execute_input":"2022-01-30T15:16:20.479478Z","iopub.status.idle":"2022-01-30T15:16:20.50579Z","shell.execute_reply.started":"2022-01-30T15:16:20.479441Z","shell.execute_reply":"2022-01-30T15:16:20.505161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = 16000\ntoxic_train_df = toxic_train_df.sample(frac=1).reset_index(drop=True)\nnontoxic_train_df = nontoxic_train_df.sample(frac=1).reset_index(drop=True)\n\nnew_train_df = pd.concat([toxic_train_df[:sample], nontoxic_train_df[:sample]])\nnew_train_df = new_train_df.sample(frac=1).reset_index(drop=True)\nnew_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:20.507087Z","iopub.execute_input":"2022-01-30T15:16:20.507317Z","iopub.status.idle":"2022-01-30T15:16:20.57555Z","shell.execute_reply.started":"2022-01-30T15:16:20.507283Z","shell.execute_reply":"2022-01-30T15:16:20.574782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(new_train_df.toxic_nontoxic)\nplt.xlabel('Toxic vs Non toxic')\nax.set_xticklabels(['Non Toxic', 'Toxic'])","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:20.576829Z","iopub.execute_input":"2022-01-30T15:16:20.577187Z","iopub.status.idle":"2022-01-30T15:16:20.749734Z","shell.execute_reply.started":"2022-01-30T15:16:20.57715Z","shell.execute_reply":"2022-01-30T15:16:20.74902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train_df = new_train_df.drop(['toxic_nontoxic'], axis=1)\nnew_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:20.750931Z","iopub.execute_input":"2022-01-30T15:16:20.751233Z","iopub.status.idle":"2022-01-30T15:16:20.765275Z","shell.execute_reply.started":"2022-01-30T15:16:20.751198Z","shell.execute_reply":"2022-01-30T15:16:20.764656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a score column for each comment text","metadata":{}},{"cell_type":"code","source":"# Assign weights to each category\nweights_per_category = {'toxic': 0.5,\n                        'severe_toxic': 1.5,\n                        'obscene': 0.25,\n                        'threat': 1.5,\n                        'insult': 0.8,\n                        'identity_hate': 1.5}","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:20.766253Z","iopub.execute_input":"2022-01-30T15:16:20.766684Z","iopub.status.idle":"2022-01-30T15:16:20.778953Z","shell.execute_reply.started":"2022-01-30T15:16:20.766644Z","shell.execute_reply":"2022-01-30T15:16:20.778231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for category in weights_per_category:\n    new_train_df[category] = new_train_df[category] * weights_per_category[category]","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:20.78025Z","iopub.execute_input":"2022-01-30T15:16:20.780517Z","iopub.status.idle":"2022-01-30T15:16:20.791064Z","shell.execute_reply.started":"2022-01-30T15:16:20.780482Z","shell.execute_reply":"2022-01-30T15:16:20.790335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train_df['score'] = new_train_df.loc[:, 'toxic':'identity_hate'].mean(axis=1)\nnew_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:20.795324Z","iopub.execute_input":"2022-01-30T15:16:20.796028Z","iopub.status.idle":"2022-01-30T15:16:20.817625Z","shell.execute_reply.started":"2022-01-30T15:16:20.795992Z","shell.execute_reply":"2022-01-30T15:16:20.816989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Determining the max length of the tokens to be encoded from comment_text (using BertTokenizer)","metadata":{}},{"cell_type":"code","source":"# Defining tokenizer instance from BertTokenizer\n# we will use \"bert base uncased\" pretrained model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:20.818787Z","iopub.execute_input":"2022-01-30T15:16:20.819184Z","iopub.status.idle":"2022-01-30T15:16:32.866123Z","shell.execute_reply.started":"2022-01-30T15:16:20.819118Z","shell.execute_reply":"2022-01-30T15:16:32.865373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer encode example on sample text\nidx = 2\nsample_text = new_train_df.loc[idx, 'comment_text']\nprint('Sample Text: ')\nprint(sample_text)\n\n# we will use encode_plus method from tokenizer instance\n# https://huggingface.co/docs/transformers/v4.15.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus\n\nencoded_sample = tokenizer.encode_plus(\n    sample_text,\n    add_special_tokens=True,     # [CLS], [SEP], [PAD] tokens\n    max_length=100,   # to be determined next!!\n    return_token_type_ids=False,\n    padding='max_length',\n    return_attention_mask=True,\n    return_tensors='pt'    # pytorch tensor\n)\n\n# encoded_sample is a dictionary \n\nencoded_sample.keys()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:32.867526Z","iopub.execute_input":"2022-01-30T15:16:32.867815Z","iopub.status.idle":"2022-01-30T15:16:32.891882Z","shell.execute_reply.started":"2022-01-30T15:16:32.867776Z","shell.execute_reply":"2022-01-30T15:16:32.891084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the encoded token ids of the sample text\nprint('Encoded tokens of sample text:')\nprint(encoded_sample['input_ids'])\nprint(f\"Shape of encoded tokens: {encoded_sample['input_ids'].shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:32.893544Z","iopub.execute_input":"2022-01-30T15:16:32.894056Z","iopub.status.idle":"2022-01-30T15:16:32.913702Z","shell.execute_reply.started":"2022-01-30T15:16:32.894019Z","shell.execute_reply":"2022-01-30T15:16:32.912867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the attention mask (contains 1(token to be considered) and 0(token not to be considered, in case of padding))\nattention_mask = encoded_sample['attention_mask']\nprint(attention_mask)\nprint(f'Shape of attention mask: {attention_mask.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:32.915215Z","iopub.execute_input":"2022-01-30T15:16:32.91549Z","iopub.status.idle":"2022-01-30T15:16:32.92275Z","shell.execute_reply.started":"2022-01-30T15:16:32.915454Z","shell.execute_reply":"2022-01-30T15:16:32.921967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the encoded tokens of the sample text\nencoded_tokens = tokenizer.convert_ids_to_tokens(encoded_sample['input_ids'].squeeze())\nprint(encoded_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:32.924424Z","iopub.execute_input":"2022-01-30T15:16:32.925055Z","iopub.status.idle":"2022-01-30T15:16:32.931421Z","shell.execute_reply.started":"2022-01-30T15:16:32.925019Z","shell.execute_reply":"2022-01-30T15:16:32.930659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_lens = []\nfor i in tqdm(range(len(new_train_df))):\n    comment_text = new_train_df.loc[i, 'comment_text']\n    # encode comment_text (using .encode method to get only the token ids)\n    tokens = tokenizer.encode(comment_text, max_length=512, truncation=True)\n    token_lens.append(len(tokens))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:16:32.933085Z","iopub.execute_input":"2022-01-30T15:16:32.933571Z","iopub.status.idle":"2022-01-30T15:17:47.630423Z","shell.execute_reply.started":"2022-01-30T15:16:32.933533Z","shell.execute_reply":"2022-01-30T15:17:47.62977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(token_lens)\nplt.xlim([0,512])\nplt.xlabel('Token count')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:47.631651Z","iopub.execute_input":"2022-01-30T15:17:47.632414Z","iopub.status.idle":"2022-01-30T15:17:48.128059Z","shell.execute_reply.started":"2022-01-30T15:17:47.632376Z","shell.execute_reply":"2022-01-30T15:17:48.127418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that a good fit for max length of tokens would be around 350","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 350","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:48.129212Z","iopub.execute_input":"2022-01-30T15:17:48.129444Z","iopub.status.idle":"2022-01-30T15:17:48.133611Z","shell.execute_reply.started":"2022-01-30T15:17:48.129409Z","shell.execute_reply":"2022-01-30T15:17:48.132973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spliting newly sampled dataframe into training and validation data frames","metadata":{}},{"cell_type":"code","source":"train_df, val_df = train_test_split(new_train_df, test_size=0.2)\ntrain_df.shape, val_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:48.13466Z","iopub.execute_input":"2022-01-30T15:17:48.135401Z","iopub.status.idle":"2022-01-30T15:17:48.152287Z","shell.execute_reply.started":"2022-01-30T15:17:48.135365Z","shell.execute_reply":"2022-01-30T15:17:48.151508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building Custom Dataset\n","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        comment_text = self.df.iloc[index, 1]\n        target = self.df.iloc[index, -1]\n        \n        encoding = self.tokenizer.encode_plus(comment_text, \n                                             add_special_tokens=True,\n                                             max_length=self.max_len,\n                                             return_token_type_ids=False,\n                                             padding='max_length',\n                                             truncation=True,\n                                             return_attention_mask=True,\n                                             return_tensors='pt'\n                                             )\n        \n        input_ids = encoding['input_ids'].squeeze()    # Shape: (max_length)\n        attention_mask = encoding['attention_mask'].squeeze()    # Shape: (max_length)\n        target = torch.tensor(target)\n            \n        return input_ids, attention_mask, target","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:48.153385Z","iopub.execute_input":"2022-01-30T15:17:48.153626Z","iopub.status.idle":"2022-01-30T15:17:48.162177Z","shell.execute_reply.started":"2022-01-30T15:17:48.153578Z","shell.execute_reply":"2022-01-30T15:17:48.161477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\nval_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:48.163287Z","iopub.execute_input":"2022-01-30T15:17:48.163995Z","iopub.status.idle":"2022-01-30T15:17:48.168633Z","shell.execute_reply.started":"2022-01-30T15:17:48.163961Z","shell.execute_reply":"2022-01-30T15:17:48.167896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids, attention_mask, targets = train_dataset[0]\ninput_ids.shape, attention_mask.shape, targets.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:48.169628Z","iopub.execute_input":"2022-01-30T15:17:48.170216Z","iopub.status.idle":"2022-01-30T15:17:48.181199Z","shell.execute_reply.started":"2022-01-30T15:17:48.170184Z","shell.execute_reply":"2022-01-30T15:17:48.180476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids, attention_mask, targets = val_dataset[0]\ninput_ids.shape, attention_mask.shape, targets.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:48.182153Z","iopub.execute_input":"2022-01-30T15:17:48.182586Z","iopub.status.idle":"2022-01-30T15:17:48.191633Z","shell.execute_reply.started":"2022-01-30T15:17:48.182551Z","shell.execute_reply":"2022-01-30T15:17:48.190705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building Train and Val DataLoaders","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 16","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:48.193131Z","iopub.execute_input":"2022-01-30T15:17:48.193382Z","iopub.status.idle":"2022-01-30T15:17:48.197715Z","shell.execute_reply.started":"2022-01-30T15:17:48.193349Z","shell.execute_reply":"2022-01-30T15:17:48.196664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:48.199053Z","iopub.execute_input":"2022-01-30T15:17:48.200703Z","iopub.status.idle":"2022-01-30T15:17:48.205588Z","shell.execute_reply.started":"2022-01-30T15:17:48.200667Z","shell.execute_reply":"2022-01-30T15:17:48.204852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = next(iter(train_loader))\ninput_ids, attention_mask, targets = data\ninput_ids.shape, attention_mask.shape, targets.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:48.206952Z","iopub.execute_input":"2022-01-30T15:17:48.207549Z","iopub.status.idle":"2022-01-30T15:17:52.687961Z","shell.execute_reply.started":"2022-01-30T15:17:48.207515Z","shell.execute_reply":"2022-01-30T15:17:52.687275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's try running a sample batch on BertModel","metadata":{}},{"cell_type":"code","source":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:52.691856Z","iopub.execute_input":"2022-01-30T15:17:52.692085Z","iopub.status.idle":"2022-01-30T15:17:52.696282Z","shell.execute_reply.started":"2022-01-30T15:17:52.692058Z","shell.execute_reply":"2022-01-30T15:17:52.695154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiating pretrained bertmodel\nbert_model = BertModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:17:52.697954Z","iopub.execute_input":"2022-01-30T15:17:52.698568Z","iopub.status.idle":"2022-01-30T15:18:15.645436Z","shell.execute_reply.started":"2022-01-30T15:17:52.698527Z","shell.execute_reply":"2022-01-30T15:18:15.644704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = bert_model(input_ids=input_ids,\n                   attention_mask=attention_mask,\n                   return_dict=True)\noutput.keys()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:18:15.646853Z","iopub.execute_input":"2022-01-30T15:18:15.647211Z","iopub.status.idle":"2022-01-30T15:18:30.921157Z","shell.execute_reply.started":"2022-01-30T15:18:15.647173Z","shell.execute_reply":"2022-01-30T15:18:30.92044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bert_model outputs two tensors last_hidden_state and pooler_output\n# Let's see their dimensions\n\nlast_hidden_state, pooler_output = output['last_hidden_state'], output['pooler_output']\n\nlast_hidden_state.shape, pooler_output.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:18:30.92249Z","iopub.execute_input":"2022-01-30T15:18:30.922743Z","iopub.status.idle":"2022-01-30T15:18:30.928586Z","shell.execute_reply.started":"2022-01-30T15:18:30.922709Z","shell.execute_reply":"2022-01-30T15:18:30.927882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**last_hidden_state** contains the hidden representations for each token in each sequence of the batch. So the size is (batch_size, seq_len, hidden_size)\n\n**pooler_output** contains a \"representation\" of each sequence in the batch, and is of size (batch_size, hidden_size)\n\n768 is the hidden size\n\n[More about bert model output](https://github.com/huggingface/transformers/issues/7540)","metadata":{}},{"cell_type":"code","source":"bert_model.config.hidden_size","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:18:30.929985Z","iopub.execute_input":"2022-01-30T15:18:30.930448Z","iopub.status.idle":"2022-01-30T15:18:30.939867Z","shell.execute_reply.started":"2022-01-30T15:18:30.930411Z","shell.execute_reply":"2022-01-30T15:18:30.939189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are interested in the pooler_output.\nSo we will feed the pooler_output to linear layer to output a score","metadata":{}},{"cell_type":"code","source":"linear_layer = nn.Linear(768, 1)\noutput = linear_layer(pooler_output)\noutput.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:18:30.941442Z","iopub.execute_input":"2022-01-30T15:18:30.94175Z","iopub.status.idle":"2022-01-30T15:18:30.951161Z","shell.execute_reply.started":"2022-01-30T15:18:30.941674Z","shell.execute_reply":"2022-01-30T15:18:30.9505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Awesome! Now let's define the loss function and print the loss value corresponding to the sample batch we used","metadata":{}},{"cell_type":"code","source":"criterion = nn.MSELoss()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:18:30.95278Z","iopub.execute_input":"2022-01-30T15:18:30.953142Z","iopub.status.idle":"2022-01-30T15:18:30.957229Z","shell.execute_reply.started":"2022-01-30T15:18:30.953104Z","shell.execute_reply":"2022-01-30T15:18:30.956506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = criterion(output.squeeze(), targets)\nprint(f'Loss on sample batch: {loss.item()}')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:18:30.958802Z","iopub.execute_input":"2022-01-30T15:18:30.959221Z","iopub.status.idle":"2022-01-30T15:18:30.973952Z","shell.execute_reply.started":"2022-01-30T15:18:30.959185Z","shell.execute_reply":"2022-01-30T15:18:30.973257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Double Awesome!!","metadata":{}},{"cell_type":"markdown","source":"### Now let's build the model architecture","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, bert_model):\n        super(Net, self).__init__()\n        self.bert_model = bert_model\n        self.fcdense = nn.Linear(self.bert_model.config.hidden_size, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, input_ids, attention_mask):\n        bert_out = self.bert_model(input_ids, attention_mask, return_dict=True)\n        pooler_output =  bert_out['pooler_output']    # (batch_size, 768)\n        output = self.fcdense(pooler_output)       # (batch_size, 1)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:18:30.975174Z","iopub.execute_input":"2022-01-30T15:18:30.975426Z","iopub.status.idle":"2022-01-30T15:18:30.981532Z","shell.execute_reply.started":"2022-01-30T15:18:30.975394Z","shell.execute_reply":"2022-01-30T15:18:30.98065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Net(bert_model=bert_model).to(DEVICE)\noutput = model(input_ids.to(DEVICE), attention_mask.to(DEVICE))\nprint(output.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:18:30.983093Z","iopub.execute_input":"2022-01-30T15:18:30.983386Z","iopub.status.idle":"2022-01-30T15:18:31.945097Z","shell.execute_reply.started":"2022-01-30T15:18:30.983352Z","shell.execute_reply":"2022-01-30T15:18:31.944345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Freeze bert layers\nfor name, param in model.named_parameters(): \n    if 'fcdense' in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n    #print(name, param.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:18:31.946254Z","iopub.execute_input":"2022-01-30T15:18:31.947752Z","iopub.status.idle":"2022-01-30T15:18:31.953721Z","shell.execute_reply.started":"2022-01-30T15:18:31.947711Z","shell.execute_reply":"2022-01-30T15:18:31.952871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's train the model","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, train_loader, criterion, optimizer, DEVICE):\n    model.train()\n    \n    losses = []\n    \n    for batch_idx, data in enumerate(tqdm(train_loader)):\n        input_ids, attention_mask, targets = data\n        input_ids = input_ids.to(DEVICE)   # (batch_size, seq_len)\n        attention_mask = attention_mask.to(DEVICE)   # (batch_size, seq_len)\n        targets = targets.to(DEVICE)  # (batch_size,)\n\n        output = model(input_ids, attention_mask)   # (batch_size, 1)\n\n        loss = criterion(output.squeeze().float(), targets.float())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        losses.append(loss.item())\n\n    return np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:19:02.014646Z","iopub.execute_input":"2022-01-30T15:19:02.015125Z","iopub.status.idle":"2022-01-30T15:19:02.022393Z","shell.execute_reply.started":"2022-01-30T15:19:02.015087Z","shell.execute_reply":"2022-01-30T15:19:02.021644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_epoch(model, val_loader, criterion, DEVICE):\n    model.eval()\n    \n    losses = []\n    \n    with torch.no_grad():\n        for batch_idx, data in enumerate(tqdm(val_loader)):\n            input_ids, attention_mask, targets = data\n            input_ids = input_ids.to(DEVICE)   # (batch_size, seq_len)\n            attention_mask = attention_mask.to(DEVICE)   # (batch_size, seq_len)\n            targets = targets.to(DEVICE)  # (batch_size,)\n\n            output = model(input_ids, attention_mask)   # (batch_size, 1)\n\n            loss = criterion(output.squeeze().float(), targets.float())\n\n            losses.append(loss.item())\n\n    return np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:19:02.353306Z","iopub.execute_input":"2022-01-30T15:19:02.353932Z","iopub.status.idle":"2022-01-30T15:19:02.361356Z","shell.execute_reply.started":"2022-01-30T15:19:02.35389Z","shell.execute_reply":"2022-01-30T15:19:02.360214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 25\nLEARNING_RATE = 2e-5\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nbest_val_loss = np.inf\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch: {epoch+1}/{EPOCHS}')\n    print('-' * 10)\n    \n    print('Training')\n    train_loss = train_epoch(model,train_loader,criterion,optimizer,DEVICE)\n    \n    print('Validating')\n    val_loss = val_epoch(model,val_loader,criterion,DEVICE)\n    \n    print(f'Train Loss: {train_loss}\\t Val Loss: {val_loss}')\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'toxicity_best_model.pth.tar')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:19:03.021443Z","iopub.execute_input":"2022-01-30T15:19:03.022105Z","iopub.status.idle":"2022-01-30T15:19:07.405643Z","shell.execute_reply.started":"2022-01-30T15:19:03.022064Z","shell.execute_reply":"2022-01-30T15:19:07.404629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the bert_model and bert_tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer.save_pretrained('./tokenizer_pretrained/')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T06:25:06.61404Z","iopub.execute_input":"2022-01-30T06:25:06.614734Z","iopub.status.idle":"2022-01-30T06:25:06.644287Z","shell.execute_reply.started":"2022-01-30T06:25:06.614696Z","shell.execute_reply":"2022-01-30T06:25:06.643608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.save_pretrained('./bert_model_pretrained/')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T06:17:01.636423Z","iopub.execute_input":"2022-01-30T06:17:01.636981Z","iopub.status.idle":"2022-01-30T06:17:02.934026Z","shell.execute_reply.started":"2022-01-30T06:17:01.636943Z","shell.execute_reply":"2022-01-30T06:17:02.933034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Done!')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}