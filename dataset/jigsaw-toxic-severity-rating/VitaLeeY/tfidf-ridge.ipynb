{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Please upvote if you like my work :)","metadata":{}},{"cell_type":"markdown","source":"<h2>Imports</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport progressbar\nimport nltk\nimport matplotlib.pyplot as plt\nimport re\n\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-27T05:32:40.565153Z","iopub.execute_input":"2021-12-27T05:32:40.565854Z","iopub.status.idle":"2021-12-27T05:32:42.426662Z","shell.execute_reply.started":"2021-12-27T05:32:40.565765Z","shell.execute_reply":"2021-12-27T05:32:42.426043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Upload Datasets</h2>","metadata":{}},{"cell_type":"code","source":"TRAIN_DATA_PATH = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\nTEST_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\nVALID_DATA_PATH = \"../input/jigsaw-toxic-severity-rating/validation_data.csv\"\nSAMPLE_SUBMISSION = \"/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:32:42.427851Z","iopub.execute_input":"2021-12-27T05:32:42.428355Z","iopub.status.idle":"2021-12-27T05:32:42.43336Z","shell.execute_reply.started":"2021-12-27T05:32:42.428268Z","shell.execute_reply":"2021-12-27T05:32:42.432566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_DATA_PATH)\ndf_test = pd.read_csv(TEST_DATA_PATH)\ndf_validation_data = pd.read_csv(VALID_DATA_PATH)\ndf_sample_submission = pd.read_csv(SAMPLE_SUBMISSION)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:32:42.434929Z","iopub.execute_input":"2021-12-27T05:32:42.435256Z","iopub.status.idle":"2021-12-27T05:32:45.271262Z","shell.execute_reply.started":"2021-12-27T05:32:42.435217Z","shell.execute_reply":"2021-12-27T05:32:45.270413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scoring training data","metadata":{}},{"cell_type":"code","source":"# for col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n#     print(f'****** {col} *******')\n#     display(df_train.loc[df_train[col]==1,['comment_text',col]].sample(10))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:32:45.273137Z","iopub.execute_input":"2021-12-27T05:32:45.273392Z","iopub.status.idle":"2021-12-27T05:32:45.277024Z","shell.execute_reply.started":"2021-12-27T05:32:45.273364Z","shell.execute_reply":"2021-12-27T05:32:45.276124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a score that messure how much toxic is a comment\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\n# cat_mtpl = {'obscene': 3, 'toxic': 4, 'threat': 4, \n#             'insult': 2, 'severe_toxic': 4, 'identity_hate': 2}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\ndf_train_new","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:32:45.278334Z","iopub.execute_input":"2021-12-27T05:32:45.278691Z","iopub.status.idle":"2021-12-27T05:32:45.396669Z","shell.execute_reply.started":"2021-12-27T05:32:45.278664Z","shell.execute_reply":"2021-12-27T05:32:45.3958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Clean text</h2>\n\nBoth data sets (train al test) need to be cleaned for better predictions.","metadata":{}},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+')  # Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) # Remove special Charecters\n    text = re.sub(' +', ' ', text) # Remove Extra Spaces\n    text = text.strip().lower() # remove spaces at the beginning and at the end of string and make string lower\n    \n    # lemmatization\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n    # del stopwords\n    text = ' '.join([word for word in text.split(' ') if word not in stop])\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:32:45.397941Z","iopub.execute_input":"2021-12-27T05:32:45.398235Z","iopub.status.idle":"2021-12-27T05:32:45.405093Z","shell.execute_reply.started":"2021-12-27T05:32:45.398204Z","shell.execute_reply":"2021-12-27T05:32:45.404317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(data, col):\n    \n    data[col] = data[col].str.replace('https?://\\S+|www\\.\\S+', ' social medium ')      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\") \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\")\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \")\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \")\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you')\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace('\\s+', ' ')  # will remove more than one whitespace character\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1') # 2 or more characters are replaced by 2 characters\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\"[:|♣|'|§|♠|*|/|?|=|%|&|-|#|•|~|^|>|<|►|_]\", '')\n    \n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:32:45.406729Z","iopub.execute_input":"2021-12-27T05:32:45.407259Z","iopub.status.idle":"2021-12-27T05:32:45.446526Z","shell.execute_reply.started":"2021-12-27T05:32:45.407212Z","shell.execute_reply":"2021-12-27T05:32:45.4456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n\nlemmatizer = WordNetLemmatizer()\nstop = stopwords.words('english')\n\n# tqdm.pandas()\n# df_train_new['clean_text'] = df_train_new['comment_text'].progress_apply(text_cleaning)\ndf_train_new = clean(df_train_new, 'comment_text')","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:32:45.447895Z","iopub.execute_input":"2021-12-27T05:32:45.448336Z","iopub.status.idle":"2021-12-27T05:32:58.163937Z","shell.execute_reply.started":"2021-12-27T05:32:45.448286Z","shell.execute_reply":"2021-12-27T05:32:58.163317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\ndf_train_new['clean_text'] = df_train_new['comment_text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:32:58.165181Z","iopub.execute_input":"2021-12-27T05:32:58.165832Z","iopub.status.idle":"2021-12-27T05:33:10.057029Z","shell.execute_reply.started":"2021-12-27T05:32:58.165795Z","shell.execute_reply":"2021-12-27T05:33:10.05644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compare few models","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport lightgbm as lgbm\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.linear_model import Ridge, LogisticRegression, RidgeCV, ElasticNet, SGDRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.ensemble import StackingRegressor, VotingRegressor","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:33:10.059306Z","iopub.execute_input":"2021-12-27T05:33:10.059537Z","iopub.status.idle":"2021-12-27T05:33:11.061872Z","shell.execute_reply.started":"2021-12-27T05:33:10.059509Z","shell.execute_reply":"2021-12-27T05:33:11.061056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train_new = df_train_new.reset_index(drop=True)\nlabels = df_train_new['y']\ncomments = df_train_new['clean_text']\n\nvectorizer = TfidfVectorizer(min_df=3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5))\ncomments_tr = vectorizer.fit_transform(comments)\ncomments_tr","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:33:11.06316Z","iopub.execute_input":"2021-12-27T05:33:11.063402Z","iopub.status.idle":"2021-12-27T05:33:22.790673Z","shell.execute_reply.started":"2021-12-27T05:33:11.063375Z","shell.execute_reply":"2021-12-27T05:33:22.78968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vectorizer = TfidfVectorizer()\n# counts = vectorizer.fit_transform(X_train)\n\n# regressor = Ridge(alpha=0.2)\n# regressor.fit(comments_tr, labels)\n\n# models = [('ridge05', Ridge(random_state=42, alpha=0.8)),\n# #           ('linSVR', LinearSVR(random_state=42)),\n#           ('sgd', SGDRegressor(random_state=42))]\n\n# final_estimator = lgbm.LGBMRegressor(random_state=42)\n\n# regressor = VotingRegressor(estimators=models)\nregressor = Ridge(random_state=42, alpha=0.8)\nregressor.fit(comments_tr, labels)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:39:55.828875Z","iopub.execute_input":"2021-12-27T05:39:55.829144Z","iopub.status.idle":"2021-12-27T05:40:00.588493Z","shell.execute_reply.started":"2021-12-27T05:39:55.829116Z","shell.execute_reply":"2021-12-27T05:40:00.587284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets test our model","metadata":{}},{"cell_type":"code","source":"# preprocess val data\n\ntqdm.pandas()\ndf_validation_data = clean(df_validation_data, 'less_toxic')\ndf_validation_data = clean(df_validation_data, 'more_toxic')\ndf_validation_data['less_toxic'] = df_validation_data['less_toxic'].progress_apply(text_cleaning)\ndf_validation_data['more_toxic'] = df_validation_data['more_toxic'].progress_apply(text_cleaning)\n\nless_toxic = vectorizer.transform(df_validation_data['less_toxic'])\nmore_toxic = vectorizer.transform(df_validation_data['more_toxic'])\n\n# make predictions\ny_pred_less = regressor.predict(less_toxic)\ny_pred_more = regressor.predict(more_toxic)\n\n(y_pred_less < y_pred_more).mean()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:40:00.590678Z","iopub.execute_input":"2021-12-27T05:40:00.591716Z","iopub.status.idle":"2021-12-27T05:41:06.779084Z","shell.execute_reply.started":"2021-12-27T05:40:00.591665Z","shell.execute_reply":"2021-12-27T05:41:06.778277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0.6765643682742128 - Ridge, TFIDF, words lower  \n0.6770293609671848 - Ridge, TFIDF, words lower, severe_toxic=1.5  \n0.6786900491563704 - LinearSVR, TFIDF, words lower, severe_toxic=1.5. 0.821 на тесте  \n0.6777932775342101 - Ridge, TFIDF, words lower, severe_toxic=1.5, alpha=1.342857142857143","metadata":{}},{"cell_type":"code","source":"# regressor = Ridge(alpha=0.8)\n# regressor.fit(comments_tr, labels)\n\n# y_pred_less = regressor.predict(less_toxic)\n# y_pred_more = regressor.predict(more_toxic)\n\n# print(0.8, (y_pred_less < y_pred_more).mean())","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:34:41.07169Z","iopub.execute_input":"2021-12-27T05:34:41.071905Z","iopub.status.idle":"2021-12-27T05:34:41.075965Z","shell.execute_reply.started":"2021-12-27T05:34:41.071879Z","shell.execute_reply":"2021-12-27T05:34:41.074826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for alpha in np.linspace(0.1, 2.5, 25):\n#     regressor = Ridge(alpha=alpha)\n#     regressor.fit(comments_tr, labels)\n\n#     y_pred_less = regressor.predict(less_toxic)\n#     y_pred_more = regressor.predict(more_toxic)\n\n#     print(alpha, (y_pred_less < y_pred_more).mean())","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:34:41.077432Z","iopub.execute_input":"2021-12-27T05:34:41.077797Z","iopub.status.idle":"2021-12-27T05:34:41.091676Z","shell.execute_reply.started":"2021-12-27T05:34:41.077757Z","shell.execute_reply":"2021-12-27T05:34:41.090903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions and load submission.csv","metadata":{}},{"cell_type":"code","source":"# df_test['text'] = df_test['text'].apply(lambda x: ' '.join([word for word in words_pattern.findall(x) if word not in stopwords]))\n# df_test['text'] = df_test['text'].apply(lambda elem: elem.strip().lower())\ndf_test = clean(df_test, 'text')\ndf_test['text'] = df_test['text'].progress_apply(text_cleaning)\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:34:41.092758Z","iopub.execute_input":"2021-12-27T05:34:41.093553Z","iopub.status.idle":"2021-12-27T05:34:47.389971Z","shell.execute_reply.started":"2021-12-27T05:34:41.093519Z","shell.execute_reply":"2021-12-27T05:34:47.389084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['prediction'] = regressor.predict(vectorizer.transform(df_test['text']))\ndf_test = df_test[['comment_id','prediction']]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:34:47.391666Z","iopub.execute_input":"2021-12-27T05:34:47.391974Z","iopub.status.idle":"2021-12-27T05:34:50.661672Z","shell.execute_reply.started":"2021-12-27T05:34:47.391931Z","shell.execute_reply":"2021-12-27T05:34:50.660594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['score'] = df_test['prediction']\ndf_test = df_test[['comment_id','score']]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:34:50.662936Z","iopub.execute_input":"2021-12-27T05:34:50.663768Z","iopub.status.idle":"2021-12-27T05:34:50.669846Z","shell.execute_reply.started":"2021-12-27T05:34:50.663724Z","shell.execute_reply":"2021-12-27T05:34:50.669158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:34:50.670919Z","iopub.execute_input":"2021-12-27T05:34:50.671203Z","iopub.status.idle":"2021-12-27T05:34:50.704946Z","shell.execute_reply.started":"2021-12-27T05:34:50.671173Z","shell.execute_reply":"2021-12-27T05:34:50.704287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Please upvote if you like my work :)","metadata":{}}]}