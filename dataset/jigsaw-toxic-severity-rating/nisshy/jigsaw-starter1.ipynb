{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport string\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom transformers import AutoTokenizer, AutoModel, AdamW\n\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:57:59.149001Z","iopub.execute_input":"2022-01-12T21:57:59.149985Z","iopub.status.idle":"2022-01-12T21:58:06.51803Z","shell.execute_reply.started":"2022-01-12T21:57:59.149867Z","shell.execute_reply":"2022-01-12T21:58:06.517248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def id_generator(size=12, chars=string.ascii_lowercase + string.digits):\n    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n\nHASH_NAME = id_generator(size=12)\nprint(HASH_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:06.519785Z","iopub.execute_input":"2022-01-12T21:58:06.520042Z","iopub.status.idle":"2022-01-12T21:58:06.533061Z","shell.execute_reply.started":"2022-01-12T21:58:06.520009Z","shell.execute_reply":"2022-01-12T21:58:06.532364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"seed\": 2021,\n          \"epochs\": 3,\n          \"model_name\": \"../input/roberta-base\",\n          \"train_batch_size\": 32,\n          \"valid_batch_size\": 64,\n          \"max_length\": 128,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-6,\n          \"T_max\": 500,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 5,\n          \"n_accumulate\": 1,\n          \"num_classes\": 1,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          \"hash_name\": HASH_NAME\n          }\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\nCONFIG['group'] = f'{HASH_NAME}-Baseline'","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:06.534587Z","iopub.execute_input":"2022-01-12T21:58:06.536619Z","iopub.status.idle":"2022-01-12T21:58:06.725212Z","shell.execute_reply.started":"2022-01-12T21:58:06.536583Z","shell.execute_reply":"2022-01-12T21:58:06.724441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:06.726571Z","iopub.execute_input":"2022-01-12T21:58:06.726819Z","iopub.status.idle":"2022-01-12T21:58:06.735871Z","shell.execute_reply.started":"2022-01-12T21:58:06.726786Z","shell.execute_reply":"2022-01-12T21:58:06.735157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:06.738724Z","iopub.execute_input":"2022-01-12T21:58:06.739093Z","iopub.status.idle":"2022-01-12T21:58:07.215757Z","shell.execute_reply.started":"2022-01-12T21:58:06.739056Z","shell.execute_reply":"2022-01-12T21:58:07.215052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.217088Z","iopub.execute_input":"2022-01-12T21:58:07.217352Z","iopub.status.idle":"2022-01-12T21:58:07.224097Z","shell.execute_reply.started":"2022-01-12T21:58:07.217319Z","shell.execute_reply":"2022-01-12T21:58:07.223375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold, (_, val_) in enumerate(skf.split(X=df, y=df.worker)):\n    df.loc[val_, \"kfold\"] = int(fold)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.22547Z","iopub.execute_input":"2022-01-12T21:58:07.225789Z","iopub.status.idle":"2022-01-12T21:58:07.266796Z","shell.execute_reply.started":"2022-01-12T21:58:07.225753Z","shell.execute_reply":"2022-01-12T21:58:07.266209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"kfold\"] = df['kfold'].astype(int)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.267802Z","iopub.execute_input":"2022-01-12T21:58:07.268046Z","iopub.status.idle":"2022-01-12T21:58:07.280546Z","shell.execute_reply.started":"2022-01-12T21:58:07.268013Z","shell.execute_reply":"2022-01-12T21:58:07.279762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n            more_toxic,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n        \n        inputs_less_toxic = self.tokenizer.encode_plus(\n            less_toxic,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'        \n        )\n        \n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask'] \n        \n        return {\n            'more_toxic_ids':torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask':torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids':torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask':torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target':torch.tensor(target, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.281884Z","iopub.execute_input":"2022-01-12T21:58:07.282124Z","iopub.status.idle":"2022-01-12T21:58:07.292258Z","shell.execute_reply.started":"2022-01-12T21:58:07.282091Z","shell.execute_reply":"2022-01-12T21:58:07.29148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):\n        out = self.model(input_ids=ids, attention_mask=mask,output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.293678Z","iopub.execute_input":"2022-01-12T21:58:07.294212Z","iopub.status.idle":"2022-01-12T21:58:07.303891Z","shell.execute_reply.started":"2022-01-12T21:58:07.29416Z","shell.execute_reply":"2022-01-12T21:58:07.303237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def criterion(outputs1, outputs2, targets):\n    return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.306739Z","iopub.execute_input":"2022-01-12T21:58:07.306944Z","iopub.status.idle":"2022-01-12T21:58:07.314925Z","shell.execute_reply.started":"2022-01-12T21:58:07.306919Z","shell.execute_reply":"2022-01-12T21:58:07.314259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    \n    for step, data in bar:\n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype=torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype=torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype=torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype=torch.long) \n        targets = data['target'].to(device, dtype=torch.long)\n        \n        batch_size = more_toxic_ids.size(0)\n        \n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        \n        loss = loss / CONFIG['n_accumulate']\n        loss.backward()\n        \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss, LR=optimizer.param_groups[0]['lr'])\n        \n    gc.collect()\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.31618Z","iopub.execute_input":"2022-01-12T21:58:07.3165Z","iopub.status.idle":"2022-01-12T21:58:07.328296Z","shell.execute_reply.started":"2022-01-12T21:58:07.316466Z","shell.execute_reply":"2022-01-12T21:58:07.327505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\n\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    \n    for step, data in bar:\n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype=torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype=torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype=torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype=torch.long) \n        targets = data['target'].to(device, dtype=torch.long)\n        \n        batch_size = more_toxic_ids.size(0)\n        \n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        \n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, LR=optimizer.param_groups[0]['lr'])\n        \n    gc.collect()\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.329704Z","iopub.execute_input":"2022-01-12T21:58:07.329962Z","iopub.status.idle":"2022-01-12T21:58:07.341153Z","shell.execute_reply.started":"2022-01-12T21:58:07.329927Z","shell.execute_reply":"2022-01-12T21:58:07.340512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n    \n    if torch.cuda.is_available():\n        print(\"[INFO] using GPU {}\\n\".format(torch.cuda.get_device_name()))\n        \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_loss = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1):\n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader,\n                                           device=CONFIG['device'], epoch=epoch)\n        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'],\n                                         epoch=epoch)\n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        \n        if val_epoch_loss <= best_epoch_loss:\n            print(f\"{b_}Validation loss improved ({best_epoch_loss} -> {val_epoch_loss})\")\n            best_epoch_loss = val_epoch_loss\n            #run.summary['Best Loss'] = best_epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"Loss-Fold-{fold}.bin\"\n            torch.save(model.state_dict(), PATH)\n            print(f\"model saves{sr_}\")\n            \n        print()\n        \n    end = time.time()\n    \n    time_elapsed = end - start\n    print('training complete in {:.0f}s'.format(time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"best loss {:.4f}\".format(best_epoch_loss))\n    \n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.345561Z","iopub.execute_input":"2022-01-12T21:58:07.345765Z","iopub.status.idle":"2022-01-12T21:58:07.357994Z","shell.execute_reply.started":"2022-01-12T21:58:07.345737Z","shell.execute_reply":"2022-01-12T21:58:07.357237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,\n                                                   T_max=CONFIG['T_max'],\n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,\n                                                             T_0=CONFIG['T_0'],\n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n    \n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.359209Z","iopub.execute_input":"2022-01-12T21:58:07.359732Z","iopub.status.idle":"2022-01-12T21:58:07.369109Z","shell.execute_reply.started":"2022-01-12T21:58:07.359695Z","shell.execute_reply":"2022-01-12T21:58:07.368233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = JigsawDataset(df_train, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n    \n    train_loader = DataLoader(train_dataset,\n                              batch_size=CONFIG['train_batch_size'],\n                              num_workers=2,\n                              shuffle=True,\n                              pin_memory=True,\n                              drop_last=True)\n    \n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CONFIG['valid_batch_size'],\n                              num_workers=2,\n                              shuffle=True,\n                              pin_memory=True)\n        \n    return train_loader, valid_loader\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.370372Z","iopub.execute_input":"2022-01-12T21:58:07.370768Z","iopub.status.idle":"2022-01-12T21:58:07.380552Z","shell.execute_reply.started":"2022-01-12T21:58:07.370731Z","shell.execute_reply":"2022-01-12T21:58:07.37981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(0, CONFIG['n_fold']):\n    print(f\"{y_} ===== fold: {fold} ====={sr_}\")\n    \n    train_loader, valid_loader = prepare_loaders(fold=fold)\n    \n    model = JigsawModel(CONFIG['model_name'])\n    model.to(CONFIG['device'])\n    \n    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n    scheduler = fetch_scheduler(optimizer)\n    \n    model, history = run_training(model, optimizer, scheduler,\n                                  device=CONFIG['device'],\n                                  num_epochs=CONFIG['epochs'],\n                                  fold=fold)\n    \n    del model, history, train_loader, valid_loader\n    _ = gc.collect()\n    \n    print()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T21:58:07.38191Z","iopub.execute_input":"2022-01-12T21:58:07.382281Z","iopub.status.idle":"2022-01-13T00:50:05.250124Z","shell.execute_reply.started":"2022-01-12T21:58:07.382245Z","shell.execute_reply":"2022-01-13T00:50:05.248601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}