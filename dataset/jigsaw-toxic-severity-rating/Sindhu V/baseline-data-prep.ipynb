{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Not even a useful baseline. But will work to evaluate the model. Last competition's public NB that uses combined submission also has a binary output, and a score of 0.97. We'll use those probability scores with weights for each comment type to generate a final regressor score. Bertweet model will get preprocessed inputs (except url and twitter handle preprocessing) and a regressor head on top of its model is finetuned. \n\n\n# Test from some prediction\n## Score generation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd, gc, wt_text_processing_utils as wtp_utils, numpy as np\nfrom tqdm import tqdm\ntqdm.pandas()\n\ndftest = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\")\ndfpred = pd.read_csv(\"../input/classifying-multi-label-comments-0-9741-lb/submission_binary.csv\")\ndftest = pd.merge(dftest,dfpred, on=\"id\", how=\"left\")\n\n#dftest = dftest.sample(1000).reset_index(drop=True)\ndel dfpred\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T04:26:45.624235Z","iopub.execute_input":"2021-11-19T04:26:45.624848Z","iopub.status.idle":"2021-11-19T04:26:49.082396Z","shell.execute_reply.started":"2021-11-19T04:26:45.624736Z","shell.execute_reply":"2021-11-19T04:26:49.081222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nweights = [1, 2, 3, 4, 1, 5]\n\ndef generate_weighted_score(row):\n    numerator = 0\n    for col, wt in zip(cols, weights):\n        numerator += row[col]*wt\n    return numerator/sum(weights) #Why weight normalise? Because HF gives 0-1 scores/\n\ndftest[\"unweighted_score\"] = dftest.progress_apply(lambda row: np.mean([row[col] for col in cols]), axis=1)\ndftest[\"weighted_score\"] = dftest.progress_apply(generate_weighted_score, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T04:26:49.084023Z","iopub.execute_input":"2021-11-19T04:26:49.084251Z","iopub.status.idle":"2021-11-19T04:26:49.233505Z","shell.execute_reply.started":"2021-11-19T04:26:49.084224Z","shell.execute_reply":"2021-11-19T04:26:49.231654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftest[\"weighted_score\"].describe() \ndftest[\"bin\"] = dftest[\"weighted_score\"].round(1)#.astype(str) \ndftest[\"bin\"] = dftest[\"bin\"]>0.\n(dftest[\"weighted_score\"]>=0.5).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T04:26:49.235165Z","iopub.execute_input":"2021-11-19T04:26:49.235536Z","iopub.status.idle":"2021-11-19T04:26:49.259066Z","shell.execute_reply.started":"2021-11-19T04:26:49.235486Z","shell.execute_reply":"2021-11-19T04:26:49.257587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```python\ntraincs = set(dftest[\"comment_text\"].unique().tolist())\ndummy = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ntestcs = set(dummy[\"less_toxic\"].unique().tolist()+dummy[\"more_toxic\"].unique().tolist())\nlen(traincs), len(testcs), len(traincs-testcs), len(testcs-traincs), len(traincs.intersection(testcs))\nOP: (153164, 14251, 153164, 14251, 0)\n```","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:31:53.781227Z","iopub.execute_input":"2021-11-18T13:31:53.781544Z","iopub.status.idle":"2021-11-18T13:31:54.694607Z","shell.execute_reply.started":"2021-11-18T13:31:53.781512Z","shell.execute_reply":"2021-11-18T13:31:54.693596Z"}}},{"cell_type":"markdown","source":"## Preprocessing\nShould be kinda similar [to source code](https://github.com/VinAIResearch/BERTweet#preprocess).  Don't need twitter like preprocessing. But following are required:\n<ol>\n    <li>Replacing \\n with spaces.</li>\n    <li>Replacing <a href=\"https://en.wikipedia.org/wiki/Help:Talk_pages#Indentation\">Wiki specific tags</a></li>\n    <li>emoji normalizing </li>\n</ol>","metadata":{}},{"cell_type":"code","source":"dftest[\"normed_text\"] = wtp_utils.preprocess_text(dftest[\"comment_text\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T04:26:49.262161Z","iopub.execute_input":"2021-11-19T04:26:49.262628Z","iopub.status.idle":"2021-11-19T04:26:59.427216Z","shell.execute_reply.started":"2021-11-19T04:26:49.262546Z","shell.execute_reply":"2021-11-19T04:26:59.426301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cois = ['id', 'comment_text', 'normed_text', 'unweighted_score', 'weighted_score']\ndftest = dftest[cois]\ndftest[cois].to_csv(\"cleaned_train.csv\", index=False) #misnomer. This is test data with roc 0.97","metadata":{"execution":{"iopub.status.busy":"2021-11-19T04:30:43.969977Z","iopub.execute_input":"2021-11-19T04:30:43.970296Z","iopub.status.idle":"2021-11-19T04:30:44.016376Z","shell.execute_reply.started":"2021-11-19T04:30:43.970252Z","shell.execute_reply":"2021-11-19T04:30:44.015494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train data","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport gc, pandas as pd\ntqdm.pandas()\n\ntraindf = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\ncols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntraindf[\"bin\"] = traindf.apply(lambda row: any([row[c]==1 for c in cols]), axis=1)\ntraindf = traindf.groupby(\"bin\").sample(16225).reset_index(drop=True)\n\nwts = pd.DataFrame([(c, traindf[c].value_counts().iloc[1]) for c in cols])\nwts[1] = 1000/wts[1]\nwts.set_index(0, inplace=True)\nwts = wts.T.iloc[0].to_dict()\n\ntot_wts = sum(wts.values())\n\ntraindf[\"new_wscore\"] = traindf.progress_apply(lambda row: sum([wts[c]*row[c] for c in cols]), axis=1)\ntraindf[\"new_wscore\"] = traindf[\"new_wscore\"].apply(lambda x: min(x, .25))\ntraindf[\"new_wscore\"] = (traindf[\"new_wscore\"]-traindf[\"new_wscore\"].min())/(traindf[\"new_wscore\"].max()-traindf[\"new_wscore\"].min())\n\ncois = [\"id\", \"comment_text\", \"new_wscore\"]\ntraindf = traindf[cois]\n\ntraindf[\"normed_text\"] = wtp_utils.preprocess_text(traindf[\"comment_text\"])\ntraindf.to_csv(\"cleaned_train_actual.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T04:30:54.743022Z","iopub.execute_input":"2021-11-19T04:30:54.743335Z","iopub.status.idle":"2021-11-19T04:30:55.517917Z","shell.execute_reply.started":"2021-11-19T04:30:54.743304Z","shell.execute_reply":"2021-11-19T04:30:55.517131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport gc, pandas as pd\ntqdm.pandas()\n\ntraindf = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\ncols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nis_specific_t = traindf.apply(lambda row: any([row[c]==1 for c in cols[1:]]), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T10:21:17.095585Z","iopub.execute_input":"2021-11-20T10:21:17.096003Z","iopub.status.idle":"2021-11-20T10:21:22.315919Z","shell.execute_reply.started":"2021-11-20T10:21:17.095909Z","shell.execute_reply":"2021-11-20T10:21:22.315307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pseudoranked train data\n\nObscene is usually swearwords. Insult and obscene are kinda hand-in-hand. So same league? threat and identity hate are at highest. So we have non-toxic < toxic < severe toxic < obscenity and insults < threat and identity hate\n","metadata":{}},{"cell_type":"code","source":"#nontoxic < toxiconly < severe toxic only < (obscene/insult/both only) < (threat or identity hate or both only)\nis_non_toxic = traindf[\"toxic\"]==0\nfor c in cols[1:]:\n    is_non_toxic &= traindf[c]==0\n    \nis_toxic_only = traindf[\"toxic\"]==1\nfor c in [\"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]:\n    is_toxic_only &= traindf[c]==0\n\nis_severe_toxic_only = traindf[\"severe_toxic\"]==1\nfor c in [\"obscene\", \"threat\", \"insult\", \"identity_hate\"]:\n    is_severe_toxic_only &= traindf[c]==0\n\nis_oi_only = (traindf[\"obscene\"]==1)|(traindf[\"insult\"]==1)\nfor c in [\"toxic\", \"severe_toxic\", \"threat\", \"identity_hate\"]:\n    is_oi_only &= traindf[c]==0\n\nis_it_only = (traindf[\"threat\"]==1)|(traindf[\"identity_hate\"]==1)\nfor c in [\"toxic\", \"severe_toxic\", \"obscene\", \"insult\"]:\n    is_it_only &= traindf[c]==0\n\n    \ntraindf[\"rank\"] = -1 #Inconclusive\ntraindf.loc[is_non_toxic, \"rank\"] = 0\ntraindf.loc[is_toxic_only, \"rank\"] = 1\ntraindf.loc[is_severe_toxic_only, \"rank\"] = 2\ntraindf.loc[is_oi_only, \"rank\"] = 3\ntraindf.loc[is_it_only, \"rank\"] = 4\ntraindf[\"rank\"].value_counts()\n\ntraindf.to_csv(\"ranked_train_actual.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T10:57:11.223414Z","iopub.execute_input":"2021-11-20T10:57:11.223673Z","iopub.status.idle":"2021-11-20T10:57:13.01005Z","shell.execute_reply.started":"2021-11-20T10:57:11.223646Z","shell.execute_reply":"2021-11-20T10:57:13.009202Z"},"trusted":true},"execution_count":null,"outputs":[]}]}