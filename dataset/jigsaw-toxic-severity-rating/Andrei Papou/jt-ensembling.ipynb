{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"from __future__ import annotations\n\nimport functools\nimport itertools\nimport random\nimport statistics\nimport typing as t\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as torch_f\nimport typing_extensions as t_ext\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\nfrom transformers.models.auto.configuration_auto import AutoConfig\nfrom transformers.models.auto.modeling_auto import AutoModel\nfrom transformers.models.auto.tokenization_auto import AutoTokenizer\nfrom scipy.stats import rankdata","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-30T17:12:46.722888Z","iopub.execute_input":"2022-01-30T17:12:46.723189Z","iopub.status.idle":"2022-01-30T17:12:49.075404Z","shell.execute_reply.started":"2022-01-30T17:12:46.723134Z","shell.execute_reply":"2022-01-30T17:12:49.074636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Datasets","metadata":{}},{"cell_type":"code","source":"class _TokenizedText(t_ext.TypedDict):\n    input_ids: torch.Tensor\n    attention_mask: torch.Tensor\n\n\ndef _preprocess_tokenizer_output(output: t.Dict[str, t.Any]) -> _TokenizedText:\n    return {\n        'input_ids': torch.tensor(output['input_ids']),\n        'attention_mask': torch.tensor(output['attention_mask']),\n    }\n\n\ndef _split_str_to_chunk_list(s: str, chunk_size: int) -> t.List[str]:\n    chunk_list = []\n    chunk = []\n    for token in s.split(' '):\n        chunk.append(token)\n        if len(chunk) >= chunk_size:\n            chunk_list.append(' '.join(chunk))\n            chunk.clear()\n    if chunk:\n        chunk_list.append(' '.join(chunk))\n    return chunk_list\n\n\ndef predict_collate_fn(\n        sample_list: t.List[t.Tuple[str, _TokenizedText]]\n        ) -> t.Tuple[t.List[str], _TokenizedText, t.List[slice]]:\n    curr_pos = 0\n\n    idx_list: t.List[str] = []\n    input_ids_list = []\n    attention_mask_list = []\n    slice_list: t.List[slice] = []\n    \n    for sample in sample_list:\n        idx_list.append(sample[0])\n        input_ids, attention_mask = sample[1]['input_ids'], sample[1]['attention_mask']\n        input_ids_list.append(input_ids)\n        attention_mask_list.append(attention_mask)\n        slice_list.append(slice(curr_pos, curr_pos + input_ids.shape[0]))\n        curr_pos += input_ids.shape[0]\n\n    tokenized_collated: _TokenizedText = {\n        'input_ids': torch.cat(input_ids_list, dim=0),\n        'attention_mask': torch.cat(attention_mask_list, dim=0),\n    }\n\n    return idx_list, tokenized_collated, slice_list\n\n\nclass PredictDataset(Dataset):\n\n    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int) -> None:\n        super().__init__()\n        self._df = df\n        self._tokenizer = tokenizer\n        self._max_len = max_len\n\n    def __len__(self) -> int:\n        return len(self._df)\n\n    def __getitem__(self, idx: int) -> t.Tuple[str, _TokenizedText]:\n        record = self._df.iloc[idx]\n        comment_id, text = str(record['comment_id']), str(record['text'])\n\n        input_ids_list, attention_mask_list = [], []\n        for chunk in _split_str_to_chunk_list(text, chunk_size=self._max_len):\n            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n                chunk,\n                add_special_tokens=True,\n                truncation=True,\n                padding='max_length',\n                max_length=self._max_len,\n                return_attention_mask=True))  # type: ignore\n            input_ids_list.append(tokenized_chunk['input_ids'])\n            attention_mask_list.append(tokenized_chunk['attention_mask'])\n\n        tokenized_text: _TokenizedText = {\n            'input_ids': torch.stack(input_ids_list, dim=0),\n            'attention_mask': torch.stack(attention_mask_list, dim=0),\n        }\n\n        return comment_id, tokenized_text","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:12:49.080226Z","iopub.execute_input":"2022-01-30T17:12:49.080694Z","iopub.status.idle":"2022-01-30T17:12:49.099057Z","shell.execute_reply.started":"2022-01-30T17:12:49.080656Z","shell.execute_reply":"2022-01-30T17:12:49.098145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Models","metadata":{}},{"cell_type":"markdown","source":"#### Base","metadata":{}},{"cell_type":"code","source":"class Model(torch.nn.Module):\n\n    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError()\n\n\nclass ModelConfig(t.NamedTuple):\n    name: str\n    model: Model\n    tokenizer: AutoTokenizer\n\n\ndef import_checkpoint(model: torch.nn.Module, checkpoint: str, device: str):\n    state_dict = torch.load(checkpoint, map_location=device)\n    model.load_state_dict(state_dict)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:12:49.100331Z","iopub.execute_input":"2022-01-30T17:12:49.100783Z","iopub.status.idle":"2022-01-30T17:12:49.113491Z","shell.execute_reply.started":"2022-01-30T17:12:49.100746Z","shell.execute_reply":"2022-01-30T17:12:49.112379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### CCC-2017","metadata":{}},{"cell_type":"code","source":"class _AttentionRegressor(torch.nn.Module):\n\n    def __init__(self, in_features: int) -> None:\n        super().__init__()\n        self.attention = torch.nn.Linear(in_features=in_features, out_features=in_features, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        weight = self.attention(x)\n        return (x * torch_f.softmax(weight, dim=1)).sum(dim=1)\n\n\nclass _CCC2017M1Model(Model):\n\n    def __init__(self, checkpoint: str, output_logits: int, num_classes: int):\n        super(Model, self).__init__()\n        self.encoder = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.blind_regressor = torch.nn.Sequential(\n            torch.nn.Linear(output_logits, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 1),\n            torch.nn.Sigmoid())\n        self.classifier = torch.nn.Sequential(\n            # torch.nn.LayerNorm(output_logits),\n            torch.nn.Linear(output_logits, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, num_classes))\n        self.regressor = _AttentionRegressor(in_features=num_classes + 1)\n\n    def forward_scores(self, blind_reg_output: torch.Tensor, label_preds: torch.Tensor) -> torch.Tensor:\n        return self.regressor(torch.cat([torch.sigmoid(label_preds), blind_reg_output], dim=1))\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        _, pooled_output = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask)\n        blind_reg_output = self.blind_regressor(pooled_output)\n        label_preds = self.classifier(pooled_output)\n        scores = self.forward_scores(blind_reg_output, label_preds)\n        return blind_reg_output, label_preds, scores\n\n    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        return self.forward(input_ids, attention_mask)[2]\n\n\ndef load_ccc2017_m1(device: str) -> ModelConfig:\n    model = _CCC2017M1Model('../input/roberta-base', 768, 6)\n    import_checkpoint(model, '../input/jt-models-to-ensemble/ccc-2017-multilabel-v3-cls-att-blind-reg.pt', device=device)\n    return ModelConfig(\n        name='ccc-2017-multilabel-v3-cls-att-blind-reg',\n        model=model,\n        tokenizer=AutoTokenizer.from_pretrained('../input/roberta-base'))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:12:49.11683Z","iopub.execute_input":"2022-01-30T17:12:49.117113Z","iopub.status.idle":"2022-01-30T17:12:49.130776Z","shell.execute_reply.started":"2022-01-30T17:12:49.117076Z","shell.execute_reply":"2022-01-30T17:12:49.130114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class _WeightedAverageLinearRegressor(torch.nn.Linear):\n\n    def __init__(self, in_features: int, device: t.Optional[str] = None, dtype: t.Optional[str] = None):\n        super().__init__(in_features=in_features, out_features=1, bias=False, device=device, dtype=dtype)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return torch_f.linear(x, torch_f.softmax(self.weight, dim=1), self.bias)\n\n\nclass _CCC2017M3Model(Model):\n    \"\"\"\n    ccc-2017-multilabel-harder-cls-loss_0p5-v2-valfreq_dynamic_v1\n    \"\"\"\n\n    def __init__(self, checkpoint: str, output_logits: int, num_classes: int):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.classifier = torch.nn.Sequential(\n            # torch.nn.LayerNorm(output_logits),\n            torch.nn.Linear(output_logits, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, num_classes))\n        self.regressor = _WeightedAverageLinearRegressor(in_features=num_classes)\n\n    def forward_scores(self, label_preds: torch.Tensor) -> torch.Tensor:\n        return self.regressor(label_preds)\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n        _, pooled_output = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask)\n        label_preds = self.classifier(pooled_output)\n        scores = self.forward_scores(torch.sigmoid(label_preds))\n        return label_preds, scores\n\n    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        return self.forward(input_ids, attention_mask)[1]\n\n\ndef load_ccc2017_m3(device: str) -> ModelConfig:\n    model = _CCC2017M3Model('../input/roberta-base', 768, 6)\n    import_checkpoint(model, '../input/jt-models-to-ensemble/ccc-2017-multilabel-harder-cls-loss_0p5-v2-valfreq_dynamic_v1.pt', device=device)\n    return ModelConfig(\n        name='ccc-2017-multilabel-harder-cls-loss_0p5-v2-valfreq_dynamic_v1',\n        model=model,\n        tokenizer=AutoTokenizer.from_pretrained('../input/roberta-base'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ruddit","metadata":{}},{"cell_type":"code","source":"class _RudditM1Model(Model):\n\n    def __init__(self, checkpoint: str, output_logits: int, dropout: float):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.regressor = torch.nn.Sequential(\n            # torch.nn.LayerNorm(output_logits),\n            torch.nn.Linear(output_logits, 1),\n            torch.nn.Sigmoid(),\n        )\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        _, pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask)\n        return self.regressor(pooled_output)\n\n    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        return self.forward(input_ids, attention_mask)\n\n\ndef load_ruddit_m1(device: str) -> ModelConfig:\n    model = _RudditM1Model('../input/roberta-base', 768, 0.6)\n    import_checkpoint(model, '../input/jt-models-to-ensemble/ruddit-v3-mse-2ep-pure_reg.pt', device=device)\n    return ModelConfig(\n        name='ruddit-v3-mse-2ep-pure_reg',\n        model=model,\n        tokenizer=AutoTokenizer.from_pretrained('../input/roberta-base'))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:12:49.147444Z","iopub.execute_input":"2022-01-30T17:12:49.147758Z","iopub.status.idle":"2022-01-30T17:12:49.157793Z","shell.execute_reply.started":"2022-01-30T17:12:49.147726Z","shell.execute_reply":"2022-01-30T17:12:49.157135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class _RudditM2Model(Model):\n\n    def __init__(self, checkpoint: str, output_logits: int, dropout: float):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.regressor = torch.nn.Sequential(\n            # torch.nn.LayerNorm(output_logits),\n            torch.nn.Linear(output_logits, 256),\n            torch.nn.Tanh(),\n            torch.nn.Linear(256, 1),\n            torch.nn.Sigmoid(),\n        )\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        _, pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask)\n        return self.regressor(pooled_output)\n\n    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        return self.forward(input_ids, attention_mask)\n\n\ndef load_ruddit_m2(device: str) -> ModelConfig:\n    model = _RudditM2Model('../input/unbiasedtoxicroberta', 768, 0.6)\n    import_checkpoint(model, '../input/jt-models-to-ensemble/ruddit-v3-mse-2ep-pure_reg-unbiased_toxic_roberta-2layer_reg.pt', device=device)\n    return ModelConfig(\n        name='ruddit-v3-mse-2ep-pure_reg-unbiased_toxic_roberta-2layer_reg',\n        model=model,\n        tokenizer=AutoTokenizer.from_pretrained('../input/unbiasedtoxicroberta'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Wiki Talk Labels","metadata":{}},{"cell_type":"code","source":"class _WikiTalkLabelsM1Model(Model):\n\n    def __init__(self, checkpoint: str, output_logits: int, dropout: float):\n        super(Model, self).__init__()\n        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.regressor = torch.nn.Sequential(\n            torch.nn.Linear(output_logits, 256),\n            torch.nn.Tanh(),\n            torch.nn.Linear(256, 1),\n            torch.nn.Sigmoid(),\n        )\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        _, pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask)\n        return self.regressor(pooled_output)\n\n    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        return self.forward(input_ids, attention_mask)\n\n\ndef load_wiki_talk_labels_m1(device: str) -> ModelConfig:\n    model = _WikiTalkLabelsM1Model('../input/roberta-base', 768, 0.6)\n    import_checkpoint(model, '../input/jt-models-to-ensemble/wiki-talk-labels-v1-1ep.pt', device=device)\n    return ModelConfig(\n        name='wiki-talk-labels-v1-1ep',\n        model=model,\n        tokenizer=AutoTokenizer.from_pretrained('../input/roberta-base'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"def do_prediction_iteration(\n        in_df: pd.DataFrame,\n        batch_size: int,\n        model_getter: t.Callable[[str], ModelConfig],\n        max_len: int,\n        num_workers: int,\n        device: str,) -> np.ndarray:\n    model_config = model_getter(device)\n    model = model_config.model.to(device)\n    dataset = PredictDataset(\n        df=in_df,\n        tokenizer=model_config.tokenizer,\n        max_len=max_len)\n    data_loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=predict_collate_fn,\n        pin_memory=device.startswith('cuda'))\n    model.eval()\n    score_list = []\n    with torch.no_grad():\n        for comment_id_list, tokenized_text, slice_list in tqdm(data_loader, desc='Prediction'):\n            scores_tensor = model.predict_scores(\n                tokenized_text['input_ids'].to(device),\n                tokenized_text['attention_mask'].to(device),)\n            scores_tensor = torch.cat([torch.max(scores_tensor[s], dim=0, keepdim=True)[0] for s in slice_list], dim=0)\n            score_list.extend(scores_tensor.flatten().tolist())\n    return np.array(score_list)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:12:49.159081Z","iopub.execute_input":"2022-01-30T17:12:49.159617Z","iopub.status.idle":"2022-01-30T17:12:49.170573Z","shell.execute_reply.started":"2022-01-30T17:12:49.159564Z","shell.execute_reply":"2022-01-30T17:12:49.169683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:12:49.173492Z","iopub.execute_input":"2022-01-30T17:12:49.174479Z","iopub.status.idle":"2022-01-30T17:12:49.235067Z","shell.execute_reply.started":"2022-01-30T17:12:49.174436Z","shell.execute_reply":"2022-01-30T17:12:49.234298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ccc2017_m1_score_arr = do_prediction_iteration(\n    in_df=eval_df,\n    batch_size=16,\n    model_getter=load_ccc2017_m1,\n    num_workers=2,\n    max_len=256,\n    device='cuda')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:12:49.237401Z","iopub.execute_input":"2022-01-30T17:12:49.237724Z","iopub.status.idle":"2022-01-30T17:14:01.829997Z","shell.execute_reply.started":"2022-01-30T17:12:49.237655Z","shell.execute_reply":"2022-01-30T17:14:01.829186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ccc2017_m3_score_arr = do_prediction_iteration(\n    in_df=eval_df,\n    batch_size=16,\n    model_getter=load_ccc2017_m3,\n    num_workers=2,\n    max_len=256,\n    device='cuda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ruddit_m1_score_arr = do_prediction_iteration(\n    in_df=eval_df,\n    batch_size=16,\n    model_getter=load_ruddit_m1,\n    num_workers=2,\n    max_len=256,\n    device='cuda')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:15:50.891184Z","iopub.execute_input":"2022-01-30T17:15:50.891848Z","iopub.status.idle":"2022-01-30T17:17:04.776049Z","shell.execute_reply.started":"2022-01-30T17:15:50.891809Z","shell.execute_reply":"2022-01-30T17:17:04.775243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ruddit_m2_score_arr = do_prediction_iteration(\n    in_df=eval_df,\n    batch_size=16,\n    model_getter=load_ruddit_m2,\n    num_workers=2,\n    max_len=256,\n    device='cuda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki_talk_labels_m1_score_arr = do_prediction_iteration(\n    in_df=eval_df,\n    batch_size=16,\n    model_getter=load_wiki_talk_labels_m1,\n    num_workers=2,\n    max_len=256,\n    device='cuda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ensemble_score_arr_list(score_arr_list: t.List[np.ndarray]) -> np.ndarray:\n    score_arr_list = [rankdata(score_arr, method='ordinal') for score_arr in score_arr_list]\n    return np.stack(score_arr_list, axis=0).mean(axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_arr = ensemble_score_arr_list([\n    ccc2017_m1_score_arr,\n    ccc2017_m3_score_arr,\n    ruddit_m1_score_arr,\n    ruddit_m2_score_arr,\n    wiki_talk_labels_m1_score_arr,\n])","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:17:21.049627Z","iopub.execute_input":"2022-01-30T17:17:21.049895Z","iopub.status.idle":"2022-01-30T17:17:21.055126Z","shell.execute_reply.started":"2022-01-30T17:17:21.04986Z","shell.execute_reply":"2022-01-30T17:17:21.054389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([\n    {'comment_id': comment_id, 'score': score}\n    for comment_id, score in zip(eval_df['comment_id'].tolist(), score_arr.tolist())\n]).to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:17:23.274612Z","iopub.execute_input":"2022-01-30T17:17:23.275205Z","iopub.status.idle":"2022-01-30T17:17:23.356111Z","shell.execute_reply.started":"2022-01-30T17:17:23.275164Z","shell.execute_reply":"2022-01-30T17:17:23.355441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head -n 4 /kaggle/working/submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-01-30T17:17:44.499158Z","iopub.execute_input":"2022-01-30T17:17:44.499432Z","iopub.status.idle":"2022-01-30T17:17:45.261799Z","shell.execute_reply.started":"2022-01-30T17:17:44.4994Z","shell.execute_reply":"2022-01-30T17:17:45.26095Z"},"trusted":true},"execution_count":null,"outputs":[]}]}