{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Esta es una primer prueba a entender el problema para el equipo de text mining. \n\nConsiderar que todo lo redactado puede tener errores de redaccion, tipograficos, etc. porque le estoy prestando cero atencion a eso. Solo intento desglozar algunas partes del problema.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T04:06:47.095437Z","iopub.execute_input":"2022-05-31T04:06:47.095757Z","iopub.status.idle":"2022-05-31T04:06:47.121015Z","shell.execute_reply.started":"2022-05-31T04:06:47.095724Z","shell.execute_reply":"2022-05-31T04:06:47.119863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una de las cosas mas dificiles de esta competencia es el propio dataset. El cual no existe y tenemos que construirlo nosotros de manera \"creativa\" usando otros data sets anteriores de competencias similares. \n\nA priori parece lo que lo mas inteligente es usar dos datasets (Del segundo no estoy tan convencido aun) Que son  [jigsaw-multilingual-toxic-comment-classification](http://../input/jigsaw-toxic-comment-classification-challenge) y [ruddit jigsaw dataset](http://../input/ruddit-jigsaw-dataset).\n\nA priori voy a usar el primero para crear un dataset y ver si finalmente necesitamos algo del segundo. Super importante entender que como creemos este dataset va a impactar de manera directa en que tan buenas sean finalmente nuestras decisiones. ","metadata":{}},{"cell_type":"code","source":"def count_upper_case_letters(str_obj):\n    count = 0\n    for elem in str_obj:\n        if elem.isupper():\n            count += 1\n    return count","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:06:47.277655Z","iopub.execute_input":"2022-05-31T04:06:47.277959Z","iopub.status.idle":"2022-05-31T04:06:47.283177Z","shell.execute_reply.started":"2022-05-31T04:06:47.277926Z","shell.execute_reply":"2022-05-31T04:06:47.282211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creamos data set**","metadata":{}},{"cell_type":"code","source":"#El data set a usar tiene filas con comentarios y columnas con categorias binarias. \n# Toxico, muy toxico, obseno, amenaza, insulto, discrimacion hacia la identidad, contenido sexual explicito\n\n#ponderador\nponderador_de_toxicidad = {\n    'toxic': 1,\n    'severe_toxic': 2,\n    'obscene': 1,\n    'threat': 1,\n    'insult': 1,\n    'identity_hate': 2,\n    'sexual_explicit': 1\n}\n\ntoxicity_types = list(ponderador_de_toxicidad.keys())\ntoxicity_types","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:06:47.435695Z","iopub.execute_input":"2022-05-31T04:06:47.435989Z","iopub.status.idle":"2022-05-31T04:06:47.445226Z","shell.execute_reply.started":"2022-05-31T04:06:47.435954Z","shell.execute_reply":"2022-05-31T04:06:47.444138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Como paso siguiente deberiamos limpiar el data set**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\n\npd.set_option('display.max_colwidth',300)\n# Importo el dataset\ndf = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv', encoding='utf-8')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:06:47.557293Z","iopub.execute_input":"2022-05-31T04:06:47.557562Z","iopub.status.idle":"2022-05-31T04:06:49.579141Z","shell.execute_reply.started":"2022-05-31T04:06:47.557533Z","shell.execute_reply":"2022-05-31T04:06:49.578177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:06:49.580841Z","iopub.execute_input":"2022-05-31T04:06:49.58108Z","iopub.status.idle":"2022-05-31T04:06:49.596753Z","shell.execute_reply.started":"2022-05-31T04:06:49.581051Z","shell.execute_reply":"2022-05-31T04:06:49.595728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef get_top_n_words(corpus, n=10):\n    cv = CountVectorizer()\n    corpus_matrix = cv.fit_transform(corpus)\n    corpus_matrix = pd.DataFrame.sparse.from_spmatrix(corpus_matrix, columns=cv.get_feature_names())\n    aux = corpus_matrix.sum()\n    aux=aux.to_frame('count')\n    aux.sort_values(by='count',ascending=False,inplace=True)\n    aux=aux.head(n)\n    return aux","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:06:49.598004Z","iopub.execute_input":"2022-05-31T04:06:49.598277Z","iopub.status.idle":"2022-05-31T04:06:49.609966Z","shell.execute_reply.started":"2022-05-31T04:06:49.598242Z","shell.execute_reply":"2022-05-31T04:06:49.608932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cantidad de tokens en todo el corpus\nimport plotly.express as px\n\nmost_importants = get_top_n_words(df.comment_text, n=500)\nprint(most_importants.head(10))\npx.bar(most_importants, x=most_importants.index, y='count')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:06:49.61257Z","iopub.execute_input":"2022-05-31T04:06:49.613133Z","iopub.status.idle":"2022-05-31T04:07:38.151098Z","shell.execute_reply.started":"2022-05-31T04:06:49.613066Z","shell.execute_reply":"2022-05-31T04:07:38.149987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_toxic = df[(df.toxic == 1) | (df.severe_toxic == 1) ]\ndf_toxic","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:07:38.152905Z","iopub.execute_input":"2022-05-31T04:07:38.153265Z","iopub.status.idle":"2022-05-31T04:07:38.188065Z","shell.execute_reply.started":"2022-05-31T04:07:38.153218Z","shell.execute_reply":"2022-05-31T04:07:38.186542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cantidad de tokens en todo el corpus\nmost_importants = get_top_n_words(df_toxic.comment_text, n=500)\nprint(most_importants.head(10))\npx.bar(most_importants, x=most_importants.index, y='count')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:07:38.189738Z","iopub.execute_input":"2022-05-31T04:07:38.190099Z","iopub.status.idle":"2022-05-31T04:07:43.376587Z","shell.execute_reply.started":"2022-05-31T04:07:38.190052Z","shell.execute_reply":"2022-05-31T04:07:43.375203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hasta aquí, hemos descargado el dataset con el que vamos a trabajar y rankeamos las principales palabras, tanto en la totalidad del dataset como en los comentarios que a priori estan definidas cómo tóxicas o super tóxicas.\n\nSe observa que gran parte de las palabras, en ambos dataset (df y df_toxic) no tienen impacto directo sobre la definicion de si el comentario es o no tóxico. En los proximos pasos, eliminaremos las stop_words, con lo cual parte de ellas deberian desaparecer. Sin embargo, habrá palabras que sigan siendo irrelevantes para el caso particuar. Sobre estas podemos realizar un analisis particular.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T23:50:32.943548Z","iopub.execute_input":"2022-05-25T23:50:32.944638Z","iopub.status.idle":"2022-05-25T23:50:32.952908Z","shell.execute_reply.started":"2022-05-25T23:50:32.944576Z","shell.execute_reply":"2022-05-25T23:50:32.951606Z"}}},{"cell_type":"markdown","source":"**Proceso NLP**","metadata":{}},{"cell_type":"markdown","source":"Definimos la función con la que eliminaremos las stop_words. También tenemos la posibilidad de agregar palabras adicionales.","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nstop_words = set( stopwords.words('english'))\nstop_words.update(string.punctuation)\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nfrom nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer('english')\n\n# Pre procesamiento basico, agregar las etapas que considere necesario\ndef pre_procesamiento_texto(text):\n  # Armo los tokens para procesar los datos\n  tokens = word_tokenize(text)\n\n  # Elimino las stopwords\n  tokens = [t.lower() for t in tokens if t.lower() not in stop_words]\n\n  return tokens","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:07:43.378179Z","iopub.execute_input":"2022-05-31T04:07:43.378445Z","iopub.status.idle":"2022-05-31T04:07:43.388974Z","shell.execute_reply.started":"2022-05-31T04:07:43.378415Z","shell.execute_reply":"2022-05-31T04:07:43.387093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:07:43.391992Z","iopub.execute_input":"2022-05-31T04:07:43.392706Z","iopub.status.idle":"2022-05-31T04:07:43.410185Z","shell.execute_reply.started":"2022-05-31T04:07:43.392652Z","shell.execute_reply":"2022-05-31T04:07:43.409453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tengo que crear el dataset limpio. Es decir solo con una columna de \"resultado\" de si es toxico o no para poder seguir probando","metadata":{}},{"cell_type":"code","source":"# Creamos un df1 para diferenciar el primer proceso (regresión) del segundo (clasificacion)\ndf1 = df\n# Combine all toxicity levels into one with the same weights set\ndf1['toxicity'] = sum([df1[type] * coef for type, coef in ponderador_de_toxicidad.items() if type in df1])\n\n# Standardize toxicity (converts to continues values)\ndf1['toxicity'] = df1['toxicity']/df1['toxicity'].max()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:07:43.411717Z","iopub.execute_input":"2022-05-31T04:07:43.41209Z","iopub.status.idle":"2022-05-31T04:07:43.433897Z","shell.execute_reply.started":"2022-05-31T04:07:43.41206Z","shell.execute_reply":"2022-05-31T04:07:43.433275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downsample\ndf1 = pd.concat([\n    df1[df1['toxicity'] <= 0].sample(n = int((df1['toxicity'] > 0).sum() * 1.5),random_state = 201),\n    df1[df1['toxicity'] > 0]\n])\nprint(f\"- New shape: {df1.shape}\")\ndf1","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:07:43.436668Z","iopub.execute_input":"2022-05-31T04:07:43.436965Z","iopub.status.idle":"2022-05-31T04:07:43.512638Z","shell.execute_reply.started":"2022-05-31T04:07:43.436932Z","shell.execute_reply":"2022-05-31T04:07:43.511996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\ndf1[\"rep_char\"] = 0\ndf1[\"rep_cap\"] = 0\ndf1[\"char_len\"] = 0\nfor row in df1.iterrows():\n    string = str(df1.loc[row[0],\"comment_text\"])\n    df1.loc[row[0],\"rep_char\"] = collections.Counter(string).most_common(1)[0][1]/len(string)\n    df1.loc[row[0],\"rep_cap\"] = count_upper_case_letters(string)/len(string)\n    df1.loc[row[0],\"char_len\"]  = len(string)\ndf1.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:07:43.513741Z","iopub.execute_input":"2022-05-31T04:07:43.514364Z","iopub.status.idle":"2022-05-31T04:09:04.809009Z","shell.execute_reply.started":"2022-05-31T04:07:43.514324Z","shell.execute_reply":"2022-05-31T04:09:04.807951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy as sp\n# Construimos una df auxiliar para poder agregar los features \"externos\" al texto en al sparse matrix (matria vectorizada palabra por palabra)\ndf_aux = df1[[\"rep_char\",\"rep_cap\",\"char_len\"]].to_numpy()\n# Vectorizacion de resultados BOW y TF-IDF\ncv = CountVectorizer(tokenizer=pre_procesamiento_texto, max_df=0.2, min_df=0.03)\nX_transform_cv = cv.fit_transform(df1.comment_text) # Armo matriz para entrenar CV\nX_transform_cv = sp.sparse.hstack((X_transform_cv, df_aux)) # Agrego las columnas numericas\n# TF-IDF \ntf = TfidfVectorizer(tokenizer=pre_procesamiento_texto, max_df=0.2, min_df=0.03)\nX_transform_tf = tf.fit_transform(df1.comment_text) # Armo matriz para entrenar tf\nX_transform_tf = sp.sparse.hstack((X_transform_tf, df_aux)) # Agrego las columnas numericas\nprint(X_transform_cv.shape)\nprint(X_transform_tf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:09:04.810534Z","iopub.execute_input":"2022-05-31T04:09:04.810804Z","iopub.status.idle":"2022-05-31T04:11:13.645854Z","shell.execute_reply.started":"2022-05-31T04:09:04.810772Z","shell.execute_reply":"2022-05-31T04:11:13.644489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Separado X,y en train y test, dos veces porque cada proceso de NLP es distinto\n\nX_traincv, X_testcv, y_train, y_test = train_test_split(X_transform_cv, df1.toxicity,test_size=.30)\nX_traintf, X_testtf, y_train, y_test = train_test_split(X_transform_tf, df1.toxicity,test_size=.30)\ny_train = y_train.astype('float32')\ny_test = y_test.astype('float32')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:11:13.647227Z","iopub.execute_input":"2022-05-31T04:11:13.647479Z","iopub.status.idle":"2022-05-31T04:11:13.700386Z","shell.execute_reply.started":"2022-05-31T04:11:13.647449Z","shell.execute_reply":"2022-05-31T04:11:13.699358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_col = cv.get_feature_names()\nlist_col.append(\"rep_char\")\nlist_col.append(\"rep_cap\")\nlist_col.append(\"char_len\")\ndf_transform = pd.DataFrame.sparse.from_spmatrix(X_traincv, columns=list_col)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:11:13.702015Z","iopub.execute_input":"2022-05-31T04:11:13.702323Z","iopub.status.idle":"2022-05-31T04:11:13.717615Z","shell.execute_reply.started":"2022-05-31T04:11:13.702289Z","shell.execute_reply":"2022-05-31T04:11:13.716602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transform.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:11:13.719525Z","iopub.execute_input":"2022-05-31T04:11:13.71976Z","iopub.status.idle":"2022-05-31T04:11:13.783998Z","shell.execute_reply.started":"2022-05-31T04:11:13.719732Z","shell.execute_reply":"2022-05-31T04:11:13.782922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aux = df_transform.drop(['rep_char','rep_cap','char_len'], axis=1)\naux = aux.sum().to_frame('count')\naux.sort_values(by='count',ascending=False,inplace=True)\naux=aux.head(100)\nprint(aux.head(10))\npx.bar(aux, x=aux.index, y='count')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:11:13.785787Z","iopub.execute_input":"2022-05-31T04:11:13.786069Z","iopub.status.idle":"2022-05-31T04:11:13.880641Z","shell.execute_reply.started":"2022-05-31T04:11:13.786036Z","shell.execute_reply":"2022-05-31T04:11:13.878952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MODELOS**","metadata":{}},{"cell_type":"markdown","source":"Se entrenan distintos modelos y para las distintas vectorizaciones planteadas originalmente","metadata":{}},{"cell_type":"code","source":"## CV\n\n# Entreno CV - regresión lineal\nfrom sklearn.linear_model import LinearRegression\nlearner1 = LinearRegression()\nlearner1.fit(X_traincv, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:11:13.882761Z","iopub.execute_input":"2022-05-31T04:11:13.883156Z","iopub.status.idle":"2022-05-31T04:11:14.190804Z","shell.execute_reply.started":"2022-05-31T04:11:13.883102Z","shell.execute_reply":"2022-05-31T04:11:14.189712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## CV\n\n#Entreno CV - Random Forest\nfrom sklearn.ensemble import RandomForestRegressor\nlearner2 = RandomForestRegressor(n_estimators = 100, random_state = 0)\nlearner2.fit(X_traincv, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:11:14.192643Z","iopub.execute_input":"2022-05-31T04:11:14.193315Z","iopub.status.idle":"2022-05-31T04:18:18.46036Z","shell.execute_reply.started":"2022-05-31T04:11:14.193255Z","shell.execute_reply":"2022-05-31T04:18:18.459002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TF\n\n# Entreno TF - Regresion\nfrom sklearn.linear_model import LinearRegression\nlearner3 = LinearRegression()\nlearner3.fit(X_traintf, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:18:18.46185Z","iopub.execute_input":"2022-05-31T04:18:18.462209Z","iopub.status.idle":"2022-05-31T04:18:18.560818Z","shell.execute_reply.started":"2022-05-31T04:18:18.462161Z","shell.execute_reply":"2022-05-31T04:18:18.559861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TF\n\n# Entreno TF - Random Forest\nfrom sklearn.linear_model import LinearRegression\nlearner4 = RandomForestRegressor(n_estimators = 100, random_state = 0)\nlearner4.fit(X_traintf, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:18:18.563278Z","iopub.execute_input":"2022-05-31T04:18:18.564173Z","iopub.status.idle":"2022-05-31T04:24:47.267604Z","shell.execute_reply.started":"2022-05-31T04:18:18.564098Z","shell.execute_reply":"2022-05-31T04:24:47.266575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## CV\n\n#Entreno CV - lightgbm\nimport lightgbm as lgb\nlearner5 = lgb.LGBMRegressor(n_estimators=1000, max_depth=10)\nlearner5.fit(X_traincv.astype('float32'), y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:24:47.269063Z","iopub.execute_input":"2022-05-31T04:24:47.269401Z","iopub.status.idle":"2022-05-31T04:24:49.554247Z","shell.execute_reply.started":"2022-05-31T04:24:47.269356Z","shell.execute_reply":"2022-05-31T04:24:49.553152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TF\n\n#Entreno CV - lightgbm\nimport lightgbm as lgb\nlearner6 = lgb.LGBMRegressor(n_estimators=1000, max_depth=10)\nlearner6.fit(X_traintf, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:24:49.559419Z","iopub.execute_input":"2022-05-31T04:24:49.560248Z","iopub.status.idle":"2022-05-31T04:24:55.453957Z","shell.execute_reply.started":"2022-05-31T04:24:49.56017Z","shell.execute_reply":"2022-05-31T04:24:55.453151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hago predicciones sobre Test\n\n## Para CV Regresion\ny_pred1 = learner1.predict(X_testcv)\n\n## Para CV RF\ny_pred2 = learner2.predict(X_testcv)\n\n## Para TF Regresion\ny_pred3 = learner3.predict(X_testtf)\n\n## Para TF RF\ny_pred4 = learner4.predict(X_testtf)\n\n## Para CV LGBM\ny_pred5 = learner5.predict(X_testcv.astype('float32'))\n\n## Para TF LGBM\ny_pred6 = learner6.predict(X_testtf)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:24:55.455392Z","iopub.execute_input":"2022-05-31T04:24:55.455873Z","iopub.status.idle":"2022-05-31T04:24:58.360448Z","shell.execute_reply.started":"2022-05-31T04:24:55.455819Z","shell.execute_reply":"2022-05-31T04:24:58.359241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RMSE (Root Mean Square Error)\n\nfrom sklearn.metrics import mean_squared_error \n\nrmse1 = float(format(np.sqrt(mean_squared_error(y_test, y_pred1)),'.3f'))\nprint(\"\\nRMSE - CV Reg Mult:\\n\",rmse1)\nrmse2 = float(format(np.sqrt(mean_squared_error(y_test, y_pred2)),'.3f'))\nprint(\"\\nRMSE - CV Random Forest:\\n\",rmse2)\nrmse3 = float(format(np.sqrt(mean_squared_error(y_test, y_pred3)),'.3f'))\nprint(\"\\nRMSE - TF Reg Mult:\\n\",rmse3)\nrmse4 = float(format(np.sqrt(mean_squared_error(y_test, y_pred4)),'.3f'))\nprint(\"\\nRMSE - TF Random Forest:\\n\",rmse4)\nrmse5 = float(format(np.sqrt(mean_squared_error(y_test, y_pred5)),'.3f'))\nprint(\"\\nRMSE - TF LGBM + CV:\\n\",rmse5)\nrmse6 = float(format(np.sqrt(mean_squared_error(y_test, y_pred6)),'.3f'))\nprint(\"\\nRMSE - TF LGBM + TF:\\n\",rmse6)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:24:58.362356Z","iopub.execute_input":"2022-05-31T04:24:58.363088Z","iopub.status.idle":"2022-05-31T04:24:58.382597Z","shell.execute_reply.started":"2022-05-31T04:24:58.363045Z","shell.execute_reply":"2022-05-31T04:24:58.381603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_pred = [[\"1 (CV+RM)\",y_pred1],[\"2 (CV+RF)\",y_pred2],[\"3 (TF+RM)\",y_pred3],[\"4 (TF+RF)\",y_pred4],[\"5 (TF+LGBM)\",y_pred5],[\"6 (CV+LGBM)\",y_pred6]]\nfor idx,i in list_pred:\n    aux1 = pd.DataFrame(i)\n    aux2 = pd.DataFrame(y_test).reset_index()\n    aux3 = aux2\n    aux3['pred'] = aux1\n    ax1 = aux3.plot.scatter(x = 'index', y = 'pred', color='b',title=\"y_test Vs. y_pred\"+idx)\n    ax2 = aux3.plot.scatter(x = 'index', y = 'toxicity', color='y', ax=ax1)\n    print(ax1==ax2) ","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:24:58.385082Z","iopub.execute_input":"2022-05-31T04:24:58.385912Z","iopub.status.idle":"2022-05-31T04:25:00.422909Z","shell.execute_reply.started":"2022-05-31T04:24:58.385862Z","shell.execute_reply":"2022-05-31T04:25:00.421629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**----------------------- SEGUNDA PRUEBA: Categorías de Toxicidad (c/ modelos de clasificación) -----------------------**","metadata":{}},{"cell_type":"code","source":"df2 = df\n# Combine all toxicity levels into one with the same weights set\ndf2['toxicity'] = sum([df2[type] * coef for type, coef in ponderador_de_toxicidad.items() if type in df2])\n\n# Standardize toxicity (converts to continues values)\ndf2['toxicity'] = np.where(df2['toxicity']<4, 0,1)\n                          #np.where(df2['toxicity']<6, 0.5, 1))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:00.424526Z","iopub.execute_input":"2022-05-31T04:25:00.424771Z","iopub.status.idle":"2022-05-31T04:25:00.447047Z","shell.execute_reply.started":"2022-05-31T04:25:00.424742Z","shell.execute_reply":"2022-05-31T04:25:00.44572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downsample\ndf2 = pd.concat([\n    df2[df2['toxicity'] <= 0].sample(n = int((df2['toxicity'] > 0).sum() * 1.5),random_state = 201),\n    df2[df2['toxicity'] > 0]\n])\nprint(f\"- New shape: {df2.shape}\")\ndf2","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:00.4487Z","iopub.execute_input":"2022-05-31T04:25:00.448979Z","iopub.status.idle":"2022-05-31T04:25:00.535334Z","shell.execute_reply.started":"2022-05-31T04:25:00.448946Z","shell.execute_reply":"2022-05-31T04:25:00.534347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\ndf2[\"rep_char\"] = 0\ndf2[\"rep_cap\"] = 0\ndf2[\"char_len\"] = 0\nfor row in df2.iterrows():\n    string = str(df2.loc[row[0],\"comment_text\"])\n    df2.loc[row[0],\"rep_char\"] = collections.Counter(string).most_common(1)[0][1]/len(string)\n    df2.loc[row[0],\"rep_cap\"] = count_upper_case_letters(string)/len(string)\n    df2.loc[row[0],\"char_len\"]  = len(string)\ndf2.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:00.539735Z","iopub.execute_input":"2022-05-31T04:25:00.54001Z","iopub.status.idle":"2022-05-31T04:25:08.874936Z","shell.execute_reply.started":"2022-05-31T04:25:00.539981Z","shell.execute_reply":"2022-05-31T04:25:08.873963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_group = df2[[\"toxicity\", \"rep_char\",\"rep_cap\",\"char_len\"]]\ndf_group = df_group.groupby('toxicity').mean()\ndf_group","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:08.876478Z","iopub.execute_input":"2022-05-31T04:25:08.877059Z","iopub.status.idle":"2022-05-31T04:25:08.898647Z","shell.execute_reply.started":"2022-05-31T04:25:08.87702Z","shell.execute_reply":"2022-05-31T04:25:08.897169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy as sp\n\n# Construimos una df auxiliar para poder agregar los features \"externos\" al texto en al sparse matrix (matria vectorizada palabra por palabra)\ndf_aux = df2[[\"rep_char\",\"rep_cap\",\"char_len\"]].to_numpy()\n\n# Vectorizacion de resultados BOW y TF-IDF\ncv = CountVectorizer(tokenizer=pre_procesamiento_texto, max_df=0.2, min_df=0.03)\nX_transform_cv = cv.fit_transform(df2.comment_text) # Armo matriz para entrenar CV\nX_transform_cv = sp.sparse.hstack((X_transform_cv, df_aux)) # Agrego las columnas numericas\n\n# TF-IDF \ntf = TfidfVectorizer(tokenizer=pre_procesamiento_texto, max_df=0.2, min_df=0.03)\nX_transform_tf = tf.fit_transform(df2.comment_text) # Armo matriz para entrenar tf\nX_transform_tf = sp.sparse.hstack((X_transform_tf, df_aux)) # Agrego las columnas numericas\n\nprint(X_transform_cv.shape)\nprint(X_transform_tf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:08.899914Z","iopub.execute_input":"2022-05-31T04:25:08.90019Z","iopub.status.idle":"2022-05-31T04:25:32.147425Z","shell.execute_reply.started":"2022-05-31T04:25:08.900158Z","shell.execute_reply":"2022-05-31T04:25:32.146267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Separado X,y en train y test, dos veces porque cada proceso de NLP es distinto\n\nX_traincv, X_testcv, y_train, y_test = train_test_split(X_transform_cv, df2.toxicity,test_size=.30)\nX_traintf, X_testtf, y_train, y_test = train_test_split(X_transform_tf, df2.toxicity,test_size=.30)\ny_train = y_train.astype('float32')\ny_test = y_test.astype('float32')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:32.148929Z","iopub.execute_input":"2022-05-31T04:25:32.149446Z","iopub.status.idle":"2022-05-31T04:25:32.165527Z","shell.execute_reply.started":"2022-05-31T04:25:32.149401Z","shell.execute_reply":"2022-05-31T04:25:32.164705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_col = cv.get_feature_names()\nlist_col.append(\"rep_char\")\nlist_col.append(\"rep_cap\")\nlist_col.append(\"char_len\")\ndf_transform = pd.DataFrame.sparse.from_spmatrix(X_traincv, columns=list_col)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:32.167156Z","iopub.execute_input":"2022-05-31T04:25:32.167736Z","iopub.status.idle":"2022-05-31T04:25:32.179334Z","shell.execute_reply.started":"2022-05-31T04:25:32.1677Z","shell.execute_reply":"2022-05-31T04:25:32.178378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transform.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:32.181169Z","iopub.execute_input":"2022-05-31T04:25:32.182Z","iopub.status.idle":"2022-05-31T04:25:32.241633Z","shell.execute_reply.started":"2022-05-31T04:25:32.181952Z","shell.execute_reply":"2022-05-31T04:25:32.240452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANÁLISIS DESCRIPTIVO **","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport string\nstop_words2 = set( stopwords.words('english'))\nstop_words2.add(\"article\")\nstop_words2.add(\"page\")\nstop_words2.add(\"'m\")\nstop_words2.add(\"'ll\")\nstop_words2.add(\"'s\")\nstop_words2.add(\"'re\")\nstop_words2.update(string.punctuation)\n\n# Pre procesamiento basico, agregar las etapas que considere necesario\ndef pre_procesamiento_texto2(text):\n  # Armo los tokens para procesar los datos\n  tokens = word_tokenize(text)\n\n  # Elimino las stopwords\n  tokens = [t.lower() for t in tokens if t.lower() not in stop_words2]\n\n  return tokens\n\n\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# Construimos una df auxiliar para poder agregar los features \"externos\" al texto en al sparse matrix (matria vectorizada palabra por palabra)\ndf_aux2 = df2[[\"toxicity\"]].to_numpy()\ndf2_pos = df2[df2.toxicity == 0]\ndf2_neg = df2[df2.toxicity == 1]\n\n# CV POS\ncv2 = CountVectorizer(tokenizer=pre_procesamiento_texto2, max_df=0.2, min_df=0.03)\nX_transform_cv2 = cv2.fit_transform(df2_pos.comment_text) # Armo matriz para entrenar CV\nX_traincv2, X_testcv2, y_train2, y_test2 = train_test_split(X_transform_cv2, df2_pos.toxicity,test_size=.30)\nlist_col2 = cv2.get_feature_names()\ndf_transform_pos = pd.DataFrame.sparse.from_spmatrix(X_traincv2, columns=list_col2)\n# CV NEG\ncv2 = CountVectorizer(tokenizer=pre_procesamiento_texto2, max_df=0.2, min_df=0.03)\nX_transform_cv2 = cv2.fit_transform(df2_neg.comment_text) # Armo matriz para entrenar CV\nX_traincv2, X_testcv2, y_train2, y_test2 = train_test_split(X_transform_cv2, df2_neg.toxicity,test_size=.30)\nlist_col2 = cv2.get_feature_names()\nprint(list_col2)\ndf_transform_neg = pd.DataFrame.sparse.from_spmatrix(X_traincv2, columns=list_col2)\n\naux_pos = \" \".join(df_transform_pos)\naux_neg = \" \".join(df_transform_neg)\n\n#generating the wordcloud - POS\nwordcloud = WordCloud(background_color=\"white\").generate(aux_pos)\n\n#plot the wordcloud\nplt.figure(figsize = (12, 12))\nplt.imshow(wordcloud)\n\n#to remove the axis value\nplt.axis(\"off\")\nplt.show()\n\n#generating the wordcloud - NEG\nwordcloud = WordCloud(background_color=\"white\").generate(aux_neg)\n\n#plot the wordcloud\nplt.figure(figsize = (12, 12))\nplt.imshow(wordcloud)\n\n#to remove the axis value\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:32.24322Z","iopub.execute_input":"2022-05-31T04:25:32.243441Z","iopub.status.idle":"2022-05-31T04:25:44.330508Z","shell.execute_reply.started":"2022-05-31T04:25:32.243413Z","shell.execute_reply":"2022-05-31T04:25:44.329281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MODELOS**","metadata":{}},{"cell_type":"code","source":"## CV\n\n# Entreno CV - regresión lineal\nfrom sklearn.linear_model import LogisticRegression\nlearner1 = LogisticRegression(solver='lbfgs', max_iter=1000)\nlearner1.fit(X_traincv, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:44.332281Z","iopub.execute_input":"2022-05-31T04:25:44.332894Z","iopub.status.idle":"2022-05-31T04:25:45.485805Z","shell.execute_reply.started":"2022-05-31T04:25:44.332847Z","shell.execute_reply":"2022-05-31T04:25:45.484569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## CV\n\n#Entreno CV - Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nlearner2 = RandomForestClassifier(n_estimators = 100, random_state = 0)\nlearner2.fit(X_traincv, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:45.487635Z","iopub.execute_input":"2022-05-31T04:25:45.487916Z","iopub.status.idle":"2022-05-31T04:25:50.038001Z","shell.execute_reply.started":"2022-05-31T04:25:45.487884Z","shell.execute_reply":"2022-05-31T04:25:50.036724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TF\n\n# Entreno TF - Regresion\nfrom sklearn.linear_model import LogisticRegression\nlearner3 = LogisticRegression(solver='lbfgs', max_iter=1000)\nlearner3.fit(X_traintf, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:50.03975Z","iopub.execute_input":"2022-05-31T04:25:50.040457Z","iopub.status.idle":"2022-05-31T04:25:50.223729Z","shell.execute_reply.started":"2022-05-31T04:25:50.040418Z","shell.execute_reply":"2022-05-31T04:25:50.222861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TF\n\n#Entreno CV - Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nlearner4 = RandomForestClassifier(n_estimators = 100, random_state = 0)\nlearner4.fit(X_traintf, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:50.225158Z","iopub.execute_input":"2022-05-31T04:25:50.225848Z","iopub.status.idle":"2022-05-31T04:25:53.152935Z","shell.execute_reply.started":"2022-05-31T04:25:50.225806Z","shell.execute_reply":"2022-05-31T04:25:53.151986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## CV\n\n#Entreno CV - lightgbm\nimport lightgbm as lgb\nlearner5 = lgb.LGBMClassifier(n_estimators=1000, max_depth=10)\nlearner5.fit(X_traincv.astype('float32'), y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:53.15461Z","iopub.execute_input":"2022-05-31T04:25:53.154855Z","iopub.status.idle":"2022-05-31T04:25:54.524377Z","shell.execute_reply.started":"2022-05-31T04:25:53.154823Z","shell.execute_reply":"2022-05-31T04:25:54.523639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TF\n\n#Entreno CV - lightgbm\nimport lightgbm as lgb\nlearner6 = lgb.LGBMClassifier(n_estimators=1000, max_depth=10)\nlearner6.fit(X_traintf, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:54.525657Z","iopub.execute_input":"2022-05-31T04:25:54.526451Z","iopub.status.idle":"2022-05-31T04:25:57.115623Z","shell.execute_reply.started":"2022-05-31T04:25:54.526399Z","shell.execute_reply":"2022-05-31T04:25:57.114772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hago predicciones sobre Test\n\n## Para CV Regresion\ny_pred1 = learner1.predict(X_testcv)\n\n## Para CV RF\ny_pred2 = learner2.predict(X_testcv)\n\n## Para TF Regresion\ny_pred3 = learner3.predict(X_testtf)\n\n## Para TF RF\ny_pred4 = learner4.predict(X_testtf)\n\n## Para CV LGBM\ny_pred5 = learner5.predict(X_testcv.astype('float32'))\n\n## Para TF LGBM\ny_pred6 = learner6.predict(X_testtf)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:57.116699Z","iopub.execute_input":"2022-05-31T04:25:57.117336Z","iopub.status.idle":"2022-05-31T04:25:57.531909Z","shell.execute_reply.started":"2022-05-31T04:25:57.117297Z","shell.execute_reply":"2022-05-31T04:25:57.531193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import classification_report\nlist_pred = [[\"1 (CV+RM)\",y_pred1],[\"2 (CV+RF)\",y_pred2],[\"3 (TF+RM)\",y_pred3],[\"4 (TF+RF)\",y_pred4],[\"5 (CV+LGBM)\",y_pred5],[\"6 (TF+LGBM)\",y_pred6]]\nfor idx,i in list_pred:\n    aux1 = pd.DataFrame(i)\n    aux2 = pd.DataFrame(y_test).reset_index()\n    aux3 = aux2\n    aux3['pred'] = aux1\n    conf_mat_aux = confusion_matrix(aux3['toxicity'], aux3['pred'])\n    conf_mat = ConfusionMatrixDisplay(confusion_matrix=conf_mat_aux)\n    target_names = ['No Tóxico (0)', 'Tóxico (1)']\n    clas_sum = classification_report(aux3['toxicity'], aux3['pred'], target_names=target_names)\n    conf_mat.plot()\n    plt.title(\"y_test Vs. y_pred\"+idx)\n    plt.show()\n    print(clas_sum)\n    print(\"Precision = True Positive / (True Positive + False Positive) - (Or Negative)\")\n    print(\"Recall = True Positive / (Positive) - (Or Negative)\")\n    print(\"Deberíamos aspirar a un precision y recall altos en la categoría tóxico (1)\")\n    print(\"\")","metadata":{"execution":{"iopub.status.busy":"2022-05-31T04:25:57.533288Z","iopub.execute_input":"2022-05-31T04:25:57.533771Z","iopub.status.idle":"2022-05-31T04:25:58.97261Z","shell.execute_reply.started":"2022-05-31T04:25:57.533711Z","shell.execute_reply":"2022-05-31T04:25:58.971645Z"},"trusted":true},"execution_count":null,"outputs":[]}]}