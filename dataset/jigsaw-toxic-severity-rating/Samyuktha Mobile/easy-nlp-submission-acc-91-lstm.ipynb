{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-24T12:45:37.447694Z","iopub.execute_input":"2021-11-24T12:45:37.44814Z","iopub.status.idle":"2021-11-24T12:45:37.458585Z","shell.execute_reply.started":"2021-11-24T12:45:37.448093Z","shell.execute_reply":"2021-11-24T12:45:37.457668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n## Train set\ntr = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ntr.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:45:37.460235Z","iopub.execute_input":"2021-11-24T12:45:37.461039Z","iopub.status.idle":"2021-11-24T12:45:37.590413Z","shell.execute_reply.started":"2021-11-24T12:45:37.46099Z","shell.execute_reply":"2021-11-24T12:45:37.589653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Validation data set\nval = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv')\nval.head(5)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-24T12:45:37.591593Z","iopub.execute_input":"2021-11-24T12:45:37.592129Z","iopub.status.idle":"2021-11-24T12:45:38.199475Z","shell.execute_reply.started":"2021-11-24T12:45:37.592091Z","shell.execute_reply":"2021-11-24T12:45:38.198794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Sample submission data set\nsub = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv')\nsub.head(5)\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-24T12:45:38.20095Z","iopub.execute_input":"2021-11-24T12:45:38.201201Z","iopub.status.idle":"2021-11-24T12:45:38.220166Z","shell.execute_reply.started":"2021-11-24T12:45:38.201168Z","shell.execute_reply":"2021-11-24T12:45:38.219499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data cleaning","metadata":{}},{"cell_type":"code","source":"import re\nimport string\ndef clean(d):\n    ## lowercase the reviews\n    d = d.apply(lambda x:x.lower())\n    ## remove punctuation marks\n    d = d.apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n    # Removing extra spaces\n    d = d.apply(lambda x: re.sub(' +',' ',x))\n    ## Remove line breaks\n    d = d.replace('\\n',' ').replace('\\r',' ').replace('...',' ')\n    # Remove special characters\n    d = d.apply(lambda x:re.sub('[^a-zA-z0-9\\s]','',x))\n\n    ## Look at the text after cleaning \n    d.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:45:38.222598Z","iopub.execute_input":"2021-11-24T12:45:38.222945Z","iopub.status.idle":"2021-11-24T12:45:38.231278Z","shell.execute_reply.started":"2021-11-24T12:45:38.222909Z","shell.execute_reply":"2021-11-24T12:45:38.230493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean(tr['text'])","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:45:38.235826Z","iopub.execute_input":"2021-11-24T12:45:38.236273Z","iopub.status.idle":"2021-11-24T12:45:38.659174Z","shell.execute_reply.started":"2021-11-24T12:45:38.236234Z","shell.execute_reply":"2021-11-24T12:45:38.658424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"Introducing the feature Polarity to score toxicity of comments.\n\nSentiment analysis is the analysis of how much a piece of text is positive and opinionated.\n","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\ntr['polarity'] = tr['text'].apply(lambda x: round(TextBlob(x).sentiment.polarity),2)\nprint(\" 3 Comments which are positive (highest polarity)\")\nfor index, t in enumerate(tr.iloc[tr['polarity'].sort_values(ascending = True)[:3].index]['text']):\n    print(index+1,t,'\\n')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:45:38.660347Z","iopub.execute_input":"2021-11-24T12:45:38.660619Z","iopub.status.idle":"2021-11-24T12:45:44.720006Z","shell.execute_reply.started":"2021-11-24T12:45:38.660574Z","shell.execute_reply":"2021-11-24T12:45:44.719196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:45:44.721867Z","iopub.execute_input":"2021-11-24T12:45:44.722266Z","iopub.status.idle":"2021-11-24T12:45:44.732601Z","shell.execute_reply.started":"2021-11-24T12:45:44.722226Z","shell.execute_reply":"2021-11-24T12:45:44.731556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr['polarity'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:45:44.734198Z","iopub.execute_input":"2021-11-24T12:45:44.734724Z","iopub.status.idle":"2021-11-24T12:45:44.742775Z","shell.execute_reply.started":"2021-11-24T12:45:44.734685Z","shell.execute_reply":"2021-11-24T12:45:44.742094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = tr.drop('polarity',axis = 1)\ny = tr['polarity'].values\n\ntexts = x.drop('comment_id',axis = 1).copy()\ntexts.reset_index(inplace = True, drop = True)\ntexts.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:45:44.744205Z","iopub.execute_input":"2021-11-24T12:45:44.744662Z","iopub.status.idle":"2021-11-24T12:45:44.758066Z","shell.execute_reply.started":"2021-11-24T12:45:44.744628Z","shell.execute_reply":"2021-11-24T12:45:44.757383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning Text\n\n* Stemming: \n\n        Extract the root element of a word.\n\n        Ex: raining, rained, had rained \n\n        Stem word: rain\n        \n\n* Stop words removal\n\n        Stopwords are the most commonly occuring  words in the text carrying no meaning. Ex: 'I', 'This','on','there','here','is','in'.\n\n        We will use nltk library to remove stopwords from the cleaned train set.","metadata":{}},{"cell_type":"code","source":"## Create a clean text corpus \n##containing lower case words + no stopwords + stem words\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nimport tqdm\n\nps = PorterStemmer()\ncorpus = []\n\nfor i in range(0, len(texts)) :\n    cleaned = re.sub('[^a-zA-Z]', ' ', texts['text'][i])\n    cleaned = cleaned.lower().split()\n    \n    cleaned = [ps.stem(word) for word in cleaned if not word in stopwords.words('english')]\n    cleaned = ' '.join(cleaned)\n    corpus.append(cleaned)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:45:44.759471Z","iopub.execute_input":"2021-11-24T12:45:44.759879Z","iopub.status.idle":"2021-11-24T12:46:57.38596Z","shell.execute_reply.started":"2021-11-24T12:45:44.759844Z","shell.execute_reply":"2021-11-24T12:46:57.385192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word2Vec\n\nConvert cleaned text into numbers using gensim","metadata":{}},{"cell_type":"code","source":"import gensim\n\nDIM = 100\n\nX = [d.split() for d in corpus]\nw2v_model = gensim.models.Word2Vec(sentences = X, vector_size = DIM, window = 10, min_count = 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:46:57.387391Z","iopub.execute_input":"2021-11-24T12:46:57.38765Z","iopub.status.idle":"2021-11-24T12:47:00.584204Z","shell.execute_reply.started":"2021-11-24T12:46:57.387617Z","shell.execute_reply":"2021-11-24T12:47:00.583458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Tokenization\n\nTokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n\nWe can pad the data to have same length","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nt = Tokenizer()\nt.fit_on_texts(X)\nX = t.texts_to_sequences(X)\nX = pad_sequences(X, maxlen = 16)\nprint('Text tokens count ',len(t.word_index))\n\nvocab_size = len(t.word_index) + 1 \nvocab = t.word_index","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:47:00.587435Z","iopub.execute_input":"2021-11-24T12:47:00.58774Z","iopub.status.idle":"2021-11-24T12:47:05.328659Z","shell.execute_reply.started":"2021-11-24T12:47:00.587712Z","shell.execute_reply":"2021-11-24T12:47:05.327885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[:3]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:47:05.329805Z","iopub.execute_input":"2021-11-24T12:47:05.3309Z","iopub.status.idle":"2021-11-24T12:47:05.340984Z","shell.execute_reply.started":"2021-11-24T12:47:05.33086Z","shell.execute_reply":"2021-11-24T12:47:05.339608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_weights_matrix(model) :\n    weights_matrix = np.zeros((vocab_size, DIM))\n    \n    for word, i in vocab.items() :\n        weights_matrix[i] = model.wv[word]\n        \n    return weights_matrix\n\n\nembedding_vectors = get_weights_matrix(w2v_model) ","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:47:05.342075Z","iopub.execute_input":"2021-11-24T12:47:05.342386Z","iopub.status.idle":"2021-11-24T12:47:05.42096Z","shell.execute_reply.started":"2021-11-24T12:47:05.342351Z","shell.execute_reply":"2021-11-24T12:47:05.420254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the deep learning model\n\nAn lstm model can remember, learn and memorise sequences of padded vectors.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\nmodel = Sequential([\n    Embedding(vocab_size, output_dim =DIM,weights = [embedding_vectors],input_length = 16,name='embedding'),\n    Dropout(0.2),\n    LSTM(64),\n    Dropout(0.2),\n    Dense(64,activation = 'relu'),\n    Dropout(0.2),\n    Dense(1,activation = 'sigmoid')\n])\nmodel.compile(optimizer='adam',loss= 'mean_squared_error',\n             metrics = ['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:47:05.422823Z","iopub.execute_input":"2021-11-24T12:47:05.423319Z","iopub.status.idle":"2021-11-24T12:47:08.264389Z","shell.execute_reply.started":"2021-11-24T12:47:05.423283Z","shell.execute_reply":"2021-11-24T12:47:08.263634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\nplot_model(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:47:08.265922Z","iopub.execute_input":"2021-11-24T12:47:08.266184Z","iopub.status.idle":"2021-11-24T12:47:09.087898Z","shell.execute_reply.started":"2021-11-24T12:47:08.26615Z","shell.execute_reply":"2021-11-24T12:47:09.087035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model\n\n","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size =0.3,random_state = 0)\n\nes = EarlyStopping(patience=3, \n                   monitor='loss', \n                   #restore_best_weights=True, \n                   mode='min', \n                   verbose=1)\n# model train\nhistory = model.fit(xtrain,ytrain,\n                    validation_data = (xtest, ytest)\n                    ,batch_size= 100, \n                    epochs= 1,\n                    validation_split=0.1,\n                    callbacks=[es],\n                    shuffle=True,\n                    )\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:47:09.089525Z","iopub.execute_input":"2021-11-24T12:47:09.089925Z","iopub.status.idle":"2021-11-24T12:47:18.589839Z","shell.execute_reply.started":"2021-11-24T12:47:09.08988Z","shell.execute_reply":"2021-11-24T12:47:18.589166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\npreds = model.predict(X)*1000\nplt.hist(preds,label='Model Prediction')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:47:18.591196Z","iopub.execute_input":"2021-11-24T12:47:18.591439Z","iopub.status.idle":"2021-11-24T12:47:19.513284Z","shell.execute_reply.started":"2021-11-24T12:47:18.591404Z","shell.execute_reply":"2021-11-24T12:47:19.51257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import binarize\nu = pd.DataFrame()\nu['comment_id'] = tr['comment_id']\nu['score'] = binarize(preds)\nu.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T13:08:57.025228Z","iopub.execute_input":"2021-11-24T13:08:57.025502Z","iopub.status.idle":"2021-11-24T13:08:57.055361Z","shell.execute_reply.started":"2021-11-24T13:08:57.025473Z","shell.execute_reply":"2021-11-24T13:08:57.054679Z"},"trusted":true},"execution_count":null,"outputs":[]}]}