{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### What are you trying to do in this notebook?\nThis notebook is to visualise the text data to see and identify some patterns in the text data which might help us in differentiating between less_toxic and more_toxic comments. This notebook attempts to perform EDA on the Jiggsaw Toxic Severity Rating dataset. The focus in this competition is on ranking the severity of comment toxicity from innocuous to outrageous.\n\n#### Why are you trying it?\nIn this competition you will be ranking comments in order of severity of toxicity. You are given a list of comments, and each comment should be scored according to their relative toxicity. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity. In order to avoid leaks, the same text needs to be put into same Folds. For a single document this is easy, but for a pair of documents to both be in same folds is a bit tricky. This simple notebook tracks pairs of text recursively to group them and try to create a leak-free Fold split.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-01-05T07:59:50.647107Z","iopub.execute_input":"2022-01-05T07:59:50.647443Z","iopub.status.idle":"2022-01-05T07:59:50.67804Z","shell.execute_reply.started":"2022-01-05T07:59:50.647338Z","shell.execute_reply":"2022-01-05T07:59:50.677218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import text\nimport re\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 150)\nimport os\nimport gc\ngc.enable()\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom scipy import stats\n%matplotlib inline\nfrom datetime import timedelta\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport warnings\nwarnings.filterwarnings('ignore')\nimport urllib        #for url stuff\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\nfrom IPython.display import display\nfrom tqdm import tqdm\nfrom collections import Counter\nimport ast\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\nimport seaborn as sns #for making plots\nimport matplotlib.pyplot as plt # for plotting\nimport os  # for os commands\n\nimport gensim\nfrom gensim import corpora, models, similarities\nimport logging\nimport tempfile\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom collections import OrderedDict\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:00:23.092311Z","iopub.execute_input":"2022-01-05T08:00:23.09319Z","iopub.status.idle":"2022-01-05T08:00:25.748196Z","shell.execute_reply.started":"2022-01-05T08:00:23.093102Z","shell.execute_reply":"2022-01-05T08:00:25.746991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv')\ntest = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv')\nsubm = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:00:51.259022Z","iopub.execute_input":"2022-01-05T08:00:51.259338Z","iopub.status.idle":"2022-01-05T08:00:51.867812Z","shell.execute_reply.started":"2022-01-05T08:00:51.259306Z","shell.execute_reply":"2022-01-05T08:00:51.866845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train))\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:01:13.394496Z","iopub.execute_input":"2022-01-05T08:01:13.394831Z","iopub.status.idle":"2022-01-05T08:01:13.420233Z","shell.execute_reply.started":"2022-01-05T08:01:13.394793Z","shell.execute_reply":"2022-01-05T08:01:13.419351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words=list(text.ENGLISH_STOP_WORDS.union([\"book\"]))\nhtml_tags = ['<P>', '</P>', '<Table>', '</Table>', '<Tr>', '</Tr>', '<Ul>', '<Ol>', '<Dl>', '</Ul>', '</Ol>', \\\n             '</Dl>', '<Li>', '<Dd>', '<Dt>', '</Li>', '</Dd>', '</Dt>', '/n', '\\n']\nr_buf = ['It', 'is', 'are', 'do', 'does', 'did', 'was', 'were', 'will', 'can', 'the', 'a', 'of', 'in', 'and', 'on', \\\n         'what', 'where', 'when', 'which'] + html_tags\nto_remove=list(set(stop_words+html_tags+r_buf))\n\ndef clean(x):\n    x = x.lower()\n    for r in to_remove:\n        x = x.replace(r, '')\n    x = re.sub(' +', ' ', x)\n    return x\n\n\ncomments=[str(clean(x)) for x in train['less_toxic'].tolist()+train['more_toxic'].tolist()]\nlabels=[0]*30108 + [1]*30108\ntrain = pd.DataFrame({'comments':comments, 'labels':labels})\n\ncomments=[str(clean(x)) for x in test['text'].tolist()]\ntest['comments']=comments","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:01:29.634836Z","iopub.execute_input":"2022-01-05T08:01:29.635148Z","iopub.status.idle":"2022-01-05T08:01:41.722073Z","shell.execute_reply.started":"2022-01-05T08:01:29.635101Z","shell.execute_reply":"2022-01-05T08:01:41.72111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:01:56.573452Z","iopub.execute_input":"2022-01-05T08:01:56.573777Z","iopub.status.idle":"2022-01-05T08:01:56.592426Z","shell.execute_reply.started":"2022-01-05T08:01:56.573745Z","shell.execute_reply":"2022-01-05T08:01:56.591419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:02:10.091529Z","iopub.execute_input":"2022-01-05T08:02:10.091973Z","iopub.status.idle":"2022-01-05T08:02:10.111171Z","shell.execute_reply.started":"2022-01-05T08:02:10.091907Z","shell.execute_reply":"2022-01-05T08:02:10.109792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n\nn = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(2,4), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train['comments'])\ntest_term_doc = vec.transform(test['comments'])","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:02:29.278176Z","iopub.execute_input":"2022-01-05T08:02:29.278505Z","iopub.status.idle":"2022-01-05T08:02:57.394288Z","shell.execute_reply.started":"2022-01-05T08:02:29.278463Z","shell.execute_reply":"2022-01-05T08:02:57.393335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_term_doc, test_term_doc\n\ndef pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)\n\n\nx = trn_term_doc\ntest_x = test_term_doc\n\n\ndef get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, solver='liblinear', dual=False)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r\n\npreds = np.zeros((len(test), 1))\n\nfor i, j in enumerate(['labels']):\n    print('fit', j)\n    m,r = get_mdl(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n    \npreds=[int(p >= 0.6) for p in preds]","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:03:03.96436Z","iopub.execute_input":"2022-01-05T08:03:03.964938Z","iopub.status.idle":"2022-01-05T08:03:15.777546Z","shell.execute_reply.started":"2022-01-05T08:03:03.964902Z","shell.execute_reply":"2022-01-05T08:03:15.776444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'comment_id': subm[\"comment_id\"], 'score': preds})","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:03:21.563188Z","iopub.execute_input":"2022-01-05T08:03:21.563644Z","iopub.status.idle":"2022-01-05T08:03:21.573967Z","shell.execute_reply.started":"2022-01-05T08:03:21.563464Z","shell.execute_reply":"2022-01-05T08:03:21.572878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:03:36.065359Z","iopub.execute_input":"2022-01-05T08:03:36.06571Z","iopub.status.idle":"2022-01-05T08:03:36.07567Z","shell.execute_reply.started":"2022-01-05T08:03:36.065678Z","shell.execute_reply":"2022-01-05T08:03:36.07467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-05T08:03:48.792863Z","iopub.execute_input":"2022-01-05T08:03:48.793195Z","iopub.status.idle":"2022-01-05T08:03:48.818868Z","shell.execute_reply.started":"2022-01-05T08:03:48.793164Z","shell.execute_reply":"2022-01-05T08:03:48.817936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Did it work?\nThere is no training data for this competition. You can refer to previous Jigsaw competitions for data that might be useful to train models. But note that the task of previous competitions has been to predict the probability that a comment was toxic, rather than the degree or severity of a comment's toxicity.\n\n#### What did you not understand about this process?\nWell, everything provides in the competition data page. I've no problem while working on it. If you guys don't understand the thing that I'll do in this notebook then please comment on this notebook.\n\n#### What else do you think you can try as part of this approach?\nWhile we don't include training data, we do provide a set of paired toxicity rankings that can be used to validate models.","metadata":{}}]}