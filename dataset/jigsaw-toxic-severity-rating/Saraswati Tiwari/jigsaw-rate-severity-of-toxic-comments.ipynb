{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### What are you trying to do in this notebook?\nThis notebook is to visualise the text data to see and identify some patterns in the text data which might help us in differentiating between less_toxic and more_toxic comments.\nThis notebook attempts to perform EDA on the Jiggsaw Toxic Severity Rating dataset. The focus in this competition is on ranking the severity of comment toxicity from innocuous to outrageous.\n\n#### Why are you trying it?\nIn this competition you will be ranking comments in order of severity of toxicity. You are given a list of comments, and each comment should be scored according to their relative toxicity. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity.\nIn order to avoid leaks, the same text needs to be put into same Folds.\nFor a single document this is easy, but for a pair of documents to both be in same folds is a bit tricky.\nThis simple notebook tracks pairs of text recursively to group them and try to create a leak-free Fold split.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:43:57.144445Z","iopub.execute_input":"2021-12-08T13:43:57.144783Z","iopub.status.idle":"2021-12-08T13:43:57.155166Z","shell.execute_reply.started":"2021-12-08T13:43:57.144752Z","shell.execute_reply":"2021-12-08T13:43:57.153813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:43:57.157097Z","iopub.execute_input":"2021-12-08T13:43:57.157513Z","iopub.status.idle":"2021-12-08T13:43:57.16468Z","shell.execute_reply.started":"2021-12-08T13:43:57.157477Z","shell.execute_reply":"2021-12-08T13:43:57.163953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits=5\nnrows = None","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:43:57.166069Z","iopub.execute_input":"2021-12-08T13:43:57.16653Z","iopub.status.idle":"2021-12-08T13:43:57.173327Z","shell.execute_reply.started":"2021-12-08T13:43:57.166495Z","shell.execute_reply":"2021-12-08T13:43:57.172586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\", nrows=nrows)\ntexts = set(df.less_toxic.to_list() + df.more_toxic.to_list())\ntext2id = {t:id for id,t in enumerate(texts)}\ndf['less_id'] = df['less_toxic'].map(text2id)\ndf['more_id'] = df['more_toxic'].map(text2id)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:43:57.174649Z","iopub.execute_input":"2021-12-08T13:43:57.175004Z","iopub.status.idle":"2021-12-08T13:43:57.444097Z","shell.execute_reply.started":"2021-12-08T13:43:57.174972Z","shell.execute_reply":"2021-12-08T13:43:57.443376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set array to store pair information\nlen_ids = len(text2id)\nidarr = np.zeros((len_ids,len_ids), dtype=bool)\n\nfor lid, mid in df[['less_id', 'more_id']].values:\n    min_id = min(lid, mid)\n    max_id = max(lid, mid)\n    idarr[max_id, min_id] = True","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:43:57.446467Z","iopub.execute_input":"2021-12-08T13:43:57.447004Z","iopub.status.idle":"2021-12-08T13:43:57.564942Z","shell.execute_reply.started":"2021-12-08T13:43:57.446963Z","shell.execute_reply":"2021-12-08T13:43:57.564203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recursively retrieve the text that is paired with the text whose id is i,\n# and store it's id in this_list.\n# then set idarr[i, j] to False\ndef add_ids(i, this_list):\n    for j in range(len_ids):\n        if idarr[i, j]:\n            idarr[i, j] = False\n            this_list.append(j)\n            this_list = add_ids(j,this_list)\n            #print(j,i)\n    for j in range(i+1,len_ids):\n        if idarr[j, i]:\n            idarr[j, i] = False\n            this_list.append(j)\n            this_list = add_ids(j,this_list)\n            #print(j,i)\n    return this_list\n\ngroup_list = []\nfor i in tqdm(range(len_ids)):\n    for j in range(i+1,len_ids):\n        if idarr[j, i]:\n            this_list = add_ids(i,[i])\n            #print(this_list)\n            group_list.append(this_list)\n\nid2groupid = {}\nfor gid,ids in enumerate(group_list):\n    for id in ids:\n        id2groupid[id] = gid\n\ndf['less_gid'] = df['less_id'].map(id2groupid)\ndf['more_gid'] = df['more_id'].map(id2groupid)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:43:57.566414Z","iopub.execute_input":"2021-12-08T13:43:57.566669Z","iopub.status.idle":"2021-12-08T13:45:26.993494Z","shell.execute_reply.started":"2021-12-08T13:43:57.566626Z","shell.execute_reply":"2021-12-08T13:45:26.992643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('unique text counts:', len_ids)\nprint('grouped text counts:', len(group_list))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:45:26.994852Z","iopub.execute_input":"2021-12-08T13:45:26.995202Z","iopub.status.idle":"2021-12-08T13:45:27.001439Z","shell.execute_reply.started":"2021-12-08T13:45:26.995167Z","shell.execute_reply":"2021-12-08T13:45:27.000673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we can use GroupKFold with group id\ngroup_kfold = GroupKFold(n_splits=n_splits)\n\n# Since df.less_gid and df.more_gid are the same, let's use df.less_gid here.\nfor fold, (trn, val) in enumerate(group_kfold.split(df, df, df.less_gid)): \n    df.loc[val , \"fold\"] = fold\n\ndf[\"fold\"] = df[\"fold\"].astype(int)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:45:27.002933Z","iopub.execute_input":"2021-12-08T13:45:27.003533Z","iopub.status.idle":"2021-12-08T13:45:27.05133Z","shell.execute_reply.started":"2021-12-08T13:45:27.003498Z","shell.execute_reply":"2021-12-08T13:45:27.050561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Did it work?\nThere is no training data for this competition. You can refer to previous Jigsaw competitions for data that might be useful to train models. But note that the task of previous competitions has been to predict the probability that a comment was toxic, rather than the degree or severity of a comment's toxicity.\n\n#### What did you not understand about this process?\nWell, everything provides in the competition data page. I've no problem while working on it. If you guys don't understand the thing that I'll do in this notebook then please comment on this notebook.\n\n#### What else do you think you can try as part of this approach?\nWhile we don't include training data, we do provide a set of paired toxicity rankings that can be used to validate models.\n\n","metadata":{}}]}