{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-08T12:29:28.745417Z","iopub.execute_input":"2021-12-08T12:29:28.746352Z","iopub.status.idle":"2021-12-08T12:29:28.7795Z","shell.execute_reply.started":"2021-12-08T12:29:28.746217Z","shell.execute_reply":"2021-12-08T12:29:28.778866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check this notebook for getting started\nhttps://www.kaggle.com/kaushikholla/0-752-eda-baseline-model-svm\n\n\n\nIn the above notebook, I have approached the problem as a classification problem and in this notebook I will look at this problem as regression problem and see how the model performs.","metadata":{}},{"cell_type":"code","source":"## Importing the core libraries\nimport pandas as pd\nimport numpy as np\nimport scipy\n\n## Sklearn packages for modelling\nfrom sklearn.linear_model import Ridge\n# from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-12-08T12:29:28.780811Z","iopub.execute_input":"2021-12-08T12:29:28.781135Z","iopub.status.idle":"2021-12-08T12:29:29.763421Z","shell.execute_reply.started":"2021-12-08T12:29:28.7811Z","shell.execute_reply":"2021-12-08T12:29:29.762544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Changing the setting to display entire column\npd.set_option('display.max_columns', None)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T12:29:29.764936Z","iopub.execute_input":"2021-12-08T12:29:29.765204Z","iopub.status.idle":"2021-12-08T12:29:29.76906Z","shell.execute_reply.started":"2021-12-08T12:29:29.765171Z","shell.execute_reply":"2021-12-08T12:29:29.768254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Note:\nIn order to make prediction on the given dataset, we will train our model on dataset from previous jigsaw competation and use that model to score the sentences given in this challenge.","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:00:52.038329Z","iopub.execute_input":"2021-12-08T13:00:52.038629Z","iopub.status.idle":"2021-12-08T13:00:53.061835Z","shell.execute_reply.started":"2021-12-08T13:00:52.038594Z","shell.execute_reply":"2021-12-08T13:00:53.061046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Getting rid of all other columns except out text and y\n\n## taking a look at first 5 columns\n","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:02:41.940919Z","iopub.execute_input":"2021-12-08T13:02:41.94121Z","iopub.status.idle":"2021-12-08T13:02:41.951462Z","shell.execute_reply.started":"2021-12-08T13:02:41.941175Z","shell.execute_reply":"2021-12-08T13:02:41.950539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:03:26.927702Z","iopub.execute_input":"2021-12-08T13:03:26.927957Z","iopub.status.idle":"2021-12-08T13:03:26.937937Z","shell.execute_reply.started":"2021-12-08T13:03:26.92793Z","shell.execute_reply":"2021-12-08T13:03:26.937057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare=pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:07:52.654109Z","iopub.execute_input":"2021-12-08T13:07:52.654926Z","iopub.status.idle":"2021-12-08T13:07:52.911675Z","shell.execute_reply.started":"2021-12-08T13:07:52.654887Z","shell.execute_reply":"2021-12-08T13:07:52.91074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less=compare['less_toxic'].value_counts()\nless=less.reset_index()\nless=less[less['less_toxic']>1]\nless['less_toxic']=-less['less_toxic']\nless.columns=['index', 'rank']","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:07:52.912959Z","iopub.execute_input":"2021-12-08T13:07:52.913174Z","iopub.status.idle":"2021-12-08T13:07:52.932684Z","shell.execute_reply.started":"2021-12-08T13:07:52.913149Z","shell.execute_reply":"2021-12-08T13:07:52.931855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"more=compare['more_toxic'].value_counts()\nmore=more.reset_index()\nmore=more[more['more_toxic']>1]\nmore.columns=['index', 'rank']","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:07:53.213015Z","iopub.execute_input":"2021-12-08T13:07:53.213282Z","iopub.status.idle":"2021-12-08T13:07:53.231448Z","shell.execute_reply.started":"2021-12-08T13:07:53.213251Z","shell.execute_reply":"2021-12-08T13:07:53.230617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=less.append(more)\ndf.columns=['comment_text', 'y']\ndf=df.groupby('comment_text').agg('sum')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:07:53.637341Z","iopub.execute_input":"2021-12-08T13:07:53.638077Z","iopub.status.idle":"2021-12-08T13:07:53.687907Z","shell.execute_reply.started":"2021-12-08T13:07:53.638033Z","shell.execute_reply":"2021-12-08T13:07:53.686942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"df.sort_values('y',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T12:47:52.95545Z","iopub.execute_input":"2021-12-08T12:47:52.955807Z","iopub.status.idle":"2021-12-08T12:47:52.969353Z","shell.execute_reply.started":"2021-12-08T12:47:52.955771Z","shell.execute_reply":"2021-12-08T12:47:52.968398Z"},"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"df=df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:07:56.366053Z","iopub.execute_input":"2021-12-08T13:07:56.366634Z","iopub.status.idle":"2021-12-08T13:07:56.372093Z","shell.execute_reply.started":"2021-12-08T13:07:56.366589Z","shell.execute_reply":"2021-12-08T13:07:56.371498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Getting rid of all other columns except out text and y\ndf = df[['comment_text', 'y']]\n## taking a look at first 5 columns\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:07:59.562726Z","iopub.execute_input":"2021-12-08T13:07:59.563124Z","iopub.status.idle":"2021-12-08T13:07:59.573681Z","shell.execute_reply.started":"2021-12-08T13:07:59.563094Z","shell.execute_reply":"2021-12-08T13:07:59.572879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nscaler = MinMaxScaler()\ndf['y']=scaler.fit_transform( np.array(df['y']).reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:19:11.832693Z","iopub.execute_input":"2021-12-08T13:19:11.833429Z","iopub.status.idle":"2021-12-08T13:19:11.839783Z","shell.execute_reply.started":"2021-12-08T13:19:11.833391Z","shell.execute_reply":"2021-12-08T13:19:11.839123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:19:24.725465Z","iopub.execute_input":"2021-12-08T13:19:24.726188Z","iopub.status.idle":"2021-12-08T13:19:24.733714Z","shell.execute_reply.started":"2021-12-08T13:19:24.726142Z","shell.execute_reply":"2021-12-08T13:19:24.732804Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Reading the data\ndf2 = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n  \n## Since this challenge is about rating the severity of the toxicity, we will give more weightage to sever_toxic column\ndf2['severe_toxic'] = df2['severe_toxic']*2.5\n\n## Lets have our y variable as sum of all the toxicity\ndf2['y'] = df2[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis = 1).astype(int)\ndf2=df2[['comment_text', 'y']]\nscaler = MinMaxScaler()\ndf2=df2[df2['y']!=0]\ndf2['y']=scaler.fit_transform(np.array(df2['y']).reshape(-1, 1))\n\ndf=df.append(df2)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:08:03.349989Z","iopub.execute_input":"2021-12-08T13:08:03.35085Z","iopub.status.idle":"2021-12-08T13:08:04.478058Z","shell.execute_reply.started":"2021-12-08T13:08:03.350796Z","shell.execute_reply":"2021-12-08T13:08:04.477413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:06:23.821703Z","iopub.execute_input":"2021-12-08T13:06:23.821962Z","iopub.status.idle":"2021-12-08T13:06:23.832148Z","shell.execute_reply.started":"2021-12-08T13:06:23.821935Z","shell.execute_reply":"2021-12-08T13:06:23.831426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Having a look at random 5 samples from the dataset\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:52:57.128701Z","iopub.execute_input":"2021-11-28T03:52:57.129985Z","iopub.status.idle":"2021-11-28T03:52:57.143029Z","shell.execute_reply.started":"2021-11-28T03:52:57.129941Z","shell.execute_reply":"2021-11-28T03:52:57.142393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lets have a look at the distribution of y variable\ndf.hist(column = \"y\")","metadata":{"execution":{"iopub.status.busy":"2021-12-08T13:08:07.063568Z","iopub.execute_input":"2021-12-08T13:08:07.063856Z","iopub.status.idle":"2021-12-08T13:08:07.309847Z","shell.execute_reply.started":"2021-12-08T13:08:07.063826Z","shell.execute_reply":"2021-12-08T13:08:07.309043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have a number values between 0 and 6(inclusive) with a lot of values being 0. Lets take a look at value count to get better idea","metadata":{}},{"cell_type":"code","source":"## Checking out the values of the distribution\ndf['y'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:52:57.424184Z","iopub.execute_input":"2021-11-28T03:52:57.424418Z","iopub.status.idle":"2021-11-28T03:52:57.431739Z","shell.execute_reply.started":"2021-11-28T03:52:57.424389Z","shell.execute_reply":"2021-11-28T03:52:57.431164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we go ahead and build a model on the about distribution, the model will overfit to 0's as the value of 0 is significantly higher than others. Lets sample the data so that the model doesnt overfit. ","metadata":{}},{"cell_type":"markdown","source":"## Sampling the data\ndf = pd.concat([df[df['y'] > 0], df[df['y'] == 0].sample(int(len(df[df['y'] > 0])*1.5))], axis = 0).sample(frac = 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:52:57.433203Z","iopub.execute_input":"2021-11-28T03:52:57.433414Z","iopub.status.idle":"2021-11-28T03:52:57.472827Z","shell.execute_reply.started":"2021-11-28T03:52:57.433387Z","shell.execute_reply":"2021-11-28T03:52:57.471886Z"}}},{"cell_type":"code","source":"## Lets have a look at the distribution of y variable\ndf.hist(column = \"y\")\n\nprint(df['y'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:52:57.475623Z","iopub.execute_input":"2021-11-28T03:52:57.476456Z","iopub.status.idle":"2021-11-28T03:52:57.719932Z","shell.execute_reply.started":"2021-11-28T03:52:57.476418Z","shell.execute_reply":"2021-11-28T03:52:57.71911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building a Regression model using Ridge Regression","metadata":{}},{"cell_type":"code","source":"## tf-idf\n## Converting text to number format to feed it as input to the ML model\nvectorizer = TfidfVectorizer(analyzer = 'char_wb', max_df = 0.5, min_df = 3, ngram_range = (3,6))\nX = vectorizer.fit_transform(df['comment_text'])\n\n## Looking at the shape of the created data\nX.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-08T12:41:55.14807Z","iopub.execute_input":"2021-12-08T12:41:55.148356Z","iopub.status.idle":"2021-12-08T12:42:09.321542Z","shell.execute_reply.started":"2021-12-08T12:41:55.148323Z","shell.execute_reply":"2021-12-08T12:42:09.320875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model\n## trying ridge regression model\nmodel = Ridge()\nmodel.fit(X, df['y'])","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:53:40.168204Z","iopub.execute_input":"2021-11-28T03:53:40.16897Z","iopub.status.idle":"2021-11-28T03:53:48.617397Z","shell.execute_reply.started":"2021-11-28T03:53:40.168933Z","shell.execute_reply":"2021-11-28T03:53:48.616438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comments to Score","metadata":{}},{"cell_type":"code","source":"## Reading the comments that we need to score\ndf_ = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:53:48.622397Z","iopub.execute_input":"2021-11-28T03:53:48.623004Z","iopub.status.idle":"2021-11-28T03:53:48.711089Z","shell.execute_reply.started":"2021-11-28T03:53:48.622954Z","shell.execute_reply":"2021-11-28T03:53:48.710056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:53:48.716766Z","iopub.execute_input":"2021-11-28T03:53:48.717709Z","iopub.status.idle":"2021-11-28T03:53:48.732075Z","shell.execute_reply.started":"2021-11-28T03:53:48.717656Z","shell.execute_reply":"2021-11-28T03:53:48.731158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Extracting the text and converting it for the model \nX_ = vectorizer.transform(df_['text'])\npred = model.predict(X_)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:53:48.733529Z","iopub.execute_input":"2021-11-28T03:53:48.733861Z","iopub.status.idle":"2021-11-28T03:53:56.956208Z","shell.execute_reply.started":"2021-11-28T03:53:48.733819Z","shell.execute_reply":"2021-11-28T03:53:56.955217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Adding prediction back to submission file\ndf_['score'] =  pred","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:53:56.95775Z","iopub.execute_input":"2021-11-28T03:53:56.958053Z","iopub.status.idle":"2021-11-28T03:53:56.962766Z","shell.execute_reply.started":"2021-11-28T03:53:56.958013Z","shell.execute_reply":"2021-11-28T03:53:56.962007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:53:56.964007Z","iopub.execute_input":"2021-11-28T03:53:56.964224Z","iopub.status.idle":"2021-11-28T03:53:56.98288Z","shell.execute_reply.started":"2021-11-28T03:53:56.964198Z","shell.execute_reply":"2021-11-28T03:53:56.982064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:53:56.983936Z","iopub.execute_input":"2021-11-28T03:53:56.984138Z","iopub.status.idle":"2021-11-28T03:53:57.029203Z","shell.execute_reply.started":"2021-11-28T03:53:56.984114Z","shell.execute_reply":"2021-11-28T03:53:57.028284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reference\nhttps://www.kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768\nhttps://www.kaggle.com/steubk/jrsotc-ridgeregression","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}