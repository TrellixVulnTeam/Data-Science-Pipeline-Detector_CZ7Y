{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sys\n\nsys.maxsize = int(1e7)\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow_datasets as tfds\nfrom transformers import DistilBertTokenizerFast\nfrom transformers import TFDistilBertModel, DistilBertConfig\nfrom transformers import logging\nfrom scipy.stats import rankdata\n\n\nlogging.set_verbosity_error()\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-03T13:31:51.015907Z","iopub.execute_input":"2022-02-03T13:31:51.016671Z","iopub.status.idle":"2022-02-03T13:31:58.141545Z","shell.execute_reply.started":"2022-02-03T13:31:51.016553Z","shell.execute_reply":"2022-02-03T13:31:58.140756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"seed\": 2022,\n          \"buffer_size\": 5000,\n          \"n_fold\": 5,\n          \"down_frac\": 1,\n          \"train_split\": 0.8,\n          \"max_length\": 128,\n          \"batch_size\": 64,\n          \"epochs\": 3,\n          \"ft_epochs\": 2,\n          \"learning_rate\": 5e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-6,\n          \"weight_decay\": 1e-6,\n          \"distilbert_dropout\": 0.2,\n          \"distilbert_att_dropout\": 0.2,\n          \"layer-dropout\": 0.2\n          }","metadata":{"execution":{"iopub.status.busy":"2022-02-03T13:31:58.14339Z","iopub.execute_input":"2022-02-03T13:31:58.143687Z","iopub.status.idle":"2022-02-03T13:31:58.150371Z","shell.execute_reply.started":"2022-02-03T13:31:58.143628Z","shell.execute_reply":"2022-02-03T13:31:58.149549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizing Text","metadata":{}},{"cell_type":"code","source":"# Define function to encode text data in batched \ndef batch_encode(tokenizer, df, text_col_name=\"comment_text\", label_col_name=None,\n                 batch_size=CONFIG[\"batch_size\"], max_length=CONFIG[\"max_length\"], \n                 frac=CONFIG[\"down_frac\"]):\n    \"\"\"\"\"\"\"\"\"\n    A function that encodes a batch of texts and returns the texts'\n    corresponding encodings and attention masks that are ready to be fed \n    into a pre-trained transformer model.\n    \n    Input:\n        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n        - texts:       List of strings where each string represents a text\n        - batch_size:  Integer controlling number of texts in a batch\n        - max_length:  Integer controlling max number of words to tokenize in a given text\n    Output:\n        - input_ids:       sequence of texts encoded as a tf.Tensor object\n        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n    \"\"\"\"\"\"\"\"\"\n    \n    def encode(text_tensor):\n        text = tf.compat.as_str(text_tensor.numpy())\n        encoded_text = tokenizer(text,\n                           max_length=max_length,\n                           padding=\"max_length\",\n                           truncation=True,\n                           return_attention_mask=True\n                           )\n        return encoded_text[\"input_ids\"], encoded_text[\"attention_mask\"]\n        \n    \n    def tf_encode(text, label=None):\n        input_ids, attention_mask = tf.py_function(encode, [text], [tf.int64, tf.int64])\n        # `tf.data.Datasets` work best if all components have a shape set\n        #  so set the shapes manually: \n        input_ids.set_shape((128,))\n        attention_mask.set_shape((128,))\n        if label is None:\n            return {\"input_ids\": input_ids, \"attention_mask\":attention_mask}\n        else:   \n            label.set_shape([])\n            return {\"input_ids\": input_ids, \"attention_mask\":attention_mask}, label\n    \n    @tf.function\n    def class_func(text, label):\n        if label == 0:\n            return 0\n        else:\n            return 1\n        \n    if  label_col_name is None:\n        # Make dataset\n        text_input = tf.data.Dataset.from_tensor_slices(np.asarray(df[text_col_name]).astype('str'))\n        text_input = text_input.shuffle(CONFIG[\"buffer_size\"], seed=CONFIG[\"seed\"])\n    else:        \n        # Make dataset\n        text_input = tf.data.Dataset.from_tensor_slices((np.asarray(df[text_col_name]).astype('str'), \n                                                         np.asarray(df[label_col_name]).astype('float32')))\n        text_input = text_input.shuffle(CONFIG[\"buffer_size\"], seed=CONFIG[\"seed\"])\n        \n        # Down sample the majority (not toxic) group\n        if frac < 1:\n            y = df[label_col_name]\n            frac0 = sum(y == 0)/y.shape[0]\n            init_frac = [frac0, 1 - frac0]\n            target_frac = [frac, 1 - frac]\n            resample = tf.data.experimental.rejection_resample(class_func,target_dist=target_frac , initial_dist=init_frac)\n            text_input = text_input.apply(resample)\n            text_input = text_input.map(lambda x, y: y)\n        \n    # Encode and batch  data\n    token_input = text_input.map(tf_encode, num_parallel_calls=tf.data.AUTOTUNE)\n    token_input = token_input.batch(batch_size)\n    \n    return token_input\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-03T13:31:58.152136Z","iopub.execute_input":"2022-02-03T13:31:58.152386Z","iopub.status.idle":"2022-02-03T13:31:58.169302Z","shell.execute_reply.started":"2022-02-03T13:31:58.15235Z","shell.execute_reply":"2022-02-03T13:31:58.16846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Model Architecture","metadata":{"execution":{"iopub.status.busy":"2022-01-18T10:24:01.530042Z","iopub.execute_input":"2022-01-18T10:24:01.530349Z","iopub.status.idle":"2022-01-18T10:24:01.53443Z","shell.execute_reply.started":"2022-01-18T10:24:01.530319Z","shell.execute_reply":"2022-01-18T10:24:01.533527Z"}}},{"cell_type":"code","source":"def build_model(transformer, max_length=CONFIG[\"max_length\"]):\n    \"\"\"\n    Template for building a model off of the BERT or DistilBERT architecture\n    for a binary classification task.\n    \n    Input:\n      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n                      with no added classification head attached.\n      - max_length:   integer controlling the maximum number of encoded tokens \n                      in a given sequence.\n    \n    Output:\n      - model:        a compiled tf.keras.Model with added classification layers \n                      on top of the base pre-trained model architecture.\n    \"\"\"\n \n\n    # Define weight initializer with a random seed to ensure reproducibility\n    weight_initializer = tf.keras.initializers.GlorotNormal(seed=CONFIG[\"seed\"]) \n    \n    # Define input layers\n    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n                                            name=\"input_ids\", \n                                            dtype=\"int32\")\n    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n                                                  name=\"attention_mask\", \n                                                  dtype=\"int32\")\n    \n    # DistilBERT outputs a tuple where the first element at index 0\n    # represents the hidden-state at the output of the model's last layer.\n    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n    \n    # We only care about DistilBERT's output for the [CLS] token, \n    # which is located at index 0 of every encoded sequence.  \n    # Splicing out the [CLS] tokens gives us 2D data.\n    cls_token = last_hidden_state[:, 0, :]\n    \n    ##                                                 ##\n    ## Define additional dropout and dense layers here ##\n    ##                                                 ##\n    \n    drout1 = tf.keras.layers.Dropout(CONFIG[\"layer-dropout\"],\n                                     seed=CONFIG[\"seed\"]\n                                     )(cls_token)\n    \n    dense1 = tf.keras.layers.Dense(256, \n                                   activation=\"relu\",\n                                   kernel_initializer=weight_initializer,  \n                                   bias_initializer=\"zeros\"\n                                   )(drout1)\n    \n    drout2 = tf.keras.layers.Dropout(CONFIG[\"layer-dropout\"],\n                                     seed=CONFIG[\"seed\"]\n                                     )(dense1)\n    \n    \n    dense2 = tf.keras.layers.Dense(32, \n                                   activation=\"relu\",\n                                   kernel_initializer=weight_initializer,  \n                                   bias_initializer=\"zeros\"\n                                   )(dense1)\n    \n    drout3 = tf.keras.layers.Dropout(CONFIG[\"layer-dropout\"],\n                                     seed=CONFIG[\"seed\"]\n                                     )(dense2)\n    \n    \n    # Define a single node that makes up the output layer (for binary classification)\n    output = tf.keras.layers.Dense(1, \n                                   activation=\"linear\",\n                                   kernel_initializer=weight_initializer,  \n                                   bias_initializer=\"zeros\"\n                                   )(drout3)\n    \n    # Define the model\n    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n    \n    # Compile the model\n    model.compile(tf.keras.optimizers.Adam(learning_rate=CONFIG[\"learning_rate\"]), \n                  loss=\"mean_squared_error\",\n                  metrics=tfa.metrics.RSquare(name=\"R^2\", dtype=tf.float32, y_shape=(1,)))\n    \n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-03T13:31:58.173135Z","iopub.execute_input":"2022-02-03T13:31:58.17342Z","iopub.status.idle":"2022-02-03T13:31:58.187256Z","shell.execute_reply.started":"2022-02-03T13:31:58.173381Z","shell.execute_reply":"2022-02-03T13:31:58.186399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"# Split the dataframe into training and validation datasets\ndef get_partitions(df, train_split=CONFIG[\"train_split\"]):\n    n_fold = df[\"kfold\"].max()\n    if n_fold == 0:\n        train_df = df.sample(frac=train_split, random_state=CONFIG[\"seed\"])\n        valid_df = df.drop(train_df.index)\n    else:\n        train_df = input_df[input_df[\"kfold\"] != k]\n        valid_df = input_df[input_df[\"kfold\"] == k]\n    \n    return train_df, valid_df\n\n# get number of steps per epoch \ndef get_num_steps(df, label_col_name, down_frac=True):\n    if down_frac:\n        num_row_df0 = df[df[label_col_name] == 0].shape[0]\n        num_row_df1 = df[df[label_col_name] != 0].shape[0]\n        num_rows =  int(CONFIG[\"down_frac\"] * num_row_df0) + num_row_df1\n    else:\n        num_rows = df.shape[0]\n    num_steps = num_rows // CONFIG[\"batch_size\"]\n    return num_steps\n\n# validate model \ndef validate_model(model, valid_df):\n    # Predict\n    print(\"      Predict less\")\n    valid_less_ds = batch_encode(tokenizer, valid_df, text_col_name=\"less_toxic\")\n    pred_less = model.predict(valid_less_ds, workers=4, use_multiprocessing=True)\n    print(\"      Predict more\")\n    valid_more_ds = batch_encode(tokenizer, valid_df, text_col_name=\"more_toxic\")\n    pred_more = model.predict(valid_more_ds, workers=4, use_multiprocessing=True)\n    # Compare predictions \n    return np.mean(pred_less < pred_more)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T13:31:58.188623Z","iopub.execute_input":"2022-02-03T13:31:58.188896Z","iopub.status.idle":"2022-02-03T13:31:58.200601Z","shell.execute_reply.started":"2022-02-03T13:31:58.18886Z","shell.execute_reply":"2022-02-03T13:31:58.199672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instantiate transformer model","metadata":{}},{"cell_type":"code","source":"# Instantiate DistilBERT tokenizer...we use the Fast version to optimize runtime\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"../input/huggingface-transformers-tfpt/distilbert-base-uncased\")\n\n# Configure DistilBERT's initialization\nconfig = DistilBertConfig(dropout=CONFIG[\"distilbert_dropout\"], \n                          attention_dropout=CONFIG[\"distilbert_att_dropout\"], \n                          output_hidden_states=True)\n                          \n# The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n# and without any specific head on top.\ndistilBERT = TFDistilBertModel.from_pretrained(\"../input/huggingface-transformers-tfpt/distilbert-base-uncased\")\n\n# Make DistilBERT layers untrainable\ndistilBERT.trainable = False\n    \n# initialize model\nmodel = build_model(distilBERT)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T13:31:58.202177Z","iopub.execute_input":"2022-02-03T13:31:58.202436Z","iopub.status.idle":"2022-02-03T13:32:12.789711Z","shell.execute_reply.started":"2022-02-03T13:31:58.2024Z","shell.execute_reply":"2022-02-03T13:32:12.78895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Classification Layer Weights","metadata":{}},{"cell_type":"code","source":"dataset = 1\n\n# load input data \ninput_df = pd.read_csv(\"../input/toxicity-data-prep/train\" + str(dataset) + \"_data.csv\")\ninput_df = input_df[input_df[\"comment_text\"] != \"deleted\"]\nprint(\"input data number:\", input_df.shape[0])\n# load validation data \nvalidation_df = pd.read_csv(\"../input/toxicity-data-prep/valid_data.csv\")\nprint(\"validation data number:\", validation_df.shape[0])\n# load test data \ntest_df = pd.read_csv(\"../input/toxicity-data-prep/test_data.csv\")\nprint(\"test data number:\", test_df.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-03T13:32:12.791171Z","iopub.execute_input":"2022-02-03T13:32:12.791429Z","iopub.status.idle":"2022-02-03T13:32:15.157538Z","shell.execute_reply.started":"2022-02-03T13:32:12.791394Z","shell.execute_reply":"2022-02-03T13:32:15.156699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_fold = input_df[\"kfold\"].max()\nfor k in range(n_fold + 1):\n    print('Train Model for fold ' + str(k))\n    # make training and validation dataset \n    print(\"    Split Data\")\n    train_fold_df, valid_fold_df = get_partitions(input_df)\n    train_fold_ds = batch_encode(tokenizer, train_fold_df, text_col_name=\"comment_text\", label_col_name=\"y\")\n    valid_fold_ds = batch_encode(tokenizer, valid_fold_df, text_col_name=\"comment_text\", label_col_name=\"y\", frac=1)\n\n    # Train the model\n    print(\"    Train Model\")\n    train_history1 = model.fit(\n        train_fold_ds.repeat(),\n        validation_data=valid_fold_ds.repeat(),\n        epochs = CONFIG[\"epochs\"],\n        batch_size = CONFIG[\"batch_size\"],\n        steps_per_epoch = get_num_steps(train_fold_df, \"y\"),\n        validation_steps = get_num_steps(valid_fold_df, \"y\", down_frac=False)\n        )\n\n    # Validate the model\n    print(\"    Validate Model\")\n    right_order_pred = validate_model(model, validation_df)\n    print(\"        Correctly ordered sentences in the validation data:\", np.round(right_order_pred*100, 3), '%' )\n\n    # Make predictions on test data\n    print(\"    Predict \")\n    test_ds = batch_encode(tokenizer, test_df, text_col_name=\"text\")\n    test_score = model.predict(test_ds)\n    test_df[\"score\"] = rankdata(test_score, method=\"ordinal\")\n\n    # Make predictions on training and validation data\n    train_fold_df[\"score\"] = model.predict(train_fold_ds)    # rankdata(train_score, method=\"ordinal\")\n    valid_fold_df[\"score\"] = model.predict(valid_fold_ds)\n    input_fold_df = pd.concat([train_fold_df, valid_fold_df], sort=True)\n\n\n    # Save results\n    print(\"    Save \\n\")\n    result_name =  str(dataset) + \"_fold\" + str(k)\n    test_df.to_csv( \"result_test\" + result_name + \".csv\", index=False)\n    input_fold_df.to_csv( \"result_train\" + result_name + \".csv\", index=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T13:32:15.158983Z","iopub.execute_input":"2022-02-03T13:32:15.159279Z","iopub.status.idle":"2022-02-03T13:32:39.655433Z","shell.execute_reply.started":"2022-02-03T13:32:15.159242Z","shell.execute_reply":"2022-02-03T13:32:39.653464Z"},"trusted":true},"execution_count":null,"outputs":[]}]}