{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \nimport umap\nfrom IPython.display import display, HTML\nfrom pprint import pprint\nimport scipy.optimize as optimize\nfrom matplotlib import pyplot as plt\nimport warnings\nimport seaborn as sns\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-25T09:39:13.875817Z","iopub.execute_input":"2021-11-25T09:39:13.876325Z","iopub.status.idle":"2021-11-25T09:39:13.884894Z","shell.execute_reply.started":"2021-11-25T09:39:13.876265Z","shell.execute_reply":"2021-11-25T09:39:13.883723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# read toxic comments data","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ndf_test_l = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\")\ndf_test_l = df_test_l[df_test_l[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']].sum(axis=1)>=0]\nprint(df_test_l.shape)\nprint(df_test.shape)\ndf_test = pd.merge(df_test, df_test_l, how=\"inner\", on = \"id\")\ndf_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:13.886811Z","iopub.execute_input":"2021-11-25T09:39:13.887068Z","iopub.status.idle":"2021-11-25T09:39:15.129391Z","shell.execute_reply.started":"2021-11-25T09:39:13.887036Z","shell.execute_reply":"2021-11-25T09:39:15.12824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\nprint(df.shape)\n\ndf = pd.concat([df, df_test])\nprint(df.shape)\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(df.loc[df[col]==1,['comment_text',col]].sample(10))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:15.130681Z","iopub.execute_input":"2021-11-25T09:39:15.131282Z","iopub.status.idle":"2021-11-25T09:39:16.390113Z","shell.execute_reply.started":"2021-11-25T09:39:15.131244Z","shell.execute_reply":"2021-11-25T09:39:16.388915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find IMPORTANT words for each type of toxicity","metadata":{}},{"cell_type":"code","source":"labels=['toxic','severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nfeature_wts_all = {}\nprint(df.shape)\n\nfor lbl in labels:\n    print(\"*\"*30 + lbl.upper() + \"*\"*30)\n    features_tmp = FeatureUnion([\n        (\"vect1\", TfidfVectorizer(min_df= 3, \n                                  max_df=0.5, \n                                  analyzer = 'word', \n                                 )),\n\n    ])\n    pipeline_tmp = Pipeline(\n        [\n            (\"features\", features_tmp),\n            (\"clf\", Ridge()),\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline_tmp.fit(df[(df[lbl]>0)|(df[labels].sum(axis=1)==1)]['comment_text'], df[(df[lbl]>0)|(df[labels].sum(axis=1)==1)][lbl])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline_tmp['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline_tmp['features'].get_feature_names(), \n                                  np.round(pipeline_tmp['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    print(\"High score features\")\n    #pprint(feature_wts[:50])\n    feature_wts_all[lbl] = [(x.replace('vect1__',''),y) for x,y in feature_wts if (y > 0.25) & (x.replace('vect1__','').isalpha()) & ( len(x.replace('vect1__','')) > 2)]\n\npprint(feature_wts_all)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:16.391893Z","iopub.execute_input":"2021-11-25T09:39:16.392277Z","iopub.status.idle":"2021-11-25T09:39:24.647467Z","shell.execute_reply.started":"2021-11-25T09:39:16.392227Z","shell.execute_reply":"2021-11-25T09:39:24.645382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put words in DF with toxicity score for each category","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:24.671994Z","iopub.execute_input":"2021-11-25T09:39:24.672768Z","iopub.status.idle":"2021-11-25T09:39:24.678626Z","shell.execute_reply.started":"2021-11-25T09:39:24.67272Z","shell.execute_reply":"2021-11-25T09:39:24.677062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wts = []\nfor k in feature_wts_all.keys():\n    df_wts.append(pd.DataFrame(feature_wts_all[k], columns = [\"word\",\"wt\"]).assign(label=k))\n\nimp_words_df = pd.concat(df_wts).pivot(index='word', columns='label', values='wt').fillna(0)#.reset_index()\nprint(imp_words_df.shape)\n\nimp_words_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:24.681071Z","iopub.execute_input":"2021-11-25T09:39:24.681451Z","iopub.status.idle":"2021-11-25T09:39:24.728499Z","shell.execute_reply.started":"2021-11-25T09:39:24.681403Z","shell.execute_reply":"2021-11-25T09:39:24.727291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bar chart showing words with highest toxicity within each group","metadata":{}},{"cell_type":"code","source":"for lbl in labels:\n    if len(feature_wts_all[lbl]) > 0:\n        print(lbl.upper())\n        ax = imp_words_df\\\n                .sort_values(lbl,ascending=False)\\\n                .head(30)\\\n                .sort_values(lbl,ascending=True)\\\n                .plot\\\n                .barh(rot=0, width=1, figsize = (12,12))\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:24.730265Z","iopub.execute_input":"2021-11-25T09:39:24.731286Z","iopub.status.idle":"2021-11-25T09:39:28.829145Z","shell.execute_reply.started":"2021-11-25T09:39:24.731242Z","shell.execute_reply":"2021-11-25T09:39:28.828535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load fasttext vectors","metadata":{}},{"cell_type":"code","source":"def load_fasttext_model(path):\n    embeddings = {}\n    f = open(path, encoding='utf-8')\n    for line in f:\n        values = line.strip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings[word] = coefs\n    f.close()\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:28.830252Z","iopub.execute_input":"2021-11-25T09:39:28.831279Z","iopub.status.idle":"2021-11-25T09:39:28.83809Z","shell.execute_reply.started":"2021-11-25T09:39:28.831244Z","shell.execute_reply":"2021-11-25T09:39:28.83672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ft_model = load_fasttext_model('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:28.840118Z","iopub.execute_input":"2021-11-25T09:39:28.840971Z","iopub.status.idle":"2021-11-25T09:39:55.721391Z","shell.execute_reply.started":"2021-11-25T09:39:28.840922Z","shell.execute_reply":"2021-11-25T09:39:55.720026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign label to word \nimp_words_df['label_max'] = imp_words_df.idxmax(axis=1)\nimp_words_df['val_max'] = imp_words_df.max(axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:55.722635Z","iopub.status.idle":"2021-11-25T09:39:55.722988Z","shell.execute_reply.started":"2021-11-25T09:39:55.722803Z","shell.execute_reply":"2021-11-25T09:39:55.722821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract sample of IMPORTANT words","metadata":{}},{"cell_type":"code","source":"imp_words_df_tmp = imp_words_df.sort_values('val_max',ascending=False).head(300).copy()\n\nvect = []\nids = []\nlbls = []\nfor idx in imp_words_df_tmp.index.tolist():\n    #print(idx)\n    if idx in ft_model:\n        vect.append(ft_model[idx])\n        ids.append(idx)\n        lbls.append(imp_words_df_tmp.loc[idx].label_max)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:55.724206Z","iopub.status.idle":"2021-11-25T09:39:55.724531Z","shell.execute_reply.started":"2021-11-25T09:39:55.724357Z","shell.execute_reply":"2021-11-25T09:39:55.724373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reduce dimensionality with UMAP","metadata":{}},{"cell_type":"code","source":"vect_arr = np.array(vect)\nreducer = umap.UMAP()\nvect_arr_red = reducer.fit_transform(vect_arr)\nvect_arr_red.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:55.725625Z","iopub.status.idle":"2021-11-25T09:39:55.725942Z","shell.execute_reply.started":"2021-11-25T09:39:55.725781Z","shell.execute_reply":"2021-11-25T09:39:55.725797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize\n\n## Can see some clear groups among the highly toxic words for each category","metadata":{}},{"cell_type":"code","source":"color_map = {\"identity_hate\":\"blue\",\n             \"insult\":\"green\",\n             \"obscene\":\"red\",\n             \"severe_toxic\":\"black\",\n             \"threat\":\"yellow\"}\nplt.figure(figsize=(15,15))\n\nplt.scatter(\n    vect_arr_red[:, 0],\n    vect_arr_red[:, 1],\n    c=[x for x in pd.Series(lbls).map(color_map)])\n\n#red_patch = mpatches.Patch(color='red', label='The red data')\nhandlelist = [plt.plot([], marker=\"o\", ls=\"\", color=color)[0] for x,color in color_map.items()]\nplt.legend(handlelist,[x for x,y in color_map.items()],loc='upper left')#plt.legend(handles={color_map})\n\nplt.title('UMAP projection of the important words', fontsize=24)\nfor i, txt in enumerate(ids):\n    plt.annotate(txt, \n                 (vect_arr_red[i, 0], vect_arr_red[i, 1]), \n                 textcoords=\"offset points\",  # how to position the text\n                 size=10,\n                 xytext=(0, 0.3),  # distance from text to points (x,y)\n                 ha='left')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T09:39:55.726943Z","iopub.status.idle":"2021-11-25T09:39:55.727259Z","shell.execute_reply.started":"2021-11-25T09:39:55.727085Z","shell.execute_reply":"2021-11-25T09:39:55.727101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}