{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 0.816 score by single TF-Idf and Ridge regression on __CLEANED__ data\n\n### Some cleaning patterns shown here - \nhttps://www.kaggle.com/samarthagarwal23/y-patterns-in-nlp-data\n","metadata":{}},{"cell_type":"markdown","source":"#### Built on top of the amazing notebook here : \n- https://www.kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768\n","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline\nimport scipy\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.max_colwidth=300","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:01:33.808251Z","iopub.execute_input":"2021-11-22T17:01:33.808588Z","iopub.status.idle":"2021-11-22T17:01:34.92622Z","shell.execute_reply.started":"2021-11-22T17:01:33.808502Z","shell.execute_reply":"2021-11-22T17:01:34.925358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training data \n\n## Convert the label to SUM of all toxic labels (This might help with maintaining toxicity order of comments)","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\nprint(df.shape)\n\n# Give more weight to severe toxic \ndf['severe_toxic'] = df.severe_toxic * 2\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:01:34.927911Z","iopub.execute_input":"2021-11-22T17:01:34.92821Z","iopub.status.idle":"2021-11-22T17:01:37.452921Z","shell.execute_reply.started":"2021-11-22T17:01:34.928179Z","shell.execute_reply":"2021-11-22T17:01:37.452021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['y'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:01:37.454168Z","iopub.execute_input":"2021-11-22T17:01:37.454415Z","iopub.status.idle":"2021-11-22T17:01:37.465171Z","shell.execute_reply.started":"2021-11-22T17:01:37.454385Z","shell.execute_reply":"2021-11-22T17:01:37.464028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reduce the rows with 0 toxicity ","metadata":{}},{"cell_type":"code","source":"df = pd.concat([df[df.y>0] , \n                df[df.y==0].sample(int(len(df[df.y>0])*1.5)) ], axis=0).sample(frac=1)\n\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:01:37.468Z","iopub.execute_input":"2021-11-22T17:01:37.468384Z","iopub.status.idle":"2021-11-22T17:01:37.521422Z","shell.execute_reply.started":"2021-11-22T17:01:37.468339Z","shell.execute_reply":"2021-11-22T17:01:37.520322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['y'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:01:37.523526Z","iopub.execute_input":"2021-11-22T17:01:37.523768Z","iopub.status.idle":"2021-11-22T17:01:37.534249Z","shell.execute_reply.started":"2021-11-22T17:01:37.52374Z","shell.execute_reply":"2021-11-22T17:01:37.533036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(data, col):\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:01:37.536065Z","iopub.execute_input":"2021-11-22T17:01:37.537018Z","iopub.status.idle":"2021-11-22T17:01:37.547082Z","shell.execute_reply.started":"2021-11-22T17:01:37.53697Z","shell.execute_reply":"2021-11-22T17:01:37.546366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Sklearn Pipeline with \n## TFIDF - Take 'char_wb' as analyzer to capture subwords well\n## Ridge - Ridge is a simple regression algorithm that will reduce overfitting ","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline(\n    [\n        (\"vect\", TfidfVectorizer(min_df= 3, \n                                 max_df=0.5, \n                                 lowercase=False,\n                                 analyzer = 'char_wb', \n                                 ngram_range = (3,5))),\n        (\"clf\", Ridge()),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:01:37.547947Z","iopub.execute_input":"2021-11-22T17:01:37.548163Z","iopub.status.idle":"2021-11-22T17:01:37.560626Z","shell.execute_reply.started":"2021-11-22T17:01:37.548137Z","shell.execute_reply":"2021-11-22T17:01:37.559503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the pipeline\ndf = clean(df, 'text')\npipeline.fit(df['text'], df['y'])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:01:37.562511Z","iopub.execute_input":"2021-11-22T17:01:37.562826Z","iopub.status.idle":"2021-11-22T17:02:37.248853Z","shell.execute_reply.started":"2021-11-22T17:01:37.562782Z","shell.execute_reply":"2021-11-22T17:02:37.248082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate the pipeline ","metadata":{}},{"cell_type":"code","source":"df_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:02:37.250382Z","iopub.execute_input":"2021-11-22T17:02:37.25088Z","iopub.status.idle":"2021-11-22T17:02:37.828991Z","shell.execute_reply.started":"2021-11-22T17:02:37.250822Z","shell.execute_reply":"2021-11-22T17:02:37.8282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = clean(df_val, 'less_toxic')\ndf_val = clean(df_val, 'more_toxic')\n\np1 = pipeline.predict(df_val['less_toxic'])\np2 = pipeline.predict(df_val['more_toxic'])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:02:37.833895Z","iopub.execute_input":"2021-11-22T17:02:37.834588Z","iopub.status.idle":"2021-11-22T17:03:57.636644Z","shell.execute_reply.started":"2021-11-22T17:02:37.834548Z","shell.execute_reply":"2021-11-22T17:03:57.635515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}'","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:03:57.638307Z","iopub.execute_input":"2021-11-22T17:03:57.638598Z","iopub.status.idle":"2021-11-22T17:03:57.646183Z","shell.execute_reply.started":"2021-11-22T17:03:57.638555Z","shell.execute_reply":"2021-11-22T17:03:57.645303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyze bad predictions","metadata":{}},{"cell_type":"markdown","source":" \n### Incorrect predictions with similar scores\n### Incorrect predictions with different scores","metadata":{}},{"cell_type":"code","source":"df_val['p1'] = p1\ndf_val['p2'] = p2\ndf_val['diff'] = np.abs(p2 - p1)\n\ndf_val['correct'] = (p1 < p2).astype('int')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:35.647828Z","iopub.execute_input":"2021-11-22T17:04:35.648715Z","iopub.status.idle":"2021-11-22T17:04:35.654715Z","shell.execute_reply.started":"2021-11-22T17:04:35.648677Z","shell.execute_reply":"2021-11-22T17:04:35.653916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val[df_val.correct == 0]['diff'].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:35.954279Z","iopub.execute_input":"2021-11-22T17:04:35.954548Z","iopub.status.idle":"2021-11-22T17:04:36.396637Z","shell.execute_reply.started":"2021-11-22T17:04:35.95452Z","shell.execute_reply":"2021-11-22T17:04:36.395794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vect_an = pipeline['vect'].build_analyzer()\n# vocab = pipeline['vect'].vocabulary_\n# [v for v in vect_an(df_val.more_toxic[5247]) if (v not in vocab) & (v.strip() not in pipeline['vect'].stop_words_)]","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:36.657646Z","iopub.execute_input":"2021-11-22T17:04:36.657977Z","iopub.status.idle":"2021-11-22T17:04:36.662662Z","shell.execute_reply.started":"2021-11-22T17:04:36.65794Z","shell.execute_reply":"2021-11-22T17:04:36.661733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n### Incorrect predictions with similar scores\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=True).head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:37.313746Z","iopub.execute_input":"2021-11-22T17:04:37.314336Z","iopub.status.idle":"2021-11-22T17:04:37.340117Z","shell.execute_reply.started":"2021-11-22T17:04:37.314298Z","shell.execute_reply":"2021-11-22T17:04:37.339344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Some of these just look incorrectly tagged \n","metadata":{}},{"cell_type":"code","source":"### Incorrect predictions with dis-similar scores\n\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:39.262507Z","iopub.execute_input":"2021-11-22T17:04:39.263159Z","iopub.status.idle":"2021-11-22T17:04:39.287817Z","shell.execute_reply.started":"2021-11-22T17:04:39.263122Z","shell.execute_reply":"2021-11-22T17:04:39.287024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on test data ","metadata":{}},{"cell_type":"code","source":"df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf_sub = clean(df_sub, 'text')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:40.880643Z","iopub.execute_input":"2021-11-22T17:04:40.881238Z","iopub.status.idle":"2021-11-22T17:04:43.149737Z","shell.execute_reply.started":"2021-11-22T17:04:40.881198Z","shell.execute_reply":"2021-11-22T17:04:43.14891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict using pipeline\n\nsub_preds = pipeline.predict(df_sub['text'])\n\ndf_sub['score'] = sub_preds","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:43.152165Z","iopub.execute_input":"2021-11-22T17:04:43.152417Z","iopub.status.idle":"2021-11-22T17:04:50.048484Z","shell.execute_reply.started":"2021-11-22T17:04:43.152388Z","shell.execute_reply":"2021-11-22T17:04:50.047634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correct the rank ordering","metadata":{}},{"cell_type":"code","source":"# Cases with duplicates scores\n\ndf_sub['score'].count() - df_sub['score'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:07.194452Z","iopub.execute_input":"2021-11-22T17:04:07.195672Z","iopub.status.idle":"2021-11-22T17:04:07.205748Z","shell.execute_reply.started":"2021-11-22T17:04:07.195624Z","shell.execute_reply":"2021-11-22T17:04:07.204933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score'].value_counts().reset_index()[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:07.207692Z","iopub.execute_input":"2021-11-22T17:04:07.208369Z","iopub.status.idle":"2021-11-22T17:04:07.226632Z","shell.execute_reply.started":"2021-11-22T17:04:07.208327Z","shell.execute_reply":"2021-11-22T17:04:07.225787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score'].rank().nunique()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:07.228521Z","iopub.execute_input":"2021-11-22T17:04:07.229015Z","iopub.status.idle":"2021-11-22T17:04:07.241045Z","shell.execute_reply.started":"2021-11-22T17:04:07.228973Z","shell.execute_reply":"2021-11-22T17:04:07.239945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rank the predictions \n\ndf_sub['score']  = scipy.stats.rankdata(df_sub['score'], method='ordinal')\n\nprint(df_sub['score'].rank().nunique())","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:07.242784Z","iopub.execute_input":"2021-11-22T17:04:07.243925Z","iopub.status.idle":"2021-11-22T17:04:07.2552Z","shell.execute_reply.started":"2021-11-22T17:04:07.243866Z","shell.execute_reply":"2021-11-22T17:04:07.25422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T17:04:07.256718Z","iopub.execute_input":"2021-11-22T17:04:07.257199Z","iopub.status.idle":"2021-11-22T17:04:07.285197Z","shell.execute_reply.started":"2021-11-22T17:04:07.257154Z","shell.execute_reply":"2021-11-22T17:04:07.28439Z"},"trusted":true},"execution_count":null,"outputs":[]}]}