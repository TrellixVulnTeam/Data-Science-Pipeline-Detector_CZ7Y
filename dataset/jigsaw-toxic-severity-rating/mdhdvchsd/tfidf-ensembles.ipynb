{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-17T04:23:16.151243Z","iopub.execute_input":"2021-12-17T04:23:16.151946Z","iopub.status.idle":"2021-12-17T04:23:16.218725Z","shell.execute_reply.started":"2021-12-17T04:23:16.151805Z","shell.execute_reply":"2021-12-17T04:23:16.217896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch.utils.data import Dataset, DataLoader\n# from transformers import BertForTokenClassification, BertForSequenceClassification,BertPreTrainedModel, BertModel\n# from transformers import AutoTokenizer, BertTokenizer\n# import torch.nn as nn\n# import torch\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nfrom scipy.optimize import minimize\n\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:23:16.220128Z","iopub.execute_input":"2021-12-17T04:23:16.220492Z","iopub.status.idle":"2021-12-17T04:23:16.788309Z","shell.execute_reply.started":"2021-12-17T04:23:16.220459Z","shell.execute_reply":"2021-12-17T04:23:16.787412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class BertForRegression(BertPreTrainedModel):\n#     def __init__(self, config,params):\n#         super().__init__(config)\n#         self.bert = BertModel(config)\n#         self.dropout = nn.Dropout(params['dropout'])\n#         self.classifier = nn.Linear(config.hidden_size, 1)\n#         self.loss_fct = nn.MSELoss()\n#         self.init_weights()\n\n#     def forward(\n#             self,\n#             input_ids=None,\n#             attention_mask=None,\n#             token_type_ids=None,\n#             position_ids=None,\n#             head_mask=None,\n#             inputs_embeds=None,\n#             labels=None,\n#     ):\n#         outputs = self.bert(\n#             input_ids,\n#             attention_mask=attention_mask,\n#             token_type_ids=token_type_ids,\n#             position_ids=position_ids,\n#             head_mask=head_mask,\n#             inputs_embeds=inputs_embeds,\n#         )\n#         pooled_output = outputs[1]\n\n#         pooled_output = self.dropout(pooled_output)\n#         logits = self.classifier(pooled_output)\n\n#         outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n#         if labels is not None:\n#             loss = self.loss_fct(logits, labels)\n#             outputs = (loss,) + outputs\n\n#         return outputs  # (loss), logits, (hidden_states), (attentions)\n\n    \n    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:35:11.05405Z","iopub.execute_input":"2021-12-11T18:35:11.054275Z","iopub.status.idle":"2021-12-11T18:35:11.061263Z","shell.execute_reply.started":"2021-12-11T18:35:11.054248Z","shell.execute_reply":"2021-12-11T18:35:11.060369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class Triage(Dataset):\n#     def __init__(self, dataframe, tokenizer, max_len):\n#         self.len = len(dataframe)\n#         self.data = dataframe\n#         self.tokenizer = tokenizer\n#         self.max_len = max_len\n        \n#     def __getitem__(self, index):\n#         title = str(self.data.text[index])\n#         title = \" \".join(title.split())\n#         inputs = self.tokenizer.encode_plus(\n#             title,\n#             None,\n#             add_special_tokens=True,\n#             max_length=self.max_len,\n#             padding='max_length',\n#             return_token_type_ids=True,\n#             truncation=True\n#         )\n#         ids = inputs['input_ids'] \n#         mask = inputs['attention_mask']\n\n#         return {\n#             'ids': torch.tensor(ids, dtype=torch.long),\n#             'mask': torch.tensor(mask, dtype=torch.long),\n#             'targets': torch.tensor(self.data.labels[index], dtype=torch.float32)\n#         } \n    \n#     def __len__(self):\n#         return self.len","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:35:11.06256Z","iopub.execute_input":"2021-12-11T18:35:11.062829Z","iopub.status.idle":"2021-12-11T18:35:11.075984Z","shell.execute_reply.started":"2021-12-11T18:35:11.062802Z","shell.execute_reply":"2021-12-11T18:35:11.075216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def predict(model, dataloader, device):\n#     predicted_label = []\n#     actual_label = []\n#     model.eval()\n#     with torch.no_grad():\n#         for step,data in tqdm(enumerate(dataloader, 0), total=len(dataloader)):\n#             input_ids = data['ids'].to(device, dtype = torch.long)\n#             attention_mask = data['mask'].to(device, dtype = torch.long)\n#             targets = data['targets'].to(device, dtype = torch.float32)\n#             targets = targets.unsqueeze(1)\n\n# #             input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n#             output = model(input_ids, attention_mask)\n                        \n#             predicted_label += output[0]\n#             actual_label += targets\n            \n#     return predicted_label","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:35:11.077769Z","iopub.execute_input":"2021-12-11T18:35:11.077996Z","iopub.status.idle":"2021-12-11T18:35:11.093657Z","shell.execute_reply.started":"2021-12-11T18:35:11.077968Z","shell.execute_reply":"2021-12-11T18:35:11.092643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch import cuda\n# device = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:35:11.095189Z","iopub.execute_input":"2021-12-11T18:35:11.095527Z","iopub.status.idle":"2021-12-11T18:35:11.10628Z","shell.execute_reply.started":"2021-12-11T18:35:11.095485Z","shell.execute_reply":"2021-12-11T18:35:11.105415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Defining some key variables that will be used later on in the training\n# MAX_LEN = 256 \n# VALID_BATCH_SIZE = 32\n# MODEL_PATHS = [\n#     '../input/bert-base-uncased-reddit',\n# #     '../input/bert-base-uncased-founta',\n#     '../input/bert-base-uncased-toxic-comment',\n# #     '../input/bert-base-uncased-toxic-unintended'\n#     '../input/bert-base-uncased-davidson'\n# ]\n\nfilename = '../input/tf-idf-ruddit/finalized_model.sav'\ntfidf_reddit = pickle.load(open(filename, 'rb'))\n\nfilename = '../input/tfidf-balanced/finalized_model.sav'\ntfidf = pickle.load(open(filename, 'rb'))\n\nfilename = '../input/notebook907c10f9d0/finalized_model.sav'\ntfidf_x = pickle.load(open(filename, 'rb'))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:23:16.789473Z","iopub.execute_input":"2021-12-17T04:23:16.7897Z","iopub.status.idle":"2021-12-17T04:23:19.715699Z","shell.execute_reply.started":"2021-12-17T04:23:16.789673Z","shell.execute_reply":"2021-12-17T04:23:19.714956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def Prediction(model_path, test_df):\n#     tokenizer = BertTokenizer.from_pretrained(model_path)\n#     test = pd.DataFrame()\n#     test['text'] = test_df.copy()\n#     test['labels'] = 0\n    \n \n#     testing_set = Triage(test, tokenizer, MAX_LEN)\n\n\n#     test_params = {'batch_size': 32,\n#                     'shuffle': False,\n#                     'num_workers': 0\n#                     }\n\n#     test_loader = DataLoader(testing_set, **test_params)\n    \n#     model = BertForRegression.from_pretrained(\n#             model_path, # Use the 12-layer BERT model, with an uncased vocab\n#             params={'dropout':0.2}).to(device)\n    \n# #     model.eval()\n    \n#     output = predict(model, test_loader, device)\n    \n#     out2 =[]\n#     for out in output:\n#         out2.append(out.cpu().detach().numpy())\n\n#     out = np.array(out2).reshape(len(out2))\n    \n#     return out\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:35:13.561942Z","iopub.execute_input":"2021-12-11T18:35:13.562304Z","iopub.status.idle":"2021-12-11T18:35:13.568639Z","shell.execute_reply.started":"2021-12-11T18:35:13.562242Z","shell.execute_reply":"2021-12-11T18:35:13.567698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom collections import defaultdict\n\nspecial_character_removal=re.compile(r'[^?!.,:a-z\\d ]',re.IGNORECASE)\n# regex to replace all numerics\nreplace_numbers=re.compile(r'\\d+',re.IGNORECASE)\nword_count_dict = defaultdict(int)\ntoxic_dict = {}\n\ndef clean_text(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    # dirty words\n    #text = text.lower()\n    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n    \n    if clean_wiki_tokens:\n        # Drop the image\n        text = re.sub(r\"image:[a-zA-Z0-9]*\\.jpg\", \" \", text)\n        text = re.sub(r\"image:[a-zA-Z0-9]*\\.png\", \" \", text)\n        text = re.sub(r\"image:[a-zA-Z0-9]*\\.gif\", \" \", text)\n        text = re.sub(r\"image:[a-zA-Z0-9]*\\.bmp\", \" \", text)\n\n        # Drop css\n        text = re.sub(r\"#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})\", \" \",text)\n        text = re.sub(r\"\\{\\|[^\\}]*\\|\\}\", \" \", text)\n        \n        # Clean templates\n        text = re.sub(r\"\\[?\\[user:.*\\]\", \" \", text)\n        text = re.sub(r\"\\[?\\[user:.*\\|\", \" \", text)        \n        text = re.sub(r\"\\[?\\[wikipedia:.*\\]\", \" \", text)\n        text = re.sub(r\"\\[?\\[wikipedia:.*\\|\", \" \", text)\n        text = re.sub(r\"\\[?\\[special:.*\\]\", \" \", text)\n        text = re.sub(r\"\\[?\\[special:.*\\|\", \" \", text)\n        text = re.sub(r\"\\[?\\[category:.*\\]\", \" \", text)\n        text = re.sub(r\"\\[?\\[category:.*\\|\", \" \", text)\n    \n   \n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\?\", \" ? \", text)\n    text = re.sub(r\"\\!\", \" ! \", text)\n    text = re.sub(r\"\\\"\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = replace_numbers.sub(' ', text)\n    #text = special_character_removal.sub('',text)\n\n    if count_null_words:\n        text = text.split()\n        for t in text:\n            word_count_dict[t] += 1\n        text = \" \".join(text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n\n    return (text)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # tokenizer = AutoTokenizer.from_pretrained(MODEL_PATHS)\nval_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\nval_df.drop_duplicates(subset='less_toxic', keep='first', inplace=True)\nval_df.reset_index(inplace=True)\nval_df.head()\n############################\n# val_df = val_df.head(1000) #","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:35:13.570316Z","iopub.execute_input":"2021-12-11T18:35:13.570649Z","iopub.status.idle":"2021-12-11T18:35:14.270509Z","shell.execute_reply.started":"2021-12-11T18:35:13.570607Z","shell.execute_reply":"2021-12-11T18:35:14.269612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:35:14.271681Z","iopub.execute_input":"2021-12-11T18:35:14.271896Z","iopub.status.idle":"2021-12-11T18:35:14.275837Z","shell.execute_reply.started":"2021-12-11T18:35:14.271869Z","shell.execute_reply":"2021-12-11T18:35:14.274865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions_less=[]\nmodel_predictions_more=[]\n\n\n# for model_path in MODEL_PATHS:\n#     print(model_path)\n#     mod1_less_tox = Prediction(model_path, val_df['less_toxic'])\n#     mod1_more_tox = Prediction(model_path, val_df['more_toxic'])\n#     model_predictions_less.append(mod1_less_tox)\n#     model_predictions_more.append(mod1_more_tox)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:35:14.277235Z","iopub.execute_input":"2021-12-11T18:35:14.277864Z","iopub.status.idle":"2021-12-11T18:35:14.289528Z","shell.execute_reply.started":"2021-12-11T18:35:14.277828Z","shell.execute_reply":"2021-12-11T18:35:14.288709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions_less.append(tfidf_reddit.predict(val_df['less_toxic']))\nmodel_predictions_more.append(tfidf_reddit.predict(val_df['more_toxic']))\n\nval_df['less_toxic'] = [clean_text(text) for text in val_df['less_toxic']]\nval_df['more_toxic'] = [clean_text(text) for text in val_df['more_toxic']]\n\nmodel_predictions_less.append(tfidf.predict(val_df['less_toxic']))\nmodel_predictions_less.append(tfidf_x.predict(val_df['less_toxic']))\n\nmodel_predictions_more.append(tfidf.predict(val_df['more_toxic']))\nmodel_predictions_more.append(tfidf_x.predict(val_df['more_toxic']))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:35:14.290916Z","iopub.execute_input":"2021-12-11T18:35:14.291607Z","iopub.status.idle":"2021-12-11T18:36:14.944041Z","shell.execute_reply.started":"2021-12-11T18:35:14.291558Z","shell.execute_reply":"2021-12-11T18:36:14.943109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred_less_tox = np.concatenate([mod1_less_tox[:, None] for mod1_less_tox in model_predictions_less], axis=1)\nval_pred_more_tox = np.concatenate([mod1_more_tox[:, None] for mod1_more_tox in model_predictions_more], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:14.946854Z","iopub.execute_input":"2021-12-11T18:36:14.947101Z","iopub.status.idle":"2021-12-11T18:36:14.953031Z","shell.execute_reply.started":"2021-12-11T18:36:14.947072Z","shell.execute_reply":"2021-12-11T18:36:14.951841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(model_predictions_less)):\n    print(np.round((model_predictions_less[i] < model_predictions_more[i]).mean(),2))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:14.954283Z","iopub.execute_input":"2021-12-11T18:36:14.954827Z","iopub.status.idle":"2021-12-11T18:36:14.96777Z","shell.execute_reply.started":"2021-12-11T18:36:14.954787Z","shell.execute_reply":"2021-12-11T18:36:14.966906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(weights):\n    \"\"\" Calculate the score of a weighted average of predictions\n    \n    Parameters\n    ----------\n    weights: array\n        the weights applied to the average of the base predictions\n        \n    Returns\n    -------\n    float\n        The mean_squared_error score of the ensemble\n    \"\"\"\n    \n    y_ens_less_tox = np.average(val_pred_less_tox, axis=1, weights=weights)\n    y_ens_more_tox = np.average(val_pred_more_tox, axis=1, weights=weights)\n    return np.round((y_ens_less_tox < y_ens_more_tox).mean(),2)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:14.969079Z","iopub.execute_input":"2021-12-11T18:36:14.969304Z","iopub.status.idle":"2021-12-11T18:36:14.980717Z","shell.execute_reply.started":"2021-12-11T18:36:14.969277Z","shell.execute_reply":"2021-12-11T18:36:14.979883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\n Finding Blending Weights ...')\nres_list = []\nweights_list = []\n\nfor k in range(100):\n    starting_values = np.random.uniform(size=val_pred_less_tox.shape[1])\n    bounds = [(0, 1)] * val_pred_less_tox.shape[1]\n    \n    res = minimize(objective,\n                   starting_values,\n                   method='L-BFGS-B',\n                   bounds=bounds,\n                   options={'disp': False,\n                            'maxiter': 100000}) \n    \n    res_list.append(res['fun'])\n    weights_list.append(res['x'])\n    \n    print('{iter}\\tScore: {score}\\tWeights: {weights}'.format(\n        iter=(k + 1),\n        score=res['fun'],\n        weights='\\t'.join([str(item) for item in res['x']])))\n\n    \nbestSC   = np.max(res_list)\nbestWght = weights_list[np.argmax(res_list)]\nweights  = bestWght\nblend_score = round(bestSC, 6)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:14.981734Z","iopub.execute_input":"2021-12-11T18:36:14.982627Z","iopub.status.idle":"2021-12-11T18:36:15.425975Z","shell.execute_reply.started":"2021-12-11T18:36:14.982587Z","shell.execute_reply":"2021-12-11T18:36:15.425207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\n Ensemble Score: {best_score}'.format(best_score=bestSC))\nprint('\\n Best Weights: {weights}'.format(weights=bestWght))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:15.427162Z","iopub.execute_input":"2021-12-11T18:36:15.427401Z","iopub.status.idle":"2021-12-11T18:36:15.433015Z","shell.execute_reply.started":"2021-12-11T18:36:15.427373Z","shell.execute_reply":"2021-12-11T18:36:15.432096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n# test_df = test_df.head(500)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:15.434151Z","iopub.execute_input":"2021-12-11T18:36:15.434533Z","iopub.status.idle":"2021-12-11T18:36:15.570041Z","shell.execute_reply.started":"2021-12-11T18:36:15.434497Z","shell.execute_reply":"2021-12-11T18:36:15.569251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions=[]\n\n# for model_path in MODEL_PATHS:\n#     mod1_tox = Prediction(model_path, test_df['text'])\n#     model_predictions.append(mod1_tox)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:15.571177Z","iopub.execute_input":"2021-12-11T18:36:15.571454Z","iopub.status.idle":"2021-12-11T18:36:15.575903Z","shell.execute_reply.started":"2021-12-11T18:36:15.571417Z","shell.execute_reply":"2021-12-11T18:36:15.574881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel_predictions.append(tfidf_reddit.predict(test_df['text']))\n\ntest_df['text'] = [clean_text(text) for text in test_df['text']]\n\nmodel_predictions.append(tfidf.predict(test_df['text']))\nmodel_predictions.append(tfidf_x.predict(test_df['text']))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:15.577897Z","iopub.execute_input":"2021-12-11T18:36:15.578257Z","iopub.status.idle":"2021-12-11T18:36:34.823629Z","shell.execute_reply.started":"2021-12-11T18:36:15.578215Z","shell.execute_reply":"2021-12-11T18:36:34.822718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred_tox = np.concatenate([model_pred[:, None] for model_pred in model_predictions], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:34.825331Z","iopub.execute_input":"2021-12-11T18:36:34.825622Z","iopub.status.idle":"2021-12-11T18:36:34.830115Z","shell.execute_reply.started":"2021-12-11T18:36:34.825589Z","shell.execute_reply":"2021-12-11T18:36:34.829428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_weights = [0.76944408, 0.39145082, 0.28043746, 0.90303933]","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:34.831374Z","iopub.execute_input":"2021-12-11T18:36:34.832026Z","iopub.status.idle":"2021-12-11T18:36:34.845806Z","shell.execute_reply.started":"2021-12-11T18:36:34.831977Z","shell.execute_reply":"2021-12-11T18:36:34.84508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def final_predictions(test_preds, best_weights):\n    \"\"\" Calculate the score of a weighted average of predictions\n    \n    Parameters\n    ----------\n    weights: array\n        the weights applied to the average of the base predictions\n        \n    Returns\n    -------\n    float\n        The mean_squared_error score of the ensemble\n    \"\"\"\n    \n    output_y = np.average(test_preds, axis=1, weights=best_weights)\n    return output_y\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:34.847177Z","iopub.execute_input":"2021-12-11T18:36:34.847648Z","iopub.status.idle":"2021-12-11T18:36:34.858675Z","shell.execute_reply.started":"2021-12-11T18:36:34.847615Z","shell.execute_reply":"2021-12-11T18:36:34.85781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output=final_predictions(test_pred_tox, bestWght)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:34.861465Z","iopub.execute_input":"2021-12-11T18:36:34.862038Z","iopub.status.idle":"2021-12-11T18:36:34.876313Z","shell.execute_reply.started":"2021-12-11T18:36:34.862002Z","shell.execute_reply":"2021-12-11T18:36:34.875407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import rankdata\n\nout2 =[]\nfor out in output:\n    out2.append(out)\n\nout = np.array(out2).reshape(len(out2))\n\nsubmission = pd.DataFrame({'comment_id': test_df['comment_id'], 'score':out})\nsubmission['score'] = rankdata(submission['score'], method='ordinal')\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:34.878182Z","iopub.execute_input":"2021-12-11T18:36:34.880624Z","iopub.status.idle":"2021-12-11T18:36:34.91715Z","shell.execute_reply.started":"2021-12-11T18:36:34.880571Z","shell.execute_reply":"2021-12-11T18:36:34.916242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:36:34.91836Z","iopub.execute_input":"2021-12-11T18:36:34.918939Z","iopub.status.idle":"2021-12-11T18:36:34.929652Z","shell.execute_reply.started":"2021-12-11T18:36:34.918906Z","shell.execute_reply":"2021-12-11T18:36:34.928928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}