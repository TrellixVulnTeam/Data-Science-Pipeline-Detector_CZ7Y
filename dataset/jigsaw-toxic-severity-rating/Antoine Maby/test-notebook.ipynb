{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom bs4 import BeautifulSoup\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nstop_words = stopwords.words(\"english\")\n\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom pandas.core.frame import DataFrame\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-29T18:54:47.506802Z","iopub.execute_input":"2022-01-29T18:54:47.507154Z","iopub.status.idle":"2022-01-29T18:55:10.590992Z","shell.execute_reply.started":"2022-01-29T18:54:47.507052Z","shell.execute_reply":"2022-01-29T18:55:10.589977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Dataset","metadata":{}},{"cell_type":"code","source":"train_path = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\ncomments_to_score_path = \"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\nvalidation_path = \"../input/jigsaw-toxic-severity-rating/validation_data.csv\"\nsample_path = \"/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-01-29T18:55:10.592418Z","iopub.execute_input":"2022-01-29T18:55:10.592668Z","iopub.status.idle":"2022-01-29T18:55:10.597037Z","shell.execute_reply.started":"2022-01-29T18:55:10.592637Z","shell.execute_reply":"2022-01-29T18:55:10.596241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(train_path)\ncomments_to_score = pd.read_csv(comments_to_score_path)\nvalidation = pd.read_csv(validation_path)\nsample = pd.read_csv(sample_path)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T18:55:10.598529Z","iopub.execute_input":"2022-01-29T18:55:10.599402Z","iopub.status.idle":"2022-01-29T18:55:13.2227Z","shell.execute_reply.started":"2022-01-29T18:55:10.599346Z","shell.execute_reply":"2022-01-29T18:55:13.221941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean text","metadata":{}},{"cell_type":"code","source":"def routine_clean(x):\n    # Remove unicode characters\n    x = x.encode(\"ascii\", \"ignore\").decode()\n    # Remove English contractions\n    x = re.sub(\"\\'\\w+\", '', x)\n    # Remove ponctuation but not # (for C# for example)\n    x = re.sub('[^\\\\w\\\\s#]', '', x)\n    # Remove links\n    x = re.sub(r'http*\\S+', '', x)\n    # Remove numbers\n    x = re.sub(r'\\w*\\d+\\w*', '', x)\n    # Remove extra spaces\n    x = re.sub('\\s+', ' ', x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-01-29T18:55:13.245456Z","iopub.execute_input":"2022-01-29T18:55:13.246002Z","iopub.status.idle":"2022-01-29T18:55:13.253334Z","shell.execute_reply.started":"2022-01-29T18:55:13.245942Z","shell.execute_reply":"2022-01-29T18:55:13.252725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['comment_text'] = [BeautifulSoup(text,\"lxml\").get_text() for text in train['comment_text']]","metadata":{"execution":{"iopub.status.busy":"2022-01-29T18:55:13.254329Z","iopub.execute_input":"2022-01-29T18:55:13.255143Z","iopub.status.idle":"2022-01-29T18:55:46.507751Z","shell.execute_reply.started":"2022-01-29T18:55:13.255108Z","shell.execute_reply":"2022-01-29T18:55:46.506826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(data:DataFrame, col:str) -> DataFrame:\n\n    data[col] = data[col].str.replace(\n        'https?://\\S+|www\\.\\S+', ' social medium ')\n\n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\")\n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\")\n    data[col] = data[col].str.replace(\"1\", \"i\")\n    data[col] = data[col].str.replace(\"!\", \"i\")\n    data[col] = data[col].str.replace(\"|\", \"i\")\n    data[col] = data[col].str.replace(\"0\", \"o\")\n    data[col] = data[col].str.replace(\"l3\", \"b\")\n    data[col] = data[col].str.replace(\"7\", \"t\")\n    data[col] = data[col].str.replace(\"7\", \"+\")\n    data[col] = data[col].str.replace(\"8\", \"ate\")\n    data[col] = data[col].str.replace(\"3\", \"e\")\n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\")\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")\n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\n        \"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \")\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \")\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you')\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    # will remove more than one whitespace character\n    data[col] = data[col].str.replace('\\s+', ' ')\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    # 2 or more characters are replaced by 2 characters\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1')\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\n        \"[:|♣|'|§|♠|*|/|?|=|%|&|-|#|•|~|^|>|<|►|_]\", '')\n\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(\n        r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)', r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}', r'\\1\\1\\1')\n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)', r' \\1 ')\n    # patterns with repeating characters\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b', r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B', r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}', ' ').str.strip()\n    data[col] = data[col].str.replace(r'[ ]{2,}', ' ').str.strip()\n    data[col] = data[col].apply(lambda x: ' '.join(\n        [word for word in x.split() if word not in (stop_words)]))\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-01-29T18:55:46.516825Z","iopub.execute_input":"2022-01-29T18:55:46.517103Z","iopub.status.idle":"2022-01-29T18:55:46.573129Z","shell.execute_reply.started":"2022-01-29T18:55:46.517073Z","shell.execute_reply":"2022-01-29T18:55:46.572412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = clean(train, 'comment_text')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T18:55:46.574258Z","iopub.execute_input":"2022-01-29T18:55:46.575182Z","iopub.status.idle":"2022-01-29T18:57:43.145665Z","shell.execute_reply.started":"2022-01-29T18:55:46.575134Z","shell.execute_reply":"2022-01-29T18:57:43.144796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['comment_text'] = df_train['comment_text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T18:57:43.170586Z","iopub.execute_input":"2022-01-29T18:57:43.17091Z","iopub.status.idle":"2022-01-29T18:57:43.418418Z","shell.execute_reply.started":"2022-01-29T18:57:43.170875Z","shell.execute_reply":"2022-01-29T18:57:43.417388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Universal Sentence Encoder","metadata":{}},{"cell_type":"code","source":"import tensorflow_hub as hub\n\npath_encoder = \"/kaggle/input/universalsentenceencoder4/\"\nencoder = hub.load(path_encoder)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T18:57:43.420046Z","iopub.execute_input":"2022-01-29T18:57:43.420267Z","iopub.status.idle":"2022-01-29T18:58:01.855206Z","shell.execute_reply.started":"2022-01-29T18:57:43.42024Z","shell.execute_reply":"2022-01-29T18:58:01.854337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\ntrain_1 = df_train['comment_text'][0:10000]\nX_train_encoder_use = encoder(train_1)\nX_train_encoder_use = X_train_encoder_use['outputs']\n\nfor i in range (1,16):\n    j = i + 1\n    i0 = i*10000\n    j0 = j*10000\n    split = df_train['comment_text'][i0:j0]\n    split_encoder = encoder(split)\n    X_train_encoder_use = tf.concat([X_train_encoder_use, split_encoder['outputs']], 0)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T18:58:01.856882Z","iopub.execute_input":"2022-01-29T18:58:01.857208Z","iopub.status.idle":"2022-01-29T19:00:47.269453Z","shell.execute_reply.started":"2022-01-29T18:58:01.857161Z","shell.execute_reply":"2022-01-29T19:00:47.267835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_encoder_use = X_train_encoder_use.numpy()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:00:47.270895Z","iopub.execute_input":"2022-01-29T19:00:47.271188Z","iopub.status.idle":"2022-01-29T19:00:47.434632Z","shell.execute_reply.started":"2022-01-29T19:00:47.271153Z","shell.execute_reply":"2022-01-29T19:00:47.433655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation['less_toxic'] = [BeautifulSoup(text,\"lxml\").get_text() for text in validation['less_toxic']]\nvalidation['less_toxic'] = [routine_clean(text) for text in validation['less_toxic']]\ndf_validation = clean(validation, 'less_toxic')\ndf_validation['less_toxic'] = df_validation['less_toxic'].str.lower()\n\n\nvalidation['more_toxic'] = [BeautifulSoup(text,\"lxml\").get_text() for text in validation['more_toxic']]\nvalidation['more_toxic'] = [routine_clean(text) for text in validation['more_toxic']]\ndf_validation = clean(validation, 'more_toxic')\ndf_validation['more_toxic'] = df_validation['more_toxic'].str.lower()\n\ntrain_1 = df_validation['less_toxic'][0:10000]\nvalidation_less_encoder_use = encoder(train_1)\nvalidation_less_encoder_use = validation_less_encoder_use['outputs']\n\ntrain_1 = df_validation['more_toxic'][0:10000]\nvalidation_more_encoder_use = encoder(train_1)\nvalidation_more_encoder_use = validation_more_encoder_use['outputs']\n\nfor i in range (1,4):\n    j = i + 1\n    i0 = i*10000\n    j0 = j*10000\n    split = df_validation['less_toxic'][i0:j0]\n    split_encoder = encoder(split)\n    validation_less_encoder_use = tf.concat([validation_less_encoder_use, split_encoder['outputs']], 0)   \n    split = df_validation['more_toxic'][i0:j0]\n    split_encoder = encoder(split)\n    validation_more_encoder_use = tf.concat([validation_more_encoder_use, split_encoder['outputs']], 0)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:00:47.435992Z","iopub.execute_input":"2022-01-29T19:00:47.436808Z","iopub.status.idle":"2022-01-29T19:03:10.539315Z","shell.execute_reply.started":"2022-01-29T19:00:47.436759Z","shell.execute_reply":"2022-01-29T19:03:10.538352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_less_encoder_use = validation_less_encoder_use.numpy()\nvalidation_more_encoder_use = validation_more_encoder_use.numpy()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:03:10.540597Z","iopub.execute_input":"2022-01-29T19:03:10.540888Z","iopub.status.idle":"2022-01-29T19:03:10.604271Z","shell.execute_reply.started":"2022-01-29T19:03:10.540855Z","shell.execute_reply":"2022-01-29T19:03:10.603374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentence Transformer","metadata":{}},{"cell_type":"markdown","source":"Encode localy and export as Dataset with All mini l12 model","metadata":{}},{"cell_type":"code","source":"X_train_encoder_sbert_path = \"../input/data-encoder-all-mini-l12-v2/Data_encoder_all_Mini.csv\"\nvalidation_less_encoder_sbert_path = \"../input/data-encoder-all-mini-l12-v2/Data_less_encoder_all_Mini.csv\"\nvalidation_more_encoder_sbert_path = \"../input/data-encoder-all-mini-l12-v2/Data_more_encoder_all_Mini.csv\"\ncomments_encoder_sbert_path = \"../input/data-encoder-all-mini-l12-v2/Data_comments_encoder_all_Mini.csv\"\n\nX_train_encoder_sbert = pd.read_csv(X_train_encoder_sbert_path)\nvalidation_less_encoder_sbert = pd.read_csv(validation_less_encoder_sbert_path)\nvalidation_more_encoder_sbert = pd.read_csv(validation_more_encoder_sbert_path)\ncomments_encoder_sbert = pd.read_csv(comments_encoder_sbert_path)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:03:10.605948Z","iopub.execute_input":"2022-01-29T19:03:10.606272Z","iopub.status.idle":"2022-01-29T19:03:40.788993Z","shell.execute_reply.started":"2022-01-29T19:03:10.60623Z","shell.execute_reply":"2022-01-29T19:03:40.788014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Concatenate","metadata":{}},{"cell_type":"code","source":"X_train_encoder_use = pd.DataFrame(X_train_encoder_use)\nX_train_sbert_use = pd.concat((X_train_encoder_sbert, X_train_encoder_use),axis=1)\nX_train_sbert_use = X_train_sbert_use.T.reset_index(drop=True).T","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:03:40.790322Z","iopub.execute_input":"2022-01-29T19:03:40.79056Z","iopub.status.idle":"2022-01-29T19:03:43.355818Z","shell.execute_reply.started":"2022-01-29T19:03:40.790532Z","shell.execute_reply":"2022-01-29T19:03:43.355021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_less_encoder_use = pd.DataFrame(validation_less_encoder_use)\nvalidation_more_encoder_use = pd.DataFrame(validation_more_encoder_use)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:03:43.356923Z","iopub.execute_input":"2022-01-29T19:03:43.357145Z","iopub.status.idle":"2022-01-29T19:03:43.362871Z","shell.execute_reply.started":"2022-01-29T19:03:43.357119Z","shell.execute_reply":"2022-01-29T19:03:43.361682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_less_toxic = pd.concat((validation_less_encoder_sbert, validation_less_encoder_use),axis=1)\nX_more_toxic = pd.concat((validation_more_encoder_sbert, validation_more_encoder_use),axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:03:43.364113Z","iopub.execute_input":"2022-01-29T19:03:43.36443Z","iopub.status.idle":"2022-01-29T19:03:43.739764Z","shell.execute_reply.started":"2022-01-29T19:03:43.364402Z","shell.execute_reply":"2022-01-29T19:03:43.738844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comments to score","metadata":{}},{"cell_type":"code","source":"comments_to_score['text'] = [BeautifulSoup(text,\"lxml\").get_text() for text in comments_to_score['text']]\ncomments_to_score['text'] = [routine_clean(text) for text in comments_to_score['text']]\ndf_comment = clean(comments_to_score, 'text')\ndf_comment['text'] = df_comment['text'].str.lower()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:03:43.741146Z","iopub.execute_input":"2022-01-29T19:03:43.741376Z","iopub.status.idle":"2022-01-29T19:03:53.264969Z","shell.execute_reply.started":"2022-01-29T19:03:43.741348Z","shell.execute_reply":"2022-01-29T19:03:53.264063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comment_toxic_use = encoder(df_comment['text'])\ncomment_toxic_use = comment_toxic_use['outputs']\ncomment_toxic_use = comment_toxic_use.numpy()\ncomment_toxic_use = pd.DataFrame(comment_toxic_use)\nX_comment_toxic = pd.concat((comments_encoder_sbert, comment_toxic_use),axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:03:53.266323Z","iopub.execute_input":"2022-01-29T19:03:53.266573Z","iopub.status.idle":"2022-01-29T19:04:02.247163Z","shell.execute_reply.started":"2022-01-29T19:03:53.266525Z","shell.execute_reply":"2022-01-29T19:04:02.246105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"df_train['sum'] = 0.32*train['toxic'] + 1.5*train['severe_toxic'] + 0.16*train['obscene'] + 1.5*train['threat'] + 0.64*train['insult'] + 1.5*train['identity_hate']\nlabel = df_train['sum']\n\nX_train_sbert_use, X_val_sbert_use, y_train, y_val = train_test_split(X_train_sbert_use, label, test_size=0.1, random_state=42)\n\ntrain_data = lgb.Dataset(X_train_sbert_use, label=y_train)\nval_data = lgb.Dataset(X_val_sbert_use, label=y_val)\n\nparams = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_leaves': 425,\n    'max_depth': 25,\n    \"feature_fraction\": 0.8,\n    'subsample_freq': 1,\n    \"bagging_fraction\": 0.98,\n    'min_data_in_leaf': 20,\n    'learning_rate': 0.05,\n    \"boosting\": \"gbdt\",\n    \"lambda_l1\": 0.2,\n    \"lambda_l2\": 10,\n    \"verbosity\": -1,\n    \"random_state\": 42\n}\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:04:30.896125Z","iopub.execute_input":"2022-01-29T19:04:30.896944Z","iopub.status.idle":"2022-01-29T19:04:34.839496Z","shell.execute_reply.started":"2022-01-29T19:04:30.896894Z","shell.execute_reply":"2022-01-29T19:04:34.838282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_round = 100\nbst = lgb.train(params, train_data, num_round, valid_sets=[val_data])","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:04:35.671117Z","iopub.execute_input":"2022-01-29T19:04:35.671389Z","iopub.status.idle":"2022-01-29T19:04:35.931796Z","shell.execute_reply.started":"2022-01-29T19:04:35.67136Z","shell.execute_reply":"2022-01-29T19:04:35.930813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation Score\n\nless_toxic_pred_sbert_use = bst.predict(X_less_toxic)\nmore_toxic_pred_sbert_use = bst.predict(X_more_toxic)\n(less_toxic_pred_sbert_use < more_toxic_pred_sbert_use).mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\npred_comment_1 = bst.predict(X_comment_toxic)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_1 = pred_comment_1","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:04:37.940886Z","iopub.execute_input":"2022-01-29T19:04:37.9412Z","iopub.status.idle":"2022-01-29T19:04:37.945754Z","shell.execute_reply.started":"2022-01-29T19:04:37.941163Z","shell.execute_reply":"2022-01-29T19:04:37.944846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample['score'] = result_1","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:15:32.917571Z","iopub.execute_input":"2022-01-29T19:15:32.91832Z","iopub.status.idle":"2022-01-29T19:15:32.924351Z","shell.execute_reply.started":"2022-01-29T19:15:32.918272Z","shell.execute_reply":"2022-01-29T19:15:32.923367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:15:34.80052Z","iopub.execute_input":"2022-01-29T19:15:34.801064Z","iopub.status.idle":"2022-01-29T19:15:34.824407Z","shell.execute_reply.started":"2022-01-29T19:15:34.801032Z","shell.execute_reply":"2022-01-29T19:15:34.823722Z"},"trusted":true},"execution_count":null,"outputs":[]}]}