{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using Toxic Comment Classification\n\nHere I have used the multi-label Toxic Comment Classification Dataset (by Jigsaw).\n\nI have used Roberta-base to predict 6 different outputs each belonging to one label (toxic, severe_toxic, etc...)\n\nI have not passed the output logits through any sigmoid function. Just used the outputs for these 6 labels and taken a linear average of them.\n\nThat's what I have used as the score for comparison.\n\n#### I have used:\n\n* BCELogitsLoss as the loss funtion : It already applies a sigmoid function on the output before calculating the loss\n* Early stooping with a patience of 1 : You can modify according to your need\n* Used CosineAnnealingLR scheduler : It changes the LR per step following a cosine funtion.\n","metadata":{}},{"cell_type":"markdown","source":"## The final layer of the model looks like this:\n\n ![NN](https://user-images.githubusercontent.com/74188336/141213710-3a1b7473-8436-4683-841e-64d87789f47e.png)","metadata":{}},{"cell_type":"markdown","source":"It has 6 output heads giving outputs to the 6 different labels to determine.\n\nThis 6 different heads are attached on top the roberta-base (for now) will implement roberta-large too.","metadata":{}},{"cell_type":"markdown","source":"# Aleron's review üëà\n\n\n### –ë—É–¥—å <span style=\"color:red\">–í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–µ–µ</span> –ï—Å—Ç—å –Ω–µ–¥–æ—á–µ—Ç—ã:)\n\n* –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–∞—Ç–∞—Å–µ—Ç –ø—Ä–æ—à–ª—ã—Ö –ª–µ—Ç.\n* –ù–µ—Å–æ–≤—Å–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –Ω–∞–ø–∏—Å–∞–Ω–∞ –ª–æ—Å —Ñ—É–Ω–∫—Ü–∏—è. \n* –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä\n* –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–∑—ã–π –Ω–∞–¥–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –Ω–æ—É—Ç–±—É–∫ (—á–∏—Å—Ç–∫ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö)\n\n\nModel used : RoBERTa-base (RoBERTa-large is way too big and takes a whole lot of time training O.o)\n\n# –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏ —Ç—É—Ç: [ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å | Final_blending_all_models | üòé](https://www.kaggle.com/aleron751/final-blending-all-models/edit) \n\n**–ë—ã–ª–æ –ø–æ–ª—É—á–µ–Ω–æ –Ω–∞ –æ–¥–Ω–æ–º —Ñ–æ–ª–¥–µ 0.798 –Ω–∞ LB (made public)**","metadata":{}},{"cell_type":"code","source":"!pip install tez -q\n!pip install iterative-stratification -q","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:41:03.875062Z","iopub.execute_input":"2022-01-24T20:41:03.875467Z","iopub.status.idle":"2022-01-24T20:41:20.883663Z","shell.execute_reply.started":"2022-01-24T20:41:03.875356Z","shell.execute_reply":"2022-01-24T20:41:20.882726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport os\nimport tez\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import AdamW, AutoTokenizer, AutoModel\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\nimport optuna\nimport time\n\n# –î–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport nltk\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-01-24T20:41:20.886359Z","iopub.execute_input":"2022-01-24T20:41:20.886583Z","iopub.status.idle":"2022-01-24T20:41:29.294009Z","shell.execute_reply.started":"2022-01-24T20:41:20.886554Z","shell.execute_reply":"2022-01-24T20:41:29.293092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –≤–Ω–µ—à–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç | –ê–Ω–¥—Ä–µ–π üî•","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/toxic-comments/train.csv')\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:41:29.29572Z","iopub.execute_input":"2022-01-24T20:41:29.296094Z","iopub.status.idle":"2022-01-24T20:41:31.150258Z","shell.execute_reply.started":"2022-01-24T20:41:29.296057Z","shell.execute_reply":"2022-01-24T20:41:31.149563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# –ß–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö | –ï–≥–æ—Ä üî•üî•","metadata":{}},{"cell_type":"code","source":"def washing_machine(comments): # –ß–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞\n    corpus=[]\n    for i in tqdm(range(len(comments))):\n        comment = re.sub('[^a-zA-Z]', ' ', comments[i])\n        comment = comment.lower()\n        comment = comment.split()\n        stemmer = SnowballStemmer('english')\n        lemmatizer = WordNetLemmatizer()\n        all_stopwords = stopwords.words('english')\n        comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n        comment = [lemmatizer.lemmatize(word) for word in comment]\n        comment = ' '.join(comment)\n        corpus.append(comment)\n\n    return corpus\n\ndf['cleaned_comment_text'] = washing_machine(df['comment_text'].values)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:41:31.154959Z","iopub.execute_input":"2022-01-24T20:41:31.155428Z","iopub.status.idle":"2022-01-24T20:44:34.536765Z","shell.execute_reply.started":"2022-01-24T20:41:31.155389Z","shell.execute_reply":"2022-01-24T20:44:34.536048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_folds(data, num_splits):\n    data.loc[:,'kfold'] = -1\n    X = data['cleaned_comment_text']\n    y = data[['toxic', 'severe_toxic', 'obscene', 'threat',\n           'insult', 'identity_hate']]\n    mskf = MultilabelStratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n    \n    for fold, (trn_, val_) in enumerate(mskf.split(X,y)):\n        data.loc[val_,'kfold'] = fold\n        \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:34.539193Z","iopub.execute_input":"2022-01-24T20:44:34.539607Z","iopub.status.idle":"2022-01-24T20:44:34.545758Z","shell.execute_reply.started":"2022-01-24T20:44:34.539568Z","shell.execute_reply":"2022-01-24T20:44:34.544794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_5 = create_folds(df.copy(), 5)\ndf_5.to_csv('5folds.csv', index=False)\n\ndf_10 = create_folds(df.copy(), 10)\ndf_10.to_csv('10folds.csv', index=False)\n\ndf_5.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:34.54713Z","iopub.execute_input":"2022-01-24T20:44:34.547418Z","iopub.status.idle":"2022-01-24T20:44:49.016563Z","shell.execute_reply.started":"2022-01-24T20:44:34.547379Z","shell.execute_reply":"2022-01-24T20:44:49.01582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_5['kfold'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.018019Z","iopub.execute_input":"2022-01-24T20:44:49.018488Z","iopub.status.idle":"2022-01-24T20:44:49.031256Z","shell.execute_reply.started":"2022-01-24T20:44:49.01845Z","shell.execute_reply":"2022-01-24T20:44:49.030453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    model_name = 'roberta-base'\n    batch_size = 96\n    lr = 1e-4\n    weight_decay = 0.01\n    scheduler = 'CosineAnnealingLR'\n    early_stopping_epochs = 1\n    epochs = 15 # 20\n    max_length = 128\n    #max_length = 196\n    num_folds = 2","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.032686Z","iopub.execute_input":"2022-01-24T20:44:49.03311Z","iopub.status.idle":"2022-01-24T20:44:49.042169Z","shell.execute_reply.started":"2022-01-24T20:44:49.033071Z","shell.execute_reply":"2022-01-24T20:44:49.039382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class ToxicDataset:\n    def __init__(self, data, tokenizer,  max_len=196):\n        self.comments = data['comment_text'].values\n        self.tokenizer = tokenizer\n        self.targets = data[[\n            'toxic', 'severe_toxic', 'obscene',\n            'threat','insult', 'identity_hate']].values\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.comments)\n    \n    def __getitem__(self, idx):\n        \n        tokenized = self.tokenizer.encode_plus(\n            self.comments[idx],\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n        \n        input_ids = tokenized['input_ids']\n        attention_mask = tokenized['attention_mask']\n        \n        toxic, severe_toxic, obscene, threat, insult, identity_hate = self.targets[idx]\n\n        return {\n            'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask' : torch.tensor(attention_mask, dtype=torch.long),\n            'toxic' : torch.tensor(toxic, dtype=torch.float),\n            'severe_toxic' : torch.tensor(severe_toxic, dtype=torch.float),\n            'obscene' : torch.tensor(obscene, dtype=torch.float),\n            'threat' : torch.tensor(threat, dtype=torch.float),\n            'insult' : torch.tensor(insult, dtype=torch.float),\n            'identity_hate' : torch.tensor(identity_hate, dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:40:59.968724Z","iopub.execute_input":"2022-01-24T21:40:59.969001Z","iopub.status.idle":"2022-01-24T21:40:59.990635Z","shell.execute_reply.started":"2022-01-24T21:40:59.968969Z","shell.execute_reply":"2022-01-24T21:40:59.989923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Model","metadata":{}},{"cell_type":"code","source":"class ToxicModel(nn.Module):\n    def __init__(self, args, model_name):\n        super(ToxicModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(self.args.model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.toxic = nn.Linear(768, 1)\n        self.stoxic = nn.Linear(768, 1)\n        self.obs = nn.Linear(768, 1)\n        self.threat = nn.Linear(768, 1)\n        self.insult = nn.Linear(768, 1)\n        self.id_hate = nn.Linear(768, 1)\n    \n        \n    def forward(self, input_ids, attention_mask):\n        \n        out = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=False\n        )\n        \n        out = self.dropout(out[1])\n        \n        toxic = self.toxic(out)\n        stoxic = self.stoxic(out)\n        obs = self.obs(out)\n        threat = self.threat(out)\n        insult = self.insult(out)\n        id_hate = self.id_hate(out)\n\n        return [toxic, stoxic, obs, threat, insult, id_hate]\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.066768Z","iopub.execute_input":"2022-01-24T20:44:49.067173Z","iopub.status.idle":"2022-01-24T20:44:49.094734Z","shell.execute_reply.started":"2022-01-24T20:44:49.067136Z","shell.execute_reply":"2022-01-24T20:44:49.093165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function","metadata":{}},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    o1, o2, o3, o4, o5, o6 = outputs\n    t1, t2, t3, t4, t5, t6 = targets\n    l1 = nn.BCEWithLogitsLoss()(o1, t1.view(-1,1))\n    l2 = nn.BCEWithLogitsLoss()(o2, t2.view(-1,1))\n    l3 = nn.BCEWithLogitsLoss()(o3, t3.view(-1,1))\n    l4 = nn.BCEWithLogitsLoss()(o4, t4.view(-1,1))\n    l5 = nn.BCEWithLogitsLoss()(o5, t5.view(-1,1))\n    l6 = nn.BCEWithLogitsLoss()(o6, t6.view(-1,1))\n    \n    total_loss = (l1+l2+l3+l4+l5+l6)/6\n    \n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.099129Z","iopub.execute_input":"2022-01-24T20:44:49.099447Z","iopub.status.idle":"2022-01-24T20:44:49.113197Z","shell.execute_reply.started":"2022-01-24T20:44:49.099409Z","shell.execute_reply":"2022-01-24T20:44:49.112271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def metrics(outputs, targets):\n    auc_scores=[]\n    for o, t in zip(outputs, targets):\n        o = o.cpu().detach().numpy()\n        t = t.cpu().detach().numpy()\n        auc = roc_auc_score(o, t)\n        auc_scores(auc)\n\n    return np.mean(auc_scores)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.117574Z","iopub.execute_input":"2022-01-24T20:44:49.119673Z","iopub.status.idle":"2022-01-24T20:44:49.126795Z","shell.execute_reply.started":"2022-01-24T20:44:49.119638Z","shell.execute_reply":"2022-01-24T20:44:49.126004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training epoch","metadata":{}},{"cell_type":"code","source":"def train_epoch(args, dataloader, model, optimizer, scheduler, epoch):\n    model.train()\n    epoch_loss = 0.0\n    running_loss = 0.0\n    dataset_size=0\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        optimizer.zero_grad()\n        \n        input_ids = data['input_ids'].cuda()\n        attention_mask = data['attention_mask'].cuda()\n        toxic = data['toxic'].cuda()\n        severe_toxic = data['severe_toxic'].cuda()\n        obscene = data['obscene'].cuda()\n        threat = data['threat'].cuda()\n        insult = data['insult'].cuda()\n        identity_hate = data['identity_hate'].cuda()\n        \n        batch_size = args.batch_size\n        \n        targets = (toxic, severe_toxic, obscene, threat, insult, identity_hate)\n        outputs = model(input_ids, attention_mask)\n        \n        \n        loss = loss_fn(outputs, targets)\n\n        \n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.131176Z","iopub.execute_input":"2022-01-24T20:44:49.133731Z","iopub.status.idle":"2022-01-24T20:44:49.147148Z","shell.execute_reply.started":"2022-01-24T20:44:49.133694Z","shell.execute_reply":"2022-01-24T20:44:49.146335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Epoch","metadata":{}},{"cell_type":"code","source":"def validation(args, dataloader, model):\n    model.eval()\n    epoch_loss = 0.0\n    running_loss = 0.0\n    dataset_size=0\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    with torch.no_grad():\n        for step, data in bar:\n            batch_size = args.batch_size\n\n            input_ids = data['input_ids'].cuda()\n            attention_mask = data['attention_mask'].cuda()\n            toxic = data['toxic'].cuda()\n            severe_toxic = data['severe_toxic'].cuda()\n            obscene = data['obscene'].cuda()\n            threat = data['threat'].cuda()\n            insult = data['insult'].cuda()\n            identity_hate = data['identity_hate'].cuda()\n\n            targets = (toxic, severe_toxic, obscene, threat, insult, identity_hate)\n\n            outputs = model(input_ids, attention_mask)\n\n            loss = loss_fn(outputs, targets)\n\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n\n            epoch_loss = running_loss / dataset_size\n\n            bar.set_postfix(Valid_Loss=epoch_loss,\n                            Stage='Validation') \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.15486Z","iopub.execute_input":"2022-01-24T20:44:49.155478Z","iopub.status.idle":"2022-01-24T20:44:49.169834Z","shell.execute_reply.started":"2022-01-24T20:44:49.155436Z","shell.execute_reply":"2022-01-24T20:44:49.16913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer","metadata":{}},{"cell_type":"code","source":"def get_optimizer(args, params):\n    opt = AdamW(params, lr=args.lr, weight_decay=args.weight_decay)\n    return opt","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.174312Z","iopub.execute_input":"2022-01-24T20:44:49.177207Z","iopub.status.idle":"2022-01-24T20:44:49.182862Z","shell.execute_reply.started":"2022-01-24T20:44:49.17711Z","shell.execute_reply":"2022-01-24T20:44:49.182267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scheduler","metadata":{}},{"cell_type":"code","source":"def get_scheduler(args, optimizer):\n    if args.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=500, \n                                                   eta_min=1e-6)\n    else:\n        schduler = None\n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.187248Z","iopub.execute_input":"2022-01-24T20:44:49.187795Z","iopub.status.idle":"2022-01-24T20:44:49.195744Z","shell.execute_reply.started":"2022-01-24T20:44:49.18776Z","shell.execute_reply":"2022-01-24T20:44:49.194773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and validation Loop","metadata":{}},{"cell_type":"code","source":"def run(data, fold, args=None, save_model=False):\n    print('-'*50)\n    print(f'Fold : {fold}')\n    print('-'*50)\n    \n    if args is None:\n        args = Config()\n        \n    start = time.time()\n    model = ToxicModel(args, args.model_name)\n    model = model.cuda()\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    \n    optimizer = get_optimizer(args, model.parameters())\n    scheduler = get_scheduler(args, optimizer)\n    \n    train = data[data['kfold']!=fold]\n    valid = data[data['kfold']==fold]\n    \n    train_dataset = ToxicDataset(train, tokenizer, args.max_length)\n    valid_dataset = ToxicDataset(valid, tokenizer, args.max_length)\n    \n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n    valid_loader = DataLoader(valid_dataset, batch_size=2*args.batch_size)\n    \n    best_val_loss = np.inf\n    patience_counter = 0\n\n    for epoch in range(args.epochs):\n        \n        train_loss = train_epoch(args, train_loader, model, optimizer, scheduler, epoch)\n        valid_loss = validation(args, valid_loader, model)\n        \n        if valid_loss <= best_val_loss:\n            print(f\"Validation Loss Improved ({best_val_loss} ---> {valid_loss})\")\n            best_val_loss = valid_loss\n            \n            if save_model:\n                PATH = f\"model_fold_{fold}.bin\"\n                torch.save(model.state_dict(), PATH)\n                print(f\"----------Model Saved----------\")\n        \n        else:\n            patience_counter += 1\n            print(f'Early stopping counter {patience_counter} of {args.early_stopping_epochs}')\n            if patience_counter == args.early_stopping_epochs:\n                print('*************** Early Stopping ***************')\n                break\n    \n    end = time.time()\n    time_elapsed = end-start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_val_loss))\n    \n    del model, train_loader, valid_loader\n    gc.collect()\n    return best_val_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.200178Z","iopub.execute_input":"2022-01-24T20:44:49.202813Z","iopub.status.idle":"2022-01-24T20:44:49.220193Z","shell.execute_reply.started":"2022-01-24T20:44:49.202777Z","shell.execute_reply":"2022-01-24T20:44:49.219483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read the data","metadata":{}},{"cell_type":"markdown","source":"I have used 5 folds that I have cleaned using this notebook: [Multi-Label Stratified K-fold | Toxic Comments](https://www.kaggle.com/kishalmandal/multi-label-stratified-k-fold-toxic-comments)\n\nThe data is the same data used in the toxicity classification challenge by Jigsaw","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('5folds.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:49.224754Z","iopub.execute_input":"2022-01-24T20:44:49.227649Z","iopub.status.idle":"2022-01-24T20:44:50.649503Z","shell.execute_reply.started":"2022-01-24T20:44:49.227595Z","shell.execute_reply":"2022-01-24T20:44:50.64875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:50.650801Z","iopub.execute_input":"2022-01-24T20:44:50.651056Z","iopub.status.idle":"2022-01-24T20:44:50.729352Z","shell.execute_reply.started":"2022-01-24T20:44:50.651023Z","shell.execute_reply":"2022-01-24T20:44:50.728633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:50.730684Z","iopub.execute_input":"2022-01-24T20:44:50.730963Z","iopub.status.idle":"2022-01-24T20:44:50.795449Z","shell.execute_reply.started":"2022-01-24T20:44:50.730926Z","shell.execute_reply":"2022-01-24T20:44:50.794538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna","metadata":{}},{"cell_type":"code","source":"# df_trial = df[:100]","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:50.796755Z","iopub.execute_input":"2022-01-24T20:44:50.797135Z","iopub.status.idle":"2022-01-24T20:44:50.801229Z","shell.execute_reply.started":"2022-01-24T20:44:50.797098Z","shell.execute_reply":"2022-01-24T20:44:50.800545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def objective(trial):\n#     args = Config()\n#     args.epochs=1\n#     args.lr = trial.suggest_uniform('lr',1e-6, 1e-3)\n#     all_losses = []\n#     for fold in range(5):\n#         temp_loss = run(df_trial, fold, args=args)\n#         all_losses.append(temp_loss)\n    \n#     return np.mean(all_losses)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:50.802646Z","iopub.execute_input":"2022-01-24T20:44:50.803164Z","iopub.status.idle":"2022-01-24T20:44:50.808418Z","shell.execute_reply.started":"2022-01-24T20:44:50.803129Z","shell.execute_reply":"2022-01-24T20:44:50.807808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=10)\n\n# print('Best Trial:')\n# trial_ = study.best_trial\n# print(trial_.values)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-24T20:44:50.809613Z","iopub.execute_input":"2022-01-24T20:44:50.8112Z","iopub.status.idle":"2022-01-24T20:44:50.819201Z","shell.execute_reply.started":"2022-01-24T20:44:50.811111Z","shell.execute_reply":"2022-01-24T20:44:50.818333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trial_.params['lr']","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:50.820638Z","iopub.execute_input":"2022-01-24T20:44:50.821136Z","iopub.status.idle":"2022-01-24T20:44:50.827484Z","shell.execute_reply.started":"2022-01-24T20:44:50.821099Z","shell.execute_reply":"2022-01-24T20:44:50.826824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run training","metadata":{}},{"cell_type":"code","source":"args=Config()\nargs.lr = 0.0005149849355804644\n\n# –ü—Ä–æ–±–µ–≥–∞–µ–º—Å—è –ø–æ –≤—Å–µ–º —Ñ–æ–ª–¥–∞–º\n# run(df, fold=0, save_model=True, args=args)\n\nfor fold in tqdm(range(5)):\n    run(df, fold=fold, save_model=True, args=args)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:44:50.829115Z","iopub.execute_input":"2022-01-24T20:44:50.829354Z","iopub.status.idle":"2022-01-24T21:36:01.205217Z","shell.execute_reply.started":"2022-01-24T20:44:50.829323Z","shell.execute_reply":"2022-01-24T21:36:01.203407Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# –ì–æ—Ç–æ–≤–∏–º submission | Inference","metadata":{}},{"cell_type":"code","source":"# com1 = washing_machine(df['less_toxic'].values)\n# com2 = washing_machine(df['more_toxic'].values)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:36:01.206013Z","iopub.status.idle":"2022-01-24T21:36:01.206312Z","shell.execute_reply.started":"2022-01-24T21:36:01.206147Z","shell.execute_reply":"2022-01-24T21:36:01.206167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(args, dataloader, model):\n    model.eval()\n    all_outputs=[]\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    with torch.no_grad():\n        for step, data in bar:\n            batch_size = args.batch_size\n\n            input_ids = data['input_ids'].cuda()\n            attention_mask = data['attention_mask'].cuda()\n            outputs = model(input_ids, attention_mask)\n            outputs = outputs.cpu().detach().numpy()\n            outputs = [sum(output) for output in outputs]\n            all_outputs.append(outputs)\n\n            bar.set_postfix(Stage='Inference') \n    return np.hstack(all_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:44:51.19386Z","iopub.execute_input":"2022-01-24T21:44:51.194686Z","iopub.status.idle":"2022-01-24T21:44:51.202132Z","shell.execute_reply.started":"2022-01-24T21:44:51.194639Z","shell.execute_reply":"2022-01-24T21:44:51.201095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(data):\n    args=Config()\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    base_path='./'\n    \n    dataset = ToxicDataset(data, tokenizer)\n    dataloader = DataLoader(dataset, batch_size=16*args.batch_size)\n    \n    final_preds = []\n    \n    num_folds = args.num_folds\n    \n    for fold in range(num_folds):\n        model = ToxicModel(args)\n        model = model.cuda()\n        path = base_path + f'model_fold_{fold}.bin'\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {fold+1}\")\n        preds = get_predictions(args, dataloader, model)\n        final_preds.append(np.vstack(preds))\n        del model\n        gc.collect()\n    return np.hstack(sum(final_preds)/num_folds)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:44:54.773639Z","iopub.execute_input":"2022-01-24T21:44:54.773926Z","iopub.status.idle":"2022-01-24T21:44:54.781853Z","shell.execute_reply.started":"2022-01-24T21:44:54.773867Z","shell.execute_reply":"2022-01-24T21:44:54.780772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred1 = inference(com1)\n# pred2 = inference(com2)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:44:04.20395Z","iopub.execute_input":"2022-01-24T21:44:04.204596Z","iopub.status.idle":"2022-01-24T21:44:04.207874Z","shell.execute_reply.started":"2022-01-24T21:44:04.204559Z","shell.execute_reply":"2022-01-24T21:44:04.20692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:44:05.859623Z","iopub.execute_input":"2022-01-24T21:44:05.860166Z","iopub.status.idle":"2022-01-24T21:44:05.904977Z","shell.execute_reply.started":"2022-01-24T21:44:05.860127Z","shell.execute_reply":"2022-01-24T21:44:05.904253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments = washing_machine(df['text'].values)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:36:09.149949Z","iopub.execute_input":"2022-01-24T21:36:09.150252Z","iopub.status.idle":"2022-01-24T21:36:18.044848Z","shell.execute_reply.started":"2022-01-24T21:36:09.150218Z","shell.execute_reply":"2022-01-24T21:36:18.044138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    model_name = 'roberta-base'\n    batch_size = 96\n    lr = 1e-4\n    weight_decay = 0.01\n    scheduler = 'CosineAnnealingLR'\n    early_stopping_epochs = 1\n    epochs = 15 # –±—ã–ª–æ 20\n    max_length = 196 # = 128\n    num_folds = 2\n    \nclass ToxicModel(nn.Module):\n    def __init__(self, args):\n        super(ToxicModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(self.args.model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.toxic = nn.Linear(768, 1)\n        self.stoxic = nn.Linear(768, 1)\n        self.obs = nn.Linear(768, 1)\n        self.threat = nn.Linear(768, 1)\n        self.insult = nn.Linear(768, 1)\n        self.id_hate = nn.Linear(768, 1)\n    \n        \n    def forward(self, input_ids, attention_mask):\n        \n        out = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=False\n        )\n        \n        out = self.dropout(out[1])\n        \n        toxic = self.toxic(out)\n        stoxic = self.stoxic(out)\n        obs = self.obs(out)\n        threat = self.threat(out)\n        insult = self.insult(out)\n        id_hate = self.id_hate(out)\n\n        return torch.cat([toxic, stoxic, obs, threat, insult, id_hate], dim=-1)\n            \n    \nclass ToxicDataset:\n    def __init__(self, comments, tokenizer, max_len=196):\n        self.comments = comments\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.comments)\n    \n    def __getitem__(self, idx):\n        \n        tokenized = self.tokenizer.encode_plus(\n            self.comments[idx],\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n        \n        input_ids = tokenized['input_ids']\n        attention_mask = tokenized['attention_mask']\n        \n\n        return {\n            'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask' : torch.tensor(attention_mask, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:43:38.270004Z","iopub.execute_input":"2022-01-24T21:43:38.270424Z","iopub.status.idle":"2022-01-24T21:43:38.293328Z","shell.execute_reply.started":"2022-01-24T21:43:38.270385Z","shell.execute_reply":"2022-01-24T21:43:38.292087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = inference(comments)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:45:02.47111Z","iopub.execute_input":"2022-01-24T21:45:02.471665Z","iopub.status.idle":"2022-01-24T21:45:10.116177Z","shell.execute_reply.started":"2022-01-24T21:45:02.471625Z","shell.execute_reply":"2022-01-24T21:45:10.114701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['score'] = pred","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:36:01.218266Z","iopub.status.idle":"2022-01-24T21:36:01.218925Z","shell.execute_reply.started":"2022-01-24T21:36:01.218671Z","shell.execute_reply":"2022-01-24T21:36:01.218695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['comment_id', 'score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:36:01.22015Z","iopub.status.idle":"2022-01-24T21:36:01.220781Z","shell.execute_reply.started":"2022-01-24T21:36:01.220543Z","shell.execute_reply":"2022-01-24T21:36:01.220566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}