{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport sklearn\nsklearn.set_config(display='diagram') #П\n\n# from tqdm.auto import tqdm\n# tqdm.pandas()\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom scipy.stats import rankdata\n\n\nimport pickle\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge,RidgeCV, LinearRegression, Lasso\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.compose import ColumnTransformer, make_column_selector\n\nfrom nltk.corpus import stopwords\nfrom pprint import pprint\nfrom time import time\nimport logging\n\nfrom bs4 import BeautifulSoup\nimport re\nimport unidecode\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T23:25:23.705628Z","iopub.execute_input":"2022-02-07T23:25:23.706098Z","iopub.status.idle":"2022-02-07T23:25:25.284408Z","shell.execute_reply.started":"2022-02-07T23:25:23.705917Z","shell.execute_reply":"2022-02-07T23:25:25.283672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --no-index --no-deps ../input/swifter/swifter-1.1.2.tar","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:25.28591Z","iopub.execute_input":"2022-02-07T23:25:25.286404Z","iopub.status.idle":"2022-02-07T23:25:29.071699Z","shell.execute_reply.started":"2022-02-07T23:25:25.286368Z","shell.execute_reply":"2022-02-07T23:25:29.070835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Settings","metadata":{}},{"cell_type":"code","source":"# !pip install --no-index --no-deps   ../input/pyphen-0100/*.whl\n# !pip install --no-index --no-deps   ../input/textstat071py3/*.whl","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:29.082387Z","iopub.execute_input":"2022-02-07T23:25:29.082866Z","iopub.status.idle":"2022-02-07T23:25:29.095354Z","shell.execute_reply.started":"2022-02-07T23:25:29.082827Z","shell.execute_reply":"2022-02-07T23:25:29.09456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import textstat","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:29.096637Z","iopub.execute_input":"2022-02-07T23:25:29.097395Z","iopub.status.idle":"2022-02-07T23:25:29.103789Z","shell.execute_reply.started":"2022-02-07T23:25:29.097357Z","shell.execute_reply":"2022-02-07T23:25:29.103058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Stop Words","metadata":{}},{"cell_type":"code","source":"# stop = stopwords.words('english')\n\n# not_stop_word = ['i','me','my','myself','we','our','ours','ourselves','you',\"you're\",\"you've\",\"you'll\",\"you'd\",'your','yours','yourself','yourselves',\n#                  'he','him','his','himself','she',\"she's\",'her', 'hers','herself','it',\"it's\",'its','itself','they','them','their','theirs','themselves',\n#                  'what','are','at','by','for','with','about','against', 'between','into','through', 'to','from','up','down','in','out','on','off','over',\n#                  'under','again','then','once','here','there','when', 'why','how','all','any', 'each','more','most','other','such','no','not',\n#                  'only','own','same','so','than','too','very','will','just', \"don't\",'should',\"should've\",'now','d','ll','m','o','re', 'ain','aren',\"aren't\",'couldn',\"couldn't\",]\n\n# filtered_stop_word = [i for i in stop if i not in not_stop_word]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:29.10523Z","iopub.execute_input":"2022-02-07T23:25:29.105942Z","iopub.status.idle":"2022-02-07T23:25:29.113479Z","shell.execute_reply.started":"2022-02-07T23:25:29.105905Z","shell.execute_reply":"2022-02-07T23:25:29.112685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(data, col):\n    \n    def remove_urls (vTEXT):\n        vTEXT = re.sub(r'http\\S+|www\\S+', 'URL', vTEXT)\n        return(vTEXT)\n\n    data[col] = data[col].swifter.apply(remove_urls)\n    \n    data[col] = data[col].str.replace(r\"-\", \" - \", regex=True)\n    \n    #Removing numbers\n    data[col] = data[col].swifter.apply(lambda x : ' '.join([tweet for tweet in x.split() if not tweet == '\\d*']))\n    \n        # Remove ip address\n    data[col] = data[col].str.replace(r'(([0-9]+\\.){2,}[0-9]+)',' ', regex=True)\n    # Remove website\n    data[col] = data[col].str.replace(r'https?://\\S+|www\\.\\S+', ' ', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', regex=True)\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', regex=True)\n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\'\"]+)',r' \\1 ', regex=True)    \n    # Remove multiple white spaces\n    data[col] = data[col].str.replace(r' +', ' ', regex=True)\n    # Remove html tags\n    data[col] = data[col].str.replace(r'<[^<]+?>', ' ', regex=True)\n    \n    #Code for removing slang words\n    \n    d = {'luv':'love','wud':'would','lyk':'like','wateva':'whatever','ttyl':'talk to you later',\n         'kul':'cool','fyn':'fine','omg':'oh my god!','fam':'family','bruh':'brother',\n         'cud':'could', 'fud':'food'} ## Need a huge dictionary\n        \n    data[col] = data[col].swifter.apply(lambda x : ' '.join(d[word] if word in d else word for word in x.split()))\n    data[col] = data[col].str.replace(r\"j k\", \"jk\", regex=True)\n    data[col] = data[col].str.replace(r\"e - mail\", \"email\", regex=True)\n    data[col] = data[col].str.replace(r\"\\0s\", \"0\", regex=True)\n    data[col] = data[col].str.replace(r\" 9 11 \", \" 911 \", regex=True)\n    data[col] = data[col].str.replace(r\" e g \", \" eg \", regex=True)\n    data[col] = data[col].str.replace(r\" b g \", \" bg \", regex=True)\n    data[col] = data[col].str.replace(r\" u s \", \" american \", regex=True)\n    data[col] = data[col].str.replace(r\"what's\", \"what is \", regex=True)\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=True)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \", regex=True)\n    data[col] = data[col].str.replace(r\"n't\", \" not \", regex=True)\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \", regex=True)\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=True)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=True)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=True)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=True)\n    data[col] = data[col].str.replace(r\"\\'s\", \" \", regex=True)\n    data[col] = data[col].str.replace('\\n', ' \\n ', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)', r'\\1 \\2 \\3', regex=True)\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}', r'\\1\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'([*!?\\']+)', r' \\1 ', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b', r'\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B', r'\\1\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'[ ]{2,}', ' ', regex=True).str.strip()\n    data[col] = data[col].str.replace(r'[ ]{2,}', ' ', regex=True).str.strip()\n\n    def count_urls(text):\n        return text.count('URL')\n    \n    data['URL_count'] = data[col].swifter.apply(count_urls)\n\n#     data[col] = data[col].swifter.apply(lambda x: ' '.join([word for word in x.split() if word not in (filtered_stop_word)]))\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:29.126618Z","iopub.execute_input":"2022-02-07T23:25:29.127165Z","iopub.status.idle":"2022-02-07T23:25:29.155554Z","shell.execute_reply.started":"2022-02-07T23:25:29.127128Z","shell.execute_reply":"2022-02-07T23:25:29.154766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"# pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\nf = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\nf.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:29.157942Z","iopub.execute_input":"2022-02-07T23:25:29.158236Z","iopub.status.idle":"2022-02-07T23:25:29.666553Z","shell.execute_reply.started":"2022-02-07T23:25:29.1582Z","shell.execute_reply":"2022-02-07T23:25:29.665843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:29.669785Z","iopub.execute_input":"2022-02-07T23:25:29.669984Z","iopub.status.idle":"2022-02-07T23:25:29.67987Z","shell.execute_reply.started":"2022-02-07T23:25:29.669959Z","shell.execute_reply":"2022-02-07T23:25:29.679118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.max_colwidth = 500\npd.options.display.max_rows = 50\nf[['more_toxic']].sample(50)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:29.688625Z","iopub.execute_input":"2022-02-07T23:25:29.688886Z","iopub.status.idle":"2022-02-07T23:25:29.707611Z","shell.execute_reply.started":"2022-02-07T23:25:29.68885Z","shell.execute_reply":"2022-02-07T23:25:29.706855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import swifter","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:29.729919Z","iopub.execute_input":"2022-02-07T23:25:29.730264Z","iopub.status.idle":"2022-02-07T23:25:31.987356Z","shell.execute_reply.started":"2022-02-07T23:25:29.730225Z","shell.execute_reply":"2022-02-07T23:25:31.986614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"from string import punctuation\n\ndef up_low(s):\n    u, l  = sum(1 for i in s if i.isupper()), sum(1 for i in s if i.islower())\n    if l == 0 : l = 1\n    return u/l\n\ndef up_low_word(s):\n    u, l  = sum(1 for i in s.split() if i.isupper()), sum(1 for i in s.split() if i.islower())\n    if l == 0 : l = 1\n    return u/l\n\n\n# расстояние левинштейна\n\n\ndef get_toxic_index(text, words = ['fuck', 'bitch', None]):\n    sen_list = sent_tokenize(text)\n    sen_list = [sen.lower() for sen in sen_list]\n    one_word_sum = 0\n    sent_words = len(text.split())\n    text = text.lower()\n    \n    \n    for word in words:\n        one_word_sum += text.count(word)/sent_words*np.log(1 + (len(sen_list)+1)/(sum(1 for sen in sen_list if sen.count(word))+1))\n    return one_word_sum\n\n\ndef make_features(df, target = 'text', show_fs=False):\n    \n    col_before = df.columns\n    \n    df['sentence_count'] = df[target].swifter.apply(lambda x : len(sent_tokenize(x)))\n    df['upper_on_lower'] = df[target].swifter.apply(up_low)\n    df['upper_on_lower_word'] = df[target].swifter.apply(up_low_word)\n    df['len_text'] = df[target].swifter.apply(lambda x : len(x)//30)\n    df['size_punct'] = df[target].swifter.apply(lambda x : sum(1 for i in x if i in punctuation)) # Надо сколько именно слов, не символов\n    df['size_letter'] = df[target].swifter.apply(lambda x : sum(1 for i in x if i not in punctuation))\n    df['punct_on_letter'] = df['size_punct'].values/df['size_letter'].values\n    df.drop(['size_letter', 'size_punct'], axis=1, inplace=True)\n#     df['mean_word_len'] = df[target].swifter.apply(lambda x : sum(len(i)//3 for i in x.split())/len(x.split()))\n    df['words'] = df[target].swifter.apply(lambda x : sum(1 for i in x.split())/len(x))\n    df['ques_punct_on_word'] = df[target].swifter.apply(lambda x : 100*sum(1 for i in x if i in '?')/len(x))\n    df['state_punct_on_word'] = df[target].swifter.apply(lambda x : 100*sum(1 for i in x if i in '!')/len(x))\n    df['duble_state_punct_on_word'] = df[target].swifter.apply(lambda x : 100*sum(1 for i in x if i in '!!')/len(x))\n    df['snow_punct_on_word'] = df[target].swifter.apply(lambda x : 100*sum(1 for i in x if i in '*')/len(x))\n    df['uniq_word_on_sentence'] = df[target].swifter.apply(lambda x : len(set(x.split()))/len(x.split()))*df['sentence_count'].values\n    df['haha_pattern'] = df[target].swifter.apply(lambda x : x.lower().count('hah')/len(x.split()))*df['sentence_count'].values\n    df['fuck_pattern'] = df[target].swifter.apply(lambda x : x.lower().count('fuck')/len(x.split()))*df['sentence_count'].values\n    df['off_pattern'] = df[target].swifter.apply(lambda x : x.lower().count('off')/len(x.split()))*df['sentence_count'].values\n    df['ques_pattern'] = df[target].swifter.apply(lambda x : x.count('???')/len(x.split()))*df['sentence_count'].values\n    df['you_pattern'] = df[target].swifter.apply(lambda x : x.lower().count(' you')/len(x.split()))*df['sentence_count'].values\n\n    ###############################\n    \n    you_are_patterns = ['you are', 'you ar ', 'you re ', 'u are ', ' u r ', ' u ', ' are you', ' ur ', \"you ' re\", \"you're\", 'youre',\n                        'you will', \"you'll\", \"you ' ll\", 'u will'] \n    \n    print('Первый признак через нормировку пошел')\n#     df['you_are_patterns'] = df[target].swifter.apply(lambda x : sum(len(x.split(i)) for i in you_are_patterns )/len(x.split()))*df['sentence_count'].values\n    df['you_are_patterns'] = df[target].swifter.apply(get_toxic_index, words = you_are_patterns)\n    print('Первый признак через нормировку готов')\n    \n    hide_punct_pattern = '*&%$#@'\n#     df['hide_punct_pattern'] = df[target].swifter.apply(lambda x : sum(x.count(i) for i in hide_punct_pattern )/len(x.split()))*df['sentence_count'].values\n    df['hide_punct_pattern'] = df[target].swifter.apply(get_toxic_index, words = hide_punct_pattern)\n\n    penis_pattern = ['dick','diick', 'penis', 'sock','suck', 'cock', 'balls', ' ball', 'bell', 'phallus','condone',\n                     'sucker', 'suker', 'penes', 'prick', 'pecker', 'dildo']\n#     df['penis_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in penis_pattern)/len(x.split()))*df['sentence_count'].values\n    df['penis_pattern'] = df[target].swifter.apply(get_toxic_index, words = penis_pattern)\n    \n    fucking_pattern = ['fuck', '*uck', 'f*ck', 'fu*k', 'fuc*','f**k','f ** k', 'fucck','fcuken','fcken','ruck', 'f *** ing', 'sh * t', 'sh*t','s**t', 'fukk', 'fricki',\n                       'f***', 'f ** ck', 'f * ck', 'b ** ch','b * ch']\n#     df['fucking_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in fucking_pattern)/len(x.split()))*df['sentence_count'].values  \n    df['fucking_pattern'] = df[target].swifter.apply(get_toxic_index, words = fucking_pattern)\n    \n    virgin_pattern = ['virgin', 'slut', 'whore', 'prostitut', 'bitch', 'biatch', 'hooker','pussy', 'beetch', 'bithc', 'butch']\n#     df['virgin_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in virgin_pattern)/len(x.split()))*df['sentence_count'].values \n    df['virgin_pattern'] = df[target].swifter.apply(get_toxic_index, words = virgin_pattern)\n    \n    dubble_word_pattern = ['buttseck', 'dumbass', 'asshole', 'arsehole', 'cocksucker', 'asscake', 'jackass']\n#     df['dubble_word_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in dubble_word_pattern)/len(x.split()))*df['sentence_count'].values\n    df['dubble_word_pattern'] = df[target].swifter.apply(get_toxic_index, words = dubble_word_pattern)\n    \n    funny_pattern = ['glad', 'happy', 'hilarious', 'fanny', 'humor']\n#     df['funny_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in funny_pattern)/len(x.split()))*df['sentence_count'].values\n    df['funny_pattern'] = df[target].swifter.apply(get_toxic_index, words = funny_pattern)\n    \n   # ДОбавить поправку растояния левенштена                                                                       \n    toxic_words = ['ass', 'a$$', 'hole', 'drool', 'anal', 'pedo', 'poopie', 'hose', 'berk', 'duffer', 'pillock', 'plonker', 'wally',\n                   'knobend', 'rotter', 'swine', 'blighter', ' cad', 'idiot', 'faggot', 'clown', 'tosser', 'cunt', 'arrogant', 'freak', 'dude',  \n                   'ugly', 'little', 'fat', 'liar', 'heck', 'vandal', 'nerd','stubborn']\n#     df['toxic_word_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in toxic_words)/len(x.split()))*df['sentence_count'].values \n    df['toxic_word_pattern'] = df[target].swifter.apply(get_toxic_index, words = toxic_words)    \n        \n    yeald_pattern = ['ggg','uuu','aaa', 'hhh', 'eee','ooo','!!!','???', '!?!', '!??']\n#     df['yeald_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in yeald_pattern)/len(x.split()))*df['sentence_count'].values    \n    df['yeald_pattern'] = df[target].swifter.apply(get_toxic_index, words = yeald_pattern)    \n        \n    noncence_toxic = ['poo', 'poop', 'poopy', 'butt','piss','shit','shut', 'testicles', 'bollocks', 'heresy', 'absurd', 'nonsense', 'crud','crap']\n#     df['noncence_toxic'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in noncence_toxic)/len(x.split()))*df['sentence_count'].values\n    df['noncence_toxic'] = df[target].swifter.apply(get_toxic_index, words = noncence_toxic)\n    \n    long_phase_pattern = ['fuck all', 'piss off', 'fuck off', 'get off', 'stay off', 'shut up', 'god damn', 'damn it', 'who care', 'kiss off', 'fuck–up', 'fuck up', 'bang up']\n#     df['long_phase_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in long_phase_pattern)/len(x.split()))*df['sentence_count'].values\n    df['long_phase_pattern'] = df[target].swifter.apply(get_toxic_index, words = long_phase_pattern)\n\n    child_toxic = ['holy', 'moly', 'jeepers', 'good', 'heavens', 'gosh', 'fiddlesticks', 'poppycock']\n#     df['child_toxic'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in child_toxic)/len(x.split()))*df['sentence_count'].values\n    df['child_toxic'] = df[target].swifter.apply(get_toxic_index, words = child_toxic)\n\n    more_toxic_word = ['damn', 'whore', 'hoe', 'jade', 'jerk', 'moron', \n                       'douchebag', 'dork', 'faggot', 'fag', 'queer', 'pirate', 'candyass',\n                       'finook', 'capon', 'butter', 'boy', 'banana', 'crammer', 'nancy', 'pansy', 'scumbag', 'scum', 'prat', 'brat', 'loser',\n                       'sack', 'goof', 'wanker', 'noob', 'retard', 'numbnuts', 'shitbox']\n#     df['more_toxic_word'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in more_toxic_word)/len(x.split()))*df['sentence_count'].values\n    df['more_toxic_word'] = df[target].swifter.apply(get_toxic_index, words = more_toxic_word)\n    \n    light_level_toxic  = ['arse', 'bloody', 'hell', 'bugger', 'damn', 'minger', 'sod-off']\n#     df['light_level_toxic'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in light_level_toxic)/len(x.split()))*df['sentence_count'].values\n    df['light_level_toxic'] = df[target].swifter.apply(get_toxic_index, words = light_level_toxic)\n    \n    middle_level_toxic = ['shit', 'son of a bitch', 'arsehole', 'balls', 'bint', 'bollocks', 'bullshit', 'feck', 'munter', 'pissed', 'pissed off']\n#     df['middle_level_toxic'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in middle_level_toxic)/len(x.split()))*df['sentence_count'].values\n    df['middle_level_toxic'] = df[target].swifter.apply(get_toxic_index, words = middle_level_toxic)\n    \n    hard_level_toxic  = ['bastard', 'dickhead', 'bloodclaat', 'jamaican', 'knob', 'prick', 'bellend',\n                        'minge', 'twat', 'twunt', 'beaver', 'curtains', 'beef', 'clunge', 'snatch', 'punani', 'gash']\n#     df['hard_level_toxic'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in hard_level_toxic)/len(x.split()))*df['sentence_count'].values\n    df['hard_level_toxic'] = df[target].swifter.apply(get_toxic_index, words = hard_level_toxic)\n    \n    very_hard_level_toxic  = ['cunt', 'fuck', 'motherfucker', 'motherfudger', 'mother fudger']\n#     df['very_hard_level_toxic'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in very_hard_level_toxic)/len(x.split()))*df['sentence_count'].values\n    df['very_hard_level_toxic'] = df[target].swifter.apply(get_toxic_index, words = very_hard_level_toxic)\n    \n    very_very_hard_level_toxic  = ['felch' ,'skullfuck', 'fuck puppet', 'bisnotch', 'rusty trombone']\n#     df['very_very_hard_level_toxic'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in very_very_hard_level_toxic)/len(x.split()))*df['sentence_count'].values\n    df['very_very_hard_level_toxic'] = df[target].swifter.apply(get_toxic_index, words = very_very_hard_level_toxic)\n    \n    away_patterns  = [' out ', 'away ', ' off ' ,' up ', ' down ', 'm done']\n#     df['away_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in away_patterns)/len(x.split()))*df['sentence_count'].values\n    df['away_patterns'] = df[target].swifter.apply(get_toxic_index, words = away_patterns) \n        \n    synon_toxic_words = ['jerk', 'stinker', 'heck', 'crud', 'schmuck', 'riffraff']    \n#     df['synon_toxic_words'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in synon_toxic_words)/len(x.split()))*df['sentence_count'].values\n    df['synon_toxic_words'] = df[target].swifter.apply(get_toxic_index, words = synon_toxic_words)\n    \n    lgbt_pattern = [ 'gay', 'homosexual', 'hetero', ' lgbt', 'bisexual', 'lesbian', 'queer']\n#     df['lgbt_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in lgbt_pattern)/len(x.split()))*df['sentence_count'].values\n    df['lgbt_pattern'] = df[target].swifter.apply(get_toxic_index, words = lgbt_pattern)\n    \n    descrimination_pattern = ['racist', 'nazi', 'antic', 'commie',  'discriminat', 'female', 'troll', 'abusing', 'nigger', 'nigga', 'jew',\n                            'fascist', 'haras', 'racism', 'bigot', 'gosh', 'kike', 'slave', 'victim', ]\n#     df['descrimination_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in descrimination_pattern)/len(x.split()))*df['sentence_count'].values\n    df['descrimination_pattern'] = df[target].swifter.apply(get_toxic_index, words = descrimination_pattern)\n    \n    god_pattern = ['catholic', 'god','religi', 'islam', 'bibl', 'muslim', 'atheist']\n#     df['god_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in god_pattern)/len(x.split()))*df['sentence_count'].values\n    df['god_pattern'] = df[target].swifter.apply(get_toxic_index, words = god_pattern)\n    \n    animal_pattern = ['monkey', 'horse', 'pig', 'animal', 'dog', 'bull', ' swine', ' pork', 'llama','skunk','racooon', 'raccoon', 'racoon', ' coon',\n                    'bat', 'bird', 'chicken', 'duck', 'goose', 'weasel', ' swan', 'dog', 'donkey', 'goat', ' rat', 'sheep']\n#     df['animal_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in animal_pattern)/len(x.split()))*df['sentence_count'].values\n    df['animal_pattern'] = df[target].swifter.apply(get_toxic_index, words = animal_pattern)\n    \n    slang_pattern = [' u ', ' ur ', 'cuz']\n#     df['slang_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in slang_pattern)/len(x.split()))*df['sentence_count'].values\n    df['slang_pattern'] = df[target].swifter.apply(get_toxic_index, words = slang_pattern)\n    \n    body_pattern = ['tits', 'mouth', 'body', 'face',  'head', ' boob', 'oral']  \n#     df['body_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in body_pattern)/len(x.split()))*df['sentence_count'].values\n    df['body_pattern'] = df[target].swifter.apply(get_toxic_index, words = body_pattern)\n    \n    not_good_words = ['rude', 'stuck', 'porn', 'mug', 'sex', 'shame', 'gang', 'bang', 'wanker']  \n#     df['not_good_words'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in not_good_words)/len(x.split()))*df['sentence_count'].values\n    df['not_good_words'] = df[target].swifter.apply(get_toxic_index, words = not_good_words)\n    \n    threat_pattern = ['going', 'wanna', 'will', 'threat', 'barbarian', 'savage', 'insult', 'rape',]\n#     df['threat_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in threat_pattern)/len(x.split()))*df['sentence_count'].values\n    df['threat_pattern'] = df[target].swifter.apply(get_toxic_index, words = threat_pattern)\n    \n    print('Примерно половина признаков готова!')\n    hard_threat_words = ['xenocidic', 'terrorist', 'akbar', 'dangerous', 'genocide', 'holocaust']\n#     df['hard_threat_words'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in hard_threat_words)/len(x.split()))*df['sentence_count'].values\n    df['hard_threat_words'] = df[target].swifter.apply(get_toxic_index, words = hard_threat_words)\n    \n    stop_pattern = ['delet', 'destroy', ' bann', 'block',  'stop', 'quit', 'unblock', 'attack', 'remove']\n#     df['stop_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in stop_pattern)/len(x.split()))*df['sentence_count'].values\n    df['stop_pattern'] = df[target].swifter.apply(get_toxic_index, words = stop_pattern)\n    \n    mom_pattern = [' mom ', 'mother', ' moth ', ' mum ', 'mothefucker', ' milf', ]  \n#     df['mom_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in mom_pattern)/len(x.split()))*df['sentence_count'].values\n    df['mom_pattern'] = df[target].swifter.apply(get_toxic_index, words = mom_pattern)\n    \n    brother_pattern = [' son ', 'son of', 'children', 'cousin', 'kids', 'baby', 'brother', 'father','family', 'daugher', 'sister']  \n#     df['brother_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in brother_pattern)/len(x.split()))*df['sentence_count'].values\n    df['brother_pattern'] = df[target].swifter.apply(get_toxic_index, words = brother_pattern)\n    \n    mental_pattern = ['retard', 'mind', 'mental', 'brain', 'stupid', 'silly', 'fool', 'dumb', 'meatball', 'numnut',\n                      'paranoid', 'mad', 'crazy', 'insane', 'frantic', 'reckless', 'lunatic', 'sick']  \n#     df['mental_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in mental_pattern)/len(x.split()))*df['sentence_count'].values\n    df['mental_pattern'] = df[target].swifter.apply(get_toxic_index, words = mental_pattern)\n    \n    negative_pattern = ['horrible', 'coward', 'crack','less','dart','hate', 'annoy', 'hurt', 'demon',\n                        'pain', 'dead', ' die',' death', 'satan', 'terrible', 'awful', 'spam', 'avil']\n#     df['negative_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in negative_pattern)/len(x.split()))*df['sentence_count'].values   \n    df['negative_pattern'] = df[target].swifter.apply(get_toxic_index, words = negative_pattern)\n    \n    negative_verb_pattern = ['smell', 'stink', ' reek', ' hum']\n#     df['negative_verb_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in negative_verb_pattern)/len(x.split()))*df['sentence_count'].values    \n    df['negative_verb_pattern'] = df[target].swifter.apply(get_toxic_index, words = negative_verb_pattern)\n    \n    ill_pattern = [ 'canser', 'coward', 'disease', 'slave', 'bizarre', 'anorexic', ' cry', 'obese', 'fart','garbage', 'schizophr']\n#     df['ill_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in ill_pattern)/len(x.split()))*df['sentence_count'].values\n    df['ill_pattern'] = df[target].swifter.apply(get_toxic_index, words = ill_pattern)\n    \n    crime_pattern = ['warmonger', 'pedophile', 'kill', 'murder', 'vandal', 'crime', 'fraud', 'scam', 'gulty', 'speculat']\n#     df['crime_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in crime_pattern)/len(x.split()))*df['sentence_count'].values\n    df['crime_pattern'] = df[target].swifter.apply(get_toxic_index, words = crime_pattern)\n    \n    nasty_pattern = ['nasty', 'bad ', 'black', 'filth', 'horrible','horror','suicid', 'masturb', 'disgust', 'slime', 'slur']\n#     df['nasty_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in nasty_pattern)/len(x.split()))*df['sentence_count'].values\n    df['nasty_pattern'] = df[target].swifter.apply(get_toxic_index, words = nasty_pattern)\n    \n    talkative_pattern = ['talker', 'chatterbox', 'windbag', 'babbler', 'gossip', 'jay'] \n#     df['talkative_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in talkative_pattern)/len(x.split()))*df['sentence_count'].values\n    df['talkative_pattern'] = df[target].swifter.apply(get_toxic_index, words = talkative_pattern)\n    \n    target_pattern = ['self', 'selves'] \n#     df['target_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in target_pattern)/len(x.split()))*df['sentence_count'].values\n    df['target_pattern'] = df[target].swifter.apply(get_toxic_index, words = target_pattern)\n    \n    WTF_pattern = ['what the', 'wtf', 'omfg', 'why the'] \n#     df['WTF_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in WTF_pattern)/len(x.split()))*df['sentence_count'].values\n    df['WTF_pattern'] = df[target].swifter.apply(get_toxic_index, words = WTF_pattern)\n    \n    country_pattern = ['moldavian', 'iraq', 'egypt','romanian','countr', 'israel', 'russia', 'europa', 'america', 'livin',  'arab','chinese','czech',\n                       'china', 'spain', 'germanic', 'germany', 'poland','texas', 'russkie', 'france', 'armenia', 'rebublic', 'macedonia', 'bulgar',\n                       'albania',' turk', 'london','greek', 'natioa', 'population', 'india', 'hindu', 'turk', 'saudi', 'pakistan'] \n#     df['country_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in country_pattern)/len(x.split()))*df['sentence_count'].values\n    df['country_pattern'] = df[target].swifter.apply(get_toxic_index, words = country_pattern)\n    \n    politics_pattern = ['putin', 'bush', 'abama', 'trump', 'stalin', 'kgb', 'saddam', 'hussein', 'politic', 'president', 'cult',\n                        'anarchist', 'osama bin', 'hitler', ' war', ' fbi', 'lenin', 'biden', 'liberal', 'propagand']  \n#     df['politics_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in politics_pattern)/len(x.split()))*df['sentence_count'].values\n    df['politics_pattern'] = df[target].swifter.apply(get_toxic_index, words = politics_pattern)\n    \n    NO_pattern = [' no', 'nooo', 'not','nor', 'none', 'no one', 'nobody', 'nothing', 'neither', 'nowhere', 'never', 'negative', ' un-', 'anti', 'pseudo'] \n#     df['NO_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in NO_pattern)/len(x.split()))*df['sentence_count'].values\n    df['NO_pattern'] = df[target].swifter.apply(get_toxic_index, words = NO_pattern)\n    \n    NO_verb_pattern = [\"do't\", \"do ' t\", 'doesn’t', 'isn’t', 'wasn’t', 'shouldn’t', 'wouldn’t', \"couldn’t\", \"can't\", \"can ' t\", \"cant\"] \n#     df['NO_verb_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in NO_verb_pattern)/len(x.split()))*df['sentence_count'].values\n    df['NO_verb_pattern'] = df[target].swifter.apply(get_toxic_index, words = NO_verb_pattern)\n    \n    max_pattern = [\"all!\", 'every', 'any', 'too ', 'such', 'very'] \n#     df['max_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in max_pattern)/len(x.split()))*df['sentence_count'].values\n    df['max_pattern'] = df[target].swifter.apply(get_toxic_index, words = max_pattern)\n    \n    thanks_pattern = [\"sorry\", \" thank\", 'thx', 'hello', 'hey']  \n#     df['thanks_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in thanks_pattern)/len(x.split()))*df['sentence_count'].values\n    df['thanks_pattern'] = df[target].swifter.apply(get_toxic_index, words = thanks_pattern)\n    \n    positive_pattern = [\"well\", \"nice\", 'good', 'cool'] \n#     df['positive_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in positive_pattern)/len(x.split()))*df['sentence_count'].values\n    df['positive_pattern'] = df[target].swifter.apply(get_toxic_index, words = positive_pattern)\n    \n    weapon_pattern = [\"gun\", \"weapon\", 'rocket', 'shoot', 'bomb'] \n#     df['weapon_pattern'] = df[target].swifter.apply(lambda x : sum(x.lower().count(word) for word in weapon_pattern)/len(x.split()))*df['sentence_count'].values\n    df['weapon_pattern'] = df[target].swifter.apply(get_toxic_index, words = weapon_pattern)\n\n    new_features = [i for i in df.columns if i not in col_before]\n    \n    if show_fs: print('new_features', len(new_features), new_features)\n    \n    return df, new_features\n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:31.992098Z","iopub.execute_input":"2022-02-07T23:25:31.993964Z","iopub.status.idle":"2022-02-07T23:25:32.115069Z","shell.execute_reply.started":"2022-02-07T23:25:31.993927Z","shell.execute_reply":"2022-02-07T23:25:32.11429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Датасеты","metadata":{}},{"cell_type":"code","source":"df_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n\ngp1=df_val.copy()\n\n# Create a hash function for every unique pair\ngp1['pair'] = gp1.swifter.apply(lambda x:\" \".join(sorted((x['less_toxic'],\n                                                  x['more_toxic']))),axis=1)\ngp1['pair_hash'] = gp1.pair.swifter.apply(lambda x: str(abs(hash(x)) % (10 ** 8)))\ndel gp1['pair']\n\nprint(f'No. of rows in val_data: {len(gp1)} and no. of unique sentence pairs: {len(gp1.pair_hash.drop_duplicates())}')\n\n############\ngp1['pair_cnt']=gp1.groupby(['pair_hash'])['worker'].transform(lambda x: x.count())\n\ngp1['cnt']=gp1.groupby(['pair_hash', \n                        'less_toxic',\n                        'more_toxic'])['worker'].transform(lambda x: x.count())\n\nprint(gp1[['less_toxic','more_toxic','cnt']].drop_duplicates().cnt.value_counts())\n\n# # Cases with 3 unique rating and 1 disagreement \n\ngp1[(gp1.pair_cnt == 3) & (gp1.cnt == 1)].sample(10)\n\ndf_val2 = gp1[~((gp1.pair_cnt == 3) & (gp1.cnt == 1))][['worker', 'less_toxic', 'more_toxic']]\nprint(df_val2.shape)\n\ndf_val3 = df_val2[['less_toxic', 'more_toxic']].drop_duplicates()\nprint(df_val3.shape) \n\ncleaning = True\n\nif cleaning:\n    df_val_l, _ = make_features(df_val[['worker','less_toxic']].rename(columns={'less_toxic': 'text'}), target = 'text')\n    df_val_l.drop('worker', axis=1, inplace=True)\n    df_val_l = clean(df_val_l, 'text') \n    print('теперь для more toxic')\n    df_val_m, _ = make_features(df_val[['worker','more_toxic']].rename(columns={'more_toxic': 'text'}), target = 'text')\n    df_val_m.drop('worker', axis=1, inplace=True)\n    df_val_m = clean(df_val_m, 'text') \n    \nif cleaning:\n    df_val2_l, _ = make_features(df_val2[['worker','less_toxic']].rename(columns={'less_toxic': 'text'}), target = 'text')\n    df_val2_l.drop('worker', axis=1, inplace=True)\n    df_val2_l = clean(df_val2_l, 'text') \n    \n    df_val2_m, _ = make_features(df_val2[['worker','more_toxic']].rename(columns={'more_toxic': 'text'}), target = 'text')\n    df_val2_m.drop('worker', axis=1, inplace=True)\n    df_val2_m = clean(df_val2_m, 'text')        \n    \nif cleaning:\n    df_val3['worker'] = 1 # Костыль\n    df_val3_l, _ = make_features(df_val3[['worker','less_toxic']].rename(columns={'less_toxic': 'text'}), target = 'text')\n    df_val3_l.drop('worker', axis=1, inplace=True)\n    df_val3_l = clean(df_val3_l, 'text') \n    \n    df_val3_m, _ = make_features(df_val3[['worker','more_toxic']].rename(columns={'more_toxic': 'text'}), target = 'text')\n    df_val3_m.drop('worker', axis=1, inplace=True)\n    df_val3_m = clean(df_val3_m, 'text')   \n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:32.119529Z","iopub.execute_input":"2022-02-07T23:25:32.121629Z","iopub.status.idle":"2022-02-07T23:25:32.130471Z","shell.execute_reply.started":"2022-02-07T23:25:32.121593Z","shell.execute_reply":"2022-02-07T23:25:32.128917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ✅ Model 1 Ridge + Lasso + toxic-comment-classification dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\nprint(df.shape) \ndf.sample(4)\n\n\ndf['severe_toxic'] = df.severe_toxic * 3\ndf['threat'] = df.threat * 2\n\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf['y'] = np.around (df[\"y\"].values , decimals = 4)\n\ndf.rename(columns={'comment_text': 'text'}, inplace=True)\ndf = df[['text','y']]\n# Reduce rows with 0 toxicity\ndf = pd.concat([df[df.y>0] , \n                df[df.y==0].sample(int(len(df[df.y>0])*1.5)) ], axis=0).sample(frac=1)\n\nprint(df.shape)\n\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:32.13472Z","iopub.execute_input":"2022-02-07T23:25:32.136865Z","iopub.status.idle":"2022-02-07T23:25:33.999125Z","shell.execute_reply.started":"2022-02-07T23:25:32.136832Z","shell.execute_reply":"2022-02-07T23:25:33.998386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO расстояние левинштейна\n\ndf, handle_features = make_features(df.copy(), target = 'text', show_fs = True)\n\nhandle_features += ['URL_count']\nprint(handle_features)\n# cor = df.corr()\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:25:34.000451Z","iopub.execute_input":"2022-02-07T23:25:34.000859Z","iopub.status.idle":"2022-02-07T23:38:20.900206Z","shell.execute_reply.started":"2022-02-07T23:25:34.000821Z","shell.execute_reply":"2022-02-07T23:38:20.899362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:38:20.901745Z","iopub.execute_input":"2022-02-07T23:38:20.902005Z","iopub.status.idle":"2022-02-07T23:38:20.933033Z","shell.execute_reply.started":"2022-02-07T23:38:20.901969Z","shell.execute_reply":"2022-02-07T23:38:20.932355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ncor = df[df['toxic_word_pattern'] >= 0.1].corr()\n\nf, ax = plt.subplots(figsize=(25,25))\nsns.heatmap(cor, vmin = 0.05, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:38:20.934125Z","iopub.execute_input":"2022-02-07T23:38:20.934897Z","iopub.status.idle":"2022-02-07T23:38:23.572425Z","shell.execute_reply.started":"2022-02-07T23:38:20.93486Z","shell.execute_reply":"2022-02-07T23:38:23.569262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.upper_on_lower_word.hist()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:38:23.573382Z","iopub.execute_input":"2022-02-07T23:38:23.57358Z","iopub.status.idle":"2022-02-07T23:38:23.803611Z","shell.execute_reply.started":"2022-02-07T23:38:23.573555Z","shell.execute_reply":"2022-02-07T23:38:23.802859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_features = ['text']\nhandle_features\n\n# df = df[text_features + handle_features + ['y']]\n# df","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:38:23.805324Z","iopub.execute_input":"2022-02-07T23:38:23.805906Z","iopub.status.idle":"2022-02-07T23:38:23.813433Z","shell.execute_reply.started":"2022-02-07T23:38:23.805873Z","shell.execute_reply":"2022-02-07T23:38:23.812745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.feature_selection import SelectKBest, f_regression, f_classif","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:38:23.814535Z","iopub.execute_input":"2022-02-07T23:38:23.815307Z","iopub.status.idle":"2022-02-07T23:38:23.834588Z","shell.execute_reply.started":"2022-02-07T23:38:23.815267Z","shell.execute_reply":"2022-02-07T23:38:23.833985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_txt_pipeline = Pipeline(steps=[\n    ('col_selector', ColumnSelector(cols=('text'),drop_axis=True)),\n    (\"tfidf\", TfidfVectorizer(\n        min_df = 5, max_df = 0.7, \n        norm = 'l2', analyzer = 'char_wb', ngram_range = (3, 5))),\n    (\"Ridge\", Ridge(alpha=1, max_iter = 600, random_state=42)), \n])\n\n\nlasso_transformer = Pipeline(steps=[\n    ('col_selector', ColumnSelector(cols=handle_features, drop_axis=True)),\n    (\"imputer\", StandardScaler()),\n    (\"fs\", SelectKBest(score_func=f_regression, k=60)),\n    (\"Ridge\", Lasso(random_state=42)), \n])\n\nridge_fs_transformer = Pipeline(steps=[\n    ('col_selector', ColumnSelector(cols=handle_features, drop_axis=True)),\n    (\"imputer\", StandardScaler()),\n#     (\"fs\", SelectKBest(score_func=f_regression, k=60)),\n    (\"Ridge\", Ridge(alpha=1, max_iter = 700, random_state=42)), \n])\n\n#   Для численных/бертовых - применяем SimpleImputer,\nrf_num_pipeline = Pipeline(steps=[\n#     (\"imputer\", SimpleImputer()),\n    ('col_selector', ColumnSelector(cols=handle_features,drop_axis=True)),\n    (\"imputer\", StandardScaler()),\n#     (\"fs\", SelectKBest(score_func=f_regression, k=50)),\n    (\"RF\", RandomForestRegressor(max_depth=9, n_estimators=100, min_samples_leaf = 7,\n                                 n_jobs=-1, verbose=True, random_state = 42)),     \n])\n\nfs_transformer = Pipeline(steps=[\n    ('col_selector', ColumnSelector(cols=handle_features, drop_axis=True)),\n    (\"imputer\", StandardScaler()),\n])\n\ntext_transformer = Pipeline(steps=[\n    ('col_selector', ColumnSelector(cols=('text'),drop_axis=True)),\n    (\"tfidf\", TfidfVectorizer( min_df = 5, max_df = 0.7,  norm = 'l2', analyzer = 'char_wb', ngram_range = (3, 5))),\n])\n\n# Собираем воедино трансформеры для текстовых и бертовых признаков\nbig_ridge_transformer = ColumnTransformer(transformers=[\n    (\"text\", text_transformer, text_features),\n    (\"hand_fs\", fs_transformer, handle_features),\n#     (\"Ridge\", Ridge(alpha=1, max_iter = 2000, random_state=42)), \n])\nbig_ridge_pipline = Pipeline(steps=[\n    (\"col_selector\", big_ridge_transformer),\n    (\"Ridge\", Ridge(alpha=1, max_iter = 600, random_state=42)),\n])\n\n\npipeline = VotingRegressor(n_jobs = -1, \n    estimators=[\n#                 (\"LinReg\",  Ridge(alpha=1/2 )), #LinearRegression(normalize=True)), \n#         ('lasso', Lasso(random_state=42)),\n        ('RandForest_fs', rf_num_pipeline),\n        ('Lasso_fs', lasso_transformer),\n#         (\"Ridge\", Ridge(alpha=1, max_iter = 2000, random_state=42)), \n        (\"Ridge_fs\", ridge_fs_transformer), \n        (\"Big_Ridge\", big_ridge_pipline), \n         (\"Ridge_txt\", ridge_txt_pipeline), \n               ] \n)\n\npipeline","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:38:23.835926Z","iopub.execute_input":"2022-02-07T23:38:23.836181Z","iopub.status.idle":"2022-02-07T23:38:26.973493Z","shell.execute_reply.started":"2022-02-07T23:38:23.836148Z","shell.execute_reply":"2022-02-07T23:38:26.972849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaning = True\n\nif cleaning:\n    df = clean(df.copy(), 'text')\n\npipeline.fit(df.drop('y', axis=1), df['y'])\n# pipeline.predict(df.drop('y', axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:38:26.974935Z","iopub.execute_input":"2022-02-07T23:38:26.975406Z","iopub.status.idle":"2022-02-07T23:40:33.794648Z","shell.execute_reply.started":"2022-02-07T23:38:26.975368Z","shell.execute_reply":"2022-02-07T23:40:33.793885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# parameters = {\n# #     \"tfidf__max_df\": [1],#  0.75, 0.9, 1.0),\n# #     'vect__max_features': (None, 5000, 10000, 50000),\n# #     \"tfidf__ngram_range\": [(2, 5)], # (1, 2), (2, 3), (2, 4), (2, 5)),  # unigrams or bigrams\n# #     'tfidf__use_idf': (True, False),\n# #     'tfidf__stop_words' : True, \n# #     'tfidf__norm': ('l1', 'l2'),\n#     \"RF__n_estimators\": [100],\n#     'fs__k': [55, 'all'],\n#     \"RF__max_depth\": [9],\n#     'RF__min_samples_leaf' : [5, 6],\n# #     'tfidf__min_df' : [5], # 3, 5, 7),\n# #     \"clf__alpha\": [1], # 0.3, 0.5, 0.7, 0.9, 1.0),\n# #     \"clf__penalty\": (\"l2\", \"elasticnet\"),\n# #     'cv' : 3, \n# #     'clf__max_iter': (10, 50, 80),\n# }\n# gr_pipline = num_transformer\n\n# grid_search = GridSearchCV(gr_pipline, parameters, \n#                            cv = 3,\n#                            n_jobs=-1, verbose=1)\n\n# print(\"Performing grid search...\")\n# print(\"pipeline:\", [name for name, _ in gr_pipline.steps])\n# print(\"parameters:\")\n# pprint(parameters)\n# t0 = time()\n# grid_search.fit(df.drop('y', axis=1), df['y'])\n# print(\"done in %0.3fs\" % (time() - t0))\n\n# print(\"Best score: %0.3f\" % grid_search.best_score_)\n# print(\"Best parameters set:\")\n# best_parameters = grid_search.best_estimator_.get_params()\n# for param_name in sorted(parameters.keys()):\n#     print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:40:33.796186Z","iopub.execute_input":"2022-02-07T23:40:33.796412Z","iopub.status.idle":"2022-02-07T23:40:33.802025Z","shell.execute_reply.started":"2022-02-07T23:40:33.796375Z","shell.execute_reply":"2022-02-07T23:40:33.801199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"handle_features","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:40:33.807155Z","iopub.execute_input":"2022-02-07T23:40:33.807628Z","iopub.status.idle":"2022-02-07T23:40:33.814517Z","shell.execute_reply.started":"2022-02-07T23:40:33.807584Z","shell.execute_reply":"2022-02-07T23:40:33.81385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_importance(handle_features, pipeline, height, top_n=50, square=False, model='rf'):\n    \n    fi = pd.DataFrame(index = handle_features, columns = [])\n#     for i, m in enumerate(best_model):\n#         fi[f'm_{i}'] = pipeline['Ridge'].coef_\n    if model == 'rf':\n        fi[f'm_{0}'] = pipeline['RF'].feature_importances_\n    elif model == 'ridge':\n        if square:\n            fi[f'm_{0}'] = abs(pipeline['Ridge'].coef_)\n        else:\n            fi[f'm_{0}'] = pipeline['Ridge'].coef_\n\n    fi = fi.stack().reset_index().iloc[:,[0, 2]]#.to_frame()\n    fi.columns = ['feature','importance']\n\n    cols_ord = list(fi.groupby('feature')['importance'].mean().sort_values(ascending=False).index)\n    print('Всего признаков', len(cols_ord), 'Усреднее по {}-ти моделям: '.format(1) )\n    cols_ord = cols_ord[:top_n]\n    \n    fi = fi[fi['feature'].isin(cols_ord)]\n    \n    plt.figure(figsize=(10, len(cols_ord)*height))\n    b = sns.boxplot(data=fi, y='feature', x='importance', orient='h', order=cols_ord)\n    \n    print('На график нанесено топ-{} признаков'.format(top_n))\n    return fi.groupby(by =['feature'], as_index=False)['importance'].mean().sort_values(by='importance', ascending=False)\n\n\n# rf_num_pipeline.fit(df.drop('y', axis=1), df['y'])\n# df_feats_imp = plot_importance(handle_features, rf_num_pipeline, 0.20, top_n=70, square = 0)\n# num_transformer['RF'].feature_importances_\n\n# pipeline['Ridge'].coef_","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:40:33.816448Z","iopub.execute_input":"2022-02-07T23:40:33.816637Z","iopub.status.idle":"2022-02-07T23:40:33.827719Z","shell.execute_reply.started":"2022-02-07T23:40:33.816608Z","shell.execute_reply":"2022-02-07T23:40:33.827057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate on actual val data 📈","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:43:27.056503Z","iopub.execute_input":"2022-02-02T07:43:27.057001Z","iopub.status.idle":"2022-02-02T07:45:17.41382Z","shell.execute_reply.started":"2022-02-02T07:43:27.056958Z","shell.execute_reply":"2022-02-02T07:45:17.41288Z"}}},{"cell_type":"markdown","source":"# Predict on test data","metadata":{}},{"cell_type":"code","source":"# Validate on modified val data  \n    \np1 = pipeline.predict(df_val_l)\np2 = pipeline.predict(df_val_m)\nprint('MSE',sum((p1-p2)**2)/len(p1)) \nprint('good MSE',sum(((p1-p2)*(p1<p2))**2)/len(p1)) \nprint('bad MSE',sum(((p1-p2)*(p1>p2))**2)/len(p1)) \nprint(f'Validation Accuracy from Model 1 is { np.round((p1 < p2).mean() * 100, 2)}')\n\n# Validate on modified val2 data    \n    \np1 = pipeline.predict(df_val2_l)\np2 = pipeline.predict(df_val2_m)\nprint('MSE',sum((p1-p2)**2)/len(p1)) \nprint('good MSE',sum(((p1-p2)*(p1<p2))**2)/len(p1)) \nprint('bad MSE',sum(((p1-p2)*(p1>p2))**2)/len(p1)) \nprint(f'Validation Accuracy from Model 1 is { np.round((p1 < p2).mean() * 100, 2)}')\n# Validate on modified val3 data\n    \n    \np1 = pipeline.predict(df_val3_l)\np2 = pipeline.predict(df_val3_m)\n\nprint('MSE',sum((p1-p2)**2)/len(p1)) \nprint('good MSE',sum(((p1-p2)*(p1<p2))**2)/len(p1)) \nprint('bad MSE',sum(((p1-p2)*(p1>p2))**2)/len(p1)) \n      \nprint(f'Validation Accuracy from Model 1 is { np.round((p1 < p2).mean() * 100, 2)}')\n ","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:40:33.829493Z","iopub.execute_input":"2022-02-07T23:40:33.829685Z","iopub.status.idle":"2022-02-07T23:40:33.839271Z","shell.execute_reply.started":"2022-02-07T23:40:33.829663Z","shell.execute_reply":"2022-02-07T23:40:33.838614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MSE 1.2850912490689608\n# MSE 1.1155849628182188\n# Validation Accuracy from Model 1 is 68.42\n# MSE 1.405483622204499\n# MSE 1.3218968138615883\n# Validation Accuracy from Model 1 is 75.12\n# MSE 1.2879478406265237\n# MSE 1.1925015694734544\n# MSE 0.09544627115307117\n# Validation Accuracy from Model 1 is 72.93","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:40:33.840667Z","iopub.execute_input":"2022-02-07T23:40:33.841134Z","iopub.status.idle":"2022-02-07T23:40:33.85166Z","shell.execute_reply.started":"2022-02-07T23:40:33.841099Z","shell.execute_reply":"2022-02-07T23:40:33.850963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Анализ ошибок 🤔","metadata":{}},{"cell_type":"code","source":"df_val3_l['less_toxic'] = df_val3_l['text']\ndf_val3_l['more_toxic'] = df_val3_m['text']\ndf_val3_l['less_score'] = p1\ndf_val3_l['more_score'] = p2\ndf_val3_l['delta_correct'] = p1 - p2\ndf_val3_l['correct'] = p1 < p2\n\n# df_val3_l['less_len'] = df_val3_l['less_toxic'].swifter.apply(len)\n# df_val3_l['more_len'] = df_val3_l['more_toxic'].swifter.apply(len)\n# df_val3_l['lenner'] = df_val3_l['less_len'].values - df_val3_l['more_len'].values\n\n\npd.options.display.max_colwidth = 800\npd.options.display.max_rows = 80\n\ndf_val3_l[['less_toxic','more_toxic', 'correct','less_score', 'more_score','delta_correct']].sort_values('delta_correct', ascending=False).head(2000).sample(50)     ","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:40:33.853059Z","iopub.execute_input":"2022-02-07T23:40:33.853461Z","iopub.status.idle":"2022-02-07T23:40:33.863834Z","shell.execute_reply.started":"2022-02-07T23:40:33.85343Z","shell.execute_reply":"2022-02-07T23:40:33.863118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.scatter(df_val3_l.lenner, df_val3_l.delta_correct)\n# sns.scatterplot(df_val3_l, y = 'lenner', x = \"delta_correct\")\n# df_val3_l_tmp = df_val3_l\n# df_val3_l_tmp['lenner'] = abs(df_val3_l.lenner.values)\n# df_val3_l_tmp['delta_correct'] = abs(df_val3_l.delta_correct.values)\n# df_val3_l_tmp.plot.scatter(x= 'lenner', y =\"delta_correct\")\n\n\n# TODO убрать скоррелированные фичи!!!","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:40:33.865116Z","iopub.execute_input":"2022-02-07T23:40:33.866405Z","iopub.status.idle":"2022-02-07T23:40:33.874439Z","shell.execute_reply.started":"2022-02-07T23:40:33.866371Z","shell.execute_reply":"2022-02-07T23:40:33.873683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\nif cleaning:\n    \n    df_test, _ = make_features(df_test, target = 'text')\n    df_test = clean(df_test, 'text')\n    coms_id = df_test.comment_id.values\n    df_test.drop('comment_id', axis=1, inplace=True)\n\nm1_preds = pipeline.predict(df_test)\n\ndf_test['score_1'] = rankdata(m1_preds, method='ordinal') # m1_preds\ndf_test['comment_id'] = coms_id\n\n\nwith open(\"pipeline.pkl\", \"wb\") as f:\n    pickle.dump(pipeline, f)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:40:33.883634Z","iopub.execute_input":"2022-02-07T23:40:33.884157Z","iopub.status.idle":"2022-02-07T23:43:20.296602Z","shell.execute_reply.started":"2022-02-07T23:40:33.884124Z","shell.execute_reply":"2022-02-07T23:43:20.295852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ✅ Model 2 - Ridge + regression-based data V2","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-regression-based-data/train_data_version2.csv\")[['text', 'y']]\n\nif cleaning:\n    \n#     df, _ = make_features(df[['text','y']], target = 'text')\n    df = clean(df, 'text')\n    \nprint(df.shape)\n\ny=np.around(df[\"y\"].values, decimals = 2)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:43:20.298252Z","iopub.execute_input":"2022-02-07T23:43:20.298512Z","iopub.status.idle":"2022-02-07T23:43:59.023981Z","shell.execute_reply.started":"2022-02-07T23:43:20.298479Z","shell.execute_reply":"2022-02-07T23:43:59.023216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:43:59.025514Z","iopub.execute_input":"2022-02-07T23:43:59.025775Z","iopub.status.idle":"2022-02-07T23:43:59.029152Z","shell.execute_reply.started":"2022-02-07T23:43:59.025741Z","shell.execute_reply":"2022-02-07T23:43:59.028321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_pipeline_1 = Pipeline(steps=[\n    ('col_selector', ColumnSelector(cols=('text'), drop_axis=True)),\n    (\"tfidf\", TfidfVectorizer(min_df = 1, max_df = 0.7, analyzer = 'char_wb', ngram_range = (2, 5))),\n    (\"Ridge\", Ridge(alpha=1, max_iter = 600, random_state=42)), \n])\n\n# # #   Для численных/бертовых - применяем SimpleImputer,\n# # num_transformer = Pipeline(steps=[\n# #     ('col_selector', ColumnSelector(cols=handle_features,drop_axis=True)),\n# #     (\"imputer\", StandardScaler()),\n# #     (\"fs\", SelectKBest(score_func=f_regression, k=50)),\n# #     (\"RF\", RandomForestRegressor(max_depth=8, n_estimators=100, min_samples_leaf = 7,\n# #                                  n_jobs=-1, verbose=True, random_state = 42)), \n# # ])\n\n# # # Собираем воедино трансформеры для текстовых и ручных признаков\n# # data_transformer = ColumnTransformer(transformers=[\n# #     (\"text_fs\", ridge_pipline, text_features),\n# #     (\"bert_fs\", num_transformer, handle_features),\n# # ])\n\n# # big_ridge_pipline = Pipeline(steps=[\n# #     ('col_selector', ColumnSelector(cols=handle_features,drop_axis=True)),\n# #     (\"imputer\", StandardScaler()),\n# #     (\"fs\", SelectKBest(score_func=f_regression, k=50)),\n# #     (\"RF\", RandomForestRegressor(max_depth=8, n_estimators=100, min_samples_leaf = 7,\n# #                                  n_jobs=-1, verbose=True, random_state = 42)), \n# # ])\n\n    \n# # ridge_pipeline_1 = VotingRegressor(\n# #     estimators=[\n# # #                 (\"LinReg\",  Ridge(alpha=1/2 )), #LinearRegression(normalize=True)), \n# # #         ('lasso', Lasso(random_state=42)),\n# #         ('lasso', num_transformer),\n# # #         (\"Ridge\", Ridge(alpha=1, max_iter = 600, random_state=42)), \n# #         (\"Ridge\", ridge_pipline), \n# #                ] \n# # )\n# # ridge_pipeline_1\n\n\n# ridge_txt_pipeline = Pipeline(steps=[\n#     ('col_selector', ColumnSelector(cols=('text'),drop_axis=True)),\n#     (\"tfidf\", TfidfVectorizer(analyzer='char_wb', max_df=0.95, min_df=13, ngram_range=(2, 5))),\n#     (\"Ridge\", Ridge(alpha=1, random_state=42)), \n# ])\n\n\n# lasso_transformer = Pipeline(steps=[\n#     ('col_selector', ColumnSelector(cols=handle_features, drop_axis=True)),\n#     (\"imputer\", StandardScaler()),\n# #     (\"fs\", SelectKBest(score_func=f_regression, k=60)),\n#     (\"Ridge\", Lasso(random_state=42)), \n# ])\n\n# ridge_fs_transformer = Pipeline(steps=[\n#     ('col_selector', ColumnSelector(cols=handle_features, drop_axis=True)),\n#     (\"imputer\", StandardScaler()),\n# #     (\"fs\", SelectKBest(score_func=f_regression, k=60)),\n#     (\"Ridge\", Ridge(alpha=1, max_iter = 400, random_state=42)), \n# ])\n\n# #   Для численных/бертовых - применяем SimpleImputer,\n# rf_num_pipeline = Pipeline(steps=[\n# #     (\"imputer\", SimpleImputer()),\n#     ('col_selector', ColumnSelector(cols=handle_features,drop_axis=True)),\n#     (\"imputer\", StandardScaler()),\n# #     (\"fs\", SelectKBest(score_func=f_regression, k=50)),\n#     (\"RF\", RandomForestRegressor(max_depth=8, n_estimators=100, min_samples_leaf = 7,\n#                                  n_jobs=-1, verbose=True, random_state = 42)),     \n# ])\n\n# fs_transformer = Pipeline(steps=[\n#     ('col_selector', ColumnSelector(cols=handle_features, drop_axis=True)),\n#     (\"imputer\", StandardScaler()),\n# ])\n\n# text_transformer = Pipeline(steps=[\n#     ('col_selector', ColumnSelector(cols=('text'),drop_axis=True)),\n#     (\"tfidf\", TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=1, ngram_range=(2, 5))),\n# ])\n\n# # Собираем воедино трансформеры для текстовых и бертовых признаков\n# big_ridge_transformer = ColumnTransformer(transformers=[\n#     (\"text\", text_transformer, text_features),\n#     (\"hand_fs\", fs_transformer, handle_features),\n# #     (\"Ridge\", Ridge(alpha=1, max_iter = 500, random_state=42)), \n# ])\n# big_ridge_pipline = Pipeline(steps=[\n#     (\"col_selector\", big_ridge_transformer),\n#     (\"Ridge\", Ridge(alpha=1, max_iter = 500, random_state=42)),\n# ])\n\n    \n# ridge_pipeline_1 = VotingRegressor(n_jobs = -1, \n#     estimators=[\n#         ('RandForest_fs', rf_num_pipeline),\n#         ('Lasso_fs', lasso_transformer),\n#         (\"Ridge_fs\", ridge_fs_transformer), \n#         (\"Big_Ridge\", big_ridge_pipline), \n#         (\"Ridge_txt\", ridge_txt_pipeline), \n#                ] \n# )\n\n\nridge_pipeline_1.fit(df.drop('y', axis=1), y)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:43:59.030759Z","iopub.execute_input":"2022-02-07T23:43:59.031273Z","iopub.status.idle":"2022-02-07T23:44:46.690677Z","shell.execute_reply.started":"2022-02-07T23:43:59.031229Z","shell.execute_reply":"2022-02-07T23:44:46.689874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ridge_txt_pipeline","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:44:46.692179Z","iopub.execute_input":"2022-02-07T23:44:46.692745Z","iopub.status.idle":"2022-02-07T23:44:46.696565Z","shell.execute_reply.started":"2022-02-07T23:44:46.692707Z","shell.execute_reply":"2022-02-07T23:44:46.695829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:44:46.698082Z","iopub.execute_input":"2022-02-07T23:44:46.699562Z","iopub.status.idle":"2022-02-07T23:44:46.707358Z","shell.execute_reply.started":"2022-02-07T23:44:46.699505Z","shell.execute_reply":"2022-02-07T23:44:46.70649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation Accuracy from Model 1 is 67.31 # Best Ridge only\n# Validation Accuracy from Model 1 is 73.49\n# Validation Accuracy from Model 1 is 71.38\n\n\n  \np1 = ridge_pipeline_1.predict(df_val_l)\np2 = ridge_pipeline_1.predict(df_val_m)\n\nprint(f'Validation Accuracy from Model 1 is { np.round((p1 < p2).mean() * 100, 2)}')\n        \np1 = ridge_pipeline_1.predict(df_val2_l)\np2 = ridge_pipeline_1.predict(df_val2_m)\n\nprint(f'Validation Accuracy from Model 1 is { np.round((p1 < p2).mean() * 100, 2)}')\n    \np1 = ridge_pipeline_1.predict(df_val3_l)\np2 = ridge_pipeline_1.predict(df_val3_m)\n\nprint(f'Validation Accuracy from Model 1 is { np.round((p1 < p2).mean() * 100, 2)}')\n \n    \n# Validation Accuracy from Model 1 is 65.93 onli ridge\n# Validation Accuracy from Model 1 is 71.52\n# Validation Accuracy from Model 1 is 69.54\n\n\n# Validation Accuracy from Model 1 is 67.39\n# Validation Accuracy from Model 1 is 73.61\n# Validation Accuracy from Model 1 is 71.5","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:44:46.708699Z","iopub.execute_input":"2022-02-07T23:44:46.709426Z","iopub.status.idle":"2022-02-07T23:44:46.720708Z","shell.execute_reply.started":"2022-02-07T23:44:46.709384Z","shell.execute_reply":"2022-02-07T23:44:46.719839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jr_preds=ridge_pipeline_1.predict(df_test)\ndf_test['score_2']=rankdata(jr_preds, method='ordinal') \n\nwith open(\"ridge_pipeline_1.pkl\", \"wb\") as f:\n    pickle.dump(ridge_pipeline_1, f)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:44:46.723924Z","iopub.execute_input":"2022-02-07T23:44:46.72608Z","iopub.status.idle":"2022-02-07T23:44:54.527757Z","shell.execute_reply.started":"2022-02-07T23:44:46.726035Z","shell.execute_reply":"2022-02-07T23:44:54.52703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ✅ Model 3 - Ridge + Ruddit dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/d/rajkumarl/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")[['txt', 'offensiveness_score']].rename(columns={'txt': 'text', 'offensiveness_score' : 'y'})[['text', 'y']]\n\nif cleaning:\n#     df, _ = make_features(df[['text','y']], target = 'text')\n    df = clean(df, 'text')\n\nprint(f\"rud_df:{df.shape}\")\n\ny = np.around (df[\"y\"].values, decimals = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:44:54.529245Z","iopub.execute_input":"2022-02-07T23:44:54.529509Z","iopub.status.idle":"2022-02-07T23:44:56.715407Z","shell.execute_reply.started":"2022-02-07T23:44:54.529472Z","shell.execute_reply":"2022-02-07T23:44:56.714694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nridge_pipeline_2 = Pipeline(steps=[\n    ('col_selector', ColumnSelector(cols=('text'),drop_axis=True)),\n    (\"tfidf\", TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=3, ngram_range=(3, 4))),\n    (\"Ridge\", Ridge(alpha=1, max_iter = 600, random_state=42)), \n])\n\n\n# lasso_transformer = Pipeline(steps=[\n#     ('col_selector', ColumnSelector(cols=handle_features, drop_axis=True)),\n#     (\"imputer\", StandardScaler()),\n# #     (\"fs\", SelectKBest(score_func=f_regression, k=60)),\n#     (\"Ridge\", Lasso(random_state=42)), \n# ])\n\n# ridge_fs_transformer = Pipeline(steps=[\n#     ('col_selector', ColumnSelector(cols=handle_features, drop_axis=True)),\n#     (\"imputer\", StandardScaler()),\n# #     (\"fs\", SelectKBest(score_func=f_regression, k=60)),\n#     (\"Ridge\", Ridge(alpha=1, max_iter = 600, random_state=42)), \n# ])\n\n# #   Для численных/бертовых - применяем SimpleImputer,\n# rf_num_pipeline = Pipeline(steps=[\n# #     (\"imputer\", SimpleImputer()),\n#     ('col_selector', ColumnSelector(cols=handle_features,drop_axis=True)),\n#     (\"imputer\", StandardScaler()),\n# #     (\"fs\", SelectKBest(score_func=f_regression, k=50)),\n#     (\"RF\", RandomForestRegressor(max_depth=9, n_estimators=80, min_samples_leaf = 3,\n#                                  n_jobs=-1, verbose=True, random_state = 42)),     \n# ])\n\n# fs_transformer = Pipeline(steps=[\n#     ('col_selector', ColumnSelector(cols=handle_features, drop_axis=True)),\n#     (\"imputer\", StandardScaler()),\n# ])\n\n# text_transformer = Pipeline(steps=[\n#     ('col_selector', ColumnSelector(cols=('text'),drop_axis=True)),\n#     (\"tfidf\", TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=3, ngram_range=(3, 4))),\n# ])\n\n# # Собираем воедино трансформеры для текстовых и бертовых признаков\n# big_ridge_transformer = ColumnTransformer(transformers=[\n#     (\"text\", text_transformer, text_features),\n#     (\"hand_fs\", fs_transformer, handle_features),\n# #     (\"Ridge\", Ridge(alpha=1, max_iter = 600, random_state=42)), \n# ])\n# big_ridge_pipline = Pipeline(steps=[\n#     (\"col_selector\", big_ridge_transformer),\n#     (\"Ridge\", Ridge(alpha=1, max_iter = 600, random_state=42)),\n# ])\n\n    \n# ridge_pipeline_2 = VotingRegressor(n_jobs = -1, \n#     estimators=[\n#         ('RandForest_fs', rf_num_pipeline),\n#         ('Lasso_fs', lasso_transformer),\n#         (\"Ridge_fs\", ridge_fs_transformer), \n#         (\"Big_Ridge\", big_ridge_pipline), \n#          (\"Ridge_txt\", ridge_txt_pipeline), \n#                ] \n# )\n\n\nridge_pipeline_2.fit(df.drop('y', axis=1), y)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:44:56.716885Z","iopub.execute_input":"2022-02-07T23:44:56.717319Z","iopub.status.idle":"2022-02-07T23:44:58.347154Z","shell.execute_reply.started":"2022-02-07T23:44:56.717281Z","shell.execute_reply":"2022-02-07T23:44:58.346392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n  \np1 = ridge_pipeline_2.predict(df_val_l)\np2 = ridge_pipeline_2.predict(df_val_m)\n\nprint(f'Validation Accuracy from Model 1 is { np.round((p1 < p2).mean() * 100, 2)}')\n        \np1 = ridge_pipeline_2.predict(df_val2_l)\np2 = ridge_pipeline_2.predict(df_val2_m)\n\nprint(f'Validation Accuracy from Model 1 is { np.round((p1 < p2).mean() * 100, 2)}')\n    \np1 = ridge_pipeline_2.predict(df_val3_l)\np2 = ridge_pipeline_2.predict(df_val3_m)\n\nprint(f'Validation Accuracy from Model 1 is { np.round((p1 < p2).mean() * 100, 2)}')\n\n# # Validation Accuracy from Model 1 is 63.01\n# # Validation Accuracy from Model 1 is 67.48\n# # Validation Accuracy from Model 1 is 65.78\n\n\n# # Validation Accuracy from Model 1 is 62.94\n# # Validation Accuracy from Model 1 is 67.41\n# # Validation Accuracy from Model 1 is 65.73","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:44:58.357548Z","iopub.execute_input":"2022-02-07T23:44:58.358398Z","iopub.status.idle":"2022-02-07T23:44:58.368537Z","shell.execute_reply.started":"2022-02-07T23:44:58.358362Z","shell.execute_reply":"2022-02-07T23:44:58.367871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rud_preds=ridge_pipeline_2.predict(df_test)\ndf_test['score_3']=rankdata(rud_preds, method='ordinal') \n\nwith open(\"ridge_pipeline_2.pkl\", \"wb\") as f:\n    pickle.dump(ridge_pipeline_2, f)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:44:58.371385Z","iopub.execute_input":"2022-02-07T23:44:58.372025Z","iopub.status.idle":"2022-02-07T23:45:02.259231Z","shell.execute_reply.started":"2022-02-07T23:44:58.371965Z","shell.execute_reply":"2022-02-07T23:45:02.258471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Submission","metadata":{}},{"cell_type":"code","source":"df_test['score']= df_test['score_1']*0.2 + df_test['score_2']*0.4 + df_test['score_3']*0.3\ndf_test['score'] =rankdata( df_test['score'].values, method='ordinal') \ndf_test[['comment_id','score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T23:45:02.26047Z","iopub.execute_input":"2022-02-07T23:45:02.260738Z","iopub.status.idle":"2022-02-07T23:45:02.286839Z","shell.execute_reply.started":"2022-02-07T23:45:02.260704Z","shell.execute_reply":"2022-02-07T23:45:02.286222Z"},"trusted":true},"execution_count":null,"outputs":[]}]}