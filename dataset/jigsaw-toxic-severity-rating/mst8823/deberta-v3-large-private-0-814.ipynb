{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## deberta-v3-large \\[Private=0.814, CV=0.7010]\n\n- Multi Task Learning\n- jigsaw 1st dataset \\(undersampling\\)","metadata":{}},{"cell_type":"code","source":"\"\"\"\nJigsaw first: targets\ndeberta-v3-large\nRMSE\nDropout=0.0\n\"\"\"\nclass Config:\n    author = \"mst8823\"\n    wandb_entity = \"mst8823\"\n    \n    competition = \"jigsaw-toxic-severity-rating\"\n    name = \"Exp-006-deberta-v3-large-Jigsaw1-Multi\"\n    debug = False\n    inference_only = True\n    use_pretrain_model = False\n    target_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n    \n    model_name = \"microsoft/deberta-v3-large\"\n    hidden_size = 1024\n    head = 256\n    tail = 0\n    max_length = head + tail\n\n    n_fold = 5\n    trn_fold = [0, 1, 2, 3, 4]\n    seed = 2022\n\n    max_epochs = 4\n    gradient_clip_val = 100\n    accumulate_grad_batches = 2\n    early_stopping = False\n    optimizer = dict(\n        optimizer=\"AdamW\", \n        lr=1e-5, \n        weight_decay=1e-5\n        )\n    scheduler = dict(\n        interval = \"step\",\n        scheduler=\"get_cosine_schedule_with_warmup\",\n        num_warmup_steps=200, \n        num_cycles=0.5)\n    \n    train_batch_size = 4\n    valid_batch_size = 8\n    num_workers = 4\n    resume_from_checkpoint = None\n\n    colab_dir = \"/content/drive/Shareddrives/Jigsaw-Rate-Severity-of-Toxic-Comments\"\n    drive_path = colab_dir + f\"/{author}\"\n    api_path = drive_path + \"/kaggle.json\"\n\n    upload_from_colab = False\n    kaggle_dataset_path = \"../input/exp-006-deberta-v3-large-jigsaw1-multi\"\n\n    \"\"\"\n    - step scheduler example\n    scheduler = dict(\n        interval = \"step\",\n        scheduler=\"get_cosine_schedule_with_warmup\",\n        num_warmup_steps=256, \n        num_cycles=0.5)\n\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-01-31T09:40:51.001379Z","iopub.execute_input":"2022-01-31T09:40:51.001644Z","iopub.status.idle":"2022-01-31T09:40:51.011596Z","shell.execute_reply.started":"2022-01-31T09:40:51.001613Z","shell.execute_reply":"2022-01-31T09:40:51.010403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport sys\nimport logging\nimport shutil\nimport json\nimport datetime\nimport requests\nimport itertools\nimport functools\nimport warnings\nimport joblib\nimport gc\nimport random\nimport string\nimport re\nimport collections\n\nimport pandas as pd\nimport numpy as np\nimport nltk\n\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom scipy.special import softmax\nfrom bs4 import BeautifulSoup\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam, AdamW\nfrom torch.optim.lr_scheduler import (\n    CosineAnnealingWarmRestarts,\n    CosineAnnealingLR,\n    MultiStepLR, \n    ReduceLROnPlateau\n    )\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-01-31T09:40:51.013368Z","iopub.execute_input":"2022-01-31T09:40:51.013976Z","iopub.status.idle":"2022-01-31T09:40:51.023823Z","shell.execute_reply.started":"2022-01-31T09:40:51.013939Z","shell.execute_reply":"2022-01-31T09:40:51.023101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =========================\n# Utils\n# =========================\nclass Logger:\n    \"\"\" ref) https://github.com/ghmagazine/kagglebook/blob/master/ch04-model-interface/code/util.py\"\"\"\n    def __init__(self, path):\n        self.general_logger = logging.getLogger(path)\n        stream_handler = logging.StreamHandler()\n        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n        if len(self.general_logger.handlers) == 0:\n            self.general_logger.addHandler(stream_handler)\n            self.general_logger.addHandler(file_general_handler)\n            self.general_logger.setLevel(logging.INFO)\n\n    def info(self, message):\n        # display time\n        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n\n    @staticmethod\n    def now_string():\n        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n\n\ndef seed_everything(seed=2022):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef setup(cfg):\n    cfg.COLAB = \"google.colab\" in sys.modules\n    if cfg.COLAB:\n        print(\"This environment is Google Colab\")\n        \n        # mount\n        from google.colab import drive\n        if not os.path.isdir(\"/content/drive\"):\n            drive.mount('/content/drive') \n        \n        # import library\n        ! pip install --quiet pytorch_lightning\n        ! pip install --quiet transformers\n        ! pip install --quiet wandb\n        ! pip install --quiet sentencepiece\n\n        # use kaggle api (need kaggle token)\n        f = open(cfg.api_path, 'r')\n        json_data = json.load(f) \n        os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n        os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n        \n        # set dirs\n        cfg.DRIVE = cfg.drive_path\n        cfg.EXP = (cfg.name if cfg.name is not None \n            else requests.get(\"http://172.28.0.2:9000/api/sessions\").json()[0][\"name\"][:-6])\n        cfg.INPUT = os.path.join(cfg.DRIVE, \"Input\")\n        cfg.OUTPUT = os.path.join(cfg.DRIVE, \"Output\")\n        cfg.SUBMISSION = os.path.join(cfg.DRIVE, \"Submission\")\n        cfg.OUTPUT_EXP = os.path.join(cfg.OUTPUT, cfg.EXP) \n        cfg.EXP_MODEL = os.path.join(cfg.OUTPUT_EXP, \"model\")\n        cfg.EXP_FIG = os.path.join(cfg.OUTPUT_EXP, \"fig\")\n        cfg.EXP_PREDS = os.path.join(cfg.OUTPUT_EXP, \"preds\")\n\n        # input data\n        cfg.INPUT_JIGSAW_01 = os.path.join(cfg.INPUT, \"jigsaw-toxic-comment-classification-challenge\")\n        cfg.INPUT_JIGSAW_02 = os.path.join(cfg.INPUT, \"jigsaw-unintended-bias-in-toxicity-classification\")\n        cfg.INPUT_JIGSAW_03 = os.path.join(cfg.INPUT, \"jigsaw-multilingual-toxic-comment-classification\")\n        cfg.INPUT_JIGSAW_04 = os.path.join(cfg.INPUT, \"jigsaw-toxic-severity-rating\")\n        cfg.INPUT_RUDDIT = os.path.join(cfg.INPUT, \"ruddit-jigsaw-dataset\")\n        cfg.jigsaw_inputs = [cfg.INPUT_JIGSAW_01, cfg.INPUT_JIGSAW_02, cfg.INPUT_JIGSAW_03, cfg.INPUT_JIGSAW_04, \n                             cfg.INPUT_RUDDIT]\n\n        # make dirs\n        for d in [cfg.INPUT, cfg.SUBMISSION, cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS] + cfg.jigsaw_inputs:\n            os.makedirs(d, exist_ok=True)\n\n        if not os.path.isfile(os.path.join(cfg.INPUT_JIGSAW_04, \"comments_to_score.csv\")):\n            print(\"load dataset\")\n            ! pip install --upgrade --force-reinstall --no-deps kaggle\n            ! kaggle competitions download -c jigsaw-toxic-comment-classification-challenge -p $cfg.INPUT_JIGSAW_01 \n            ! kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification -p $cfg.INPUT_JIGSAW_02 \n            ! kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification -p $cfg.INPUT_JIGSAW_03 \n            ! kaggle competitions download -c jigsaw-toxic-severity-rating -p $cfg.INPUT_JIGSAW_04 \n            ! kaggle datasets download -d rajkumarl/ruddit-jigsaw-dataset -p $cfg.INPUT_RUDDIT\n\n            for input_path in cfg.jigsaw_inputs:\n                filepath = f'{input_path}/{input_path.split(\"/\")[-1]}'\n                ! unzip -d $input_path $filepath\n\n    else:\n        print(\"This environment is Kaggle Kernel\")\n        if not cfg.inference_only:\n            ! pip install --quiet pytorch_lightning==1.5.8 \n\n        # set dirs\n        cfg.INPUT = f\"../input\"\n\n        # input data\n        cfg.INPUT_JIGSAW_01 = os.path.join(cfg.INPUT, \"jigsaw-toxic-comment-classification-challenge\")\n        cfg.INPUT_JIGSAW_02 = os.path.join(cfg.INPUT, \"jigsaw-unintended-bias-in-toxicity-classification\")\n        cfg.INPUT_JIGSAW_03 = os.path.join(cfg.INPUT, \"jigsaw-multilingual-toxic-comment-classification\")\n        cfg.INPUT_JIGSAW_04 = os.path.join(cfg.INPUT, \"jigsaw-toxic-severity-rating\")\n        cfg.INPUT_RUDDIT = os.path.join(cfg.INPUT, \"ruddit-jigsaw-dataset\")\n        cfg.jigsaw_inputs = [cfg.INPUT_JIGSAW_01, cfg.INPUT_JIGSAW_02, cfg.INPUT_JIGSAW_03, cfg.INPUT_JIGSAW_04, \n                             cfg.INPUT_RUDDIT]\n\n        cfg.EXP = cfg.OUTPUT_EXP = \"./\"\n        if cfg.kaggle_dataset_path is not None:\n            cfg.EXP_MODEL = os.path.join(cfg.kaggle_dataset_path, \"model\")\n        else:\n            cfg.EXP_MODEL = os.path.join(cfg.EXP, \"model\")\n\n        cfg.SUBMISSION = \"./\"\n        cfg.EXP_FIG = os.path.join(cfg.EXP, \"fig\")\n        cfg.EXP_PREDS = os.path.join(cfg.EXP, \"preds\")\n\n        # make dirs\n        make_dirs = [cfg.EXP_FIG, cfg.EXP_PREDS]\n        if not cfg.inference_only:\n            make_dirs.append(cfg.EXP_MODEL)\n        for d in make_dirs:\n            os.makedirs(d, exist_ok=True)\n\n    # set device    \n    cfg.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    warnings.filterwarnings(\"ignore\")\n    seed_everything(cfg.seed)\n\n    cfg.logger = Logger(cfg.OUTPUT_EXP)\n\n    return cfg\n\n\n# =========================\n# SetUp\n# =========================\nConfig = setup(Config)\n\n# 2nd import\nimport pytorch_lightning as pl\nimport wandb\n\nfrom transformers import (AutoConfig, AutoModel, AutoTokenizer)\nfrom transformers import (get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup)\n\n# wandb setting\nif not Config.COLAB:\n    if  not Config.inference_only:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        api_key = user_secrets.get_secret(\"WANDB_API\")\n        wandb.login(key=api_key)\nelse:\n    wandb.login()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-31T09:40:51.025358Z","iopub.execute_input":"2022-01-31T09:40:51.025805Z","iopub.status.idle":"2022-01-31T09:40:51.287504Z","shell.execute_reply.started":"2022-01-31T09:40:51.025765Z","shell.execute_reply":"2022-01-31T09:40:51.286566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"# =============================\n# Dataset\n# =============================\nclass JigsawTrainDataset(Dataset):\n    def __init__(self, cfg, df, tokenizer, text_col):\n        self.cfg = cfg\n        self.comment_text = df[text_col].values\n        self.targets = df[cfg.target_cols].values\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.comment_text)\n    \n    def __getitem__(self, idx):\n\n        text = str(self.comment_text[idx])\n        inputs = prepare_input(self.cfg, text, self.tokenizer)\n        targets = torch.tensor(self.targets[idx]).float()\n\n        return inputs, targets\n\n\nclass JigsawTestDataset(Dataset):\n    def __init__(self, cfg, df, tokenizer, text_col):\n        self.cfg = cfg\n        self.comment_text = df[text_col].fillna(\"none\").values\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.comment_text)\n    \n    def __getitem__(self, idx):\n        text = str(self.comment_text[idx])\n        inputs = prepare_input(self.cfg, text, self.tokenizer)\n        return inputs\n\n\ndef prepare_input(cfg, text, tokenizer):\n    if cfg.tail == 0:\n        inputs = tokenizer.encode_plus(\n            text, \n            return_tensors=None, \n            add_special_tokens=True, \n            max_length=cfg.max_length,\n            pad_to_max_length=True,\n            truncation=True)\n        \n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n\n    else:\n        inputs = tokenizer.encode_plus(\n            text,\n            return_tensors=None, \n            add_special_tokens=True, \n            truncation=True)\n        \n        for k, v in inputs.items():\n            v_length = len(v)\n            if v_length > cfg.max_length:\n                v = np.hstack([v[:cfg.head], v[-cfg.tail:]])\n\n            if k == 'input_ids':\n                new_v = np.ones(cfg.max_length) * tokenizer.pad_token_id\n\n            else:\n                new_v = np.zeros(cfg.max_length)\n\n            new_v[:v_length] = v \n            inputs[k] = torch.tensor(new_v, dtype=torch.long)\n\n    return inputs\n\n\nclass JigsawDataModule(pl.LightningDataModule):\n    def __init__(self, cfg, tokenizer, train_df, valid_df, text_col):\n        super(JigsawDataModule).__init__()\n\n        self.cfg = cfg\n        self.text_col = text_col\n        self.tokenizer = tokenizer\n        self.train_df = train_df\n        self.valid_df = valid_df\n\n        self.train_dataset = None\n        self.val_dataset = None\n\n    def setup(self, stage=None):\n        self.train_dataset = JigsawTrainDataset(\n            cfg=self.cfg, df=self.train_df, tokenizer=self.tokenizer, text_col=self.text_col)\n        self.val_dataset = JigsawTrainDataset(\n            cfg=self.cfg, df=self.valid_df, tokenizer=self.tokenizer, text_col=self.text_col)\n        \n    def train_dataloader(self):\n        train_dataloader = DataLoader(\n            self.train_dataset, \n            batch_size=self.cfg.train_batch_size, \n            shuffle=True, \n            num_workers=self.cfg.num_workers, \n            pin_memory=True, \n            drop_last=True)\n        \n        return train_dataloader\n\n    def val_dataloader(self):\n        val_dataloader = DataLoader(\n            self.val_dataset,\n            batch_size=self.cfg.valid_batch_size,\n            shuffle=False,\n            num_workers=self.cfg.num_workers, \n            pin_memory=True, \n            drop_last=False)\n\n        return val_dataloader","metadata":{"execution":{"iopub.status.busy":"2022-01-31T09:40:51.289908Z","iopub.execute_input":"2022-01-31T09:40:51.290217Z","iopub.status.idle":"2022-01-31T09:40:51.309852Z","shell.execute_reply.started":"2022-01-31T09:40:51.290178Z","shell.execute_reply":"2022-01-31T09:40:51.309173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# =============================\n# Model\n# =============================\ndef get_optimizer(cfg, parameters):\n    opt = cfg.optimizer\n    if opt[\"optimizer\"] == \"AdamW\":\n        optimizer = AdamW(\n            parameters,\n            lr=opt[\"lr\"],\n            weight_decay=opt[\"weight_decay\"]\n            )\n    \n    elif opt[\"optimizer\"] == \"Adam\":\n        optimizer = Adam(\n            parameters,\n            lr=opt[\"lr\"],\n            weight_decay=opt[\"weight_decay\"]\n            )\n    \n    else:\n        raise NotImplementedError\n    \n    return optimizer\n\n\ndef get_scheduler(cfg, optimizer, num_train_steps):\n    sch = cfg.scheduler\n    if sch[\"scheduler\"] == \"get_linear_schedule_with_warmup\":\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, \n            num_warmup_steps=sch[\"num_warmup_steps\"],\n            num_training_steps=num_train_steps)\n    \n    elif sch[\"scheduler\"] == \"get_cosine_schedule_with_warmup\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=sch[\"num_warmup_steps\"],\n            num_training_steps=num_train_steps,\n            num_cycles=sch[\"num_cycles\"]\n            )\n\n    elif sch[\"scheduler\"] == \"MultiStepLR\":\n        scheduler = MultiStepLR(\n            optimizer, \n            milestones=sch[\"milestones\"], \n            gamma=sch[\"gamma\"]\n        )\n\n    else:\n        raise NotImplementedError\n    \n    return scheduler\n\n\nclass JigsawModel(pl.LightningModule):\n    def __init__(self, cfg):\n        super(JigsawModel, self).__init__()\n        self.cfg = cfg\n        self.total_steps = None\n        self.dataset_size = None\n\n        self.backborn = get_backborn(cfg)   \n        self.out = nn.Linear(cfg.hidden_size, len(cfg.target_cols))\n\n    def forward(self, inputs):\n        x = self.backborn(**inputs)\n        x = x[0]\n        x = x[:, 0, :]\n        x = self.out(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        inputs, targets = batch\n        outputs = self.forward(inputs)\n        loss = self.loss(outputs, targets)\n        self.log(\"train_loss\", loss, on_step=True, logger=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        inputs, targets = batch\n        outputs = self.forward(inputs)\n        loss = self.loss(outputs, targets)\n        self.log(\"val_loss\", loss, on_step=True, logger=True, prog_bar=True)\n        return loss\n\n    def loss(self, outputs, targets):\n        loss_fn = nn.BCEWithLogitsLoss()\n        loss = loss_fn(outputs, targets)\n        loss = torch.sqrt(loss)\n        return loss\n\n    def setup(self, stage=None):\n        if stage != \"fit\":\n            return\n\n        # calculate total steps\n        if self.dataset_size is None:\n            dataset = self.trainer._data_connector._train_dataloader_source.dataloader()\n            self.dataset_size = len(dataset)\n        num_devices = max(1, self.trainer.num_gpus, self.trainer.num_processes)  # gpus=-1だとそれが反映されちゃう\n        effective_batch_size = self.cfg.train_batch_size * self.trainer.accumulate_grad_batches * num_devices\n        print(self.dataset_size, effective_batch_size)\n        self.total_steps = (self.dataset_size // effective_batch_size) * self.cfg.max_epochs\n\n    def configure_optimizers(self):\n        optimizer = get_optimizer(self.cfg, parameters=self.parameters())\n\n        if self.cfg.scheduler is None:\n            return [optimizer]\n        else:\n            scheduler = get_scheduler(self.cfg, optimizer, num_train_steps=self.total_steps)\n            return [optimizer], [{\"scheduler\": scheduler, \"interval\": self.cfg.scheduler[\"interval\"]}]\n","metadata":{"execution":{"iopub.status.busy":"2022-01-31T09:40:51.311369Z","iopub.execute_input":"2022-01-31T09:40:51.311993Z","iopub.status.idle":"2022-01-31T09:40:51.332149Z","shell.execute_reply.started":"2022-01-31T09:40:51.311947Z","shell.execute_reply":"2022-01-31T09:40:51.331429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"# =============================\n# Metrics\n# ============================= \ndef get_validation_data_hat(cfg, tokenizer, filename, validation_data):\n    validation_data_ = validation_data.copy()\n    df = pd.DataFrame({\"text\":sorted(set(validation_data_[\"less_toxic\"].unique()) |\n                                     set(validation_data_[\"more_toxic\"].unique()))})\n    \n    if filename is None:\n        preds = predict_cv(cfg, df, tokenizer, text_col=\"text\")\n    else:\n        preds = predict(cfg, df, tokenizer, filename, text_col=\"text\")\n\n    if np.ndim(preds) > 1:\n        df[\"preds\"] = np.mean(preds, axis=1)  # mean of targets\n    else:\n        df[\"preds\"] = preds.reshape(-1)\n\n    validation_data_ = (pd.merge(\n        validation_data_, df, left_on=\"less_toxic\", right_on=\"text\", how=\"left\").\n        rename(columns={\"preds\":\"less_toxic_preds\"}).\n        drop(\"text\", axis=1))\n    \n    validation_data_ = (pd.merge(\n        validation_data_, df, left_on=\"more_toxic\", right_on=\"text\", how=\"left\").\n        rename(columns={\"preds\":\"more_toxic_preds\"}).\n        drop(\"text\", axis=1))\n    \n    return validation_data_\n\n\ndef get_score(validation_data_hat):\n    less_toxic, more_toxic = validation_data_hat[\"less_toxic_preds\"], validation_data_hat[\"more_toxic_preds\"]\n    return np.mean(more_toxic > less_toxic)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T09:40:51.334511Z","iopub.execute_input":"2022-01-31T09:40:51.335092Z","iopub.status.idle":"2022-01-31T09:40:51.345549Z","shell.execute_reply.started":"2022-01-31T09:40:51.335053Z","shell.execute_reply":"2022-01-31T09:40:51.344692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train & Predict","metadata":{}},{"cell_type":"code","source":"# =============================\n# Train & Predict\n# =============================\ndef class2dict(f):\n    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n\n\ndef train_fold(cfg, train_df, valid_df, tokenizer, filename, text_col):\n\n    wandblogger = pl.loggers.WandbLogger(\n        project=cfg.competition, \n        config=class2dict(cfg),\n        group=f\"{cfg.author}_{cfg.name}\",  \n        name=\"_\".join(filename.split(\"-\")[-2:]),\n        job_type=\"train\",\n        reinit=True,\n        anonymous=None,\n        entity=cfg.wandb_entity\n        )\n\n    lightning_datamodule = JigsawDataModule(\n        cfg=cfg, \n        tokenizer=tokenizer,\n        train_df=train_df, \n        valid_df=valid_df, \n        text_col=text_col\n        )\n    \n    lightning_model = JigsawModel(cfg=cfg)\n    lightning_model.dataset_size = len(train_df)  # cuz setup donot work?\n\n    checkpoint = pl.callbacks.ModelCheckpoint(\n        dirpath=cfg.EXP_MODEL,\n        filename=filename,\n        save_top_k=1,\n        verbose=True,\n        monitor=\"val_loss\",\n        mode=\"min\",\n    )\n    lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n    callbacks = [checkpoint, lr_monitor]\n\n    if cfg.early_stopping:\n        early_stopping = pl.callbacks.EarlyStopping(\n            monitor=\"val_loss\", \n            min_delta=0.0, \n            patience=8, \n            mode='min', \n        )\n        callbacks += [early_stopping]\n    \n    trainer = pl.Trainer(\n        max_epochs=cfg.max_epochs,\n        callbacks=callbacks,\n        logger=[wandblogger],\n        gradient_clip_val=cfg.gradient_clip_val,\n        accumulate_grad_batches=cfg.accumulate_grad_batches,\n        resume_from_checkpoint=cfg.resume_from_checkpoint,\n        deterministic=False,\n        gpus=-1,\n        precision=16,\n    )\n\n    trainer.fit(lightning_model, datamodule=lightning_datamodule)\n    wandb.finish(quiet=True)\n    torch.cuda.empty_cache()\n\n\ndef get_filname_listdir(dirctory):\n    listdir = os.listdir(dirctory)\n    out_lst = [os.path.splitext(d)[0] for d in listdir]\n    return out_lst\n\n\ndef train_cv(cfg, df, tokenizer, text_col=None, validation_data=None, get_oof=True):\n    \"\"\"cross validation & get oof\"\"\"\n    oof_df = pd.DataFrame(np.zeros((len(df), len(cfg.target_cols))), columns=cfg.target_cols)\n\n    for i_fold in range(cfg.n_fold):\n\n        if i_fold in cfg.trn_fold:\n            filename = f\"{cfg.name}-seed{cfg.seed}-fold{i_fold}\"\n            filelist = get_filname_listdir(cfg.EXP_MODEL)\n\n            val_mask = (df[\"fold\"] == i_fold).astype(bool)\n            train_df = df[~val_mask].reset_index(drop=True)\n            valid_df = df[val_mask].reset_index(drop=True)\n\n            if not filename in filelist:\n                print(f\"# --------- # Start Training Fold={i_fold} # --------- #\")\n                # training\n                train_fold(\n                    cfg=cfg, \n                    train_df=train_df, \n                    valid_df=valid_df, \n                    tokenizer=tokenizer, \n                    filename=filename, \n                    text_col=text_col\n                    )\n\n            # get validation data score\n            if validation_data is not None:\n                validation_data_hat = get_validation_data_hat(cfg, tokenizer, filename, validation_data)\n                val_score = get_score(validation_data_hat)\n                log = f\"{cfg.name}-seed{cfg.seed}-fold{i_fold}: validation data score={val_score:.4f}\"\n                cfg.logger.info(log)\n\n            # get validation prediction\n            if get_oof:\n                preds = predict(\n                    cfg=cfg,\n                    df=valid_df, \n                    tokenizer=tokenizer, \n                    filename=filename, \n                    text_col=text_col)\n                \n                oof_df.loc[val_mask] = preds\n                return oof_df\n\n\ndef predict(cfg, df, tokenizer, filename, text_col):\n    test_dataset = JigsawTestDataset(\n        cfg=cfg, tokenizer=tokenizer, df=df, text_col=text_col)\n    \n    test_dataloader = DataLoader(\n        test_dataset,\n        batch_size=cfg.valid_batch_size,\n        shuffle=False,\n        num_workers=cfg.num_workers, \n        pin_memory=True, \n        drop_last=False\n        ) \n    \n    lightning_model = JigsawModel(cfg=cfg).to(cfg.DEVICE).eval()\n    checkpoint_path = os.path.join(cfg.EXP_MODEL, filename + \".ckpt\") \n    lightning_model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n\n    num_targets = len(cfg.target_cols)\n    preds = np.zeros((len(df), num_targets))  # N * num targets\n    fill_start_idx = 0\n\n    for inputs in tqdm(test_dataloader,total=len(test_dataloader)):\n        # get predicted labels by batch\n        for k, v in inputs.items():\n            inputs[k] = v.to(cfg.DEVICE)\n\n        with torch.no_grad():\n            pred = lightning_model(inputs)\n            pred = pred.cpu().numpy()  # bs * num targets\n        \n        fill_end_idx = pred.shape[0] + fill_start_idx  # bs + idx\n        preds[fill_start_idx:fill_end_idx] = pred\n        fill_start_idx = fill_end_idx\n        \n    \n    del test_dataset, test_dataloader, lightning_model\n    gc.collect()\n\n    return preds\n\n\ndef predict_cv(cfg, df, tokenizer, text_col):\n    num_targets = len(cfg.target_cols)\n    preds = []\n    \n    for i_fold in range(cfg.n_fold):\n        if i_fold in cfg.trn_fold:\n            filename =f\"{cfg.name}-seed{cfg.seed}-fold{i_fold}\"\n            preds_fold = predict(cfg, df, tokenizer, filename, text_col)\n            preds.append(preds_fold)\n    \n    preds = np.mean(preds, axis=0)  # fold mean\n    return preds\n","metadata":{"execution":{"iopub.status.busy":"2022-01-31T09:40:51.347038Z","iopub.execute_input":"2022-01-31T09:40:51.347409Z","iopub.status.idle":"2022-01-31T09:40:51.377558Z","shell.execute_reply.started":"2022-01-31T09:40:51.347372Z","shell.execute_reply":"2022-01-31T09:40:51.376853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Model","metadata":{}},{"cell_type":"code","source":"# =============================\n# Load Model\n# =============================\ndef get_tokenizer(cfg):\n\n    pretrained_dir = os.path.join(cfg.EXP_MODEL, \"Pretrain\")\n    tokenizer_path = os.path.join(pretrained_dir, \"tokenizer_config.json\")  # tokenizer.json??\n    if not os.path.isfile(tokenizer_path):\n        tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n        tokenizer.save_pretrained(pretrained_dir)\n    \n    else:\n        tokenizer = AutoTokenizer.from_pretrained(pretrained_dir)\n\n    return tokenizer\n\n\ndef get_backborn(cfg):\n    pretrained_dir = os.path.join(cfg.EXP_MODEL, \"Pretrain\")\n    backborn_path = os.path.join(pretrained_dir, \"pytorch_model.bin\")\n    if not os.path.isfile(backborn_path):\n        model_config = AutoConfig.from_pretrained(cfg.model_name)\n\n        # No dropout\n        model_config.attention_probs_dropout_prob = 0.0\n        model_config.hidden_dropout_prob = 0.0\n\n        backborn = AutoModel.from_pretrained(cfg.model_name, config=model_config)\n\n        backborn.save_pretrained(pretrained_dir)\n    \n    else:\n        model_config = AutoConfig.from_pretrained(pretrained_dir)\n\n        # No dropout\n        model_config.attention_probs_dropout_prob = 0.0\n        model_config.hidden_dropout_prob = 0.0\n        \n        if cfg.use_pretrain_model:\n            backborn = AutoModel.from_pretrained(pretrained_dir, config=model_config)\n        else:\n            backborn = AutoModel.from_config(model_config)  # inference 時は pretrain weight いらない：cfg.use_pretrain_model=False\n\n    return backborn","metadata":{"execution":{"iopub.status.busy":"2022-01-31T09:41:48.854115Z","iopub.execute_input":"2022-01-31T09:41:48.85437Z","iopub.status.idle":"2022-01-31T09:41:48.865233Z","shell.execute_reply.started":"2022-01-31T09:41:48.854341Z","shell.execute_reply":"2022-01-31T09:41:48.864486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create DataSet","metadata":{}},{"cell_type":"code","source":"# =============================\n# Create Data\n# =============================\ndef read_csv(filepath, **kwargs):\n    if os.path.isdir(filepath):\n        filename = filepath.split(\"/\")[-1]\n        filepath = os.path.join(filepath, filename)\n        \n    try:\n        csv_data = pd.read_csv(filepath,  **kwargs)\n    except:\n        csv_data = pd.read_csv(filepath + \".zip\",  **kwargs)\n\n    return csv_data\n\n\ndef text_cleaning(text):\n    '''\n    ref) # https://www.kaggle.com/manabendrarout/pytorch-roberta-ranking-baseline-jrstc-train\n\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    # text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    ipPattern = re.compile('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}') # Removes IP address\n    text = ipPattern.sub(r'', text)\n    bikkuri = re.compile('!') # Removes bikkuri\n    text = bikkuri.sub(r' ', text)\n    text = text.replace('\\n','')\n    text = text.replace(\"\\'\",\"\")\n    text = text.replace(\"|\",\"\")\n    text = text.replace(\"=\",\"\")\n    text = text.replace(\"F**K\", \"FUCK\")\n    text = text.replace(\"F__K\", \"FUCK\")\n    text = text.replace(\"f**k\", \"fuck\")\n    text = text.replace(\"f__k\", \"fuck\")\n    text = text.replace(\"f*ck\", \"fuck\")    \n    text = text.replace(\"S$X\", \"SEX\")\n    text = text.replace(\"s$x\", \"sex\")\n    text = text.replace(\" u \", \" you \")\n    text = text.replace(\" u \", \" you \")\n    text = text.replace(\" U \", \" you \")\n    text = text.replace(\" U \", \" you \")\n    text = text.replace(\"YOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUUUUUUUUUU\", \"YOU\")\n    text = text.strip() # remove spaces at the beginning and at the end of string\n    return text\n\n\ndef text_normalization(s:pd.Series):\n    x = s.apply(text_cleaning)\n    return x\n\n\ndef get_jigsaw_01_dataset(cfg):\n    \"\"\"\n    jigsaw-toxic-comment-classification-challenge\n    - text_col : \"comment_text2\n    - target_cols : [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\n    \"\"\"\n    jigsaw1_train = read_csv(os.path.join(cfg.INPUT_JIGSAW_01 , \"train.csv\"))\n    jigsaw1_test = read_csv(os.path.join(cfg.INPUT_JIGSAW_01 , \"test.csv\"))\n    jigsaw1_test_label = read_csv(os.path.join(cfg.INPUT_JIGSAW_01 , \"test_labels.csv\"))\n    scoring_mask = jigsaw1_test_label[\"toxic\"] != -1\n    jigsaw1_test = pd.merge(jigsaw1_test[scoring_mask], jigsaw1_test_label[scoring_mask], on=\"id\", how=\"left\")\n    jigsaw1_train = pd.concat([jigsaw1_train, jigsaw1_test], axis=0).reset_index(drop=True)\n\n    return jigsaw1_train\n\n\ndef get_jigsaw_02_dataset(cfg, cat_threshold=0.5):\n    \"\"\"\n    jigsaw-unintended-bias-in-toxicity-classification\n    - text_col : \"comment_text\"\n    - target_cols : [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n    \"\"\"\n    jigsaw2_data = read_csv(os.path.join(cfg.INPUT_JIGSAW_02 , \"all_data.csv\"), usecols=[\"id\", \"comment_text\"])\n    jigsaw2_labels = read_csv(os.path.join(cfg.INPUT_JIGSAW_02 , \"toxicity_individual_annotations.csv\"))\n    jigsaw2_agg_labels = jigsaw2_labels.groupby([\"id\"]).agg(\"mean\")\n\n    if cat_threshold is not None:\n        jigsaw2_agg_labels = pd.DataFrame(\n            np.where(jigsaw2_agg_labels >= cat_threshold, 1, 0), \n            index=jigsaw2_agg_labels.index,\n            columns=jigsaw2_agg_labels.columns)\n    \n    jigsaw2_train = pd.merge(jigsaw2_data, jigsaw2_agg_labels, on=\"id\", how=\"left\")\n    jigsaw2_train = jigsaw2_train.dropna(axis=0).reset_index(drop=True)\n    jigsaw2_train = (jigsaw2_train.\n                        rename(columns={\"identity_attack\":\"identity_hate\"}).\n                        drop([\"sexual_explicit\", \"worker\"], axis=1))\n    \n    return jigsaw2_train\n\n\ndef get_ruddit_dataset(cfg):\n    \"\"\"\n    Ruddit Dataset\n    - text_col : \"comment_text\"\n    - target_cols : \"offensiveness_score\"\n    \"\"\"\n    ruddit_df = read_csv(os.path.join(cfg.INPUT_RUDDIT, \"Dataset\", \"ruddit_with_text.csv\"))\n    ruddit_df = ruddit_df[~ruddit_df[\"txt\"].isin([\"[deleted]\", \"[removed]\"])].reset_index(drop=True)\n    ruddit_df[\"comment_text\"] = text_normalization(ruddit_df[\"txt\"])\n    return ruddit_df.drop(\"txt\", axis=1)\n\n\ndef get_fold_idx(cfg, df):\n    df[\"fold\"] = -1\n    y = df[cfg.target_cols].sum(axis=1)\n    cv_strategy = KFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n    for i_fold, (tr_idx, va_idx) in enumerate(cv_strategy.split(X=df, y=y)):\n        df.loc[va_idx, \"fold\"] = i_fold\n    \n    return df\n\n\ndef get_custom_jigsaw_dataset(cfg, train_data, validation_data):\n    \"\"\"\n    ref) https://www.kaggle.com/toru59er/0-866-tfidf-ridge-simple-baseline\n    target_cols : [\"toxic_score\"]\n    weighted sum of targets:[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n    undersampling\n    \"\"\"\n\n    train_data[\"toxic_score\"] = train_data[cfg.target_cols].sum(axis=1)\n    \n    # undersample\n    toxic_mask = (train_data[\"toxic_score\"] > 0).astype(bool)\n    min_len = np.sum(toxic_mask)\n\n    sampled_data = train_data[train_data[\"toxic_score\"] == 0].sample(n=min_len, random_state=cfg.seed)\n    train_data = pd.concat([train_data[toxic_mask], sampled_data]).reset_index(drop=True).drop(\"toxic_score\", axis=1)\n\n    val_comment_unq = np.unique(validation_data['less_toxic'].tolist() + validation_data['more_toxic'].tolist())\n    duplicate_idx = np.isin(train_data['comment_text'], val_comment_unq)\n    train_data = train_data.iloc[~duplicate_idx].reset_index(drop=True)\n\n    return train_data\n","metadata":{"execution":{"iopub.status.busy":"2022-01-31T09:41:49.469477Z","iopub.execute_input":"2022-01-31T09:41:49.469918Z","iopub.status.idle":"2022-01-31T09:41:49.490528Z","shell.execute_reply.started":"2022-01-31T09:41:49.469876Z","shell.execute_reply":"2022-01-31T09:41:49.489681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"print(\"# ------------------ # Load Data # ------------------ #\")\n\n# load tokenizer\ntokenizer = get_tokenizer(Config)\n\ncomments_to_score = read_csv(os.path.join(Config.INPUT_JIGSAW_04 , \"comments_to_score.csv\"))\ncomments_to_score[\"text\"] = text_normalization(comments_to_score[\"text\"])\nsample_submission = read_csv(os.path.join(Config.INPUT_JIGSAW_04 , \"sample_submission.csv\"))\n\nif not Config.inference_only:\n\n    # load validation data\n    validation_data = read_csv(os.path.join(Config.INPUT_JIGSAW_04 , \"validation_data.csv\"))\n\n    # load train data\n    train_data = get_jigsaw_01_dataset(cfg=Config)\n    train_data = get_custom_jigsaw_dataset(cfg=Config, train_data=train_data, validation_data=validation_data)\n    train_data = get_fold_idx(cfg=Config, df=train_data)\n\n    train_data[\"comment_text\"] = text_normalization(train_data[\"comment_text\"])\n    validation_data[\"less_toxic\"] = text_normalization(validation_data[\"less_toxic\"])\n    validation_data[\"more_toxic\"] = text_normalization(validation_data[\"more_toxic\"])\n\n    print(\"# ------------------ # Training # ------------------ #\")\n    # training\n    train_cv(\n        cfg=Config, \n        df=train_data, \n        tokenizer=tokenizer, \n        text_col=\"comment_text\", \n        validation_data=validation_data, \n        get_oof=False)\n\n    print(\"# ------------------ # Validation # ------------------ #\")\n    # validation\n    validation_data_hat = get_validation_data_hat(\n        cfg=Config, \n        tokenizer=tokenizer, \n        filename=None, \n        validation_data=validation_data\n        )\n    filepath = os.path.join(Config.EXP_PREDS, \"validation_data.csv\")\n    validation_data.to_csv(filepath, index=False)\n    score = get_score(validation_data_hat)\n    Config.logger.info(f\"validation score = {score:.4f}\")\n\nprint(\"# ------------------ # Inference # ------------------ #\")\npreds = predict_cv(\n    cfg=Config, \n    df=comments_to_score, \n    tokenizer=tokenizer, \n    text_col=\"text\")\n\nprint(preds.shape)\nif np.ndim(preds) > 1:\n    sub_preds = np.mean(preds, axis=1)  # mean of target\nelse:\n    sub_preds = preds\n\nsample_submission[\"score\"] = sub_preds\nfilename = Config.name + \".csv\" if Config.COLAB else \"submission.csv\"\nsample_submission.to_csv(os.path.join(Config.SUBMISSION, filename), index=False)\n\n# upload output folder to kaggle dataset\nif Config.upload_from_colab:\n    from kaggle.api.kaggle_api_extended import KaggleApi\n\n    def dataset_create_new(dataset_name, upload_dir):\n        dataset_metadata = {}\n        dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n        dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n        dataset_metadata['title'] = dataset_name\n        with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n            json.dump(dataset_metadata, f, indent=4)\n        api = KaggleApi()\n        api.authenticate()\n        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n    dataset_create_new(dataset_name=Config.EXP, upload_dir=Config.OUTPUT_EXP)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T09:41:51.381784Z","iopub.execute_input":"2022-01-31T09:41:51.382122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}