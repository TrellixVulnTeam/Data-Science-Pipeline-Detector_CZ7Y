{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\nsubmission_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:17:21.034159Z","iopub.execute_input":"2022-02-07T22:17:21.034491Z","iopub.status.idle":"2022-02-07T22:17:21.148809Z","shell.execute_reply.started":"2022-02-07T22:17:21.034387Z","shell.execute_reply":"2022-02-07T22:17:21.148038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DistilBERT","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import DistilBertModel, DistilBertForMaskedLM, get_linear_schedule_with_warmup\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.nn.utils.rnn as rnn\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\n\nfrom collections import namedtuple\nfrom copy import deepcopy\nimport random\n\nimport re\nfrom bs4 import BeautifulSoup\n\n%matplotlib inline\n\n\ntokenizer = DistilBertTokenizer.from_pretrained('../input/jigsaw-bert-lm')\n\n\nclass CommentDataset(data.Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        text = self.X.iloc[idx]\n        target = self.y.iloc[idx]\n        return text, target\n    \n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nRegressionOutput = namedtuple(\"RegressionOutput\", ['loss', 'logits'])\n\nclass DistilBertGruRegression(nn.Module):\n    def __init__(self, base):\n        super(DistilBertGruRegression, self).__init__()\n        self.distilbert = base\n        self.gru = nn.GRU(768, 50, batch_first=True, bidirectional=True)\n        self.dropout = nn.Dropout(p=0.2)\n        self.classifier = nn.Linear(100, 1)\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        context_embs = self.distilbert(input_ids, attention_mask)[0]  # {bs, seq_len, 768}\n        context_embs = rnn.pack_padded_sequence(context_embs, attention_mask.sum(axis=1).cpu(),\n                                                   batch_first=True, enforce_sorted=False)\n        \n        gru_output, _ = self.gru(context_embs)  # {bs, seq_len, 50 * 2}\n        gru_output = rnn.pad_packed_sequence(gru_output, batch_first=True, total_length=120)[0]\n        \n        pooled_output = torch.max(gru_output, axis=1)[0]  # {bs, 50 * 2}\n        pooled_output = self.dropout(pooled_output)\n        output = nn.ReLU()(self.classifier(pooled_output))\n        \n        loss = None\n        if labels is not None:\n            loss_fct = nn.MSELoss()\n            loss = loss_fct(output.squeeze(), labels.squeeze())\n\n        return RegressionOutput(loss, output)\n    \n    \nbase_model = DistilBertModel.from_pretrained(\"../input/jigsaw-bert-lm\")\n\n\nreg_model = DistilBertGruRegression(base_model)\nreg_model.load_state_dict(torch.load(\"../input/jigsaw-bert-lm/finetuned-model-e1-l0.405.pt\"))\n\n\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip().lower() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\ntqdm.pandas()\ndf_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\n\n\nsub_dataset = CommentDataset(df_sub.text, df_sub.comment_id)  # comment_id will not be used, just dummy arg\n\nsub_loader = data.DataLoader(sub_dataset, shuffle=False, batch_size=32, num_workers=2)\n\n\nscores = []\n\nreg_model.to(device)\nreg_model.eval()\nwith torch.no_grad():\n    with tqdm(total=len(sub_loader)) as t:\n        for X, _ in sub_loader:\n            inputs = tokenizer.batch_encode_plus(X, return_tensors='pt',\n                                       padding='max_length', max_length=120,\n                                       truncation=True).to(device)\n\n            pred = reg_model(**inputs).logits[:, 0].cpu().detach().tolist()\n            scores.extend(pred)\n            \n            t.update()\n            \n\ndel reg_model\ndel tokenizer\ndel base_model\n\n            \ndf_sub['score_distilbert'] = scores\ndf_sub['score_distilbert'] /= df_sub['score_distilbert'].max()\n\n\n\"DONE.\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T22:17:21.150712Z","iopub.execute_input":"2022-02-07T22:17:21.15101Z","iopub.status.idle":"2022-02-07T22:18:26.353163Z","shell.execute_reply.started":"2022-02-07T22:17:21.150973Z","shell.execute_reply":"2022-02-07T22:18:26.352286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ridge Ensemble","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import KeyedVectors, FastText\n\nimport re \nfrom scipy import sparse\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns=100\n\nfrom sklearn.linear_model import Ridge\n\nimport pickle as pkl\n\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\nwith open('../input/jigsaw-preprocessed-tc/tfidf-vectorizer.pkl', mode='rb') as f:\n    vec = pkl.load(f)\n    \n    \nfmodel = FastText.load('../input/jigsaw-regression-based-data/FastText-jigsaw-256D/Jigsaw-Fasttext-Word-Embeddings-256D.bin')\n\n\ndef splitter(text):\n    tokens = []\n    \n    for word in text.split(' '):\n        tokens.append(word)\n    \n    return tokens\n\ndef vectorizer(text):\n    tokens = splitter(text)\n    \n    x1 = vec.transform([text]).toarray()\n    x2 = np.mean(fmodel.wv[tokens], axis = 0).reshape(1, -1)\n    x = np.concatenate([x1, x2], axis = -1).astype(np.float16)\n    del x1\n    del x2 \n    \n    return x  \n\n\nwith open('../input/ridge-models/m_05.pkl', 'rb') as f:\n    model = pkl.load(f)\n\nwith open('../input/ridge-models/m_1.pkl', 'rb') as f:\n    l_model = pkl.load(f)\n\nwith open('../input/ridge-models/m_15.pkl', 'rb') as f:\n    d_model = pkl.load(f)\n\nwith open('../input/ridge-models/m_2.pkl', 'rb') as f:\n    s_model = pkl.load(f)\n\n    \ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\n\ntqdm.pandas()\ndf_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\n\n\nX_sub_temp = []\nfor text in tqdm(df_sub.text):\n    X_sub_temp.append(vectorizer(text))\n\n    \nEMB_DIM = len(vec.vocabulary_) + 256\nEMB_DIM\n\n\nX_sub_temp = np.array(X_sub_temp).reshape(-1, EMB_DIM)\nX_test = sparse.csr_matrix(X_sub_temp)\n\ndel X_sub_temp\n\n\nptl = l_model.predict(X_test)\nptd = d_model.predict(X_test)\npts = s_model.predict(X_test)\n\n\ndf_sub['score'] = (ptl + ptd + pts) / 3\ndf_sub['score'] -= df_sub.score.min()\ndf_sub['score'] /= df_sub.score.max()\n\n\n\"DONE.\"","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:18:26.354686Z","iopub.execute_input":"2022-02-07T22:18:26.354905Z","iopub.status.idle":"2022-02-07T22:19:36.337476Z","shell.execute_reply.started":"2022-02-07T22:18:26.354876Z","shell.execute_reply":"2022-02-07T22:19:36.336653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Active Learning","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle\n\nimport fasttext\nimport lightgbm as lgb\nfrom tqdm.auto import tqdm\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup\nimport re\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split\nimport keras.preprocessing\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Model\nimport keras.layers\nfrom sklearn.model_selection import train_test_split\nimport skopt\n\n\ndef convert_to_tensor(text,tokenizer):\n    text = tokenizer.texts_to_sequences(text)\n    pre = keras.preprocessing.sequence.pad_sequences(\n    text, maxlen=63, dtype='int32', padding='post',\n    truncating='post', value=0)\n    text = tf.convert_to_tensor(pre)\n    return text\n\n\nwith open('../input/bogdan-dataset/tokenizer_bogdan.pkl', 'rb') as handle:\n    tokenizer = pickle.load(handle)\nmodel = keras.models.load_model('../input/bogdan-dataset/keras_bogdan.h5')\ncoefs= [3.606628582431543,\n 1.3735886700947442,\n 3.482668891973004,\n 2.7860855581794235,\n 2.686352276545167,\n 1.009564541977703]\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove specialrandom_state=harecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip().lower() # remove spaces at the beginning and at the end of string\n    \n    # lemmatization\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n    # del stopwords\n    text = ' '.join([word for word in text.split(' ') if word not in stop])\n    \n    return text\n\nlemmatizer = WordNetLemmatizer()\nstop = stopwords.words('english')\n\ntqdm.pandas()\n\nTEST_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\ndf_test = pd.read_csv(TEST_DATA_PATH)\ndf_test['clean_text'] = df_test['text'].progress_apply(text_cleaning)\ntest_data = convert_to_tensor(df_test['clean_text'],tokenizer)\ntest_pred = tf.squeeze(model(test_data)).numpy()\ndf_test['score'] = (test_pred * coefs).sum(axis=1)\ndf_test['score']-=df_test['score'].min()\ndf_test['score']/=df_test['score'].max()\n\n\n\"DONE.\"","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:19:36.340362Z","iopub.execute_input":"2022-02-07T22:19:36.341099Z","iopub.status.idle":"2022-02-07T22:19:50.948468Z","shell.execute_reply.started":"2022-02-07T22:19:36.341059Z","shell.execute_reply":"2022-02-07T22:19:50.947703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk import pos_tag\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nimport string\n\nfrom scipy import sparse\nfrom bs4 import BeautifulSoup\nimport re\n\nfrom gensim.models import FastText\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\n\nimport pickle\nfrom tqdm import tqdm\n\nimport lightgbm as lgbm\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import utils\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ntest_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\ndef splitter(text):\n    return [word for word in text.split(\" \")]\n\n\ndef get_fasttext_embeddings(text):\n    tokens = splitter(text)\n    return np.mean(fmodel.wv[tokens], axis=0).reshape(1, -1)\n\n\ndef send_to_words(sentences):\n    for sentence in sentences:\n        yield(splitter(sentence))\n        \n        \ndef get_doc2vec_embeddings(model, corpus_size, vectors_size, words_list):\n    vectors = np.zeros((corpus_size, vectors_size))\n    for i in tqdm(range(0, corpus_size)):\n        vectors[i] = model.infer_vector(words_list[i])\n    return vectors\n\n\nvec = pickle.load(open(\"../input/lgbm-tuned/vec.pickle\", \"rb\"))\nsvd = pickle.load(open(\"../input/lgbm-tuned/svd.pickle\", \"rb\"))\n\nlgbm_model = lgbm.Booster(model_file=\"../input/lgbm-tuned/lgbm_tuned_v18.txt\")\n\nfmodel = FastText.load(\"../input/jigsaw-regression-based-data/FastText-jigsaw-256D/Jigsaw-Fasttext-Word-Embeddings-256D.bin\")\nfasttext_embeddings_test = [get_fasttext_embeddings(text) for text in tqdm(test_df.text)]\nfasttext_embeddings_test = np.array(fasttext_embeddings_test).reshape(-1, 256)\n\ndoc2vec_model = Doc2Vec.load(\"../input/lgbm-tuned/doc2vecmodel.mod\")\ncorpus_test = test_df.text.values.tolist()\ntext_words_test = list(send_to_words(corpus_test))\ndoc2vec_embeddings_test = get_doc2vec_embeddings(doc2vec_model, len(text_words_test), 200, text_words_test)\n\n\nX_sub = vec.transform(test_df[\"text\"])\nX_sub = svd.transform(X_sub)\n\nX_sub = np.hstack((X_sub, fasttext_embeddings_test, doc2vec_embeddings_test))\n\ny_test_preds = lgbm_model.predict(X_sub)\n\n# test_df[\"score\"] = y_test_preds\n# scaler = MinMaxScaler()\n# test_df[\"score\"] = scaler.fit_transform(test_df[\"score\"].values.reshape(-1, 1))\n\n\n\"DONE.\"","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:19:50.99092Z","iopub.execute_input":"2022-02-07T22:19:50.991608Z","iopub.status.idle":"2022-02-07T22:20:33.707409Z","shell.execute_reply.started":"2022-02-07T22:19:50.991569Z","shell.execute_reply":"2022-02-07T22:20:33.70665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_lightgbm = pd.Series(y_test_preds)\npred_lightgbm -= pred_lightgbm.min()\npred_lightgbm[pred_lightgbm > 4.5] = 4.5 + 0.01 * pred_lightgbm[pred_lightgbm > 4.5]\npred_lightgbm /= pred_lightgbm.max()\npred_lightgbm.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:22:16.353265Z","iopub.execute_input":"2022-02-07T22:22:16.354042Z","iopub.status.idle":"2022-02-07T22:22:16.378841Z","shell.execute_reply.started":"2022-02-07T22:22:16.354001Z","shell.execute_reply":"2022-02-07T22:22:16.378111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_ridge_ensemble = pd.Series(ptl + ptd + pts)\npred_ridge_ensemble -= pred_ridge_ensemble.min()\npred_ridge_ensemble[pred_ridge_ensemble > 10.9] = 10.9 + 0.01 * pred_ridge_ensemble[pred_ridge_ensemble > 10.9]\npred_ridge_ensemble /= pred_ridge_ensemble.max()\npred_ridge_ensemble.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:22:50.420061Z","iopub.execute_input":"2022-02-07T22:22:50.420645Z","iopub.status.idle":"2022-02-07T22:22:50.438834Z","shell.execute_reply.started":"2022-02-07T22:22:50.420604Z","shell.execute_reply":"2022-02-07T22:22:50.438143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_active_learning = pd.Series((test_pred * coefs).sum(axis=1))\npred_active_learning -= pred_active_learning.min()\npred_active_learning[pred_active_learning > 10.2] = 10.2 + 0.01 * pred_active_learning[pred_active_learning > 10.2]\npred_active_learning /= pred_active_learning.max()\npred_active_learning.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:23:21.539286Z","iopub.execute_input":"2022-02-07T22:23:21.539842Z","iopub.status.idle":"2022-02-07T22:23:21.55915Z","shell.execute_reply.started":"2022-02-07T22:23:21.539786Z","shell.execute_reply":"2022-02-07T22:23:21.558282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_distilbert = pd.Series(scores)\npred_distilbert -= pred_distilbert.min()\npred_distilbert[pred_distilbert > 3] = 3 + 0.01 * pred_distilbert[pred_distilbert > 3]\npred_distilbert /= pred_distilbert.max()\npred_distilbert.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:23:49.236082Z","iopub.execute_input":"2022-02-07T22:23:49.236782Z","iopub.status.idle":"2022-02-07T22:23:49.2544Z","shell.execute_reply.started":"2022-02-07T22:23:49.236743Z","shell.execute_reply":"2022-02-07T22:23:49.253664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Aggregate scores and submit","metadata":{}},{"cell_type":"code","source":"submission_df['score'] = (pred_distilbert * 0.19 +\n                          pred_ridge_ensemble * 0.35 +\n                          pred_active_learning * 0.19 +\n                          pred_lightgbm * 0.27)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:24:02.422752Z","iopub.execute_input":"2022-02-07T22:24:02.423529Z","iopub.status.idle":"2022-02-07T22:24:02.430602Z","shell.execute_reply.started":"2022-02-07T22:24:02.423489Z","shell.execute_reply":"2022-02-07T22:24:02.429656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df[['score']].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:24:25.339703Z","iopub.execute_input":"2022-02-07T22:24:25.340451Z","iopub.status.idle":"2022-02-07T22:24:25.360389Z","shell.execute_reply.started":"2022-02-07T22:24:25.340415Z","shell.execute_reply":"2022-02-07T22:24:25.359525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:25:19.429008Z","iopub.execute_input":"2022-02-07T22:25:19.429835Z","iopub.status.idle":"2022-02-07T22:25:19.471052Z","shell.execute_reply.started":"2022-02-07T22:25:19.429768Z","shell.execute_reply":"2022-02-07T22:25:19.470358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}