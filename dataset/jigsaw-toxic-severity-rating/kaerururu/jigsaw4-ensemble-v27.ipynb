{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport sys\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nfrom bs4 import BeautifulSoup\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy import sparse\nfrom sklearn.linear_model import Ridge","metadata":{"papermill":{"duration":2.036616,"end_time":"2021-12-31T12:05:46.476168","exception":false,"start_time":"2021-12-31T12:05:44.439552","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    ######################\n    # Globals #\n    ######################\n    EXP_ID = 'XX' # '029'\n    seed = 2021 # 71\n    epochs = 3\n    folds = [0, 1, 2, 3, 4]\n    N_FOLDS = 5\n    LR = 2e-5\n    max_len = 128 # 256\n    train_bs = 16 \n    valid_bs = 64 # 128\n    log_interval = 4 # 10\n    model_name = '../input/roberta-base'\n    ID_COL = 'Id'\n    TARGET_COL = 'Pawpularity'\n    TARGET_DIM = 1\n    EARLY_STOPPING = True\n    DEBUG = True # False # True\n    margin = 0.5\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    \ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nset_seed(CFG.seed)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Jigsaw4DatasetTest:\n    def __init__(self, df, cfg):\n        self.tokenizer = cfg.tokenizer\n        self.max_len = cfg.max_len\n        self.text = df['text'].values\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n\n        inputs = self.tokenizer(\n            self.text[item], \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True,\n            add_special_tokens=True,\n        )\n        \n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTaBase(nn.Module):\n    def __init__(self, model_path):\n        super(RoBERTaBase, self).__init__()\n        self.in_features = 768\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.dropout = nn.Dropout(0.2)\n        self.l0 = nn.Linear(self.in_features, 1)\n\n    def forward(self, ids, mask):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask,\n            output_hidden_states=False\n        )\n        x = roberta_outputs[1]\n        logits = self.l0(self.dropout(x))\n        return logits.squeeze(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_preds(model_paths1, model_paths2, model_paths3, model_paths4,\n               model_paths5, model_paths6):\n    df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n    y_pred1 = []\n    y_pred2 = []\n    y_pred3 = []\n    y_pred4 = []\n    y_pred5 = []\n    y_pred6 = []\n    for fold, (model_path1, model_path2, model_path3, model_path4, \n               model_path5, model_path6) in enumerate(zip(model_paths1, model_paths2, model_paths3, model_paths4, \n                                                                       model_paths5, model_paths6)):\n        model1 = RoBERTaBase(CFG.model_name)\n        model1.to(device)\n        model1.load_state_dict(torch.load(model_path1))\n        model1.eval()\n        \n        model2 = RoBERTaBase(CFG.model_name)\n        model2.to(device)\n        model2.load_state_dict(torch.load(model_path2))\n        model2.eval()\n        \n        model3 = RoBERTaBase(CFG.model_name)\n        model3.to(device)\n        model3.load_state_dict(torch.load(model_path3))\n        model3.eval()\n        \n        model4 = RoBERTaBase(CFG.model_name)\n        model4.to(device)\n        model4.load_state_dict(torch.load(model_path4))\n        model4.eval()\n        \n        model5 = RoBERTaBase(CFG.model_name)\n        model5.to(device)\n        model5.load_state_dict(torch.load(model_path5))\n        model5.eval()\n        \n        model6 = RoBERTaBase(CFG.model_name)\n        model6.to(device)\n        model6.load_state_dict(torch.load(model_path6))\n        model6.eval()\n        \n        dataset = Jigsaw4DatasetTest(df=df, cfg=CFG)\n        data_loader = torch.utils.data.DataLoader(\n            dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n        )\n\n        final_output1 = []\n        final_output2 = []\n        final_output3 = []\n        final_output4 = []\n        final_output5 = []\n        final_output6 = []\n        for b_idx, data in tqdm(enumerate(data_loader)):\n            with torch.no_grad():\n                inputs = data['input_ids'].to(device)\n                masks = data['attention_mask'].to(device)\n                output1 = model1(inputs, masks)\n                output2 = model2(inputs, masks)\n                output3 = model3(inputs, masks)\n                output4 = model4(inputs, masks)\n                output5 = model5(inputs, masks)\n                output6 = model6(inputs, masks)\n                \n                # output = torch.sigmoid(output)\n                output1 = output1.detach().cpu().numpy().tolist()\n                output2 = output2.detach().cpu().numpy().tolist()\n                output3 = output3.detach().cpu().numpy().tolist()\n                output4 = output4.detach().cpu().numpy().tolist()\n                output5 = output5.detach().cpu().numpy().tolist()\n                output6 = output6.detach().cpu().numpy().tolist()\n                \n                final_output1.extend(output1)\n                final_output2.extend(output2)\n                final_output3.extend(output3)\n                final_output4.extend(output4)\n                final_output5.extend(output5)\n                final_output6.extend(output6)\n                \n        y_pred1.append(np.array(final_output1))\n        y_pred2.append(np.array(final_output2))\n        y_pred3.append(np.array(final_output3))\n        y_pred4.append(np.array(final_output4))\n        y_pred5.append(np.array(final_output5))\n        y_pred6.append(np.array(final_output6))\n        torch.cuda.empty_cache()\n        \n    y_pred1 = np.mean(y_pred1, 0)\n    y_pred2 = np.mean(y_pred2, 0)\n    y_pred3 = np.mean(y_pred3, 0)\n    y_pred4 = np.mean(y_pred4, 0)\n    y_pred5 = np.mean(y_pred5, 0)\n    y_pred6 = np.mean(y_pred6, 0)\n    return y_pred1, y_pred2, y_pred3, y_pred4, y_pred5, y_pred6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths1 = [f'../input/kaerururu-jigsaw4-017/fold-{i}.bin' for i in CFG.folds]\nmodel_paths2 = [f'../input/kaerururu-jigsaw4-029/fold-{i}.bin' for i in CFG.folds]\nmodel_paths3 = [f'../input/kaerururu-jigsaw4-047/fold-{i}.bin' for i in CFG.folds]\nmodel_paths4 = [f'../input/kaerururu-jigsaw4-052/fold-{i}.bin' for i in CFG.folds]\nmodel_paths5 = [f'../input/kaerururu-jigsaw4-076/fold-{i}.bin' for i in CFG.folds]\nmodel_paths6 = [f'../input/kaerururu-jigsaw4-0080/fold-{i}.bin' for i in CFG.folds]\n\nsub = pd.read_csv('../input/jigsaw-toxic-severity-rating/sample_submission.csv')\npreds_017, preds_029, preds_047, preds_052, preds_076, preds_080 = make_preds(model_paths1, model_paths2, model_paths3, model_paths4, \n                                                                                         model_paths5, model_paths6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# electra base discriminator\n\nclass CFG:\n    ######################\n    # Globals #\n    ######################\n    EXP_ID = '0XX'\n    seed = 2021 # 71\n    epochs = 3\n    folds = [0, 1, 2, 3, 4]\n    N_FOLDS = 5\n    LR = 2e-5\n    max_len = 128 # 256\n    train_bs = 16 \n    valid_bs = 128 # 32 * 2\n    log_interval = 4 # 10\n    model_name = '../input/electra/base-discriminator'\n    TARGET_DIM = 1\n    EARLY_STOPPING = True\n    DEBUG = True # False # True\n    margin = 0.5\n    tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ElectraBaseDiscriminator(nn.Module):\n    def __init__(self, model_path):\n        super(ElectraBaseDiscriminator, self).__init__()\n        self.in_features = 768\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.dropout = nn.Dropout(0.2)\n        self.l0 = nn.Linear(self.in_features, 1)\n\n    def forward(self, ids, mask):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask,\n            output_hidden_states=False\n        )\n        x = roberta_outputs['last_hidden_state'][:, 0, :]\n        logits = self.l0(self.dropout(x))\n        return logits.squeeze(-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_preds(model_paths1):\n    df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n    y_pred1 = []\n    for fold, model_path1 in enumerate(model_paths1):\n        model1 = ElectraBaseDiscriminator(CFG.model_name)\n        model1.to(device)\n        model1.load_state_dict(torch.load(model_path1))\n        model1.eval()\n        \n        dataset = Jigsaw4DatasetTest(df=df, cfg=CFG)\n        data_loader = torch.utils.data.DataLoader(\n            dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n        )\n\n        final_output1 = []\n        for b_idx, data in tqdm(enumerate(data_loader)):\n            with torch.no_grad():\n                inputs = data['input_ids'].to(device)\n                masks = data['attention_mask'].to(device)\n                output1 = model1(inputs, masks)\n                output1 = output1.detach().cpu().numpy().tolist()\n                \n                final_output1.extend(output1)\n                \n        y_pred1.append(np.array(final_output1))\n        torch.cuda.empty_cache()\n        \n    y_pred1 = np.mean(y_pred1, 0)\n    return y_pred1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths = [f'../input/kaerururu-jigsaw4-0082/fold-{i}.bin' for i in CFG.folds]\n\npreds_082 = make_preds(model_paths)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distil bert base uncased\n# 050, 066\n\nclass CFG1:\n    ######################\n    # Globals #\n    ######################\n    EXP_ID = '050'\n    seed = 2021 # 71\n    epochs = 3\n    folds = [0, 1, 2, 3, 4]\n    N_FOLDS = 5\n    LR = 2e-5\n    max_len = 128 # 256\n    train_bs = 16 \n    valid_bs = 128 # 32 * 2\n    model_name = '../input/distilbertbaseuncased'\n    TARGET_DIM = 1\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    \nclass CFG2:\n    ######################\n    # Globals #\n    ######################\n    EXP_ID = '050'\n    seed = 2021 # 71\n    epochs = 3\n    folds = [0, 1, 2, 3, 4]\n    N_FOLDS = 5\n    LR = 2e-5\n    max_len = 224\n    train_bs = 16 \n    valid_bs = 128 # 32 * 2\n    model_name = '../input/distilbertbaseuncased'\n    TARGET_DIM = 1\n    tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DistilBertBaseUncased(nn.Module):\n    def __init__(self, model_path):\n        super(DistilBertBaseUncased, self).__init__()\n        self.in_features = 768\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.dropout = nn.Dropout(0.2)\n        self.l0 = nn.Linear(self.in_features, 1)\n\n    def forward(self, ids, mask):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask,\n            output_hidden_states=False\n        )\n        x = roberta_outputs['last_hidden_state'][:, 0, :]\n        logits = self.l0(self.dropout(x))\n        return logits.squeeze(-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_preds(model_paths1, model_paths2):\n    df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n    y_pred1 = []\n    y_pred2 = []\n    for fold, (model_path1, model_path2) in enumerate(zip(model_paths1, model_paths2)):\n        model1 = DistilBertBaseUncased(CFG1.model_name)\n        model1.to(device)\n        model1.load_state_dict(torch.load(model_path1))\n        model1.eval()\n        \n        model2 = DistilBertBaseUncased(CFG2.model_name)\n        model2.to(device)\n        model2.load_state_dict(torch.load(model_path2))\n        model2.eval()\n        \n        dataset1 = Jigsaw4DatasetTest(df=df, cfg=CFG1)\n        data_loader1 = torch.utils.data.DataLoader(\n            dataset1, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n        )\n        \n        dataset2 = Jigsaw4DatasetTest(df=df, cfg=CFG2)\n        data_loader2 = torch.utils.data.DataLoader(\n            dataset2, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n        )\n\n        final_output1 = []\n        final_output2 = []\n        for b_idx, (data1, data2) in tqdm(enumerate(zip(data_loader1, data_loader2))):\n            with torch.no_grad():\n                inputs1 = data1['input_ids'].to(device)\n                masks1 = data1['attention_mask'].to(device)\n                                          \n                inputs2 = data2['input_ids'].to(device)\n                masks2 = data2['attention_mask'].to(device)\n                                          \n                output1 = model1(inputs1, masks1)\n                output2 = model2(inputs2, masks2)\n                \n                # output = torch.sigmoid(output)\n                output1 = output1.detach().cpu().numpy().tolist()\n                output2 = output2.detach().cpu().numpy().tolist()\n                \n                final_output1.extend(output1)\n                final_output2.extend(output2)\n                \n        y_pred1.append(np.array(final_output1))\n        y_pred2.append(np.array(final_output2))\n        torch.cuda.empty_cache()\n        \n    y_pred1 = np.mean(y_pred1, 0)\n    y_pred2 = np.mean(y_pred2, 0)\n    return y_pred1, y_pred2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths1 = [f'../input/kaerururu-jigsaw4-050/fold-{i}.bin' for i in CFG.folds]\nmodel_paths2 = [f'../input/kaerururu-jigsaw4-066/fold-{i}.bin' for i in CFG.folds]\n\npreds_050, preds_066 = make_preds(model_paths1, model_paths2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Albert base v2\n\nclass CFG:\n    ######################\n    # Globals #\n    ######################\n    EXP_ID = '049'\n    seed = 2021 # 71\n    epochs = 3\n    folds = [0, 1, 2, 3, 4]\n    N_FOLDS = 5\n    LR = 2e-5\n    max_len = 128 # 256\n    train_bs = 16 \n    valid_bs = 128 # 32 * 2\n    log_interval = 4 # 10\n    model_name = '../input/pretrained-albert-pytorch/albert-base-v2'\n    ID_COL = 'Id'\n    TARGET_COL = 'Pawpularity'\n    TARGET_DIM = 1\n    EARLY_STOPPING = True\n    DEBUG = True # False # True\n    margin = 0.5\n    tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AlbertBaseV2(nn.Module):\n    def __init__(self, model_path):\n        super(AlbertBaseV2, self).__init__()\n        self.in_features = 768\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.dropout = nn.Dropout(0.2)\n        self.l0 = nn.Linear(self.in_features, 1)\n\n    def forward(self, ids, mask):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask,\n            output_hidden_states=False\n        )\n        x = roberta_outputs[1]\n        logits = self.l0(self.dropout(x))\n        return logits.squeeze(-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_preds(model_paths1, model_paths2):\n    df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n    y_pred1 = []\n    y_pred2 = []\n    for fold, (model_path1, model_path2) in enumerate(zip(model_paths1, model_paths2)):\n        model1 = AlbertBaseV2(CFG.model_name)\n        model1.to(device)\n        model1.load_state_dict(torch.load(model_path1))\n        model1.eval()\n        \n        model2 = AlbertBaseV2(CFG.model_name)\n        model2.to(device)\n        model2.load_state_dict(torch.load(model_path2))\n        model2.eval()\n        \n        dataset = Jigsaw4DatasetTest(df=df, cfg=CFG)\n        data_loader = torch.utils.data.DataLoader(\n            dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n        )\n\n        final_output1 = []\n        final_output2 = []\n        for b_idx, data in tqdm(enumerate(data_loader)):\n            with torch.no_grad():\n                inputs = data['input_ids'].to(device)\n                masks = data['attention_mask'].to(device)\n                output1 = model1(inputs, masks)\n                output2 = model2(inputs, masks)\n                \n                # output = torch.sigmoid(output)\n                output1 = output1.detach().cpu().numpy().tolist()\n                output2 = output2.detach().cpu().numpy().tolist()\n                \n                final_output1.extend(output1)\n                final_output2.extend(output2)\n                \n        y_pred1.append(np.array(final_output1))\n        y_pred2.append(np.array(final_output2))\n        torch.cuda.empty_cache()\n        \n    y_pred1 = np.mean(y_pred1, 0)\n    y_pred2 = np.mean(y_pred2, 0)\n    return y_pred1, y_pred2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths1 = [f'../input/kaerururu-jigsaw4-049/fold-{i}.bin' for i in CFG.folds]\nmodel_paths2 = [f'../input/kaerururu-jigsaw4-051/fold-{i}.bin' for i in CFG.folds]\n\npreds_049, preds_051 = make_preds(model_paths1, model_paths2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# microsoft/deberta-v3-base\n\nclass CFG:\n    seed = 2021 # 71\n    epochs = 3\n    folds = [0, 1, 2, 3, 4]\n    N_FOLDS = 5\n    max_len = 128 # 256\n    train_bs = 16 \n    valid_bs = 128 # 32 * 2\n    model_name = '../input/deberta-v3-base/deberta-v3-base'\n    tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DebertaV3Base(nn.Module):\n    def __init__(self, model_path):\n        super(DebertaV3Base, self).__init__()\n        self.in_features = 768\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.dropout = nn.Dropout(0.2)\n        self.l0 = nn.Linear(self.in_features, 1)\n\n    def forward(self, ids, mask):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask,\n            output_hidden_states=False\n        )\n        x = roberta_outputs['last_hidden_state'][:, 0, :]\n        logits = self.l0(self.dropout(x))\n        return logits.squeeze(-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_preds(model_paths1, model_paths2):\n    df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n    y_pred1 = []\n    y_pred2 = []\n    for fold, (model_path1, model_path2) in enumerate(zip(model_paths1, model_paths2)):\n        model1 = DebertaV3Base(CFG.model_name)\n        model1.to(device)\n        model1.load_state_dict(torch.load(model_path1))\n        model1.eval()\n        \n        model2 = DebertaV3Base(CFG.model_name)\n        model2.to(device)\n        model2.load_state_dict(torch.load(model_path2))\n        model2.eval()\n        \n        dataset = Jigsaw4DatasetTest(df=df, cfg=CFG)\n        data_loader = torch.utils.data.DataLoader(\n            dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n        )\n\n        final_output1 = []\n        final_output2 = []\n        for b_idx, data in tqdm(enumerate(data_loader)):\n            with torch.no_grad():\n                inputs = data['input_ids'].to(device)\n                masks = data['attention_mask'].to(device)\n                output1 = model1(inputs, masks)\n                output2 = model2(inputs, masks)\n                \n                # output = torch.sigmoid(output)\n                output1 = output1.detach().cpu().numpy().tolist()\n                output2 = output2.detach().cpu().numpy().tolist()\n                \n                final_output1.extend(output1)\n                final_output2.extend(output2)\n                \n        y_pred1.append(np.array(final_output1))\n        y_pred2.append(np.array(final_output2))\n        torch.cuda.empty_cache()\n        \n    y_pred1 = np.mean(y_pred1, 0)\n    y_pred2 = np.mean(y_pred2, 0)\n    return y_pred1, y_pred2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths1 = [f'../input/kaerururu-jigsaw4-055/fold-{i}.bin' for i in CFG.folds]\nmodel_paths2 = [f'../input/kaerururu-jigsaw4-059/fold-{i}.bin' for i in CFG.folds]\n\npreds_055, preds_059 = make_preds(model_paths1, model_paths2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distil roberta base\n\nclass CFG:\n    seed = 2021 # 71\n    epochs = 3\n    folds = [0, 1, 2, 3, 4]\n    N_FOLDS = 5\n    max_len = 128 # 256\n    train_bs = 16 \n    valid_bs = 128 # 32 * 2\n    model_name = '../input/distilroberta-base'\n    tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DistilRoBERTaBase(nn.Module):\n    def __init__(self, model_path):\n        super(DistilRoBERTaBase, self).__init__()\n        self.in_features = 768\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.dropout = nn.Dropout(0.2)\n        self.l0 = nn.Linear(self.in_features, 1)\n\n    def forward(self, ids, mask):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask,\n            output_hidden_states=False\n        )\n        x = roberta_outputs['last_hidden_state'][:, 0, :]\n        logits = self.l0(self.dropout(x))\n        return logits.squeeze(-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_preds(model_paths1, model_paths2):\n    df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n    y_pred1 = []\n    y_pred2 = []\n    for fold, (model_path1, model_path2) in enumerate(zip(model_paths1, model_paths2)):\n        model1 = DistilRoBERTaBase(CFG.model_name)\n        model1.to(device)\n        model1.load_state_dict(torch.load(model_path1))\n        model1.eval()\n        \n        model2 = DistilRoBERTaBase(CFG.model_name)\n        model2.to(device)\n        model2.load_state_dict(torch.load(model_path2))\n        model2.eval()\n        \n        dataset = Jigsaw4DatasetTest(df=df, cfg=CFG)\n        data_loader = torch.utils.data.DataLoader(\n            dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n        )\n\n        final_output1 = []\n        final_output2 = []\n        for b_idx, data in tqdm(enumerate(data_loader)):\n            with torch.no_grad():\n                inputs = data['input_ids'].to(device)\n                masks = data['attention_mask'].to(device)\n                output1 = model1(inputs, masks)\n                output2 = model2(inputs, masks)\n                \n                # output = torch.sigmoid(output)\n                output1 = output1.detach().cpu().numpy().tolist()\n                output2 = output2.detach().cpu().numpy().tolist()\n                \n                final_output1.extend(output1)\n                final_output2.extend(output2)\n                \n        y_pred1.append(np.array(final_output1))\n        y_pred2.append(np.array(final_output2))\n        torch.cuda.empty_cache()\n        \n    y_pred1 = np.mean(y_pred1, 0)\n    y_pred2 = np.mean(y_pred2, 0)\n    return y_pred1, y_pred2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths1 = [f'../input/kaerururu-jigsaw4-057/fold-{i}.bin' for i in CFG.folds]\nmodel_paths2 = [f'../input/kaerururu-jigsaw4-067/fold-{i}.bin' for i in CFG.folds]\n\npreds_057, preds_067 = make_preds(model_paths1, model_paths2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data ","metadata":{}},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    ###\n    text = text.replace(\"'s\", ' ')\n    text = text.replace('\\n', ' ')\n    text = text.replace('\\t', ' ')\n    text = text.replace('.', ' ')\n    text = text.replace(',', ' ')\n    text = text.replace(':', ' ')\n    text = text.replace(';', ' ')\n    text = text.replace('\"', ' ')\n    ###\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    text = text.lower()\n    return text\n\n\nspaces = ['\\u200b', '\\u200e', '\\u202a', '\\u2009', '\\u2028', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\u3000', '\\x10', '\\x7f', '\\x9d', '\\xad',\n                       '\\x97', '\\x9c', '\\x8b', '\\x81', '\\x80', '\\x8c', '\\x85', '\\x92', '\\x88', '\\x8d', '\\x80', '\\x8e', '\\x9a', '\\x94', '\\xa0', \n                       '\\x8f', '\\x82', '\\x8a', '\\x93', '\\x90', '\\x83', '\\x96', '\\x9b', '\\x9e', '\\x99', '\\x87', '\\x84', '\\x9f',]\n\ndef rm_spaces(x):\n    for space in spaces:\n        x = x.replace(space, ' ')\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.sparse import hstack\nfrom scipy import sparse\n\n\ndef splitter(text):\n    tokens = []\n    \n    for word in text.split(' '):\n        tokens.append(word)\n    \n    return tokens\n\ndef vectorizer(text):\n    tokens = splitter(text)\n    \n    x1 = vec.transform([text]).toarray()\n    x2 = np.mean(fmodel.wv[tokens], axis = 0).reshape(1, -1)\n    x = np.concatenate([x1, x2], axis = -1).astype(np.float16)\n    del x1, x2\n    \n    return x ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\ndef to_pickle(filename, obj):\n    with open(filename, mode='wb') as f:\n        pickle.dump(obj, f)\n\ndef unpickle(filename):\n    with open(filename, mode='rb') as fo:\n        p = pickle.load(fo)\n    return p","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import KeyedVectors, FastText\n\nfmodel = FastText.load('../input/jigsaw-rate-severity-good-score-train-dataset/FastText-jigsaw-256D/Jigsaw-Fasttext-Word-Embeddings-256D.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = unpickle('../input/jigsaw4-ridge-0-874-rm-space/cv_0.6772_lb_0.874_Ridge.pkl')\nvec = unpickle('../input/jigsaw4-ridge-0-874-rm-space/cv_0.6772_lb_0.874_TfidfVectorizer.pkl')\nEMB_DIM = len(vec.vocabulary_) + 256","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\n\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\ndf_sub['text'] = df_sub['text'].progress_apply(lambda x: rm_spaces(x).strip())","metadata":{"papermill":{"duration":0.182312,"end_time":"2021-12-31T12:20:54.509148","exception":false,"start_time":"2021-12-31T12:20:54.326836","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_sub_temp = []\nfor text in tqdm(df_sub.text):\n    X_sub_temp.append(vectorizer(text))\n    \nX_sub_temp = np.array(X_sub_temp).reshape(-1, EMB_DIM)\nX_test = sparse.csr_matrix(X_sub_temp)\n\ndel X_sub_temp; gc.collect()","metadata":{"papermill":{"duration":23.57102,"end_time":"2021-12-31T12:21:21.080839","exception":false,"start_time":"2021-12-31T12:20:57.509819","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_preds = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec2 = unpickle('../input/jigsaw4-ridge-0-874-ruddit/TfidfVectorizer_Ridge_ruddit.pkl')\nmodel2 = unpickle('../input/jigsaw4-ridge-0-874-ruddit/Tfidf_Ridge_ruddit.pkl')\n\nvec3 = unpickle('../input/jigsaw4-ridge-jigsaw1-weighted/jigsaw1_weighted_TfidfVectorizer.pkl')\nmodel3 = unpickle('../input/jigsaw4-ridge-jigsaw1-weighted/jigsaw1_weighted_Ridge.pkl')\n\nvec4 = unpickle('../input/jigsaw4-ridge-jigsaw1-v2/jigsaw1_v2_TfidfVectorizer.pkl')\nmodel4 = unpickle('../input/jigsaw4-ridge-jigsaw1-v2/jigsaw1_v2_Ridge.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test2 = vec2.transform(df_sub['text'])\nreddit_preds = model2.predict(X_test2)\n\nX_test3 = vec3.transform(df_sub['text'])\nj1_weighted_preds = model3.predict(X_test3)\n\nX_test4 = vec4.transform(df_sub['text'])\nj1_v2_preds = model4.predict(X_test4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec = unpickle('../input/jigsaw4-ridge-0-874-pl-pub-test-v2/0874_pseudo_label_only_public_test_tfidf_TfidfVectorizer_v2.pkl')\nmodel5 = unpickle('../input/jigsaw4-ridge-0-874-pl-pub-test-v2/0874_pseudo_label_only_public_test_tfidf_Ridge_v2.pkl')\nEMB_DIM = len(vec.vocabulary_) + 256\n\nX_sub_temp = []\nfor text in tqdm(df_sub.text):\n    X_sub_temp.append(vectorizer(text))\n    \nX_sub_temp = np.array(X_sub_temp).reshape(-1, EMB_DIM)\nX_test5 = sparse.csr_matrix(X_sub_temp)\n\ndel X_sub_temp; gc.collect()\n\nridge_pseudo_labeled_preds = model5.predict(X_test5) # 0.876","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ensembling the Ridge Regression models\n","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/kaerunantoka/jigsaw4-ensemble-v27-cv?scriptVersionId=87223243\n\nw = [0.06324371, 0.01615546, 0.09597985, 0.03304393, 0.05108132,\n        0.03772867, 0.02205191, 0.04967104, 0.08629689, 0.08855165,\n        0.04448891, 0.07703826, 0.09451529, 0.09916092, 0.02796084,\n        0.01080381, 0.01141286, 0.06257114, 0.02824354] # 0.7790952570745316\n\ndf_sub['score'] = w[0] * ridge_preds \\\n                + w[1] * preds_017 \\\n                + w[2] * preds_029 \\\n                + w[3] * preds_047 \\\n                + w[4] * preds_049 \\\n                + w[5] * preds_050 \\\n                + w[6] * preds_051 \\\n                + w[7] * preds_052 \\\n                + w[8] * preds_055 \\\n                + w[9] * preds_057 \\\n                + w[10] * preds_059 \\\n                + w[11] * reddit_preds \\\n                + w[12] * j1_weighted_preds \\\n                + w[13] * j1_v2_preds \\\n                + w[14] * preds_066 \\\n                + w[15] * preds_067 \\\n                + w[16] * preds_076 \\\n                + w[17] * preds_080 \\\n                + w[18] * preds_082","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score'].count()","metadata":{"papermill":{"duration":0.070296,"end_time":"2021-12-31T12:21:21.964993","exception":false,"start_time":"2021-12-31T12:21:21.894697","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score'] = df_sub['score'].rank(method='first')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score'].nunique()","metadata":{"papermill":{"duration":0.070769,"end_time":"2021-12-31T12:21:22.227052","exception":false,"start_time":"2021-12-31T12:21:22.156283","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Prepare submission file</h2>","metadata":{"papermill":{"duration":0.062375,"end_time":"2021-12-31T12:21:22.351868","exception":false,"start_time":"2021-12-31T12:21:22.289493","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"papermill":{"duration":0.087992,"end_time":"2021-12-31T12:21:22.500656","exception":false,"start_time":"2021-12-31T12:21:22.412664","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}