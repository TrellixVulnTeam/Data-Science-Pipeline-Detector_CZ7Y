{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#! pip install transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:01:33.592205Z","iopub.execute_input":"2021-11-17T09:01:33.59255Z","iopub.status.idle":"2021-11-17T09:01:40.900901Z","shell.execute_reply.started":"2021-11-17T09:01:33.592471Z","shell.execute_reply":"2021-11-17T09:01:40.899664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CONFIG :\n    MAX_LEN = 512\n\n    TRAIN_BATCH_SIZE = 8\n    VALID_BATCH_SIZE = 4\n    TEST_BATCH_SIZE = 4\n\n    EPOCHS = 1\n\n    BERT_PATH = \"../input/bertbaseuncased/\"\n\n    MODEL_PATH = \"model3.bin\"\n\n    # training file, validationj file, test file\n\n    TRAINING_FILE = \"input/train_folds.csv\"\n    \n    VALIDATION_FILE = \"input/validation_data_cleaned.csv\"\n    \n    TEST_FILE = \"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\n\n    # define the tokenizer\n\n    TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case = True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:02:53.424588Z","iopub.execute_input":"2021-11-17T09:02:53.424933Z","iopub.status.idle":"2021-11-17T09:02:53.625735Z","shell.execute_reply.started":"2021-11-17T09:02:53.424902Z","shell.execute_reply":"2021-11-17T09:02:53.62472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTDataset :\n    def __init__(self, comment, target) :\n        \"\"\"\n        :param comment : list or numpy array of strings\n        :param targets : list or numpy array which is binary\n        \"\"\"\n\n        self.comment = comment\n        self.target = target\n        self.tokenizer = CONFIG.TOKENIZER\n        self.max_len = CONFIG.MAX_LEN      \n        \n    def __len__(self) :\n        # this returns the length of the dataset\n        return len(self.comment)\n\n    def __getitem__(self, item):\n        # for a given item index, return a dictionary of inputs\n\n        comment = str(self.comment[item])\n        comment = \" \".join(comment.split())\n\n        inputs = self.tokenizer.encode_plus(comment, None, add_special_tokens = True, \n                                            max_length = self.max_len, pad_to_max_length = True,)\n        # ids are ids of tokens generated\n        ids = inputs[\"input_ids\"]\n        # mask is 1 where we have input and 0 where we have padding\n        mask = inputs[\"attention_mask\"]\n        # token_type_ids behave the same way as mask , in case of 2 sentences this is 0 for first sentence and 1 for the first sentence\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return {\n            \"ids\" : torch.tensor(ids, dtype = torch.long),\n\n            \"mask\" : torch.tensor(mask, dtype = torch.long), \n\n            \"token_type_ids\" : torch.tensor(token_type_ids, dtype = torch.long), \n\n            \"targets\" : torch.tensor(self.target[item], dtype = torch.float)\n        }\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:03:00.387202Z","iopub.execute_input":"2021-11-17T09:03:00.387544Z","iopub.status.idle":"2021-11-17T09:03:00.398864Z","shell.execute_reply.started":"2021-11-17T09:03:00.387498Z","shell.execute_reply":"2021-11-17T09:03:00.397381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n\n    def __init__(self) :\n        super(BERTBaseUncased, self).__init__()\n        # we fetch the model from the BERT_PATH defined in config_bert.py\n\n        self.bert = transformers.BertModel.from_pretrained(CONFIG.BERT_PATH)\n\n        # add a dropout for regularization\n        self.bert_drop = nn.Dropout(0.3)\n        # a simple linear layer for output\n        self.out = nn.Linear(768, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        # BERT in default settings return 2 outputs - last hidden state and output of bert pooler layer\n        # we use the output of the pooler layer which is of size (batch_size, hidden_size)\n        # hidden size will be 768 in this case\n\n        _, o2 = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict = False)\n\n        # pass through the dropout layer\n        bo = self.bert_drop(o2)\n        # pass through the linear layer\n        output = self.out(bo)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:03:04.991988Z","iopub.execute_input":"2021-11-17T09:03:04.992657Z","iopub.status.idle":"2021-11-17T09:03:05.002514Z","shell.execute_reply.started":"2021-11-17T09:03:04.992625Z","shell.execute_reply":"2021-11-17T09:03:05.001233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    \"\"\"\n    This function returns loss\n    \"\"\"\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:03:09.155006Z","iopub.execute_input":"2021-11-17T09:03:09.155335Z","iopub.status.idle":"2021-11-17T09:03:09.159904Z","shell.execute_reply.started":"2021-11-17T09:03:09.155308Z","shell.execute_reply":"2021-11-17T09:03:09.159063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler):\n    \"\"\"\n    This is the training function which trains for one epoch\n    \"\"\"\n\n    # put the model in training mode\n    model.train()\n\n    for d in data_loader :\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets = d[\"targets\"]\n\n        ids = ids.to(device, dtype = torch.long)\n        token_type_ids = token_type_ids.to(device, dtype = torch.long)\n        mask = mask.to(device, dtype = torch.long)\n        targets = targets.to(device, dtype = torch.float)\n\n        # zero-grad the optimizer\n        optimizer.zero_grad()\n\n        # pass through the model\n        outputs = model(ids = ids, mask = mask, token_type_ids = token_type_ids)\n\n        # calculate the loss\n        loss = loss_fn(outputs, targets)\n\n        # backward step the loss\n        loss.backward()\n        # step optimizer\n        optimizer.step()\n        # step scheduler\n        scheduler.step()\n\n\ndef eval_fn(data_loader, model, device) :\n    \"\"\"this is the validatiuon function that generates prediction on validation data\n    \"\"\"\n\n    # put the model in eval mode\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n\n    with torch.no_grad() :\n        for d in data_loader:\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype = torch.long)\n            token_type_ids = token_type_ids.to(device, dtype = torch.long)\n            mask = mask.to(device, dtype = torch.long)\n            targets = targets.to(device, dtype = torch.float)\n\n            # pass through the model\n            outputs = model(ids = ids, mask = mask, token_type_ids = token_type_ids)\n\n            targets = targets.cpu().detach()\n            fin_targets.extend(targets.numpy().tolist())\n\n            outputs = torch.sigmoid(outputs.cpu().detach())\n            fin_outputs.extend(outputs.numpy().tolist())\n\n    return fin_outputs, fin_targets","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:03:11.57896Z","iopub.execute_input":"2021-11-17T09:03:11.579835Z","iopub.status.idle":"2021-11-17T09:03:11.594083Z","shell.execute_reply.started":"2021-11-17T09:03:11.579801Z","shell.execute_reply":"2021-11-17T09:03:11.592939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n\n    \"\"\"this function trains the model\n    \"\"\"\n\n    df = pd.read_csv(CONFIG.TRAINING_FILE)\n\n    fold = 3\n\n    df_train = df[df.kfold != fold].reset_index(drop = True)\n\n    df_valid = df[df.kfold == fold].reset_index(drop = True)\n    # initialize BERT dataset from dataset.py\n    # for training dataset\n    train_dataset = BERTDataset(comment = df_train.cleaned_text, target=df_train.y)\n\n    # create training dataloader\n    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = CONFIG.TRAIN_BATCH_SIZE, num_workers = 4)\n\n    # for validation dataset\n    valid_dataset = BERTDataset(comment = df_valid.cleaned_text, target=df_valid.y)\n\n    # create validation dataloader\n    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = CONFIG.VALID_BATCH_SIZE, num_workers = 1)\n\n    # initialize the cuda device\n    device = torch.device(\"cuda\")\n    # load the model\n    model = BERTBaseUncased()\n    model.to(device)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimized_parameters = [{\n         \"params\" : [p for n,p in param_optimizer if not any(nd in n for nd in no_decay)], \n\n         \"weight_decay\" : 0.001, \n    }, \n    {\n        \"params\" : [p for n,p in param_optimizer if any(nd in n for nd in no_decay)],\n\n        \"weight_decay\" : 0.0,\n    },]\n    \n    # calculate the no of training steps\n    # this is used by scheduler\n    num_train_steps = int(len(df_train) / CONFIG.TRAIN_BATCH_SIZE * CONFIG.EPOCHS)\n\n    # AdamW optimizer\n    optimizer = AdamW(optimized_parameters, lr = 3e-5)\n\n    # fetch a scheduler \n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,\n                                                num_training_steps = num_train_steps)\n    # start training the epochs\n    best_accuracy = 0\n\n    for epoch in range(CONFIG.EPOCHS) :\n        train_fn(train_data_loader, model, optimizer, device, scheduler)\n\n        outputs, targets = eval_fn(valid_data_loader, model, device)\n\n        outputs = np.array(outputs) >= 0.5\n        accuracy = metrics.accuracy_score(targets, outputs)\n        print (f\"Accuracy score: {accuracy}\")\n\n        torch.save(model.state_dict(), CONFIG.MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:03:16.114067Z","iopub.execute_input":"2021-11-17T09:03:16.114987Z","iopub.status.idle":"2021-11-17T09:03:16.129654Z","shell.execute_reply.started":"2021-11-17T09:03:16.114945Z","shell.execute_reply":"2021-11-17T09:03:16.128367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(checkpoint) : \n    \"\"\"\n    this function will get predictions from the validation file\n    \"\"\"\n    \n    df = pd.read_csv(CONFIG.VALIDATION_FILE)\n    # filling labels for class BERTDataset\n    df[\"y\"] = -1 \n    \n    test_dataset_more_toxic = BERTDataset(comment = df.more_toxic, target = df.y)\n    \n    # creating test data loader more toxic\n    test_data_loader_more_toxic = torch.utils.data.DataLoader(test_dataset_more_toxic , batch_size = CONFIG.VALID_BATCH_SIZE, num_workers = 1)\n    \n    test_dataset_less_toxic = BERTDataset(comment = df.less_toxic, target = df.y)\n    \n    #creating test data loader less toxic\n    test_data_loader_less_toxic = torch.utils.data.DataLoader(test_dataset_less_toxic, batch_size = CONFIG.VALID_BATCH_SIZE, num_workers = 1)\n    # initialize the cuda device\n    device = torch.device(\"cuda\")\n    # load the model\n    model = BERTBaseUncased()\n    model.to(device)\n    model.load_state_dict(torch.load(checkpoint))\n    \n    outputs_more_toxic, targets_more_toxic = eval_fn(test_data_loader_more_toxic, model, device)\n    del targets_more_toxic\n    \n    outputs_less_toxic, targets_less_toxic = eval_fn(test_data_loader_less_toxic, model, device)\n    del targets_less_toxic\n    \n    outputs_more_toxic_m = [j for i in outputs_more_toxic for j in i]\n    outputs_less_toxic_m = [j for i in outputs_less_toxic for j in i]\n    \n    score = [i>j for i,j in zip(outputs_more_toxic_m, outputs_less_toxic_m)]\n    \n    accuracy = np.mean(score)\n    \n    #score = (outputs_more_toxic > outputs_less_toxic).mean()\n    \n    print (f\"Accuracy on kaggle validation dataset is {accuracy}\")\n    \n    #return outputs_more_toxic, outputs_less_toxic","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checkpoint = \"model.bin\"\n\n#p1, p2 = get_predictions(checkpoint)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions_test(checkpoint) : \n    \"\"\"\n    this function will get predictions from the test file\n    \"\"\"\n    \n    df = pd.read_csv(CONFIG.TEST_FILE)\n    # filling labels for class BERTDataset\n    df[\"y\"] = -1 \n    \n    test_dataset = BERTDataset(comment = df.text, target = df.y)\n    \n    # creating test data loader more toxic\n    test_data_loader = torch.utils.data.DataLoader(test_dataset , batch_size = CONFIG.TEST_BATCH_SIZE, num_workers = 1)\n    \n    \n    # initialize the cuda device\n    device = torch.device(\"cuda\")\n    # load the model\n    model = BERTBaseUncased()\n    model.to(device)\n    model.load_state_dict(torch.load(checkpoint))\n    \n    outputs, targets = eval_fn(test_data_loader, model, device)\n    \n    outputs = [j for i in outputs for j in i]\n    \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:08:22.41873Z","iopub.execute_input":"2021-11-17T09:08:22.419001Z","iopub.status.idle":"2021-11-17T09:08:22.428266Z","shell.execute_reply.started":"2021-11-17T09:08:22.418974Z","shell.execute_reply":"2021-11-17T09:08:22.426773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = \"../input/bertbaseline5folds/model0.bin\"\npreds0 = get_predictions_test(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:09:22.001827Z","iopub.execute_input":"2021-11-17T09:09:22.002168Z","iopub.status.idle":"2021-11-17T09:12:14.453176Z","shell.execute_reply.started":"2021-11-17T09:09:22.002124Z","shell.execute_reply":"2021-11-17T09:12:14.451932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint1 = \"../input/bertbaseline5folds/model.bin\"\npreds1 = get_predictions_test(checkpoint1)\n\ncheckpoint2 = \"../input/bertbaseline5folds/model2.bin\"\npreds2 = get_predictions_test(checkpoint2)\n\ncheckpoint3 = \"../input/bertbaseline5folds/model3.bin\"\npreds3 = get_predictions_test(checkpoint3)\n\ncheckpoint4 = \"../input/bertbaseline5folds/model4.bin\"\npreds4 = get_predictions_test(checkpoint4)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:14:42.80494Z","iopub.execute_input":"2021-11-17T09:14:42.805509Z","iopub.status.idle":"2021-11-17T09:25:28.200909Z","shell.execute_reply.started":"2021-11-17T09:14:42.805477Z","shell.execute_reply":"2021-11-17T09:25:28.199458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = np.mean((preds0, preds1, preds2, preds3, preds4), axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:32:11.45173Z","iopub.execute_input":"2021-11-17T09:32:11.452064Z","iopub.status.idle":"2021-11-17T09:32:11.461926Z","shell.execute_reply.started":"2021-11-17T09:32:11.452Z","shell.execute_reply":"2021-11-17T09:32:11.46042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(CONFIG.TEST_FILE)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:32:47.856019Z","iopub.execute_input":"2021-11-17T09:32:47.856373Z","iopub.status.idle":"2021-11-17T09:32:47.903847Z","shell.execute_reply.started":"2021-11-17T09:32:47.856345Z","shell.execute_reply":"2021-11-17T09:32:47.902933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"score\"] = final_preds","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:35:18.986105Z","iopub.execute_input":"2021-11-17T09:35:18.986398Z","iopub.status.idle":"2021-11-17T09:35:18.993469Z","shell.execute_reply.started":"2021-11-17T09:35:18.98637Z","shell.execute_reply":"2021-11-17T09:35:18.992179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"score\"] = test_df[\"score\"].rank(method = \"first\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[\"comment_id\", \"score\"]].to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:39:02.590977Z","iopub.execute_input":"2021-11-17T09:39:02.591349Z","iopub.status.idle":"2021-11-17T09:39:02.631226Z","shell.execute_reply.started":"2021-11-17T09:39:02.59132Z","shell.execute_reply":"2021-11-17T09:39:02.630418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}