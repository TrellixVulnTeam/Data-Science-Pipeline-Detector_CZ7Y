{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel,AutoConfig\n\n# Utils\nfrom tqdm import tqdm\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    \n    model_name = '../input/roberta-base'\n        \n    learning_rate = 1e-4\n    epochs = 1\n    train_bs =32\n    valid_bs = 64\n    test_bs = 128\n        \n    seed = 2021\n    max_length = 128\n    min_lr = 1e-7\n    scheduler = 'CosineAnnealingLR' # 学习率衰减策略\n    T_max  = 500\n    weight_decay = 1e-6 # 权重衰减 L2正则化 减少过拟合\n    max_grad_norm = 1.0 # 用于控制梯度膨胀，如果梯度向量的L2模超过max_grad_norm，则等比例缩小\n    num_classes = 1\n    margin = 0.5\n    n_fold = 5\n    n_accululate = 1\n    device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    hidden_size =768\n    num_hidden_layers = 24\n    \n    dropout = 0.2\n\n\ntokenizer = AutoTokenizer.from_pretrained(Config.model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATHS = [\n    '../input/robertabase5fold2-linear-256/Loss-Fold-0.bin',\n    '../input/robertabase5fold2-linear-256/Loss-Fold-1.bin',\n    '../input/robertabase5fold2-linear-256/Loss-Fold-2.bin',\n    '../input/robertabase5fold2-linear-256/Loss-Fold-3.bin',\n    '../input/robertabase5fold2-linear-256/Loss-Fold-4.bin'\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(Config.seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']        \n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = JigsawDataset(df, tokenizer, max_length=Config.max_length)\ntest_loader = DataLoader(test_dataset, batch_size=Config.test_bs,\n                         num_workers=2, shuffle=False, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JModel(nn.Module):\n    def __init__(self, checkpoint=Config.model_name, Config=Config):\n        super(JModel, self).__init__()\n        self.checkpoint = checkpoint\n        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.layer_norm = nn.LayerNorm(Config.hidden_size)\n        self.dropout = nn.Dropout(Config.dropout)\n        self.dense = nn.Sequential(\n            nn.Linear(Config.hidden_size, 256),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.Dropout(Config.dropout),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = self.layer_norm(pooled_output)\n        pooled_output = self.dropout(pooled_output)\n        preds = self.dense(pooled_output)\n        return preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JModel(Config.model_name)\n        model.to(Config.device)\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = inference(MODEL_PATHS, test_loader, Config.device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['score'] = preds\ndf['score'] = df['score'].rank(method='first')\ndf.drop('text', axis=1, inplace=True)\ndf.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}