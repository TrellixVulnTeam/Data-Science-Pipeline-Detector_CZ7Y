{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# reading files\ntrain = pd.read_csv(\"/kaggle/input/crowdflower-weather-twitter/train.csv\")\ntest = pd.read_csv('/kaggle/input/crowdflower-weather-twitter/test.csv')\nsub = pd.read_csv('/kaggle/input/crowdflower-weather-twitter/sampleSubmission.csv')\nprint(train.shape)\n\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding taget cols\ntarget_cols = train.columns.difference(test.columns)\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving target columns\ntarget = train[target_cols]\ntrain.drop(target_cols,axis=1,inplace=True)\n#df = pd.concat([train,test],axis=0,ignore_index=True)\n#print(df.shape)\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# flag where location is missing\nfor data in (train,test):\n    missing_loc_idx = data.index[data.location.isnull()].tolist()\n    missing_loc_dict = {}\n    for i in range(len(data)):\n        if i in missing_loc_idx:\n            missing_loc_dict[i] = 1\n        else:\n            missing_loc_dict[i] = 0\n    data['missing_loc']= data.index.map(missing_loc_dict)\n    \n\n###########################################################################\n# flag where state is missing\nfor data in (train,test):\n    missing_state_idx = data.index[data.state.isnull()].tolist()\n    missing_state_dict = {}\n    for i in range(len(data)):\n        if i in missing_state_idx:\n            missing_state_dict[i] = 1\n        else:\n            missing_state_dict[i] = 0\n    data['missing_state']= data.index.map(missing_state_dict)\n#################################################################################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combining text of columns - location,state,tweet\nfor data in (train,test):\n    data['location'] = data['location'].replace(np.nan, '', regex=True)\n    data['state'] = data['state'].replace(np.nan, '', regex=True)\n    data['full_text'] = data['tweet']+' '+data['state']+' '+data['location']\n#####################################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### replacing abbreviation of american states with their names\n\nstates = {\"AL\":\"Alabama\",\"AK\":\"Alaska\",\"AZ\":\"Arizona\",\"AR\":\"Arkansas\",\"CA\":\"California\",\"CO\":\"Colorado\",\"CT\":\"Connecticut\",\"DE\":\"Delaware\",\"FL\":\"Florida\",\"GA\":\"Georgia\",\"HI\":\"Hawaii\",\"ID\":\"Idaho\",\"IL\":\"Illinois\",\"IN\":\"Indiana\",\"IA\":\"Iowa\",\"KS\":\"Kansas\",\"KY\":\"Kentucky\",\"LA\":\"Louisiana\",\"ME\":\"Maine\",\"MD\":\"Maryland\",\"MA\":\"Massachusetts\",\"MI\":\"Michigan\",\"MN\":\"Minnesota\",\"MS\":\"Mississippi\",\"MO\":\"Missouri\",\"MT\":\"Montana\",\"NE\":\"Nebraska\",\"NV\":\"Nevada\",\"NH\":\"New Hampshire\",\"NJ\":\"New Jersey\",\"NM\":\"New Mexico\",\"NY\":\"New York\",\"NC\":\"North Carolina\",\"ND\":\"North Dakota\",\"OH\":\"Ohio\",\"OK\":\"Oklahoma\",\"OR\":\"Oregon\",\"PA\":\"Pennsylvania\",\"RI\":\"Rhode Island\",\"SC\":\"South Carolina\",\"SD\":\"South Dakota\",\"TN\":\"Tennessee\",\"TX\":\"Texas\",\"UT\":\"Utah\",\"VT\":\"Vermont\",\"VA\":\"Virginia\",\"WA\":\"Washington\",\"WV\":\"West Virginia\",\"WI\":\"Wisconsin\",\"WY\":\"Wyoming\",\"NYC\":\"New York\"}\n\nimport re\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|)|\\|/||:]',r' ',cleaned)      \n    return cleaned\n\ndef acronym(s, dct):\n  return ' '.join([dct.get(i, i) for i in s.split()])\n\nfor data in (train,test):\n    full_text_dict = {}\n    for i,row in data.iterrows():\n            full_text_dict[i] = acronym(cleanpunc(row['full_text']),states)\n            #print(i)       \n    data['full_txt2'] = data.index.map(full_text_dict)\n############################################    \nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# straight forward text features\nfor data in (train,test):\n    data['num_word'] = data['tweet'].apply(lambda x : len(x.split()))\n    data['num_char'] = data['tweet'].apply(lambda x : len(x))\n    data['avg_word_length']= data['num_char']+1/data['num_word']\n######################################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# whether text contain link or not\nfor data in (train,test):\n    linkword = \"{link}\"\n    link_dict = {}\n    for idx in range(len(data)):\n        text = data.loc[idx,'full_text']\n        if linkword in text.split():\n            link_dict[idx]=1\n        else:\n            link_dict[idx]=0\n    data['link_word'] = data.index.map(link_dict)\n\n# whether '@' is present or not in text\nfor data in (train,test):\n    mention= \"@mention\"\n    mention_dict = {}\n    for idx in range(len(data)):\n        text = data.loc[idx,'full_text']\n        if mention in cleanpunc(text).split():\n            mention_dict[idx]=1\n        else:\n            mention_dict[idx]=0\n    data['mention_word'] = data.index.map(mention_dict)\n\n############################################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### cleaning full_text \nimport re\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words('english'))\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\ndef cleanhtml (sentence):\n    cleantext = re.sub(r'http\\S+',r'',sentence)\n    return cleantext\n\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|)|\\|/|:]',r' ',cleaned)      \n    return cleaned\n\nfor data in (train,test):\n    str1=' '\n    final_string=[]\n    s=''\n    for sent in data['full_text']:\n        filter_sent = []\n        rem_html = cleanhtml(sent)\n        rem_punc = cleanpunc (rem_html)\n        for w in rem_punc.split():\n            if ((w.isalpha())):\n                if (w.lower() not in stopwords):\n                    s=(ps.stem(w.lower())).encode('utf8')\n                    s=(w.lower()).encode('utf8')\n                    filter_sent.append(s)\n                else:\n                    continue\n            else:\n                continue\n        str1 = b\" \".join(filter_sent)\n        final_string.append(str1)\n    data['clean_text'] = np.array(final_string)\n#################################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generating tf-idf vector of clean text\nimport scipy as sp\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import OneHotEncoder\n\nvectorizer = TfidfVectorizer(encoding='utf-8', ngram_range=(1, 3), max_df=0.75, min_df=5) #max_features=12500\n\ntrain_vec = vectorizer.fit_transform(train.clean_text)\ntest_vec = vectorizer.transform(test.clean_text)\n############################################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TruncatedSVD\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\ntrain_svd = svd.fit_transform(train_vec)\ntest_svd = svd.transform(test_vec)\n############################################\nprint('done')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['svd1','svd2']\nsvd_train =  pd.DataFrame(train_svd, columns=col)\nsvd_test = pd.DataFrame(test_svd, columns=col)\n############################################\nprint('done')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import time\n\n#import numpy as np\n#import matplotlib.pyplot as plt\n\n#from sklearn.cluster import MiniBatchKMeans, KMeans\n#from sklearn.metrics.pairwise import pairwise_distances_argmin\n#from sklearn.datasets import make_blobs\n#batch_size = 45\n#mbk = MiniBatchKMeans(init='k-means++', n_clusters=10, batch_size=batch_size,\n#                      n_init=25, max_no_improvement=10,verbose=0)\n\n#t0 = time.time()\n#grp_x = mbk.fit_predict(train_vec)\n#grp_test = mbk.predict(test_vec)\n#t_mini_batch = time.time() - t0\n#print(t_mini_batch )\n#train['grp'] = grp_x\n#test['grp']= grp_test \n############################################\n#print('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = pd.get_dummies(data=train, columns=['grp'])\n#test = pd.get_dummies(data=test, columns=['grp'])\n############################################\n#print('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as sp\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\nvectorizer2 = TfidfVectorizer(encoding='utf-8', ngram_range=(1, 3), max_df=0.75, min_df=5,max_features=50000)\n\nX = sp.sparse.hstack((vectorizer2.fit_transform(train.clean_text),normalize(train[['num_word', 'num_char', 'avg_word_length']].values),train[['missing_loc', 'missing_state','link_word', 'mention_word']],svd_train [['svd1','svd2']]),format='csr')\n\nX_columns=vectorizer.get_feature_names()+train[['num_word', 'num_char', 'avg_word_length','missing_loc', 'missing_state','link_word', 'mention_word']].columns.tolist() + svd_train [['svd1','svd2']].columns.tolist()\nprint(X.shape)\ntest_sp = sp.sparse.hstack((vectorizer2.transform(test.clean_text),normalize(test[['num_word', 'num_char', 'avg_word_length']].values),test[['missing_loc', 'missing_state','link_word', 'mention_word']],svd_test [['svd1','svd2']]),format='csr')\n\ntest_columns=vectorizer.get_feature_names()+test[['num_word', 'num_char', 'avg_word_length','missing_loc', 'missing_state','link_word', 'mention_word']].columns.tolist() + svd_test [['svd1','svd2']].columns.tolist()\n\nprint(test_sp.shape)\n############################################\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting train data for cross validation\nfrom sklearn.model_selection import train_test_split\nx, x_test, y, y_test = train_test_split(X,target,test_size=0.2,train_size=0.8, random_state = 0)\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# running single model for each target columns\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn import linear_model\nfrom sklearn.svm import LinearSVR\n# Fit regression model\ncols = target.columns\nparams = {'n_estimators': 500, 'max_depth': 3, 'min_samples_split': 2,\n          'learning_rate': 0.01, 'loss': 'ls'}\ndict_preds = {}\n#clf = GradientBoostingRegressor(**params)\nclf = Ridge(random_state=25)\n#clf = SGDRegressor(max_iter=6000)\n#clf = linear_model.Lasso(alpha=1)\n#clf = LinearSVR(max_iter=2500)\n#clf = LinearRegression()\nfor col in cols:\n    print('start',col)\n    clf.fit(x,y[col])\n    rmse = sqrt(mean_squared_error(y_test[col], clf.predict(x_test)))\n    print(\"RMSE: %.4f\" % rmse)\n    dict_preds[col]=clf.predict(test_sp)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction dataframe and file saved\ndf_res = pd.DataFrame(dict_preds)\ndf_res[df_res < 0] = 0\ndf_res[df_res > 1] = 1\ndf_res['id']= sub['id']\ndf_res = df_res[sub.columns]\ndf_res.to_csv('sub1.csv',index=False)\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using classifier chains\nfrom sklearn.multioutput import RegressorChain\n#from sklearn.naive_bayes import GaussianNB\n# initialize classifier chains multi-label classifier\n# with a gaussian naive bayes base classifier\nregressor = RegressorChain(Ridge(random_state=25))\n\n# train\nregressor.fit(x,y)\n\n# predict - cross validation\npredictions = regressor.predict(x_test)\nsqrt(mean_squared_error(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict - test_sp dataset\npred2 = regressor.predict(test_sp)\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction dataframe and file saved\n\ncols = target.columns\ndf_res2 = pd.DataFrame(pred2,columns=cols)\ndf_res2[df_res2 < 0] = 0\ndf_res2[df_res2 > 1] = 1\ndf_res2['id']= sub['id']\ndf_res2 = df_res2[sub.columns]\ndf_res2.to_csv('sub2.csv',index=False)\nprint('done')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}