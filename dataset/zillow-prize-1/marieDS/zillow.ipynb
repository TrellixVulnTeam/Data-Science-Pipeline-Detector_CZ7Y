{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/zillow-prize-1//train_2017.csv', parse_dates=[\"transactiondate\"])\nprop_df = pd.read_csv('../input/zillow-prize-1/properties_2017.csv',low_memory=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n\nplt.rcParams['figure.figsize'] = (8.5, 5)\nplt.rcParams[\"patch.force_edgecolor\"] = True\nsns.mpl.rc(\"figure\", figsize=(8.5,5))\npd.set_option('max_colwidth', 400)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ANALYSE","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prop_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prop_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#je fusionne les 2 jeux de données en un seul\ndf= train_df.merge(prop_df, how='left', on='parcelid')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CLEANING","metadata":{}},{"cell_type":"markdown","source":"##  Logerror:\n\nObservation de la variable cible","metadata":{}},{"cell_type":"code","source":"plt.scatter(range(df.shape[0]), np.sort(df.logerror.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('logerror', fontsize=12)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extreme values","metadata":{}},{"cell_type":"code","source":"# choose the max and min value \nlogerror_max = np.percentile(df.logerror.values, 99)\nlogerror_min = np.percentile(df.logerror.values, 1)\n\ndf = df[df['logerror']<logerror_max]\ndf = df[df['logerror']>logerror_min]\n\nsns.distplot(df.logerror.values, bins=50, kde=False)\nplt.xlabel('logerror', fontsize=12)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Duplicates","metadata":{}},{"cell_type":"code","source":"print(\"Nombre de lignes dupliquées\", df.duplicated().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing values","metadata":{}},{"cell_type":"code","source":"print(\"Nombre de valeurs manquantes\", df.isna().sum().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete columns with missing values > 40%\ndf = df.loc[:, df.isna().sum() < 0.4*df.shape[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep only numeric variabls\ndf_quant = df.select_dtypes(include=['float64','int64'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_nan = df_quant.isna().sum().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Correlation\n","metadata":{}},{"cell_type":"code","source":"corr = df_quant.corr()\ncorr10 = corr['logerror'].abs().sort_values(ascending=False)\ncorr10.head(11)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aucune des variables ne presente de correlation forte, ici le model de regression lineaire n'a pas de sens.","metadata":{}},{"cell_type":"markdown","source":"# MACHINE LEARNING","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error \nfrom sklearn.tree import DecisionTreeRegressor\n\nimport xgboost as xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_metrics(data, target, model):\n    ''' \n    Get R2, RMSE and MAE for a model\n    data = X\n    target = y\n    '''\n    X_train, X_test, y_train, y_test = train_test_split(data, target, train_size=0.8, random_state=1)\n    model = model\n    model.fit(X_train, y_train)\n    ytrainpred = model.predict(X_train)\n    ytestpred = model.predict(X_test)\n    print(model)\n    print (\"Training set : R2 = {} RMSE = {}\\nTesting set : R2 = {} RMSE = {}\" \n      .format(round(r2_score(y_train, ytrainpred),3), round(mean_squared_error(y_train, ytrainpred, squared=False),3), round(r2_score(y_test, ytestpred),3), round(mean_squared_error(y_test, ytestpred, squared=False),3)))\n    print(\"Training Set : Mean Absolute Error = {}\".format(mean_absolute_error(ytrainpred, y_train)))\n    print(\"Testing Set : Mean Absolute Error = {}\".format(mean_absolute_error(ytestpred, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# definit target\ny = df.logerror\n\n# defniti explanation's variables\ndf_quant = df_quant.drop(columns = ['logerror'])\nX = df_quant","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X = pd.DataFrame(my_imputer.fit_transform(X))\n\n# Imputation removed column names; put them back\nimputed_X.columns = X.columns\n\nX = imputed_X.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## REGRESSION LINEAIRE","metadata":{}},{"cell_type":"markdown","source":"meme si nous avons vu plus haut qu'il n'existait pas de correlation lineaire antre notre target et une variable quantitative, je vais vérifier la possible existence d'une telle correlation avec les variables categorielles.","metadata":{}},{"cell_type":"markdown","source":"- propertycountylandusecode:  County land use code i.e. it's zoning at the county level\n\n- propertyzoningdesc:  Description of the allowed land uses (zoning) for that property","metadata":{}},{"cell_type":"code","source":"df_cat = df[['propertycountylandusecode','propertyzoningdesc']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df_cat = pd.get_dummies(df_cat, prefix=['pcluc', 'pzd'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df_cat.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df_cat.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regression lineaire multiple avec regulation Ridge","metadata":{}},{"cell_type":"code","source":"from sklearn import linear_model\nregr = linear_model.Ridge(alpha=100)\nget_metrics(new_df_cat, y, regr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ces variables categorielle n'ont pas assez de correlation avec logerror pour etre pertinentes","metadata":{}},{"cell_type":"markdown","source":"## DecisionTreeRegression ","metadata":{}},{"cell_type":"markdown","source":"j'utilise ce model pour definir quelles sont les variables les plus utiles à la prediction.","metadata":{}},{"cell_type":"code","source":"get_metrics(X, y, DecisionTreeRegressor(random_state=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RANDOMFOREST","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1)\nmodel = RandomForestRegressor(random_state=1)\nmodel.fit(X_train, y_train)\nytrainpred = model.predict(X_train)\nytestpred = model.predict(X_test)\nprint(model)\nprint (\"Training set : R2 = {} RMSE = {}\\nTesting set : R2 = {} RMSE = {}\" .format(round(r2_score(y_train, ytrainpred),3), round(mean_squared_error(y_train, ytrainpred, squared=False),3), round(r2_score(y_test, ytestpred),3), round(mean_squared_error(y_test, ytestpred, squared=False),3)))\nprint(\"Training Set : Mean Absolute Error = {}\".format(mean_absolute_error(ytrainpred, y_train)))\nprint(\"Testing Set : Mean Absolute Error = {}\".format(mean_absolute_error(ytestpred, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = model.feature_importances_\nnp.sort(a)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = X.columns\ncorr = model.feature_importances_\ndf = pd.DataFrame({'name':name, 'corr':corr})\ndf.sort_values(by='corr', ascending = False)\n# df.plot('name', 'corr', kind='scatter')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBOOST","metadata":{}},{"cell_type":"code","source":"\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.15)\n\nxgbr = xgb.XGBRegressor(verbosity=0)\nprint(xgbr)\n\nxgbr.fit(xtrain, ytrain) \n\nscore = xgbr.score(xtrain, ytrain)   \n\nprint(\"Training score: \", score)  \n\n# - cross validataion \nscores = cross_val_score(xgbr, xtrain, ytrain, cv=5)\nprint(\"Mean cross-validation score: %.2f\" % scores.mean())\n\nkfold = KFold(n_splits=10, shuffle=True)\nkf_cv_scores = cross_val_score(xgbr, xtrain, ytrain, cv=kfold )\nprint(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean()) \n\nypred = xgbr.predict(xtest)\nmse = mean_squared_error(ytest, ypred)\nprint(\"MSE: %.2f\" % mse)\nprint(\"RMSE: %.2f\" % (mse**(1/2.0)))\n\n\nx_ax = range(len(ytest))\nplt.scatter(x_ax, ytest, s=5, color=\"blue\", label=\"original\")\nplt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training Set : Mean Absolute Error = {}\".format(mean_absolute_error(ytrainpred, y_train)))\nprint(\"Testing Set : Mean Absolute Error = {}\".format(mean_absolute_error(ytestpred, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SUBMISSION","metadata":{}},{"cell_type":"code","source":"# read the sample_submisison file into a dataframe, for sake of clarity let's limit the number of rows to 5\ndf_samples = pd.read_csv('../input/zillow-prize-1/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's unpivot the dataframe to produce the shape regression models usually expect\n# consisting of pairs `parcelid`, `transactiondate``\n\ndef reshape_for_model(df):\n    \"\"\"Unipivot the submission data and apply some renamings\"\"\"\n    df = pd.melt(df, ['ParcelId'])\n    df.drop('value', axis=1, inplace=True)\n    df.columns = ['parcelid', 'transactiondate']\n    df['transactiondate'] = df['transactiondate'].apply(\n        lambda date_str: \"%s-%s-01\" %(date_str[:4], date_str[-2:]))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_reshaped = reshape_for_model(df_samples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.columns\nnew_df = prop_df[['parcelid', 'bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid',\n       'calculatedbathnbr', 'calculatedfinishedsquarefeet',\n       'finishedsquarefeet12', 'fips', 'fullbathcnt', 'heatingorsystemtypeid',\n       'latitude', 'longitude', 'lotsizesquarefeet', 'propertylandusetypeid',\n       'rawcensustractandblock', 'regionidcity', 'regionidcounty',\n       'regionidzip', 'roomcnt', 'unitcnt', 'yearbuilt',\n       'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'assessmentyear',\n       'landtaxvaluedollarcnt', 'taxamount', 'censustractandblock']]\n\n\n# new_df = new_df.sample(n=10000)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\np_1 = xgbr.predict(new_df)\np_2 = xgbr.predict(new_df)\np_3 = xgbr.predict(new_df)\np_4 = xgbr.predict(new_df)\np_5 = xgbr.predict(new_df)\np_6 = xgbr.predict(new_df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = np.concatenate((p_1, p_2, p_3, p_4, p_5, p_6), axis=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_reshaped['prediction']  = pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### to get the original shape of our submission sample file, we can now pivot the table again\n# and remove the column names\ndef reshape_for_submission(df):\n    \"\"\"Reformat the transactiondate and pivot the data\"\"\"\n    df['transactiondate'] = df['transactiondate'].apply(lambda td: \"%s%s\" %(td[:4],td[5:7]))\n    df = df.pivot(index='parcelid', columns='transactiondate', values='prediction')\n    df = df.reset_index()\n#     df = df.rename_axis(None,1)\n    df = df.rename(index=str, columns={\"parcelid\": \"ParcelId\"})\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tosubmit = reshape_for_submission(df_reshaped)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tosubmit.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tosubmit.to_csv('final.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}