{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport sys\nimport os\nimport random\npd.options.display.max_columns = None\npd.options.mode.chained_assignment = None\npd.options.display.float_format\n\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostRegressor, Pool\nimport lightgbm as lgb\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nsns.set_style( 'white' )\npylab.rcParams[ 'figure.figsize' ] = 8 , 6\n\n\"\"\"\n    Load training data from csv file\n\"\"\"\ndef load_training_data(file_name):\n    return pd.read_csv(file_name)\n\n\n\"\"\"\n    Load house properties data\n\"\"\"\ndef load_properties_data(file_name):\n\n    # Helper function for parsing the flag attributes\n    def convert_true_to_float(df, col):\n        df.loc[df[col] == 'true', col] = '1'\n        df.loc[df[col] == 'Y', col] = '1'\n        df[col] = df[col].astype(float)\n\n    prop = pd.read_csv(file_name, dtype={\n        'propertycountylandusecode': str,\n        'hashottuborspa': str,\n        'propertyzoningdesc': str,\n        'fireplaceflag': str,\n        'taxdelinquencyflag': str\n    })\n\n    for col in ['hashottuborspa', 'fireplaceflag', 'taxdelinquencyflag']:\n        convert_true_to_float(prop, col)\n\n    return prop\n\n\n\"\"\"\n    Assign better names to all feature columns of 'properties' table\n\"\"\"\ndef rename_columns(prop):\n    prop.rename(columns={\n        'parcelid': 'parcelid',  # Unique identifier of parcels\n        'airconditioningtypeid': 'cooling_id',  # type of cooling system (if any), 1~13\n        'architecturalstyletypeid': 'architecture_style_id',  # Architectural style of the home, 1~27\n        'basementsqft': 'basement_sqft',  # Size of the basement\n        'bathroomcnt': 'bathroom_cnt',  # Number of bathrooms (including fractional bathrooms)\n        'bedroomcnt': 'bedroom_cnt',  # Number of bedrooms\n        'buildingclasstypeid': 'framing_id',  # The building framing type, 1~5\n        'buildingqualitytypeid': 'quality_id',  # building condition from best (lowest) to worst (highest)\n        'calculatedbathnbr': 'bathroom_cnt_calc',  # Same meaning as 'bathroom_cnt'?\n        'decktypeid': 'deck_id',  # Type of deck (if any)\n        'finishedfloor1squarefeet': 'floor1_sqft',  # Size of finished living area on first floor\n        'calculatedfinishedsquarefeet': 'finished_area_sqft_calc',  # calculated total finished living area\n        'finishedsquarefeet12': 'finished_area_sqft',  # Same meaning as 'finished_area_sqft_calc'?\n        'finishedsquarefeet13': 'perimeter_area',  # Perimeter living area\n        'finishedsquarefeet15': 'total_area',  # Total area\n        'finishedsquarefeet50': 'floor1_sqft_unk',  # Same meaning as 'floor1_sqft'?\n        'finishedsquarefeet6': 'base_total_area',  # Base unfinished and finished area\n        'fips': 'fips',  # Federal Information Processing Standard code\n        'fireplacecnt': 'fireplace_cnt',  # Number of fireplaces in the home (if any)\n        'fullbathcnt': 'bathroom_full_cnt',  # Number of full bathrooms\n        'garagecarcnt': 'garage_cnt',  # Total number of garages\n        'garagetotalsqft': 'garage_sqft',  # Total size of the garages\n        'hashottuborspa': 'spa_flag',  # Whether the home has a hot tub or spa\n        'heatingorsystemtypeid': 'heating_id',  # type of heating system, 1~25\n        'latitude': 'latitude',  # latitude of the middle of the parcel multiplied by 1e6\n        'longitude': 'longitude',  # longitude of the middle of the parcel multiplied by 1e6\n        'lotsizesquarefeet': 'lot_sqft',  # Area of the lot in sqft\n        'poolcnt': 'pool_cnt', # Number of pools in the lot (if any)\n        'poolsizesum': 'pool_total_size',  # Total size of the pools\n        'pooltypeid10': 'pool_unk_1',\n        'pooltypeid2': 'pool_unk_2',\n        'pooltypeid7': 'pool_unk_3',\n        'propertycountylandusecode': 'county_landuse_code',\n        'propertylandusetypeid': 'landuse_type_id' ,  # Type of land use the property is zoned for, 25 categories\n        'propertyzoningdesc': 'zoning_description',  # Allowed land uses (zoning) for that property\n        'rawcensustractandblock': 'census_1',\n        'regionidcity': 'city_id',  # City in which the property is located (if any)\n        'regionidcounty': 'county_id',  # County in which the property is located\n        'regionidneighborhood': 'neighborhood_id',  # Neighborhood in which the property is located\n        'regionidzip': 'region_zip',\n        'roomcnt': 'room_cnt',  # Total number of rooms in the principal residence\n        'storytypeid': 'story_id',  # Type of floors in a multi-story house, 1~35\n        'threequarterbathnbr': 'bathroom_small_cnt',  # Number of 3/4 bathrooms\n        'typeconstructiontypeid': 'construction_id',  # Type of construction material, 1~18\n        'unitcnt': 'unit_cnt',  # Number of units the structure is built into (2=duplex, 3=triplex, etc)\n        'yardbuildingsqft17': 'patio_sqft',  # Patio in yard\n        'yardbuildingsqft26': 'storage_sqft',  # Storage shed/building in yard\n        'yearbuilt': 'year_built',  # The year the principal residence was built\n        'numberofstories': 'story_cnt',  # Number of stories or levels the home has\n        'fireplaceflag': 'fireplace_flag',  # Whether the home has a fireplace\n        'structuretaxvaluedollarcnt': 'tax_structure',\n        'taxvaluedollarcnt': 'tax_parcel',\n        'assessmentyear': 'tax_year',  # The year of the property tax assessment (2015 for 2016 data)\n        'landtaxvaluedollarcnt': 'tax_land',\n        'taxamount': 'tax_property',\n        'taxdelinquencyflag': 'tax_overdue_flag',  # Property taxes are past due as of 2015\n        'taxdelinquencyyear': 'tax_overdue_year',  # Year for which the unpaid propert taxes were due\n        'censustractandblock': 'census_2'\n    }, inplace=True)\n\n\n\"\"\"\n    Convert some categorical variables to 'category' type\n    Convert float64 variables to float32\n    Note: In LightGBM, negative integer value for a categorical feature will be treated as missing value\n\"\"\"\ndef retype_columns(prop):\n\n    def float_to_categorical(df, col):\n        df[col] = df[col] - df[col].min()  # Convert the categories to have smaller labels (start from 0)\n        df.loc[df[col].isnull(), col] = -1\n        df[col] = df[col].astype(int).astype('category')\n\n    list_float2categorical = ['cooling_id', 'architecture_style_id', 'framing_id',\n                             'heating_id', 'county_id', 'construction_id', 'fips', 'landuse_type_id',\n                             'county_landuse_code_id','zoning_description_id']\n\n    # Convert categorical variables to 'category' type, and float64 variables to float32\n    for col in prop.columns:\n        if col in list_float2categorical:\n            float_to_categorical(prop, col)\n        elif prop[col].dtype.name == 'float64':\n            prop[col] = prop[col].astype(np.float32)\n\n    gc.collect()\n\n\n\"\"\"\n    Compute and return datetime aggregate feature tables from a training set\n    The returned tables can be joined for both training and inference\n\"\"\"\ndef compute_datetime_aggregate_features(train):\n    # Add temporary year/month/quarter columns\n    dt = pd.to_datetime(train.transactiondate).dt\n    train['year'] = dt.year\n    train['month'] = dt.month\n    train['quarter'] = dt.quarter\n\n    # Median logerror within the category\n    logerror_year = train.groupby('year').logerror.median().to_frame() \\\n                                .rename(index=str, columns={\"logerror\": \"logerror_year\"})\n    logerror_month = train.groupby('month').logerror.median().to_frame() \\\n                                .rename(index=str, columns={\"logerror\": \"logerror_month\"})\n    logerror_quarter = train.groupby('quarter').logerror.median().to_frame() \\\n                                .rename(index=str, columns={\"logerror\": \"logerror_quarter\"})\n\n    logerror_year.index = logerror_year.index.map(int)\n    logerror_month.index = logerror_month.index.map(int)\n    logerror_quarter.index = logerror_quarter.index.map(int)\n\n    # Drop the temporary columns\n    train.drop(['year', 'month', 'quarter'], axis=1, errors='ignore', inplace=True)\n\n    return logerror_year, logerror_month, logerror_quarter\n\n\n\"\"\"\n    Add aggregrate datetime features to a feature table\n    The input table needs to have a 'transactiondate' columns\n    The 'transactiondate' column is deleted from the table in the end\n\"\"\"\ndef add_datetime_aggregate_features(df, logerror_year, logerror_month, logerror_quarter):\n    # Add temporary year/month/quarter columns\n    dt = pd.to_datetime(df.transactiondate).dt\n    df['year'] = dt.year\n    df['month'] = dt.month\n    df['quarter'] = dt.quarter\n\n    # Join the aggregate features\n    df = df.merge(how='left', right=logerror_year, on='year')\n    df = df.merge(how='left', right=logerror_month, on='month')\n    df = df.merge(how='left', right=logerror_quarter, on='quarter')\n\n    # Drop the temporary columns\n    df = df.drop(['year', 'month', 'quarter', 'transactiondate'], axis=1, errors='ignore')\n    return df\n\n\n\"\"\"\n    Add simple 'year', 'month', and 'quarter' categorical features to a DataFrame\n\"\"\"\ndef add_simple_datetime_features(df):\n    dt = pd.to_datetime(df.transactiondate).dt\n    df['year'] = (dt.year - 2016).astype(int)\n    df['month'] = (dt.month).astype(int)\n    df['quarter'] = (dt.quarter).astype(int)\n    df.drop(['transactiondate'], axis=1, inplace=True)\n\n\n\"\"\"\n    Look at how complete (i.e. no missing value) each feature is\n\"\"\"\ndef print_complete_percentage(df):\n    complete_percent = []\n    total_cnt = len(df)\n    for col in df.columns:\n        complete_cnt = total_cnt - (df[col].isnull()).sum()\n        complete_cnt -= (df[col] == -1).sum()\n        complete_percent.append((col, complete_cnt * 1.00 / total_cnt))\n    complete_percent.sort(key=lambda x: x[1], reverse=True)\n    for col, percent in complete_percent:\n        print(\"{}: {}\".format(col, percent))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Load in properties data\nprop_2016 = load_properties_data(\"/kaggle/input/zillow-prize-1/properties_2016.csv\")\nprop_2017 = load_properties_data(\"/kaggle/input/zillow-prize-1/properties_2017.csv\")\n\nassert len(prop_2016) == len(prop_2017)\nprint(\"Number of properties: {}\".format(len(prop_2016)))\nprint(\"Number of property features: {}\".format(len(prop_2016.columns)-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_landuse_code_df(prop_2016, prop_2017):\n    temp = prop_2016.groupby('county_landuse_code')['county_landuse_code'].count()\n    landuse_codes = list(temp[temp >= 300].index)\n    temp = prop_2017.groupby('county_landuse_code')['county_landuse_code'].count()\n    landuse_codes += list(temp[temp >= 300].index)\n    landuse_codes = list(set(landuse_codes))\n    df_landuse_codes = pd.DataFrame({'county_landuse_code': landuse_codes,\n                                     'county_landuse_code_id': range(len(landuse_codes))})\n    return df_landuse_codes\n\ndef get_zoning_desc_code_df(prop_2016, prop_2017):\n    temp = prop_2016.groupby('zoning_description')['zoning_description'].count()\n    zoning_codes = list(temp[temp >= 5000].index)\n    temp = prop_2017.groupby('zoning_description')['zoning_description'].count()\n    zoning_codes += list(temp[temp >= 5000].index)\n    zoning_codes = list(set(zoning_codes))\n    df_zoning_codes = pd.DataFrame({'zoning_description': zoning_codes,\n                                     'zoning_description_id': range(len(zoning_codes))})\n    return df_zoning_codes\n\ndef process_columns(df, df_landuse_codes, df_zoning_codes):\n    df = df.merge(how='left', right=df_landuse_codes, on='county_landuse_code')\n    df = df.drop(['county_landuse_code'], axis=1)\n    \n    df = df.merge(how='left', right=df_zoning_codes, on='zoning_description')\n    df = df.drop(['zoning_description'], axis=1)\n    \n    df.loc[df.county_id == 3101, 'county_id'] = 0\n    df.loc[df.county_id == 1286, 'county_id'] = 1\n    df.loc[df.county_id == 2061, 'county_id'] = 2\n    \n    df.loc[df.landuse_type_id == 279, 'landuse_type_id'] = 261\n    return df\n\nrename_columns(prop_2016)\nrename_columns(prop_2017)\n\ndf_landuse_codes = get_landuse_code_df(prop_2016, prop_2017)\ndf_zoning_codes = get_zoning_desc_code_df(prop_2016, prop_2017)\nprop_2016 = process_columns(prop_2016, df_landuse_codes, df_zoning_codes)\nprop_2017 = process_columns(prop_2017, df_landuse_codes, df_zoning_codes)\n\nretype_columns(prop_2016)\nretype_columns(prop_2017)\n\nprop_2017.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2016 = load_training_data(\"/kaggle/input/zillow-prize-1/train_2016_v2.csv\")\ntrain_2017 = load_training_data(\"/kaggle/input/zillow-prize-1/train_2017.csv\")\n\nprint(\"Number of 2016 transaction records: {}\".format(len(train_2016)))\nprint(\"Number of 2017 transaction records: {}\".format(len(train_2017)))\nprint(\"\\n\", train_2016.head())\nprint(\"\\n\", train_2017.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for prop in [prop_2016, prop_2017]:\n    prop['avg_garage_size'] = prop['garage_sqft'] / prop['garage_cnt']\n    \n    prop['property_tax_per_sqft'] = prop['tax_property'] / prop['finished_area_sqft_calc']\n    \n    # Rotated Coordinates\n    prop['location_1'] = prop['latitude'] + prop['longitude']\n    prop['location_2'] = prop['latitude'] - prop['longitude']\n    prop['location_3'] = prop['latitude'] + 0.5 * prop['longitude']\n    prop['location_4'] = prop['latitude'] - 0.5 * prop['longitude']\n    \n    # 'finished_area_sqft' and 'total_area' cover only a strict subset of 'finished_area_sqft_calc' in terms of \n    # non-missing values. Also, when both fields are not null, the values are always the same.\n    # So we can probably drop 'finished_area_sqft' and 'total_area' since they are redundant\n    # If there're some patterns in when the values are missing, we can add two isMissing binary features\n    prop['missing_finished_area'] = prop['finished_area_sqft'].isnull().astype(np.float32)\n    prop['missing_total_area'] = prop['total_area'].isnull().astype(np.float32)\n    prop.drop(['finished_area_sqft', 'total_area'], axis=1, inplace=True)\n    prop['missing_bathroom_cnt_calc'] = prop['bathroom_cnt_calc'].isnull().astype(np.float32)\n    prop.drop(['bathroom_cnt_calc'], axis=1, inplace=True)\n    \n    # 'room_cnt' has many zero or missing values\n    # On the other hand, 'bathroom_cnt' and 'bedroom_cnt' have few zero or missing values\n    # Add an derived room_cnt feature by adding bathroom_cnt and bedroom_cnt\n    prop['derived_room_cnt'] = prop['bedroom_cnt'] + prop['bathroom_cnt']\n    \n    # Average area in sqft per room\n    mask = (prop.room_cnt >= 1)  # avoid dividing by zero\n    prop.loc[mask, 'avg_area_per_room'] = prop.loc[mask, 'finished_area_sqft_calc'] / prop.loc[mask, 'room_cnt']\n    \n    # Use the derived room_cnt to calculate the avg area again\n    mask = (prop.derived_room_cnt >= 1)\n    prop.loc[mask,'derived_avg_area_per_room'] = prop.loc[mask,'finished_area_sqft_calc'] / prop.loc[mask,'derived_room_cnt']\n    \nprop_2017.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_aggregate_features(df, group_col, agg_cols):\n    df[group_col + '-groupcnt'] = df[group_col].map(df[group_col].value_counts())\n    print(df[group_col + '-groupcnt'])\n    new_columns = []  # New feature columns added to the DataFrame\n\n    for col in agg_cols:\n        aggregates = df.groupby(group_col, as_index=False)[col].agg([np.mean])\n        aggregates.columns = [group_col + '-' + col + '-' + s for s in ['mean']]\n        new_columns += list(aggregates.columns)\n        df = df.merge(how='left', right=aggregates, on=group_col)\n        \n    for col in agg_cols:\n        mean = df[group_col + '-' + col + '-mean']\n        diff = df[col] - mean\n        \n        df[group_col + '-' + col + '-' + 'diff'] = diff\n        if col != 'year_built':\n            df[group_col + '-' + col + '-' + 'percent'] = diff / mean\n        \n    # Set the values of the new features to NaN if the groupcnt is too small (prevent overfitting)\n    threshold = 100\n    df.loc[df[group_col + '-groupcnt'] < threshold, new_columns] = np.nan\n    \n    # Drop the mean features since they turn out to be not useful\n    df.drop([group_col+'-'+col+'-mean' for col in agg_cols], axis=1, inplace=True)\n    \n    gc.collect()\n    return df\n\ngroup_col = 'region_zip'\nagg_cols = ['lot_sqft', 'year_built', 'finished_area_sqft_calc',\n            'tax_structure', 'tax_land', 'tax_property', 'property_tax_per_sqft']\nprop_2016 = add_aggregate_features(prop_2016, group_col, agg_cols)\nprop_2017 = add_aggregate_features(prop_2017, group_col, agg_cols)\n\nprop_2017.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2016 = train_2016.merge(how='left', right=prop_2016, on='parcelid')\ntrain_2017 = train_2017.merge(how='left', right=prop_2017, on='parcelid')\ntrain = pd.concat([train_2016, train_2017], axis=0, ignore_index=True)\n\nprint(\"\\nCombined training set size: {}\".format(len(train)))\n\n# Add datetime features to training data\nadd_simple_datetime_features(train)\n\ntrain.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def catboost_drop_features(features):\n    # id and label (not features)\n    unused_feature_list = ['parcelid', 'logerror']\n\n    # too many missing (LightGBM is robust against bad/unrelated features, so this step might not be needed)\n    missing_list = ['framing_id', 'architecture_style_id', 'story_id', 'perimeter_area', 'basement_sqft', 'storage_sqft']\n    unused_feature_list += missing_list\n\n    # not useful\n    bad_feature_list = ['county_landuse_code_id','zoning_description_id','fireplace_flag', 'deck_id', 'pool_unk_1', 'construction_id', 'county_id', 'fips']\n    unused_feature_list += bad_feature_list\n\n    \n\n    return features.drop(unused_feature_list, axis=1, errors='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Read DataFrames from hdf5\nfeatures_2016 = prop_2016  # All features except for datetime for 2016\nfeatures_2017 = prop_2017  # All features except for datetime for 2017\ntrain = train # Concatenated 2016 and 2017 training data with labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncatboost_features = catboost_drop_features(train)\nprint(\"Number of features for CatBoost: {}\".format(len(catboost_features.columns)))\ncatboost_features.head(5)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify feature names and categorical features for CatBoost\nfeature_names = [s for s in catboost_features.columns]\ncategorical_features = ['cooling_id', 'heating_id', 'landuse_type_id', 'year', 'month', 'quarter']\n\ncategorical_indices = []\nfor i, n in enumerate(catboost_features.columns):\n    if n in categorical_features:\n        categorical_indices.append(i)\nprint(categorical_indices)\n# Prepare training and cross-validation data\ncatboost_label = train.logerror.astype(np.float32)\nprint(catboost_label.head())\n\n# Transform to Numpy matrices\ncatboost_X = catboost_features.values\ncatboost_X=catboost_X.astype(object)\nfor i, n in enumerate(catboost_X):\n    for j, m in enumerate(n):\n        if j in categorical_indices:\n            catboost_X[i,j]=int(catboost_X[i,j])\ncatboost_y = catboost_label.values\n\n# Perform shuffled train/test split\nnp.random.seed(42)\nrandom.seed(10)\nX_train, X_val, y_train, y_val = train_test_split(catboost_X, catboost_y, test_size=0.2)\n\n\n\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nprint(\"X_val shape: {}\".format(X_val.shape))\nprint(\"y_val shape: {}\".format(y_val.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {}\nparams['loss_function'] = 'MAE'\nparams['eval_metric'] = 'MAE'\nparams['nan_mode'] = 'Min'  # Method to handle NaN (set NaN to either Min or Max)\nparams['random_seed'] = 0\n\nparams['iterations'] = 1000  # default 1000, use early stopping during training\nparams['learning_rate'] = 0.17  # default 0.03\n\nparams['border_count'] = 254  # default 254 (alias max_bin, suggested to keep at default for best quality)\n\nparams['max_depth'] = 6  # default 6 (must be <= 16, 6 to 10 is recommended)\nparams['random_strength'] = 1  # default 1 (used during splitting to deal with overfitting, try different values)\nparams['l2_leaf_reg'] = 5  # default 3 (used for leaf value calculation, try different values)\nparams['allow_const_label'] = True\nparams['bagging_temperature'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_test_features(features_2016, features_2017):\n    test_features_2016 = catboost_drop_features(features_2016)\n    test_features_2017 = catboost_drop_features(features_2017)\n    \n    test_features_2016['year'] = 0\n    test_features_2017['year'] = 1\n    \n    # 11 and 12 lead to bad results, probably due to the fact that there aren't many training examples for those two\n    test_features_2016['month'] = 10\n    test_features_2017['month'] = 10\n    \n    test_features_2016['quarter'] = 4\n    test_features_2017['quarter'] = 4\n    \n    return test_features_2016, test_features_2017\n\n\"\"\"\n    Helper method that makes predictions on the test set and exports results to csv file\n    'models' is a list of models for ensemble prediction (len=1 means using just a single model)\n\"\"\"\ndef predict_and_export(models, features_2016, features_2017, file_name):\n    # Construct DataFrame for prediction results\n    submission_2016 = pd.DataFrame()\n    submission_2017 = pd.DataFrame()\n    submission_2016['ParcelId'] = features_2016.parcelid\n    submission_2017['ParcelId'] = features_2017.parcelid\n    \n    test_features_2016, test_features_2017 = transform_test_features(features_2016, features_2017)\n    \n    pred_2016, pred_2017 = [], []\n    for i, model in enumerate(models):\n        print(\"Start model {} (2016)\".format(i))\n        pred_2016.append(model.predict(test_features_2016))\n        print(\"Start model {} (2017)\".format(i))\n        pred_2017.append(model.predict(test_features_2017))\n    \n    # Take average across all models\n    mean_pred_2016 = np.mean(pred_2016, axis=0)\n    mean_pred_2017 = np.mean(pred_2017, axis=0)\n    \n    submission_2016['201610'] = [float(format(x, '.4f')) for x in mean_pred_2016]\n    submission_2016['201611'] = submission_2016['201610']\n    submission_2016['201612'] = submission_2016['201610']\n\n    submission_2017['201710'] = [float(format(x, '.4f')) for x in mean_pred_2017]\n    submission_2017['201711'] = submission_2017['201710']\n    submission_2017['201712'] = submission_2017['201710']\n    \n    submission = submission_2016.merge(how='inner', right=submission_2017, on='ParcelId')\n    \n    print(\"Length of submission DataFrame: {}\".format(len(submission)))\n    print(\"Submission header:\")\n    print(submission.head())\n    submission.to_csv(file_name, index=False)\n    return submission, pred_2016, pred_2017 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers (if any) from training data\noutlier_threshold = 0.4\nmask = (abs(catboost_y) <= outlier_threshold)\ncatboost_X = catboost_X[mask, :]\ncatboost_y = catboost_y[mask]\nprint(\"catboost_X: {}\".format(catboost_X.shape))\nprint(\"catboost_y: {}\".format(catboost_y.shape))\n\n# Train multiple models\nbags = 8\nmodels = []\nparams['iterations'] = 1000\nfor i in range(bags):\n    print(\"Start training model {}\".format(i))\n    params['random_seed'] = i\n    np.random.seed(42)\n    random.seed(36)\n    model = CatBoostRegressor(**params)\n    model.fit(catboost_X, catboost_y, cat_features=categorical_indices, verbose=False)\n    models.append(model)\n    \n# Sanity check (make sure scores on a small portion of the dataset are reasonable)\nfor i, model in enumerate(models):\n    print(\"model {}: {}\".format(i, abs(model.predict(X_val) - y_val).mean() * 100))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = '/kaggle/working/final_catboost_ensemble_x8.csv'\nsubmission, pred_2016, pred_2017 = predict_and_export(models, features_2016, features_2017, file_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nlgb_single = pd.read_csv('/kaggle/input/lgb-catboost/final_lgb_single.csv')\ncatboost_x8 = pd.read_csv('/kaggle/input/lgb-catboost/final_catboost_ensemble_x8.csv')\nprint(\"Finished Loading the prediction results.\")\n\nweight = 0.7\nstack = pd.DataFrame()\nstack['ParcelId'] = lgb_single['ParcelId']\nfor col in ['201610', '201611', '201612', '201710', '201711', '201712']:\n    stack[col] = weight * catboost_x8[col] + (1 - weight) * lgb_single[col]\n\nprint(stack.head())\nstack.to_csv('/kaggle/working/final_stack.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}