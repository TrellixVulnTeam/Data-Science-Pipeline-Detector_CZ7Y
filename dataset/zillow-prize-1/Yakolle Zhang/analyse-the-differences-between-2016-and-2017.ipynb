{"cells":[{"metadata":{"_cell_guid":"b8075710-a8c0-4c4c-8404-2e88dc2a0a72","_uuid":"93f2343bba9cc069ac61bfdcdd56c255dbd665a5"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMRegressor\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b4de41d8-4773-4c87-9b55-b372720e8667","_uuid":"09768fd3224e35bb9a5df7b2769902be8e9bd651"},"source":"# unique the train_data\ndef unique_train_data(rec):\n    return rec.sort_values(by='transactiondate').iloc[-1]","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"c68959fc-43dc-4a01-9c8b-71511040d625","_uuid":"9c8e73953bc2484e20f46cf49cb4975415d62715"},"source":"# load features\ndata_dir='../input/'\nprop_2016=pd.read_csv(data_dir+'properties_2016.csv',index_col='parcelid').fillna(-1)\nprop_2017=pd.read_csv(data_dir+'properties_2017.csv',index_col='parcelid').fillna(-1)\nprop_2017=prop_2017.loc[prop_2016.index]\nprint(prop_2016.shape,prop_2017.shape,np.sum(prop_2016.index!=prop_2017.index))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"db3f2a83-ae6a-4b9b-a28a-02bba9d65e28","_uuid":"750f483799b7cf8ff5b9a36b67658282fedeec3a"},"source":"# compare the features between 2016 and 2017\nfor col in prop_2017.columns:\n    s=prop_2017[col]\n    s1=prop_2016[col]\n    print(col,np.sum(s!=s1))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"5b5ef18d-903e-4e05-b931-e07644b6e06f","_uuid":"74d0f2314ac3b528088c6b865536aed0fde77377"},"source":"# simple preprocess\nprop_2016 = prop_2016.drop(['regionidcounty', 'rawcensustractandblock','assessmentyear','propertyzoningdesc', \n                'propertycountylandusecode'],axis=1)\nobj_columns = prop_2016.dtypes.loc[np.object == prop_2016.dtypes].index\nfor col in obj_columns:\n    prop_2016[col] = prop_2016[col].apply(lambda ele: str(ele))\n    prop_2016[col] = LabelEncoder().fit_transform(prop_2016[col])\n    \nprop_2017 = prop_2017.drop(['regionidcounty', 'rawcensustractandblock','assessmentyear','propertyzoningdesc', \n                'propertycountylandusecode'],axis=1)\nobj_columns = prop_2017.dtypes.loc[np.object == prop_2017.dtypes].index\nfor col in obj_columns:\n    prop_2017[col] = prop_2017[col].apply(lambda ele: str(ele))\n    prop_2017[col] = LabelEncoder().fit_transform(prop_2017[col])\n    \nprint('preprocess done')","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"6d1fe8d4-2e1a-41dc-aef0-e701c3760284","_uuid":"03aaf9145326c7eeeedbd6f73f09e48afe3907bc"},"source":"# load train data\ntrain_2016 = pd.read_csv(data_dir+'train_2016_v2.csv',index_col='parcelid')\ntrain_2016 = train_2016.groupby('parcelid').apply(unique_train_data)\ntrain_2016['sale_month'] = pd.to_datetime(train_2016['transactiondate']).dt.month\n\ntrain_2017 = pd.read_csv(data_dir+'train_2017.csv',index_col='parcelid')\ntrain_2017 = train_2017.groupby('parcelid').apply(unique_train_data)\ntrain_2017['sale_month'] = pd.to_datetime(train_2017['transactiondate']).dt.month\n\nco_index = np.intersect1d(train_2016.index,train_2017.index)\nprint(len(co_index))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"# use the common data between 2016 and 2017 to predict the 2016 logerror\ny = train_2016.loc[co_index,'logerror']\nx = prop_2016.loc[co_index]\nx['sale_month'] = train_2016.loc[co_index,'sale_month']\n\n# use LGBMRegressor to fit the data and print features' importances\nmodel = LGBMRegressor(objective='regression', n_estimators=200, learning_rate=.0125, num_leaves=24, max_depth=11,\n                      max_bin=80, min_child_samples=1, min_child_weight=0, min_split_gain=4e-05, subsample=.3,\n                      colsample_bytree=.45, subsample_freq=1, reg_alpha=4, reg_lambda=4, seed=0, nthread=2)\nmodel.fit(x,y)\nims = []\nfor i in range(len(model.feature_importances_)):\n    ims.append((x.columns[i], model.feature_importances_[i]))\nims = sorted(ims, key=lambda pair : pair[1], reverse=True)\nprint('which factors impact the 2016 logerror:')\nfor col, im in ims:\n    print(col,im)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"# use the common data between 2016 and 2017 to predict the 2017 logerror\ny = train_2017.loc[co_index,'logerror']\nx = prop_2017.loc[co_index]\nx['sale_month'] = train_2017.loc[co_index,'sale_month']\n\n# use LGBMRegressor to fit the data and print features' importances\nmodel = LGBMRegressor(objective='regression', n_estimators=200, learning_rate=.0125, num_leaves=24, max_depth=11,\n                      max_bin=80, min_child_samples=1, min_child_weight=0, min_split_gain=4e-05, subsample=.3,\n                      colsample_bytree=.45, subsample_freq=1, reg_alpha=4, reg_lambda=4, seed=0, nthread=2)\nmodel.fit(x,y)\nims = []\nfor i in range(len(model.feature_importances_)):\n    ims.append((x.columns[i], model.feature_importances_[i]))\nims = sorted(ims, key=lambda pair : pair[1], reverse=True)\nprint('which factors impact the 2017 logerror:')\nfor col, im in ims:\n    print(col,im)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_kg_hide-input":false,"_cell_guid":"d29db91a-3308-486d-a88a-3f3c697c488c","_kg_hide-output":false,"_uuid":"fece84bf0632db0c6577305ea62ce21036682bad"},"source":"# use the gap of features to predict the gap of logerror\ny = train_2017.loc[co_index,'logerror'] - train_2016.loc[co_index,'logerror']\nx = prop_2017.loc[co_index] - prop_2016.loc[co_index]\nx['sale_month'] = train_2017.loc[co_index,'sale_month'] - train_2016.loc[co_index,'sale_month']\n\n# use LGBMRegressor to fit the data and print features' importances\nmodel = LGBMRegressor(objective='regression', n_estimators=200, learning_rate=.0125, num_leaves=24, max_depth=11,\n                      max_bin=80, min_child_samples=1, min_child_weight=0, min_split_gain=4e-05, subsample=.3,\n                      colsample_bytree=.45, subsample_freq=1, reg_alpha=4, reg_lambda=4, seed=0, nthread=2)\nmodel.fit(x,y)\nims = []\nfor i in range(len(model.feature_importances_)):\n    ims.append((x.columns[i], model.feature_importances_[i]))\nims = sorted(ims, key=lambda pair : pair[1], reverse=True)\nprint('which factors cause the change of logerror:')\nfor col, im in ims:\n    print(col,im)","execution_count":null,"cell_type":"code","outputs":[]}],"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1}