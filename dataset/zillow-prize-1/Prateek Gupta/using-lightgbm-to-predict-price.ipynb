{"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n#reference:\n#https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-zillow-prize/notebook\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"142265d9-eb75-4f61-8ca2-0ea4a951784f","_uuid":"1ed71497a84ad2ce4fde665256692a001e115434"},"cell_type":"code"},{"source":"train_df = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\ntrain_df.shape","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"89953174-8876-4b6d-8191-4c30e975c553","_uuid":"0893414b8556865f4e1ed763947e03961c41204d"},"cell_type":"code"},{"source":"train_df.head()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"dbd5e805-75cf-408f-958d-abb9143cd79e","_uuid":"03a171068a9dc23b5a52f3650cb87e0154da8287"},"cell_type":"code"},{"source":"train_df.dtypes","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9f1d8c10-2ca7-48ca-bb92-7758dd2a55ad","_uuid":"46e4513cf20d578e7b4994462068ab55d85dc36a"},"cell_type":"code"},{"source":"**Logerror:\nTarget variable for this competition is \"logerror\" field. So let us do some analysis on this field first.**","metadata":{"_cell_guid":"a124c4a5-0d98-47e9-a113-446a7bc9eb39","_uuid":"243c9bd1e1a9345117d632daad6624d81319a882","collapsed":true},"cell_type":"markdown"},{"source":"plt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.logerror.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('logerror', fontsize=12)\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"058bc426-e6ae-45d2-ae1c-f070ed786ed8","_uuid":"2e2029e9da8901b7a0680b071990bc231680ccdb"},"cell_type":"code"},{"source":"**This looks nice with some outliers at both the ends.!**","metadata":{},"cell_type":"markdown"},{"source":"**these outliers are the most interesting datapoints.  It looks like this is where the Zillow algorithm fails, so if we can predict these failures it would mean a huge score increase.****","metadata":{},"cell_type":"markdown"},{"source":"#replacing outliers with the values for the 1st and 99th percentile\nulimit = np.percentile(train_df.logerror.values, 99)\nllimit = np.percentile(train_df.logerror.values, 1)\ntrain_df['logerror'].loc[train_df['logerror']>ulimit] = ulimit\ntrain_df['logerror'].loc[train_df['logerror']<llimit] = llimit\n\nplt.figure(figsize=(12,8))\nsns.distplot(train_df.logerror.values, bins=50, kde=False)\nplt.xlabel('logerror', fontsize=12)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#let us explore the date field. Let us first check the number of transactions in each month.\ntrain_df['transaction_month'] = train_df['transactiondate'].dt.month\ncnt_srs = train_df['transaction_month'].value_counts()\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])\nplt.xticks(rotation='vertical')\nplt.xlabel('Month of transaction', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#let's see parcelid\n(train_df['parcelid'].value_counts().reset_index())['parcelid'].value_counts()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"**So most of the parcel ids are appearing only once in the dataset.**","metadata":{},"cell_type":"markdown"},{"source":"#Now let us explore the properties_2016 file\nprop_df = pd.read_csv(\"../input/properties_2016.csv\",low_memory=False)\nprop_df.shape","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"prop_df.head()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#check missing data in properties\nmissing_df = prop_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missing_df.missing_count.values, color='orange')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#Let us explore the latitude and longitude variable to begin with\nplt.figure(figsize=(12,12))\nsns.jointplot(x=prop_df.latitude.values, y=prop_df.longitude.values, size=10)\nplt.ylabel('Longitude', fontsize=12)\nplt.xlabel('Latitude', fontsize=12)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#So let us merge the two files and then carry out our analysis.\ntrain_df = pd.merge(train_df, prop_df, on='parcelid', how='left')\ntrain_df.head()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#check the dtypes of different types of variable\npd.options.display.max_rows = 65\ndtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#count the dtypes\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#check the number of Nulls in this new merged dataset\nmissing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df['missing_ratio'] = missing_df['missing_count'] / train_df.shape[0]\nmissing_df.loc[missing_df['missing_ratio']>0.999]","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"**let us first take the 'float' variables alone and then get the correlation with the target variable to see how they are related.**","metadata":{},"cell_type":"markdown"},{"source":"# Let us just impute the missing values with mean values to compute correlation coefficients\nmean_values = train_df.mean(axis=0)\ntrain_df_new = train_df.fillna(mean_values, inplace=True)\n\n# Now let us look at the correlation coefficient of each of these variables\nx_cols = [col for col in train_df_new.columns if col not in ['logerror'] if train_df_new[col].dtype=='float64']\n\nlabels = []\nvalues = []\nfor col in x_cols:\n    labels.append(col)\n    values.append(np.corrcoef(train_df_new[col].values, train_df_new.logerror.values)[0,1])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,40))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\n#autolabel(rects)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"**There are few variables at the top of this graph without any correlation values. I guess they have only one unique value and hence no correlation value. Let us confirm the same.**","metadata":{},"cell_type":"markdown"},{"source":"corr_zero_cols = ['assessmentyear', 'storytypeid', 'pooltypeid2', 'pooltypeid7', 'pooltypeid10', 'poolcnt', 'decktypeid', 'buildingclasstypeid']\nfor col in corr_zero_cols:\n    print(col, len(train_df_new[col].unique()))","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#Let us take the variables with high correlation values and then do some analysis on them\ncorr_df_sel = corr_df.loc[(corr_df['corr_values']>0.02) | (corr_df['corr_values'] < -0.01)]\ncorr_df_sel","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"cols_to_use = corr_df_sel.col_labels.tolist()\n\ntemp_df = train_df[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#Let us seee how the finished square feet 12 varies with the log error\ncol = \"finishedsquarefeet12\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].loc[train_df[col]>ulimit] = ulimit\ntrain_df[col].loc[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df.finishedsquarefeet12.values, y=train_df.logerror.values, size=10, color=color[4])\nplt.ylabel('Log Error', fontsize=12)\nplt.xlabel('Finished Square Feet 12', fontsize=12)\nplt.title(\"Finished square feet 12 Vs Log error\", fontsize=15)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"**Seems the range of logerror narrows down with increase in finished square feet 12 variable. Probably larger houses are easy to predict**","metadata":{},"cell_type":"markdown"},{"source":"#Calculated finished square feet:\ncol = \"calculatedfinishedsquarefeet\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].loc[train_df[col]>ulimit] = ulimit\ntrain_df[col].loc[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df.calculatedfinishedsquarefeet.values, y=train_df.logerror.values, size=10, color=color[5])\nplt.ylabel('Log Error', fontsize=12)\nplt.xlabel('Calculated finished square feet', fontsize=12)\nplt.title(\"Calculated finished square feet Vs Log error\", fontsize=15)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#Bathroom Count:\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"bathroomcnt\", data=train_df)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Bathroom', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Bathroom count\", fontsize=15)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#check how the log error changes based on this\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"bathroomcnt\", y=\"logerror\", data=train_df)\nplt.ylabel('Log error', fontsize=12)\nplt.xlabel('Bathroom Count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"How log error changes with bathroom count?\", fontsize=15)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#Bedroom count:\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"bedroomcnt\", data=train_df)\nplt.ylabel('Frequency', fontsize=12)\nplt.xlabel('Bedroom Count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Bedroom count\", fontsize=15)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"train_df['bedroomcnt'].loc[train_df['bedroomcnt']>7] = 7\nplt.figure(figsize=(12,8))\nsns.violinplot(x='bedroomcnt', y='logerror', data=train_df)\nplt.xlabel('Bedroom count', fontsize=12)\nplt.ylabel('Log Error', fontsize=12)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"col = \"taxamount\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].loc[train_df[col]>ulimit] = ulimit\ntrain_df[col].loc[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df['taxamount'].values, y=train_df['logerror'].values, size=10, color='g')\nplt.ylabel('Log Error', fontsize=12)\nplt.xlabel('Tax Amount', fontsize=12)\nplt.title(\"Tax Amount Vs Log error\", fontsize=15)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#YearBuilt:\nfrom ggplot import *\nggplot(aes(x='yearbuilt', y='logerror'), data=train_df) + \\\n    geom_point(color='steelblue', size=1) + \\\n    stat_smooth()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#let us see how the logerror varies with respect to latitude and longitude\nggplot(aes(x='latitude', y='longitude', color='logerror'), data=train_df) + \\\n    geom_point() + \\\n    scale_color_gradient(low = 'red', high = 'blue')","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#Let us take the variables with highest positive correlation and highest negative correlation to see if we can see some visible patterns\nggplot(aes(x='finishedsquarefeet12', y='taxamount', color='logerror'), data=train_df) + \\\n    geom_point(alpha=0.7) + \\\n    scale_color_gradient(low = 'pink', high = 'blue')","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"**Now let us build a non-linear model to get the important variables by building Extra Trees model**","metadata":{},"cell_type":"markdown"},{"source":"train_y = train_df['logerror'].values\ncat_cols = [\"hashottuborspa\", \"propertycountylandusecode\", \"propertyzoningdesc\", \"fireplaceflag\", \"taxdelinquencyflag\"]\ntrain_df = train_df.drop(['parcelid', 'logerror', 'transactiondate', 'transaction_month']+cat_cols, axis=1)\nfeat_names = train_df.columns.values\n\nfrom sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=25, max_depth=30, max_features=0.3, n_jobs=-1, random_state=0)\nmodel.fit(train_df, train_y)\n\n## plot the importances ##\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"**Seems \"tax amount\" is the most importanct variable followed by \"structure tax value dollar count\" and \"land tax value dollor count\"**","metadata":{},"cell_type":"markdown"},{"source":"import xgboost as xgb\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1,\n    'seed' : 0\n}\ndtrain = xgb.DMatrix(train_df, train_y, feature_names=train_df.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"**Using xgboost, the important variables are: 'structured tax value dollar count' followed by 'latitude' and 'calculated finished square feet'**","metadata":{},"cell_type":"markdown"},{"source":"# Parameters\nXGB_WEIGHT = 0.6500\nBASELINE_WEIGHT = 0.0056\nBASELINE_PRED = 0.0115\n\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport gc","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"Reference:https://www.kaggle.com/aharless/xgb-w-o-outliers-lgb-with-outliers-combined/code","metadata":{},"cell_type":"markdown"},{"source":"print( \"\\nReading data from disk ...\")\nprop = pd.read_csv('../input/properties_2016.csv',low_memory=False)\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\",low_memory=False)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"print( \"\\nProcessing data for LightGBM ...\" )\nfor c, dtype in zip(prop.columns, prop.dtypes):\n    if dtype == np.float64:\n        prop[c] = prop[c].astype(np.float32)\n\ndf_train = train.merge(prop, how='left', on='parcelid')\ndf_train.fillna(df_train.median(),inplace = True)\n\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"train_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\ndel df_train; gc.collect()\n\nx_train = x_train.values.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label=y_train)","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"params = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          \nparams['sub_feature'] = 0.5      \nparams['bagging_fraction'] = 0.85 \nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        \nparams['min_data'] = 500         \nparams['min_hessian'] = 0.05     \nparams['verbose'] = 0\n\nprint(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 430)\n\ndel d_train; gc.collect()\ndel x_train; gc.collect()\n\nprint(\"\\nPrepare for LightGBM prediction ...\")\nprint(\"   Read sample file ...\")\nsample = pd.read_csv('../input/sample_submission.csv')\nprint(\"   ...\")\nsample['parcelid'] = sample['ParcelId']\nprint(\"   Merge with property data ...\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\nprint(\"   ...\")\ndel sample, prop; gc.collect()\nprint(\"   ...\")\nx_test = df_test[train_columns]\nprint(\"   ...\")\ndel df_test; gc.collect()\nprint(\"   Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nprint(\"   ...\")\nx_test = x_test.values.astype(np.float32, copy=False)\n\nprint(\"\\nStart LightGBM prediction ...\")\n# num_threads > 1 will predict very slow in kernal\nclf.reset_parameter({\"num_threads\":1})\np_test = clf.predict(x_test)\n\ndel x_test; gc.collect()\n\nprint( \"\\nUnadjusted LightGBM predictions:\" )\nprint( pd.DataFrame(p_test).head() )","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"print( \"\\nPreparing results for write ...\" )\ny_pred=[]\n\nfor i,predict in enumerate(p_test):\n    y_pred.append(str(round(predict,4)))\ny_pred=np.array(y_pred)\n\nproperties = pd.read_csv('../input/properties_2016.csv')\n\noutput = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n# set col 'ParceID' to first col\ncols = output.columns.tolist()\ncols = cols[-1:] + cols[:-1]\noutput = output[cols]\nfrom datetime import datetime\n\nprint( \"\\nWriting results to disk ...\" )\noutput.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n\nprint( \"\\nFinished ...\" )","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.3","file_extension":".py","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3}}}}