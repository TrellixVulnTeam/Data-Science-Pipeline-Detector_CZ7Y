{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b000e33-f0c2-9e94-14b3-ea474d5154c3"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom scipy import stats\nimport missingno as msno\nfrom datetime import datetime\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kendalltau\nimport warnings\n\ncolor = sns.color_palette()\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8623318b-8184-2f20-4827-258a31c6861b"},"outputs":[],"source":"train_df = pd.read_csv(\"../input/train_2016.csv\", parse_dates=[\"transactiondate\"])\nprop_df = pd.read_csv(\"../input/properties_2016.csv\")\ntrain_df = pd.merge(train_df, prop_df, on='parcelid', how='left')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57cfa686-3236-8e2c-b838-7d94bff32867"},"outputs":[],"source":"# initial\ncol_tar = \"logerror\"\ncol_id = \"parcelid\"\nprint(\"SHAPE: \" + str(train_df.shape))\nprint(\"\\n\" + \"=\"*90)\nprint(\"\\nSAMPLE DATA:\" )\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(train_df.head(2).transpose())\n    \n    \n# target distribution\n\nulimit = np.percentile(train_df[col_tar].values, 99)\nllimit = np.percentile(train_df[col_tar].values, 1)\ntrain_df[col_tar].ix[train_df[col_tar]>ulimit] = ulimit\ntrain_df[col_tar].ix[train_df[col_tar]<llimit] = llimit\n\nfig,ax = plt.subplots()\nfig.set_size_inches(20,5)\nsns.distplot(train_df[col_tar].values, bins=50,kde=False,color=\"#34495e\",ax=ax)\nax.set(xlabel=col_tar, ylabel='VIF Score',title=\"Distribution Of Dependent Variable\")\nplt.show()\n\n# dtype print, you should encode this   \n\ndtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"count\", \"column_type\"]\nprint(\"\\n\" + \"=\"*90)\nprint(\"\\n DTYPE_DF: \" )\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(dtype_df)\n\ndtype_count_df = dtype_df.groupby(\"column_type\").aggregate('count').reset_index()\nprint(\"\\n\" + \"=\"*90)\nprint(\"\\n DTYPE_COUNT_DF: \" )\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(dtype_count_df)\n    \nfig,ax = plt.subplots()\nfig.set_size_inches(20,len(dtype_count_df))\nplt.ylabel('column_type', fontsize=20)\nplt.xlabel('count', fontsize=20)\nplt.title(\"Dtype Count\", fontsize=30)\nsns.barplot(data=dtype_count_df,y=\"column_type\",x=\"count\",ax=ax,palette=\"Blues_d\",orient=\"h\")\nplt.show()\n\n# missing values\n\nmissing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df['missing_ratio'] = missing_df['missing_count'] / train_df.shape[0]\nmissing_df_top = missing_df.ix[missing_df['missing_ratio']>0.8].sort_values(by=\"missing_ratio\", ascending=False)\nprint(\"\\n\" + \"=\"*90)\nprint(\"\\n MISSING_DF: \" )\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(missing_df_top)\n\nfig,ax = plt.subplots()\nfig.set_size_inches(20,len(missing_df_top))\nsns.barplot(data=missing_df_top,y=\"column_name\",x=\"missing_count\",ax=ax,color=\"#34495e\",orient=\"h\")\nplt.ylabel('column_type', fontsize=20)\nplt.xlabel('count', fontsize=20)\nplt.title(\"Missing Value Count\", fontsize=30)\nplt.show()\n\n\n\n# dtype print, you should encode this \n# missing values\n# drop col Object\n# detect col Date, generate year, month, day_month, day_week, plot with target value and # of occurences\n# detect long, lat and plot\n# correlation, plot, imp1\n# make predict with xgboost and random_forest, plot importance, imp2, imp3\n# imp1, imp2, imp3 => 10 most important feature\n#"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"349fec29-ba58-0eca-44e7-87c25e50a86e"},"outputs":[],"source":"# dtype print, you should encode this \n# missing values\n# drop col Object\ndropping = dtype_df[dtype_df['column_type'] == 'object']['count'].values.tolist() + missing_df_top[missing_df_top['missing_ratio'] > 0.9]['column_name'].values.tolist()\nfor drop in dropping:\n    print(\"Drop: \"+ str(drop))\n\ntrain_df_0 = train_df.drop(labels=dropping,axis=1)\ntrain_df_0.head(2)\n# detect col Date, generate year, month, day_month, day_week, plot with target value and # of occurences\n# detect long, lat and plot\n# correlation, plot, imp1\n# make predict with xgboost and random_forest, plot importance, imp2, imp3\n# imp1, imp2, imp3 => 10 most important feature"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e526d296-a681-89cb-6679-f117612c16f1"},"outputs":[],"source":"col_date = dtype_df[dtype_df['column_type'] == 'datetime64[ns]']['count'].values.tolist()[0]\n\ntrain_date_df = train_df.copy()\n\ntrain_date_df[col_date+'_year'] = train_date_df[col_date].dt.year\ntrain_date_df[col_date+'_month'] = train_date_df[col_date].dt.month\ntrain_date_df[col_date+'_day'] = train_date_df[col_date].dt.day\ntrain_date_df[col_date+'_dayofyear'] = train_date_df[col_date].dt.dayofyear\ntrain_date_df[col_date+'_weekday'] = train_date_df[col_date].dt.weekday\n\nfor col in [col_date+'_year',col_date+'_month',col_date+'_day',col_date+'_dayofyear',col_date+'_weekday']:\n    print(col)\n    cnt_srs = train_date_df[col].value_counts()\n    plt.figure(figsize=(12,6))\n    sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=\"#34495e\")\n    plt.xticks(rotation='vertical')\n    plt.xlabel(col + ' of transaction', fontsize=12)\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.show()\n\n    train_group_temp = train_date_df.groupby(col)['logerror'].mean().to_frame().reset_index()\n    plt.figure(figsize=(12,6))\n    sns.pointplot(x=train_group_temp[col], y=train_group_temp[\"logerror\"], data=train_group_temp, join=True,color=\"#34495e\")\n    plt.xlabel(col + ' of transaction', fontsize=12)\n    plt.ylabel('Mean target', fontsize=12)\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d2cb46e-54b6-baa3-e879-ccb44056a16b"},"outputs":[],"source":"lng = False\nlat = False\nfor col in train_df.columns:\n    if \"longitude\" in col:\n        lng = True\n    if \"latitude\" in col:\n        lat = True\nif(lng and lat):\n    plt.figure(figsize=(12,12))\n    sns.jointplot(x=prop_df.latitude.values, y=prop_df.longitude.values, size=10)\n    plt.ylabel('Longitude', fontsize=12)\n    plt.xlabel('Latitude', fontsize=12)\n    plt.show()\n# detect long, lat and plot\n# correlation, plot, imp1\n# make predict with xgboost and random_forest, plot importance, imp2, imp3\n# imp1, imp2, imp3 => 10 most important feature"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c3c345d-82c0-aab4-a29a-19211babcf10"},"outputs":[],"source":"# Let us just impute the missing values with mean values to compute correlation coefficients #\nmean_values = train_df.mean(axis=0)\ntrain_df_new = train_df.fillna(mean_values, inplace=True)\n\n# Now let us look at the correlation coefficient of each of these variables #\nx_cols = [col for col in train_df_new.columns if col not in [col_tar] if train_df_new[col].dtype=='float64']\n\nlabels = []\nvalues = []\nfor col in x_cols:\n    labels.append(col)\n    values.append(np.corrcoef(train_df_new[col].values, train_df_new[col_tar].values)[0,1])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n\nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,40))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='#34495e')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\n#autolabel(rects)\nplt.show()\n\ncorr_df[\"abs\"] = np.abs(corr_df[\"corr_values\"])\ncol_use_0 = corr_df.sort_values(by=\"abs\",ascending=False)[:10][\"col_labels\"].tolist()\n\ntemp_df = train_df[col_use_0 + [col_tar]]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()\n\n\n# correlation, plot, imp1\n# make predict with xgboost and random_forest, plot importance, imp2, imp3\n# imp1, imp2, imp3 => 10 most important feature"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86ece756-d470-0c34-b0f1-9b8145c9973d"},"outputs":[],"source":"from sklearn import model_selection, preprocessing\nimport xgboost as xgb\n\ntrain_df_new = train_df_new.fillna(-999)\nfor f in train_df_new.columns:\n    if train_df_new[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df_new[f].values)) \n        train_df_new[f] = lbl.transform(list(train_df_new[f].values))\n        \ntrain_y = train_df_new[col_tar].values\ntrain_X = train_df_new.drop([col_tar,col_id,col_date], axis=1)\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\ndtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d03bb6af-e204-5cee-e4e5-4e30016bdef7"},"outputs":[],"source":"featureImportance = model.get_fscore()\nfeatures = pd.DataFrame()\nfeatures['features'] = featureImportance.keys()\nfeatures['importance'] = featureImportance.values()\nfeatures.sort_values(by=['importance'],ascending=False,inplace=True)\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nplt.xticks(rotation=90)\nsns.barplot(data=features.head(15),x=\"importance\",y=\"features\",ax=ax,orient=\"h\",color=\"#34495e\")\ncol_use_1 = features[:10][\"features\"].tolist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79ac2fda-82f9-3f9f-c229-3b51a703d9cd"},"outputs":[],"source":"from sklearn import ensemble\nmodel0 = ensemble.ExtraTreesRegressor(n_estimators=25, max_depth=30, max_features=0.3, n_jobs=-1, random_state=0)\nmodel0.fit(train_X, train_y)\n\nimportances = model0.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model0.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\nfeat_names = train_X.columns\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"#34495e\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()\ncol_use_2 = feat_names[indices][:10].tolist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"270f452b-66b8-2922-dffd-588dfbfe4920"},"outputs":[],"source":"cols = {}\nfor col in (col_use_0 + col_use_1 + col_use_2):\n    if col not in cols.keys():\n        cols[col] = 1\n    else:\n        cols[col] = cols[col] + 1\n\ncol_df = pd.DataFrame(cols,index=[0]).transpose()\ncol_df.columns = [\"count\"]\ncol_df.sort_values(by=\"count\",inplace=True,ascending=False)\ncol_list = col_df.index[:10].tolist()\ncol_df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a684345-20ed-bb01-190d-75a7b40dd44b"},"outputs":[],"source":"col_list"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c9618e4e-97f8-8741-bbfe-e60c75aa1893"},"outputs":[],"source":"for col in col_list:\n    uniq = len(train_df[col].unique())\n    print(uniq)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0ef0211f-55b0-1d9c-73de-0d57b74d921f"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}