{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Import Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nfrom numpy import NaN\npd.set_option('display.max_rows', 70)\npd.set_option('display.max_columns', 70)\npd.set_option('display.width', 100)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Load Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"prop16 = pd.read_csv(\"../input/zillow-prize-1/properties_2016.csv\")\nprop17 = pd.read_csv(\"../input/zillow-prize-1/properties_2017.csv\")\nsmplsub = pd.read_csv(\"../input/zillow-prize-1/sample_submission.csv\")\ntrain16 = pd.read_csv(\"../input/zillow-prize-1/train_2016_v2.csv\")\ntrain17 = pd.read_csv(\"../input/zillow-prize-1/train_2017.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature & Observation Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prop16.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prop17.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train16.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train17.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smplsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to get all info in one go\ndef full_info(df):\n    df_column=[]\n    df_dtype=[]\n    df_null=[]\n    df_nullc=[]\n    df_mean=[]\n    df_median=[]\n    df_std=[]\n    df_min=[]\n    df_max=[]\n    df_uniq=[]\n    for col in df.columns: \n        df_column.append( col)\n        df_dtype.append( df[col].dtype)\n        df_null.append( round(100 * df[col].isnull().sum(axis=0)/len(df[col]),2))\n        df_nullc.append( df[col].isnull().sum(axis=0))\n        df_uniq.append( df[col].nunique()) if df[col].dtype == 'object' else df_uniq.append( NaN)\n        df_mean.append(  '{0:.2f}'.format(df[col].mean())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_mean.append( NaN)\n        df_median.append( '{0:.2f}'.format(df[col].median())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_median.append( NaN)\n        df_std.append( '{0:.2f}'.format(df[col].std())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_std.append( NaN)\n        df_max.append( '{0:.2f}'.format(df[col].max())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_max.append( NaN)\n        df_min.append( '{0:.2f}'.format(df[col].min())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_min.append( NaN)\n    return pd.DataFrame(data = {'ColName': df_column, 'ColType': df_dtype, 'NullCnt': df_nullc, 'NullCntPrcntg': df_null,  'Min': df_min, 'Max': df_max, 'Mean': df_mean, 'Med': df_median, 'Std': df_std, 'UniqCnt': df_uniq})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prop16_Info = full_info(prop16)\nprop16_Info.sort_values(by=['NullCnt'], ascending=False, inplace=True, ignore_index=True)\nprop16_Info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_info(train16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('size of properties_2016.csv: ', prop16.shape)\nprint('size of train_2016_v2.csv: ', train16.shape)\nprint('size of properties_2017.csv: ', prop17.shape)\nprint('size of train_2017.csv: ', train17.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_props = len(train16['parcelid'].unique())\nmultiple_sales = len(train16) - unique_props\nprint('number of unique sales: ', unique_props)\nprint('Number of duplicate: ', multiple_sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets visualize the Null Count percentage graphically\nprop16_Info.plot.bar(x = 'ColName', y = 'NullCnt', figsize=(25, 6),rot=90, title='Missing (null) Feature Values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# interactive feature transfromation.\nprop16['prop_age'] = 2018 - prop16['yearbuilt']  # property age\nprop16['has_basement'] = prop16['basementsqft'].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)\nprop16['has_pool'] = prop16[['poolcnt','poolsizesum','pooltypeid10','pooltypeid2','pooltypeid7']].apply(lambda x: 1 if(np.all(pd.notnull(x[1]))) else 0, axis = 1)\nprop16['has_patio_yard'] = prop16['yardbuildingsqft17'].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)\nprop16['has_starage_yard'] = prop16['yardbuildingsqft26'].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)\nprop16['has_garage'] = prop16['garagecarcnt'].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some nan features actually make sense, lets fill them with 0\nprop16.yardbuildingsqft17.fillna(0, inplace=True)\nprop16.yardbuildingsqft26.fillna(0, inplace=True)\nprop16.basementsqft.fillna(0, inplace=True)\nprop16.poolcnt.fillna(0, inplace=True)\nprop16.poolsizesum.fillna(0, inplace=True)\nprop16.pooltypeid10.fillna(0, inplace=True)\nprop16.pooltypeid2.fillna(0, inplace=True)\nprop16.pooltypeid7.fillna(0, inplace=True)\nprop16.garagecarcnt.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns with data that has > 90% null\nprop16_trim = prop16.drop(prop16_Info[(prop16_Info.NullCntPrcntg>=90)].ColName.values.tolist(),axis=1)\nprop16_trim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prop16_trim.select_dtypes(include=['object']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# these object dtype not categorical. can be ignored.\nprop16_trim[['propertycountylandusecode', 'propertyzoningdesc']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the object dtype columns\nprop16_trim=prop16_trim.drop(['propertycountylandusecode', 'propertyzoningdesc'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets fill rest of the NaNs with medians\nprop16_median_imputed = prop16_trim.fillna(prop16_trim.median())\nprop16_median_imputed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets concatenated both property and train data\ntrain16_merge = pd.merge(prop16_median_imputed, train16, on='parcelid', how='inner')\ntrain16_merge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" train16_merge=train16_merge.drop(['parcelid'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check the correlation of the feature to target\nfrom yellowbrick.target.feature_correlation import feature_correlation\nX, y = train16_merge.drop(columns =[ 'logerror', 'transactiondate', 'fireplacecnt']), train16_merge['logerror']\n\nfeatures = np.array(train16_merge.drop(columns = [ 'logerror', 'transactiondate', 'fireplacecnt']).columns)\nfig, ax = plt.subplots(figsize=(10,18))\nvisualizer = feature_correlation(X, y, labels=features, sort= True, color='gray', show=True, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. LightGBM model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import LightGBM Libraries\nimport lightgbm as lgb\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LightGBM accepts numphy array as input\nx_train = train16_merge.drop(columns =[ 'logerror', 'transactiondate', 'fireplacecnt']).values.astype(np.float32) # np array\ny_train = train16_merge['logerror'].values.astype(np.float32)  # np array\nx_test = train16_merge.drop([ 'logerror', 'transactiondate', 'fireplacecnt'], axis=1).values.astype(np.float32)  # np array\ntrain_columns = train16_merge.drop(columns = [ 'logerror', 'transactiondate', 'fireplacecnt']).columns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# manually added the features as the numbers in some features is not acceptable for lgb\nd_train = lgb.Dataset(x_train, y_train, feature_name=['airconditioningtypeid', 'bathroomcnt', 'bedroomcnt',\n        'buildingqualitytypeid', 'calculatedbathnbr',\n       'calculatedfinishedsquarefeet', 'finishedsquarefeet', 'fips',\n       'fullbathcnt', 'garagecarcnt', 'garagetotalsqft',\n       'heatingorsystemtypeid', 'latitude', 'longitude', 'lotsizesquarefeet',\n       'poolcnt', 'pooltypeid', 'propertylandusetypeid',\n       'rawcensustractandblock', 'regionidcity', 'regionidcounty',\n       'regionidneighborhood', 'regionidzip', 'roomcnt', 'threequarterbathnbr',\n       'unitcnt', 'yearbuilt', 'numberofstories', 'structuretaxvaluedollarcnt',\n       'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt',\n       'taxamount', 'censustractandblock', 'prop_age', 'has_basement',\n       'has_pool', 'has_patio_yard', 'has_starage_yard','has_garage'])  # lightgbm data model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgb hyper parameters\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.01  # shrinkage_rate 0.0021 grid search = 0.01\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'mae'  # l1\nparams['sub_feature'] = 0.5  # feature_fraction\nparams['bagging_fraction'] = 0.85  # sub_row\nparams['num_leaves'] = 512  # num_leaf\nparams['min_data'] = 500  # min_data_in_leaf\nparams['min_hessian'] = 0.05  # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nparams['feature_fraction_seed'] = 2\nparams['bagging_seed'] = 3\n#params['n_estimators'] = 10  # grid search\nparams['colsample_bytree'] = 0.85\nparams['num_leaves'] = 22\nparams['subsample'] = 0.7\n\nnp.random.seed(0)\nrandom.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgb train\nclf = lgb.train(params, d_train, 430)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgb predict\np_test = clf.predict(x_test)\npd.DataFrame(p_test).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgb feature importance\nlgb.plot_importance(clf, figsize=(20,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgb tree plot\nimport os\nos.environ[\"PATH\"] += os.pathsep + '/opt/anaconda3/lib/python3.7/site-packages/sphinx/templates/graphviz'\nlgb.plot_tree(clf, figsize=(100,40))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. **XGBoost model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the library\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost instead accepts dataframe as input\nx_train_xgb = train16_merge.drop(columns =[ 'logerror', 'transactiondate', 'fireplacecnt']) \ny_train_xgb = train16_merge['logerror']\nx_test_xgb = train16_merge.drop([ 'logerror', 'transactiondate', 'fireplacecnt'], axis=1)\ntrain_columns_xgb = train16_merge.drop(columns = [ 'logerror', 'transactiondate', 'fireplacecnt']).columns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb hyperparameters\n\ny_mean = np.mean(y_train_xgb)\nxgb_params1 = {\n    'eta' : 0.04,  # 0.037 grid search = .04\n    'max_depth' : 6,  #5\n    'subsample' : 0.80,\n    'objective' : 'reg:linear',\n    'eval_metric' : 'mae',\n    'lambda' : 0.8,\n    'alpha' : 0.4,\n    'base_score' : y_mean,\n    'silent' : 1,\n    'min_child_weight': 5  # grid search\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain1 = xgb.DMatrix(x_train_xgb, y_train_xgb, feature_names=train_columns_xgb)\ndtest1 = xgb.DMatrix(x_test_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training\nmodel1 = xgb.train(dict(xgb_params1, silent=1), dtrain1, num_boost_round=150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb feature importance\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model1, max_num_features=50, height=0.8, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(100, 60))\nxgb.plot_tree(model1, num_trees=4, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. XGBoost with a variation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb2 hyperparameters\nxgb_params2 = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:squarederror',\n    'silent': 1,\n    'seed' : 0\n}\ndtrain2 = xgb.DMatrix(x_train_xgb, y_train_xgb, feature_names=train_columns)\ndtest2 = xgb.DMatrix(x_test_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training\nmodel2 = xgb.train(dict(xgb_params2, silent=0), dtrain2, num_boost_round=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict\nxgb_pred2 = model2.predict(dtest2)\nxgb_pred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model2, max_num_features=50, height=0.8, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(100, 60))\nxgb.plot_tree(model2, num_trees=4, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Please upvote if any of this useful*","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}