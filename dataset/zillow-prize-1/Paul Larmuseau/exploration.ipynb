{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","version":"3.6.1","mimetype":"text/x-python"}},"cells":[{"metadata":{"_cell_guid":"7352f94b-b636-41c7-96d2-715f95842e2a","_uuid":"3908d8c205853aac84133ddde4851d3487c8cc81"},"execution_count":null,"cell_type":"code","outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n"},{"metadata":{"_cell_guid":"c4ea28d4-baca-4571-9157-f919b46bdbde","_uuid":"2b24f75ba4e11175340be89adba3fd782a0b126a"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\n\nproperties = pd.read_csv(\"../input/properties_2016.csv\")\n\nfor c in properties.columns:\n    properties[c]=properties[c].fillna(-1)\n    if properties[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties[c].values))\n        properties[c] = lbl.transform(list(properties[c].values))\n        \nproperties.describe().T"},{"metadata":{"_cell_guid":"3df1e8b2-109f-4ceb-9cbc-cf257ffcd6af","_uuid":"cf470c10f203b3c004af58c27a965f8dcf1bbe73"},"execution_count":null,"cell_type":"code","outputs":[],"source":"train = pd.read_csv(\"../input/train_2016_v2.csv\")\nimport matplotlib.pyplot as plt\nplt.hist(train['logerror'],200)\nplt.title(\"Histogram of logerror\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\ntrain=pd.merge(train, properties, on='parcelid', how='left')"},{"metadata":{"_cell_guid":"19d1ae0d-e295-45da-9b07-513910239c3f","collapsed":true,"_uuid":"938b9bdf4f47315036d52158c8c86bc807476eb9"},"execution_count":null,"cell_type":"code","outputs":[],"source":"def dddraw(X_reduced,name):\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\n    # To getter a better understanding of interaction of the dimensions\n    # plot the first three PCA dimensions\n    fig = plt.figure(1, figsize=(8, 6))\n    ax = Axes3D(fig, elev=-150, azim=110)\n    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,cmap=plt.cm.Paired)\n    titel=\"First three directions of \"+name \n    ax.set_title(titel)\n    ax.set_xlabel(\"1st eigenvector\")\n    ax.w_xaxis.set_ticklabels([])\n    ax.set_ylabel(\"2nd eigenvector\")\n    ax.w_yaxis.set_ticklabels([])\n    ax.set_zlabel(\"3rd eigenvector\")\n    ax.w_zaxis.set_ticklabels([])\n\n    plt.show()"},{"metadata":{"_cell_guid":"4b877912-a16d-4b8c-93ab-aa3ced911d98","_uuid":"f4866cf3b6614e45eb84c55e42f9d94604646d9c"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from collections import Counter\ndef todrop_col(df,tohold):\n    # use todrop_col(dataframe,['listtohold'])\n    # Categorical features\n    df.replace([np.inf, -np.inf], np.nan).fillna(value=-1)\n    \n    cat_cols = []\n    for c in df.columns:\n        if df[c].dtype == 'object':\n            cat_cols.append(c)\n    #print('Categorical columns:', cat_cols)\n    \n    \n    # Constant columns\n    cols = df.columns.values    \n    const_cols = []\n    for c in cols:   \n        if len(df[c].unique()) == 1:\n            const_cols.append(c)\n    #print('Constant cols:', const_cols)\n    \n    \n    # Dublicate features\n    d = {}; done = []\n    cols = df.columns.values\n    for c in cols:\n        d[c]=[]\n    for i in range(len(cols)):\n        if i not in done:\n            for j in range(i+1, len(cols)):\n                if all(df[cols[i]] == df[cols[j]]):\n                    done.append(j)\n                    d[cols[i]].append(cols[j])\n    dub_cols = []\n    for k in d.keys():\n        if len(d[k]) > 0: \n            # print k, d[k]\n            dub_cols += d[k]        \n    #print('Dublicates:', dub_cols)\n    \n    kolom=list(set(dub_cols+const_cols+cat_cols))\n    kolom=[k for k in kolom if k not in tohold]\n    \n    return kolom\n\ntohold=[]\nprint(todrop_col(train,tohold))\n#print(todrop_col(properties,tohold))"},{"metadata":{"_cell_guid":"8cc1066b-18a7-4048-8c7f-8a7505f1a505","scrolled":false,"_uuid":"3893d1feae3b08b8efd0f42368c777c436faf134"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.decomposition import PCA, FastICA,SparsePCA,NMF, LatentDirichletAllocation,FactorAnalysis\nfrom sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection\nfrom sklearn.cluster import KMeans,Birch\nimport statsmodels.formula.api as sm\nfrom scipy import linalg\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return ( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ).round()\n\nn_col=50\nX=train.drop(['logerror','assessmentyear', 'transactiondate'],axis=1)\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n    \nY=train['logerror'].fillna(value=0)\nX=X.fillna(value=0)  #nasty NaN\nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\n#poly = PolynomialFeatures(2)\n#X=poly.fit_transform(X)\n\n\nnames = [\n         'PCA',\n         'FastICA',\n         'Gauss',\n         'KMeans',\n         'SparsePCA',\n         'SparseRP',\n         'Birch',\n         'NMF',    \n         'LatentDietrich',    \n        ]\n\nclassifiers = [\n    \n    PCA(n_components=n_col),\n    FastICA(n_components=n_col),\n    GaussianRandomProjection(n_components=3),\n    KMeans(n_clusters=n_col),\n    SparsePCA(n_components=n_col),\n    SparseRandomProjection(n_components=n_col, dense_output=True),\n    Birch(branching_factor=10, n_clusters=7, threshold=0.5),\n    NMF(n_components=n_col),    \n    #LatentDirichletAllocation(n_topics=n_col),\n    \n]\ncorrection= [1,1,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    Xr=clf.fit_transform(X,Y)\n    dddraw(Xr,name)\n    res = sm.OLS(Y,Xr).fit()\n    print(res.summary())  # show OLS regression\n    #print(res.predict(Xr).round()+correct)  #show OLS prediction\n    #print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction\n    \n    print('Ypredict',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction\n    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y)) #\n    \n    \n    "},{"metadata":{"_cell_guid":"7a3e2b59-1b0d-4223-a9a7-864cf0958eee","_uuid":"31fbc314851641fb82ddf76c7c4978e600e661b4"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n\nX=train.drop(['fireplaceflag', 'taxdelinquencyflag', 'propertycountylandusecode', 'propertyzoningdesc', 'assessmentyear', 'transactiondate', 'hashottuborspa'],axis=1)\nY=np.round(train['logerror'].fillna(value=0)*2)\n\nX=X.replace([np.inf, -np.inf], np.nan).fillna(value=0)\nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\n#poly = PolynomialFeatures(2)\n#X=poly.fit_transform(X)\n\n\nnames = [\n         #677% 'ElasticNet',\n         #timeout 'SVC',\n         #98.61% 'kSVC',\n         'KNN',\n         'DecisionTree',\n         #'RandomForestClassifier',\n         #'GridSearchCV',\n         #400%error 'HuberRegressor',\n         #683% 'Ridge',\n         #511% 'Lasso',\n         #683% 'LassoCV',\n         #681% 'Lars',\n         'BayesianRidge',\n         #415% 'SGDClassifier',\n         #410%'RidgeClassifier',\n         #406% 'LogisticRegression',\n         #684% 'OrthogonalMatchingPursuit',\n         #'RANSACRegressor',\n         ]\n\nclassifiers = [\n    #ElasticNetCV(cv=10, random_state=0),\n    #SVC(),\n    #SVC(kernel = 'rbf', random_state = 0),\n    KNeighborsClassifier(n_neighbors = 1),\n    DecisionTreeClassifier(),\n    #RandomForestClassifier(n_estimators = 200),\n    #GridSearchCV(SVC(),param_grid, refit = True, verbose = 1),\n    #error HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95),\n    #Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True),\n    #Lasso(alpha=0.05),\n    #LassoCV(),\n    #Lars(n_nonzero_coefs=10),\n    BayesianRidge(),\n    #SGDClassifier(),\n    #RidgeClassifier(),\n    #LogisticRegression(),\n    # OrthogonalMatchingPursuit(),\n    #RANSACRegressor(),\n]\ncorrection= [0,0,0,0,0,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    regr=clf.fit(X,Y)\n    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)\n    print(name,'%error',procenterror(regr.predict(X),Y),'rmsle',rmsle(regr.predict(X),Y))\n    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n\n    # Confusion Matrix\n    print(name,'Confusion Matrix')\n    print(confusion_matrix(Y, np.round(regr.predict(X) ) ) )\n    print('--'*40)\n\n    # Classification Report\n    print('Classification Report')\n    print(classification_report(Y,np.round( regr.predict(X) ) ))\n\n    # Accuracy\n    print('--'*40)\n    logreg_accuracy = round(accuracy_score(Y, np.round( regr.predict(X) ) ) * 100,2)\n    print('Accuracy', logreg_accuracy,'%')"}],"nbformat_minor":1,"nbformat":4}