{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"MAKE_SUBMISSION = True          # Generate output file.\nCV_ONLY = False                 # Do validation only; do not generate predicitons.\nFIT_FULL_TRAIN_SET = True       # Fit model to full training set after doing validation.\nFIT_2017_TRAIN_SET = False      # Use 2017 training data for full fit (no leak correction)\nFIT_COMBINED_TRAIN_SET = True   # Fit combined 2016-2017 training set\nUSE_SEASONAL_FEATURES = True\nVAL_SPLIT_DATE = '2016-09-15'   # Cutoff date for validation split\nLEARNING_RATE = 0.007           # shrinkage rate for boosting roudns\nROUNDS_PER_ETA = 20             # maximum number of boosting rounds times learning rate\nOPTIMIZE_FUDGE_FACTOR = False   # Optimize factor by which to multiply predictions.\nFUDGE_FACTOR_SCALEDOWN = 0.3    # exponent to reduce optimized fudge factor for prediction","metadata":{"_uuid":"47787be314e3cc68e7071ae827aef9710fc8341a","_cell_guid":"f16ac0de-de1c-4b65-a85e-a3227fbe0c47","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport pylab\nimport calendar\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nfrom scipy import stats\nimport missingno as msno\nfrom datetime import datetime\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kendalltau\nfrom sklearn import preprocessing\nimport warnings\nmatplotlib.style.use('ggplot')\npd.options.mode.chained_assignment = None\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\n#XGB import comes here\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport patsy\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.regression.quantile_regression import QuantReg\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline","metadata":{"_uuid":"a6d445499b3664bf165ab992f309a417471707f4","_cell_guid":"1c1a4c8e-9379-47b8-aa2e-3df8e5d22918","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"properties2016 = pd.read_csv('../input/properties_2016.csv', low_memory = False)\nproperties2017 = pd.read_csv('../input/properties_2017.csv', low_memory = False)\n\n# Number of properties in the zip\nzip_count = properties2016['regionidzip'].value_counts().to_dict()\n# Number of properties in the city\ncity_count = properties2016['regionidcity'].value_counts().to_dict()\n# Median year of construction by neighborhood\nmed_year = properties2016.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n# Mean square feet by neighborhood\nmean_area = properties2016.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n# Neighborhood latitude and longitude\nmed_lat = properties2016.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\nmed_long = properties2016.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n\n# For this one it only read for the 2016 for now, it will be concat at the later stage\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\")\nfor c in properties2016.columns:\n    properties2016[c]=properties2016[c].fillna(-1)\n    if properties2016[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties2016[c].values))\n        properties2016[c] = lbl.transform(list(properties2016[c].values))","metadata":{"_uuid":"43ea1cf09b04ca53bb99c95d428b4624572ba75a","_cell_guid":"a6cc2bdd-9d38-4f66-8903-3ce019f7109a","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2016 = train.merge(properties2016, how='left', on='parcelid')\nselect_qtr4 = pd.to_datetime(train2016[\"transactiondate\"]) >= VAL_SPLIT_DATE\nif USE_SEASONAL_FEATURES:\n    basedate = pd.to_datetime('2015-11-15').toordinal()\n","metadata":{"_uuid":"dfe82baffa246f7ebe77d472de49ee6a6d43a419","_cell_guid":"b5752c23-b3d7-4403-ab0b-2813d88e14f5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train\ngc.collect()","metadata":{"_uuid":"95c7a84848e30fa37a75d319b9e7e04deaff9ef9","_cell_guid":"101437b6-c54e-4452-9c81-a824818d651a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inputs to features that depend on target variable\n# (Ideally these should be recalculated, and the dependent features recalculated,\n#  when fitting to the full training set.  But I haven't implemented that yet.)\n\n# Standard deviation of target value for properties in the city/zip/neighborhood\ncity_std = train2016[~select_qtr4].groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\nzip_std = train2016[~select_qtr4].groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\nhood_std = train2016[~select_qtr4].groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()","metadata":{"_uuid":"9722eb834d3cf02e943b8c85bf99e7a7515173e4","_cell_guid":"d9460c6d-bf39-4bfb-a584-f3aafcabc8fa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_type(prop):\n    # Convert Object features to Categorical\n    # Convert float64 variables to float32\n    for col in prop.columns:\n        if prop[col].dtype.name =='object':\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(prop[col].values)) \n            prop[col] = lbl.transform(list(prop[col].values))\n        elif prop[col].dtype.name == 'float64':\n            prop[col] = prop[col].astype(np.float32)\n\n    gc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_features(df):\n    # Nikunj's features\n    # Number of properties in the zip\n    df['N-zip_count'] = df['regionidzip'].map(zip_count)\n    # Number of properties in the city\n    df['N-city_count'] = df['regionidcity'].map(city_count)\n    # Does property have a garage, pool or hot tub and AC?\n    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n                         (df['pooltypeid10']>0) & \\\n                         (df['airconditioningtypeid']!=5))*1 \n\n    # More features\n    # Mean square feet of neighborhood properties\n    df['mean_area'] = df['regionidneighborhood'].map(mean_area)\n    # Median year of construction of neighborhood properties\n    df['med_year'] = df['regionidneighborhood'].map(med_year)\n    # Neighborhood latitude and longitude\n    df['med_lat'] = df['regionidneighborhood'].map(med_lat)\n    df['med_long'] = df['regionidneighborhood'].map(med_long)\n\n    df['zip_std'] = df['regionidzip'].map(zip_std)\n    df['city_std'] = df['regionidcity'].map(city_std)\n    df['hood_std'] = df['regionidneighborhood'].map(hood_std)\n    \n    if USE_SEASONAL_FEATURES:\n        df['cos_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n                             (2*np.pi/365.25) ).apply(np.cos)\n        df['sin_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n                             (2*np.pi/365.25) ).apply(np.sin)\n","metadata":{"_uuid":"4b9916ad9eb1d2a0b1aab78e71191c34a78a9b77","_cell_guid":"058d3299-005e-4458-a8cd-cd3c4d854190","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropvars = ['airconditioningtypeid', 'buildingclasstypeid',\n            'buildingqualitytypeid', 'regionidcity']\ndroptrain = ['parcelid', 'logerror', 'transactiondate']\ndroptest = ['ParcelId']","metadata":{"_uuid":"d816890fe0edcab13c415144e76ecd12938ab17e","_cell_guid":"e389a360-b1dc-4257-b3c3-fea69026863d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_features(train2016)\n\nx_valid = train2016.drop(dropvars+droptrain, axis=1)[select_qtr4]\ny_valid = train2016[\"logerror\"].values.astype(np.float32)[select_qtr4]\n\nprint('Shape full training set: {}'.format(train2016.shape))\nprint('Dropped vars: {}'.format(len(dropvars+droptrain)))\nprint('Shape valid X: {}'.format(x_valid.shape))\nprint('Shape valid y: {}'.format(y_valid.shape))\n\ntrain2016=train2016[ train2016.logerror > -0.4 ]\ntrain2016=train2016[ train2016.logerror < 0.419 ]\nprint('\\nFull training set after removing outliers, before dropping vars:')     \nprint('Shape training set: {}\\n'.format(train2016.shape))\n\nif FIT_FULL_TRAIN_SET:\n    full_train = train2016.copy()\n\ntrain2016=train2016[~select_qtr4]\nx_train=train2016.drop(dropvars+droptrain, axis=1)\ny_train = train2016[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\nn_train = x_train.shape[0]\nprint('Training subset after removing outliers:')     \nprint('Shape train X: {}'.format(x_train.shape))\nprint('Shape train y: {}'.format(y_train.shape))\n\nif FIT_FULL_TRAIN_SET:\n    x_full = full_train.drop(dropvars+droptrain, axis=1)\n    y_full = full_train[\"logerror\"].values.astype(np.float32)\n    n_full = x_full.shape[0]\n    print('\\nFull trainng set:')     \n    print('Shape train X: {}'.format(x_train.shape))\n    print('Shape train y: {}'.format(y_train.shape))","metadata":{"_uuid":"233443f9b4c2af79e91d235fdc4f660ff1d630bc","_cell_guid":"720a950e-5392-4571-8fd6-e849a9c6967e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not CV_ONLY:\n    # Generate test set data\n    \n    sample_submission = pd.read_csv('../input/sample_submission.csv', low_memory = False)\n    \n    # Process properties for 2016\n    test_df = pd.merge( sample_submission[['ParcelId']], \n                        properties2016.rename(columns = {'parcelid': 'ParcelId'}), \n                        how = 'left', on = 'ParcelId' )\n    if USE_SEASONAL_FEATURES:\n        test_df['transactiondate'] = '2016-10-31'\n        droptest += ['transactiondate']\n    calculate_features(test_df)\n    x_test = test_df.drop(dropvars+droptest, axis=1)\n    print('Shape test: {}'.format(x_test.shape))\n\n    # Process properties for 2017\n    for c in properties2017.columns:\n        properties2017[c]=properties2017[c].fillna(-1)\n        if properties2017[c].dtype == 'object':\n            lbl = LabelEncoder()\n            lbl.fit(list(properties2017[c].values))\n            properties2017[c] = lbl.transform(list(properties2017[c].values))\n    zip_count = properties2017['regionidzip'].value_counts().to_dict()\n    city_count = properties2017['regionidcity'].value_counts().to_dict()\n    med_year = properties2017.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n    mean_area = properties2017.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n    med_lat = properties2017.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n    med_long = properties2017.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n\n    test_df = pd.merge( sample_submission[['ParcelId']], \n                        properties2017.rename(columns = {'parcelid': 'ParcelId'}), \n                        how = 'left', on = 'ParcelId' )\n    if USE_SEASONAL_FEATURES:\n        test_df['transactiondate'] = '2017-10-31'\n    calculate_features(test_df)\n    x_test17 = test_df.drop(dropvars+droptest, axis=1)\n\n    del test_df","metadata":{"_uuid":"f3a9ea8db9161705b9e0d61a82736b44c7dfcb01","_cell_guid":"d0c5819c-0873-4b49-a8a7-5295632ccbb9","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train2016\ndel select_qtr4\ngc.collect()","metadata":{"_uuid":"c6d653da30197972f12f1a7a0ea4d0a36f28c554","_cell_guid":"03fb6b2f-5166-4646-8445-304765a404ad","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params = {  # best as of 2017-09-28 13:20 UTC\n    'eta': LEARNING_RATE,\n    'max_depth': 7, \n    'subsample': 0.6,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 5.0,\n    'alpha': 0.65,\n    'colsample_bytree': 0.5,\n    'base_score': y_mean,'taxdelinquencyyear'\n    'silent': 1\n}\n\nclean_type(x_train)\nclean_type(x_valid)\nclean_type(x_test)\nclean_type(x_test17)\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndvalid_x = xgb.DMatrix(x_valid)\ndvalid_xy = xgb.DMatrix(x_valid, y_valid)\nif not CV_ONLY:\n    dtest = xgb.DMatrix(x_test)\n    dtest17 = xgb.DMatrix(x_test17)\n    del x_test","metadata":{"_uuid":"c8c1043d72790dc926ae76f398487e8313caeed1","_cell_guid":"de59798a-1b8a-4d0e-a92c-846577f4406b","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x_train\ngc.collect()","metadata":{"_uuid":"dd0776733ec3d3ac960b9ab9ac6e518a99e012d9","_cell_guid":"45e61d05-da7f-4841-811a-3a95c86a3306","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_boost_rounds = round( ROUNDS_PER_ETA / xgb_params['eta'] )\nearly_stopping_rounds = round( num_boost_rounds / 20 )\nprint('Boosting rounds: {}'.format(num_boost_rounds))\nprint('Early stoping rounds: {}'.format(early_stopping_rounds))","metadata":{"_uuid":"fe6761fd44aaf7a5c063897c9ca3b90b957b7108","_cell_guid":"de8e97e6-d681-465b-a68a-6178e7593e49","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evals = [(dtrain,'train'),(dvalid_xy,'eval')]\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n                  evals=evals, early_stopping_rounds=early_stopping_rounds, \n                  verbose_eval=10)","metadata":{"_uuid":"09c99f75bcfb64225f390eee5e5dbf4f47f4103b","_cell_guid":"95d8ac93-f2ff-4ce3-9cb6-8044d73f8600","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\nprint( \"XGBoost validation set predictions:\" )\nprint( pd.DataFrame(valid_pred).head() )\nprint(\"\\nMean absolute validation error:\")\nmean_absolute_error(y_valid, valid_pred)","metadata":{"_uuid":"02a37067bd8ba72b050d6634e9994b234bdf6389","_cell_guid":"c241b1e2-d2c6-4936-a6ee-282a4e90ca27","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if OPTIMIZE_FUDGE_FACTOR:\n    mod = QuantReg(y_valid, valid_pred)\n    res = mod.fit(q=.5)\n    print(\"\\nLAD Fit for Fudge Factor:\")\n    print(res.summary())\n\n    fudge = res.params[0]\n    print(\"Optimized fudge factor:\", fudge)\n    print(\"\\nMean absolute validation error with optimized fudge factor: \")\n    print(mean_absolute_error(y_valid, fudge*valid_pred))\n\n    fudge **= FUDGE_FACTOR_SCALEDOWN\n    print(\"Scaled down fudge factor:\", fudge)\n    print(\"\\nMean absolute validation error with scaled down fudge factor: \")\n    print(mean_absolute_error(y_valid, fudge*valid_pred))\nelse:\n    fudge=1.0","metadata":{"_uuid":"dbd7ae96da81ab900d6eee1c186b2b56f17e4fc8","_cell_guid":"7dd798fb-9160-4a48-9b52-9bc16f582e0f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if FIT_FULL_TRAIN_SET and not CV_ONLY:\n    if FIT_COMBINED_TRAIN_SET:\n        # Merge 2016 and 2017 data sets\n        train16 = pd.read_csv('../input/train_2016_v2.csv')\n        train17 = pd.read_csv('../input/train_2017.csv')\n        train16 = pd.merge(train16, properties2016, how = 'left', on = 'parcelid')\n        train17 = pd.merge(train17, properties2017, how = 'left', on = 'parcelid')\n        train17[['structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount']] = np.nan\n        train_df = pd.concat([train16, train17], axis = 0)\n        # Generate features\n        city_std = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n        zip_std = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n        hood_std = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n        calculate_features(train_df)\n        # Remove outliers\n        train_df=train_df[ train_df.logerror > -0.4 ]\n        train_df=train_df[ train_df.logerror < 0.419 ]\n        # Create final training data sets\n        x_full = train_df.drop(dropvars+droptrain, axis=1)\n        y_full = train_df[\"logerror\"].values.astype(np.float32)\n        n_full = x_full.shape[0]     \n    elif FIT_2017_TRAIN_SET:\n        train = pd.read_csv('../input/train_2017.csv')\n        train_df = train.merge(properties2017, how='left', on='parcelid')\n        # Generate features\n        city_std = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n        zip_std = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n        hood_std = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n        calculate_features(train_df)\n        # Remove outliers\n        train_df=train_df[ train_df.logerror > -0.4 ]\n        train_df=train_df[ train_df.logerror < 0.419 ]\n        # Create final training data sets\n        x_full = train_df.drop(dropvars+droptrain, axis=1)\n        y_full = train_df[\"logerror\"].values.astype(np.float32)\n        n_full = x_full.shape[0]     \n    dtrain = xgb.DMatrix(x_full, y_full)\n    num_boost_rounds = int(model.best_ntree_limit*n_full/n_train)\n    full_model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds, \n                           evals=[(dtrain,'train')], verbose_eval=10)","metadata":{"_uuid":"4f9649857441bbc57beaeb9f5e7a91234c03f4c7","_cell_guid":"6d782218-3b72-431c-8f5e-ef3a2723bbfb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del properties2016\ndel properties2017\ngc.collect()","metadata":{"_uuid":"a39b9309cbaa1588b0646a079c4bfea929990cc1","_cell_guid":"74ad3d77-75eb-447c-9524-21c225507879","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not CV_ONLY:\n    if FIT_FULL_TRAIN_SET:\n        pred = fudge*full_model.predict(dtest)\n        pred17 = fudge*full_model.predict(dtest17)\n    else:\n        pred = fudge*model.predict(dtest, ntree_limit=model.best_ntree_limit)\n        pred17 = fudge*model.predict(dtest17, ntree_limit=model.best_ntree_limit)\n        \n    print( \"XGBoost test set predictions for 2016:\" )\n    print( pd.DataFrame(pred).head() )\n    print( \"XGBoost test set predictions for 2017:\" )\n    print( pd.DataFrame(pred17).head() )    ","metadata":{"_uuid":"a710f272857376365c6a02929197a06be72ae28b","_cell_guid":"c8f7bb43-015e-4c2c-a1bd-afa92219bd87","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if MAKE_SUBMISSION and not CV_ONLY:\n   y_pred=[]\n   y_pred17=[]\n\n   for i,predict in enumerate(pred):\n       y_pred.append(str(round(predict,4)))\n   for i,predict in enumerate(pred17):\n       y_pred17.append(str(round(predict,4)))\n   y_pred=np.array(y_pred)\n   y_pred17=np.array(y_pred17)\n\n   output = pd.DataFrame({'ParcelId': sample_submission['ParcelId'].astype(np.int32),\n           '201610': y_pred, '201611': y_pred, '201612': y_pred,\n           '201710': y_pred17, '201711': y_pred17, '201712': y_pred17})\n   # set col 'ParceID' to first col\n   cols = output.columns.tolist()\n   cols = cols[-1:] + cols[:-1]\n   output = output[cols]\n\n   output.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)","metadata":{"_uuid":"b85616fbde70ef9d6fe220ef12947fb5d2dd3df6","_cell_guid":"8d793a38-6df2-45e7-a725-cc2161ed3cee","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean absolute validation error without fudge factor: \", )\nprint( mean_absolute_error(y_valid, valid_pred) )\nif OPTIMIZE_FUDGE_FACTOR:\n    print(\"Mean absolute validation error with fudge factor:\")\n    print( mean_absolute_error(y_valid, fudge*valid_pred) )","metadata":{"_uuid":"c5ab53465012b552d5b08be82864f972d02946e4","_cell_guid":"50d1d19d-26e8-4227-aa77-7d452d6c7d36","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"b26353143ba91c98ba970f5051bc9656b39c9bb4","_cell_guid":"b66c3b07-5941-44ab-b2f2-58fc46071055","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"9944aa5d56350e51a0b151af9cf38c7b464dcf0b","_cell_guid":"e5b97a3f-4428-487c-abd4-9507d7850260","trusted":true},"execution_count":null,"outputs":[]}]}