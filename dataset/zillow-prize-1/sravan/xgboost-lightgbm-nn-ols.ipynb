{"cells":[{"metadata":{"collapsed":true,"_cell_guid":"95e56fb1-1226-48cb-ae4b-c4cb44c3f37b","_uuid":"b22a1bfde673d92d11e29297cbfea55c8b7ea996"},"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport gc\nfrom sklearn.linear_model import LinearRegression\nimport random\nimport datetime as dt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.noise import GaussianDropout\nfrom keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"3a9237e3-750c-4902-a415-0a8b9ed2de86","_uuid":"b38f6d5011fdaba9aee98da9affb96dd9e330911"},"cell_type":"markdown","source":"##### Parameters\nFUDGE_FACTOR = 1.1200  # Multiply forecasts by this\n\nXGB_WEIGHT = 0.6200\nBASELINE_WEIGHT = 0.0100\nOLS_WEIGHT = 0.0620\nNN_WEIGHT = 0.0800\n\nXGB1_WEIGHT = 0.8540  # Weight of first in combination of two XGB models\n\nBASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg"},{"metadata":{"_cell_guid":"65e2d239-48c1-40e1-96fa-b025010a6f9b","_uuid":"75252f52d39e4d388a4d2cea5db3b70aa7d0c07f"},"source":"prox=pd.read_csv('../input/properties_2016.csv')\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"f0306fca-4f37-4b4d-b129-abcc4801df8c","_uuid":"623f72155e33c10a78f46f43396bb6b093795bdc"},"source":"for c in prox.columns:\n #   print (c)\n    if(c=='airconditioningtypeid' or c=='taxdelinquencyflag' or c=='poolcnt' or c=='regionidcity' or c=='fireplacecnt' or c=='propertyzoningdesc' or c=='propertycountylandusecode'):\n        prox[c]=prox[c].fillna(-1)\n    else:\n        prox[c]=prox[c].fillna(prox[c].median())\n#print (cols)\nprox.head()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"b58eecba-adee-483b-a378-7ee98cbea9ad","_uuid":"0539afda008fa96b573dca18ce57bb630e550a76"},"source":"lbl = LabelEncoder()\nfor c in prox.columns:\n    prox[c]=prox[c].fillna(-1)\n    if prox[c].dtype == 'object':\n        lbl.fit(list(prox[c].values))\n        prox[c] = lbl.transform(list(prox[c].values))\nprox.head()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"39670566-0739-466b-8778-41b70016cd90","_uuid":"41e6d4e272b213359476f3c619a956501a8cfb69"},"source":"#life of property\nprox['N-life'] = 2018 - prox['yearbuilt']\n#error in calculation of the finished living area of home\nprox['N-LivingAreaError'] = prox['calculatedfinishedsquarefeet']/prox['finishedsquarefeet12']\n\n#proportion of living area\nprox['N-LivingAreaProp'] = prox['calculatedfinishedsquarefeet']/prox['lotsizesquarefeet']\nprox['N-LivingAreaProp2'] = prox['finishedsquarefeet12']/prox['finishedsquarefeet15']\n\n#Amout of extra space\nprox['N-ExtraSpace'] = prox['lotsizesquarefeet'] - prox['calculatedfinishedsquarefeet'] \nprox['N-ExtraSpace-2'] = prox['finishedsquarefeet15'] - prox['finishedsquarefeet12'] \n\n#Total number of rooms\nprox['N-TotalRooms'] = prox['bathroomcnt']*prox['bedroomcnt']\n\n#Average room size\nprox['N-AvRoomSize'] = prox['calculatedfinishedsquarefeet']/prox['roomcnt'] \n\n# Number of Extra rooms\nprox['N-ExtraRooms'] = prox['roomcnt'] - prox['N-TotalRooms'] \n\n#Ratio of the built structure value to land area\nprox['N-ValueProp'] = prox['structuretaxvaluedollarcnt']/prox['landtaxvaluedollarcnt']\n\n#Does property have a garage, pool or hot tub and AC?\nprox['N-GarPoolAC'] = ((prox['garagecarcnt']>0) & (prox['pooltypeid10']>0) & (prox['airconditioningtypeid']!=5))*1 \n\nprox[\"N-location\"] = prox[\"latitude\"] + prox[\"longitude\"]\nprox[\"N-location-2\"] = prox[\"latitude\"]*prox[\"longitude\"]\nprox[\"N-location-2round\"] = prox[\"N-location-2\"].round(-4)\n\nprox[\"N-latitude-round\"] = prox[\"latitude\"].round(-4)\nprox[\"N-longitude-round\"] = prox[\"longitude\"].round(-4)\n\nprox.head()\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"31a20d30-c448-49cb-a355-d8f01e0bc199","_uuid":"1c704aa3b2ced6d167a93cdbb2d64f00021912d6"},"source":"#Ratio of tax of property over parcel\nprox['N-ValueRatio'] = prox['taxvaluedollarcnt']/prox['taxamount']\n\n#TotalTaxScore\nprox['N-TaxScore'] = prox['taxvaluedollarcnt']*prox['taxamount']\n\n#polnomials of tax delinquency year\nprox[\"N-taxdelinquencyyear-2\"] = prox[\"taxdelinquencyyear\"] ** 2\nprox[\"N-taxdelinquencyyear-3\"] = prox[\"taxdelinquencyyear\"] ** 3\n\n#Length of time since unpaid taxes\nprox['N-life'] = 2018 - prox['taxdelinquencyyear']\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"283ff3fc-7b95-43b6-8fdd-8dce2d750689","_uuid":"5af950c1606ccfc0e5b464fa9f76c768c79a3712"},"source":"#Number of prox in the zip\nzip_count = prox['regionidzip'].value_counts().to_dict()\nprox['N-zip_count'] = prox['regionidzip'].map(zip_count)\n\n#Number of prox in the city\ncity_count = prox['regionidcity'].value_counts().to_dict()\nprox['N-city_count'] = prox['regionidcity'].map(city_count)\n\n#Number of prox in the city\nregion_count = prox['regionidcounty'].value_counts().to_dict()\nprox['N-county_count'] = prox['regionidcounty'].map(city_count)\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e096f4c7-7deb-4834-8e2a-cb5021691b27","_uuid":"b535d9b9bc30a27e3c7cdd4646d6f088a73c49e0"},"source":"#Indicator whether it has AC or not\nprox['N-ACInd'] = (prox['airconditioningtypeid']!=5)*1\n\n#Indicator whether it has Heating or not \nprox['N-HeatInd'] = (prox['heatingorsystemtypeid']!=13)*1\n\n#There's 25 different property uses - let's compress them down to 4 categories\nprox['N-PropType'] = prox.propertylandusetypeid.replace({31 : \"Mixed\", 46 : \"Other\", 47 : \"Mixed\", 246 : \"Mixed\", 247 : \"Mixed\", 248 : \"Mixed\", 260 : \"Home\", 261 : \"Home\", 262 : \"Home\", 263 : \"Home\", 264 : \"Home\", 265 : \"Home\", 266 : \"Home\", 267 : \"Home\", 268 : \"Home\", 269 : \"Not Built\", 270 : \"Home\", 271 : \"Home\", 273 : \"Home\", 274 : \"Other\", 275 : \"Home\", 276 : \"Home\", 279 : \"Home\", 290 : \"Not Built\", 291 : \"Not Built\" })","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6a7d6339-4b06-4944-81af-ef6e9b2aa6e0","_uuid":"89d435d02e46901dceceaa6d9963a11d285ab5ae"},"source":"#polnomials of the variable\nprox[\"N-structuretaxvaluedollarcnt-2\"] = prox[\"structuretaxvaluedollarcnt\"] ** 2\nprox[\"N-structuretaxvaluedollarcnt-3\"] = prox[\"structuretaxvaluedollarcnt\"] ** 3\n\n#Average structuretaxvaluedollarcnt by city\ngroup = prox.groupby('regionidcity')['structuretaxvaluedollarcnt'].aggregate('mean').to_dict()\nprox['N-Avg-structuretaxvaluedollarcnt'] = prox['regionidcity'].map(group)\n\n#Deviation away from average\nprox['N-Dev-structuretaxvaluedollarcnt'] = abs((prox['structuretaxvaluedollarcnt'] - prox['N-Avg-structuretaxvaluedollarcnt']))/prox['N-Avg-structuretaxvaluedollarcnt']\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e4d14a88-3aee-47a1-b959-ac6cc8cac16c","_uuid":"0cc51d9afedb1664b35395bd60bf2dd6edfb4004"},"source":"print( \"\\nReading data from disk ...\")\nprop = prox\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\")","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"674356e1-e133-430c-afbd-e03a4bc14f58","_uuid":"aade565c835aa7835a6186c81a8fda51a74a5b72"},"source":"#LightGBM\nprint( \"\\nProcessing data for LightGBM ...\" )\nfor c, dtype in zip(prop.columns, prop.dtypes):\t\n    if dtype == np.float64:\n        prop[c] = prop[c].astype(np.float32)\n\ndf_train = train.merge(prop, how='left', on='parcelid')\ndf_train.fillna(df_train.median(),inplace = True)\n\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n#x_train['Ratio_1'] = x_train['taxvaluedollarcnt']/x_train['taxamount']\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)\n\n\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\ndel df_train; gc.collect()\n\nx_train = x_train.values.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label=y_train)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"17df07ae-b3a8-4df2-af6a-99def686d2bc","_uuid":"a19c8980415a738503d84e9d471911d8d4ae66cb"},"source":"\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021 # shrinkage_rate\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          # or 'mae'\nparams['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\nparams['bagging_fraction'] = 0.85 # sub_row\nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        # num_leaf\nparams['min_data'] = 500         # min_data_in_leaf\nparams['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nparams['feature_fraction_seed'] = 2\nparams['bagging_seed'] = 3\n\nnp.random.seed(0)\nrandom.seed(0)\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"4aca313f-27a7-4387-8493-1baa3bb3d5a8","_uuid":"2abe4335c4fdaba59627565f38242e4dba4c2a1b"},"source":"print(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 430)\n\ndel d_train; gc.collect()\ndel x_train; gc.collect()\n\nprint(\"\\nPrepare for LightGBM prediction ...\")\nprint(\"   Read sample file ...\")\nsample = pd.read_csv('../input/sample_submission.csv')\nprint(\"   ...\")\nsample['parcelid'] = sample['ParcelId']\nprint(\"   Merge with property data ...\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\nprint(\"   ...\")\ndel sample, prop; gc.collect()\nprint(\"   ...\")\n#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\nx_test = df_test[train_columns]\nprint(\"   ...\")\ndel df_test; gc.collect()\nprint(\"   Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nprint(\"   ...\")\nx_test = x_test.values.astype(np.float32, copy=False)\n\nprint(\"\\nStart LightGBM prediction ...\")\np_test = clf.predict(x_test)\n\ndel x_test; gc.collect()\n\nprint( \"\\nUnadjusted LightGBM predictions:\" )\nprint( pd.DataFrame(p_test).head() )\n\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"87f859ba-3ee0-451a-bee7-921de04a23bf","_uuid":"9e7940335d5c11bb3198a1d75f9a332a1825bdab"},"source":"#XGBOOST\n\nprint( \"\\nRe-reading properties file ...\")\nproperties = prox\n\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"fef69f74-b4dc-4908-aba5-fbcc6ea99c3e","_uuid":"471279bd1df7c281e66cd32aa21c5ae4c6a6cd7b"},"source":"print( \"\\nProcessing data for XGBoost ...\")\nfor c in properties.columns:\n    properties[c]=properties[c].fillna(-1)\n    if properties[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties[c].values))\n        properties[c] = lbl.transform(list(properties[c].values))\n\ntrain_df = train.merge(properties, how='left', on='parcelid')\nx_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\nx_test = properties.drop(['parcelid'], axis=1)\n# shape        \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"27ad2556-3dc9-4bad-be70-3f5e707f3a33","_uuid":"a583ba74ec83c0daa566408febfeea2f84ceec7b"},"source":"#outliers\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.419 ]\nx_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\ny_train = train_df[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\n\nprint('After removing outliers:')     \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"fe31ddd8-6c9d-485b-ad89-3daee661a9fd","_uuid":"35f4bca42413296074ce6f7fcdf98f631d3e7937"},"source":"print(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.037,\n    'max_depth': 5,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 0.8,   \n    'alpha': 0.4, \n    'base_score': y_mean,\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\nnum_boost_rounds = 250\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\n# train model\nprint( \"\\nTraining XGBoost ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost ...\")\nxgb_pred1 = model.predict(dtest)\n\nprint( \"\\nFirst XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred1).head() )\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ddf80d0b-0524-4efa-9f92-2907633df64d","_uuid":"fe6b4be138978a7db450be56dd0a4187955fe52d"},"source":"print(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.033,\n    'max_depth': 6,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'base_score': y_mean,\n    'silent': 1\n}\n\nnum_boost_rounds = 150\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\nprint( \"\\nTraining XGBoost again ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost again ...\")\nxgb_pred2 = model.predict(dtest)\n\nprint( \"\\nSecond XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred2).head() )\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f5e7293c-cf73-4ec8-9a7f-3ab3553679a8","_uuid":"d3f373938ce6c8d93ab53c4d53d357a94af04007"},"source":"xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n#xgb_pred = xgb_pred1\n\nprint( \"\\nCombined XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred).head() )","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9b6e8f5f-54df-4383-a1ab-16b27017a7e3","_uuid":"a76cfc80fd53516782cb27f2549ad8421f8268f7"},"source":"del train_df\ndel x_train\ndel x_test\ndel properties\ndel dtest\ndel dtrain\ndel xgb_pred1\ndel xgb_pred2 \ngc.collect()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"affd4eaf-b9c9-4921-b0cb-42eed006bd47","_uuid":"75a6c78c40f1e3a1f78e61186cf7b2d1b1d0d50a"},"source":"#neural nets\n\nprint( \"\\n\\nProcessing data for Neural Network ...\")\nprint('\\nLoading train, prop and sample data...')\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\nprop = prox\nsample = pd.read_csv('../input/sample_submission.csv')\n \nprint('Fitting Label Encoder on properties...')\nfor c in prop.columns:\n    prop[c]=prop[c].fillna(-1)\n    if prop[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(prop[c].values))\n        prop[c] = lbl.transform(list(prop[c].values))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ee8c74f5-8f12-465f-91e4-b7aa15ce34a0","_uuid":"c408f57bc333f53585db2b0a18ce8fbe947ed08b"},"source":"print('Creating training set...')\ndf_train = train.merge(prop, how='left', on='parcelid')\n\ndf_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\ndf_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\ndf_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\ndf_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\ndf_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day\n\nprint('Filling NA/NaN values...' )\ndf_train.fillna(-1.0)\n\nprint('Creating x_train and y_train from df_train...' )\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)\ny_train = df_train[\"logerror\"]\n\ny_mean = np.mean(y_train)\nprint(x_train.shape, y_train.shape)\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d2e30e0e-8795-4cd6-a380-780d698e55ba","_uuid":"32b9a336ffe5d82b1802fcd4709248d61d26b7db"},"source":"print('Creating df_test...')\nsample['parcelid'] = sample['ParcelId']\n\nprint(\"Merging Sample with property data...\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\n\ndf_test[\"transactiondate\"] = pd.to_datetime('2016-11-15')  # placeholder value for preliminary version\ndf_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\ndf_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\ndf_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\ndf_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day     \nx_test = df_test[train_columns]\n\nprint('Shape of x_test:', x_test.shape)\nprint(\"Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\n  ","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6ce4a6b1-9b0c-47f7-9843-e3cca3b11447","_uuid":"699ddefbcd8a6eb055d43c3449d17c0501d26a95"},"source":"print(\"\\nPreprocessing neural network data...\")\nimputer= Imputer()\nimputer.fit(x_train.iloc[:, :])\nx_train = imputer.transform(x_train.iloc[:, :])\nimputer.fit(x_test.iloc[:, :])\nx_test = imputer.transform(x_test.iloc[:, :])\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\nlen_x=int(x_train.shape[1])\nprint(\"len_x is:\",len_x)\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e6fac4cd-0e5a-4758-a758-a7a66a401450","_uuid":"7504b9d59449fb9d00996da4f9880046fe580f7b"},"source":"# Neural Network\nprint(\"\\nSetting up neural network model...\")\nnn = Sequential()\nnn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\nnn.add(PReLU())\nnn.add(Dropout(.4))\nnn.add(Dense(units = 160 , kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.6))\nnn.add(Dense(units = 64 , kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.5))\nnn.add(Dense(units = 26, kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.6))\nnn.add(Dense(1, kernel_initializer='normal'))\nnn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"70d83c1f-624e-4ce4-a9c7-2af4dbfbfc98","_uuid":"301e771bfecb0d3389e657a258568b5e49e7ea65"},"source":"print(\"\\nFitting neural network model...\")\nnn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 52, verbose=2)\n\nprint(\"\\nPredicting with neural network model...\")\n#print(\"x_test.shape:\",x_test.shape)\ny_pred_ann = nn.predict(x_test)\n\nprint( \"\\nPreparing results for write...\" )\nnn_pred = y_pred_ann.flatten()\nprint( \"Type of nn_pred is \", type(nn_pred) )\nprint( \"Shape of nn_pred is \", nn_pred.shape )\n\nprint( \"\\nNeural Network predictions:\" )\nprint( pd.DataFrame(nn_pred).head() )","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"fe63c72a-e73a-4aba-9a03-10e105ba7eaa","_uuid":"3d62aa6ce8e06011a04b51f006f018875633ff3d"},"source":"del train\ndel prop\ndel sample\ndel x_train\ndel x_test\ndel df_train\ndel df_test\ndel y_pred_ann\ngc.collect()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"dabd6555-fed9-4293-92fd-ad9f5c023f48","_uuid":"d05a30dc5a5c5ecd3302ce0d69027fd8e44c8e53"},"source":"#OLS\n\nnp.random.seed(17)\nrandom.seed(17)\n\nprint( \"\\n\\nProcessing data for OLS ...\")\n\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\nproperties = pd.read_csv('../input/properties_2016.csv')\nsubmission = pd.read_csv(\"../input/sample_submission.csv\")\nprint(len(train),len(properties),len(submission))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"10b02cf8-c626-44ea-b92d-96ba56352d88","_uuid":"207ae184f0b4e79771841d357cb1b89b5137e909"},"source":"def get_features(df):\n    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n    df['transactiondate'] = df['transactiondate'].dt.quarter\n    df = df.fillna(-1.0)\n    return df\n\ndef MAE(y, ypred):\n    #logerror=log(Zestimate)−log(SalePrice)\n    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2f7de0da-bbb8-4e77-bf83-0ade9c004a8b","_uuid":"65fc9894bd97f563e46f0145e3eb0a9a7c624ef5"},"source":"train = pd.merge(train, properties, how='left', on='parcelid')\ny = train['logerror'].values\ntest = pd.merge(submission, properties, how='left', left_on='ParcelId', right_on='parcelid')\nproperties = [] #memory\n\nexc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\ncol = [c for c in train.columns if c not in exc]\n\ntrain = get_features(train[col])\ntest['transactiondate'] = '2016-01-01' #should use the most common training date\ntest = get_features(test[col])","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b559282b-0ac9-4b26-823c-0589602af5f4","_uuid":"c43eda8ff8a6e68238c477f4e20e8c9478cea6db"},"source":"print(\"\\nFitting OLS...\")\nreg = LinearRegression(n_jobs=-1)\nreg.fit(train, y); print('fit...')\nprint(MAE(y, reg.predict(train)))\ntrain = [];  y = [] #memory\n\ntest_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\ntest_columns = ['201610','201611','201612','201710','201711','201712']\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8507e2de-f1a7-4f6c-af98-0a66cbcb92c8","_uuid":"e25a8cee64c04059ade97c6217c409e428221f94"},"source":"#combining all predictions\n\nprint( \"\\nCombining XGBoost, LightGBM, NN, and baseline predicitons ...\" )\nlgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT \nlgb_weight0 = lgb_weight / (1 - OLS_WEIGHT)\nxgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\nbaseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\nnn_weight0 = NN_WEIGHT / (1 - OLS_WEIGHT)\npred0 = 0\npred0 += xgb_weight0*xgb_pred\npred0 += baseline_weight0*BASELINE_PRED\npred0 += lgb_weight0*p_test\npred0 += nn_weight0*nn_pred\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b565accf-2ab9-4980-990f-dc821b53d644","_uuid":"1eb7dd82579da617fa45ba5aceae8d6f230511b5"},"source":"print( \"\\nCombined XGB/LGB/NN/baseline predictions:\" )\nprint( pd.DataFrame(pred0).head() )\n\nprint( \"\\nPredicting with OLS and combining with XGB/LGB/NN/baseline predicitons: ...\" )\nfor i in range(len(test_dates)):\n    test['transactiondate'] = test_dates[i]\n    pred = FUDGE_FACTOR * ( OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0 )\n    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n    print('predict...', i)\n\nprint( \"\\nCombined XGB/LGB/NN/baseline/OLS predictions:\" )\nprint( submission.head() )","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6e754544-b0f2-407d-a617-25df099e3905","_uuid":"f320611dede2df5c90e02bf7e8cf833da21946ed"},"source":"##### WRITE THE RESULTS\n\nfrom datetime import datetime\n\nprint( \"\\nWriting results to disk ...\" )\nsubmission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n\nprint( \"\\nFinished ...\")","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"","execution_count":null,"cell_type":"code","outputs":[]}],"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1}