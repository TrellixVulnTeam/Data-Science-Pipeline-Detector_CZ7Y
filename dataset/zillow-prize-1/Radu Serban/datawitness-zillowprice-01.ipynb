{"cells":[{"source":"**DataWitness team - Zillow Price competition - notebook 01:**\n\nThis notebook contains preliminary data exploration for Zillow prize competition, from ACN DataWitness team.\n\n**Objective** build in Python a model that predicts house prices in Los Angeles Orange county and improves the Zestimates residual error for Kaggle Zillow Price competition: https://www.kaggle.com/c/zillow-prize-1#Competition%20Overview \n**Details** This notebook uses Python3 environment based on the kaggle/python docker image: https://github.com/kaggle/docker-python. You can run it locally using JupyterHub/Anaconda, Rodeo, pyCharm IDE or your favourite Python IDE\n\nWe perform the following steps:\n1. Data loading\n2. Exploratory Data Analysis: cleaning, visualising, pre-processing and feature engineering\n3. Modelling (several models)\n4. Results and interpretation\n5. Prediction","metadata":{"_uuid":"c023f967e8766e85f9287022fb7200c1d3eb0934","_cell_guid":"6cd23899-6746-41d3-8374-8b9834127c89"},"cell_type":"markdown"},{"source":"#Import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb # ML\nimport matplotlib.pyplot as plt # Data visualization\nimport random\t#random number generator\nimport datetime as dt\nimport gc\nimport seaborn as sns #python visualization library\nimport sklearn as sk # ML\nfrom matplotlib.pyplot import show\nfrom matplotlib.colors import ListedColormap\nfrom ggplot import *\nfrom sklearn import ensemble\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom datetime import datetime\nimport numpy as numpy\nimport pylab\nimport calendar\nfrom scipy import stats\nfrom sklearn import model_selection, preprocessing\nfrom scipy.stats import kendalltau\nimport warnings\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Imputer\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nimport gc\n\n# Inputs\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Data viz\nfrom mlens.visualization import corr_X_y, corrmat\n\n# Model evaluation\nfrom mlens.metrics import make_scorer\nfrom mlens.model_selection import Evaluator\nfrom mlens.preprocessing import EnsembleTransformer\n\n# Ensemble\nfrom mlens.ensemble import SuperLearner\n\nfrom scipy.stats import uniform, randint\n\nfrom subprocess import check_output\n\nnp.random.seed(1)\n\n\ncolor = sns.color_palette()\nnp.random.seed(1)\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\ncolor = sns.color_palette()\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\n#Statistics on input files\nprint(\"\\n1. Input files:\\n\",check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n#### DATA LOADING\n\n#read training input files and returns data structure\ndef load_data():\n    train_2016 = pd.read_csv('../input/train_2016_v2.csv')\n    train_2017 = pd.read_csv('../input/train_2017.csv')\n    \n    train = pd.concat([train_2016, train_2017], ignore_index=True)\n    properties = pd.read_csv('../input/properties_2017.csv')\n    sample = pd.read_csv('../input/sample_submission.csv')\n    \n    print(\"Preprocessing...\")\n    for c, dtype in zip(properties.columns, properties.dtypes):\n        if dtype == np.float64:\n            properties[c] = properties[c].astype(np.float32)\n            \n    print(\"Set train/test data...\")\n    \n    # Add Features\n    # life of property\n    properties['N-life'] = 2018 - properties['yearbuilt']\n\n    properties['A-calculatedfinishedsquarefeet'] = properties['finishedsquarefeet12'] + properties['finishedsquarefeet15']\n\n    # error in calculation of the finished living area of home\n    properties['N-LivingAreaError'] = properties['calculatedfinishedsquarefeet'] / properties['finishedsquarefeet12']\n\n    # proportion of living area\n    properties['N-LivingAreaProp'] = properties['calculatedfinishedsquarefeet'] / properties['lotsizesquarefeet']\n    properties['N-LivingAreaProp2'] = properties['finishedsquarefeet12'] / properties['finishedsquarefeet15']\n\n    # Amout of extra space\n    properties['N-ExtraSpace'] = properties['lotsizesquarefeet'] - properties['calculatedfinishedsquarefeet']\n    properties['N-ExtraSpace-2'] = properties['finishedsquarefeet15'] - properties['finishedsquarefeet12']\n\n    # Total number of rooms\n    properties['N-TotalRooms'] = properties['bathroomcnt'] + properties['bedroomcnt']\n\n    # Average room size\n    #properties['N-AvRoomSize'] = properties['calculatedfinishedsquarefeet'] / properties['roomcnt']\n\n    # Number of Extra rooms\n    properties['N-ExtraRooms'] = properties['roomcnt'] - properties['N-TotalRooms']\n\n    # Ratio of the built structure value to land area\n    properties['N-ValueProp'] = properties['structuretaxvaluedollarcnt'] / properties['landtaxvaluedollarcnt']\n\n    # Does property have a garage, pool or hot tub and AC?\n    #properties['N-GarPoolAC'] = ((properties['garagecarcnt'] > 0) & (properties['pooltypeid10'] > 0) & (properties['airconditioningtypeid'] != 5)) * 1\n\n    properties[\"N-location\"] = properties[\"latitude\"] + properties[\"longitude\"]\n    properties[\"N-location-2\"] = properties[\"latitude\"] * properties[\"longitude\"]\n    #properties[\"N-location-2round\"] = properties[\"N-location-2\"].round(-4)\n\n    # Ratio of tax of property over parcel\n    properties['N-ValueRatio'] = properties['taxvaluedollarcnt'] / properties['taxamount']\n\n    # TotalTaxScore\n    properties['N-TaxScore'] = properties['taxvaluedollarcnt'] * properties['taxamount']\n\n    # polnomials of tax delinquency year\n    properties[\"N-taxdelinquencyyear-2\"] = properties[\"taxdelinquencyyear\"] ** 2\n    properties[\"N-taxdelinquencyyear-3\"] = properties[\"taxdelinquencyyear\"] ** 3\n\n    # Length of time since unpaid taxes\n    properties['N-live'] = 2018 - properties['taxdelinquencyyear']\n\n    # Number of properties in the zip\n    zip_count = properties['regionidzip'].value_counts().to_dict()\n    properties['N-zip_count'] = properties['regionidzip'].map(zip_count)\n\n    # Number of properties in the city\n    city_count = properties['regionidcity'].value_counts().to_dict()\n    properties['N-city_count'] = properties['regionidcity'].map(city_count)\n\n    # Number of properties in the city\n    region_count = properties['regionidcounty'].value_counts().to_dict()\n    properties['N-county_count'] = properties['regionidcounty'].map(region_count)\n\n\n    id_feature = ['heatingorsystemtypeid','propertylandusetypeid', 'storytypeid', 'airconditioningtypeid',\n        'architecturalstyletypeid', 'buildingclasstypeid', 'buildingqualitytypeid', 'typeconstructiontypeid']\n    for c in properties.columns:\n        properties[c]=properties[c].fillna(-1)\n        if properties[c].dtype == 'object':\n            lbl = LabelEncoder()\n            lbl.fit(list(properties[c].values))\n            properties[c] = lbl.transform(list(properties[c].values))\n        if c in id_feature:\n            lbl = LabelEncoder()\n            lbl.fit(list(properties[c].values))\n            properties[c] = lbl.transform(list(properties[c].values))\n            dum_df = pd.get_dummies(properties[c])\n            dum_df = dum_df.rename(columns=lambda x:c+str(x))\n            properties = pd.concat([properties,dum_df],axis=1)\n            properties = properties.drop([c], axis=1)\n            #print np.get_dummies(properties[c])\n    \n    #\n    # Make train and test dataframe\n    #\n    train = train.merge(properties, on='parcelid', how='left')\n    sample['parcelid'] = sample['ParcelId']\n    test = sample.merge(properties, on='parcelid', how='left')\n\n    # drop out ouliers\n    train = train[train.logerror > -0.4]\n    train = train[train.logerror < 0.418]\n\n    train[\"transactiondate\"] = pd.to_datetime(train[\"transactiondate\"])\n    train[\"Month\"] = train[\"transactiondate\"].dt.month\n    train[\"quarter\"] = train[\"transactiondate\"].dt.quarter\n    \n    test[\"Month\"] = 10\n    test['quarter'] = 4\n\n    x_train = train.drop(['parcelid', 'logerror','transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n    y_train = train[\"logerror\"].values\n    \n    x_test = test[x_train.columns]\n    del test, train    \n    print(x_train.shape, y_train.shape, x_test.shape)\n    \n    return x_train, y_train, x_test\n\nx_train, y_train, x_test = load_data()\n\n","outputs":[],"execution_count":null,"metadata":{"_uuid":"f6c823cd89790059af34c1cfa2b9fac49d98bb9b","_cell_guid":"a8ff378b-cd62-4d6a-972c-8cc09a96658f"},"cell_type":"code"},{"source":"**Data Quality**","metadata":{"collapsed":true,"_uuid":"da6f8f0fb3806b6c259eadef4b6cd8707e3255a5","_cell_guid":"c3c0a6f9-a01e-4908-be3c-cbe7cc0f92da"},"cell_type":"markdown"},{"source":"**Exploratory data analysis.**\nPlot distribution of transactions per month","metadata":{"_uuid":"ae932d1b6ec8b0a2723572c4d922c4775f275a02","_cell_guid":"ee9deffb-86cc-45f8-b17b-9e520a224d66"},"cell_type":"markdown"},{"source":"train_df = x_train\ntrain_y = y_train\n\n##Feature importance\ncat_cols = [\"hashottuborspa\", \"propertycountylandusecode\", \"propertyzoningdesc\", \"fireplaceflag\", \"taxdelinquencyflag\"]\n#train_df = train_df.drop(['parcelid', 'logerror', 'transactiondate', 'transaction_month']+cat_cols, axis=1)\nfeat_names = train_df.columns.values\n\nfrom sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=25, max_depth=30, max_features=0.3, n_jobs=-1, random_state=0)\nmodel.fit(train_df, train_y)\n\n## plot the feature importance ##\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()\n","outputs":[],"execution_count":null,"metadata":{"_uuid":"a79a7366a10d5358702024edc03d375fdcb2a122","_cell_guid":"8aa71868-3470-42ff-b867-6d9a2e82d64c"},"cell_type":"code"},{"source":"**Data prep**","metadata":{"_uuid":"d854eb2c1d170cb18206fdd254643412f5f50e24","_cell_guid":"ff15bef8-8660-4026-9f65-5a90b37da183"},"cell_type":"markdown"},{"source":"#Drop properties that are identifiers and will not have any relevance in prediction\nx_train = train_df.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\ny_train = train_df['logerror'].values\nprint(\"\\n---\\n\",x_train.shape, y_train.shape)\n\ntrain_columns = x_train.columns","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"14739009535b59d8a024fb99f81f050305190183","_cell_guid":"da43cc2e-7078-4a25-b804-dfdd98ea5718"},"cell_type":"code"},{"source":"**Properties 2016:** Exploration of the actual properties from 2016 file.","metadata":{"_uuid":"d212c90b2bd81764830f67ff4a648cfe772fa245","_cell_guid":"1817da44-2226-4814-9b83-4cf9f7c30dcc"},"cell_type":"markdown"},{"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nproperties = pd.read_csv(\"../input/properties_2016.csv\")\nprint(\"\\n---\\nnumber of rows x cols in train data: \")\nproperties.shape\nprint (\"\\n---\\n\")\n#show first 10 rows \nproperties.head(5)\n\nmissing_df = properties.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.sort_values(by='missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\n(fig, ax) = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missing_df.missing_count.values, color='blue')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()\n","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"a8605ca53c9ae1faac16d3d62c1bd53797b15b25","_cell_guid":"44bb8f6e-2c55-4559-b0d9-4b42493c719e"},"cell_type":"code"},{"source":"**02: MODEL BUILDING**","metadata":{"collapsed":true,"_uuid":"3e36de2ced778329825ed69a4d554a642274b4fa","_cell_guid":"8c48c6b7-6247-49b9-b4a2-3d1113a39325"},"cell_type":"markdown"},{"source":"### Importing Libraries or Packages that are needed throughout the Program ###\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport random\nimport datetime as dt\nimport gc\n\nimport seaborn as sns #python visualization library \ncolor = sns.color_palette()\n\n#%matplotlib inline\nnp.random.seed(1)\n###Load the Datasets ###\n\n# We need to load the datasets that will be needed to train our machine learning algorithms, handle our data and make predictions. Note that these datasets are the ones that are already provided once you enter the competition by accepting terms and conditions #\n\ntrain = pd.read_csv('../input/train_2016_v2.csv' , parse_dates=[\"transactiondate\"]) \nproperties = pd.read_csv('../input/properties_2016.csv')   \ntest = pd.read_csv('../input/sample_submission.csv') \ntest= test.rename(columns={'ParcelId': 'parcelid'}) #To make it easier for merging datasets on same column_id later\n\n\n### Analyse the Dimensions of our Datasets.\n\nprint(\"Training Size:\" + str(train.shape))\nprint(\"Property Size:\" + str(properties.shape))\nprint(\"Sample Size:\" + str(test.shape))\n\n### Type Converting the DataSet ###\n# The processing of some of the algorithms can be made quick if data representation is made in int/float32 instead of int/float64. Therefore, in order to make sure that all of our columns types are in float32, we are implementing the following lines of code #\nfor c, dtype in zip(properties.columns, properties.dtypes):\n    if dtype == np.float64:        \n        properties[c] = properties[c].astype(np.float32)\n    if dtype == np.int64:\n        properties[c] = properties[c].astype(np.int32)\n\n\nfor column in test.columns:\n    if test[column].dtype == int:\n        test[column] = test[column].astype(np.int32)\n    if test[column].dtype == float:\n        test[column] = test[column].astype(np.float32)\n\n\n### feature engineering\n#living area proportions \nproperties['living_area_prop'] = properties['calculatedfinishedsquarefeet'] / properties['lotsizesquarefeet']\n#tax value ratio\nproperties['value_ratio'] = properties['taxvaluedollarcnt'] / properties['taxamount']\n#tax value proportions\nproperties['value_prop'] = properties['structuretaxvaluedollarcnt'] / properties['landtaxvaluedollarcnt']\n\n\n###Merging the Datasets ###\n\n# We are merging the properties dataset with training and testing dataset for model building and testing prediction #\n\ndf_train = train.merge(properties, how='left', on='parcelid') \ndf_test = test.merge(properties, how='left', on='parcelid')\n\n\n### Remove previos variables to keep some memory\ndel properties, train\ngc.collect();\n\n\nprint('Memory usage reduction...')\ndf_train[['latitude', 'longitude']] /= 1e6\ndf_test[['latitude', 'longitude']] /= 1e6\n\ndf_train['censustractandblock'] /= 1e12\ndf_test['censustractandblock'] /= 1e12\n\n### Let's do some pre-exploratory analysis to identify how much missing values do we have in our datasets. \n\n# Let's do some engineering with fireplaceflag variable.\n\nprint(df_train.fireplaceflag.isnull().sum())\nprint(df_train.fireplacecnt.isnull().sum())\n# By using fireplacecnt variable we can recover some fields of fireplaceflag\n\ndf_train['fireplaceflag']= \"No\"\ndf_train.loc[df_train['fireplacecnt']>0,'fireplaceflag']= \"Yes\"\n\n# Remaining Missing fireplacecnt will be replaced with 0.\nindex = df_train.fireplacecnt.isnull()\ndf_train.loc[index,'fireplacecnt'] = 0\n\n#Tax deliquency flag - assume if it is null then doesn't exist\nindex = df_train.taxdelinquencyflag.isnull()\ndf_train.loc[index,'taxdelinquencyflag'] = \"None\"\n\n\n# Similar step performed for Pool/Spa/hot tub\nprint(df_train.hashottuborspa.value_counts())\nprint(df_train.pooltypeid10.value_counts())\n\n#lets remove 'pooltypeid10' as has more missing values\nprint(df_train.hashottuborspa.value_counts())\nprint(df_train.pooltypeid10.value_counts())\n\n#Assume if the pooltype id is null then pool/hottub doesnt exist \nindex = df_train.pooltypeid2.isnull()\ndf_train.loc[index,'pooltypeid2'] = 0\n\nindex = df_train.pooltypeid7.isnull()\ndf_train.loc[index,'pooltypeid7'] = 0\n\nindex = df_train.poolcnt.isnull()\ndf_train.loc[index,'poolcnt'] = 0\n\n### Label Encoding For Machine Learning & Filling Missing Values ###\n# We are now label encoding our datasets. All of the machine learning algorithms employed in scikit learn assume that the data being fed to them is in numerical form. LabelEncoding ensures that all of our categorical variables are in numerical representation. Also note that we are filling the missing values in our dataset with a zero before label encoding them. This is to ensure that label encoder function does not experience any problems while carrying out its operation #\n\nfrom sklearn.preprocessing import LabelEncoder  \n\nlbl = LabelEncoder()\nfor c in df_train.columns:\n    df_train[c]=df_train[c].fillna(0)\n    if df_train[c].dtype == 'object':\n        lbl.fit(list(df_train[c].values))\n        df_train[c] = lbl.transform(list(df_train[c].values))\n\nfor c in df_test.columns:\n    df_test[c]=df_test[c].fillna(0)\n    if df_test[c].dtype == 'object':\n        lbl.fit(list(df_test[c].values))\n        df_test[c] = lbl.transform(list(df_test[c].values))     \n\n\n# Drop unuseful features and align/include same features in test as in the training set #\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode' ], axis=1)\n\nx_test = df_test.drop(['parcelid', 'propertyzoningdesc',\n                       'propertycountylandusecode', '201610', '201611', \n                       '201612', '201710', '201711', '201712'], axis = 1) \n\nx_train = x_train.values\ny_train = df_train['logerror'].values\n\n### Cross Validation ###\n# We are dividing our datasets into the training and validation sets so that we could monitor and the test the progress of our machine learning algorithm. This would let us know when our model might be over or under fitting on the dataset that we have employed. #\n\nfrom sklearn.model_selection import train_test_split\n\nX = x_train\ny = y_train \n\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.1, random_state=47)\n\n###Implement the Xgboost### \n\n# We can now select the parameters for Xgboost and monitor the progress of results on our validation set. The explanation of the xgboost parameters and what they do can be found on the following link http://xgboost.readthedocs.io/en/latest/parameter.html #\n\ndtrain = xgb.DMatrix(Xtrain, label=ytrain)\ndvalid = xgb.DMatrix(Xvalid, label=yvalid)\ndtest = xgb.DMatrix(x_test.values)\n\n# Try out different parameters\nxgb_params = {'min_child_weight': 10, 'eta': 0.035, 'colsample_bytree': 0.5, 'max_depth': 4,\n            'subsample': 0.85, 'lambda': 0.9, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n            'eval_metric': 'mae', 'objective': 'reg:linear' }           \n\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\nmodel_xgb = xgb.train(xgb_params, dtrain, 1000, watchlist, early_stopping_rounds=100,\n                  maximize=False, verbose_eval=10)\n\n###Predicting the results###\n\n# Let us now predict the target variable for our test dataset. All we have to do now is just fit the already trained model on the test set that we had made merging the sample file with properties dataset #\n\nPredicted_test_xgb = model_xgb.predict(dtest)\n\n### Submitting the Results ###\n\n# Once again load the file and start submitting the results in each column #\nsample_file = pd.read_csv('../input/sample_submission.csv') \nfor c in sample_file.columns[sample_file.columns != 'ParcelId']:\n    sample_file[c] = Predicted_test_xgb\n\nprint('Preparing the csv file ...')\nsample_file.to_csv('xgb_predicted_results.csv', index=False, float_format='%.4f')\nprint(\"Finished writing the file\")\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model_xgb, max_num_features=50, height=0.8, ax=ax)\nplt.show()","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"88086c2ef0c343f1ff6fdacf2c7473e6bd2c997b","_cell_guid":"bc8e8cc2-a43a-4719-8f4e-7f1c1673e0a7"},"cell_type":"code"},{"source":"#Neural Networks\nfrom datetime import datetime\nimport numpy as np\nimport numpy as numpy\nimport pandas as pd\nimport pylab\nimport calendar\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn import model_selection, preprocessing\nfrom scipy.stats import kendalltau\nimport warnings\nimport matplotlib.pyplot as plt\nimport pandas\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\n\n##  READ DATA  ##\n# Load train, Prop and sample\n#print('Loading train, prop and sample data')\n#train = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n#prop = pd.read_csv('../input/properties_2016.csv')\n#sample = pd.read_csv('../input/sample_submission.csv')\n\nprint('Fitting Label Encoder on properties')\nfor c in prop.columns:\n    prop[c]=prop[c].fillna(-1)\n    if prop[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(prop[c].values))\n        prop[c] = lbl.transform(list(prop[c].values))\n        \n#Create df_train and x_train y_train from that\nprint('Creating training set:')\ndf_train = train.merge(prop, how='left', on='parcelid')\n\n###########################################################\ndf_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\ndf_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\ndf_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\ndf_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\ndf_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day\n\n\n###########################################\n\nprint('Fill  NA/NaN values using suitable method' )\ndf_train.fillna(-1.0)\n\nprint('Create x_train and y_train from df_train' )\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)\ny_train = df_train[\"logerror\"]\n\ny_mean = np.mean(y_train)\nprint(x_train.shape, y_train.shape)\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n# Create df_test and test set\nprint('Creating df_test  :')\nsample['parcelid'] = sample['ParcelId']\n\nprint(\"Merge Sample with property data :\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\n\n\n########################\ndf_test[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\ndf_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\ndf_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\ndf_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\ndf_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day     \n\n#################################\nx_test = df_test[train_columns]\n\nprint('Shape of x_test:', x_test.shape)\nprint(\"Preparing x_test:\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\n  \nfrom sklearn.preprocessing import Imputer\nimputer= Imputer()\nimputer.fit(x_train.iloc[:, :])\nx_train = imputer.transform(x_train.iloc[:, :])\nimputer.fit(x_test.iloc[:, :])\nx_test = imputer.transform(x_test.iloc[:, :])\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n\n##  RUN NETWORK  ##\nlen_x=int(x_train.shape[1])\nprint(\"len_x is:\",len_x)\n\n####################ANN Starts here#\n\nnn = Sequential()\nnn.add(Dense(units = 360 , kernel_initializer = 'normal', activation = 'tanh', input_dim = len_x))\nnn.add(Dropout(.17))\nnn.add(Dense(units = 150 , kernel_initializer = 'normal', activation = 'relu'))\nnn.add(BatchNormalization())\nnn.add(Dropout(.4))\nnn.add(Dense(units = 60 , kernel_initializer = 'normal', activation = 'relu'))\nnn.add(BatchNormalization())\nnn.add(Dropout(.32))\nnn.add(Dense(units = 25, kernel_initializer = 'normal', activation = 'relu'))\nnn.add(BatchNormalization())\nnn.add(Dropout(.22))\nnn.add(Dense(1, kernel_initializer='normal'))\nnn.compile(loss='mae', optimizer='adam')\n\nnn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 100, verbose=2)\n\nprint(\"x_test.shape:\",x_test.shape)\ny_pred_ann = nn.predict(x_test)\n\n#######################################################################################\nprint( \"\\nWriting predictions...\" )\n##  WRITE RESULTS  ##\ny_pred = y_pred_ann.flatten()\n\n#output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\noutput = pd.DataFrame({'ParcelId': prop['parcelid'].astype(np.int32),\n        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n# set col 'ParceID' to first col\ncols = output.columns.tolist()\ncols = cols[-1:] + cols[:-1]\noutput = output[cols]\n\nprint( \"\\nWriting results to disk:\" )\noutput.to_csv('NN_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n\nprint( \"\\nFinished!\" )","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"d7a516673ccf6c1743e11f61eebf7765d58370a4","_cell_guid":"8155fc97-f664-4701-ba14-35e1eafc3a71"},"cell_type":"code"},{"source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nh = .02  # step size in the mesh\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [train, properties, test]\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.4, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # and testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot also the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # and testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nplt.tight_layout()\nplt.show()","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"c07ac53e817f92466a646a846b9863682ccd6574","_cell_guid":"ea7a3967-f8bb-4ee6-a737-e74dd821c3b8"},"cell_type":"code"},{"source":"**SUBMISSION**","metadata":{"collapsed":true,"_uuid":"3fe20d1e594d62cc6e95cffa04213c2f11bc4c9f","_cell_guid":"68dfd686-1c63-4199-9fa8-81e3ecdb333f"},"cell_type":"markdown"}],"nbformat_minor":1,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","version":"3.6.3"}},"nbformat":4}