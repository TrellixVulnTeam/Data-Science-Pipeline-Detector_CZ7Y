{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7bb19dd-a984-d55e-7b69-98dea421945e"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport gc\nfrom sklearn.linear_model import ElasticNetCV, LassoLarsCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import r2_score\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d97bd8d-6191-d698-d889-686a8d605d5e"},"outputs":[],"source":"train = pd.read_csv('../input/train_2016.csv')\nprop = pd.read_csv('../input/properties_2016.csv')\nsample = pd.read_csv('../input/sample_submission.csv')\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d1788e8-c4ba-e901-ed3d-b10773f739a9"},"outputs":[],"source":"for c, dtype in zip(prop.columns, prop.dtypes):\n\tif dtype == np.float64:\n\t\tprop[c] = prop[c].astype(np.float32)\n\n\n\ndf_train = train.merge(prop, how='left', on='parcelid')\n\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode', 'basementsqft', 'buildingclasstypeid', 'finishedsquarefeet13', 'storytypeid'], axis=1)\ny_train = df_train['logerror'].values\n\n\n\n\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n    \n    \nprint(x_train.shape, y_train.shape)\n\n#clean data \nx_train = x_train.fillna(0.0)\nx_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c7564b4-19e1-3d48-2a07-1d4936588df0"},"outputs":[],"source":"n_comp = 12\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(x_train)\n\n\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(x_train)\n\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(x_train)\n\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(x_train)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(x_train)\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp + 1):\n    x_train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n    #test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n\n    x_train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n    #test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n    x_train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n    #test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n    x_train['grp_' + str(i)] = grp_results_train[:, i - 1]\n    #test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    x_train['srp_' + str(i)] = srp_results_train[:, i - 1]\n    #test['srp_' + str(i)] = srp_results_test[:, i - 1]\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3bb7486a-eb37-73bd-2625-78365359c464"},"outputs":[],"source":"x_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03e5cc6a-d872-49ff-53e3-f7fe58a04f47"},"outputs":[],"source":"split = 80000\nx_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n\nprint('building data matrix...')\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\n\nprint('training ...')\nparams = {}\nparams['eta'] = 0.02\nparams['objective'] = 'reg:linear'\nparams['eval_metric'] = ['rmse', 'logloss']\nparams['max_depth'] = 9\nparams['silent'] = 1\nparams['subsample'] = 0.7\nparams['colsample_bytree'] = 0.7\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\ncv_output = xgb.cv(params, d_train, num_boost_round=3000, early_stopping_rounds=20,\n    verbose_eval=50, show_stdv=False)\n\nnum_boost_rounds = len(cv_output)\n\nclf = xgb.train(params, d_train, num_boost_round= num_boost_rounds)\n\ndel d_train, d_valid\n\nprint('Building test set ...')\n\nsample['parcelid'] = sample['ParcelId']\ndf_test = sample.merge(prop, on='parcelid', how='left')\n\ndel prop; gc.collect()\n\nx_test = df_test[train_columns]\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\n\ndel df_test, sample; gc.collect()\n\nd_test = xgb.DMatrix(x_test)\n\ndel x_test; gc.collect()\n\nprint('Predicting on test ...')\n\np_test = clf.predict(d_test)\n\ndel d_test; gc.collect()\n\nresult = pd.read_csv('../input/sample_submission.csv')\n\nfor c in result.columns[result.columns != 'ParcelId']:\n    result[c] = p_test\n\nprint('writing csv output ...')\nresult.to_csv('predictions.csv', index=False, float_format='%.4f') # Thanks to @inversion"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"079ea6f2-598f-89b1-3541-8ed65cd3f7e7"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}