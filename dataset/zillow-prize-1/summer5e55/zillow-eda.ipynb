{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import packages\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport time \nimport datetime\nimport collections\n# from plotnine import *\nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge \nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nimport xgboost as xgb\nimport lightgbm as lgb\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:07:42.585388Z","iopub.execute_input":"2021-11-28T10:07:42.585734Z","iopub.status.idle":"2021-11-28T10:07:42.594681Z","shell.execute_reply.started":"2021-11-28T10:07:42.585698Z","shell.execute_reply":"2021-11-28T10:07:42.593729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Table of Content\n* [Overview](#1)\n    - [Raw Data Structure](#1.1)\n    - [Data Size](#1.2)\n    - [Feature Type](#1.3)\n    - [Missing Rate and Null Treatment](#1.4) \n* [Target Variable](#2)\n    - [Time Series](#2.1)\n    - [Unconditional Distribution](#2.2)\n    - [Conditional Distribution](#2.3)\n* [Features](#3)\n    - [Categorical Features](#3.1)\n        - [Distribution](#3.1.1)\n        - [Correlation with Y](#3.1.2)\n        - [Correlation within X](#3.1.3)\n        - [Feature Importance via RF](#3.1.4)        \n    - [Numerical Features](#3.2)\n        - [Distribution](#3.2.1)\n        - [Correlation with Y](#3.2.1)\n        - [Correlation within X](#3.2.2)\n        - [Feature Importance via RF](#3.2.3)\n* [New Features](#4)\n \n ","metadata":{}},{"cell_type":"markdown","source":"# EDA\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n## Overview","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n### Raw Data Structure\n  - raw features include properties features in 2016 and 2017\n  - target data includes house transaction data with time stamp and log error between transaction price and zestimate in 2016 and 2017\n  - submission data: list of properties to predict.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n### Data Size ","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/zillow-prize-1/'\ny_train_2016 = pd.read_csv(f\"{data_dir}train_2016_v2.csv\")\ny_train_2017 = pd.read_csv(f\"{data_dir}train_2017.csv\")\nX_train_2016 = pd.read_csv(f\"{data_dir}properties_2016.csv\")\nX_train_2017 = pd.read_csv(f\"{data_dir}properties_2017.csv\")\ndf_submission = pd.read_csv(f\"{data_dir}sample_submission.csv\")\n\nprint(f\"\\nX_train_2016 shape: {X_train_2016.shape}\\ny_train_2016 shape: {y_train_2016.shape}\\n2017 X_train shape: {X_train_2017.shape} \\\n      \\n y_train_2017 shape: {y_train_2017.shape}\\ntest_data shape:{df_submission.shape}\")\ndel df_submission","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:07:42.596549Z","iopub.execute_input":"2021-11-28T10:07:42.596813Z","iopub.status.idle":"2021-11-28T10:08:21.117132Z","shell.execute_reply.started":"2021-11-28T10:07:42.596779Z","shell.execute_reply":"2021-11-28T10:08:21.116141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.3\"></a>\n### Feature Types\n* 52 float, 1 integer and 5 object type\n* Many features have small number of unique values, likely to be categorical variables\n* Based on data dictionary and variable names, there are quite a few cateogrical variables related geographic location too. We need to encode cateogrical variables if we want to use linear regression. \n* Encode string values using numeric integer code","metadata":{}},{"cell_type":"code","source":"data_type = X_train_2016.dtypes\ndata_cnt = X_train_2016.nunique()\nprint(f\"frequencey count of differnet types:{collections.Counter(data_type)}\")\n\ndata_summary = pd.concat([data_type,data_cnt],axis=1)\ndata_summary.columns = ['dtype','unique_cnt']\nprint(data_summary.sort_values(['dtype','unique_cnt']))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:08:21.118909Z","iopub.execute_input":"2021-11-28T10:08:21.119442Z","iopub.status.idle":"2021-11-28T10:08:25.108506Z","shell.execute_reply.started":"2021-11-28T10:08:21.119393Z","shell.execute_reply":"2021-11-28T10:08:25.107599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#object type deepdive\ncol_object = data_summary.loc[data_summary['dtype']=='object',:].index\nX_train_2016.loc[:,col_object].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:08:25.10987Z","iopub.execute_input":"2021-11-28T10:08:25.110121Z","iopub.status.idle":"2021-11-28T10:08:27.372363Z","shell.execute_reply.started":"2021-11-28T10:08:25.11009Z","shell.execute_reply":"2021-11-28T10:08:27.37131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert boolean type to integer\nX_2016_clean = X_train_2016.copy()\nX_2016_clean[['hashottuborspa','fireplaceflag']] = X_2016_clean[['hashottuborspa','fireplaceflag']].fillna(value=0).astype(int)\nX_2016_clean.loc[X_2016_clean.taxdelinquencyflag=='Y','taxdelinquencyflag'] = 1.0\nX_2016_clean.loc[X_2016_clean.taxdelinquencyflag!='Y','taxdelinquencyflag'] = 0.0\ncol_str = ['propertycountylandusecode','propertyzoningdesc']\nfor col in col_str:\n    uniq_val = X_2016_clean[col].unique()\n    df_encoding = pd.DataFrame(np.arange(len(uniq_val)),columns=[col+'_e'])\n    df_encoding[col] = uniq_val\n    X_2016_clean = pd.merge(X_2016_clean, df_encoding, on=[col], how='left')\n    X_2016_clean.drop(col,axis=1,inplace=True)\n    X_2016_clean.rename(columns={col+\"_e\":col},inplace=True)\n\n#convert boolean type to integer\nX_2017_clean = X_train_2017.copy()\nX_2017_clean[['hashottuborspa','fireplaceflag']] = X_2017_clean[['hashottuborspa','fireplaceflag']].fillna(value=0).astype(int)\nX_2017_clean.loc[X_2017_clean.taxdelinquencyflag=='Y','taxdelinquencyflag'] = 1.0\nX_2017_clean.loc[X_2017_clean.taxdelinquencyflag!='Y','taxdelinquencyflag'] = 0.0\ncol_str = ['propertycountylandusecode','propertyzoningdesc']\nfor col in col_str:\n    uniq_val = X_2017_clean[col].unique()\n    df_encoding = pd.DataFrame(np.arange(len(uniq_val)),columns=[col+'_e'])\n    df_encoding[col] = uniq_val\n    X_2017_clean = pd.merge(X_2017_clean, df_encoding, on=[col], how='left')\n    X_2017_clean.drop(col,axis=1,inplace=True)\n    X_2017_clean.rename(columns={col+\"_e\":col},inplace=True)\n\ndel X_train_2016, X_train_2017\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:08:27.374557Z","iopub.execute_input":"2021-11-28T10:08:27.374853Z","iopub.status.idle":"2021-11-28T10:09:04.525202Z","shell.execute_reply.started":"2021-11-28T10:08:27.374816Z","shell.execute_reply":"2021-11-28T10:09:04.524007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.4\"></a>\n### Missing Rate and Null Treatment\n* Missing Rate: 17 features with more than 90% missing, could drop these. \n* Null treatment: 9 features with significant missing rate between 0.5 to 0.9, worth deepdive to identify appropriate imputation strategy: impute 4 count variable missing to 0.","metadata":{}},{"cell_type":"code","source":"data_summary['missing_rate'] = 1 - X_2016_clean.count()/X_2016_clean.shape[0]\n#missing rate when joined with target\ntrain_data = pd.merge(y_train_2016, X_2016_clean, on=['parcelid'],how='left')\nmr = 1-train_data.count()/train_data.shape[0]\nmr.name = 'missing_rate_sample'\n# data_summary.drop('missing_rate_sample', axis=1, inplace=True)\ndata_summary = pd.concat([data_summary, mr],axis=1)\n\nmr = data_summary.copy()\nmr.loc[mr.missing_rate==0,'mr_type'] = 'No Missing'\nmr.loc[mr.missing_rate>0,'mr_type'] = '(0,0.1]'\nmr.loc[mr.missing_rate>0.1,'mr_type'] = '(0.1,0.5]'\nmr.loc[mr.missing_rate>0.5,'mr_type'] = '(0.5,0.9]'\nmr.loc[mr.missing_rate>0.9,'mr_type'] = '(0.9, 1)'\ndf_plt = mr.groupby('mr_type')['dtype'].count()\nax = df_plt.plot(kind='bar')\nax.set_ylabel('No. of Features')\nax.set_xlabel('missing rate range')\nfor i, v in enumerate(df_plt):\n    ax.text(i,v + 0.5,str(v), color='blue', fontweight='bold')\n# data_summary.sort_values('missing_rate_sample',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:04.526969Z","iopub.execute_input":"2021-11-28T10:09:04.527617Z","iopub.status.idle":"2021-11-28T10:09:07.396467Z","shell.execute_reply.started":"2021-11-28T10:09:04.527564Z","shell.execute_reply":"2021-11-28T10:09:07.395774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_impute = mr.loc[mr.mr_type.isin(['(0.5,0.9]','(0.1,0.5]']),:]\nX_2016_clean[df_impute.index].describe().T","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:07.397493Z","iopub.execute_input":"2021-11-28T10:09:07.397855Z","iopub.status.idle":"2021-11-28T10:09:08.459301Z","shell.execute_reply.started":"2021-11-28T10:09:07.397823Z","shell.execute_reply":"2021-11-28T10:09:08.458378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#impute missing count without 0 with 0\ncol1 = list(set(list(df_impute.index.values)) - \nset([c for c in df_impute.index if 'garage' in c]+\n    ['regionidneighborhood','numberofstories','unitcnt','airconditioningtypeid','buildingqualitytypeid','heatingorsystemtypeid'])) \nX_2016_clean[col1] = X_2016_clean[col1].fillna(value=0)\n\n#fill pool id and area with 0 if poolcnt = 0\ncol1 = ['pooltypeid2','pooltypeid7','pooltypeid10','poolsizesum']\nX_2016_clean.loc[X_2016_clean.poolcnt==0,col1] = 0","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:08.460839Z","iopub.execute_input":"2021-11-28T10:09:08.461329Z","iopub.status.idle":"2021-11-28T10:09:08.74388Z","shell.execute_reply.started":"2021-11-28T10:09:08.461288Z","shell.execute_reply":"2021-11-28T10:09:08.743178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_summary['missing_rate'] = 1 - X_2016_clean.count()/X_2016_clean.shape[0]\n#missing rate when joined with target\ntrain_data = pd.merge(y_train_2016, X_2016_clean, on=['parcelid'],how='left')\nmr = 1-train_data.count()/train_data.shape[0]\nmr.name = 'missing_rate_sample'\ndata_summary.drop('missing_rate_sample', axis=1, inplace=True)\ndata_summary = pd.concat([data_summary, mr],axis=1)\n\nmr = data_summary.copy()\nmr.loc[mr.missing_rate==0,'mr_type'] = 'No Missing'\nmr.loc[mr.missing_rate>0,'mr_type'] = '(0,0.1]'\nmr.loc[mr.missing_rate>0.1,'mr_type'] = '(0.1,0.5]'\nmr.loc[mr.missing_rate>0.5,'mr_type'] = '(0.5,0.9]'\nmr.loc[mr.missing_rate>0.9,'mr_type'] = '(0.9, 1)'\ndf_plt = mr.groupby('mr_type')['dtype'].count()\nax = df_plt.plot(kind='bar')\nax.set_ylabel('No. of Features')\nax.set_xlabel('missing rate range')\nfor i, v in enumerate(df_plt):\n    ax.text(i,v + 0.5,str(v), color='blue', fontweight='bold')\n# data_summary.sort_values('missing_rate_sample',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:08.744973Z","iopub.execute_input":"2021-11-28T10:09:08.745861Z","iopub.status.idle":"2021-11-28T10:09:11.644582Z","shell.execute_reply.started":"2021-11-28T10:09:08.745819Z","shell.execute_reply":"2021-11-28T10:09:11.643592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n## Target Variable\n* Clearly there are quite a bit of outliers in target variable","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n### Time Series\n* Clear Seasonality in Volume, seasonal dummy as features\n* Outliers each month\n* Multiple transaction per properties: 320\n","metadata":{}},{"cell_type":"code","source":"#count over time\ndate_col ='transactiondate'\ny_train_2016['date_'] = y_train_2016[date_col].apply(lambda x: pd.Timestamp(x))\ny_train_2017['date_'] = y_train_2017[date_col].apply(lambda x: pd.Timestamp(x))\n\nfig, ax = plt.subplots(2,1,figsize=(10,10))\nys = [y_train_2016, y_train_2017]\ntitles = ['Transanction Volume over Time in 2016', 'Transaction Volume over Time in 2017']\nfor i, y in enumerate(ys):\n    plti = y.groupby('date_')['logerror'].count()\n    ax[i].plot(plti.index, plti.values)\n    ax[i].set_title(titles[i])\n    ax[i].set_ylabel('Transaction Count')\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:11.64585Z","iopub.execute_input":"2021-11-28T10:09:11.646076Z","iopub.status.idle":"2021-11-28T10:09:13.137465Z","shell.execute_reply.started":"2021-11-28T10:09:11.646048Z","shell.execute_reply":"2021-11-28T10:09:13.136531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#boxplot over time\nfig, ax = plt.subplots(2,1,figsize=(10,10))\nys = [y_train_2016, y_train_2017]\ntitles = ['Logerror Distribution over Time in 2016', 'Log Error Distribution over Time in 2017']\nfor i, y in enumerate(ys):\n    y['ym'] = y.date_.apply(lambda x: x.year*100 + x.month)\n    sns.boxplot(x='ym',y='logerror', data=y, ax=ax[i])\n    ax[i].set_title(titles[i])\n    ax[i].set_ylabel('Transaction Count')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:13.138945Z","iopub.execute_input":"2021-11-28T10:09:13.139721Z","iopub.status.idle":"2021-11-28T10:09:15.374735Z","shell.execute_reply.started":"2021-11-28T10:09:13.139677Z","shell.execute_reply":"2021-11-28T10:09:15.373777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* How many properties have more than one transactions in 2016/2017?\n","metadata":{}},{"cell_type":"code","source":"transaction_2016_cnt = y_train_2016.groupby('parcelid')['transactiondate'].count()\ntransaction_2017_cnt = y_train_2017.groupby('parcelid')['transactiondate'].count()\nmultiple_2016 = transaction_2016_cnt[transaction_2016_cnt>1]\nmultiple_2017 = transaction_2017_cnt[transaction_2017_cnt>1]\nprint(f\"{len(multiple_2016)+len(multiple_2017)} properties have multiple transactions within 2016 or 2017\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:15.376Z","iopub.execute_input":"2021-11-28T10:09:15.376255Z","iopub.status.idle":"2021-11-28T10:09:15.448807Z","shell.execute_reply.started":"2021-11-28T10:09:15.376222Z","shell.execute_reply":"2021-11-28T10:09:15.448104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n### Unconditional Distribution \n* comparison between raw and winsorized data\n* winsorize by yearmon at different threshold","metadata":{}},{"cell_type":"code","source":"#raw historgram\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nax[0].hist(y_train_2016.logerror,500)\nax[0].set_title('Histogram of log error in 2016')\nax[0].set_ylabel('Count')\nax[0].set_xlabel('logerror')\nax[1].hist(y_train_2017.logerror,500)\nax[1].set_title('Histogram of log error in 2017')\nax[1].set_ylabel('Count')\nax[1].set_xlabel('logerror')","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:15.449847Z","iopub.execute_input":"2021-11-28T10:09:15.450557Z","iopub.status.idle":"2021-11-28T10:09:17.862625Z","shell.execute_reply.started":"2021-11-28T10:09:15.450513Z","shell.execute_reply":"2021-11-28T10:09:17.861567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#winsorize\ndef winsorize(df,date_col,data_col,limit=[0.01,0.99]):\n    df['ym'] = df[date_col].apply(lambda x: x.year*100 + x.month)\n    df[data_col+'_wc_'+str(limit[0])] = df.groupby('ym')[data_col].transform(lambda x: x.clip(*x.quantile(limit)))\n    df[data_col+'_w_'+str(limit[0])] = df[data_col].transform(lambda x: x.clip(*x.quantile(limit)))\n    return df\ny_train_2016 = winsorize(y_train_2016,'date_','logerror',[0.01,0.99])\ny_train_2017 = winsorize(y_train_2017,'date_','logerror',[0.01,0.99])\n\n#outlier transform\nfig, ax = plt.subplots(2,2,figsize=(15,10))\nax = ax.ravel()\nax[0].hist(y_train_2016['logerror_w_0.01'],100)\nax[0].set_title('Histogram of winsorized logerror in 2016')\nax[0].set_ylabel('Count')\nax[0].set_xlabel('winsorized at [0.01,0.99] logerror')\nax[1].hist(y_train_2017['logerror_w_0.01'],100)\nax[1].set_title('Histogram of winsorized logerror in 2017')\nax[1].set_ylabel('Count')\nax[1].set_xlabel('winsorized at [0.01,0.99] logerror')\nax[2].hist(y_train_2016['logerror_wc_0.01'],100)\nax[2].set_title('Histogram of winsorized logerror in 2016')\nax[2].set_ylabel('Count')\nax[2].set_xlabel('winsorized by ym at [0.01,0.99] logerror')\nax[3].hist(y_train_2017['logerror_wc_0.01'],100)\nax[3].set_title('Histogram of winsorized logerror in 2017')\nax[3].set_ylabel('Count')\nax[3].set_xlabel('winsorized by ym at [0.01,0.99] logerror')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:17.866509Z","iopub.execute_input":"2021-11-28T10:09:17.86678Z","iopub.status.idle":"2021-11-28T10:09:20.840989Z","shell.execute_reply.started":"2021-11-28T10:09:17.866748Z","shell.execute_reply":"2021-11-28T10:09:20.840067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_2016 = winsorize(y_train_2016,'date_','logerror',[0.05,0.95])\ny_train_2017 = winsorize(y_train_2017,'date_','logerror',[0.05,0.95])\nfig, ax = plt.subplots(2,2,figsize=(15,10))\nax = ax.ravel()\nax[0].hist(y_train_2016['logerror_w_0.05'],100)\nax[0].set_title('Histogram of winsorized logerror in 2016')\nax[0].set_ylabel('Count')\nax[0].set_xlabel('winsorized at [0.05,0.95] logerror')\nax[1].hist(y_train_2017['logerror_w_0.05'],100)\nax[1].set_title('Histogram of winsorized logerror in 2017')\nax[1].set_ylabel('Count')\nax[1].set_xlabel('winsorized at [0.01,0.99] logerror')\nax[2].hist(y_train_2016['logerror_wc_0.05'],100)\nax[2].set_title('Histogram of winsorized logerror in 2016')\nax[2].set_ylabel('Count')\nax[2].set_xlabel('winsorized by ym at [0.05,0.95] logerror')\nax[3].hist(y_train_2017['logerror_wc_0.05'],100)\nax[3].set_title('Histogram of winsorized logerror in 2017')\nax[3].set_ylabel('Count')\nax[3].set_xlabel('winsorized by ym at [0.05,0.95] logerror')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:20.842448Z","iopub.execute_input":"2021-11-28T10:09:20.843184Z","iopub.status.idle":"2021-11-28T10:09:23.799456Z","shell.execute_reply.started":"2021-11-28T10:09:20.843143Z","shell.execute_reply":"2021-11-28T10:09:23.798484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n### Conditional Distribution\n* Simple 1-d Gaussian Mixture Model ","metadata":{}},{"cell_type":"code","source":"from sklearn import mixture \nvname = 'logerror_wc_0.01'\ntitles = ['GMM of 2016 '+vname,'GMM of 2017'+vname]\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nfor i, y in enumerate(ys):\n        \n    f = y[[vname]]\n\n    g = mixture.GaussianMixture(n_components=3,covariance_type='full')\n    g.fit(f)\n    weights = g.weights_\n    means = g.means_\n    covars = g.covariances_\n\n    ax[i].hist(f, bins=100, histtype='bar', density=True, ec='red', alpha=0.5)\n    f_axis = f.copy()\n    f_axis.sort_values(vname,inplace=True)\n    ax[i].plot(f_axis,weights[0]*stats.norm.pdf(f_axis,means[0][0],np.sqrt(covars[0][0])).ravel(), c='blue')\n    ax[i].plot(f_axis,weights[1]*stats.norm.pdf(f_axis,means[1][0],np.sqrt(covars[1][0])).ravel(), c='green')\n    ax[i].plot(f_axis,weights[2]*stats.norm.pdf(f_axis,means[2][0],np.sqrt(covars[2][0])).ravel(), c='m')\n    ax[i].set_title(titles[i])","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:23.801065Z","iopub.execute_input":"2021-11-28T10:09:23.801422Z","iopub.status.idle":"2021-11-28T10:09:26.502492Z","shell.execute_reply.started":"2021-11-28T10:09:23.801374Z","shell.execute_reply":"2021-11-28T10:09:26.501746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Anything special for properties with multiple transactions?","metadata":{}},{"cell_type":"code","source":"vname = 'logerror_wc_0.01'\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nax = ax.ravel()\ntitles = ['Multiple vs Unique Transactions of 2016 '+vname,'Multiple vs Unique Transactions of 2017'+vname]\nfor i, y in enumerate(ys):\n    cnt = y.groupby('parcelid')['transactiondate'].count()\n    idx = cnt[cnt>1].index\n    f = y[[vname]]\n    f1 = y.loc[y.parcelid.isin(idx),vname]\n    f2 = y.loc[~y.parcelid.isin(idx),vname]  \n    ax[i].hist(f, bins=100, histtype='bar', density=True, ec='red', alpha=0.5)\n    f_axis = f.copy()\n    f_axis.sort_values(vname,inplace=True)\n    ax[i].plot(f_axis,stats.norm.pdf(f_axis,np.mean(f1),np.std(f1)).ravel(), c='blue',label='multiple transactions')\n    ax[i].plot(f_axis,stats.norm.pdf(f_axis,np.mean(f2),np.std(f2)).ravel(), c='green',label='unique transaction')    \n    ax[i].set_title(titles[i])\n    ax[i].legend()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:26.503902Z","iopub.execute_input":"2021-11-28T10:09:26.504847Z","iopub.status.idle":"2021-11-28T10:09:27.832604Z","shell.execute_reply.started":"2021-11-28T10:09:26.504795Z","shell.execute_reply":"2021-11-28T10:09:27.831682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n## Features","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n### Catgorical Features\n* Filter by Missing Rate: three categorical variables with many unique values, drop ['propertyzoningdesc','censustractandblock','rawcensustractandblock']\n* Distribution: class imbalance, drop ['pooltypeid10','pooltypeid2','fireplaceflag','taxdelinquencyflag']\n* Encode categorical variables with One Hot Encoder. \n<!-- * Relationship with Target: scatter plot of group median\n# * Relationship with each other: correlation, hierarchical clustering (gower taking too long), remove highly correlated ones. \n# * Feature Importance: RF\n#  -->","metadata":{}},{"cell_type":"code","source":"col_categorical = col_str + [c for c in X_2016_clean.columns if 'typeid' in c or 'region' in c] +\\\n['fips','rawcensustractandblock','censustractandblock']\ncol_rest = list(set(X_2016_clean.columns) - set(col_categorical))\ncol_rest = [c for c in col_rest if data_summary.loc[c,'unique_cnt']==1]\ncol_categorical = col_categorical + col_rest\nprint(f'{len(col_categorical)} categorical featrues identified')\ncol_categorical_filtered = [c for c in col_categorical if data_summary.loc[c,'missing_rate']<=0.5]\nprint(f'{len(col_categorical_filtered)} categorical features after filtering out high missing rate.')\nl1 = X_2016_clean.shape[1] \ndata_summary.loc[col_categorical_filtered,:].sort_values('unique_cnt')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:27.833875Z","iopub.execute_input":"2021-11-28T10:09:27.834133Z","iopub.status.idle":"2021-11-28T10:09:27.858466Z","shell.execute_reply.started":"2021-11-28T10:09:27.834084Z","shell.execute_reply":"2021-11-28T10:09:27.857652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.1.1\"></a>\n#### Distribution\n* Four variables have very high class imabalance or only one value for all observations, should be dropped: pooltypeid10,pooltypeid2,fireplaceflag,taxdelinquencyflag","metadata":{}},{"cell_type":"code","source":"#Frequency chart\ncol_drop = ['propertyzoningdesc','censustractandblock','rawcensustractandblock']\ncol_categorical_filtered = [c for c in col_categorical_filtered if c not in col_drop]\nprint(f'{len(col_categorical_filtered)} categorical features after filtering out features with too many unique values.')\nfig, ax =plt.subplots(3,5, figsize=(30,15))\nax = ax.ravel()\nfor i, c in enumerate(col_categorical_filtered):\n    print(f\"{c} start.\")\n    df_plt = X_2016_clean.loc[X_2016_clean[c].notnull(),:]\n    df_br= df_plt.groupby(c)['parcelid'].count()\n    bri = np.arange(df_br.shape[0])\n    ax[i].bar(bri, df_br.values)\n    if data_summary.loc[c,'unique_cnt']<100:\n        ax[i].set_xticks(bri)\n        ax[i].set_xticklabels(df_br.index,rotation=45)\n        ax[i].set_title(c)\n#     df_plt = X_2016_clean.loc[X_2016_clean[c].notnull(),:]\n#     ax[i].hist(df_plt[c])\n#     ax[i].set_title(c)\ncol_drop =  ['pooltypeid10','pooltypeid2','fireplaceflag','taxdelinquencyflag']\ncol_categorical_filtered = [c for c in col_categorical_filtered if c not in col_drop]\nprint(f'{len(col_categorical_filtered)} categorical features after filtering out features with highly imbalanced class distrubtion.')\ndel df_plt, df_br","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:27.859987Z","iopub.execute_input":"2021-11-28T10:09:27.860463Z","iopub.status.idle":"2021-11-28T10:09:43.429146Z","shell.execute_reply.started":"2021-11-28T10:09:27.860413Z","shell.execute_reply":"2021-11-28T10:09:43.428221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- <a id=\"3.1.2\"></a>\n#### Correlation with Y -->","metadata":{}},{"cell_type":"code","source":"# import category_encoders as ce\n\n# c = 'propertycountylandusecode'\n\n# # dummy_i = pd.get_dummies(X_2016_clean[c])\n# encoder=ce.HashingEncoder(cols=[c],n_components=100)\n# dummy_i = encoder.fit_transform(X_2016_clean.iloc[1:500000])","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:43.430506Z","iopub.execute_input":"2021-11-28T10:09:43.430784Z","iopub.status.idle":"2021-11-28T10:09:43.435372Z","shell.execute_reply.started":"2021-11-28T10:09:43.430749Z","shell.execute_reply":"2021-11-28T10:09:43.434186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# c = 'propertycountylandusecode'\n# encoder=ce.HashingEncoder(cols=c,n_components=50)\n# dummy_i = encoder.fit_transform(X_2016_clean)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:43.436752Z","iopub.execute_input":"2021-11-28T10:09:43.43711Z","iopub.status.idle":"2021-11-28T10:09:43.448367Z","shell.execute_reply.started":"2021-11-28T10:09:43.437073Z","shell.execute_reply":"2021-11-28T10:09:43.447591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummies = []\nfor c in col_categorical_filtered:\n    cnt = X_2016_clean[c].nunique()\n    if cnt <100:\n        print(f\"dummy for {c} created.\")\n        dummy_i = pd.get_dummies(X_2016_clean[c], prefix=f'dummy_{c}')\n        dummy_i = dummy_i.iloc[:,:-1]\n        dummies.append(dummy_i)\ndummies = pd.concat(dummies, axis=1)\nX_2016_clean = pd.concat([X_2016_clean,dummies], axis=1)\nl2 = X_2016_clean.shape[1]\nprint(f\"{l2-l1} categorical dummies created.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:43.449868Z","iopub.execute_input":"2021-11-28T10:09:43.45036Z","iopub.status.idle":"2021-11-28T10:09:45.099701Z","shell.execute_reply.started":"2021-11-28T10:09:43.450318Z","shell.execute_reply":"2021-11-28T10:09:45.099001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax =plt.subplots(4,5, figsize=(30,20))\n# ax = ax.ravel()\n\n\n# for i, c in enumerate(col_categorical_filtered):\n#     print(f\"{c} start.\")\n#     df_plt = train_data.loc[train_data[c].notnull(),:]\n#     df_br= df_plt.groupby(c)['logerror_wc_0.01'].median()\n#     bri = np.arange(df_br.shape[0])\n#     ax[i].scatter(bri, df_br.values)\n#     if data_summary.loc[c,'unique_cnt']<100:\n#         ax[i].set_xticks(bri)\n#         ax[i].set_xticklabels(df_br.index,rotation=45)\n#     ax[i].set_title(c)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:45.100579Z","iopub.execute_input":"2021-11-28T10:09:45.100816Z","iopub.status.idle":"2021-11-28T10:09:45.107635Z","shell.execute_reply.started":"2021-11-28T10:09:45.100789Z","shell.execute_reply":"2021-11-28T10:09:45.106483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- <a id=\"3.1.3\"></a>\n#### Correlation within X -->","metadata":{}},{"cell_type":"code","source":"# col_categorical_filtered = [c for c in col_categorical_filtered if c not in ['pooltypeid2','pooltypeid7','pooltypeid10','fireplaceflag','taxdelinquencyflag']]\n# print(f\"Drop uninformative features. Left with {len(col_categorical_filtered)} categorical features.\")\n# fig, ax = plt.subplots(figsize=(15, 15)) \n# mask = np.zeros_like(train_data[col_categorical_filtered].corr())\n# mask[np.triu_indices_from(mask)] = 1\n# sns.heatmap(train_data[col_categorical_filtered].corr(method='spearman'), mask= mask, ax= ax, annot= True)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:45.109296Z","iopub.execute_input":"2021-11-28T10:09:45.109516Z","iopub.status.idle":"2021-11-28T10:09:45.118578Z","shell.execute_reply.started":"2021-11-28T10:09:45.109487Z","shell.execute_reply":"2021-11-28T10:09:45.117843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n### Numeric Features\n\n* Filter by Missing Rate\n* Distribution: histogram, summary table: outlier, needs winsorize\n* Relationship with Target: time series correlation stable.\n* Relationship with each other: correlation, remove highly correlated ones\n* Feature Importance: RF\n","metadata":{}},{"cell_type":"code","source":"col_dummy = [c for c in X_2016_clean if 'dummy' in c]\ncol_numeric = list(set(X_2016_clean.columns) - set(col_categorical+col_dummy))\n\nprint(f'{len(col_numeric)} numeric featrues identified')\ncol_numeric_filtered = [c for c in col_numeric if data_summary.loc[c,'missing_rate']<0.5]\nprint(f'{len(col_numeric_filtered)} numeric features after filtering out high missing rate.')\ndata_summary.loc[col_numeric,:]","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:11:39.617957Z","iopub.execute_input":"2021-11-28T10:11:39.618579Z","iopub.status.idle":"2021-11-28T10:11:39.643799Z","shell.execute_reply.started":"2021-11-28T10:11:39.618527Z","shell.execute_reply":"2021-11-28T10:11:39.642933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2.1\"></a>\n#### Distribution\n* Many distributions indicate outliers. Some might need winsorization depending on the model we use. These include: unitcnt, finishedsquarefeet2, structuretaxvaluedollarcnt, landtaxvaluedollarcnt, calculatedfinishedsquarefeet, taxamount,landtaxvaluedollarcnt,lotsizesquarefeet, taxvaluedollarcnt, roomcnt.","metadata":{}},{"cell_type":"code","source":"#Histogram in full sample\nfig, ax =plt.subplots(4,6, figsize=(30,20))\nax = ax.ravel()\nfor i, c in enumerate(col_numeric_filtered):\n#     print(f\"{c} start.\")\n    df_plt = X_2016_clean.loc[X_2016_clean[c].notnull(),:]\n    ax[i].hist(df_plt[c])\n    ax[i].set_title(c)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:11:46.24187Z","iopub.execute_input":"2021-11-28T10:11:46.242377Z","iopub.status.idle":"2021-11-28T10:12:14.444382Z","shell.execute_reply.started":"2021-11-28T10:11:46.242323Z","shell.execute_reply":"2021-11-28T10:12:14.443477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_summary = X_2016_clean[col_numeric_filtered].describe()\ndf_summary.T","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:12:14.446287Z","iopub.execute_input":"2021-11-28T10:12:14.447075Z","iopub.status.idle":"2021-11-28T10:12:16.967094Z","shell.execute_reply.started":"2021-11-28T10:12:14.447025Z","shell.execute_reply":"2021-11-28T10:12:16.966409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2.2\"></a>\n#### Correlation with Y","metadata":{}},{"cell_type":"code","source":"# correlation time series with y\ndate_col = 'transactiondate'\ntrain_data['date_'] = train_data[date_col].apply(lambda x: pd.Timestamp(x))\n# train_data['ym'] = train_data['date_'].apply(lambda x:x.year*100+x.month)\ntrain_data = winsorize(train_data,'date_','logerror',[0.01,0.99])\ncorr1 = train_data.groupby('ym').corr(method='spearman').reset_index()\ncorr1 = corr1.loc[corr1.level_1=='logerror_wc_0.01',['ym']+col_numeric_filtered]\ncorr1['date_'] = corr1['ym'].apply(lambda x: pd.Timestamp(f\"{str(x)[0:4]}-{str(x)[4:6]}-01\"))\nfig, ax =plt.subplots(4,6, figsize=(30,20))\nax = ax.ravel()\n\nfor i, c in enumerate(col_numeric_filtered):\n    ax[i].plot(corr1.date_, corr1[c].values)\n    ax[i].set_title(c)\n    ax[i].set_ylim([-0.1,0.1])\n    ax[i].axhline(y=0,color='r',linestyle='--')","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:15:48.675942Z","iopub.execute_input":"2021-11-28T10:15:48.676315Z","iopub.status.idle":"2021-11-28T10:16:05.700453Z","shell.execute_reply.started":"2021-11-28T10:15:48.676274Z","shell.execute_reply":"2021-11-28T10:16:05.699599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2.3\"></a>\n#### Correlation within X","metadata":{}},{"cell_type":"code","source":"#correlation with each other\ncol_numeric_filtered = [c for c in col_numeric_filtered if c!='assessmentyear']\nfig, ax = plt.subplots(figsize=(15, 15)) \nmask = np.zeros_like(train_data[col_numeric_filtered].corr())\nmask[np.triu_indices_from(mask)] = 1\nsns.heatmap(train_data[col_numeric_filtered].corr(method='spearman'), mask= mask, ax= ax, annot= True)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:16:10.542524Z","iopub.execute_input":"2021-11-28T10:16:10.542888Z","iopub.status.idle":"2021-11-28T10:16:15.676443Z","shell.execute_reply.started":"2021-11-28T10:16:10.542853Z","shell.execute_reply":"2021-11-28T10:16:15.675601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #clustering of features\n# X = train_data[col_numeric_filtered]\n# # setting distance_threshold=0 ensures we compute the full tree.\n# model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n\n# model = model.fit(X)\n# plt.title(\"Hierarchical Clustering Dendrogram: Numerical Features\")\n# # plot the top three levels of the dendrogram\n# plot_dendrogram(model, truncate_mode=\"level\", p=3)\n# plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:09:45.159542Z","iopub.status.idle":"2021-11-28T10:09:45.159872Z","shell.execute_reply.started":"2021-11-28T10:09:45.159704Z","shell.execute_reply":"2021-11-28T10:09:45.159722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2.4\"></a>\n#### Feature Importance via RF","metadata":{}},{"cell_type":"code","source":"vname = 'logerror_wc_0.01'\ntrain_data_temp = train_data[col_numeric_filtered+[vname]]\ntrain_data_filtered = train_data_temp.dropna(how='any', axis=0)\nprint(f\"data size reduced from {train_data.shape[0]} to {train_data_filtered.shape[0]}\")\nX = train_data_filtered[col_numeric_filtered]\nY = train_data_filtered['logerror_wc_0.01']\nrf = RandomForestRegressor(max_depth=8)\nrf.fit(X, Y)\ny_pred = rf.predict(X)\nprint(\"Features sorted by their score:\")\ndf_importance = pd.DataFrame(rf.feature_importances_,index=col_numeric_filtered,columns=['importance'])\nax = df_importance.sort_values('importance').plot(kind='barh')\nax.set_title('Feature Selection vis RF for Numeric Variables')\n# print(f'oob score is {rf.oob_score_}')\nprint(f\"In sample MAE: {mean_absolute_error(Y, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:16:22.016306Z","iopub.execute_input":"2021-11-28T10:16:22.016844Z","iopub.status.idle":"2021-11-28T10:16:49.251165Z","shell.execute_reply.started":"2021-11-28T10:16:22.016797Z","shell.execute_reply":"2021-11-28T10:16:49.249729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #winsorize\n# def winsorize(df,date_col,data_col,limit=[0.01,0.99]):\n#     df['ym'] = df[date_col].apply(lambda x: x.year*100 + x.month)\n#     df[data_col+'_wc_'+str(limit[0])] = df.groupby('ym')[data_col].transform(lambda x: x.clip(*x.quantile(limit)))\n#     df[data_col+'_w_'+str(limit[0])] = df[data_col].transform(lambda x: x.clip(*x.quantile(limit)))\n#     return df\n# train_data['date_'] = train_data[date_col].apply(lambda x: pd.Timestamp(x))\n# train_data = winsorize(train_data,'date_','logerror',[0.01,0.99])\n\n# col1 = list(set(list(df_impute.index.values)) - \n# set([c for c in df_impute.index if 'garage' in c]+\n#     ['regionidneighborhood','numberofstories','unitcnt','airconditioningtypeid','buildingqualitytypeid','heatingorsystemtypeid'])) \n# X_2017_clean[col1] = X_2017_clean[col1].fillna(value=0)\n# #fill pool id and area with 0 if poolcnt = 0\n# col1 = ['pooltypeid2','pooltypeid7','pooltypeid10','poolsizesum']\n# X_2017_clean.loc[X_2017_clean.poolcnt==0,col1] = 0\n\n# train_data_2017 = pd.merge(y_train_2017, X_2017_clean, on=['parcelid'],how='left')\n# train_data_2017_tmp = train_data_2017[col_numeric_filtered+col_categorical_filtered+['logerror']]\n# train_data_2017_filtered = train_data_2017_tmp.dropna(how='any', axis=0)\n# y_test = train_data_2017_filtered['logerror']\n# X_test = train_data_2017_filtered[col_numeric_filtered+col_categorical_filtered]\ntrain_data = pd.merge(y_train_2016, X_2016_clean, on=['parcelid'],how='left')\nvname = 'logerror_wc_0.01'\ncol_keep = col_numeric_filtered+[vname]\ncol_dummy = [c for c in train_data.columns if \"dummy_\" in c]\ncolnames = col_keep + col_dummy\ntrain_data_temp = train_data[colnames]\ntrain_data_temp = train_data_temp.fillna(-999)\ntrain_data_filtered = train_data_temp.dropna(how='any', axis=0)\nprint(f\"data size reduced from {train_data.shape[0]} to {train_data_filtered.shape[0]}\")\nX = train_data_filtered[col_numeric_filtered+col_dummy]\nY = train_data_filtered[vname]\nrf = RandomForestRegressor(max_depth=8)\nrf.fit(X, Y)\ny_pred = rf.predict(X)\nprint(\"Features sorted by their score:\")\ndf_importance = pd.DataFrame(rf.feature_importances_,index=col_numeric_filtered+col_dummy,columns=['importance'])\nax = df_importance.sort_values('importance').plot(kind='barh',figsize=(10,10))\nax.set_title('Feature Selection vis RF for All Variables')\n# print(f'oob score is {rf.oob_score_}')\nprint(f\"Initial MAE: {mean_absolute_error(Y, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-28T10:23:03.34236Z","iopub.execute_input":"2021-11-28T10:23:03.342676Z","iopub.status.idle":"2021-11-28T10:24:28.340943Z","shell.execute_reply.started":"2021-11-28T10:23:03.342635Z","shell.execute_reply":"2021-11-28T10:24:28.340024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n## New Features\n\n* Interaction of important features: location interaction with volume, location interaction with area\n* Higher order ones\n* Seasonal dummies: seasonal volumes\n* External data: AHS survey: https://www.census.gov/programs-surveys/ahs.html\n* Expanding location related features\n* Include macroeconomic features: mortgage rates","metadata":{}},{"cell_type":"markdown","source":"## Tasks\n* Clean up steps: further imputation of missing values, rename variables? - DM, code up data_clean function - XH\n* External data: Zip code, Census data, AHS data - SC, other external datasets - DM, XH \n* Model design: stage 2 discussion","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}