{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python","file_extension":".py","name":"python","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"collapsed":true,"_uuid":"ba200d60fada725fe30f949cdfc60484b5e01963","_cell_guid":"a9a81e69-0c62-46f7-a2b3-da2dcb2cea16"},"source":"#import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\ntrain_df = pd.read_csv(\"input/train_2016.csv\", parse_dates=[\"transactiondate\"])\ntrain_df.shape\ntrain_df.head()\nplt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.logerror.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('logerror', fontsize=12)\nplt.show()\n\n#for missing values\n\nprop_df = pd.read_csv(\"input/properties_2016.csv\")\nprop_df.shape\nprop_df.head()\nmissing_df = prop_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missing_df.missing_count.values, color='blue')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()\n#to merge both tables\ntrain_df = pd.merge(train_df, prop_df, on='parcelid', how='left')\ntrain_df.head()\n#find data type of var i.g text,float,int \npd.options.display.max_rows = 65\n\ndtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df\n#not understand\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()\n#find missing in new data set\nmissing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df['missing_ratio'] = missing_df['missing_count'] / train_df.shape[0]\nmissing_df.loc[missing_df['missing_ratio']>0.999]\n# Let us just impute the missing values with mean values to compute correlation coefficients #\nmean_values = train_df.mean(axis=0)\ntrain_df_new = train_df.fillna(mean_values, inplace=True)\n\n# Now let us look at the correlation coefficient of each of these variables #\nx_cols = [col for col in train_df_new.columns if col not in ['logerror'] if train_df_new[col].dtype=='float64']\n\nlabels = []\nvalues = []\nfor col in x_cols:\n    labels.append(col)\n    values.append(np.corrcoef(train_df_new[col].values, train_df_new.logerror.values)[0,1])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,40))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\n#autolabel(rects)\nplt.show()\n#here are few variables at the top of this graph without any correlation values. I guess they have only one unique value and hence no correlation value. Let us confirm the same.\ncorr_zero_cols = ['assessmentyear', 'storytypeid', 'pooltypeid2', 'pooltypeid7', 'pooltypeid10', 'poolcnt', 'decktypeid', 'buildingclasstypeid']\nfor col in corr_zero_cols:\n    print(col, len(train_df_new[col].unique()))\n#Let us take the variables with high correlation values and then do some analysis on them.\ncorr_df_sel = corr_df.ix[(corr_df['corr_values']>0.02) | (corr_df['corr_values'] < -0.01)]\ncorr_df_sel\n#\ncols_to_use = corr_df_sel.col_labels.tolist()\n\ntemp_df = train_df[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()    This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"execution_count":null,"cell_type":"code"}]}