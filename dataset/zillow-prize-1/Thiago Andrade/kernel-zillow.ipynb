{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Compilação de resultados das predições submetidas\n\n### XGBoost: \nBaseline:<br/>\n    Private Score: 0.12437<br/>\n    Public Score: 0.11853<br/><br/>\nGridsearch:\n1. Version 1\n    * Private Score: 0.11582\n    * Public Score: 0.10853\n2. Version 2\n    * Private Score: 0.09201\n    * Public Score: 0.08213\n\n### Random Forest:\nBaseline:<br/>\n    Private Score: 0.15999<br/>\n    Public Score: 0.15315<br/><br/>\nGridsearch:\n1. Version 1\n    * Private Score: 0.14568\n    * Public Score: 0.14060\n\n### Linear Regressor:\nBaseline:<br/>\nPrivate Score: 1.36184<br/>\nPublic Score: 1.11183"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variável para indicar para qual modelo se deseja realizar grid search.\n# Por limitações do kernel do kaggle, fazer o grid search apenas de um modelo por vez.\n#    \"RFR\"      -> Random Forest Regressor grid search\n#    \"XGB\"      -> XGBoost Regressor grid search\n#    \"TPOT\"     -> TPOT Regressor\n#    \"MLPR\"     -> MLP Regressor grid search\n#    \"Default\"  -> Executar os modelos com os parâmetros padrão (sem grid search)\n#    \"Best\"     -> Executar os melhores modelos de acordo com grid search anteriores\ngrid_search = \"Best\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport warnings\nimport gc\nimport xgboost\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport datetime as dt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ShuffleSplit, train_test_split\nfrom sklearn.metrics import make_scorer, mean_absolute_error, r2_score\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.svm import LinearSVR\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Carregando Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"properties2016 = pd.read_csv('../input/properties_2016.csv', low_memory = False)\nproperties2017 = pd.read_csv('../input/properties_2017.csv', low_memory = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def changeToFloat32(properties):\n    for c, dtype in zip(properties.columns, properties.dtypes):\n        if dtype == np.float64:\n            properties[c] = properties[c].astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"changeToFloat32(properties2016)\nchangeToFloat32(properties2017)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2016 = pd.read_csv('../input/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\ntrain2017 = pd.read_csv('../input/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/sample_submission.csv', low_memory = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Análise de dados"},{"metadata":{},"cell_type":"markdown","source":"## Função para adicionar features envolvendo datas"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_date_features(df):\n    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transaction_month\"] = (df[\"transactiondate\"].dt.year - 2016)*12 + df[\"transactiondate\"].dt.month\n    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n    df[\"transaction_quarter\"] = (df[\"transactiondate\"].dt.year - 2016)*4 +df[\"transactiondate\"].dt.quarter\n    df.drop([\"transactiondate\"], inplace=True, axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removendo colunas que não agregam informações novas"},{"metadata":{"trusted":true},"cell_type":"code","source":"dropcols = ['finishedsquarefeet12', 'finishedsquarefeet13'\\\n            ,'finishedsquarefeet15', 'finishedsquarefeet6'\\\n            ,'finishedsquarefeet50', 'fullbathcnt', 'calculatedbathnbr']\n\nproperties2016.drop(dropcols, axis=1, inplace=True)\nproperties2017.drop(dropcols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configurando dataframes de treinamento e teste"},{"metadata":{"trusted":true},"cell_type":"code","source":"train2016 = add_date_features(train2016)\ntrain2017 = add_date_features(train2017)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fazendo o merge dos dados das propriedades com os dados de treinamento"},{"metadata":{"trusted":true},"cell_type":"code","source":"train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\ntrain2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2016.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2017.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Necessário anular os valores das colunas de 'tax' de acordo com as regras da competição\ntrain2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concatenando propriedades de 2016 e 2017"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.concat([train2016, train2017], axis = 0)\ntest_df = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del properties2016, properties2017, train2016, train2017\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tratamento das bases de treinamento e teste"},{"metadata":{},"cell_type":"markdown","source":"### Criando features nos conjuntos de treinamento e teste"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_new_features(df):\n    df['AREA_UTIL_IMOVEL'] = df['calculatedfinishedsquarefeet'] / df['lotsizesquarefeet']\n    df['AREA_QUINTAL'] = df['lotsizesquarefeet'] - df['calculatedfinishedsquarefeet']\n    df['TOTAL_COMODOS'] = df['bathroomcnt'] + df['bedroomcnt']\n    \n    import datetime\n    now = datetime.datetime.now()\n    df['IDADE_IMOVEL'] = now.year - df['yearbuilt']\n    \n    df['TOTAL_IMPOSTOS'] = df['taxvaluedollarcnt'] + df['taxamount']\n    \n    #Taxa de impostos total por taxas de uma pesquisa\n    df['TAXA_IMPOSTOS'] = df['taxvaluedollarcnt'] / df['taxamount']\n       \n    #Quantidade de imoveis por municipio\n    county_count = df['regionidcounty'].value_counts().to_dict()\n    df['IMOVEIS_MUNICIPIO'] = df['regionidcounty'].map(county_count)\n\n    #Quantidade de imoveis por cidade\n    city_count = df['regionidcity'].value_counts().to_dict()\n    df['IMOVEIS_CIDADE'] = df['regionidcity'].map(city_count)\n    \n    df['latitude'] = df['latitude'] / 1e7\n    df['longitude'] = df['longitude'] / 1e7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_new_features(train_df)\nadd_new_features(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convertendo colunas categóricas para numéricas"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_to_code(df):\n    object_type = df.select_dtypes(include=['object']).columns.values\n    df[object_type] = df[object_type].astype('category')\n    for column in object_type:\n        df[column] = df[column].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_to_code(train_df)\ncat_to_code(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tratamento do conjunto de testes"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['transactiondate'] = pd.Timestamp('2016-12-01') \ntest_df = add_date_features(test_df)\ntest_df.fillna(-999, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remoção de dados ausentes"},{"metadata":{},"cell_type":"markdown","source":"### Remover colunas com mais de 70% de dados ausentes"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dropna(thresh=0.70*len(train_df), axis=1, inplace=True)\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remover colunas com valores únicos"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[:, (train_df != train_df.iloc[0]).any()].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removendo linhas com valores NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataframe resultante"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ajustando os datasets para a execução dos modelos de aprendizagem"},{"metadata":{},"cell_type":"markdown","source":"## Separando train_df em conjuntos de treinamento e holdout para visualização de resultados"},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_train, x_test, y_train, y_test = train_test_split(train_df.drop(['logerror', 'parcelid'], axis=1), train_df.logerror, test_size=1/3, random_state=42)#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"x_train: {}, x_test: {}, y_train: {}, y_test: {}\".format(x_train.shape, x_test.shape, y_train.shape, y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Definindo funções para execução dos modelos "},{"metadata":{},"cell_type":"markdown","source":"## Definindo Regressor Linear que servirá como baseline para os demais modelos"},{"metadata":{"trusted":true},"cell_type":"code","source":"def linearRegressor(X_train, Y_train, X_test, Y_test):\n  regressor = LinearRegression(fit_intercept=True)\n\n  model = regressor.fit(X_train, Y_train)\n\n  pred_LR = regressor.predict(X_test)\n  \n  resultados(Y_test, pred_LR, \"Linear Regressor\")\n\n  # Returns the trained model\n  return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Definindo Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"def randomForestRegressor(X_train, Y_train, X_test, Y_test):\n  # Gerar conjuntos de validação-cruzada para o treinamento de dados\n  cv_sets = ShuffleSplit(n_splits=5, test_size = 0.25)\n\n  rfr =  RandomForestRegressor(random_state=42)\n\n  # Numero de arvores no random forest\n  n_estimators = [int(x) for x in np.linspace(start = 180, stop = 220, num = 11)]\n  # Numero de features para considerar a cada separacao\n  max_features = ['auto', 'sqrt']\n  # Profundidade maxima da arvore\n  max_depth = [int(x) for x in np.linspace(20, 30, num = 6)]\n  # Quantidade minima de amostras para se separar um no\n  min_samples_split = list(range(4,8))\n  # Quantidade minima de amostras requeridas em cada no folha\n  min_samples_leaf = list(range(1,4))\n\n  # Grid search anterior\n  #n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n  #max_features = ['auto', 'sqrt']\n  #max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n  #min_samples_split = [2, 5, 10, 15, 100]\n  #min_samples_leaf = [1, 2, 5, 10]\n\n  # Create the random grid\n  params = {'n_estimators': n_estimators,\n                 'max_features': max_features,\n                 'max_depth': max_depth,\n                 'min_samples_split': min_samples_split,\n                 'min_samples_leaf': min_samples_leaf}\n\n  #Transformar 'performance_metric' em uma função de pontuação utilizando 'make_scorer' \n  scoring_fnc = make_scorer(mean_absolute_error)\n\n  # Gerar o objeto de busca em matriz\n  grid = RandomizedSearchCV(estimator = rfr, param_distributions = params, scoring=scoring_fnc, \n                            cv=cv_sets, n_iter=100, verbose=2)\n\n  # Ajustar o objeto de busca em matriz com os dados para calcular o modelo ótimo\n  grid = grid.fit(X_train, Y_train)\n\n  # Escolher o melhor estimador para predizer os dados de teste\n  best_rfr = grid.best_estimator_\n\n  print(\"Best Estimator: \\n{}\\n\".format(grid.best_estimator_))\n  print(\"Best Parameters: \\n{}\\n\".format(grid.best_params_))\n\n  pred_RFR = best_rfr.predict(X_test)\n  \n  resultados(Y_test, pred_RFR, \"Random Forest Regressor\")\n   \n  # Returns the best trained model\n  return best_rfr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Definindo XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb(X_train, Y_train, X_test, Y_test):\n  \n  cv_sets = ShuffleSplit(n_splits=5, test_size = 0.25)\n  \n  xgb = XGBRegressor()\n\n  # xgboost params\n  params = {\n              'objective':['reg:linear'],\n              'learning_rate': [.03, .033, .035, .037],\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [2, 4, 10],\n              'silent': [1],\n              'subsample': [0.6, 0.7, 0.8],\n              'colsample_bytree': [0.6, 0.7, 0.8],\n              'n_estimators': [500],\n              'base_score': [np.mean(Y_train), 0.5],\n              'reg_lambda': [0.3, 0.5, 0.8],\n              'eval_metric': ['mae', 'logloss']\n              \n  }\n\n  # Gerar o objeto de busca em matriz\n  grid = RandomizedSearchCV(estimator = xgb, param_distributions = params, scoring='neg_mean_squared_error', \n                            cv=cv_sets, n_iter=100, verbose=2)\n\n  grid = grid.fit(X_train, Y_train)\n\n  # Escolher o melhor estimador para predizer os dados de teste\n  best_xgb = grid.best_estimator_\n\n  print(\"Best Estimator: \\n{}\\n\".format(grid.best_estimator_))\n  print(\"Best Parameters: \\n{}\\n\".format(grid.best_params_))\n\n  pred_xgb = best_xgb.predict(X_test)\n  \n  resultados(Y_test, pred_xgb, \"XGBoost Regressor\")\n\n  # Returns the trained model\n  return best_xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Definindo TPOTRegressor\n[API refference](https://epistasislab.github.io/tpot/api/), [Using TPOT](https://epistasislab.github.io/tpot/using/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tpotRegressor(X_train, Y_train, X_test, Y_test):\n    from tpot import TPOTRegressor\n\n    tpot = TPOTRegressor(generations=10, population_size=100, scoring='neg_mean_absolute_error', verbosity=2)\n    tpot.fit(X_train, Y_train)\n    print(tpot.score(X_test, Y_test))\n    tpot.export('tpot_zillow_pipeline.py')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Função para exibir resultados no conjunto de treinamento"},{"metadata":{"trusted":true},"cell_type":"code","source":"def resultados(Y_test, predictions, model_name):\n\n    #calculando o erro de uma árvore de decisão para regressão:\n    mae_RFR = mean_absolute_error(predictions, Y_test)\n    print (\"Erro médio absoluto: {}\".format(mae_RFR))\n\n    #Acurácia do modelo\n    #r2_RFR = r2_score(predictions, Y_test)\n    #print (\"Índice R² (score): {}\".format(r2_RFR))\n    \n    sns.set(style=\"whitegrid\")\n    fig, axs = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n    sns.residplot(predictions, Y_test, color=\"g\", ax=axs[0]).set_title(\"Residuals plot of \" + model_name)\n    sns.scatterplot(x=Y_test, y=predictions, ax=axs[1]).set_title(\"Model Error\")\n    axs[1].set(xlabel='True Values', ylabel='Predicted Values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Função para submeter (para o kaggle) as predições de um modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"def submitPredictions(predictions, model_name):\n    y_pred=[]\n\n    for i,predict in enumerate(predictions):\n        y_pred.append(str(round(predict,4)))\n    y_pred=np.array(y_pred)\n\n    output = pd.DataFrame({'ParcelId': test_df['ParcelId'].astype(np.int32),\n            '201610': y_pred, '201611': y_pred, '201612': y_pred,\n            '201710': y_pred, '201711': y_pred, '201712': y_pred})\n\n    cols = output.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    output = output[cols]\n\n    from datetime import datetime\n\n    print( \"\\nWriting results to disk ...\" )\n    output.to_csv('sub{}_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'), model_name), index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Melhor RFR de acordo com o Gridsearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"def submitBestModelRFR(X_train, Y_train, X_test, Y_test):\n\n    print(\"Using Best Random Forest Regressor Model...\")\n    \n    # Best model according to previous grid search\n    best_rfr = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n               max_features='auto', max_leaf_nodes=None,\n               min_impurity_decrease=0.0, min_impurity_split=None,\n               min_samples_leaf=1, min_samples_split=5,\n               min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n               oob_score=False, random_state=42, verbose=0, warm_start=False)\n    \n    # Visualizing model performance on training dataset\n    best_rfr.fit(X_train, Y_train)\n    # Results for the training dataset\n    predictions_best_rfr = best_rfr.predict(X_test)\n    resultados(Y_test, predictions_best_rfr, \"Best RFR\")\n\n    # Testing the model on test dataframe and submiting results\n    submitPredictions(best_rfr.predict(test_df[X_train.columns.values]), \"best_RFR\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Melhor XGBRegressor de acordo com o grid search"},{"metadata":{"trusted":true},"cell_type":"code","source":"def submitBestModelXGB(X_train, Y_train, X_test, Y_test):\n    \n    print(\"Using Best XGBoost Regressor Model...\")\n    \n    # Best model according to previous grid search\n    best_xgb = XGBRegressor(base_score=0.014415016651286966, booster='gbtree',\n       colsample_bylevel=1, colsample_bytree=0.6, eval_metric='logloss',\n       gamma=0, importance_type='gain', learning_rate=0.03,\n       max_delta_step=0, max_depth=5, min_child_weight=10, missing=None,\n       n_estimators=500, n_jobs=1, nthread=None, objective='reg:linear',\n       random_state=0, reg_alpha=0, reg_lambda=0.3, scale_pos_weight=1,\n       seed=None, silent=1, subsample=0.8)\n\n    #Previous XGB Parameters\n#     XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#            colsample_bytree=0.7, gamma=0, importance_type='gain',\n#            learning_rate=0.03, max_delta_step=0, max_depth=5,\n#            min_child_weight=4, missing=None, n_estimators=500, n_jobs=1,\n#            nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n#            reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,\n#            subsample=0.7)\n\n    # Visualizing model performance on training dataset\n    best_xgb.fit(X_train, Y_train)\n    # Results for the training dataset\n    predictions_best_xgb = best_xgb.predict(X_test)\n    resultados(Y_test, predictions_best_xgb, \"Best XGB\")\n\n    # Testing the model on test dataframe and submiting results\n    submitPredictions(best_xgb.predict(test_df[X_train.columns.values]), \"best_XGB\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Melhor TPOT Regressor de acordo com execução anterior"},{"metadata":{"trusted":true},"cell_type":"code","source":"def submitBestModelTPOT(X_train, Y_train, X_test, Y_test):\n    \n    print(\"Using Best TPOT Model (Linear SVR)...\")\n    \n    best_svr = LinearSVR(C=0.1, dual=True, epsilon=0.001, loss=\"squared_epsilon_insensitive\", tol=0.0001)\n    \n    # Visualizing model performance on training dataset\n    best_svr.fit(X_train, Y_train)\n    # Results for the training dataset\n    predictions_best_svr = best_svr.predict(X_test)\n    resultados(Y_test, predictions_best_svr, \"Best SVR\")\n\n    # Testing the model on test dataframe and submiting results\n    submitPredictions(best_svr.predict(test_df[X_train.columns.values]), \"best_SVR\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Definindo modelos XGB e RFR com seus parâmetros padrão"},{"metadata":{"trusted":true},"cell_type":"code","source":"def defaultXGB(X_train, Y_train, X_test, Y_test):\n    print (\"Using default XGB Regressor...\")\n    \n    default_xgb = XGBRegressor()\n    \n    # Visualizing model performance on training dataset\n    default_xgb.fit(X_train, Y_train)\n    # Results for the training dataset\n    predictions_default_xgb = default_xgb.predict(X_test)\n    resultados(Y_test, predictions_default_xgb, \"Default XGB\")\n\n    # Testing the model on test dataframe and submiting results\n    submitPredictions(default_xgb.predict(test_df[X_train.columns.values]), \"default_XGB\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def defaultRFR(X_train, Y_train, X_test, Y_test):\n    print (\"Using default RFR Regressor...\")\n    \n    default_rfr = RandomForestRegressor()\n        \n    # Visualizing model performance on training dataset\n    default_rfr.fit(X_train, Y_train)\n    # Results for the training dataset\n    predictions_default_rfr = default_rfr.predict(X_test)\n    resultados(Y_test, predictions_default_rfr, \"Default RFR\")\n\n    # Testing the model on test dataframe and submiting results\n    submitPredictions(default_rfr.predict(test_df[X_train.columns.values]), \"default_RFR\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encontrando os melhores modelos e submetendo suas predições no conjunto de teste"},{"metadata":{},"cell_type":"markdown","source":"*Devido as limitações do kaggle (tempo de execução limitado a 9h), para treinar os modelos usamos somente dados do município com mais imóveis*"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_county = train_df[train_df.propertycountylandusecode == train_df.propertycountylandusecode.value_counts().argmax()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_county.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(train_county.drop(['logerror', 'parcelid'], axis=1), train_county.logerror, test_size=1/3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"x_train: {}, x_test: {}, y_train: {}, y_test: {}\".format(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regressão Linear foi escolhida como baseline para os demais modelos."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Running Linear Regressor...\")\nmodel_lr = linearRegressor(X_train, Y_train, X_test, Y_test)\nsubmitPredictions(model_lr.predict(test_df[X_train.columns.values]), \"LR\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Executar um dos grid search definidos acima de acordo com o valor definido da variável"},{"metadata":{"trusted":true},"cell_type":"code","source":"if (grid_search == \"Default\"):\n    defaultXGB(X_train, Y_train, X_test, Y_test) \n    defaultRFR(X_train, Y_train, X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (grid_search == \"RFR\"):\n    print(\"Running Random Forest Regressor...\")\n    model_rfr = randomForestRegressor(X_train, Y_train, X_test, Y_test)\n    submitPredictions(model_rfr.predict(test_df[X_train.columns.values]), \"RFR\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (grid_search == \"XGB\"):\n    print(\"Running XGBoost...\")\n    model_xgb = xgb(X_train, Y_train, X_test, Y_test)\n    submitPredictions(model_xgb.predict(test_df[X_train.columns.values]), \"XGB\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (grid_search == \"TPOT\"):\n    tpotRegressor(X_train, Y_train, X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Caso não se deseje executar grid search, submeter os resultados dos melhores modelos"},{"metadata":{"trusted":true},"cell_type":"code","source":"if (grid_search == \"Best\"):\n    submitBestModelRFR(X_train, Y_train, X_test, Y_test)\n    submitBestModelXGB(X_train, Y_train, X_test, Y_test)\n    submitBestModelTPOT(X_train, Y_train, X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Retorno do TPOT\n'''\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVR\n\n# NOTE: Make sure that the class is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1).values\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'].values, random_state=None)\n\n# Average CV score on the training set was:-0.0678352369864653\nexported_pipeline = LinearSVR(C=0.1, dual=True, epsilon=1.0, loss=\"squared_epsilon_insensitive\", tol=0.0001)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os modelos foram avaliados de acordo com o score retornado pela submissão ao kaggle do conjunto de testes.\nApesar de o desempenho do modelo de Regressão Linear ser bom no conjunto de treinamento, quando utilizado o conjunto de testes ele apresentou o pior desempenho. \n"},{"metadata":{},"cell_type":"markdown","source":"## Regressor usando Redes Neurais"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submitNeuralNetwork(X_train, X_test, Y_train, Y_test):\n    \n    print(\"Using Neural Network ...\")\n    scaler = StandardScaler()\n    X_train_sc = scaler.fit_transform(X_train)\n    X_test_sc = scaler.transform(X_test)\n    \n    rn_regressor = MLPRegressor(hidden_layer_sizes = (350,200,120,),early_stopping = True)\n    rn_regressor.fit(X_train_sc, Y_train)\n    \n    predictions_rn = rn_regressor.predict(X_test_sc)\n    \n    # Visualizing model performance on training dataset\n    # Results for the training dataset\n    resultados(Y_test, predictions_rn, \"Neural Network\")\n\n    # Testing the model on test dataframe and submiting results\n    #submitPredictions(rn_regressor.predict(test_df[X_train.columns.values]), \"Neural_Network\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submitNeuralNetwork(X_train, X_test, Y_train, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid Search com Redes Neurais"},{"metadata":{"trusted":true},"cell_type":"code","source":"def NeuralNetwork_gs(X_train, X_test, Y_train, Y_test):\n    \n    cv_sets = ShuffleSplit(n_splits=5, test_size = 0.25)\n    \n    print(\"Using Neural Network with Grid Search...\")\n    scaler = StandardScaler()\n    X_train_sc = scaler.fit_transform(X_train)\n    X_test_sc = scaler.transform(X_test)\n\n    rn_regressor = MLPRegressor()\n\n    # neural network params\n    params = {\n        'hidden_layer_sizes':[(150,100,),(200,150,100,),(300,200,100)],\n        'activation':['relu'],\n        'solver':['sgd','adam'],\n        'learning_rate': ['constant','adaptive'],\n        'learning_rate_init':[.001],\n        'early_stopping':[True]\n    }\n\n    # Gerar o objeto de busca em matriz\n    grid = GridSearchCV(estimator = rn_regressor, param_grid = params, scoring='neg_mean_squared_error',cv=cv_sets, verbose=2,n_jobs = -1)\n\n    grid = grid.fit(X_train_sc, Y_train)\n\n    # Escolher o melhor estimador para predizer os dados de teste\n    best_model = grid.best_estimator_\n\n    print(\"Best Estimator: \\n{}\\n\".format(grid.best_estimator_))\n    print(\"Best Parameters: \\n{}\\n\".format(grid.best_params_))\n\n    pred_rn = best_model.predict(X_test_sc)\n  \n    resultados(Y_test, pred_rn, \"MLP Regressor\")\n\n    # Returns the trained model\n    return best_model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (grid_search == \"MLPR\"):\n    rn_model = NeuralNetwork_gs(X_train, X_test, Y_train, Y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}