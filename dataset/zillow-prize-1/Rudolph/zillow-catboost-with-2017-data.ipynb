{"cells":[{"metadata":{"_cell_guid":"ab7d7229-938a-406b-a02d-5ea41d13cf91","_uuid":"17a33ae07b3c42f8eb24c72970cb514e68365826"},"cell_type":"markdown","source":"This a single catboost prediction using the new 2017 data. My understanding is that we can replace the 2016 properties data with the 2017 properties data and combine the training data from 2017 and 2016 into a single train set to make prediction for the three months in 2017. The three months in 2016 should be now irrelevant and I use it just for sanity check that everything continues to work since it is still possible to use LB to test the 2016 predistion (the score is 0.06435 - improved slightly from the one based on 2016 data - perhaps because of some data leakage and because we have more data)."},{"metadata":{},"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport datetime as dt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import MiniBatchKMeans\nfrom catboost import CatBoostRegressor\n\nmyfolder = '../input/'\nprint('loading files...')\n\nprop = pd.read_csv(myfolder+'properties_2017.csv',low_memory=False)\nprop.rename(columns={'parcelid': 'ParcelId'}, inplace=True)   # make it the same as sample_submission\ntrain = pd.read_csv(myfolder+'train_2016_v2.csv')\ntrain.rename(columns={'parcelid': 'ParcelId'},inplace=True)\nsample = pd.read_csv(myfolder+'sample_submission.csv')\nprint(train.shape, prop.shape, sample.shape)\ntrain17 = pd.read_csv(myfolder+'train_2017.csv')\ntrain17.rename(columns={'parcelid': 'ParcelId'},inplace=True)\nprint(train17.shape)\ntrain=pd.concat([train,train17])\ndel train17\nprint(train.shape)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"print('preprocessing, fillna, dtypes ...')\n\nprop['longitude']=prop['longitude'].fillna(prop['longitude'].median()) / 1e6   #  convert to float32 later\nprop['latitude'].fillna(prop['latitude'].median()) / 1e6\nprop['censustractandblock'].fillna(prop['censustractandblock'].median()) / 1e12\ntrain = train[train['logerror'] <  train['logerror'].quantile(0.9975)]  # exclude 0.5% of outliers\ntrain = train[train['logerror'] >  train['logerror'].quantile(0.0025)]\n\nprint('qualitative ...')\nqualitative = [f for f in prop.columns if prop.dtypes[f] == object]\nprop[qualitative] = prop[qualitative].fillna('Missing')\nfor c in qualitative:  prop[c] = LabelEncoder().fit(list(prop[c].values)).transform(list(prop[c].values)).astype(int)\n\nprint('smallval ...')\nsmallval = [f for f in prop.columns if np.abs(prop[f].max())<100]\nprop[smallval] = prop[smallval].fillna('Missing')\nfor c in smallval:  prop[c] = LabelEncoder().fit(list(prop[c].values)).transform(list(prop[c].values)).astype(np.int8)\n\nprint('other ...')\nother=['regionidcounty','fips','propertycountylandusecode','propertyzoningdesc','propertylandusetypeid']\nprop[other] = prop[other].fillna('Missing')\nfor c in other:  prop[c] = LabelEncoder().fit(list(prop[c].values)).transform(list(prop[c].values)).astype(int)\n\nrandomyears=pd.Series(np.random.choice(prop['yearbuilt'].dropna().values,len(prop)))\nprop['yearbuilt']=prop['yearbuilt'].fillna(randomyears).astype(int)\nmed_yr=prop['yearbuilt'].quantile(0.5)\nprop['New']=prop['yearbuilt'].apply(lambda x: 1 if x > med_yr else 0).astype(np.int8)  # adding a new feature\n\nprop['unitcnt'] = prop['unitcnt'].fillna(1).astype(int)    \nprop['Condo']=prop['unitcnt'].apply(lambda x: 1 if x > 1 else 0).astype(np.int8)    # adding a new feature\n    \nfeat_to_drop=[ 'finishedsquarefeet50', 'finishedfloor1squarefeet', 'finishedsquarefeet15', \n              'finishedsquarefeet13','assessmentyear']\nprop.drop(feat_to_drop,axis=1,inplace=True)   # remove because too many missing or irrelevant\n\nprop['lotsizesquarefeet'].fillna(prop['lotsizesquarefeet'].quantile(0.001),inplace=True)\nprop['finishedsquarefeet12'].fillna(prop['finishedsquarefeet12'].quantile(0.001),inplace=True)\nprop['calculatedfinishedsquarefeet'].fillna(prop['finishedsquarefeet12'],inplace=True)\nprop['taxamount'].fillna(prop['taxamount'].quantile(0.001),inplace=True)\nprop['landtaxvaluedollarcnt'].fillna(prop['landtaxvaluedollarcnt'].quantile(0.001),inplace=True)\nprop.fillna(0,inplace=True)\n    \nprint('quantitative ...')   \nquantitative = [f for f in prop.columns if prop.dtypes[f] == np.float64]\nprop[quantitative] = prop[quantitative].astype(np.float32) \n\ncfeatures = list(prop.select_dtypes(include = ['int64', 'int32', 'uint8', 'int8']).columns)\nfor c in qualitative:  prop[c] = LabelEncoder().fit(list(prop[c].values)).transform(list(prop[c].values))\n\n# some quantitative features have a limited number of values (eg ZIP code)    \nfor c in ['rawcensustractandblock',  'regionidcity',  'regionidneighborhood',  'regionidzip',  'censustractandblock'] :\n    prop[c] = LabelEncoder().fit(list(prop[c].values)).transform(list(prop[c].values))\n\ngc.collect()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"print('create new features and the final dataframes ...')\n\n#replace latitudes and longitudes with 500 clusters  (similar to ZIP codes)\ncoords = np.vstack(prop[['latitude', 'longitude']].values)\nsample_ind = np.random.permutation(len(coords))[:1000000]\nkmeans = MiniBatchKMeans(n_clusters=500, batch_size=100000).fit(coords[sample_ind])\nprop['Cluster'] = kmeans.predict(prop[['latitude', 'longitude']])\n\nprop['Living_area_prop'] = prop['calculatedfinishedsquarefeet'] / prop['lotsizesquarefeet']\nprop['Value_ratio'] = prop['taxvaluedollarcnt'] / prop['taxamount']\nprop['Value_prop'] = prop['structuretaxvaluedollarcnt'] / prop['landtaxvaluedollarcnt']\nprop['Value_prop'].fillna(0,inplace=True)\nprop['Taxpersqrtfoot']=prop['taxamount'] / prop['finishedsquarefeet12']\n\ntrain['transactiondate'] = pd.to_datetime(train.transactiondate)\ntrain['Month'] = train['transactiondate'].dt.month.astype(np.int8)\ntrain['Day'] = train['transactiondate'].dt.day.astype(np.int8)\ntrain['Season'] = train['Month'].apply(lambda x: 1 if x in [1,2,9,10,11,12] else 0).astype(np.int8)\n\nmonth_err=(train.groupby('Month').aggregate({'logerror': lambda x: np.mean(x)})- train['logerror'].mean()).values\ntrain['Meanerror']=train['Month'].apply(lambda x: month_err[x-1]).astype(np.float32)\n\ntrain['abserror']=train['logerror'].abs()\nmonth_abs_err=(train.groupby('Month').aggregate({'abserror': lambda x: np.mean(x)})- train['abserror'].mean()).values\ntrain['Meanabserror']=train['Month'].apply(lambda x: month_abs_err[x-1]).astype(np.float32)\ntrain.drop(['abserror'], axis=1,inplace=True)\n\nfor c in ['Meanerror','Meanabserror']: train[c]=LabelEncoder().fit(list(train[c].values)).transform(list(train[c].values))\nfor c in ['Meanerror','Meanabserror']: train[c]=train[c].astype(np.int8)\n\nprint(prop.shape, train.shape)\ngc.collect()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"# define X,y that can be used either as the training set or be split into train and eval sets (combine prop and train)\n# and define X_test for the final prediction to submit (combine prop and sample)\n\nX = train.merge(prop, how='left', on='ParcelId')\ny = X['logerror']\nX.drop(['ParcelId', 'logerror', 'transactiondate'], axis=1,inplace=True)\n\nfeatures=list(X.columns)\ncfeatures = list(X.select_dtypes(include = ['int64', 'int32', 'uint8', 'int8']).columns)\n\nX_test = (sample.merge(prop, on='ParcelId', how='left')).loc[:,features]\nX_test['Season']=np.int8(1)\nX_test['Day']=np.int8(15)\nX_test['Month']=np.int8(10) \nX_test['Meanerror']=np.int8(10)\nX_test['Meanabserror']=np.int8(10)\n\nprint(X.shape, y.shape, X_test.shape)\ndel prop, train\ngc.collect()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"print('catboost training ...')\nX_train, X_eval, y_train, y_eval = train_test_split(X,y, test_size=0.15, random_state=1)\nmodel = CatBoostRegressor(iterations=1000,learning_rate=0.002, depth=7, loss_function='MAE', \n                          eval_metric='MAE', random_seed=1)\nmodel.fit(X_train, y_train, eval_set=(X_eval, y_eval), use_best_model=True, verbose=False, plot=True)\npred1 = model.predict(X_train)\npred2 = model.predict(X_eval)\nprint(' catboost MAE train  {:.4f}'.format(np.mean(np.abs(y_train.values-pred1) )))\nprint(' catboost MAE eval   {:.4f}'.format(np.mean(np.abs(y_eval.values-pred2) )))\ndel pred1, pred2\ngc.collect()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"FeatImp=pd.DataFrame(model.feature_importances_, index=features, columns=['Importance'])\nFeatImp=FeatImp.sort_values('Importance')\nFeatImp.plot(kind='barh', figsize=(8,14))\nplt.show()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"print('catboost predict and submit ...')\n\nfor month in [ 10,11,12]:\n    print('month ',month)\n    X_test['Month']=np.int8(month) \n    X_test['Meanerror']=X['Meanerror'].loc[X['Month']==month].mean().astype(np.int8)\n    X_test['Meanabserror']=X['Meanerror'].loc[X['Month']==month].mean().astype(np.int8)\n    pred = model.predict(X_test)\n    sample['2016' + str(month)] = pred*1.05\n    sample['2017' + str(month)] = pred\n    print(' catboost MAE {}  {:.4f}'.format(month,np.mean(np.abs(sample['2017' + str(month)]-0) )))\n\nsample.to_csv('submission_cat1.csv', index = False, float_format = '%.5f')\ngc.collect()","execution_count":null,"cell_type":"code","outputs":[]}],"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1}