{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I. Import libraries","metadata":{}},{"cell_type":"code","source":"#Import libraries\nimport pandas as pd \nimport numpy as np\nimport sys\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n# Pandas configurations \nsns.set()\n%matplotlib inline\n#pd.set_option('display.float_format', lambda x: '%.2f' % x)\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# II. Import dataset","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            print(\"******************************\")\n            print(\"Column: \",col)\n            print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n            print(\"dtype after: \",props[col].dtype)\n            print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props, NAlist\n\n\n\n\n#Import data\nprint('Loading data...')\n\ntrain_2016 = pd.read_csv('../input/zillow-prize-1/train_2016_v2.csv', low_memory=False)\nproperties_2016, NAlist_2016 = reduce_mem_usage(pd.read_csv('../input/zillow-prize-1/properties_2016.csv',low_memory=False))\ntrain_2017 = pd.read_csv('../input/zillow-prize-1/train_2017.csv',low_memory=False)\nproperties_2017, NAlist_2017 = reduce_mem_usage(pd.read_csv('../input/zillow-prize-1/properties_2017.csv',low_memory=False))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# III. Data exploratory","metadata":{}},{"cell_type":"markdown","source":"## III.1 Merging the data","metadata":{}},{"cell_type":"code","source":"properties_2016.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Merging properties with the train dataset for exploratory analysis\nprint('Merging the data...')\n\ndf_train_2016 = train_2016.merge(properties_2016, how='left', on='parcelid')\ndf_train_2017 = train_2017.merge(properties_2017, how='left', on='parcelid')\n\nfull_df = pd.concat([df_train_2017,df_train_2016])\n#Check the train dataset\nprint('Our dataset contains {} rows and {} columns.'.format(full_df.shape[0], full_df.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## III.2 Data analysis","metadata":{}},{"cell_type":"markdown","source":"### III.2.1 Data type check","metadata":{}},{"cell_type":"code","source":"# Just as informations about our data types\nsns.set_theme(style=\"whitegrid\")\nplt.title('Data types repartition')\nfull_df.dtypes.value_counts().plot.pie()\nprint('We can see that most of our data are numerical values')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### III.2.2 Dataset Columns","metadata":{}},{"cell_type":"code","source":"full_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Renaming columns for better understanding of our features as they are a bit confusing at first","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    Assign better names to all feature columns of 'properties' table\n\"\"\"\ndef rename_columns(df):\n     df.rename(columns={\n          'parcelid': 'parcelid',  # Unique identifier of parcels\n          'airconditioningtypeid': 'cooling_id',  # type of cooling system (if any), 1~13\n          'architecturalstyletypeid': 'architecture_style_id',  # Architectural style of the home, 1~27\n          'basementsqft': 'basement_sqft',  # Size of the basement\n          'bathroomcnt': 'bathroom_cnt',  # Number of bathrooms (including fractional bathrooms)\n          'bedroomcnt': 'bedroom_cnt',  # Number of bedrooms\n          'buildingclasstypeid': 'framing_id',  # The building framing type, 1~5\n          'buildingqualitytypeid': 'quality_id',  # building condition from best (lowest) to worst (highest)\n          'calculatedbathnbr': 'bathroom_cnt_calc',  # Same meaning as 'bathroom_cnt'?\n          'decktypeid': 'deck_id',  # Type of deck (if any)\n          'finishedfloor1squarefeet': 'floor1_sqft',  # Size of finished living area on first floor\n          'calculatedfinishedsquarefeet': 'finished_area_sqft_calc',  # calculated total finished living area\n          'finishedsquarefeet12': 'finished_area_sqft',  # Same meaning as 'finished_area_sqft_calc'?\n          'finishedsquarefeet13': 'perimeter_area',  # Perimeter living area\n          'finishedsquarefeet15': 'total_area',  # Total area\n          'finishedsquarefeet50': 'floor1_sqft_unk',  # Same meaning as 'floor1_sqft'?\n          'finishedsquarefeet6': 'base_total_area',  # Base unfinished and finished area\n          'fips': 'fips',  # Federal Information Processing Standard code\n          'fireplacecnt': 'fireplace_cnt',  # Number of fireplaces in the home (if any)\n          'fullbathcnt': 'bathroom_full_cnt',  # Number of full bathrooms\n          'garagecarcnt': 'garage_cnt',  # Total number of garages\n          'garagetotalsqft': 'garage_sqft',  # Total size of the garages\n          'hashottuborspa': 'spa_flag',  # Whether the home has a hot tub or spa\n          'heatingorsystemtypeid': 'heating_id',  # type of heating system, 1~25\n          'latitude': 'latitude',  # latitude of the middle of the parcel multiplied by 1e6\n          'longitude': 'longitude',  # longitude of the middle of the parcel multiplied by 1e6\n          'lotsizesquarefeet': 'lot_sqft',  # Area of the lot in sqft\n          'poolcnt': 'pool_cnt', # Number of pools in the lot (if any)\n          'poolsizesum': 'pool_total_size',  # Total size of the pools\n          'pooltypeid10': 'pool_unk_1',\n          'pooltypeid2': 'pool_unk_2',\n          'pooltypeid7': 'pool_unk_3',\n          'propertycountylandusecode': 'county_landuse_code',\n          'propertylandusetypeid': 'landuse_type_id' ,  # Type of land use the property is zoned for, 25 categories\n          'propertyzoningdesc': 'zoning_description',  # Allowed land uses (zoning) for that property\n          'rawcensustractandblock': 'census_1',\n          'regionidcity': 'city_id',  # City in which the property is located (if any)\n          'regionidcounty': 'county_id',  # County in which the property is located\n          'regionidneighborhood': 'neighborhood_id',  # Neighborhood in which the property is located\n          'regionidzip': 'region_zip',\n          'roomcnt': 'room_cnt',  # Total number of rooms in the principal residence\n          'storytypeid': 'story_id',  # Type of floors in a multi-story house, 1~35\n          'threequarterbathnbr': 'bathroom_small_cnt',  # Number of 3/4 bathrooms\n          'typeconstructiontypeid': 'construction_id',  # Type of construction material, 1~18\n          'unitcnt': 'unit_cnt',  # Number of units the structure is built into (2=duplex, 3=triplex, etc)\n          'yardbuildingsqft17': 'patio_sqft',  # Patio in yard\n          'yardbuildingsqft26': 'storage_sqft',  # Storage shed/building in yard\n          'yearbuilt': 'year_built',  # The year the principal residence was built\n          'numberofstories': 'story_cnt',  # Number of stories or levels the home has\n          'fireplaceflag': 'fireplace_flag',  # Whether the home has a fireplace\n          'structuretaxvaluedollarcnt': 'tax_structure',\n          'taxvaluedollarcnt': 'tax_parcel',\n          'assessmentyear': 'tax_year',  # The year of the property tax assessment (2015 for 2016 data)\n          'landtaxvaluedollarcnt': 'tax_land',\n          'taxamount': 'tax_property',\n          'taxdelinquencyflag': 'tax_overdue_flag',  # Property taxes are past due as of 2015\n          'taxdelinquencyyear': 'tax_overdue_year',  # Year for which the unpaid propert taxes were due\n          'censustractandblock': 'census_2'\n     }, inplace=True)\n        \nrename_columns(full_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#full_df = full_df.drop('parcelid', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### III.2.3 Missing Values check","metadata":{}},{"cell_type":"code","source":"# Visualization of the  missing value per columns\nplt.figure(figsize=(13, 40))\nplt.rcParams['axes.facecolor'] = '#eee'\nplt.rc('grid', color='#fff')\n(full_df.isnull().mean(axis=0)*100).sort_values().plot.barh(color =\"Lightblue\")\nplt.xlim(xmax=100)\nplt.title(\"Missing values rate\",fontsize=18)\nplt.xlabel(\"percentage\",fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice there are a lot of columns with more than 90 % missing values.\nLet's check if our target variable has missing values.","metadata":{}},{"cell_type":"code","source":"print('Our target variable \"logerror\" has {} missing value(s)'.format(full_df['logerror'].isnull().sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IV. Splitting the dataset into the train set and the test set","metadata":{}},{"cell_type":"code","source":"y = full_df['logerror']\nX = full_df.drop(['logerror', 'transactiondate', 'county_landuse_code', 'zoning_description'], axis=1)\n\n#print(f\"X shape: {X.shape}\")\n#print(f\"y shape: {y.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For preprocessing purpose, I identify the numerical and object columns.","metadata":{}},{"cell_type":"code","source":"numeric_cols = X.select_dtypes(include=[\"float64\",\"int64\"]).columns\ncategory_cols = X.select_dtypes(include=\"object\").columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in category_cols:\n    print(\"Unique values of the column {} : {}\".format(col, X[col].unique()))\n    print(\"Unique values of the column {} : {}\".format(col, X[col].nunique(dropna=True)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since spa_flag, fireplace_flag and tax_overdue_flag have only one unique value and it's either True or 'Y', we could replace the missing values with False when the unique value is True and 'N' when the unique value is 'Y'.","metadata":{}},{"cell_type":"code","source":"X[['spa_flag', 'fireplace_flag']] = X[['spa_flag', 'fireplace_flag']].fillna(False)\nX['tax_overdue_flag'] = X['tax_overdue_flag'].fillna('N')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")\nprint(f\"y_test shape: {y_test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VI. XGBoost model","metadata":{}},{"cell_type":"markdown","source":"## VI.1 Imputing missing values for numerical and categorical variables","metadata":{}},{"cell_type":"code","source":"# Preprocessing data\nnum_imp = SimpleImputer(missing_values=np.nan, strategy=\"constant\")\n# Preprocessing for categorical data\ncat_imp = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\ncol_trans = ColumnTransformer(\n    transformers=[\n        ('numerical', num_imp, numeric_cols),\n        ('category', cat_imp, category_cols)\n])\n# Random forest model\nmodel_xgboost = XGBRegressor(random_state=0)\n\n#Pipeline\npipe = Pipeline(steps=[\n    ('preprocessor', col_trans),\n    ('model', model_xgboost )\n])\n\n# Defining parameters for the GridSearchCV\nparameters = {\n    'model__n_estimators': [100, 120, 150, 200],\n    'model__learning_rate': [0.02,0.05,0.07]\n}\n\nsearch = GridSearchCV(\n    estimator = pipe,\n    param_grid = parameters,\n    cv = 3\n)\n\n# Fit the model\nsearch.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {search.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: '+ \\\n    f'{search.best_score_:.3f}'\n)\nprint('-----')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(search.score(X_train, y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = search.predict(X_test)\ny_pred = pd.DataFrame(y_pred)\ny_pred.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean absolute error: ', mean_absolute_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub = pd.read_csv('../input/zillow-prize-1/sample_submission.csv')\nsample_sub['parcelid'] = sample_sub['ParcelId']\n\n\nX_valid = properties_2016\n\n\nsub = sample_sub.merge(X_valid, on='parcelid', how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rename_columns(X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.4f' % x)\nX_valid = X_valid.drop(['county_landuse_code', 'zoning_description' ],axis=1)\npredictions = search.predict(X_valid)\npd.DataFrame(predictions).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['201610'] = predictions\nsub['201611'] = predictions\nsub['201612'] = predictions\nsub['201710'] = predictions\nsub['201711'] = predictions\nsub['201712'] = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = sub[['ParcelId', '201610', '201611', '201612', '201710', '201711', '201712']]\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Writing csv ...')\nsub.to_csv('xgboost_model.csv', index=False, float_format='%.4f') # Thanks to @inversion","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}