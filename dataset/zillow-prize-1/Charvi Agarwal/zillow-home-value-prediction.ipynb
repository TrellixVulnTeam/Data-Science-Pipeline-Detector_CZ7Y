{"nbformat_minor":1,"cells":[{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e67a6649-657f-4e39-bc95-bae76edc614f","_uuid":"9a9ae7c5c22aa3fc23e138837f51e4083c72744d"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nfrom sklearn.linear_model import LinearRegression\nimport random\nimport datetime as dt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.noise import GaussianDropout\nfrom keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"941abe54-d706-4325-b23e-f3c06e8c059a","_uuid":"b7b7323ae18e5c47a712065fb861b71632e6cb8d"},"source":"df16 = pd.read_csv(\"../input/properties_2016.csv\")\ndf17 = pd.read_csv(\"../input/properties_2017.csv\")\ntrain16 = pd.read_csv(\"../input/train_2016_v2.csv\")\ntrain17 = pd.read_csv(\"../input/train_2017.csv\")\nsamplesub = pd.read_csv(\"../input/sample_submission.csv\")"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"dd4b164a-bba7-437c-a57e-7769cad042c6","_uuid":"ff4c560ea3dd85140aa9d4a0706e64bcc08eff39"},"source":"df16.head()"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6f20c18d-d283-4060-b7a7-ec32d5a5da71","_uuid":"364bac9b626bc1df3a1dcaf4d0e1cfdd95ee94e8"},"source":"data16 = pd.merge(df16,train16)\ndata17 = pd.merge(df17,train17)\ndata = pd.concat([data16,data17],keys=('parcelid','transactiondate'))\ndata.head()"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ff4726be-7e57-479d-bf96-ec6c4e2e7140","_uuid":"32cd5bb014c2d52488bd92587efe21dc95a4671c"},"source":"num_cols = [col for col in data.columns if (data[col].dtype in ['float64','int64'] and col not in ['parcelid','transactiondate']) or data[col].dtype.name=='category']\ntemp_df = data[num_cols]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(12, 12))\n\nsns.heatmap(corrmat, vmax=1., square=True,cmap='PiYG')\nplt.title(\"Variables correlation map\", fontsize=15)\nplt.show()"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"bd3dca4c-0e2d-452b-aace-9a291c375543","_uuid":"ab72542c22493810b40d4f5ae6077bd014e27ae3"},"source":"for c in data.columns:\n    data[c]=data[c].fillna(-1)\n    if data[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(data[c].values))\n        data[c] = lbl.transform(list(data[c].values))\n\ndata[\"transactiondate\"] = pd.to_datetime(data[\"transactiondate\"])\ndata[\"transactiondate_year\"] = data[\"transactiondate\"].dt.year\ndata[\"transactiondate_month\"] = data[\"transactiondate\"].dt.month\ndata['transactiondate_quarter'] = data['transactiondate'].dt.quarter\ndata[\"transactiondate\"] = data[\"transactiondate\"].dt.day\n\ndata = data.fillna(-1.0)"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"c68a97d5-15ce-428e-9d04-8d8c7d82a5e8","_uuid":"056156b640afa75c2aff75e2824bf93603462258"},"source":"x_train = data.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)\ny_train = data[\"logerror\"]\n\ny_mean = np.mean(y_train)\nprint(x_train.shape, y_train.shape)\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\nsamplesub['parcelid'] = samplesub['ParcelId']\ndf_test = samplesub.merge(df16, on='parcelid', how='left')\n"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"cc5e6c5b-c252-4fa9-84c4-758182838f7e","_uuid":"8a7cbec178d02332010c13982431b36633fbc8cb"},"source":"df_test[\"transactiondate\"] = pd.to_datetime('2016-11-15')  # placeholder value for preliminary version\ndf_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\ndf_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\ndf_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\ndf_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day     \nx_test = df_test[train_columns]\n\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"5cf89d06-2130-4ccc-9b3a-84b980b55e76","_uuid":"b080cd454456031e8f7fd856f7e56cdf0c5d2f58"},"source":"imputer= Imputer()\nimputer.fit(x_train.iloc[:, :])\nx_train = imputer.transform(x_train.iloc[:, :])\nimputer.fit(x_test.iloc[:, :])\nx_test = imputer.transform(x_test.iloc[:, :])\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\nlen_x=int(x_train.shape[1])\nprint(len_x)"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"c5b58d17-d3af-4f91-99fc-fa033896012a","_uuid":"28469fdea616c7c5fe1cf00ff7d228dd4ad9a14e"},"source":"# model taken from Andy Harless\nnn = Sequential()\nnn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\nnn.add(PReLU())\nnn.add(Dropout(.4))\nnn.add(Dense(units = 160 , kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.6))\nnn.add(Dense(units = 64 , kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.5))\nnn.add(Dense(units = 26, kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.6))\nnn.add(Dense(1, kernel_initializer='normal'))\nnn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{},"source":"nn.fit(np.array(x_train), np.array(y_train), batch_size = 1000, epochs = 20, verbose=2)\n"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{},"source":"y_pred_ann = nn.predict(x_test)\nnn_pred = y_pred_ann.flatten()\n\npd.DataFrame(nn_pred).head()"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{},"source":"y_pred=[]\n\nfor i,predict in enumerate(nn_pred):\n    y_pred.append(str(round(predict,4)))\ny_pred=np.array(y_pred)\n\noutput = pd.DataFrame({'ParcelId': df16['parcelid'].astype(np.int32),\n        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n\n# set col 'ParceID' to first col\ncols = output.columns.tolist()\ncols = cols[-1:] + cols[:-1]\noutput = output[cols]\noutput.head()"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"source":"output.to_csv(\"zillow_sub.csv\",index=False)"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"source":""}],"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.4","nbconvert_exporter":"python"}}}