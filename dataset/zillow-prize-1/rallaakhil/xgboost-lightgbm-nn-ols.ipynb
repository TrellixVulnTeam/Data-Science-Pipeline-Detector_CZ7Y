{"cells":[{"metadata":{"_cell_guid":"95e56fb1-1226-48cb-ae4b-c4cb44c3f37b","_uuid":"b22a1bfde673d92d11e29297cbfea55c8b7ea996"},"execution_count":null,"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport gc\nfrom sklearn.linear_model import LinearRegression\nimport random\nimport datetime as dt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.noise import GaussianDropout\nfrom keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"d25ff7ea-e183-4ba4-bbae-fb51ad698046","_uuid":"b178704aa03056c8088e52f080d180ad2921d169","collapsed":true},"execution_count":null,"source":"# Parameters\nFUDGE_FACTOR = 1.1200  # Multiply forecasts by this\n\nXGB_WEIGHT = 0.6200\nBASELINE_WEIGHT = 0.0100\nOLS_WEIGHT = 0.0620\nNN_WEIGHT = 0.0800\n\nXGB1_WEIGHT = 0.8000  # Weight of first in combination of two XGB models\n\nBASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e2905bb5-59cb-4fa4-b193-176a7839f8a9","_uuid":"6f6110456af0dd26b636f4e04820f7bf07cec1a2"},"execution_count":null,"source":"print( \"\\nReading data from disk ...\")\nprop = pd.read_csv('../input/properties_2016.csv')\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\")","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"1bd0f5d8-e0f2-4af2-ac36-66d6c67505e8","_uuid":"fadad10d71f355b486878779af078460838704c4"},"execution_count":null,"source":"#LightGBM\nprint( \"\\nProcessing data for LightGBM ...\" )\nfor c, dtype in zip(prop.columns, prop.dtypes):\t\n    if dtype == np.float64:\n        prop[c] = prop[c].astype(np.float32)\n\ndf_train = train.merge(prop, how='left', on='parcelid')\n\n\nmissing_perc_thresh = 0.98\nnum_rows = df_train.shape[0]\nfor c in df_train.columns:\n    #print (c)\n    num_missing = df_train[c].isnull().sum()\n    if num_missing == 0:\n        continue\n    missing_frac = num_missing / float(num_rows)\n    if missing_frac > missing_perc_thresh:\n        df_train.drop([c],axis=1)\n        #exclude_missing.append(c)\n#df_train=df_train.drop(exclude_missing,axis=1)\n##39\n##excluding having unique value\n\n# exclude where we only have one unique value :D\nfor c in df_train.columns:\n    num_uniques = len(df_train[c].unique())\n    if df_train[c].isnull().sum() != 0:\n        num_uniques -= 1\n    if num_uniques == 1:\n        df_train.drop([c],axis=1)\n        #exclude_unique.append(c)\ndf_train.fillna(df_train.median(),inplace = True)\n\n#x_train['Ratio_1'] = x_train['taxvaluedollarcnt']/x_train['taxamount']\n                         \nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc','propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)\n\n\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\ndel df_train; gc.collect()\n\nx_train = x_train.values.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label=y_train)\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"ab7c3327-6d9c-438d-b033-42c80a1066ae","_uuid":"b94370b1b8ffc9fcbdcce22b7bc43d0ec39471ac","collapsed":true},"execution_count":null,"source":"\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021 # shrinkage_rate\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          # or 'mae'\nparams['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\nparams['bagging_fraction'] = 0.85 # sub_row\nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        # num_leaf\nparams['min_data'] = 500         # min_data_in_leaf\nparams['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nparams['feature_fraction_seed'] = 2\nparams['bagging_seed'] = 3\n\nnp.random.seed(0)\nrandom.seed(0)\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"065ef06d-8935-460b-916b-58b4a21cb836","_uuid":"a5371b188ced451623e923cc687287966281b836"},"execution_count":null,"source":"print(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 430)\n\ndel d_train; gc.collect()\ndel x_train; gc.collect()\n\nprint(\"\\nPrepare for LightGBM prediction ...\")\nprint(\"   Read sample file ...\")\nsample = pd.read_csv('../input/sample_submission.csv')\nprint(\"   ...\")\nsample['parcelid'] = sample['ParcelId']\nprint(\"   Merge with property data ...\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\nprint(\"   ...\")\ndel sample, prop; gc.collect()\nprint(\"   ...\")\n#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\nx_test = df_test[train_columns]\nprint(\"   ...\")\ndel df_test; gc.collect()\nprint(\"   Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nprint(\"   ...\")\nx_test = x_test.values.astype(np.float32, copy=False)\n\nprint(\"\\nStart LightGBM prediction ...\")\np_test = clf.predict(x_test)\n\ndel x_test; gc.collect()\n\nprint( \"\\nUnadjusted LightGBM predictions:\" )\nprint( pd.DataFrame(p_test).head() )\n\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"05b90bf4-34cf-4eb1-af53-45a108c7d1ad","_uuid":"00cc1b279ccd8676d8e9b840aacc3824c91c1063"},"execution_count":null,"source":"#XGBOOST\n\nprint( \"\\nRe-reading properties file ...\")\nproperties = pd.read_csv('../input/properties_2016.csv')\n\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"37f55393-bd08-4635-ac65-011c2f17f0f2","_uuid":"26aaf960e2d1399b1bd3431f8bb7b1ea4e5f01f8"},"execution_count":null,"source":"print( \"\\nProcessing data for XGBoost ...\")\nfor c in properties.columns:\n    properties[c]=properties[c].fillna(-1)\n    if properties[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties[c].values))\n        properties[c] = lbl.transform(list(properties[c].values))\n\ntrain_df = train.merge(properties, how='left', on='parcelid')\nmissing_perc_thresh = 0.98\nnum_rows = train_df.shape[0]\nfor c in train_df.columns:\n    #print (c)\n    num_missing = train_df[c].isnull().sum()\n    if num_missing == 0:\n        continue\n    missing_frac = num_missing / float(num_rows)\n    if missing_frac > missing_perc_thresh:\n        train_df.drop([c],axis=1)\n        #exclude_missing.append(c)\n#df_train=df_train.drop(exclude_missing,axis=1)\n##39\n##excluding having unique value\n\n# exclude where we only have one unique value :D\nfor c in train_df.columns:\n    num_uniques = len(train_df[c].unique())\n    if train_df[c].isnull().sum() != 0:\n        num_uniques -= 1\n    if num_uniques == 1:\n        train_df.drop([c],axis=1)\nx_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\nx_test = properties.drop(['parcelid'], axis=1)\n# shape        \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"a74cf9d6-5f16-4ec5-a6a4-653ad843b2df","_uuid":"787f16a16229f1809cc9795ec1404d7be1800ac0"},"execution_count":null,"source":"#outliers\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.419 ]\nx_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\ny_train = train_df[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\n\nprint('After removing outliers:')     \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e199db4b-df9c-48af-9474-fe168da46710","_uuid":"d44c9357dac59c5d07d3fcfbbb4d81660d4ad599"},"execution_count":null,"source":"print(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.037,\n    'max_depth': 5,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 0.8,   \n    'alpha': 0.4, \n    'base_score': y_mean,\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\nnum_boost_rounds = 250\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\n# train model\nprint( \"\\nTraining XGBoost ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost ...\")\nxgb_pred1 = model.predict(dtest)\n\nprint( \"\\nFirst XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred1).head() )\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"6f31dd9b-0e9d-4131-88e0-df82592aff38","_uuid":"1a063bb8ea96163396ca7c1f009bbdb4c9397b62","collapsed":true},"execution_count":null,"source":"print(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.033,\n    'max_depth': 6,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'base_score': y_mean,\n    'silent': 1\n}\n\nnum_boost_rounds = 150\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\nprint( \"\\nTraining XGBoost again ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost again ...\")\nxgb_pred2 = model.predict(dtest)\n\nprint( \"\\nSecond XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred2).head() )\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"d4f546eb-023f-4559-989d-17c22c59dcef","_uuid":"405812386b853307eab12e48b169c33ef82ffdf1","collapsed":true},"execution_count":null,"source":"xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n#xgb_pred = xgb_pred1\n\nprint( \"\\nCombined XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred).head() )","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"cbd602b1-141f-4677-a0f9-4e8ad10debd5","_uuid":"b75d17082f45cc09500acfb9d484fb28f932db76","collapsed":true},"execution_count":null,"source":"del train_df\ndel x_train\ndel x_test\ndel properties\ndel dtest\ndel dtrain\ndel xgb_pred1\ndel xgb_pred2 \ngc.collect()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"2fc0a8f2-21d5-4b5f-bd7d-3dc2b790bc73","_uuid":"204d094c0b4d07b1bc9b6ef6b2e32c940d09184a","collapsed":true},"execution_count":null,"source":"#OLS\n\nnp.random.seed(17)\nrandom.seed(17)\n\nprint( \"\\n\\nProcessing data for OLS ...\")\n\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\nproperties = pd.read_csv(\"../input/properties_2016.csv\")\nsubmission = pd.read_csv(\"../input/sample_submission.csv\")\nprint(len(train),len(properties),len(submission))","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"44991967-6227-41e8-9a2d-c4b574f965d8","_uuid":"ad26df23cf1219c49c4b94428bc9feb45928ae0f","collapsed":true},"execution_count":null,"source":"def get_features(df):\n    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n    df['transactiondate'] = df['transactiondate'].dt.quarter\n    df = df.fillna(-1.0)\n    return df\n\ndef MAE(y, ypred):\n    #logerror=log(Zestimate)−log(SalePrice)\n    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"894421ea-d590-4a17-b9f5-cbf0e7dfa5d9","_uuid":"ecb697e01323c7e4af29facddc2e0e892fd70cf5","collapsed":true},"execution_count":null,"source":"train = pd.merge(train, properties, how='left', on='parcelid')\ny = train['logerror'].values\ntest = pd.merge(submission, properties, how='left', left_on='ParcelId', right_on='parcelid')\nproperties = [] #memory\n\nexc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\ncol = [c for c in train.columns if c not in exc]\n\ntrain = get_features(train[col])\ntest['transactiondate'] = '2016-01-01' #should use the most common training date\ntest = get_features(test[col])","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"288c0ae0-653a-4dbb-9c5c-b3b4d3d7ab13","_uuid":"7128e2ac04f3fd67714732757b2ff753429499c1","collapsed":true},"execution_count":null,"source":"print(\"\\nFitting OLS...\")\nreg = LinearRegression(n_jobs=-1)\nreg.fit(train, y); print('fit...')\nprint(MAE(y, reg.predict(train)))\ntrain = [];  y = [] #memory\n\ntest_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\ntest_columns = ['201610','201611','201612','201710','201711','201712']\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"b2c39b82-22bf-4c98-9f2e-7f0c0901d4e7","_uuid":"467f3c06677ec641b08b8b0a60a181cb6ba2eecf","collapsed":true},"execution_count":null,"source":"#combining all predictions\n\nprint( \"\\nCombining XGBoost, LightGBM, NN, and baseline predicitons ...\" )\n#lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT \nlgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - OLS_WEIGHT \nlgb_weight0 = lgb_weight / (1 - OLS_WEIGHT)\nxgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\nbaseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\n#nn_weight0 = NN_WEIGHT / (1 - OLS_WEIGHT)\npred0 = 0\npred0 += xgb_weight0*xgb_pred\npred0 += baseline_weight0*BASELINE_PRED\npred0 += lgb_weight0*p_test\n#pred0 += nn_weight0*nn_pred\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e5192adc-957c-48fb-95b8-ebb4647fd868","_uuid":"347ae511f7e67548760e43a5e255d34474240ca4","collapsed":true},"execution_count":null,"source":"print( \"\\nCombined XGB/LGB/NN/baseline predictions:\" )\nprint( pd.DataFrame(pred0).head() )\n\nprint( \"\\nPredicting with OLS and combining with XGB/LGB/NN/baseline predicitons: ...\" )\nfor i in range(len(test_dates)):\n    test['transactiondate'] = test_dates[i]\n    pred = FUDGE_FACTOR * ( OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0 )\n    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n    print('predict...', i)\n\nprint( \"\\nCombined XGB/LGB/NN/baseline/OLS predictions:\" )\nprint( submission.head() )","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"1c03d7c4-8b28-488b-b995-1030b2afdb8a","_uuid":"4f13c5f6f9c2695146a74076ad173ebacd725541","collapsed":true},"execution_count":null,"source":"##### WRITE THE RESULTS\n\nfrom datetime import datetime\n\nprint( \"\\nWriting results to disk ...\" )\nsubmission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n\nprint( \"\\nFinished ...\")","cell_type":"code","outputs":[]}],"nbformat":4,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"version":"3.6.1","file_extension":".py","name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1}