{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","name":"python"}},"cells":[{"metadata":{"_uuid":"292a5acbaae2317b091771c2cca106821a328fb4","_cell_guid":"717d4d47-a82d-474b-bcc1-4011aa264d69","collapsed":true},"cell_type":"code","outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n#------------------------------------------------------------------------------\n\n\ntrain = pd.read_csv('../input/train_2016_v2.csv', parse_dates=[\"transactiondate\"])\nprop = pd.read_csv('../input/properties_2016.csv')\nsample = pd.read_csv('../input/sample_submission.csv')\n\n##### PROCESS DATA FOR REGRESSION\n\nprint( \"\\nProcessing data for REGRESSION ...\" )\nfor c, dtype in zip(prop.columns, prop.dtypes):\t\n    if dtype == np.float64:\t\t\n        prop[c] = prop[c].astype(np.float32)\n\ndf_train = train.merge(prop, how='left', on='parcelid')\n\n\n# drop out ouliers\nax = sns.boxplot(y=df_train[\"logerror\"])\n\n\ndef IQR(x):\n    return np.percentile(x, 75) - np.percentile(x, 25)\n\nupper = np.percentile(df_train.logerror.values, 75)+1.5*IQR(df_train.logerror)\nlower = np.percentile(df_train.logerror.values, 25)-1.5*IQR(df_train.logerror)\n\nprint (\"upper,lower\" ,upper, lower)\n\n\n#Subset the data after removing the outliers\ndf_train=df_train[(df_train['logerror'] < upper) & (df_train['logerror'] > lower)]\ndf_train.shape\n\n#------------------------------------------------------------------------------\n#Feature Engineering\n#------------------------------------------------------------------------------\n###Property age in years\ndf_train['D_Prop_Age'] = 2018 - df_train['yearbuilt']   \n\n\n#Total number of rooms\ndf_train['D_TotalRooms'] = df_train['bathroomcnt']*df_train['bedroomcnt']\n\n#Missing Count\ndf_train['D_miss_count']=df_train.apply(lambda x: sum(x.isnull().values), axis = 1) # For rows\n#------------------------------------------------------------------------------\n\n\ncount = 0\nfor col in list(df_train):\n    if (len(df_train[col].unique())==1):\n        print(col)\n        count=count+1\nprint(count)        \n\n\n\ndf_train['assessmentyear'].value_counts()\n\n#Find out the missing values by columnwise\ndf_train.apply(lambda x: sum(x.isnull().values), axis = 0) # For columns\n\ndf_train['D_miss_count']=df_train.apply(lambda x: sum(x.isnull().values), axis = 1) # For rows\n\ndf_train=df_train[df_train['D_miss_count']<30]\ndf_train.shape\n\n\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','assessmentyear'], axis=1)\n\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)\n\nx_train = x_train.apply(lambda x:x.fillna(x.value_counts().index[0]))\n\nx_train.describe()\n#x_train=np.nan_to_num(x_train)\n#from sklearn.model_selection import train_test_split\n#X_train,X_valid,Y_train,Y_valid=train_test_split(x_train,y_train,test_size=0.2,random_state=42)\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport gc\nfrom sklearn.linear_model import LinearRegression\nimport random\nimport datetime as dt\n\n\n#------------------------------------------------------------------------------\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\ndel df_train; gc.collect()\n\nsplit = 90000\nx_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\nx_train = x_train.values.astype(np.float32, copy=False)\nx_valid = x_valid.values.astype(np.float32, copy=False)\n\nd_train = lgb.Dataset(x_train, label=y_train)\nd_valid = lgb.Dataset(x_valid, label=y_valid)\n\nparams = {}\nparams['learning_rate'] = 0.002\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'mae'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 60\nparams['min_data'] = 500\nparams['min_hessian'] = 1\n\nwatchlist = [d_valid]\nclf = lgb.train(params, d_train, 500, watchlist)\n\ndel d_train, d_valid; gc.collect()\ndel x_train, x_valid; gc.collect()\n\nprint(\"Prepare for the prediction ...\")\nsample = pd.read_csv('../input/sample_submission.csv')\nsample['parcelid'] = sample['ParcelId']\ndf_test = sample.merge(prop, on='parcelid', how='left')\ndel sample, prop; gc.collect()\n\n#------------------------------------------------------------------------------\n###Property age in years\ndf_test['D_Prop_Age'] = 2018 - df_test['yearbuilt']   \n\n\n#Total number of rooms\ndf_test['D_TotalRooms'] = df_test['bathroomcnt']*df_test['bedroomcnt']\n\n#Missing Count\ndf_test['D_miss_count']=df_test.apply(lambda x: sum(x.isnull().values), axis = 1) # For rows\n#------------------------------------------------------------------------------\nx_test = df_test[train_columns]\ndel df_test; gc.collect()\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nx_test = x_test.values.astype(np.float32, copy=False)\n\nprint(\"Start prediction ...\")\n# num_threads > 1 will predict very slow in kernal\nclf.reset_parameter({\"num_threads\":1})\np_test = clf.predict(x_test)\n\ndel x_test; gc.collect()\n\nprint(\"Start write result ...\")\nsub = pd.read_csv('sample_submission.csv')\nfor c in sub.columns[sub.columns != 'ParcelId']:\n    sub[c] = p_test\n\nsub.to_csv('lgb_starter.csv', index=False, float_format='%.4f')\n\n\n# Any results you write to the current directory are saved as output.\n\n","execution_count":null}],"nbformat":4,"nbformat_minor":1}