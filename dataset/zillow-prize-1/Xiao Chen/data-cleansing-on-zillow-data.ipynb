{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Start with importing essentials\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Read the train set and property set of Zillow dataset, and name them as train and properties."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train_2016_v2.csv')\ndf_property = pd.read_csv('../input/properties_2016.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_property.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_property.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Merge train and properties to one dataframe on parcelid and call it as df_train. Drop the column of 'parcelid' and 'transactiondate'. Check the first 5 rows to see how this merged dataset looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.merge(df_property, how='left', on='parcelid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train.drop(['parcelid','transactiondate'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.  (a) Generate a dataframe called missing_df from df_train, in which there are two columns, one is the column names of our features, the other column is the missing_count (the number of missing values) of that feature. The table should be ordered by missing_count decendingly.  "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"missing_df = df_train.isna().sum().to_frame().reset_index()  #create dataframe with missing data information of each column\nmissing_df.columns = ['column name','#missing values'] # rename column names \nmissing_df = missing_df.loc[missing_df['#missing values']>0] # only keep those who have more than 1 missing value\nmissing_df.sort_values(by='#missing values',ascending=True,inplace=True) # sort missing value\nmissing_df ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"missing_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.(b) Draw a horizontal bar plot to visualize it. "},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"missing_df.plot.barh(x='column name', y = '#missing values', figsize=(15,20))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,30))\nplt.barh(missing_df['column name'],missing_df['#missing values'],log=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Generate the correlation matrix for all the numerical features, and plot it by using heatmap or related visualization methods. "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"catcols = ['airconditioningtypeid','architecturalstyletypeid','buildingqualitytypeid','buildingclasstypeid','decktypeid','fips','hashottuborspa','heatingorsystemtypeid','pooltypeid10','pooltypeid2','pooltypeid7','propertycountylandusecode','propertylandusetypeid','propertyzoningdesc','rawcensustractandblock','regionidcity','regionidcounty','regionidneighborhood','regionidzip','storytypeid','typeconstructiontypeid','yearbuilt','taxdelinquencyflag']\nnumcols = [x for x in df_train.columns if x not in catcols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numcols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (14,12))\nsns.heatmap(data=df_train[numcols].corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5. From the results from Step 4,  list those features having a strong correlation. Generate a list called dropcols, and put those redundent variables into it."},{"metadata":{"trusted":true},"cell_type":"code","source":"dropcols = []\ndropcols.append('finishedsquarefeet12')\ndropcols.append('finishedsquarefeet13')\ndropcols.append('finishedsquarefeet15')\ndropcols.append('finishedsquarefeet6')\ndropcols.append('finishedsquarefeet50')\ndropcols.append('calculatedbathnbr')\ndropcols.append('fullbathcnt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6. Some variables where it is NA can be considered as the object does not exist. Such as 'hashottuborspa', if it is NA, we can assume the house doesn't contain the hot tub or spa. So we need to fix this kind of variables."},{"metadata":{},"cell_type":"markdown","source":"(a) Fix the hashottuborspa variable, fill the na part as None."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['hashottuborspa'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['hashottuborspa']=df_train['hashottuborspa'].fillna('None', inplace=True)\n# index = df_train.hashottuborspa.isnull()\n# df_train.loc[index,'hashottuborspa'] = 'None'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(b) Assume if the pooltype id and its related features is null then pool/hottub doesn't exist."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train.pooltypeid10.isnull()\n         |df_train.pooltypeid2.isnull()\n         |df_train.pooltypeid7.isnull()].hashottuborspa.fillna('None', inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train.loc[0:5,['hashottuborspa']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(c) taxdelinquencyflag - assume if it is null then doesn't exist"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['taxdelinquencyflag'] = df_train['taxdelinquencyflag'].fillna(\"doesn't exist\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[0:5,['taxdelinquencyflag']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(d) If Null in garage count (garagecarcnt) it means there are no garages, and no garage means the size (garagetotalsqft) is 0 by default"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[df_train.garagecarcnt.isnull(),'garagetotalsqft']=0\n# setting value for items matching condition ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train.loc[0:10,['garagecarcnt','garagetotalsqft']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7. There are more missing values in the 'poolsizesum' than in 'poolcnt'. Fill in median values for poolsizesum where pool count is >0 and missing."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"poolsizesum_median = df_train.loc[df_train['poolcnt']>0,'poolsizesum'].median()\npoolsizesum_median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[(df_train['poolcnt']>0) & (df_train['poolsizesum'].isnull()),'poolsizesum']=poolsizesum_median\ndf_train.loc[(df_train['poolcnt']==0),'poolsizesum']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[:5,'poolsizesum']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8. The number of missing value of 'fireplaceflag' is more than the 'fireplacecnt'. So we need to mark the missing 'fireplaceflag' as Yes when fireplacecnt>0, then the rest of 'fireplaceflag' should be marked as No. Then for the missing part in fireplacecnt, we can consider the number of fire place is 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.fireplaceflag.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.fireplaceflag='No'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.fireplaceflag.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[df_train['fireplacecnt']>0,'fireplaceflag']='Yes'\ndf_train.loc[df_train['fireplacecnt'].isnull(),'fireplaceflag']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.fireplaceflag.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 9. Fill some features with the most common value for those variables where this might be a sensible approach:"},{"metadata":{},"cell_type":"markdown","source":"(a) AC Type (airconditioningtypeid)- Mostly 1's, which corresponds to central AC. It is reasonable to assume most other properties where this feature is missing are similar."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train.airconditioningtypeid.fillna(1,inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(b) heating or system (heatingorsystemtypeid)- Mostly 2, which corresponds to central heating so seems reasonable to assume most other properties have central heating."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.heatingorsystemtypeid.fillna(2,inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 10. If the features where missing proportion is too much, we can directly delete them. Here we set 97% as our threshold (This is subjective) and add them into the dropcols. Then drop those features in dropcols from the full table."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"missingvalues_prop = (df_train.isnull().sum()/len(df_train)).reset_index()\nmissingvalues_prop.columns = ['name','proportion']\nmissingvalues_prop = missingvalues_prop.sort_values (by = 'proportion', ascending=False)\nprint(missingvalues_prop)\nmissingvaluescols = missingvalues_prop [missingvalues_prop['proportion']>0.97].field.tolist()\ndropcols = dropcols + missingvaluescols\ndf_train = df_train.drop (dropcols,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 11. We can also use some machine learning algorithm to fill the missing data. \nIn this dataset, there's quite a few variables which are probably dependant on longtitude and latitude data. It is reasonable to fill in some of the missing variables using geographically nearby properties (by using the longtitude and latitude information)."},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.array([True,False])\nprint(~a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code comes from the link:\nhttps://www.kaggle.com/auroralht/restoring-the-missing-geo-data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import neighbors\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n## Works on categorical feature\ndef fillna_knn( df, base, target, fraction = 1, threshold = 10, n_neighbors = 5 ):\n    assert isinstance( base , list ) or isinstance( base , np.ndarray ) and isinstance( target, str ) \n    whole = [ target ] + base\n    \n    miss = df[target].isnull()\n    notmiss = ~miss \n    nummiss = miss.sum()\n    \n    enc = OneHotEncoder()\n    X_target = df.loc[ notmiss, whole ].sample( frac = fraction )\n    \n    enc.fit( X_target[ target ].unique().reshape( (-1,1) ) )\n    \n    Y = enc.transform( X_target[ target ].values.reshape((-1,1)) ).toarray()\n    X = X_target[ base  ]\n    \n    print( 'fitting' )\n    n_neighbors = n_neighbors\n    clf = neighbors.KNeighborsClassifier( n_neighbors, weights = 'uniform' )\n    clf.fit( X, Y )\n    \n    print( 'the shape of active features: ' ,enc.active_features_.shape )\n    \n    print( 'predicting' )\n    Z = clf.predict(df.loc[miss, base])\n    \n    numunperdicted = Z[:,0].sum()\n    if numunperdicted / nummiss *100 < threshold :\n        print( 'writing result to df' )    \n        df.loc[ miss, target ]  = np.dot( Z , enc.active_features_ )\n        print( 'num of unperdictable data: ', numunperdicted )\n        return enc\n    else:\n        print( 'out of threshold: {}% > {}%'.format( numunperdicted / nummiss *100 , threshold ) )\n\n#function to deal with variables that are actually string/categories\ndef zoningcode2int( df, target ):\n    storenull = df[ target ].isnull()\n    enc = LabelEncoder( )\n    df[ target ] = df[ target ].astype( str )\n\n    print('fit and transform')\n    df[ target ]= enc.fit_transform( df[ target ].values )\n    print( 'num of categories: ', enc.classes_.shape  )\n    df.loc[ storenull, target ] = np.nan\n    print('recover the nan value')\n    return enc\n\n### Example: \n### If you want to impute buildingqualitytypeid with geological information:\n\"\"\"\nfillna_knn( df = df_train,\n                  base = [ 'latitude', 'longitude' ] ,\n                  target = 'buildingqualitytypeid', fraction = 0.15, n_neighbors = 1 )\n\"\"\"\n\n## Works on regression\ndef fillna_knn_reg( df, base, target, n_neighbors = 5 ):\n    cols = base + [target]\n    X_train = df[cols]\n    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_train[base].values.reshape(-1, 1))\n    rescaledX = scaler.transform(X_train[base].values.reshape(-1, 1))\n\n    X_train = rescaledX[df[target].notnull()]\n    Y_train = df.loc[df[target].notnull(),target].values.reshape(-1, 1)\n\n    knn = KNeighborsRegressor(n_neighbors, n_jobs = -1)    \n    # fitting the model\n    knn.fit(X_train, Y_train)\n    # predict the response\n    X_test = rescaledX[df[target].isnull()]\n    pred = knn.predict(X_test)\n    df.loc[df_train[target].isnull(),target] = pred\n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find out some features you can use this knn to fill the missing data, and use the above funtion to impute them**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns.values","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train.loc[:,['latitude','longitude']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[:,['latitude','longitude']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fillna_knn( df = df_train,\n                  base = [ 'latitude', 'longitude' ] ,\n                  target = 'regionidcity', fraction = 0.15, n_neighbors = 1 )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}