{"metadata":{"language_info":{"version":"3.6.3","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"6700f168-bbea-438e-be85-c9cf4a59aa00","_uuid":"81a342f3f618e5a5aa962da1f211d18432f28ea1"},"cell_type":"code","outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom catboost import CatBoostRegressor\nfrom tqdm import tqdm\nimport gc\nimport datetime as dt\n\nprint('Loading Properties ...')\nproperties2016 = pd.read_csv('../input/properties_2016.csv', low_memory = False)\nproperties2017 = pd.read_csv('../input/properties_2017.csv', low_memory = False)\n\nprint('Loading Train ...')\ntrain2016 = pd.read_csv('../input/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\ntrain2017 = pd.read_csv('../input/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n\ndef add_date_features(df):\n    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transaction_month\"] = (df[\"transactiondate\"].dt.year - 2016)*12 + df[\"transactiondate\"].dt.month\n    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n    df[\"transaction_quarter\"] = (df[\"transactiondate\"].dt.year - 2016)*4 +df[\"transactiondate\"].dt.quarter\n    df.drop([\"transactiondate\"], inplace=True, axis=1)\n    return df\n\ntrain2016 = add_date_features(train2016)\ntrain2017 = add_date_features(train2017)\n\nprint('Loading Sample ...')\nsample_submission = pd.read_csv('../input/sample_submission.csv', low_memory = False)\n\nprint('Merge Train with Properties ...')\ntrain2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\ntrain2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n\nprint('Tax Features 2017  ...')\ntrain2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan\n\nprint('Concat Train 2016 & 2017 ...')\ntrain_df = pd.concat([train2016, train2017], axis = 0)\ntest_df = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')\n\ndel properties2016, properties2017, train2016, train2017\ngc.collect();\n\nprint('Remove missing data fields ...')\n\nmissing_perc_thresh = 0.98\nexclude_missing = []\nnum_rows = train_df.shape[0]\nfor c in train_df.columns:\n    num_missing = train_df[c].isnull().sum()\n    if num_missing == 0:\n        continue\n    missing_frac = num_missing / float(num_rows)\n    if missing_frac > missing_perc_thresh:\n        exclude_missing.append(c)\nprint(\"We exclude: %s\" % len(exclude_missing))\n\ndel num_rows, missing_perc_thresh\ngc.collect();\n\nprint (\"Remove features with one unique value !!\")\nexclude_unique = []\nfor c in train_df.columns:\n    num_uniques = len(train_df[c].unique())\n    if train_df[c].isnull().sum() != 0:\n        num_uniques -= 1\n    if num_uniques == 1:\n        exclude_unique.append(c)\nprint(\"We exclude: %s\" % len(exclude_unique))\n\nprint (\"Define training features !!\")\nexclude_other = ['parcelid', 'logerror','propertyzoningdesc']\ntrain_features = []\nfor c in train_df.columns:\n    if c not in exclude_missing \\\n       and c not in exclude_other and c not in exclude_unique:\n        train_features.append(c)\nprint(\"We use these for training: %s\" % len(train_features))\n\nprint (\"Define categorial features !!\")\ncat_feature_inds = []\ncat_unique_thresh = 1000\nfor i, c in enumerate(train_features):\n    num_uniques = len(train_df[c].unique())\n    if num_uniques < cat_unique_thresh \\\n       and not 'sqft' in c \\\n       and not 'cnt' in c \\\n       and not 'nbr' in c \\\n       and not 'number' in c:\n        cat_feature_inds.append(i)\n        \nprint(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])\n\nprint (\"Replacing NaN values by 0 !!\")\ntrain_df.fillna(0, inplace=True)\ntest_df.fillna(0, inplace=True)\n\n\nprint (\"remove outliers\")\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.419 ]\n\n\nprint (\"Training time !!\")\nX_train = train_df[train_features]\ny_train = train_df.logerror\nprint(X_train.shape, y_train.shape)\n\ntest_df['transactiondate'] = pd.Timestamp('2016-12-01') \ntest_df = add_date_features(test_df)\nX_test = test_df[train_features]\nprint(X_test.shape)\n\nnum_ensembles = 5\ny_pred = 0.0\nfor i in tqdm(range(num_ensembles)):\n    model = CatBoostRegressor(\n        iterations=630, learning_rate=0.03,\n        depth=6, l2_leaf_reg=3,\n        loss_function='MAE',\n        eval_metric='MAE',\n        random_seed=i)\n    model.fit(\n        X_train, y_train,\n        cat_features=cat_feature_inds)\n    y_pred += model.predict(X_test)\ny_pred /= num_ensembles\n\n\n\n\n\n\n\n\n\n\nsubmission = pd.DataFrame({\n    'ParcelId': test_df['ParcelId'],\n})\ntest_dates = {\n    '201610': pd.Timestamp('2016-10-01'),\n    '201611': pd.Timestamp('2016-11-01'),\n    '201612': pd.Timestamp('2016-12-01'),\n    '201710': pd.Timestamp('2017-10-01'),\n    '201711': pd.Timestamp('2017-11-01'),\n    '201712': pd.Timestamp('2017-12-02')\n}\nfor label, test_date in test_dates.items():\n    print(\"Predicting for: %s ... \" % (label))\n    submission[label] = y_pred\n\nprint( \"\\nCombined XGB/LGB/baseline/OLS predictions:\" )\nprint( submission.head() )\n\nsubmission.to_csv('final_solution_0.csv', float_format='%.6f',index=False)","execution_count":1},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"nbformat_minor":1}