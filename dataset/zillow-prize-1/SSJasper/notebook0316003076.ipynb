{"nbformat":4,"cells":[{"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom catboost import CatBoostRegressor\nfrom tqdm import tqdm\nimport gc\nimport datetime as dt\n\nprint('Loading Properties ...')\nproperties2016 = pd.read_csv('../input/properties_2016.csv', low_memory = False)\nproperties2017 = pd.read_csv('../input/properties_2017.csv', low_memory = False)\n\nprint('Loading Train ...')\ntrain2016 = pd.read_csv('../input/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\ntrain2017 = pd.read_csv('../input/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n\ndef add_date_features(df):\n    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transaction_month\"] = (df[\"transactiondate\"].dt.year - 2016)*12 + df[\"transactiondate\"].dt.month\n    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n    df[\"transaction_quarter\"] = (df[\"transactiondate\"].dt.year - 2016)*4 +df[\"transactiondate\"].dt.quarter\n    df.drop([\"transactiondate\"], inplace=True, axis=1)\n    return df\n\ntrain2016 = add_date_features(train2016)\ntrain2017 = add_date_features(train2017)\n\nprint('Loading Sample ...')\nsample_submission = pd.read_csv('../input/sample_submission.csv', low_memory = False)\n\nprint('Merge Train with Properties ...')\ntrain2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\ntrain2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n\nprint('Tax Features 2017  ...')\ntrain2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan\n\nprint('Concat Train 2016 & 2017 ...')\ntrain_df = pd.concat([train2016, train2017], axis = 0)\ntest_df = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')\n\ndel properties2016, properties2017, train2016, train2017\ngc.collect();\n\nprint('Remove missing data fields ...')\n\nmissing_perc_thresh = 0.98\nexclude_missing = []\nnum_rows = train_df.shape[0]\nfor c in train_df.columns:\n    num_missing = train_df[c].isnull().sum()\n    if num_missing == 0:\n        continue\n    missing_frac = num_missing / float(num_rows)\n    if missing_frac > missing_perc_thresh:\n        exclude_missing.append(c)\nprint(\"We exclude: %s\" % len(exclude_missing))\n\ndel num_rows, missing_perc_thresh\ngc.collect();\n\nprint (\"Remove features with one unique value !!\")\nexclude_unique = []\nfor c in train_df.columns:\n    num_uniques = len(train_df[c].unique())\n    if train_df[c].isnull().sum() != 0:\n        num_uniques -= 1\n    if num_uniques == 1:\n        exclude_unique.append(c)\nprint(\"We exclude: %s\" % len(exclude_unique))\n\nprint (\"Define training features !!\")\nexclude_other = ['parcelid', 'logerror','propertyzoningdesc']\ntrain_features = []\nfor c in train_df.columns:\n    if c not in exclude_missing \\\n       and c not in exclude_other and c not in exclude_unique:\n        train_features.append(c)\nprint(\"We use these for training: %s\" % len(train_features))\n\nprint (\"Define categorial features !!\")\ncat_feature_inds = []\ncat_unique_thresh = 1000\nfor i, c in enumerate(train_features):\n    num_uniques = len(train_df[c].unique())\n    if num_uniques < cat_unique_thresh \\\n       and not 'sqft' in c \\\n       and not 'cnt' in c \\\n       and not 'nbr' in c \\\n       and not 'number' in c:\n        cat_feature_inds.append(i)\n        \nprint(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])\n\nprint (\"Replacing NaN values by 0 !!\")\ntrain_df.fillna(0, inplace=True)\ntest_df.fillna(0, inplace=True)\n\n\nprint (\"remove outliers\")\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.419 ]\n\n\nprint (\"Training time !!\")\nX_train = train_df[train_features]\ny_train = train_df.logerror\nprint(X_train.shape, y_train.shape)\n\ntest_df['transactiondate'] = pd.Timestamp('2016-12-01') \ntest_df = add_date_features(test_df)\nX_test = test_df[train_features]\nprint(X_test.shape)\n\nnum_ensembles = 5\ny_pred = 0.0\nfor i in tqdm(range(num_ensembles)):\n    model = CatBoostRegressor(\n        iterations=630, learning_rate=0.03,\n        depth=6, l2_leaf_reg=3,\n        loss_function='MAE',\n        eval_metric='MAE',\n        random_seed=i)\n    model.fit(\n        X_train, y_train,\n        cat_features=cat_feature_inds)\n    y_pred += model.predict(X_test)\ny_pred /= num_ensembles\n\n\n\n\n\n\n\n\n\n\nsubmission = pd.DataFrame({\n    'ParcelId': test_df['ParcelId'],\n})\ntest_dates = {\n    '201610': pd.Timestamp('2016-10-01'),\n    '201611': pd.Timestamp('2016-11-01'),\n    '201612': pd.Timestamp('2016-12-01'),\n    '201710': pd.Timestamp('2017-10-01'),\n    '201711': pd.Timestamp('2017-11-01'),\n    '201712': pd.Timestamp('2017-12-02')\n}\nfor label, test_date in test_dates.items():\n    print(\"Predicting for: %s ... \" % (label))\n    submission[label] = y_pred\n\nprint( \"\\nCombined XGB/LGB/baseline/OLS predictions:\" )\nprint( submission.head() )\n\nsubmission.to_csv('final_solution_0.csv', float_format='%.6f',index=False)","metadata":{"_uuid":"5db8a7a62b4feb94d0c743f3b735a64fd5045e0e","_cell_guid":"1a910f61-bd78-44bf-ba00-03d118d46534","_kg_hide-output":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"###########################################\n#scripts 2\n###########################################\n\n\n# Parameters\nXGB_WEIGHT = 0.6840\nBASELINE_WEIGHT = 0.0056\nOLS_WEIGHT = 0.0550\n\nXGB1_WEIGHT = 0.8083  # Weight of first in combination of two XGB models\n\nBASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport gc\nfrom sklearn.linear_model import LinearRegression\nimport random\nimport datetime as dt\n\n\n##### READ IN RAW DATA\n\nprint( \"\\nReading data from disk ...\")\nprop_2016 = pd.read_csv('../input/properties_2016.csv')\nprop_2017 = pd.read_csv('../input/properties_2017.csv')\ntrain_2016 = pd.read_csv(\"../input/train_2016_v2.csv\")\ntrain_2017 = pd.read_csv(\"../input/train_2017.csv\")\n\ntrain = pd.concat([train_2016, train_2017], axis = 0)\nprop = pd.concat([prop_2016, prop_2017], axis = 0)\n\n\n################\n################\n##  LightGBM  ##\n################\n################\n\n# This section is (I think) originally derived from SIDHARTH's script:\n#   https://www.kaggle.com/sidharthkumar/trying-lightgbm\n# which was forked and tuned by Yuqing Xue:\n#   https://www.kaggle.com/yuqingxue/lightgbm-85-97\n# and updated by me (Andy Harless):\n#   https://www.kaggle.com/aharless/lightgbm-with-outliers-remaining\n# and a lot of additional changes have happened since then,\n#   the most recent of which are documented in my comments above\n \n\n##### PROCESS DATA FOR LIGHTGBM\n\nprint( \"\\nProcessing data for LightGBM ...\" )\nfor c, dtype in zip(prop.columns, prop.dtypes):\t\n    if dtype == np.float64:\t\t\n        prop[c] = prop[c].astype(np.float32)\n\ndf_train = train.merge(prop, how='left', on='parcelid')\ndf_train.fillna(df_train.median(),inplace = True)\n\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n#x_train['Ratio_1'] = x_train['taxvaluedollarcnt']/x_train['taxamount']\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)\n\n\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\ndel df_train; gc.collect()\n\nx_train = x_train.values.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label=y_train)\n\n\n\n##### RUN LIGHTGBM\n\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021 # shrinkage_rate\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          # or 'mae'\nparams['sub_feature'] = 0.5      # feature_fraction -- OK, back to .5, but maybe later increase this\nparams['bagging_fraction'] = 0.85 # sub_row\nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        # num_leaf\nparams['min_data'] = 500         # min_data_in_leaf\nparams['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\nparams['verbose'] = 0\n\nprint(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 430)\n\ndel d_train; gc.collect()\ndel x_train; gc.collect()\n\nprint(\"\\nPrepare for LightGBM prediction ...\")\nprint(\"   Read sample file ...\")\nsample = pd.read_csv('../input/sample_submission1.csv')\nprint(\"   ...\")\nsample['parcelid'] = sample['ParcelId']\nprint(\"   Merge with property data ...\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\nprint(\"   ...\")\ndel sample, prop; gc.collect()\nprint(\"   ...\")\n#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\nx_test = df_test[train_columns]\nprint(\"   ...\")\ndel df_test; gc.collect()\nprint(\"   Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nprint(\"   ...\")\nx_test = x_test.values.astype(np.float32, copy=False)\n\nprint(\"\\nStart LightGBM prediction ...\")\np_test = clf.predict(x_test)\n\ndel x_test; gc.collect()\n\nprint( \"\\nUnadjusted LightGBM predictions:\" )\nprint( pd.DataFrame(p_test).head() )\n\n\n\n\n################\n################\n##  XGBoost   ##\n################\n################\n\n# This section is (I think) originally derived from Infinite Wing's script:\n#   https://www.kaggle.com/infinitewing/xgboost-without-outliers-lb-0-06463\n# inspired by this thread:\n#   https://www.kaggle.com/c/zillow-prize-1/discussion/33710\n# but the code has gone through a lot of changes since then\n\n\n##### RE-READ PROPERTIES FILE\n##### (I tried keeping a copy, but the program crashed.)\n\nprint( \"\\nRe-reading properties file ...\")\nproperties = pd.read_csv('../input/properties_2016.csv')\n\n\n\n##### PROCESS DATA FOR XGBOOST\n\nprint( \"\\nProcessing data for XGBoost ...\")\nfor c in properties.columns:\n    properties[c]=properties[c].fillna(-1)\n    if properties[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties[c].values))\n        properties[c] = lbl.transform(list(properties[c].values))\n\ntrain_df = train.merge(properties, how='left', on='parcelid')\nx_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\nx_test = properties.drop(['parcelid'], axis=1)\n# shape        \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n\n# drop out ouliers\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.419 ]\nx_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\ny_train = train_df[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\n\nprint('After removing outliers:')     \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n\n\n\n\n##### RUN XGBOOST\n\nprint(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.037,\n    'max_depth': 5,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 0.8,   \n    'alpha': 0.4, \n    'base_score': y_mean,\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\nnum_boost_rounds = 250\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\n# train model\nprint( \"\\nTraining XGBoost ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost ...\")\nxgb_pred1 = model.predict(dtest)\n\nprint( \"\\nFirst XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred1).head() )\n\n\n\n##### RUN XGBOOST AGAIN\n\nprint(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.033,\n    'max_depth': 6,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'base_score': y_mean,\n    'silent': 1\n}\n\nnum_boost_rounds = 150\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\nprint( \"\\nTraining XGBoost again ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost again ...\")\nxgb_pred2 = model.predict(dtest)\n\nprint( \"\\nSecond XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred2).head() )\n\n\n\n##### COMBINE XGBOOST RESULTS\nxgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n#xgb_pred = xgb_pred1\n\nprint( \"\\nCombined XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred).head() )\n\ndel train_df\ndel x_train\ndel x_test\ndel properties\ndel dtest\ndel dtrain\ndel xgb_pred1\ndel xgb_pred2 \ngc.collect()\n\n\n\n################\n################\n##    OLS     ##\n################\n################\n\n# This section is derived from the1owl's notebook:\n#    https://www.kaggle.com/the1owl/primer-for-the-zillow-pred-approach\n# which I (Andy Harless) updated and made into a script:\n#    https://www.kaggle.com/aharless/updated-script-version-of-the1owl-s-basic-ols\n\nnp.random.seed(17)\nrandom.seed(17)\n\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\nproperties = pd.read_csv(\"../input/properties_2016.csv\")\nsubmission1 = pd.read_csv(\"../input/sample_submission.csv\")\nprint(len(train),len(properties),len(submission1))\n\ndef get_features(df):\n    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n    df['transactiondate'] = df['transactiondate'].dt.quarter\n    df = df.fillna(-1.0)\n    return df\n\ndef MAE(y, ypred):\n    #logerror=log(Zestimate)−log(SalePrice)\n    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)\n\ntrain = pd.merge(train, properties, how='left', on='parcelid')\ny = train['logerror'].values\ntest = pd.merge(submission1, properties, how='left', left_on='ParcelId', right_on='parcelid')\nproperties = [] #memory\n\nexc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\ncol = [c for c in train.columns if c not in exc]\n\ntrain = get_features(train[col])\ntest['transactiondate'] = '2016-01-01' #should use the most common training date\ntest = get_features(test[col])\n\nreg = LinearRegression(n_jobs=-1)\nreg.fit(train, y); print('fit...')\nprint(MAE(y, reg.predict(train)))\ntrain = [];  y = [] #memory\n\ntest_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\ntest_columns = ['201610','201611','201612','201710','201711','201712']\n\n\n\n\n########################\n########################\n##  Combine and Save  ##\n########################\n########################\n\n\n##### COMBINE PREDICTIONS\n\nprint( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\nlgb_weight = (1 - XGB_WEIGHT - BASELINE_WEIGHT) / (1 - OLS_WEIGHT)\nxgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\nbaseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\npred0 = xgb_weight0*xgb_pred + baseline_weight0*BASELINE_PRED + lgb_weight*p_test\n\n\n\n\n\nprint( \"\\nCombined XGB/LGB/baseline predictions:\" )\nprint( pd.DataFrame(pred0).head() )\n\nprint( \"\\nPredicting with OLS and combining with XGB/LGB/baseline predicitons: ...\" )\nfor i in range(len(test_dates)):\n    test['transactiondate'] = test_dates[i]\n    pred = OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0\n    submission1[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n    print('predict...', i)\n\nprint( \"\\nCombined XGB/LGB/baseline/OLS predictions:\" )\nprint( submission1.head() )\n\n\nsubmission1.to_csv('final_solution_1.csv', float_format='%.6f',index=False)","metadata":{},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]}],"metadata":{"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","name":"python","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":1}