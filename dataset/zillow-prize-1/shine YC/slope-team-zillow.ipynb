{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this competition, Zillow is asking you to predict the log-error between their Zestimate and the actual sale price, given all the features of a home. The log error is defined as\n\n                                 logerror=log(Zestimate)âˆ’log(SalePrice)\nand it is recorded in the transactions file train.csv. In this competition, you are going to predict the logerror for the months in Fall 2017. Since all the real estate transactions in the U.S. are publicly available, we will close the competition (no longer accepting submissions) before the evaluation period begins.\n\n* Train/Test split\nYou are provided with a full list of real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016.\n* The train data has all the transactions before October 15, 2016, plus some of the transactions after October 15, 2016.\n* The test data in the public leaderboard has the rest of the transactions between October 15 and December 31, 2016.\n* The rest of the test data, which is used for calculating the private leaderboard, is all the properties in October 15, 2017, to December 15, 2017. This period is called the \"sales tracking period\", during which we will not be taking any submissions.\n* You are asked to predict 6 time points for all properties: October 2016 (201610), November 2016 (201611), December 2016 (201612), October 2017 (201710), November 2017 (201711), and December 2017 (201712).\n* Not all the properties are sold in each time period. If a property was not sold in a certain time period, that particular row will be ignored when calculating your score.\n* If a property is sold multiple times within 31 days, we take the first reasonable value as the ground truth. By \"reasonable\", we mean if the data seems wrong, we will take the transaction that has a value that makes more sense.\n* File descriptions\n* properties_2016.csv - all the properties with their home features for 2016. Note: Some 2017 new properties don't have any data yet except for their parcelid's. Those data points should be populated when properties_2017.csv is available.\n* properties_2017.csv - all the properties with their home features for 2017 (released on 10/2/2017)\n* train_2016.csv - the training set with transactions from 1/1/2016 to 12/31/2016\n* train_2017.csv - the training set with transactions from 1/1/2017 to 9/15/2017 (released on 10/2/2017)\nsample_submission.csv - a sample submission file in the correct format"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport missingno as msno\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import ttest_ind\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.feature_selection import SelectFromModel\nimport h2o\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\n\n#sets up pandas table display\npd.set_option('display.width', 800)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\n#stop scientific notation\npd.options.display.float_format = '{:.2f}'.format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making a list of missing value types\nmissing_values = [\"n/a\", \"na\", \"--\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the 2016 properties data and target variable\nhouse_2016_df = pd.read_csv('../input/properties_2016.csv', na_values = missing_values, low_memory=False)\nhouse_2017_df = pd.read_csv('../input/properties_2017.csv', na_values = missing_values, low_memory=False)\nhouse_log_2016 = pd.read_csv('../input/train_2016_v2.csv', low_memory=False)\nhouse_log_2017 = pd.read_csv('../input/train_2017.csv', low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the size\nprint(house_2016_df.shape)\nprint(house_2017_df.shape)\nprint(house_log_2016.shape)\nprint(house_log_2017.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_2017_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge the trasaction dataset fro 2016-2017\nhouse_log_full = pd.concat([house_log_2016,house_log_2017],ignore_index=True)\nhouse_log_full.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#join the 2017 train set with 2016&2017 target variable\nhouse_2017_full = house_2017_df.merge(house_log_full, on = 'parcelid')\nhouse_2017_full.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#impute boolean variables \nhouse_2017_full['fireplaceflag'].replace(True, 1, inplace=True)\nhouse_2017_full['fireplaceflag'].fillna(0, inplace = True)\nhouse_2017_full['hashottuborspa'].replace(True, 1, inplace=True)\nhouse_2017_full['hashottuborspa'].fillna(0, inplace = True) \nhouse_2017_full['pooltypeid10'].fillna(0, inplace = True) \nhouse_2017_full['pooltypeid2'].fillna(0, inplace = True)\nhouse_2017_full['pooltypeid7'].fillna(0, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot distribution of target variable log error\nfrom scipy.stats import zscore\nhouse_2017_full[\"logerror_zscore\"] = zscore(house_2017_full[\"logerror\"])\nhouse_2017_full[\"is_outlier\"] = house_2017_full[\"logerror_zscore\"].apply(\n  lambda x: x <= -2.5 or x >= 2.5\n)\n\nplt.figure(figsize=(12,8))\nsb.distplot(house_2017_full[~house_2017_full['is_outlier']].logerror.values, bins=50, kde=False)\nplt.xlabel('logerror', fontsize=12)\nplt.title('logerror distribution')\nplt.show()                     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#explore the missing value\nmissing_df = house_2017_full.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missing_df.missing_count.values, color='lightblue')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for continues variables, plot the scatter plot\nlow_na_num_features = ['lotsizesquarefeet','finishedsquarefeet12','calculatedbathnbr','fullbathcnt','roomcnt','bedroomcnt','bathroomcnt','landtaxvaluedollarcnt', 'taxamount', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_2017_full[low_na_num_features].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#explore categorical features with low missing values\nlow_na_cat_features = ['airconditioningtypeid', 'decktypeid', 'heatingorsystemtypeid', \n                       'hashottuborspa', 'heatingorsystemtypeid','regionidcounty', 'regionidcity',\n                       'propertycountylandusecode','propertylandusetypeid','yearbuilt','assessmentyear']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for continues variables, plot the scatter plot#correlation matrix to measure the corr between continuous variables\nnum_var = ['basementsqft', 'bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid','buildingclasstypeid','threequarterbathnbr','calculatedfinishedsquarefeet',\n           'calculatedbathnbr','finishedsquarefeet6','finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15','finishedsquarefeet50','fireplacecnt',\n           'fullbathcnt','garagecarcnt', 'garagetotalsqft','lotsizesquarefeet','poolsizesum', 'poolcnt', 'numberofstories', 'poolcnt', 'poolsizesum','roomcnt', \n           'unitcnt','yardbuildingsqft17','yearbuilt', 'yardbuildingsqft26', 'latitude', 'longitude','taxvaluedollarcnt', 'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', \n           'taxamount','logerror']\ncorr = house_2017_full[num_var].corr()\nig, ax = plt.subplots(figsize=(25,15)) \nsb.heatmap(corr, cmap=\"YlGnBu\", annot=True, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Multicollinearity variables\nhouse_2017_full.drop(['taxvaluedollarcnt', 'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'bathroomcnt',\n                      'fullbathcnt','finishedsquarefeet6', 'finishedsquarefeet12', 'finishedsquarefeet13','finishedsquarefeet15',\n                      'finishedsquarefeet50'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#one way ANOVA analysis between cat variables has more than two levels and logerror\nfor i in range(0, len(low_na_cat_features)):\n    formular = str('logerror ~ '+low_na_cat_features[i])\n    model = ols(formular,data = house_2017_full).fit()\n    anova_result = sm.stats.anova_lm(model, typ=2)\n    print (anova_result)\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* significant var:'airconditioningtypeid', 'decktypeid', 'heatingorsystemtypeid', 'hashottuborspa', 'heatingorsystemtypeid','regionidcounty', 'regionidcity', 'regionidneighborhood', 'storytypeid', 'typeconstructiontypeid', 'assessmentyear','yearbuilt'\n* not significant var: 'architecturalstyletypeid','propertylandusetypeid'regionidzip'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#t test between cat variables has two levels and taxamount\nttest_ind(house_2017_full['logerror'], house_2017_full['fireplaceflag'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since the dataset has many categorical variables, and using sklean random forest requires one-hot encoding\n# so we use h2o random forest instead\n# create h2o object\nh2o.init()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove logerror outlier and other add-in variables\nhouse_2017_temp = house_2017_full[~house_2017_full['is_outlier']]\nhouse_2017_tree = house_2017_temp.drop(['logerror_zscore','is_outlier','parcelid'], axis =1)\n# house_2017_full_hf = h2o.H2OFrame(house_2017_tree)\nhouse_2017_tree.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#track the data shape\nhouse_2017_tree.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#defind the model\n# h2o_tree = H2ORandomForestEstimator(ntrees = 50, max_depth = 20, nfolds =10)\n#train the model,if x not specify,model will use all x except the y column\n# h2o_tree.train(y = 'logerror', training_frame = house_2017_full_hf)\n#print variable importance\n# h2o_tree_df = h2o_tree._model_json['output']['variable_importances'].as_data_frame()\n#visualize the importance\n\"\"\"\nplt.rcdefaults()\nfig, ax = plt.subplots(figsize = (10, 10))\nvariables = h2o_tree._model_json['output']['variable_importances']['variable']\ny_pos = np.arange(len(variables))\nscaled_importance = h2o_tree._model_json['output']['variable_importances']['scaled_importance']\nax.barh(y_pos, scaled_importance, align='center', color='green', ecolor='black')\nax.set_yticks(y_pos)\nax.set_yticklabels(variables)\nax.invert_yaxis()\nax.set_xlabel('Scaled Importance')\nax.set_title('Variable Importance')\nplt.show()\n\n#choose features have importance score >0.2\nfeature_score = 0.2\nselected_features = h2o_tree_df[h2o_tree_df.scaled_importance>=feature_score]['variable']\nselected_features\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = ['transactiondate', 'regionidneighborhood','taxamount','calculatedfinishedsquarefeet'\n                     ,'yearbuilt','lotsizesquarefeet','propertyzoningdesc','garagetotalsqft'\n                     ,'latitude','longitude','bedroomcnt','buildingqualitytypeid'\n                     ,'calculatedbathnbr','yardbuildingsqft17']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\nH2O: Missing values are interpreted as containing information (i.e., missing for a reason), rather than missing at random. During tree building, split decisions for every node are found by minimizing the loss function and treating missing values as a separate category that can go either left or right. XGBoost will automatically learn which is the best direction to go when a value is missing."},{"metadata":{},"cell_type":"markdown","source":"### Build XGboosting model by using h2o<br>\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html<br>\nhttp://uc-r.github.io/regression_preparation<br>\nhttps://dzone.com/articles/machine-learning-with-h2o-hands-on-guide-for-data<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_cols = (pd.Series(selected_features)).append(pd.Series(['logerror']))\n#split data to training and test data set\nX_train,X_test= train_test_split(house_2017_tree[selected_cols], test_size=0.33, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transfer to h2o dataframe\nX_train_h2o = h2o.H2OFrame(X_train)\nX_test_h2o = h2o.H2OFrame(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {\n      \"ntrees\" : 100\n    , \"learn_rate\" : 0.02\n    , \"max_depth\" : 10\n    , \"sample_rate\" : 0.7\n    , \"col_sample_rate_per_tree\" : 0.9\n    , \"min_rows\" : 5\n    , \"seed\": 4241\n    , \"score_tree_interval\": 100\n    ,  'nfolds': 10\n    , \"stopping_metric\" : \"MSE\"\n}\nfrom h2o.estimators import H2OXGBoostEstimator\nmodel = H2OXGBoostEstimator(**param)\nmodel.train(y = 'logerror', training_frame = X_train_h2o)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"model evaluation: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html#metric-best-practices-regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print training model summary\nprint(model.summary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the new test set metrics\nmy_metrics = model.model_performance(test_data=X_test_h2o) \nmy_metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper-Parameter Search"},{"metadata":{},"cell_type":"markdown","source":"* https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n* https://www.dataiku.com/learn/guide/code/python/advanced-xgboost-tuning.html\n* https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"},{"metadata":{},"cell_type":"markdown","source":"Row and column sampling (sample_rate and col_sample_rate) can improve generalization and lead to lower validation and test set errors. Good general values for large datasets are around 0.7 to 0.8 (sampling 70-80 percent of the data) for both parameters, as higher values generally improve training accuracy.<br>\ncol_sample_rate_per_tree: This option specifies the column sampling rate for each tree (without replacement). This can be a value from 0.0 to 1.0 and defaults to 1. Note that it is multiplicative with col_sample_rate, so setting both parameters to 0.8, for example, results in 64% of columns being considered at any given node to split."},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_params = {'max_depth' : [4,6,8,12,16,20]\n               ,\"learn_rate\" : [0.1, 0.01, 0.0001]\n               }\nparam_grid = {\n      \"ntrees\" : 50\n    , \"sample_rate\" : 0.7\n    , \"col_sample_rate_per_tree\" : 0.9\n    , \"min_rows\" : 5\n    , \"seed\": 4241\n    , \"score_tree_interval\": 100\n    ,  'nfolds': 10\n    , \"stopping_metric\" : \"MSE\"\n}\nmodel_grid = H2OXGBoostEstimator(**param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build grid search with previously made GBM and hyper parameters\n#In a cartesian grid search, users specify a set of values for each hyperparamter that they want to search over, and H2O will train a model for every combination of the hyperparameter values. This means that if you have three hyperparameters and you specify 5, 10 and 2 values for each, your grid will contain a total of 5*10*2 = 100 models.\n\"\"\"\ngrid = H2OGridSearch(model_grid,hyper_params,\n                         grid_id = 'depth_grid',\n                         search_criteria = {'strategy': \"Cartesian\"})\n\n\n#Train grid search\ngrid.train(y='logerror',\n           training_frame = X_train_h2o)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print the grid search results\n# Get the grid results, sorted by validation AUC\n\"\"\"\nxgb_gridperf = grid.get_grid(sort_by='mse', decreasing=True)\nxgb_gridperf\n\n# Grab the top xgb model, chosen by validation mse\nbest_xgb = xgb_gridperf.models[17]\n\n# Now let's evaluate the model performance on a test set\n# so we get an honest estimate of top model performance\nbest_xgb_model = best_xgb.model_performance(test_data=X_test_h2o)\n#0.1         6   depth_grid_model_4  0.006894977436261656\n\nbest_xgb_model.mse()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build the best model based on previous training\nbest_param = {\n      \"ntrees\" : 100\n    , \"learn_rate\" : 0.1\n    , \"max_depth\" : 6\n    , \"sample_rate\" : 0.7\n    , \"col_sample_rate_per_tree\" : 0.9\n    , \"min_rows\" : 5\n    , \"seed\": 4241\n    , \"score_tree_interval\": 100\n    ,  'nfolds': 10\n    , \"stopping_metric\" : \"MSE\"\n}\n\nbest_model = H2OXGBoostEstimator(**best_param)\nbest_model.train(y = 'logerror', training_frame = X_train_h2o)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the test set metrics for the best model\nbest_metrics = best_model.model_performance(test_data=X_test_h2o) \nbest_metrics","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}