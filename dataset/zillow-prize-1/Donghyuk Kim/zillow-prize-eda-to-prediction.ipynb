{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Zillow Prize\n\nThis notebook was created with reference to [\"How to Compete for Zillow Prize at Kaggle\"](https://towardsdatascience.com/how-to-compete-for-zillow-prize-at-kaggle-535852243906)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb\nimport random\nimport datetime as dt\nimport gc\nimport seaborn as sns #python visualization library\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\ncolor = sns.color_palette()\nnp.random.seed(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset path\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load 2016 Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/zillow-prize-1/train_2016_v2.csv', parse_dates=['transactiondate'])\nproperties = pd.read_csv('/kaggle/input/zillow-prize-1/properties_2016.csv')\ntest = pd.read_csv('/kaggle/input/zillow-prize-1/sample_submission.csv')\ntest= test.rename(columns={'ParcelId': 'parcelid'})  # To make it easier for merging datasets on same column_id later\n\nprint(f'train.shape: {train.shape}')\nprint(f'properties.shape: {properties.shape}')\nprint(f'test.shape: {test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Type Converting\n\nThe processing of some of the algorithms can be made quick if data representation is made in int/float32 instead of int/float64. Therefore, in order to make sure that all of our columns types are in float32, we are implementing the following lines of code"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert properties df\nfor c, dtype in zip(properties.columns, properties.dtypes):\n    if dtype == np.float64:\n        properties[c] = properties[c].astype(np.float32)  # np.float64 -> np.float32\n    if dtype == np.int64:\n        properties[c] = properties[c].astype(np.int32)  # np.int64 -> np.int32\n\n# convert test df\nfor column in test.columns:\n    if test[column].dtype == int:\n        test[column] = test[column].astype(np.int32)  # int -> np.int32\n    if test[column].dtype == float:\n        test[column] = test[column].astype(np.float32)  # float -> np.float32","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# New Features\n# living area proportions\nproperties['living_area_prop'] = properties['calculatedfinishedsquarefeet'] / properties['lotsizesquarefeet']\n# tax value ratio\nproperties['value_ratio'] = properties['taxvaluedollarcnt'] / properties['taxamount']\n# tax value proportions\nproperties['value_prop'] = properties['structuretaxvaluedollarcnt'] / properties['landtaxvaluedollarcnt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the Datasets\n# We are merging the properties dataset with training and testing dataset for model building and testing prediction\ndf_train = train.merge(properties, how='left', on='parcelid')\ndf_test = test.merge(properties, how='left', on='parcelid')\n\n# Remove previos variables to keep some memory\ndel properties, train\ngc.collect()\nprint('Memory usage reduction…')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some scaling\ndf_train[['latitude', 'longitude']] /= 1e6\ndf_test[['latitude', 'longitude']] /= 1e6\ndf_train['censustractandblock'] /= 1e12\ndf_test['censustractandblock'] /= 1e12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nlbl = LabelEncoder()\n\n# encoding df_train\nfor c in df_train.columns:\n    df_train[c]=df_train[c].fillna(0)\n    if df_train[c].dtype == 'object':\n        lbl.fit(list(df_train[c].values))\n        df_train[c] = lbl.transform(list(df_train[c].values))\n        \n# encoding df_test        \nfor c in df_test.columns:\n    df_test[c]=df_test[c].fillna(0)\n    if df_test[c].dtype == 'object':\n        lbl.fit(list(df_test[c].values))\n        df_test[c] = lbl.transform(list(df_test[c].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rearranging the DataSets\n# We will now drop the features that serve no useful purpose\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\nx_test = df_test.drop(['parcelid', 'propertyzoningdesc', 'propertycountylandusecode', '201610', '201611', '201612', '201710', '201711', '201712'], axis = 1)\n\nprint(f'x_train.shape: {x_train.shape}')\nprint(f'x_test.shape: {x_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Split\n\nWe are dividing our datasets into the training and validation sets so that we could monitor and the test the progress of our machine learning algorithm. This would let us know when our model might be over or under fitting on the dataset that we have employed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split dataset for cross validation\nx_train = x_train.values\ny_train = df_train['logerror'].values\nX = x_train\ny = y_train\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f'Xtrain.shape: {Xtrain.shape}')\nprint(f'Xvalid.shape: {Xvalid.shape}')\nprint(f'ytrain.shape: {ytrain.shape}')\nprint(f'yvalid.shape: {yvalid.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Model: XGBoost\n\nImplement the Xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can now select the parameters for Xgboost and monitor the progress of results on our validation set.\ndtrain = xgb.DMatrix(Xtrain, label=ytrain)\ndvalid = xgb.DMatrix(Xvalid, label=yvalid)\ndtest = xgb.DMatrix(x_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(dtrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try different parameters\nxgb_params = {\n    'min_child_weight': 5, 'eta': 0.035, 'colsample_bytree': 0.5, 'max_depth': 4,\n    'subsample': 0.85, 'lambda': 0.8, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n    'eval_metric': 'mae', 'objective': 'reg:linear'\n}\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Train\nmodel_xgb = xgb.train(xgb_params, dtrain, 1000, watchlist, early_stopping_rounds=100,maximize=False, verbose_eval=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction\n\nLet us now predict the target variable for our test dataset. All we have to do now is just fit the already trained model on the test set that we had made merging the sample file with properties dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the results\nPredicted_test_xgb = model_xgb.predict(dtest)  # ndarray\nprint(f'Predicted_test_xgb.shape: {Predicted_test_xgb}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submitting the Results\nsample_file = pd.read_csv('/kaggle/input/zillow-prize-1/sample_submission.csv')\nprint(f'sample_file.shape: {sample_file.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_file.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submitting the Results\nfor c in sample_file.columns[sample_file.columns != 'ParcelId']:\n    sample_file[c] = Predicted_test_xgb\n    print('Preparing the csv file …')\n    \n# write csv file\nsample_file.to_csv('xgb_predicted_results.csv', index=False, float_format='%.4f')\nprint('Finished writing the file')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}