{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA + tensorflow object detection API training\nThis kernel aims to: \n- explore the dataset for the competition [TensorFlow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef/overview/description)\n- show how to train a TensorFlow Object Detection API model and do transfer learning for this task (this part is taken from https://www.kaggle.com/khanhlvg/cots-detection-w-tensorflow-object-detection-api/)","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents:\n* **1. [TensorFlow Object Detection API installation and Libraries](#Libraries)** <br>\n* **2. [Data Analysis](#Data_Analysis)** <br>\n 2.0  [Helper functions](#Helper_functions) <br>\n 2.1. [Video Id](#Video_Id) <br>\n 2.2. [Sequences](#Sequences) <br>\n 2.3. [Annotations](#Annotations) <br>\n 2.4. [Image visualiation](#Image_visualization) <br>\n* **3. [Data Preparation and install API](#Data_Preparation)** <br>\n* **4. [Model](#Model)** <br>\n* **5. [Results](#Results)** <br>\n* **6. [Zip and Download trained model](#Download)** <br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Libraries\"></a> <br> \n# **1. TensorFlow Object Detection API installation and Libraries** ","metadata":{}},{"cell_type":"markdown","source":"## Install TensorFlow Object Detection API","metadata":{"papermill":{"duration":0.021045,"end_time":"2021-11-19T08:38:07.403389","exception":false,"start_time":"2021-11-19T08:38:07.382344","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Clone github project","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/tensorflow/models\n    \n# Check out a certain commit to ensure that future changes in the TF ODT API codebase won't affect this notebook.\n!cd models && git checkout ac8d06519","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":25.822618,"end_time":"2021-11-19T08:38:33.247836","exception":false,"start_time":"2021-11-19T08:38:07.425218","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-12T13:19:10.699775Z","iopub.execute_input":"2022-02-12T13:19:10.700044Z","iopub.status.idle":"2022-02-12T13:19:33.326849Z","shell.execute_reply.started":"2022-02-12T13:19:10.700008Z","shell.execute_reply":"2022-02-12T13:19:33.326051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Install tensorflow object detection API","metadata":{}},{"cell_type":"code","source":"%%bash\ncd models/research\n\n# Compile protos.\nprotoc object_detection/protos/*.proto --python_out=.\n\nwget https://storage.googleapis.com/odml-dataset/others/setup.py\npip install -q --user .\n\n# Test if the Object Dectection API is working correctly\npython object_detection/builders/model_builder_tf2_test.py","metadata":{"papermill":{"duration":79.154739,"end_time":"2021-11-19T08:39:52.448588","exception":false,"start_time":"2021-11-19T08:38:33.293849","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-12T13:19:34.37946Z","iopub.execute_input":"2022-02-12T13:19:34.380039Z","iopub.status.idle":"2022-02-12T13:21:01.840937Z","shell.execute_reply.started":"2022-02-12T13:19:34.379996Z","shell.execute_reply":"2022-02-12T13:21:01.840087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import dependencies","metadata":{}},{"cell_type":"code","source":"# Libraries\nimport os\nimport io\nimport json\nimport sys\nimport cv2\nfrom PIL import Image, ImageDraw\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mply\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport contextlib2\nimport IPython\nimport time\nimport pathlib\nimport tensorflow as tf\nimport random\nfrom PIL import Image, ImageDraw","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:01.842792Z","iopub.execute_input":"2022-02-12T13:21:01.843472Z","iopub.status.idle":"2022-02-12T13:21:04.035849Z","shell.execute_reply.started":"2022-02-12T13:21:01.843424Z","shell.execute_reply":"2022-02-12T13:21:04.035129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR = '/kaggle/input/tensorflow-great-barrier-reef/'\nsys.path.insert(0, INPUT_DIR)","metadata":{"papermill":{"duration":1.526137,"end_time":"2021-11-19T08:39:54.022253","exception":false,"start_time":"2021-11-19T08:39:52.496116","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-12T13:21:04.036976Z","iopub.execute_input":"2022-02-12T13:21:04.037953Z","iopub.status.idle":"2022-02-12T13:21:04.041186Z","shell.execute_reply.started":"2022-02-12T13:21:04.037918Z","shell.execute_reply":"2022-02-12T13:21:04.040532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The notebook is supposed to run with TF 2.6.0\nprint(tf.__version__)\nprint(tf.test.is_gpu_available())\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"papermill":{"duration":0.729459,"end_time":"2021-11-19T08:39:54.798333","exception":false,"start_time":"2021-11-19T08:39:54.068874","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-12T13:21:04.043321Z","iopub.execute_input":"2022-02-12T13:21:04.043721Z","iopub.status.idle":"2022-02-12T13:21:04.731766Z","shell.execute_reply.started":"2022-02-12T13:21:04.043685Z","shell.execute_reply":"2022-02-12T13:21:04.730631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Data_Analysis\"></a> <br> \n# **2. Data Analysis** ","metadata":{}},{"cell_type":"code","source":"# Read metadata\ntrain_df = pd.read_csv(os.path.join(INPUT_DIR,\"train.csv\"))\ntest_df =  pd.read_csv(os.path.join(INPUT_DIR,\"test.csv\"))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:04.733975Z","iopub.execute_input":"2022-02-12T13:21:04.734834Z","iopub.status.idle":"2022-02-12T13:21:04.795552Z","shell.execute_reply.started":"2022-02-12T13:21:04.73479Z","shell.execute_reply":"2022-02-12T13:21:04.794788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:04.797864Z","iopub.execute_input":"2022-02-12T13:21:04.798536Z","iopub.status.idle":"2022-02-12T13:21:04.806887Z","shell.execute_reply.started":"2022-02-12T13:21:04.798497Z","shell.execute_reply":"2022-02-12T13:21:04.805858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:04.808567Z","iopub.execute_input":"2022-02-12T13:21:04.808956Z","iopub.status.idle":"2022-02-12T13:21:04.827141Z","shell.execute_reply.started":"2022-02-12T13:21:04.808869Z","shell.execute_reply":"2022-02-12T13:21:04.826308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:04.828371Z","iopub.execute_input":"2022-02-12T13:21:04.828677Z","iopub.status.idle":"2022-02-12T13:21:04.834939Z","shell.execute_reply.started":"2022-02-12T13:21:04.828642Z","shell.execute_reply":"2022-02-12T13:21:04.83379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:04.836565Z","iopub.execute_input":"2022-02-12T13:21:04.83695Z","iopub.status.idle":"2022-02-12T13:21:04.849829Z","shell.execute_reply.started":"2022-02-12T13:21:04.836908Z","shell.execute_reply":"2022-02-12T13:21:04.849041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have looked at the content of [train/test].csv - \nThese files contains the metadata for the images. Most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download.\n\nWe have 6 columns (train data has also the annotations for each frame):\n* **video_id** - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n* **video_frame** - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n* **sequence - ID** of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n* **sequence_frame** - The frame number within a given sequence.\n* **image_id** - ID code for the image, in the format '{video_id}-{video_frame}'\n* **annotations** - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate (x_min, y_min) of its upper left corner within the image together with its width and height in pixels.","metadata":{}},{"cell_type":"code","source":"# Look at the data types\ntrain_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:04.853183Z","iopub.execute_input":"2022-02-12T13:21:04.853408Z","iopub.status.idle":"2022-02-12T13:21:04.85956Z","shell.execute_reply.started":"2022-02-12T13:21:04.853384Z","shell.execute_reply":"2022-02-12T13:21:04.858593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_df['annotations'][0])","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:04.861132Z","iopub.execute_input":"2022-02-12T13:21:04.861638Z","iopub.status.idle":"2022-02-12T13:21:04.870592Z","shell.execute_reply.started":"2022-02-12T13:21:04.861601Z","shell.execute_reply":"2022-02-12T13:21:04.869649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See if the training dataset contains null values\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:04.87204Z","iopub.execute_input":"2022-02-12T13:21:04.872408Z","iopub.status.idle":"2022-02-12T13:21:04.888373Z","shell.execute_reply.started":"2022-02-12T13:21:04.872369Z","shell.execute_reply":"2022-02-12T13:21:04.887562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Helper_functions\"></a> <br> \n# **2.0 Helper functions** ","metadata":{}},{"cell_type":"code","source":"def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n    '''Plots the value at the end of the a seaborn barplot.\n    axs: the ax of the plot\n    h_v: weather or not the barplot is vertical/ horizontal'''\n    \n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() / 2\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_height())\n                ax.text(_x, _y, format(value, ','), ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_width())\n                ax.text(_x, _y, format(value, ','), ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n\n        \n#----------------------------------------------\ndef show_image(path, annot, axs=None):\n    '''Shows an image and marks any COTS annotated within the frame.\n    path: full path to the .jpg image\n    annot: string of the annotation for the coordinates of COTS'''\n    \n    # This is in case we plot only 1 image\n    if axs==None:\n        fig, axs = plt.subplots(figsize=(23, 8))\n    \n    img = plt.imread(path)\n    axs.imshow(img)\n\n    if annot:\n        for a in eval(annot):\n            rect = patches.Rectangle((a[\"x\"], a[\"y\"]), a[\"width\"], a[\"height\"], \n                                     linewidth=3, edgecolor=\"#FF6103\", facecolor='none')\n            axs.add_patch(rect)\n\n    axs.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:04.890003Z","iopub.execute_input":"2022-02-12T13:21:04.891197Z","iopub.status.idle":"2022-02-12T13:21:04.904344Z","shell.execute_reply.started":"2022-02-12T13:21:04.89116Z","shell.execute_reply":"2022-02-12T13:21:04.903551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Video_Id\"></a> <br> \n# **2.1 Video Id** ","metadata":{}},{"cell_type":"code","source":"# count the number of occurences in a video\nfig_dims = (10, 8)\nfig, ax = plt.subplots(figsize=fig_dims)\ndf1 = train_df[\"video_id\"].value_counts().reset_index()\nsns.barplot(data=df1, x=\"index\", y=\"video_id\", ax=ax,\n            palette=[\"r\",\"g\",\"b\"])\nshow_values_on_bars(ax, h_v=\"v\", space=0.1)\nax.set_xlabel(\"Video ID\")\nax.set_ylabel(\"\")\nax.title.set_text(\"Frequency of Frames per Video\")\nax.set_yticks([])","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:04.906875Z","iopub.execute_input":"2022-02-12T13:21:04.907355Z","iopub.status.idle":"2022-02-12T13:21:05.050092Z","shell.execute_reply.started":"2022-02-12T13:21:04.907314Z","shell.execute_reply":"2022-02-12T13:21:05.04939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have only 3 videos. The third video (index 2) has a large number of frames.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Sequences\"></a> <br> \n# **2.2 Sequences** ","metadata":{}},{"cell_type":"code","source":"groups = train_df.groupby([\"video_id\",\"sequence\"]).size()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:05.051451Z","iopub.execute_input":"2022-02-12T13:21:05.051855Z","iopub.status.idle":"2022-02-12T13:21:05.059569Z","shell.execute_reply.started":"2022-02-12T13:21:05.051816Z","shell.execute_reply":"2022-02-12T13:21:05.058815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"groups","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:05.061359Z","iopub.execute_input":"2022-02-12T13:21:05.061633Z","iopub.status.idle":"2022-02-12T13:21:05.071851Z","shell.execute_reply.started":"2022-02-12T13:21:05.061596Z","shell.execute_reply":"2022-02-12T13:21:05.071117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('sequences first video: {} \\nsequences second video: {} \\nsequences third video: {}'.format(len(groups[0]),len(groups[1]),len(groups[2])))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:05.073332Z","iopub.execute_input":"2022-02-12T13:21:05.07392Z","iopub.status.idle":"2022-02-12T13:21:05.080918Z","shell.execute_reply.started":"2022-02-12T13:21:05.073882Z","shell.execute_reply":"2022-02-12T13:21:05.080123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 3 videos, the first and the second video contains 8 sequences each, the third video contains only 4 sequences but has more frames (see previous section).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Annotations\"></a> <br> \n# **2.3 Annotations** ","metadata":{}},{"cell_type":"code","source":"# Calculate the number of total number of annotations within each frame. \ntrain_df[\"no_annotations\"] = train_df[\"annotations\"].apply(lambda x: len(eval(x)))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:05.08205Z","iopub.execute_input":"2022-02-12T13:21:05.082918Z","iopub.status.idle":"2022-02-12T13:21:05.298256Z","shell.execute_reply.started":"2022-02-12T13:21:05.082881Z","shell.execute_reply":"2022-02-12T13:21:05.297505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (10, 8)\nfig, ax = plt.subplots(figsize=fig_dims)\ntrain_df[\"no_annotations\"].hist()\nn_of_images = len(train_df)\nno_annotations = round(train_df[train_df.no_annotations==0].shape[0])\nwith_annotations = round(train_df[train_df.no_annotations!=0].shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:05.299947Z","iopub.execute_input":"2022-02-12T13:21:05.300393Z","iopub.status.idle":"2022-02-12T13:21:05.564435Z","shell.execute_reply.started":"2022-02-12T13:21:05.300356Z","shell.execute_reply":"2022-02-12T13:21:05.563771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total number of frames: {} \\nframes with annotations: {} \\nframes without annotations: {}'.format(n_of_images,with_annotations,no_annotations))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:05.56564Z","iopub.execute_input":"2022-02-12T13:21:05.565907Z","iopub.status.idle":"2022-02-12T13:21:05.57212Z","shell.execute_reply.started":"2022-02-12T13:21:05.565871Z","shell.execute_reply":"2022-02-12T13:21:05.571291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that most of the frame have only 1 annotion. We have 23501 total frames, 18582 with annotations and 4919 without annotations.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Image_visualization\"></a> <br> \n# **2.5 Image visualization** ","metadata":{}},{"cell_type":"code","source":"# Create a \"path\" column containing full path to the frames\nbase_folder = os.path.join(INPUT_DIR,\"train_images\")\n\ntrain_df[\"path\"] = base_folder + \"/video_\" + \\\n                         train_df['video_id'].astype(str) + \"/\" +\\\n                         train_df['video_frame'].astype(str) +\".jpg\"\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:05.573757Z","iopub.execute_input":"2022-02-12T13:21:05.574247Z","iopub.status.idle":"2022-02-12T13:21:05.658683Z","shell.execute_reply.started":"2022-02-12T13:21:05.574208Z","shell.execute_reply":"2022-02-12T13:21:05.657923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look for an image with some annotations\nimage_path = list(train_df[train_df[\"no_annotations\"] == 4][\"path\"])[0]\nannotation = list(train_df[train_df[\"no_annotations\"] == 4][\"annotations\"])[0]\nshow_image(image_path, annotation, axs=None)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:05.660054Z","iopub.execute_input":"2022-02-12T13:21:05.660313Z","iopub.status.idle":"2022-02-12T13:21:06.221435Z","shell.execute_reply.started":"2022-02-12T13:21:05.66028Z","shell.execute_reply":"2022-02-12T13:21:06.220656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Data_Preparation\"></a> <br> \n# **3. Data Preparation** ","metadata":{}},{"cell_type":"code","source":"train_df=train_df.loc[train_df[\"annotations\"].astype(str) != \"[]\"] # remove images without annotations\ntrain_df['annotations'] = train_df['annotations'].apply(eval)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:06.222535Z","iopub.execute_input":"2022-02-12T13:21:06.222765Z","iopub.status.idle":"2022-02-12T13:21:06.521739Z","shell.execute_reply.started":"2022-02-12T13:21:06.222735Z","shell.execute_reply":"2022-02-12T13:21:06.521014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_RATIO = 0.8\n\n# Split the dataset so that no sequence is leaked from the training dataset into the validation dataset.\nsplit_index = int(TRAINING_RATIO * len(train_df))\nwhile train_df.iloc[split_index - 1].sequence == train_df.iloc[split_index].sequence:\n    split_index += 1\n    \n# Shuffle both the training and validation datasets.\ntrain_data_df = train_df.iloc[:split_index].sample(frac=1).reset_index(drop=True)\nval_data_df = train_df.iloc[split_index:].sample(frac=1).reset_index(drop=True)\n\nprint('Training ratio:', \n      float(len(train_data_df)) / (len(train_data_df) + len(val_data_df)))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:06.523137Z","iopub.execute_input":"2022-02-12T13:21:06.523404Z","iopub.status.idle":"2022-02-12T13:21:06.604361Z","shell.execute_reply.started":"2022-02-12T13:21:06.523369Z","shell.execute_reply":"2022-02-12T13:21:06.603632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_df","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:06.605738Z","iopub.execute_input":"2022-02-12T13:21:06.606014Z","iopub.status.idle":"2022-02-12T13:21:06.61047Z","shell.execute_reply.started":"2022-02-12T13:21:06.605979Z","shell.execute_reply":"2022-02-12T13:21:06.609719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert the training and validation dataset into TFRecord format as required by the TensorFlow Object Detection API.","metadata":{"papermill":{"duration":0.115403,"end_time":"2021-11-19T08:40:50.494003","exception":false,"start_time":"2021-11-19T08:40:50.3786","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from object_detection.utils import dataset_util\nfrom object_detection.dataset_tools import tf_record_creation_util\nfrom object_detection.utils import visualization_utils as viz_utils\nfrom object_detection.utils import label_map_util\n\ndef create_tf_example(video_id, video_frame, data_df, image_path):\n    \"\"\"Create a tf.Example entry for a given training image.\"\"\"\n    full_path = os.path.join(image_path, os.path.join(f'video_{video_id}', f'{video_frame}.jpg'))\n    with tf.io.gfile.GFile(full_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n\n    height = image.size[1] # Image height\n    width = image.size[0] # Image width\n    filename = f'{video_id}:{video_frame}'.encode('utf8') # Unique id of the image.\n    encoded_image_data = None # Encoded image bytes\n    image_format = 'jpeg'.encode('utf8') # b'jpeg' or b'png'\n\n    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = [] # List of normalized right x coordinates in bounding box\n             # (1 per box)\n    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n             # (1 per box)\n    classes_text = [] # List of string class name of bounding box (1 per box)\n    classes = [] # List of integer class id of bounding box (1 per box)\n    \n    rows = data_df[(data_df.video_id == video_id) & (data_df.video_frame == video_frame)]\n    for _, row in rows.iterrows():\n        for annotation in row.annotations:\n            xmins.append(annotation['x'] / width) \n            xmaxs.append((annotation['x'] + annotation['width']) / width) \n            ymins.append(annotation['y'] / height) \n            ymaxs.append((annotation['y'] + annotation['height']) / height) \n\n            classes_text.append('COTS'.encode('utf8'))\n            classes.append(1)\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image/height': dataset_util.int64_feature(height),\n      'image/width': dataset_util.int64_feature(width),\n      'image/filename': dataset_util.bytes_feature(filename),\n      'image/source_id': dataset_util.bytes_feature(filename),\n      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image/format': dataset_util.bytes_feature(image_format),\n      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n      'image/object/class/label': dataset_util.int64_list_feature(classes),\n    }))\n    \n    return tf_example\n\n\ndef convert_to_tfrecord(data_df, tfrecord_filebase, image_path, num_shards = 10):\n    \"\"\"Convert the object detection dataset to TFRecord as required by the TF ODT API.\"\"\"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n            tf_record_close_stack, tfrecord_filebase, num_shards)\n        \n        for index, row in data_df.iterrows():\n            if index % 500 == 0:\n                print('Processed {0} images.'.format(index))\n            tf_example = create_tf_example(row.video_id, row.video_frame, data_df, image_path)\n            output_shard_index = index % num_shards\n            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())\n        \n        print('Completed processing {0} images.'.format(len(data_df)))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:21:06.611853Z","iopub.execute_input":"2022-02-12T13:21:06.612392Z","iopub.status.idle":"2022-02-12T13:21:06.688862Z","shell.execute_reply.started":"2022-02-12T13:21:06.612354Z","shell.execute_reply":"2022-02-12T13:21:06.688122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir dataset\nimage_path = os.path.join(INPUT_DIR, 'train_images')\n\n# Convert train images to TFRecord\nprint('Converting TRAIN images...')\nconvert_to_tfrecord(\n  train_data_df,\n  'dataset/cots_train',\n  image_path,\n  num_shards = 4\n)\n\n# Convert validation images to TFRecord\nprint('Converting VALIDATION images...')\nconvert_to_tfrecord(\n  val_data_df,\n  'dataset/cots_val',\n  image_path,\n  num_shards = 4\n)","metadata":{"papermill":{"duration":38.916848,"end_time":"2021-11-19T08:41:29.641271","exception":false,"start_time":"2021-11-19T08:40:50.724423","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-12T13:21:06.690873Z","iopub.execute_input":"2022-02-12T13:21:06.691284Z","iopub.status.idle":"2022-02-12T13:22:06.788652Z","shell.execute_reply.started":"2022-02-12T13:21:06.691247Z","shell.execute_reply":"2022-02-12T13:22:06.787757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a label map to map between label index and human-readable label name.\n\nlabel_map_str = \"\"\"item {\n  id: 1\n  name: 'COTS'\n}\"\"\"\n\nwith open('dataset/label_map.pbtxt', 'w') as f:\n  f.write(label_map_str)\n\n!more dataset/label_map.pbtxt","metadata":{"papermill":{"duration":16.05844,"end_time":"2021-11-19T08:41:45.838392","exception":false,"start_time":"2021-11-19T08:41:29.779952","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-12T13:22:06.793729Z","iopub.execute_input":"2022-02-12T13:22:06.793968Z","iopub.status.idle":"2022-02-12T13:22:07.543512Z","shell.execute_reply.started":"2022-02-12T13:22:06.793938Z","shell.execute_reply":"2022-02-12T13:22:07.542533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Model\"></a> <br> \n# **4. Model** ","metadata":{}},{"cell_type":"markdown","source":"## Train an object detection model\n\nI'll use [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) and an [EfficientDet-D0](https://arxiv.org/pdf/1911.09070v7.pdf) base model and apply transfer learning to train a COTS detection model. ","metadata":{"papermill":{"duration":3.355764,"end_time":"2021-11-19T08:41:50.909779","exception":false,"start_time":"2021-11-19T08:41:47.554015","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Download the pretrained EfficientDet-D0 checkpoint","metadata":{}},{"cell_type":"code","source":"!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n!tar -xvzf efficientdet_d0_coco17_tpu-32.tar.gz","metadata":{"papermill":{"duration":3.256545,"end_time":"2021-11-19T08:41:54.358822","exception":false,"start_time":"2021-11-19T08:41:51.102277","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-12T13:22:07.547021Z","iopub.execute_input":"2022-02-12T13:22:07.547293Z","iopub.status.idle":"2022-02-12T13:22:09.96213Z","shell.execute_reply.started":"2022-02-12T13:22:07.547263Z","shell.execute_reply":"2022-02-12T13:22:09.961228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare templete for model configuration\n\nFor this task we need to copy the content of the pipeline.config file (this file can be found in the dataset repository) and modify some fields:\n\n- *num_classes* should be changed to 1\n- *fine_tune_checkpoint* should be set to the checkpoint path\n- *label_map_path* inside train_input_reader should be set to the labels file\n- *label_map_path* inside eval_input_reader should be set to the labels file\n- *input_path* inside train_input_reader should be set to the train/validation dataset\n- *input_path* inside eval_input_reader should be set to the train/validation dataset\n\nOther parameters can be changed to optimize the model","metadata":{}},{"cell_type":"code","source":"from string import Template\n\nconfig_file_template = \"\"\"\n# SSD with EfficientNet-b0 + BiFPN feature extractor,\n# shared box predictor and focal loss (a.k.a EfficientDet-d0).\n# See EfficientDet, Tan et al, https://arxiv.org/abs/1911.09070\n# See Lin et al, https://arxiv.org/abs/1708.02002\n# Initialized from an EfficientDet-D0 checkpoint.\n#\n# Train on GPU\n\nmodel {\n  ssd {\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n    num_classes: 1\n    add_background_class: false\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    encode_background_as_zeros: true\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: [1.0, 2.0, 0.5]\n        scales_per_octave: 3\n      }\n    }\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 1280\n        max_dimension: 1280\n        pad_to_max_dimension: true\n        }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        depth: 64\n        class_prediction_bias_init: -4.6\n        conv_hyperparams {\n          force_use_bias: true\n          activation: SWISH\n          regularizer {\n            l2_regularizer {\n              weight: 0.00004\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              stddev: 0.01\n              mean: 0.0\n            }\n          }\n          batch_norm {\n            scale: true\n            decay: 0.99\n            epsilon: 0.001\n          }\n        }\n        num_layers_before_predictor: 3\n        kernel_size: 3\n        use_depthwise: true\n      }\n    }\n    feature_extractor {\n      type: 'ssd_efficientnet-b0_bifpn_keras'\n      bifpn {\n        min_level: 3\n        max_level: 7\n        num_iterations: 3\n        num_filters: 64\n      }\n      conv_hyperparams {\n        force_use_bias: true\n        activation: SWISH\n        regularizer {\n          l2_regularizer {\n            weight: 0.00004\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            stddev: 0.03\n            mean: 0.0\n          }\n        }\n        batch_norm {\n          scale: true,\n          decay: 0.99,\n          epsilon: 0.001,\n        }\n      }\n    }\n    loss {\n      classification_loss {\n        weighted_sigmoid_focal {\n          alpha: 0.25\n          gamma: 1.5\n        }\n      }\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    normalize_loss_by_num_matches: true\n    normalize_loc_loss_by_codesize: true\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 1e-8\n        iou_threshold: 0.5\n        max_detections_per_class: 100\n        max_total_detections: 100\n      }\n      score_converter: SIGMOID\n    }\n  }\n}\n\ntrain_config: {\n  fine_tune_checkpoint: \"efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\"\n  fine_tune_checkpoint_version: V2\n  fine_tune_checkpoint_type: \"detection\"\n  batch_size: 2\n  sync_replicas: false\n  startup_delay_steps: 0\n  replicas_to_aggregate: 1\n  use_bfloat16: false\n  num_steps: $training_steps\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_scale_crop_and_pad_to_square {\n      output_size: 1280\n      scale_min: 0.5\n      scale_max: 2.0\n    }\n  }\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        cosine_decay_learning_rate {\n          learning_rate_base: 5e-3\n          total_steps: $training_steps\n          warmup_learning_rate: 5e-4\n          warmup_steps: $warmup_steps\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n}\n\ntrain_input_reader: {\n  label_map_path: \"dataset/label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"dataset/cots_train-?????-of-00004\"\n  }\n}\n\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 2;\n}\n\neval_input_reader: {\n  label_map_path: \"dataset/label_map.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"dataset/cots_val-?????-of-00004\"\n  }\n}\n\"\"\"","metadata":{"papermill":{"duration":0.133468,"end_time":"2021-11-19T08:41:54.609774","exception":false,"start_time":"2021-11-19T08:41:54.476306","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-12T13:22:09.96516Z","iopub.execute_input":"2022-02-12T13:22:09.965459Z","iopub.status.idle":"2022-02-12T13:22:09.974645Z","shell.execute_reply.started":"2022-02-12T13:22:09.965419Z","shell.execute_reply":"2022-02-12T13:22:09.97373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define the training pipeline","metadata":{}},{"cell_type":"code","source":"# Here I redefine the training and warmup steps\n# Note. by setting TRAINING_STEPS = 20000 and WARMUP_STEPS = 2000 --> I can obtaina  score of 0.335\nTRAINING_STEPS = 28000 # change to improve results\nWARMUP_STEPS = 2000 # change to improve results\nPIPELINE_CONFIG_PATH='dataset/pipeline.config'\n\npipeline = Template(config_file_template).substitute(\n    training_steps=TRAINING_STEPS, warmup_steps=WARMUP_STEPS)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","metadata":{"papermill":{"duration":0.121946,"end_time":"2021-11-19T08:41:54.846958","exception":false,"start_time":"2021-11-19T08:41:54.725012","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-12T13:22:09.975999Z","iopub.execute_input":"2022-02-12T13:22:09.976925Z","iopub.status.idle":"2022-02-12T13:22:09.986566Z","shell.execute_reply.started":"2022-02-12T13:22:09.976888Z","shell.execute_reply":"2022-02-12T13:22:09.985845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the object detection model","metadata":{}},{"cell_type":"code","source":"MODEL_DIR='cots_efficientdet_d0'\n!mkdir {MODEL_DIR}\n!python models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --alsologtostderr","metadata":{"papermill":{"duration":18463.625406,"end_time":"2021-11-19T13:49:38.586506","exception":false,"start_time":"2021-11-19T08:41:54.9611","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-12T13:22:09.987883Z","iopub.execute_input":"2022-02-12T13:22:09.988474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the object detection model","metadata":{"papermill":{"duration":0.226793,"end_time":"2021-11-19T13:49:39.043281","exception":false,"start_time":"2021-11-19T13:49:38.816488","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!python models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --checkpoint_dir={MODEL_DIR} \\\n    --eval_timeout=0 \\\n    --alsologtostderr","metadata":{"papermill":{"duration":323.73381,"end_time":"2021-11-19T13:55:03.004285","exception":false,"start_time":"2021-11-19T13:49:39.270475","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Export as SavedModel for inference","metadata":{"papermill":{"duration":0.239823,"end_time":"2021-11-19T13:55:03.483464","exception":false,"start_time":"2021-11-19T13:55:03.243641","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!python models/research/object_detection/exporter_main_v2.py \\\n    --input_type image_tensor \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --trained_checkpoint_dir={MODEL_DIR} \\\n    --output_directory={MODEL_DIR}/output","metadata":{"papermill":{"duration":122.093553,"end_time":"2021-11-19T13:57:05.815158","exception":false,"start_time":"2021-11-19T13:55:03.721605","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {MODEL_DIR}/output","metadata":{"papermill":{"duration":0.931074,"end_time":"2021-11-19T13:57:06.998967","exception":false,"start_time":"2021-11-19T13:57:06.067893","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_DIR","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Results\"></a> <br> \n# **5. Results** ","metadata":{}},{"cell_type":"code","source":"# Define some utils method for prediction and display images\ndef load_image_into_numpy_array(path):\n    \"\"\"Load an image from file into a numpy array.\n\n    Puts image into numpy array to feed into tensorflow graph.\n    Note that by convention we put it into a numpy array with shape\n    (height, width, channels), where channels=3 for RGB.\n\n    Args:\n    path: a file path (this can be local or on colossus)\n\n    Returns:\n    uint8 numpy array with shape (img_height, img_width, 3)\n    \"\"\"\n    img_data = tf.io.gfile.GFile(path, 'rb').read()\n    image = Image.open(io.BytesIO(img_data))\n    (im_width, im_height) = image.size\n    \n    return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n\ndef detect(image_np, model):\n    \"\"\"Detect COTS from a given numpy image.\"\"\"\n\n    input_tensor = np.expand_dims(image_np, 0)\n    start_time = time.time()\n    detections = model(input_tensor)\n    return detections\n\n# redefine function to show images\ndef show_image(path, annot, axs=None):\n    '''Shows an image and marks any COTS annotated within the frame.\n    path: full path to the .jpg image\n    annot: string of the annotation for the coordinates of COTS'''\n        \n    # This is in case we plot only 1 image\n    if axs==None:\n        fig, axs = plt.subplots(figsize=(23, 8))\n        \n    img = plt.imread(path)\n    axs.imshow(img)\n\n    if annot:\n        for a in annot:\n            rect = patches.Rectangle((a[\"x\"], a[\"y\"]), a[\"width\"], a[\"height\"], \n                                     linewidth=3, edgecolor=\"#FF6103\", facecolor='none')\n            axs.add_patch(rect)\n\n    axs.axis(\"off\") \n  \n# disp predicted detection box on images\ndef disp_prediction(path, detections, detection_threshold, axs=None):\n    '''Shows an image and marks any COTS annotated within the frame.\n    path: full path to the .jpg image\n    annot: string of the annotation for the coordinates of COTS'''\n    \n    image_np = load_image_into_numpy_array(image_path)\n    height, width, _ = image_np.shape\n    \n    num = len(detections['detection_boxes'].numpy()[0])\n    detection_array = detections['detection_boxes'].numpy()[0]\n    \n    \n    # This is in case we plot only 1 image\n    if axs==None:\n        fig, axs = plt.subplots(figsize=(23, 8))\n    \n    img = plt.imread(path)\n    axs.imshow(img)\n\n    if detection_array is not None:\n        for i in range(0, num):\n            score = detections['detection_scores'][0][i].numpy()\n            \n            if score < detection_threshold:\n                continue\n        \n            bbox = detection_array[i]\n            y_min = int(bbox[0] * height)\n            x_min = int(bbox[1] * width)\n            y_max = int(bbox[2] * height)\n            x_max = int(bbox[3] * width)\n                                   \n            bbox_width = x_max - x_min\n            bbox_height = y_max - y_min\n                                   \n            rect = patches.Rectangle((x_min, y_min), bbox_width, bbox_height, \n                                     linewidth=3, edgecolor=\"#FF6103\", facecolor='none')\n            axs.add_patch(rect)\n\n    axs.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-02-13T00:27:52.524772Z","iopub.execute_input":"2022-02-13T00:27:52.525312Z","iopub.status.idle":"2022-02-13T00:27:52.568365Z","shell.execute_reply.started":"2022-02-13T00:27:52.525205Z","shell.execute_reply":"2022-02-13T00:27:52.567296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the TensorFlow COTS detection model into memory.\nstart_time = time.time()\ntf.keras.backend.clear_session()\ndetect_fn_tf_odt = tf.saved_model.load(os.path.join(os.path.join(MODEL_DIR, 'output'), 'saved_model'))\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint('Elapsed time: ' + str(elapsed_time) + 's')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look for an image with some annotations\nimage_path = list(train_data_df[train_data_df[\"no_annotations\"] == 4][\"path\"])[0]\nannotation = list(train_data_df[train_data_df[\"no_annotations\"] == 4][\"annotations\"])[0]\nshow_image(image_path, annotation, axs=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_np = load_image_into_numpy_array(image_path)\ndetections = detect(image_np, detect_fn_tf_odt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detection_threshold = 0.3\ndisp_prediction(image_path, detections, detection_threshold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0.25\n# Visulize more images at the same time\ndef compare_predictions_real_detection(number_images, detection_threshold):\n    # randomnly select some images from the validation set\n    # disp on the left original images with annotations\n    # on the right prediction\n    indices = []\n    for i in range(0, number_images):\n        indices.append(random.randint(0, len(val_data_df)-1))\n    fig, axs = plt.subplots(number_images, 2,figsize=(20,20))\n    for i in range(0, number_images):\n        index = indices[i]\n        image_path = val_data_df[\"path\"][index]\n        image_np = load_image_into_numpy_array(image_path)\n        detections = detect(image_np, detect_fn_tf_odt)\n        annotations = val_data_df[\"annotations\"][index]\n        if number_images>1:\n            show_image(image_path, annotations, axs=axs[i, 0])\n            axs[i, 0].set_title('Image with annotations')\n            disp_prediction(image_path, detections, detection_threshold, axs=axs[i, 1])\n            axs[i, 1].set_title('Image with predicted bounding boxes')\n        else:\n            show_image(image_path, annotations, axs=axs[0])\n            axs[0].set_title('Image with annotations')\n            detection_threshold = 0.001\n            disp_prediction(image_path, detections, detection_threshold, axs=axs[1])\n            axs[1].set_title('Image with predicted bounding boxes')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Display some detections and original images with annotations","metadata":{}},{"cell_type":"code","source":"number_images = 5\ncompare_predictions_real_detection(number_images, threshold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results can be improved by fine tuning the model","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Download\"></a> <br> \n# **6. Zip and download trained model** \nHere I zip and download the trained model. I will do the inference in a second notebook since the competition rules don't allow internet connection","metadata":{}},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r trained_model.zip /kaggle/working/cots_efficientdet_d0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'trained_model.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model can then be downloaded and opened in a new notebook to make a submission","metadata":{}}]}