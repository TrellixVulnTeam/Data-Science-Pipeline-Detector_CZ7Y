{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Save Name: 703 + eca-nfnet + nfnetf0-haiku x2 + effnetb3","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:38:12.957286Z","iopub.execute_input":"2022-02-11T13:38:12.957917Z","iopub.status.idle":"2022-02-11T13:38:12.977768Z","shell.execute_reply.started":"2022-02-11T13:38:12.95783Z","shell.execute_reply":"2022-02-11T13:38:12.977058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom tqdm import tqdm\nimport sys\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport torch\nfrom PIL import Image\nimport ast\n\nimport cv2\n\nfrom albumentations import Normalize, Resize, Compose\nfrom albumentations.pytorch import ToTensorV2\n\nsys.path.append('../input/tensorflow-great-barrier-reef')\nsys.path.append('/kaggle/input/weightedboxesfusion/')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":3.921653,"end_time":"2022-02-07T06:31:41.419334","exception":false,"start_time":"2022-02-07T06:31:37.497681","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:38:12.984613Z","iopub.execute_input":"2022-02-11T13:38:12.985216Z","iopub.status.idle":"2022-02-11T13:38:16.93637Z","shell.execute_reply.started":"2022-02-11T13:38:12.985184Z","shell.execute_reply":"2022-02-11T13:38:16.935662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ensemble_boxes import *\n","metadata":{"papermill":{"duration":0.056084,"end_time":"2022-02-07T06:31:41.496533","exception":false,"start_time":"2022-02-07T06:31:41.440449","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:38:16.938043Z","iopub.execute_input":"2022-02-11T13:38:16.938725Z","iopub.status.idle":"2022-02-11T13:38:16.970918Z","shell.execute_reply.started":"2022-02-11T13:38:16.938687Z","shell.execute_reply":"2022-02-11T13:38:16.970255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/","metadata":{"papermill":{"duration":1.357195,"end_time":"2022-02-07T06:31:42.872706","exception":false,"start_time":"2022-02-07T06:31:41.515511","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:38:16.973012Z","iopub.execute_input":"2022-02-11T13:38:16.973647Z","iopub.status.idle":"2022-02-11T13:38:18.396792Z","shell.execute_reply.started":"2022-02-11T13:38:16.973606Z","shell.execute_reply":"2022-02-11T13:38:18.395628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"papermill":{"duration":0.041743,"end_time":"2022-02-07T06:31:42.931147","exception":false,"start_time":"2022-02-07T06:31:42.889404","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:38:18.40123Z","iopub.execute_input":"2022-02-11T13:38:18.401634Z","iopub.status.idle":"2022-02-11T13:38:18.428065Z","shell.execute_reply.started":"2022-02-11T13:38:18.401594Z","shell.execute_reply":"2022-02-11T13:38:18.427373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"YOLO_MODEL_PTHS = [\n#     \"/kaggle/input/bestf2/fold0_aug_2560.pt\",  # best fold 0\n#     \"/kaggle/input/bestf2/best_f1.pt\",  # best fold 1\n    \"/kaggle/input/yolov5cots/s6_3600_bs3_fit_hflip_ezaug_v1-LDM.pt\",  # best fold 2\n#     \"/kaggle/input/l63600uflipvm5f1all/weights/best.pt\",\n    \"/kaggle/input/yolov5s6/f2_sub2.pt\",\n]","metadata":{"papermill":{"duration":0.022482,"end_time":"2022-02-07T06:31:42.970033","exception":false,"start_time":"2022-02-07T06:31:42.947551","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:40:13.359459Z","iopub.execute_input":"2022-02-11T13:40:13.35992Z","iopub.status.idle":"2022-02-11T13:40:13.363711Z","shell.execute_reply.started":"2022-02-11T13:40:13.359884Z","shell.execute_reply":"2022-02-11T13:40:13.363018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\nfor index, model_pth in enumerate(YOLO_MODEL_PTHS):\n    model = torch.hub.load('../input/yolov5-lib-ds', \n#     model = torch.hub.load('./yolov5-lib-ds',  \n                       'custom', \n                       path=model_pth,\n                       source='local',\n                       force_reload=True)  # local repo\n    if index == 0:\n        model.conf = 0.28\n    else:\n        model.conf = 0.28\n        \n    model.iou = 0.7\n        \n    models.append(model)\n    del model\n    gc.collect()","metadata":{"papermill":{"duration":7.713755,"end_time":"2022-02-07T06:31:50.699873","exception":false,"start_time":"2022-02-07T06:31:42.986118","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:40:13.477428Z","iopub.execute_input":"2022-02-11T13:40:13.478066Z","iopub.status.idle":"2022-02-11T13:40:14.674321Z","shell.execute_reply.started":"2022-02-11T13:40:13.47803Z","shell.execute_reply":"2022-02-11T13:40:14.673516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CLASSIFIERS_PATHS = [\n    '../input/cotsclassifiers/effnet-b3-spret-classification-model.pth',\n#     '../input/cotsclassifiers/resnet152-padbbox-e27-sgd-v1.pth',\n    '../input/cotsclassifiers/effnet-b3-padbbox-e19-sgd-v1.pth',\n#     '../input/cotsclassifiers/eca-nfnet-f0-padbbox-e19-adam-v1.pth',\n#     '../input/cotsclassifiers/nfnet-f0-haiku-padbbox-e11-sgd-agc-v3.pth',\n]","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:43:57.359821Z","iopub.execute_input":"2022-02-11T13:43:57.360078Z","iopub.status.idle":"2022-02-11T13:43:57.364212Z","shell.execute_reply.started":"2022-02-11T13:43:57.360051Z","shell.execute_reply":"2022-02-11T13:43:57.363431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_classifier(path):\n    classification_model = torch.load(path)\n    classification_model.to(device)\n    classification_model.eval()\n    print(f'loaded - {path}')\n    \n    return classification_model","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:43:58.260916Z","iopub.execute_input":"2022-02-11T13:43:58.261477Z","iopub.status.idle":"2022-02-11T13:43:58.265748Z","shell.execute_reply.started":"2022-02-11T13:43:58.26144Z","shell.execute_reply":"2022-02-11T13:43:58.264747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nsys.path.append(\"../input/efficientnet-pytorch\")\nfrom efficientnet_pytorch import EfficientNet\n\n%cd /kaggle/working","metadata":{"papermill":{"duration":0.771246,"end_time":"2022-02-07T06:31:51.49041","exception":false,"start_time":"2022-02-07T06:31:50.719164","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:43:58.426525Z","iopub.execute_input":"2022-02-11T13:43:58.426979Z","iopub.status.idle":"2022-02-11T13:43:58.435754Z","shell.execute_reply.started":"2022-02-11T13:43:58.426945Z","shell.execute_reply":"2022-02-11T13:43:58.434954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:43:58.559281Z","iopub.execute_input":"2022-02-11T13:43:58.559759Z","iopub.status.idle":"2022-02-11T13:43:58.563968Z","shell.execute_reply.started":"2022-02-11T13:43:58.559716Z","shell.execute_reply":"2022-02-11T13:43:58.563224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append('../input/nfnets-pytorch/content/nfnets_pytorch')\nfrom nfnets import pretrained_nfnet","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:43:58.690691Z","iopub.execute_input":"2022-02-11T13:43:58.690944Z","iopub.status.idle":"2022-02-11T13:43:58.695367Z","shell.execute_reply.started":"2022-02-11T13:43:58.690916Z","shell.execute_reply":"2022-02-11T13:43:58.694442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_models = []\n\nfor cls_path in CLASSIFIERS_PATHS:\n    cls_models.append(load_classifier(cls_path))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:43:58.993508Z","iopub.execute_input":"2022-02-11T13:43:58.993968Z","iopub.status.idle":"2022-02-11T13:43:59.309083Z","shell.execute_reply.started":"2022-02-11T13:43:58.993929Z","shell.execute_reply":"2022-02-11T13:43:59.308418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","metadata":{"papermill":{"duration":0.02702,"end_time":"2022-02-07T06:31:51.536261","exception":false,"start_time":"2022-02-07T06:31:51.509241","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:43:59.393665Z","iopub.execute_input":"2022-02-11T13:43:59.394021Z","iopub.status.idle":"2022-02-11T13:43:59.402114Z","shell.execute_reply.started":"2022-02-11T13:43:59.393989Z","shell.execute_reply":"2022-02-11T13:43:59.401114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def voc2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    voc  => [x1, y1, x2, y1]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0]\n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    bboxes[..., 0] = bboxes[..., 0] + w/2\n    bboxes[..., 1] = bboxes[..., 1] + h/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    \n    return bboxes\n\n\ndef yolo2coco(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\n\ndef voc2coco(bboxes, image_height=720, image_width=1280):\n    bboxes  = voc2yolo(bboxes, image_height, image_width)\n    bboxes  = yolo2coco(bboxes, image_height, image_width)\n    return bboxes","metadata":{"papermill":{"duration":0.031721,"end_time":"2022-02-07T06:31:51.586678","exception":false,"start_time":"2022-02-07T06:31:51.554957","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:44:00.899981Z","iopub.execute_input":"2022-02-11T13:44:00.900234Z","iopub.status.idle":"2022-02-11T13:44:00.911333Z","shell.execute_reply.started":"2022-02-11T13:44:00.900205Z","shell.execute_reply":"2022-02-11T13:44:00.910315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n                h  = round(float(bbox[3])*image.shape[0]/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"papermill":{"duration":0.04594,"end_time":"2022-02-07T06:31:51.651337","exception":false,"start_time":"2022-02-07T06:31:51.605397","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:44:01.458415Z","iopub.execute_input":"2022-02-11T13:44:01.458834Z","iopub.status.idle":"2022-02-11T13:44:01.487274Z","shell.execute_reply.started":"2022-02-11T13:44:01.458804Z","shell.execute_reply":"2022-02-11T13:44:01.486348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot","metadata":{"papermill":{"duration":0.026408,"end_time":"2022-02-07T06:31:51.697042","exception":false,"start_time":"2022-02-07T06:31:51.670634","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:44:02.473211Z","iopub.execute_input":"2022-02-11T13:44:02.473486Z","iopub.status.idle":"2022-02-11T13:44:02.479305Z","shell.execute_reply.started":"2022-02-11T13:44:02.473457Z","shell.execute_reply":"2022-02-11T13:44:02.478626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_wbf(models, img, size=3600, augment=False):\n    height, width = img.shape[:2]\n    \n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    \n    weights = [1]*len(models)\n    iou_thr = 0.50\n\n    for index, model in enumerate(models):\n        \n        if index == 0:\n            results = model(img, size=9600, augment=augment)\n        else:\n            results = model(img, size=6400, augment=augment)\n            \n        preds   = results.pandas().xyxy[0]\n        bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n        scores  = preds.confidence.values  \n        boxes_list.append(bboxes)\n        scores_list.append(scores)\n        labels_list.append([0]*len(scores))\n    \n    bboxes, scores, bbclasses = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=0.0)\n    \n    \n    return bboxes, scores, bbclasses\n    \n    \n#     if len(bboxes):\n#         bboxes  = voc2coco(bboxes,height,width).astype(int)\n#         confs = scores\n#         return bboxes, confs\n#     else:\n#         return [],[]","metadata":{"papermill":{"duration":0.029014,"end_time":"2022-02-07T06:31:51.744834","exception":false,"start_time":"2022-02-07T06:31:51.71582","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:44:04.091925Z","iopub.execute_input":"2022-02-11T13:44:04.092606Z","iopub.status.idle":"2022-02-11T13:44:04.100828Z","shell.execute_reply.started":"2022-02-11T13:44:04.092569Z","shell.execute_reply":"2022-02-11T13:44:04.10013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = torch.hub.load('../input/yolov5-lib-ds', \n#                        'custom', \n#                        path='../input/yolov5cots/s6_3600_bs3_fit_hflip_ezaug_v1-LDM.pt',\n#                        source='local',\n#                        force_reload=True)  # local repo\n# model.conf = 0.20","metadata":{"papermill":{"duration":0.024469,"end_time":"2022-02-07T06:31:51.787963","exception":false,"start_time":"2022-02-07T06:31:51.763494","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:44:04.2208Z","iopub.execute_input":"2022-02-11T13:44:04.221439Z","iopub.status.idle":"2022-02-11T13:44:04.227765Z","shell.execute_reply.started":"2022-02-11T13:44:04.221383Z","shell.execute_reply":"2022-02-11T13:44:04.22667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n#     anno = ''\n#     r = model(img, size=3600, augment=True)\n#     if r.pandas().xyxy[0].shape[0] == 0:\n#         anno = ''\n#     else:\n#         for idx, row in r.pandas().xyxy[0].iterrows():\n#             if row.confidence > 0.15:\n#                 anno += '{} {} {} {} {} '.format(row.confidence, int(row.xmin), int(row.ymin), int(row.xmax-row.xmin), int(row.ymax-row.ymin))\n# #                 pred.append([row.confidence, row.xmin, row.ymin, row.xmax-row.xmin, row.ymax-row.ymin])\n#     pred_df['annotations'] = anno.strip(' ')\n#     env.predict(pred_df)","metadata":{"papermill":{"duration":0.024544,"end_time":"2022-02-07T06:31:51.831175","exception":false,"start_time":"2022-02-07T06:31:51.806631","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T13:44:04.378685Z","iopub.execute_input":"2022-02-11T13:44:04.378949Z","iopub.status.idle":"2022-02-11T13:44:04.382639Z","shell.execute_reply.started":"2022-02-11T13:44:04.378921Z","shell.execute_reply":"2022-02-11T13:44:04.381814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms = Compose([\n#             Resize(256, 256), \n            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1), \n            ToTensorV2()\n        ])\n\ndef crop_img_2_bbox(img, box):\n    return img[int(box[1]):int(box[1]+box[3]), int(box[0]):int(box[0]+box[2])]\n\n# def crop_img_2_bbox(img, box, crop_size):\n    \n#     origin = [box[0] + box[2]//2, box[1] + box[3]//2]\n#     x_new = int(origin[0] - crop_size//2)\n#     y_new = int(origin[1] - crop_size//2)\n    \n#     if x_new < 0:\n#         x_new = 0\n#     if x_new > 1280:\n#         x_new = 1280\n        \n#     if y_new < 0:\n#         y_new = 0\n#     if y_new > 720:\n#         y_new = 720\n        \n#     print(img.shape)\n    \n#     return img[\n#         y_new:y_new + crop_size if y_new + crop_size < 720 else 720,\n#         x_new:x_new + crop_size if x_new + crop_size < 1280 else 1280\n#     ]\n\n\ndef classifier_predict(crop_sample, cls_models):\n    crop_sample_01 = transforms(image=crop_sample)['image']\n    crop_sample_01 = crop_sample_01.unsqueeze(0).to(device)\n    \n    crop_sample = np.flipud(crop_sample)\n    crop_sample_02 = transforms(image=crop_sample)['image']\n    crop_sample_02 = crop_sample_02.unsqueeze(0).to(device)\n\n    results = []\n#     softmax = torch.nn.Softmax(dim=0)\n    with torch.no_grad():\n        for cls_model in cls_models:\n            result = cls_model(crop_sample_01)\n#             print(\"->\", softmax(result[0]))\n            result = result.sigmoid()\n            result = result[:, 1].item()\n#             print(result)\n            results.append(result)\n            \n            result = cls_model(crop_sample_02)\n#             print(\"->\", softmax(result[0]))\n            result = result.sigmoid()\n            result = result[:, 1].item()\n#             result = result.argmax(dim=1)   # 0 - cots, 1 - not cots\n#             result = abs(1 - result)        # 1 - cots, 0 - not cots\n#             print(result)\n            results.append(result)\n    \n    result = np.mean(results)\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:44:04.871832Z","iopub.execute_input":"2022-02-11T13:44:04.87208Z","iopub.status.idle":"2022-02-11T13:44:04.887949Z","shell.execute_reply.started":"2022-02-11T13:44:04.872053Z","shell.execute_reply":"2022-02-11T13:44:04.886949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread(\"../input/tensorflow-great-barrier-reef/train_images/video_0/100.jpg\")[..., ::-1]\n\nbboxes, confis, _ = predict_wbf(models, img, size=9000, augment=True)\nbboxes, confis = np.array(bboxes), np.array(confis)\nbboxes=bboxes[confis > 0.25]\n\nbboxes[..., 2] = bboxes[..., 2] - bboxes[..., 0]\nbboxes[..., 3] = bboxes[..., 3] - bboxes[..., 1]\n\ndisplay(show_img(img, bboxes, bbox_format='coco'))\n\npadding = 0\ntemp_boxes = []\nSTARFISH_CONF = 0.3\nfor conf, box in zip(confis, bboxes):\n    print(box)\n    print(conf)\n\n    # simple scale tta from classifier\n#     if box[1] > 0.2 * 720: # 1/5 and lower\n    crop_sample = crop_img_2_bbox(img, box)\n    display(Image.fromarray(crop_sample).resize((256, 256)))\n    result = classifier_predict(crop_sample, cls_models)\n\n    print(f'[[  Result: {result}  ]]')\n    if result < STARFISH_CONF: \n        print(\"\\t-- not cots found -- \")\n        continue\n    print(\"\\t-- cots found !!! -- \")\n\n    temp_boxes.append(box)\n\nprint(temp_boxes)\n# [343 464  81  64]\n\n\n# small boxes shrink 5%\n# t_boxes = []\n# for box in temp_boxes:\n#     box = resize_bbox(box, 500, -10, 'pad')\n#     t_boxes.append(box)\n# temp_boxes = np.array(t_boxes)\n\n# print(bboxes, confis)\n# display(show_img(img, temp_boxes, bbox_format='coco'))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:44:53.924635Z","iopub.execute_input":"2022-02-11T13:44:53.924996Z","iopub.status.idle":"2022-02-11T13:44:56.177386Z","shell.execute_reply.started":"2022-02-11T13:44:53.924954Z","shell.execute_reply":"2022-02-11T13:44:56.176461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# norfair dependencies\n%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index","metadata":{"papermill":{"duration":75.529707,"end_time":"2022-02-07T06:33:07.379389","exception":false,"start_time":"2022-02-07T06:31:51.849682","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-10T17:32:04.577728Z","iopub.execute_input":"2022-02-10T17:32:04.577993Z","iopub.status.idle":"2022-02-10T17:33:19.220283Z","shell.execute_reply.started":"2022-02-10T17:32:04.577955Z","shell.execute_reply":"2022-02-10T17:33:19.219402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{"papermill":{"duration":0.060658,"end_time":"2022-02-07T06:33:07.48434","exception":false,"start_time":"2022-02-07T06:33:07.423682","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-10T17:33:19.222643Z","iopub.execute_input":"2022-02-10T17:33:19.222924Z","iopub.status.idle":"2022-02-10T17:33:19.229829Z","shell.execute_reply.started":"2022-02-10T17:33:19.222886Z","shell.execute_reply":"2022-02-10T17:33:19.22909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize_bbox(bbox, area_thr, shape_percent, tp='shrink'):\n    area = bbox[2] * bbox[3] \n    \n    # TODO: refactor this -- change condition check\n    if tp == 'shrink':\n        if area < area_thr:\n            center_xy = [bbox[0] + bbox[2]//2, bbox[1] + bbox[3]//2]\n            new_w = shape_percent * bbox[2] \n            new_h = shape_percent * bbox[3] \n\n            new_x = center_xy[0] - new_w//2\n            new_y = center_xy[1] - new_h//2\n\n            if new_x < 0:\n                new_x = 0\n\n            if new_x + new_w > 1280:\n                new_w = 1280 - new_x\n\n            if new_y < 0:\n                new_y = 0\n\n            if new_y + new_h > 720:\n                new_h = 720 - new_y\n\n\n            bbox = [\n                new_x,\n                new_y,\n                new_w, \n                new_h\n            ]\n            \n    if tp == 'pad':\n        if area > area_thr:\n            center_xy = [bbox[0] + bbox[2]//2, bbox[1] + bbox[3]//2]\n            new_w = shape_percent + bbox[2] \n            new_h = shape_percent + bbox[3] \n\n            new_x = center_xy[0] - new_w//2\n            new_y = center_xy[1] - new_h//2\n\n            if new_x < 0:\n                new_x = 0\n\n            if new_x + new_w > 1280:\n                new_w = 1280 - new_x\n\n            if new_y < 0:\n                new_y = 0\n\n            if new_y + new_h > 720:\n                new_h = 720 - new_y\n\n\n            bbox = [\n                new_x,\n                new_y,\n                new_w, \n                new_h\n            ]\n            \n    \n    \n    return bbox","metadata":{"papermill":{"duration":0.039304,"end_time":"2022-02-07T06:33:07.565581","exception":false,"start_time":"2022-02-07T06:33:07.526277","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-11T09:53:18.719694Z","iopub.execute_input":"2022-02-11T09:53:18.720144Z","iopub.status.idle":"2022-02-11T09:53:18.73659Z","shell.execute_reply.started":"2022-02-11T09:53:18.720099Z","shell.execute_reply":"2022-02-11T09:53:18.735521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.035688,"end_time":"2022-02-07T06:33:07.628022","exception":false,"start_time":"2022-02-07T06:33:07.592334","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##############################################################\n#                      Tracking helpers                      #\n##############################################################\n\nimport numpy as np\nfrom norfair import Detection, Tracker\n\n# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n        ","metadata":{"papermill":{"duration":0.08579,"end_time":"2022-02-07T06:33:07.74076","exception":false,"start_time":"2022-02-07T06:33:07.65497","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-10T17:33:19.24633Z","iopub.execute_input":"2022-02-10T17:33:19.247048Z","iopub.status.idle":"2022-02-10T17:33:19.309789Z","shell.execute_reply.started":"2022-02-10T17:33:19.247009Z","shell.execute_reply":"2022-02-10T17:33:19.30914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#######################################################\n#                      Tracking                       #\n#######################################################\n\n# Tracker will update tracks based on detections from current frame\n# Matching based on euclidean distance between bbox centers of detections \n# from current frame and tracked_objects based on previous frames\n# You can check it's parameters in norfair docs\n# https://github.com/tryolabs/norfair/blob/master/docs/README.md\ntracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\n# Save frame_id into detection to know which tracks have no detections on current frame\nframe_id = 0\n#######################################################\n\nBBOX_THR = 0.2\nSTARFISH_CONF = 0.32\n\nfor idx, (img, sample_prediction_df) in enumerate(tqdm(iter_test)):\n    anno = ''\n    predictions = []\n    detects = []\n    \n    height, width = img.shape[:2]            \n    bboxes, confs, bbclasses  = predict_wbf(models, img, size=9000, augment=True)\n    \n    \n    \n    # small boxes shrink 5%\n    t_boxes = []\n    for box in bboxes:\n        box = resize_bbox(box, 300, 0.95, 'shrink')\n        t_boxes.append(box)\n    bboxes = np.array(t_boxes)\n    \n#     t_boxes = []\n#     for box in bboxes:\n#         box = resize_bbox(box, 500, -10, 'pad')\n#         t_boxes.append(box)\n#     bboxes = np.array(t_boxes)\n    \n    \n    tmp_bboxes = []\n    for box, conf in  zip(bboxes, confs):\n        if conf > 0.25:\n            x_min = int(box[0])\n            y_min = int(box[1])\n            x_max = int(box[2])\n            y_max = int(box[3])\n            score = conf\n            \n            \n            bbox_width = x_max - x_min\n            bbox_height = y_max - y_min\n            \n            \n            # CLASSIFIER \n            # classify the box crop if it is starfish \n            # ---- simple scale tta for classifier\n            _box = [x_min, y_min, bbox_width, bbox_height]\n            \n            # Predict on lower part of the video | remove FP\n            if _box[1] > 0.2 * 720: # 1/5 and lower\n#             crop_sample = crop_img_2_bbox(img, _box)\n#             result = classifier_predict(crop_sample)\n        \n#             if not (result):\n#                 continue # skip not cots\n\n\n                crop_sample = crop_img_2_bbox(img, _box)\n                result = classifier_predict(crop_sample, cls_models)\n                if result < STARFISH_CONF: \n                    continue\n\n            detects.append([x_min, y_min, x_max, y_max, score])\n            \n            tmp_bboxes.append([x_min, y_min, bbox_width, bbox_height])\n            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n            \n            \n\n    if idx<3:\n        print(confs)\n        display(show_img(img, tmp_bboxes, bbox_format='coco'))\n    \n    \n    #######################################################\n    #                      Tracking                       #\n    #######################################################\n    \n    # Update tracks using detects from current frame\n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        \n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n        \n            \n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n\n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    #######################################################\n    \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n\n#     print('Prediction:', prediction_str)\n    frame_id += 1","metadata":{"papermill":{"duration":11.030294,"end_time":"2022-02-07T06:33:18.79709","exception":false,"start_time":"2022-02-07T06:33:07.766796","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-10T17:33:19.310943Z","iopub.execute_input":"2022-02-10T17:33:19.311552Z","iopub.status.idle":"2022-02-10T17:33:25.253468Z","shell.execute_reply.started":"2022-02-10T17:33:19.311514Z","shell.execute_reply":"2022-02-10T17:33:25.252727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"papermill":{"duration":0.09896,"end_time":"2022-02-07T06:33:18.980048","exception":false,"start_time":"2022-02-07T06:33:18.881088","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-10T17:33:25.25459Z","iopub.execute_input":"2022-02-10T17:33:25.255312Z","iopub.status.idle":"2022-02-10T17:33:25.270622Z","shell.execute_reply.started":"2022-02-10T17:33:25.255273Z","shell.execute_reply":"2022-02-10T17:33:25.269846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.083884,"end_time":"2022-02-07T06:33:19.149514","exception":false,"start_time":"2022-02-07T06:33:19.06563","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.082736,"end_time":"2022-02-07T06:33:19.314475","exception":false,"start_time":"2022-02-07T06:33:19.231739","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}