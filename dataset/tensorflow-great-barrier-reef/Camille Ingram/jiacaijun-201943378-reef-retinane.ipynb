{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# 导入库","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n# show images inline\n%matplotlib inline\n\nimport keras\nimport tensorflow\n\n# import miscellaneous modules\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport numpy as np\nimport time\n\nimport tensorflow as tf\n\n\n\n\n#Clone Git Repository\n!git clone https://github.com/fizyr/keras-retinanet.git\n%cd keras-retinanet/\n!python setup.py build_ext --inplace\n\n# import keras_retinanet\nfrom keras_retinanet import models\nfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption\nfrom keras_retinanet.utils.colors import label_color\nfrom keras_retinanet import models","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:40:07.795753Z","iopub.execute_input":"2021-12-08T02:40:07.796095Z","iopub.status.idle":"2021-12-08T02:40:20.021168Z","shell.execute_reply.started":"2021-12-08T02:40:07.796055Z","shell.execute_reply":"2021-12-08T02:40:20.020188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 加载数据","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/tensorflow-great-barrier-reef/train.csv\")\n\ndf_train=df_train.loc[df_train[\"annotations\"].astype(str) != \"[]\"]\ndf_train['annotations'] = df_train['annotations'].apply(eval)\n\ndf_train['image_path'] = \"/kaggle/input/tensorflow-great-barrier-reef/train_images/video_\" + df_train['video_id'].astype(str) + \"/\" + df_train['video_frame'].astype(str) + \".jpg\"\ndf_extrain=df_train.explode('annotations') # Single annotation per row\ndf_extrain.reset_index(inplace=True)\ndf_extrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:40:20.023848Z","iopub.execute_input":"2021-12-08T02:40:20.024157Z","iopub.status.idle":"2021-12-08T02:40:20.259136Z","shell.execute_reply.started":"2021-12-08T02:40:20.024111Z","shell.execute_reply":"2021-12-08T02:40:20.258443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_extrain_main=pd.DataFrame(pd.json_normalize(df_extrain['annotations']), columns=['x', 'y', 'width', 'height']).join(df_extrain)\ndf_extrain_main['class']='Fish'\ndf_extrain_main=df_extrain_main[['image_path','x','y','width','height','class','video_id','video_frame']]\ndf_extrain_main.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:40:20.260494Z","iopub.execute_input":"2021-12-08T02:40:20.260776Z","iopub.status.idle":"2021-12-08T02:40:20.344904Z","shell.execute_reply.started":"2021-12-08T02:40:20.260705Z","shell.execute_reply":"2021-12-08T02:40:20.344193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# 下载预训练权重","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade git+https://github.com/broadinstitute/keras-resnet\nimport keras\nimport keras_resnet\nimport urllib.request\nPRETRAINED_MODEL = './snapshots/_pretrained_model.h5'\n#### OPTION 1: DOWNLOAD INITIAL PRETRAINED MODEL FROM FIZYR ####\nURL_MODEL = 'https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5'\nurllib.request.urlretrieve(URL_MODEL, PRETRAINED_MODEL)\nprint('Downloaded pretrained model to ' + PRETRAINED_MODEL)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:40:20.346895Z","iopub.execute_input":"2021-12-08T02:40:20.347169Z","iopub.status.idle":"2021-12-08T02:41:15.801479Z","shell.execute_reply.started":"2021-12-08T02:40:20.34713Z","shell.execute_reply":"2021-12-08T02:41:15.799402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 转换数据格式","metadata":{}},{"cell_type":"code","source":"\ndef create_tf_example(rowss,data_df):\n    \"\"\"Create a tf.Example entry for a given training image.\"\"\"\n    full_path = os.path.join(rowss.image_path)\n    with tf.io.gfile.GFile(full_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n\n    height = image.size[1] # Image height\n    width = image.size[0] # Image width\n    #print(width,height)\n    filename = f'{rowss.video_id}:{rowss.video_frame}'.encode('utf8') # Unique id of the image.\n    encoded_image_data = None # Encoded image bytes\n    image_format = 'jpeg'.encode('utf8') # b'jpeg' or b'png'\n\n    xmins = [] \n    xmaxs = [] \n    ymins = [] \n    ymaxs = [] \n    \n    # Convert ---> [xmin,ymin,width,height] to [xmins,xmaxs,ymins,ymaxs]\n    xmin = rowss['x']\n    xmax = rowss['x']+rowss['width']\n    ymin = rowss['y']\n    ymax = rowss['y']+rowss['height']\n    \n\n    #main_data.append((rowss['image_path'],xmins,xmaxs,ymins,ymaxs))\n    return rowss['image_path'],xmin,ymin,xmax,ymax","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:41:15.8032Z","iopub.execute_input":"2021-12-08T02:41:15.803777Z","iopub.status.idle":"2021-12-08T02:41:15.81355Z","shell.execute_reply.started":"2021-12-08T02:41:15.803729Z","shell.execute_reply":"2021-12-08T02:41:15.81277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport contextlib2\nimport io\nimport IPython\nimport json\nimport numpy as np\nimport os\nimport pathlib\nimport pandas as pd\nimport sys\nimport tensorflow as tf\nimport time\n\ntf_example1=[]\n\nfrom PIL import Image, ImageDraw\nfor index, row in df_extrain_main.iterrows():\n            if index % 500 == 0:\n                print('Processed {0} images.'.format(index))\n            image_path,xmins,ymins,xmaxs,ymaxs=create_tf_example(row,df_extrain_main)\n            #print(image_path,xmins,xmaxs,ymins,ymaxs)\n            df_extrain_main.loc[index,'image_path']=image_path\n            df_extrain_main.loc[index,'x']=xmins\n            df_extrain_main.loc[index,'y']=ymins\n            df_extrain_main.loc[index,'width']=xmaxs\n            df_extrain_main.loc[index,'height']=ymaxs\n","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:41:15.815267Z","iopub.execute_input":"2021-12-08T02:41:15.815782Z","iopub.status.idle":"2021-12-08T02:42:35.45711Z","shell.execute_reply.started":"2021-12-08T02:41:15.815747Z","shell.execute_reply":"2021-12-08T02:42:35.456245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 创建训练数据","metadata":{}},{"cell_type":"code","source":"classes=pd.DataFrame([{'class':'Fish','label':0}])\nclasses.to_csv(\"classes.csv\",index=False,header=False)  # This CSV will be use in training\n\ndf_extrain_main['class']='Fish'\ndf_extrain_main[['image_path','x','y','width','height','class']].to_csv(\"annotation.csv\",index=False,header=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:42:35.458585Z","iopub.execute_input":"2021-12-08T02:42:35.458833Z","iopub.status.idle":"2021-12-08T02:42:35.53031Z","shell.execute_reply.started":"2021-12-08T02:42:35.458798Z","shell.execute_reply":"2021-12-08T02:42:35.529584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练Training RetinaNe","metadata":{}},{"cell_type":"code","source":"!keras_retinanet/bin/train.py --freeze-backbone --random-transform --no-resize --weights {PRETRAINED_MODEL} --batch-size 1 --steps 550 --epochs 40 csv annotation.csv classes.csv","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:42:35.531404Z","iopub.execute_input":"2021-12-08T02:42:35.531655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 加载训练模型","metadata":{}},{"cell_type":"code","source":"model_path = os.path.join('snapshots', sorted(os.listdir('snapshots'), reverse=True)[0])\nprint(model_path)\n\n# load retinanet model\nmodel = models.load_model(model_path, backbone_name='resnet50')  ## Use backbone as resnet50\nmodel = models.convert_model(model)\n\n# load label to names mapping for visualization purposes\nlabels_to_names = pd.read_csv('classes.csv',header=None).T.loc[0].to_dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 预测与实际","metadata":{}},{"cell_type":"code","source":"THRES_SCORE = 0.25  # Set Score Threshold Value\n\ndef df_plot_orinal(drawOG,img_path,df):\n    df=df[df['image_path']==img_path]\n    for i,r in df.iterrows():\n        cv2.rectangle(drawOG, (r['x'], r['y']), (r['width'], r['height']), (255,0,0),2)\n    \n\ndef img_inference(img_path):\n  image = read_image_bgr(img_path)\n\n  # copy to draw on\n  draw = image.copy()\n  draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n  drawOG = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n  # preprocess image for network\n  image = preprocess_image(image)\n  image, scale = resize_image(image)\n\n  # process image\n  start = time.time()\n  boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n  df_plot_orinal(drawOG,img_path,df_extrain_main)\n  # correct for image scale\n  boxes /= scale\n  # visualize detections\n  for box, score, label in zip(boxes[0], scores[0], labels[0]):\n      # scores are sorted so we can break\n      if score < THRES_SCORE:\n          break\n      color = label_color(label)\n      b = box.astype(int)\n      draw_box(draw, b, color=color)\n      caption = \"{} {:.3f}%\".format(labels_to_names[label], score*100)\n    \n  fig = plt.figure(figsize=(20, 20))\n  ax1=fig.add_subplot(1, 2, 1)\n  plt.imshow(draw)\n  ax2=fig.add_subplot(1, 2, 2)\n  plt.imshow(drawOG)\n\n  ax1.title.set_text('Predicted')\n  ax2.title.set_text('Actual')\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=df_extrain_main.sample(n=5)  #Predict on Random 5 Image\nfor i,r in data.iterrows():\n    img_inference(r['image_path'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reef API","metadata":{}},{"cell_type":"code","source":"# Import the library that is used to submit the prediction result.\nimport sys\nINPUT_DIR = '/kaggle/input/tensorflow-great-barrier-reef/greatbarrierreef/'\nsys.path.insert(0, INPUT_DIR)\nimport greatbarrierreef\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"THRES_SCORE = 0.8\n\nsubmission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor (image, sample_prediction_df) in iter_test:\n  print(image.shape,sample_prediction_df)\n\n  image = preprocess_image(image)\n  image, scale = resize_image(image)\n\n  # process image\n  start = time.time()\n  boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n  predictions=[]\n  # correct for image scale\n  boxes /= scale\n  # visualize detections\n  for box, score, label in zip(boxes[0], scores[0], labels[0]):\n      # scores are sorted so we can break\n      if score < 0.0:\n          break\n      predictions = []\n      x_min = int(box[0])  \n      y_min = int(box[1])\n      x_max = int(box[2])\n      y_max = int(box[3])\n\n      bbox_width = x_max - x_min\n      bbox_height = y_max - y_min\n      predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n\n  prediction_str = ' '.join(predictions)\n  sample_prediction_df['annotations'] = prediction_str\n  env.predict(sample_prediction_df)\n  print('Prediction:', prediction_str)\n\nmy_submission = pd.DataFrame(sample_prediction_df)\n# you could use any filename. We choose submission here\nsample_prediction_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!ls\n#!mv 'submission.csv' '/kaggle/working/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}