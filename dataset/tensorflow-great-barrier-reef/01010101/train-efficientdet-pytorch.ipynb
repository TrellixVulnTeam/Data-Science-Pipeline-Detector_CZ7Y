{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\nVersion Control Log\n</div>\n\n|  Version  |  Date  |  Description of change  |\n| ---- | ---- |--------|\n|  2  |  27/11/2021  | Initial release|\n|  3  |  28/11/2021  | Add weights for d0~d7 and table and correct typo|\n|  8  |  29/11/2021  | Add log plot|","metadata":{}},{"cell_type":"markdown","source":"# Message\nThis is my first public notebook for me.  \nSo if you find the notebook is something wrong or strange, please tell me!  \nTo create the notebook, I spare the much time😅  \nI work for it from the evening to the midnight😪 So If you find the notebook is useful please upvote👍  \nThen, I will be very HAPPY😆  \nLet's fight together for helping our Great Barrier Reef💪  ","metadata":{}},{"cell_type":"markdown","source":"# Other Resources\nI also prepare [the discussion](https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/290992) about EfficientDet.  \nSo please check it out.\nAlso **inference** notebook is under preparation.","metadata":{}},{"cell_type":"markdown","source":"# Install","metadata":{}},{"cell_type":"code","source":"!pip install --no-deps '../input/timm-package/timm-0.4.12-py3-none-any.whl' > /dev/null\n!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:50:51.848548Z","iopub.execute_input":"2021-11-29T04:50:51.848887Z","iopub.status.idle":"2021-11-29T04:51:36.093417Z","shell.execute_reply.started":"2021-11-29T04:50:51.848812Z","shell.execute_reply":"2021-11-29T04:51:36.092417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/timm-efficientdet-pytorch-fix-v3/timm_efficientdet_pytorch_fix_v3\")\nsys.path.insert(0, \"../input/omegaconf\")\nsys.path.insert(0, \"../input/albumentations-fix-v1/albumentations_fix_v1\")\n\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\n\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:51:36.095212Z","iopub.execute_input":"2021-11-29T04:51:36.095661Z","iopub.status.idle":"2021-11-29T04:51:39.961793Z","shell.execute_reply.started":"2021-11-29T04:51:36.095626Z","shell.execute_reply":"2021-11-29T04:51:39.961005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df = pd.read_csv('../input/tensorflow-great-barrier-reef/train.csv')\ndata_df = data_df[data_df.annotations != '[]'].reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:51:39.963295Z","iopub.execute_input":"2021-11-29T04:51:39.963565Z","iopub.status.idle":"2021-11-29T04:51:40.028248Z","shell.execute_reply.started":"2021-11-29T04:51:39.96353Z","shell.execute_reply":"2021-11-29T04:51:40.027573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['image_id','video_id', 'x', 'y', 'w', 'h']\nnew_data_df = pd.DataFrame(index=[], columns=cols)\n\nfor index, row in data_df.iterrows():\n    annotations = eval(row['annotations'])\n    for annotation in annotations:\n        tmp_row = pd.Series({\"image_id\":row[\"image_id\"],\"video_id\":row[\"video_id\"], \"x\":annotation[\"x\"], \"y\":annotation[\"y\"],\"w\":annotation[\"width\"],\"h\":annotation[\"height\"]})\n        new_data_df = new_data_df.append(tmp_row, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:51:40.030342Z","iopub.execute_input":"2021-11-29T04:51:40.030611Z","iopub.status.idle":"2021-11-29T04:52:11.317856Z","shell.execute_reply.started":"2021-11-29T04:51:40.030576Z","shell.execute_reply":"2021-11-29T04:52:11.317109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df = new_data_df","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:11.319059Z","iopub.execute_input":"2021-11-29T04:52:11.319416Z","iopub.status.idle":"2021-11-29T04:52:11.324069Z","shell.execute_reply.started":"2021-11-29T04:52:11.319379Z","shell.execute_reply":"2021-11-29T04:52:11.323246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\ndf_folds = data_df[['image_id']].copy()\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('image_id').count()\ndf_folds.loc[:, 'video_id'] = data_df[['image_id', 'video_id']].groupby('image_id').min()['video_id']\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['video_id'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x // 15}').values.astype(str)\n)\ndf_folds.loc[:, 'fold'] = 0\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:11.325724Z","iopub.execute_input":"2021-11-29T04:52:11.326081Z","iopub.status.idle":"2021-11-29T04:52:11.841145Z","shell.execute_reply.started":"2021-11-29T04:52:11.326043Z","shell.execute_reply":"2021-11-29T04:52:11.840404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_transforms():\n    return A.Compose(\n        [\n            A.RandomSizedCrop(min_max_height=(600, 600), height=720, width=1280, p=0.5),\n            A.OneOf([\n                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n                                     val_shift_limit=0.2, p=0.9),\n                A.RandomBrightnessContrast(brightness_limit=0.2, \n                                           contrast_limit=0.2, p=0.9),\n            ],p=0.9),\n            A.ToGray(p=0.01),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.Resize(height=512, width=512, p=1),\n            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:11.842481Z","iopub.execute_input":"2021-11-29T04:52:11.842727Z","iopub.status.idle":"2021-11-29T04:52:11.852585Z","shell.execute_reply.started":"2021-11-29T04:52:11.842693Z","shell.execute_reply":"2021-11-29T04:52:11.851624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"TRAIN_ROOT_PATH = '../input/tensorflow-great-barrier-reef/train_images'\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, marking, image_ids, transforms=None, test=False):\n        super().__init__()\n\n        self.image_ids = image_ids\n        self.marking = marking\n        self.transforms = transforms\n        self.test = test\n\n    def __getitem__(self, index: int):\n        video_id_image_id = self.image_ids[index]\n        video_id_image_ids = video_id_image_id.split('-')\n        video_id = video_id_image_ids[0]\n        image_id = video_id_image_ids[1]\n        \n        image, boxes = self.load_image_and_boxes(index)\n\n        # there is only one class\n        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n\n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n                    break\n\n        return image, target, image_id, video_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def load_image_and_boxes(self, index):\n        video_id_image_id = self.image_ids[index]\n        video_id_image_ids = video_id_image_id.split('-')\n        video_id = \"video_\" + video_id_image_ids[0]\n        image_id = video_id_image_ids[1]\n        \n        img_path = f'{TRAIN_ROOT_PATH}/{video_id}/{image_id}.jpg'\n        \n        assert os.path.isfile(img_path) == True\n        \n        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        records = self.marking[self.marking['image_id'] == video_id_image_id]\n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        return image, boxes","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:11.853962Z","iopub.execute_input":"2021-11-29T04:52:11.854447Z","iopub.status.idle":"2021-11-29T04:52:11.873076Z","shell.execute_reply.started":"2021-11-29T04:52:11.854411Z","shell.execute_reply":"2021-11-29T04:52:11.872264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold_number = 0\n\ntrain_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n    marking=data_df,\n    transforms=get_train_transforms(),\n    test=False,\n)\n\nvalidation_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n    marking=data_df,\n    transforms=get_valid_transforms(),\n    test=True,\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:11.874732Z","iopub.execute_input":"2021-11-29T04:52:11.875209Z","iopub.status.idle":"2021-11-29T04:52:11.88943Z","shell.execute_reply.started":"2021-11-29T04:52:11.875171Z","shell.execute_reply":"2021-11-29T04:52:11.888305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example","metadata":{}},{"cell_type":"code","source":"# If you find the image looks red, please run the cell again several times. \n# It is just caused by switching R and B chanell in visualization (maybe).\n# I will solve this bug in the next version.\nimage, target, image_id, video_id = train_dataset[0]\nboxes = target['boxes'].cpu().numpy().astype(np.int32)\nnumpy_image = image.permute(1,2,0).cpu().numpy()\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(numpy_image, (box[1], box[0]), (box[3],  box[2]), (0, 1, 0), 2)\n    \nax.set_axis_off()\nax.imshow(numpy_image);","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:11.892706Z","iopub.execute_input":"2021-11-29T04:52:11.892951Z","iopub.status.idle":"2021-11-29T04:52:12.549624Z","shell.execute_reply.started":"2021-11-29T04:52:11.89292Z","shell.execute_reply":"2021-11-29T04:52:12.54884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:12.550544Z","iopub.execute_input":"2021-11-29T04:52:12.550761Z","iopub.status.idle":"2021-11-29T04:52:12.558024Z","shell.execute_reply.started":"2021-11-29T04:52:12.550732Z","shell.execute_reply":"2021-11-29T04:52:12.557396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nclass Fitter:\n    \n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n\n        self.base_dir = f'./{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}/log.csv'\n        self.best_summary_loss = 10**5\n\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        print(f'Fitter prepared. Device is {self.device}')\n        self.log(f'status,epoch,loss,time')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                print(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss = self.train_one_epoch(train_loader)\n\n            self.log(f'train,{self.epoch},{summary_loss.avg:.5f},{(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n\n            t = time.time()\n            summary_loss = self.validation(validation_loader)\n\n            self.log(f'val,{self.epoch},{summary_loss.avg:.5f},{(time.time() - t):.5f}')\n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n                    os.remove(path)\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        t = time.time()\n        for step, sample in enumerate(val_loader):\n            images = sample[0]\n            targets = sample[1]\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n                boxes = [target['boxes'].to(self.device).float() for target in targets]\n                labels = [target['labels'].to(self.device).float() for target in targets]\n\n                loss, _, _ = self.model(images, boxes, labels)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n        return summary_loss\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        t = time.time()\n            \n        # for step, (images, targets, image_ids) in enumerate(train_loader):\n        for step, sample in enumerate(train_loader):\n            images = sample[0]\n            targets = sample[1]\n            \n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            images = torch.stack(images)\n            images = images.to(self.device).float()\n            batch_size = images.shape[0]\n            boxes = [target['boxes'].to(self.device).float() for target in targets]\n            labels = [target['labels'].to(self.device).float() for target in targets]\n\n            self.optimizer.zero_grad()\n            \n            loss, _, _ = self.model(images, boxes, labels)\n            \n            loss.backward()\n\n            summary_loss.update(loss.detach().item(), batch_size)\n\n            self.optimizer.step()\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n        return summary_loss\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:12.559302Z","iopub.execute_input":"2021-11-29T04:52:12.55959Z","iopub.status.idle":"2021-11-29T04:52:12.593133Z","shell.execute_reply.started":"2021-11-29T04:52:12.559551Z","shell.execute_reply":"2021-11-29T04:52:12.59241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers = 2\n    batch_size = 16\n    n_epochs = 40#4\n    lr = 0.0002\n\n    folder = 'effdet'\n\n    # -------------------\n    verbose = True\n    verbose_step = 1\n    # -------------------\n\n    # --------------------\n    step_scheduler = False  # do scheduler.step after optimizer.step\n    validation_scheduler = True  # do scheduler.step after validation stage loss\n\n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.5,\n        patience=1,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    # --------------------","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:12.594611Z","iopub.execute_input":"2021-11-29T04:52:12.595071Z","iopub.status.idle":"2021-11-29T04:52:12.605979Z","shell.execute_reply.started":"2021-11-29T04:52:12.595006Z","shell.execute_reply":"2021-11-29T04:52:12.605265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef run_training():\n    device = torch.device('cuda:0')\n    net.to(device)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TrainGlobalConfig.batch_size,\n        sampler=RandomSampler(train_dataset),\n        pin_memory=False,\n        drop_last=True,\n        num_workers=TrainGlobalConfig.num_workers,\n        collate_fn=collate_fn,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n        collate_fn=collate_fn,\n    )\n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n    fitter.fit(train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:12.607131Z","iopub.execute_input":"2021-11-29T04:52:12.607482Z","iopub.status.idle":"2021-11-29T04:52:12.618633Z","shell.execute_reply.started":"2021-11-29T04:52:12.607443Z","shell.execute_reply":"2021-11-29T04:52:12.617825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\nfrom effdet.efficientdet import HeadNet\n\ndef get_net():\n    config = get_efficientdet_config('tf_efficientdet_d0')\n    config.image_size = 512\n    config.norm_kwargs=dict(eps=.001, momentum=.01)\n\n    net = EfficientDet(config, pretrained_backbone=False)\n    checkpoint = torch.load('../input/efficientdet-init-weights/efficientdet_d0-d92fd44f.pth')\n\n    net.load_state_dict(checkpoint)\n    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n\n    return DetBenchTrain(net, config)\n\nnet = get_net()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:12.61989Z","iopub.execute_input":"2021-11-29T04:52:12.620145Z","iopub.status.idle":"2021-11-29T04:52:18.508817Z","shell.execute_reply.started":"2021-11-29T04:52:12.62011Z","shell.execute_reply":"2021-11-29T04:52:18.508023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_training()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T04:52:18.510048Z","iopub.execute_input":"2021-11-29T04:52:18.5103Z","iopub.status.idle":"2021-11-29T05:39:36.356405Z","shell.execute_reply.started":"2021-11-29T04:52:18.510265Z","shell.execute_reply":"2021-11-29T05:39:36.355571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training time,loss summary\nI tried one epoch training for d0 through d7.  \nHere is the summary of training time and loss.  \nIt depends on the situation. So please use it for rough indication.  \nI don't know comparing the loss between different models is meaningfull but I add the section for them.\n\n|  D#  |  Time of one epoch[sec] <br>total/train/val  |Loss of one epoch <br> train/val |\n| :----: | :----: |:----: |\n|  0  |  384/319/65 | 20491/3643 |\n|  1  |  418/351/67 | 22103/4575 |\n|  2  |  451/381/70 | 18487/2568 |\n|  3  |  536/461/75 | 13545/1556 |\n|  4  |  632/554/78 | 12094/1421 |\n|  5  | 739/661/78  | 7951/839 |\n|  6  |  779/691/88 | 7504/589 |\n|  7  | 763/676/87  | 12749/901 |\n\n<div style=\"text-align: center;\">\nNote. this data were created on version 2 on 28/11/2021.\n</div>","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"num_d = np.array([0, 1, 2, 3, 4, 5, 6, 7])\ntime = np.array([384, 418, 451,536,632,739,779,763])\nlabel = [\"D0\", \"D1\", \"D2\", \"D3\", \"D4\", \"D5\", \"D6\", \"D7\"]\nplt.bar(num_d, time, tick_label=label, align=\"center\")\nplt.title(\"Relationship between D# and time for one epoch  \")\nplt.xlabel(\"D#\")\nplt.ylabel(\"Time [sec]\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:39:36.358518Z","iopub.execute_input":"2021-11-29T05:39:36.35902Z","iopub.status.idle":"2021-11-29T05:39:36.598061Z","shell.execute_reply.started":"2021-11-29T05:39:36.358976Z","shell.execute_reply":"2021-11-29T05:39:36.597396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot log","metadata":{}},{"cell_type":"code","source":"log_df = pd.read_csv('./effdet/log.csv')\ntrain_log_df = log_df[log_df[\"status\"]==\"train\"]\nval_log_df = log_df[log_df[\"status\"]==\"val\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:39:36.599281Z","iopub.execute_input":"2021-11-29T05:39:36.599576Z","iopub.status.idle":"2021-11-29T05:39:36.610739Z","shell.execute_reply.started":"2021-11-29T05:39:36.59954Z","shell.execute_reply":"2021-11-29T05:39:36.610016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_log_df.plot(x='epoch', y='loss')\nplt.ylabel(u'loss') \nplt.title(u'train loss', size=16)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:39:36.612338Z","iopub.execute_input":"2021-11-29T05:39:36.612626Z","iopub.status.idle":"2021-11-29T05:39:36.840879Z","shell.execute_reply.started":"2021-11-29T05:39:36.61259Z","shell.execute_reply":"2021-11-29T05:39:36.840174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_log_df.plot(x='epoch', y='loss')\nplt.ylabel(u'loss')\nplt.title(u'validation loss', size=16)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:39:36.842145Z","iopub.execute_input":"2021-11-29T05:39:36.842403Z","iopub.status.idle":"2021-11-29T05:39:37.059159Z","shell.execute_reply.started":"2021-11-29T05:39:36.842368Z","shell.execute_reply":"2021-11-29T05:39:37.058501Z"},"trusted":true},"execution_count":null,"outputs":[]}]}