{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importy","metadata":{}},{"cell_type":"code","source":"import warnings\nimport ast\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom shutil import copyfile\nfrom IPython.display import display\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport random\n%matplotlib inline\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:28:41.256559Z","iopub.execute_input":"2022-02-11T12:28:41.256861Z","iopub.status.idle":"2022-02-11T12:28:41.514967Z","shell.execute_reply.started":"2022-02-11T12:28:41.256779Z","shell.execute_reply":"2022-02-11T12:28:41.514073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Przygotowanie środowiska","metadata":{}},{"cell_type":"code","source":"DATASET_PATH = '/kaggle/working/dataset'\nIMAGES_TRAIN = f\"{DATASET_PATH}/images_train\"\nIMAGES_VAL = f\"{DATASET_PATH}/images_val\"\n\nPROJECT   = 'great-barrier-reef'\n\nFOLD = 1\nMODEL     = 'yolov5s6'\nOPTMIZER  = 'Adam'\nIMGSIZE = 2000\nBATCH     = 4\nEPOCHS    = 18\nNAME      = f'{MODEL}-imgSize{IMGSIZE}-epochs{EPOCHS}-fold{FOLD}'\ncolors = None\n\nCONF = 0.3\nIOU = 0.4\n\n!mkdir {DATASET_PATH}\n!mkdir {IMAGES_VAL}\n!mkdir {IMAGES_TRAIN}","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:28:41.516609Z","iopub.execute_input":"2022-02-11T12:28:41.516937Z","iopub.status.idle":"2022-02-11T12:28:43.453596Z","shell.execute_reply.started":"2022-02-11T12:28:41.516898Z","shell.execute_reply":"2022-02-11T12:28:43.452725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ładowanie wcześniej wytrenowanego modelu","metadata":{}},{"cell_type":"code","source":"\n!mkdir -p /root/.config/Ultralytics/ && cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/\nMODEL =  f\"/kaggle/input/starfishmodel/best\"","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:28:43.457025Z","iopub.execute_input":"2022-02-11T12:28:43.457246Z","iopub.status.idle":"2022-02-11T12:28:44.135439Z","shell.execute_reply.started":"2022-02-11T12:28:43.457217Z","shell.execute_reply":"2022-02-11T12:28:44.134579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Funkcje pomocnicze","metadata":{}},{"cell_type":"markdown","source":"## Bbox-utils (awsaf49)","metadata":{}},{"cell_type":"code","source":"#https://github.com/awsaf49/bbox/blob/main/bbox/utils.py\n\ndef coco2yolo(bboxes, height=720, width=1280):\n    bboxes[..., 0::2] /= width\n    bboxes[..., 1::2] /= height\n    bboxes[..., 0:2] += bboxes[..., 2:4]/2\n    return bboxes\n\ndef voc2coco(bboxes, height=720, width=1280):\n    bboxes[..., 2:4] -= bboxes[..., 0:2]\n    return bboxes\n\ndef coco2voc(bboxes, height=720, width=1280):\n    bboxes[..., 2:4] += bboxes[..., 0:2]\n    return bboxes\n\ndef clip_bbox(bboxes_voc, height=720, width=1280):\n    bboxes_voc[..., 0::2] = bboxes_voc[..., 0::2].clip(0, width)\n    bboxes_voc[..., 1::2] = bboxes_voc[..., 1::2].clip(0, height)\n    return bboxes_voc\n\ndef voc2yolo(bboxes, height=720, width=1280):\n#     bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int  \n    bboxes[..., 0::2] /= width\n    bboxes[..., 1::2] /= height\n    bboxes[..., 2] -= bboxes[..., 0]\n    bboxes[..., 3] -= bboxes[..., 1]\n    bboxes[..., 0] += bboxes[..., 2]/2\n    bboxes[..., 1] += bboxes[..., 3]/2\n    \n    return bboxes\n\ndef str2annot(data):\n    data  = data.replace('\\n', ' ')\n    data  = data.strip().split(' ')\n    data  = np.array(data)\n    annot = data.astype(float).reshape(-1, 5)\n    return annot\n\ndef annot2str(data):\n    data   = data.astype(str)\n    string = '\\n'.join([' '.join(annot) for annot in data])\n    return string\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    if bbox_format == 'yolo':\n        for idx in range(len(bboxes)):     \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            if cls in show_classes:\n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n                h  = round(float(bbox[3])*image.shape[0]/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n    elif bbox_format == 'coco':\n        for idx in range(len(bboxes)):    \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    elif bbox_format == 'voc':\n        for idx in range(len(bboxes)):  \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n    return image","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:28:44.137554Z","iopub.execute_input":"2022-02-11T12:28:44.138099Z","iopub.status.idle":"2022-02-11T12:28:44.264233Z","shell.execute_reply.started":"2022-02-11T12:28:44.138058Z","shell.execute_reply":"2022-02-11T12:28:44.263483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Funkcje z Great-Barrier-Reef: YOLOv5 (awsaf49)","metadata":{}},{"cell_type":"code","source":"def load_model(ckpt_path, conf=0.25, iou=0.50):\n    model = torch.hub.load('/kaggle/input/yolov5-lib-ds',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 1000  # maximum number of detections per image\n    return model\n\ndef predict(model, img, size=768, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    global colors\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    colors = (0, 255 ,0) if colors is None else colors\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:28:44.267033Z","iopub.execute_input":"2022-02-11T12:28:44.267303Z","iopub.status.idle":"2022-02-11T12:28:44.280502Z","shell.execute_reply.started":"2022-02-11T12:28:44.267251Z","shell.execute_reply":"2022-02-11T12:28:44.279856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Własne","metadata":{}},{"cell_type":"code","source":"def get_path(row):\n    return f'{dataset_root}train_images/video_{row.video_id}/{row.video_frame}.jpg'\n\ndef extractLables(df, dest_path):\n    for i in range(len(df)):\n        row = df.iloc[i]\n        with open(f\"{dest_path}/{row.image_id}.txt\", \"w\") as f:\n            if(row.annotations == \"[]\"):\n                f.write('')\n                continue\n            bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy()\n            bboxes_voc  = coco2voc(bboxes_coco, row.height, row.width)\n            bboxes_voc  = clip_bbox(bboxes_voc, row.height, row.width)\n            bboxes_yolo = voc2yolo(bboxes_voc, row.height, row.width).astype(str)\n            \n            labels = np.array([0]*len(bboxes_coco))[..., None].astype(str)\n            annots = np.concatenate([labels, bboxes_yolo], axis=1)\n            string = annot2str(annots)\n            f.write(string)\n            \ndef fancy_rhea_augumentation(path):\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    i =  random.randint(0, 4)\n    if(i == 0 or i == 4):\n        #contrast\n        rand = random.uniform(0.3, 1.2)\n        alpha = rand\n        img = cv2.convertScaleAbs(img, alpha=alpha)\n    if (i == 1 or i == 4):\n        #sharpness\n        kernel = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])\n        img = cv2.filter2D(img, -1, kernel)\n    if (i == 2 or i == 4):\n        #saturation\n        img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n        (h, s, v) = cv2.split(img_hsv)\n        rand = random.choice([2,3,0.5])\n        if(rand == 0.5):\n            s = s * 2\n        else:\n            s = s // rand \n        s = np.clip(s,0,255)\n        imghsv = cv2.merge([h,s,v])\n        img = cv2.cvtColor(imghsv.astype(\"uint8\"), cv2.COLOR_HSV2BGR)\n    if (i == 3 or i == 4):\n        #noise\n        gauss = np.random.normal(0,1,img.size)\n        gauss = gauss.reshape(img.shape[0],img.shape[1],img.shape[2]).astype('uint8')\n        img = img * gauss + img\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:28:44.28217Z","iopub.execute_input":"2022-02-11T12:28:44.282751Z","iopub.status.idle":"2022-02-11T12:28:44.299206Z","shell.execute_reply.started":"2022-02-11T12:28:44.282714Z","shell.execute_reply":"2022-02-11T12:28:44.298565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wczytanie i preprocesowanie danych","metadata":{}},{"cell_type":"code","source":"dataset_root = '/kaggle/input/tensorflow-great-barrier-reef/'\n\ndf = pd.read_csv(f'{dataset_root}/train.csv')\n\ndf[\"width\"] = 1280\ndf[\"height\"] = 720\n\n\ndf = df[df['annotations'] != \"[]\"]\ndf['bboxes'] = df['annotations'].progress_apply(eval)\ndf['bboxes'] = df[\"bboxes\"].progress_apply(get_bbox)\ndf[\"img_path\"] = df.progress_apply(get_path, axis=1)\ndf = df.reset_index(drop=True)\ndf.head(5)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-11T12:28:44.301797Z","iopub.execute_input":"2022-02-11T12:28:44.301989Z","iopub.status.idle":"2022-02-11T12:28:44.793793Z","shell.execute_reply.started":"2022-02-11T12:28:44.301966Z","shell.execute_reply":"2022-02-11T12:28:44.793149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fold","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\n\nkf = GroupKFold(n_splits = 3)\ndf = df.reset_index(drop=True)\ndf['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, groups=df.video_id.tolist())):\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.fold.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:28:44.795112Z","iopub.execute_input":"2022-02-11T12:28:44.795414Z","iopub.status.idle":"2022-02-11T12:28:45.529633Z","shell.execute_reply.started":"2022-02-11T12:28:44.795378Z","shell.execute_reply":"2022-02-11T12:28:45.528951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = df.query(\"fold!=@FOLD\")\nvalid_df = df.query(\"fold==@FOLD\")\nlen(train_df), len(valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:28:45.530945Z","iopub.execute_input":"2022-02-11T12:28:45.531229Z","iopub.status.idle":"2022-02-11T12:28:45.547997Z","shell.execute_reply.started":"2022-02-11T12:28:45.531192Z","shell.execute_reply":"2022-02-11T12:28:45.547243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Przenoszenie zdjęć + augumentacja","metadata":{}},{"cell_type":"code","source":"%ls {IMAGES_VAL} | wc -l\n%ls {IMAGES_TRAIN} | wc -l","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:28:45.549181Z","iopub.execute_input":"2022-02-11T12:28:45.549846Z","iopub.status.idle":"2022-02-11T12:28:46.866857Z","shell.execute_reply.started":"2022-02-11T12:28:45.549807Z","shell.execute_reply":"2022-02-11T12:28:46.866036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(train_df)):\n    row = train_df.iloc[i]\n    copyfile(f'{row.img_path}', f'{IMAGES_TRAIN}/{row.image_id}.jpg')\n#     img = fancy_rhea_augumentation(f'{IMAGES_TRAIN}/{row.image_id}.jpg')\n#     cv2.imwrite(f'{IMAGES_TRAIN}/{row.image_id}_2.jpg', img)\n    \nfor i in range(len(valid_df)):\n    row = valid_df.iloc[i]\n    copyfile(f'{row.img_path}', f'{IMAGES_VAL}/{row.image_id}.jpg')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:28:46.868384Z","iopub.execute_input":"2022-02-11T12:28:46.868775Z","iopub.status.idle":"2022-02-11T12:29:40.541859Z","shell.execute_reply.started":"2022-02-11T12:28:46.86873Z","shell.execute_reply":"2022-02-11T12:29:40.541105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%ls {IMAGES_VAL} | wc -l\n%ls {IMAGES_TRAIN} | wc -l","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:29:40.543227Z","iopub.execute_input":"2022-02-11T12:29:40.543504Z","iopub.status.idle":"2022-02-11T12:29:42.068783Z","shell.execute_reply.started":"2022-02-11T12:29:40.543466Z","shell.execute_reply":"2022-02-11T12:29:42.067929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Zapisywanie labeli do osobnych plików","metadata":{}},{"cell_type":"code","source":"extractLables(train_df, IMAGES_TRAIN)\nextractLables(valid_df, IMAGES_VAL)\n\n# for file_name in os.listdir(IMAGES_TRAIN):\n#     if (\".txt\" not in file_name or \"_2\" in file_name):\n#         continue\n#     source = IMAGES_TRAIN + \"/\" + file_name\n#     copyfile(source, source.replace(\".txt\", \"_2.txt\"))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:29:42.070497Z","iopub.execute_input":"2022-02-11T12:29:42.070764Z","iopub.status.idle":"2022-02-11T12:29:44.715948Z","shell.execute_reply.started":"2022-02-11T12:29:42.070727Z","shell.execute_reply":"2022-02-11T12:29:44.715211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%ls {IMAGES_VAL} | wc -l\n%ls {IMAGES_TRAIN} | wc -l","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:29:44.72265Z","iopub.execute_input":"2022-02-11T12:29:44.724885Z","iopub.status.idle":"2022-02-11T12:29:46.164314Z","shell.execute_reply.started":"2022-02-11T12:29:44.724843Z","shell.execute_reply":"2022-02-11T12:29:46.163511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# YOLOv5","metadata":{}},{"cell_type":"code","source":"import torch\n\n%cd /kaggle/working\n!rm -r /kaggle/working/yolov5\n# !git clone https://github.com/ultralytics/yolov5 # clone\n!cp -r /kaggle/input/yolov5-lib-ds /kaggle/working/yolov5\n%cd yolov5\n%pip install -qr requirements.txt \n\nfrom yolov5 import utils\n# display = utils.notebook_init()  # check","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:29:46.166771Z","iopub.execute_input":"2022-02-11T12:29:46.16706Z","iopub.status.idle":"2022-02-11T12:32:18.83411Z","shell.execute_reply.started":"2022-02-11T12:29:46.16702Z","shell.execute_reply":"2022-02-11T12:32:18.833224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pliki YAML","metadata":{}},{"cell_type":"code","source":"%%writefile {DATASET_PATH}/yolo.yaml\npath: /kaggle/working/dataset/\ntrain: images_train\nval: images_val\n\nnc: 1\nnames: ['starfish']","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:18.836138Z","iopub.execute_input":"2022-02-11T12:32:18.836598Z","iopub.status.idle":"2022-02-11T12:32:18.843209Z","shell.execute_reply.started":"2022-02-11T12:32:18.836558Z","shell.execute_reply":"2022-02-11T12:32:18.842559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile {DATASET_PATH}/hyp.yaml\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 4.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+/- deg)\ntranslate: 0.10  # image translation (+/- fraction)\nscale: 0.5  # image scale (+/- gain)\nshear: 0.0  # image shear (+/- deg)\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\nflipud: 0.5  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 0.5  # image mosaic (probability)\nmixup: 0.5 # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:18.844776Z","iopub.execute_input":"2022-02-11T12:32:18.845463Z","iopub.status.idle":"2022-02-11T12:32:18.857411Z","shell.execute_reply.started":"2022-02-11T12:32:18.845414Z","shell.execute_reply":"2022-02-11T12:32:18.856645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trenowanie","metadata":{}},{"cell_type":"code","source":"# from kaggle_secrets import UserSecretsClient\n# import wandb\n# user_secrets = UserSecretsClient()\n# wandb.login(key=user_secrets.get_secret(\"wandb\"))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:18.858783Z","iopub.execute_input":"2022-02-11T12:32:18.859114Z","iopub.status.idle":"2022-02-11T12:32:18.864679Z","shell.execute_reply.started":"2022-02-11T12:32:18.859075Z","shell.execute_reply":"2022-02-11T12:32:18.863867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python train.py --img {IMGSIZE}\\\n# --batch {BATCH}\\\n# --epochs {EPOCHS}\\\n# --optimizer {OPTMIZER}\\\n# --data {DATASET_PATH}/yolo.yaml\\\n# --hyp {DATASET_PATH}/hyp.yaml\\\n# --weights {MODEL}.pt\\\n# --project {PROJECT}\\\n# --name {NAME}\\\n# --exist-ok","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:18.865974Z","iopub.execute_input":"2022-02-11T12:32:18.866341Z","iopub.status.idle":"2022-02-11T12:32:18.877675Z","shell.execute_reply.started":"2022-02-11T12:32:18.866257Z","shell.execute_reply":"2022-02-11T12:32:18.876847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OUTPUT_DIR = '{}/{}'.format(PROJECT, NAME)\n# MODEL = f\"/kaggle/working/yolov5/{OUTPUT_DIR}/weights/best\"","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:18.87912Z","iopub.execute_input":"2022-02-11T12:32:18.879485Z","iopub.status.idle":"2022-02-11T12:32:18.886807Z","shell.execute_reply.started":"2022-02-11T12:32:18.879449Z","shell.execute_reply":"2022-02-11T12:32:18.886147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predykcja","metadata":{"execution":{"iopub.status.busy":"2022-02-10T18:22:21.235798Z","iopub.execute_input":"2022-02-10T18:22:21.236062Z","iopub.status.idle":"2022-02-10T18:22:21.240015Z","shell.execute_reply.started":"2022-02-10T18:22:21.236036Z","shell.execute_reply":"2022-02-10T18:22:21.239332Z"}}},{"cell_type":"code","source":"model = load_model(MODEL + \".pt\", conf=CONF, iou=IOU)\nimage_paths = train_df[train_df['annotations'] != \"[]\"].sample(100).img_path.tolist()\nfor idx, path in enumerate(image_paths):\n    img = cv2.imread(path)[...,::-1]\n    bboxes, confis = predict(model, img, size=IMGSIZE, augment=False)\n    display(show_img(img, bboxes, bbox_format='coco'))\n    if idx>5:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:18.887764Z","iopub.execute_input":"2022-02-11T12:32:18.890084Z","iopub.status.idle":"2022-02-11T12:32:30.016624Z","shell.execute_reply.started":"2022-02-11T12:32:18.890046Z","shell.execute_reply":"2022-02-11T12:32:30.015641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model(MODEL + \".pt\", conf=CONF, iou=IOU)\nimage_paths = train_df[train_df['annotations'] != \"[]\"].sample(100).img_path.tolist()\nfor idx, path in enumerate(image_paths):\n    img = cv2.imread(path)[...,::-1]\n    bboxes, confis = predict(model, img, size=IMGSIZE, augment=False)\n    display(show_img(img, bboxes, bbox_format='coco'))\n    if idx>5:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:30.018236Z","iopub.execute_input":"2022-02-11T12:32:30.018812Z","iopub.status.idle":"2022-02-11T12:32:31.848026Z","shell.execute_reply.started":"2022-02-11T12:32:30.018767Z","shell.execute_reply":"2022-02-11T12:32:31.84417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model(MODEL + \".pt\", conf=CONF, iou=IOU)\nimage_paths = valid_df[valid_df['annotations'] != \"[]\"].sample(100).img_path.tolist()\nfor idx, path in enumerate(image_paths):\n    img = cv2.imread(path)[...,::-1]\n    bboxes, confis = predict(model, img, size=IMGSIZE, augment=False)\n    display(show_img(img, bboxes, bbox_format='coco'))\n    if idx>5:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:31.850091Z","iopub.execute_input":"2022-02-11T12:32:31.850531Z","iopub.status.idle":"2022-02-11T12:32:33.859216Z","shell.execute_reply.started":"2022-02-11T12:32:31.850435Z","shell.execute_reply":"2022-02-11T12:32:33.858375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predykcja do konkursu","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:33.860846Z","iopub.execute_input":"2022-02-11T12:32:33.861101Z","iopub.status.idle":"2022-02-11T12:32:33.888123Z","shell.execute_reply.started":"2022-02-11T12:32:33.861068Z","shell.execute_reply":"2022-02-11T12:32:33.887468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model(MODEL + \".pt\", conf=CONF, iou=IOU)\nfor idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    bboxes, confs  = predict(model, img, size=IMGSIZE, augment=False)\n    annot          = format_prediction(bboxes, confs)\n    pred_df['annotations'] = annot\n    env.predict(pred_df)\n    if idx<3:\n        display(show_img(img, bboxes, bbox_format='coco'))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:33.889413Z","iopub.execute_input":"2022-02-11T12:32:33.889664Z","iopub.status.idle":"2022-02-11T12:32:35.160122Z","shell.execute_reply.started":"2022-02-11T12:32:33.889629Z","shell.execute_reply":"2022-02-11T12:32:35.159365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:32:35.161891Z","iopub.execute_input":"2022-02-11T12:32:35.162334Z","iopub.status.idle":"2022-02-11T12:32:35.176975Z","shell.execute_reply.started":"2022-02-11T12:32:35.162281Z","shell.execute_reply":"2022-02-11T12:32:35.176185Z"},"trusted":true},"execution_count":null,"outputs":[]}]}