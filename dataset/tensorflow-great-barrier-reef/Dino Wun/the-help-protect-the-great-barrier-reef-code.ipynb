{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **The \"Help Protect the Great Barrier Reef\" Code Explained by Dino** #","metadata":{}},{"cell_type":"markdown","source":"## Intro ##\nThe Great Barrier Reef is the world's largest coral reef species in the world. However, the Great Barrier Reef is at a critical tipping point that will determine its long-term survival. Coral bleaching as a result of global warming is a key reason for the reef's decline. Not only that, the infamous starfishes overpopulate too. Like sea urchins devouring kelp in California (the place where I live), the starfishes devoured every coral, day by day. Because of that, scientists are worried about the corals being eaten up by starfishes, so the Great Barrier Reef Foundation and Google teamed up together to create a one full competition on Kaggle. Today, we are explaining the solution about the \"Help Protect the Great Barrier Reef\" contest.","metadata":{}},{"cell_type":"markdown","source":"## Imports and Modules ##\nBefore classifying the images whether there is a seastar or not, we have to import the neccessary libraries. Importing them can make the Python code classify more and more of seastars well, for, at least.","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport ast\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:04.023504Z","iopub.execute_input":"2021-11-24T06:35:04.023869Z","iopub.status.idle":"2021-11-24T06:35:04.292838Z","shell.execute_reply.started":"2021-11-24T06:35:04.023832Z","shell.execute_reply":"2021-11-24T06:35:04.291732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variables setup ##\nWell, well, we had made our good progress with that imports, let's now define variables! First, define the \"path\" variable, which defines the path of the files toward the \"tensorflow-great-barrier-reef' folder, at least. From there, you can find all 6 files inside listed in the listdir thing.","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/tensorflow-great-barrier-reef/'\nos.listdir(path)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:04.2952Z","iopub.execute_input":"2021-11-24T06:35:04.295527Z","iopub.status.idle":"2021-11-24T06:35:04.305952Z","shell.execute_reply.started":"2021-11-24T06:35:04.295486Z","shell.execute_reply":"2021-11-24T06:35:04.304777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And... We have to set up train_data, test_data, and even samp_subm! How? We use pandas to read the csvs with the path variable concatenated with the strings: train.csv, test.csv, and example_sample_submission.csv.","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(path+'train.csv')\ntest_data = pd.read_csv(path+'test.csv')\nsamp_subm = pd.read_csv(path+'example_sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:04.308137Z","iopub.execute_input":"2021-11-24T06:35:04.308769Z","iopub.status.idle":"2021-11-24T06:35:04.415005Z","shell.execute_reply.started":"2021-11-24T06:35:04.308704Z","shell.execute_reply":"2021-11-24T06:35:04.413447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:04.416686Z","iopub.execute_input":"2021-11-24T06:35:04.417032Z","iopub.status.idle":"2021-11-24T06:35:04.440991Z","shell.execute_reply.started":"2021-11-24T06:35:04.416991Z","shell.execute_reply":"2021-11-24T06:35:04.4401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Box Setup Classification of a Crown-of-Thorns Starfish ##\nNow we defined all the variables, then we should know that we have to define the video_frame first, then concatenate the video_frame variable to the '.jpg' string. Then, we can call train_data down towards the train_data key inside, video_frame, and set equal to the video frame variable.","metadata":{}},{"cell_type":"code","source":"video_frame = 621\nfile_name = str(video_frame)+'.jpg'\ntrain_data[train_data['video_frame']==video_frame]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:04.443586Z","iopub.execute_input":"2021-11-24T06:35:04.444167Z","iopub.status.idle":"2021-11-24T06:35:04.465281Z","shell.execute_reply.started":"2021-11-24T06:35:04.444126Z","shell.execute_reply":"2021-11-24T06:35:04.464506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The train_data dictionary returned two results, one of which contained annotations about where the sea stars prowl. Since we know that video_id 1 has annotations, we can assign one of the image_folder variables, which each made the OpenCV library read an image file from 3 different paths, from the video_0 path to the video_2 path.","metadata":{}},{"cell_type":"code","source":"image_folder_0 = cv2.imread(path+'train_images/video_0/'+file_name)\nimage_folder_1 = cv2.imread(path+'train_images/video_1/'+file_name)\nimage_folder_2 = cv2.imread(path+'train_images/video_2/'+file_name)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:04.467133Z","iopub.execute_input":"2021-11-24T06:35:04.467626Z","iopub.status.idle":"2021-11-24T06:35:04.601782Z","shell.execute_reply.started":"2021-11-24T06:35:04.467581Z","shell.execute_reply":"2021-11-24T06:35:04.600804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have to call out the train_data dictionary variable again, but this time, we combine the two key terms: the one with the video_frame key equals to the video frame variable and the one with the video_id key equals to the number which contained the annotations. If the number in the video_id has the annotation, then, the row is displayed, that contains the coordinates.","metadata":{}},{"cell_type":"code","source":"train_data[(train_data['video_frame']==video_frame)&(train_data['video_id']==1)]['annotations']","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:04.603429Z","iopub.execute_input":"2021-11-24T06:35:04.604072Z","iopub.status.idle":"2021-11-24T06:35:04.615803Z","shell.execute_reply.started":"2021-11-24T06:35:04.604028Z","shell.execute_reply":"2021-11-24T06:35:04.614798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since row 7329 contains the annotations, we want to set the variable row to the row where it contained the annotations.","metadata":{}},{"cell_type":"code","source":"row = 7329","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:04.617296Z","iopub.execute_input":"2021-11-24T06:35:04.618326Z","iopub.status.idle":"2021-11-24T06:35:04.631868Z","shell.execute_reply.started":"2021-11-24T06:35:04.618275Z","shell.execute_reply":"2021-11-24T06:35:04.630894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can assign the boxes variable to the ast module over the literal_eval function, over the .loc dataframe over the locations of the annotations by the row variable.","metadata":{}},{"cell_type":"code","source":"boxes = ast.literal_eval(train_data.loc[row, 'annotations'])","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:04.633652Z","iopub.execute_input":"2021-11-24T06:35:04.635048Z","iopub.status.idle":"2021-11-24T06:35:04.652724Z","shell.execute_reply.started":"2021-11-24T06:35:04.634982Z","shell.execute_reply":"2021-11-24T06:35:04.651044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now let's plot! ##","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nax.imshow(image_folder_1)\nfor box in boxes:\n    p = matplotlib.patches.Rectangle((box['x'], box['y']), box['width'], box['height'],\n                                     ec='r', fc='none', lw=2.)\n    ax.add_patch(p)\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.savefig(\"oh_no_one_reef_eater.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:04.654283Z","iopub.execute_input":"2021-11-24T06:35:04.655454Z","iopub.status.idle":"2021-11-24T06:35:05.575432Z","shell.execute_reply.started":"2021-11-24T06:35:04.655396Z","shell.execute_reply":"2021-11-24T06:35:05.574692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alas, we see at least one crown-of-thorns starfish looking for coral to devour. But if there are more crown-of-thorns starfish lurking around the coral reef, how does the machine detect more of those? Well, let's find out ways to tackle the reef-eater problem.","metadata":{}},{"cell_type":"markdown","source":"## Multi-Box Object Detection over more \"Reef-Eaters\" ##\nAfter we detect one sea-star, we are going detect more of that! First of all, we now define row again first, then define file_name again, but this time, locate the row with the video_frame key in train_data and concatenate it with .jpg inside the string. And then, we are now define video_folder variable with the string of video_ concatenated with the string of the location of train_data over the video_id key and the row variable. Lastly, we can again assign the boxes variable to the ast module over the literal_eval function, over the .loc dataframe over the locations of the annotations by the row variable.","metadata":{}},{"cell_type":"code","source":"row = 2845\nfile_name = str(train_data.loc[row, 'video_frame'])+'.jpg'\nvideo_folder = 'video_'+str(train_data.loc[row, 'video_id'])\nboxes = ast.literal_eval(train_data.loc[row, 'annotations'])","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:05.576398Z","iopub.execute_input":"2021-11-24T06:35:05.576628Z","iopub.status.idle":"2021-11-24T06:35:05.583162Z","shell.execute_reply.started":"2021-11-24T06:35:05.5766Z","shell.execute_reply":"2021-11-24T06:35:05.582239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('video folder:', video_folder)\nprint('file name:', file_name)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:05.58468Z","iopub.execute_input":"2021-11-24T06:35:05.584966Z","iopub.status.idle":"2021-11-24T06:35:05.601452Z","shell.execute_reply.started":"2021-11-24T06:35:05.584931Z","shell.execute_reply":"2021-11-24T06:35:05.59976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can define the image variable with OpenCV using concatenation under the imread attribute. If we find out the shape of the image variable, it will return the tuple containing three numbers of the entire path.","metadata":{}},{"cell_type":"code","source":"image = cv2.imread(path+'train_images/'+video_folder+'/'+file_name)\nimage.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:05.602988Z","iopub.execute_input":"2021-11-24T06:35:05.603651Z","iopub.status.idle":"2021-11-24T06:35:05.657759Z","shell.execute_reply.started":"2021-11-24T06:35:05.603608Z","shell.execute_reply":"2021-11-24T06:35:05.656874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## And now, let's plot again! ##","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nax.imshow(image)\nfor box in boxes:\n    p = matplotlib.patches.Rectangle((box['x'], box['y']), box['width'], box['height'],\n                                     ec='r', fc='none', lw=2.)\n    ax.add_patch(p)\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:05.660821Z","iopub.execute_input":"2021-11-24T06:35:05.661258Z","iopub.status.idle":"2021-11-24T06:35:06.270335Z","shell.execute_reply.started":"2021-11-24T06:35:05.66122Z","shell.execute_reply":"2021-11-24T06:35:06.26924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our model now captured three reef-eating sea-stars in this image. After we detected all of the seastars in some images, now it's time to submit predictions.","metadata":{}},{"cell_type":"markdown","source":"## API Usage ##\nSince we detected all of the seastars in all of the images, let's use the api afterall. In order to use the api, we have to follow directions of [this notebook made by Sohier](https://www.kaggle.com/sohier/great-barrier-reef-api-tutorial/notebook).","metadata":{}},{"cell_type":"code","source":"import PIL.Image\n\n# these sys calls aren't actually necesarry in Kaggle notebooks, but you may need to add the data directory to your pythonpath to run the sample API off of Kaggle\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')   \n\nimport greatbarrierreef\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test() ","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:06.271414Z","iopub.execute_input":"2021-11-24T06:35:06.271663Z","iopub.status.idle":"2021-11-24T06:35:06.362757Z","shell.execute_reply.started":"2021-11-24T06:35:06.271633Z","shell.execute_reply":"2021-11-24T06:35:06.361449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pixel_array, sample_prediction_df = next(iter_test)\npixel_array","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:35:29.465826Z","iopub.execute_input":"2021-11-24T06:35:29.466866Z","iopub.status.idle":"2021-11-24T06:35:29.862142Z","shell.execute_reply.started":"2021-11-24T06:35:29.466822Z","shell.execute_reply":"2021-11-24T06:35:29.861438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PIL.Image.fromarray(pixel_array)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:39:44.10845Z","iopub.execute_input":"2021-11-24T06:39:44.108766Z","iopub.status.idle":"2021-11-24T06:39:44.669406Z","shell.execute_reply.started":"2021-11-24T06:39:44.108732Z","shell.execute_reply":"2021-11-24T06:39:44.667633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## And finally, let's submit the data!!!!! ##","metadata":{}},{"cell_type":"code","source":"samp_subm.to_csv('submission.csv', index=False) # Remember: name your submission, submission.csv!","metadata":{"execution":{"iopub.status.busy":"2021-11-24T06:40:31.864157Z","iopub.execute_input":"2021-11-24T06:40:31.864657Z","iopub.status.idle":"2021-11-24T06:40:31.872882Z","shell.execute_reply.started":"2021-11-24T06:40:31.864607Z","shell.execute_reply":"2021-11-24T06:40:31.871799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion ##\nJob well done, we just detect a lot of starfishes in a pile of images. Overall, we will know that a lot of environmental conservationists and scientists will inject vinegar over to the infamous reef-eaters and kill them up for once and for all! When there are no crown-of-thorn seastars lurking through the Great Barrier Reef, the corals in there would thrive happily ever after... since I can make up as a fairy tale story in this notebook hehe.","metadata":{}},{"cell_type":"markdown","source":"## Acknowledgements ##\nSpecial thanks to DrCapa, who provided the starter notebook as an example to some people who are new to computer coding competition like me. Details over here: \nhttps://www.kaggle.com/drcapa/great-barrier-reef-starter","metadata":{}}]}