{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Tensorflow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef)\n> Detect crown-of-thorns starfish in underwater image data\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/31703/logos/header.png?t=2021-10-29-00-30-04\">","metadata":{}},{"cell_type":"markdown","source":"## ÂΩì„Åü„ÇäÂâç„Å´„ÇÑ„Çâ„Çå„Å¶„ÅÑ„Çã„Åì„Å®\n- [ ] Data„ÅÆ‰øÆÊ≠£„ÉªËøΩÂä†\n    - [ ] „Çµ„Ç§„Ç∫„ÅåÂ§ß„Åç„Åô„Åé„Çã„ÉªÂ∞è„Åï„Åô„Åé„Çãbbox„ÇíÁÑ°Ë¶ñÔºàÂÆüË£ÖÔºâ\n    - [ ] Êâã‰ΩúÊ•≠„Åßbbox„ÇíËøΩÂä†ÔºàËß£Ë™¨Ôºâ\n- [ ] Data Augmentation\n    - [ ] Albumentations„É©„Ç§„Éñ„É©„É™„Çí‰Ωø„Å£„ÅüaugmentationÔºàRandomSizedCrop, HueSaturationValue, RandomBrightnessContrast, ToGray, HorizontalFlip, VerticalFlip, Cutout, etcÔºâ\n    - [ ] mixupÔºàÁîªÂÉè„ÅÆÂêàÊàê„ÄÇÂÆüË£ÖÔºâ\n    - [ ] cutmixÔºàcutout+mixup: cutout„Åó„ÅüÈÉ®ÂàÜ„Å´Âà•ÁîªÂÉè„ÇíÂêàÊàê„ÄÇÂÆüË£Ö Ôºâ\n    - [ ] „Ç∏„Ç∞„ÇΩ„Éº„Éë„Ç∫„É´„Å´„Çà„ÇãÁîªÂÉèÁîüÊàêÔºàÊó¢Ëø∞Ôºâ\n- [ ] „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÈÅ∏Êäû\n    - [ ] YOLOÔºà„Ç≥„É≥„ÉöÂèÇÂä†Áõ¥Âæå„Å´Ëß¶„Å£„Å¶„ÅÑ„Åü„ÄÇv5„ÅØ„É©„Ç§„Çª„É≥„Çπ„ÅÆÂïèÈ°å„Åß‰ΩøÁî®Á¶ÅÊ≠¢„Å´„ÄÇÂçòÁã¨„Åß„ÅØ„Åä„Åù„Çâ„ÅèÊúÄÈ´òÁ≤æÂ∫¶„ÅåÂá∫„Åõ„Çã„É¢„Éá„É´„Å†„Å£„ÅüÔºâ\n    - [ ] EfficientDetÔºàYOLOv5„ÅåÁ¶ÅÊ≠¢„Å´„Å™„Å£„Å¶„Åã„Çâ„ÅØ„Å≤„Åü„Åô„ÇâD5„Çí‰∏≠ÂøÉ„Å´EfficientDet„ÅßÂÆüÈ®ì„Åó„Å¶„ÅÑ„Åü„ÄÇEfficientNet„ÅÆËÄÉ„ÅàÊñπ„ÇíÂèñ„ÇäÂÖ•„Çå„ÅüÁâ©‰ΩìÊ§úÂá∫„É¢„Éá„É´„ÄÇÂÆüË£ÖÔºâ\n    - [ ] ‰ªñ„ÅØË©¶„Åó„Å¶„Å™„ÅÑ„Åå„ÄÅDetectorRS„ÇÑUniverseNet„ÅåËâØ„ÅÑ„Å™„Å©„ÅÆÂ†±Âëä„ÅÇ„Çä\n- [ ] È´òËß£ÂÉèÂ∫¶„ÅßÂ≠¶Áøí\n    - [ ] „É™„Çµ„Ç§„Ç∫„ÇíË°å„Çè„Åö1024 x 1024„ÅÆÁîªÂÉè„ÅßÂ≠¶ÁøíÔºàColab Pro„Åß„ÅØbatch size 1„Åß„ÇÆ„É™„ÇÆ„É™CUDA out of memory„ÇíÂõûÈÅø„Åß„Åç„ÇãÔºâ\n- [x] TTAÔºà„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÇÇaugmentation„ÄÇÂÆüË£ÖÔºâ\n- [ ] Pseudo Labeling („ÉÜ„Éº„Éñ„É´„Éá„Éº„Çø„Åß„ÇÇ„ÅäÈ¶¥Êüì„Åø„ÄÅ„ÉÜ„Çπ„Éà„Éá„Éº„Çø„Çí‰∫àÊ∏¨„ÅóÁ¢∫‰ø°Â∫¶„ÅÆÈ´ò„ÅÑ„É©„Éô„É´„ÅÆ„ÅøË®ìÁ∑¥„Éá„Éº„Çø„Å´Âèñ„ÇäÂÖ•„Çå„Å¶ÂÜç‰∫àÊ∏¨„ÄÇÂÆüË£Ö)\n- [ ] Ensemble (Á≤æÂ∫¶„ÇíÊ±Ç„ÇÅ„ÇãKaggle„Åß„ÅØWBF„ÅåÂº∑„ÅÑÂ†¥Âêà„ÅåÂ§ö„Åù„ÅÜ„ÄÇÂÆüË£Ö, Ëß£Ë™¨)\n    - [ ] NMSÔºàIoU„Åå„ÅÇ„ÇãÈñæÂÄ§„ÇíË∂Ö„Åà„Å¶Èáç„Å™„Å£„Å¶„ÅÑ„Çãbbox„ÅÆÈõÜÂêà„Åã„Çâ„ÄÅ„Çπ„Ç≥„Ç¢„ÅåÊúÄÂ§ß„ÅÆbbox„ÇíÊÆã„Åó„Å¶„ÄÅ„Åù„Çå‰ª•Â§ñ„ÇíÈô§ÂéªÔºâ\n    - [ ] SoftNMSÔºàIoUÈñæÂÄ§„ÇíË∂Ö„Åà„Åübbox„ÇíÊÆã„Åó„Å§„Å§„ÄÅ„Çπ„Ç≥„Ç¢„ÅåÊúÄÂ§ß„ÅÆbbox‰ª•Â§ñ„ÇÇÈô§Âéª„Åõ„Åö„ÄÅ„Çπ„Ç≥„Ç¢„ÇíÂâ≤„ÇäÂºï„ÅÑ„Å¶ÊÆã„ÅôÔºâ\n    - [ ] NMW (Èáç„Å™„Çä„ÅÇ„Å£„Åübbox„Çí„Çπ„Ç≥„Ç¢„Å®IoU„ÅßÈáç„Åø‰ªò„Åë„Åó„Å¶Ë∂≥„ÅóÂêà„Çè„Åõ„Çã„Åì„Å®„Åß„ÄÅ1„Å§„ÅÆÊñ∞„Åü„Å™bbox„Çí‰Ωú„ÇäÂá∫„Åô)\n    - [ ] WBFÔºàÊ§úÂá∫„Åï„Çå„Åü„É¢„Éá„É´„ÅÆÊï∞„ÅåÂ∞ë„Å™„ÅÑbbox„Åª„Å©„Çπ„Ç≥„Ç¢„Çí‰∏ã„Åí„Çã„Åì„Å®„Åß„ÄÅÂ∞ëÊï∞„ÅÆ„É¢„Éá„É´„Å†„Åë„ÅßÊ§úÂá∫„Åï„Çå„Åübbox„Çí„Çπ„Ç≥„Ç¢„ÅßË∂≥Âàá„Çä„Åô„ÇãÔºâ","metadata":{}},{"cell_type":"markdown","source":"„Ç¢„Ç§„Éá„Ç¢\n\n- „Éí„Éà„Éá„ÅåÊò†„Å£„Å¶„Å™„ÅÑÁîªÂÉè„ÇÇÂ≠¶Áøí„Éá„Éº„Çø„Å´„Åó„Åü„Çâ„ÉÄ„É°„ÅãÔºü\n    - ÂÖ®ÈÉ®‰Ωø„ÅÜ„Å®„Åª„Å®„Çì„Å©„Éí„Éà„Éá„ÅåÊò†„Å£„Å¶„Å™„ÅÑ„ÄÅ„Å®Âà§ÂÆö„Åó„Åù„ÅÜ\n        - Âêå„ÅòÊûöÊï∞(5k)„Å†„Å£„Åü„Çâ„ÅÑ„ÅÑ„Åã„ÇÇÔºü\n- „É¢„Éá„É´„ÇíÂ§â„Åà„Çã\n    - YoloX\n    - EfficientDet\n    - FasterRCNN\n    - DETR\n- nofair tracking\n- „Ç≥„É≥„Éà„É©„Çπ„ÉàÂùáÁ≠âÂåñ„Åô„Çã\n- Augmentation„Çí„Åó„Å™„ÅÑ\n    - „É°„É¢„É™ÁØÄÁ¥Ñ„ÅÆ„Åü„ÇÅ\n- Adam„Çí‰Ωø„ÅÜ","metadata":{}},{"cell_type":"markdown","source":"„Çè„Åã„Å£„Åü„Åì„Å®\n- yolov5„Å´„Å§„ÅÑ„Å¶\n    - Â≠¶Áøí„Å´„ÅØtrain.py„Å®„ÅÑ„ÅÜÂÖÉ„ÄÖ„ÅÇ„Çã„Ç≥„Éº„Éâ„Çí‰Ωø„Å£„Å¶„ÅÑ„Çã\n    - „Åù„Çå„Å´ÂºïÊï∞„Å®„Åó„Å¶Â≠¶Áøí„Éá„Éº„Çø„ÅÆ„Éë„Çπ„ÇÑ„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅÆÊÉÖÂ†±„ÅåË®òËø∞„Åï„Çå„Åü.yaml„Éï„Ç°„Ç§„É´„Çí‰∏é„Åà„Å¶„ÅÑ„Çã","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"„ÇÑ„Çã‰∫ã\n- yolov5„Å®„ÅØ‰Ωï„ÅãË™øÊüª\n    - ‰Ωï„ÇíÂ≠¶Áøí„Åô„Çã„ÅÆ„ÅãÔºü\n    - Á≤æÂ∫¶„Å®„Åã„ÅØ„Å©„ÅÜ„ÇÑ„Å£„Å¶Âá∫„Åô„ÅÆ„ÅãÔºü\n    - Ë¶ã„Å§„Åë„Åü„ÅÑ„ÇÇ„ÅÆ„ÅåÊò†„Å£„Å¶„Å™„ÅÑ„Éá„Éº„Çø„ÅØÂ≠¶Áøí„Å´Âê´„ÇÅ„Å¶„Çà„ÅÑ„ÅÆ„ÅãÔºü\n- „ÅÑ„Çâ„Å™„ÅÑÈÉ®ÂàÜ„ÇíÂâä„Å£„Å¶„Ç∑„É≥„Éó„É´„Å´„Åô„Çã\n- W&B„Å´„É≠„Ç∞„Ç§„É≥„Åó„Å¶Â≠¶ÁøíÁµåÈÅé„ÅÆ„Ç∞„É©„Éï„Å™„Å©„ÇíË¶ã„Çã\n    - ~„Å®„ÅÑ„ÅÜ„Åã„ÄÅÁ¥∞„Åã„ÅÑÂá¶ÁêÜ„Å®„Åãwandb„Å´ÁôªÈå≤„Åó„Å¶„Åì„Å£„Å°„Åß„ÇÑ„Å£„Å¶„Åù„ÅÜ~\n- „ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„Åô„Çã„É¢„Éá„É´5ÂÄã‰Ωú„Å£„Å¶„Ç¢„É≥„Çµ„É≥„Éñ„É´","metadata":{}},{"cell_type":"markdown","source":"## „Éï„Ç°„Ç§„É´ÊßãÈÄ†\n\nworking/labels/‰ª•‰∏ã„Å´„Åô„Åπ„Å¶„ÅÆ„Éï„Ç°„Ç§„É´„ÅÆlabel.txt„ÅåÂÖ•„Å£„Å¶„ÅÑ„Çã\n\nimages„ÇíÂêå„Åò","metadata":{}},{"cell_type":"markdown","source":"# üõ† Install Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -qU wandb\n!pip install -qU bbox-utility # check https://github.com/awsaf49/bbox for source code","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:22:47.906495Z","iopub.execute_input":"2022-02-03T12:22:47.906966Z","iopub.status.idle":"2022-02-03T12:23:08.793093Z","shell.execute_reply.started":"2022-02-03T12:22:47.906852Z","shell.execute_reply":"2022-02-03T12:23:08.792117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìö Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\n\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\n\nfrom joblib import Parallel, delayed\n\nfrom IPython.display import display, HTML\n\nfrom matplotlib import animation, rc\nrc('animation', html='jshtml')\n\n# for DA\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.utils.data as Data\nimport ast #?\nfrom fastprogress.fastprogress import master_bar, progress_bar #?","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-03T12:23:08.79951Z","iopub.execute_input":"2022-02-03T12:23:08.801189Z","iopub.status.idle":"2022-02-03T12:23:10.813715Z","shell.execute_reply.started":"2022-02-03T12:23:08.801145Z","shell.execute_reply":"2022-02-03T12:23:10.812994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìå Key-Points\n* Êèê‰æõ„Åï„Çå„Å¶„ÅÑ„ÇãpythonÊôÇÁ≥ªÂàóAPI„Çí‰ΩøÁî®„Åó„Å¶‰∫àÊ∏¨„ÇíÈÄÅ‰ø°„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ„Åì„ÅÆ„Ç≥„É≥„ÉÜ„Çπ„Éà„ÅØ‰ª•Ââç„ÅÆ„Ç™„Éñ„Ç∏„Çß„ÇØ„ÉàÊ§úÂá∫„Ç≥„É≥„ÉÜ„Çπ„Éà„Å®„ÅØÁï∞„Å™„Çä„Åæ„Åô„ÄÇ\n* ÂêÑ‰∫àÊ∏¨Ë°å„Å´„ÅØ„ÄÅÁîªÂÉè„ÅÆ„Åô„Åπ„Å¶„ÅÆÂ¢ÉÁïå„Éú„ÉÉ„ÇØ„Çπ„ÇíÂê´„ÇÅ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÊèêÂá∫„ÅØ„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÇÇCOCO„ÅÆ„Çà„ÅÜ„Åß„Åô„ÄÇ„Åì„Çå„ÅØ`[x_min„ÄÅy_min„ÄÅÂπÖ„ÄÅÈ´ò„Åï]`„ÇíÊÑèÂë≥„Åó„Åæ„Åô\n* Copmetition„É°„Éà„É™„ÉÉ„ÇØF2„ÅØ„ÄÅ„Éí„Éà„Éá„ÇíË¶ãÈÄÉ„Åô„Åì„Å®„Åå„Åª„Å®„Çì„Å©„Å™„ÅÑ„Åì„Å®„Çí‰øùË®º„Åô„Çã„Åü„ÇÅ„Å´„ÄÅ„ÅÑ„Åè„Å§„Åã„ÅÆË™§Ê§úÁü•ÔºàFPÔºâ„ÇíË®±ÂÆπ„Åó„Åæ„Åô„ÄÇ„Å§„Åæ„Çä„ÄÅË™§Ê§úÁü•ÔºàFNÔºâ„ÅØ„ÄÅË™§Ê§úÁü•ÔºàFPÔºâ„Çà„Çä„ÇÇÈáçË¶Å„Åß„Åô„ÄÇ\n$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$","metadata":{}},{"cell_type":"markdown","source":"# ‚≠ê WandB","metadata":{}},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_team_iforine\")\n    wandb.login(key=api_key)\n    anonymous = None\nexcept:\n    wandb.login(anonymous='must')\n    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:10.814839Z","iopub.execute_input":"2022-02-03T12:23:10.815072Z","iopub.status.idle":"2022-02-03T12:23:13.145894Z","shell.execute_reply.started":"2022-02-03T12:23:10.815039Z","shell.execute_reply":"2022-02-03T12:23:13.145118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìñ Meta Data\n* `train_images/` - Folder containing training set photos of the form `video_{video_id}/{video_frame}.jpg`.\n\n* `[train/test].csv` - ÁîªÂÉè„ÅÆ„É°„Çø„Éá„Éº„Çø„Åß„Åô„ÄÇ‰ªñ„ÅÆ„ÉÜ„Çπ„Éà„Éï„Ç°„Ç§„É´„Å®ÂêåÊßò„Å´„ÄÅ„ÉÜ„Çπ„Éà„ÅÆ„É°„Çø„Éá„Éº„Çø„Éá„Éº„Çø„ÅÆ„Åª„Å®„Çì„Å©„ÅØ„ÄÅÊèêÂá∫ÊôÇ„Å´„Åó„Åã„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„Å´Ë°®Á§∫„Åï„Çå„Åæ„Åõ„Çì„ÄÇ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åß„Åç„Çã„ÅÆ„ÅØÊúÄÂàù„ÅÆÊï∞Ë°å„Å†„Åë„Åß„Åô„ÄÇ\n\n* `video_id` - ÁîªÂÉè„ÅåÂê´„Åæ„Çå„Çã„Éì„Éá„Ç™„ÅÆIDÁï™Âè∑„ÄÇ„Éì„Éá„Ç™ID„ÅØÊÑèÂë≥„ÅÆ„ÅÇ„ÇãÈ†ÜÂ∫è„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n* `video_frame` - Êò†ÂÉèÂÜÖ„ÅÆÁîªÂÉè„ÅÆ„Éï„É¨„Éº„É†Áï™Âè∑„Åß„Åô„ÄÇ„ÉÄ„Ç§„Éê„Éº„ÅåÊµÆ‰∏ä„Åó„Åü„Å®„Åç„Åã„Çâ„Éï„É¨„Éº„É†Áï™Âè∑„Å´„Åö„Çå„ÅåÁîü„Åò„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.ÊåáÂÆö„Åï„Çå„Åü„Éì„Éá„Ç™„ÅÆ„ÇÆ„É£„ÉÉ„Éó„Éï„É™„ÉºÈÉ®ÂàÜÈõÜÂêà„ÅÆID„ÄÇ„Ç∑„Éº„Ç±„É≥„ÇπID„ÅØÊÑèÂë≥„ÅÆ„ÅÇ„ÇãÈ†ÜÂ∫è„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n* `sequence_frame` - ÊåáÂÆö„Åï„Çå„Åü„Ç∑„Éº„Ç±„É≥„ÇπÂÜÖ„ÅÆ„Éï„É¨„Éº„É†Áï™Âè∑„ÄÇ\n* `image_id` - ID code for the image, in the format `{video_id}-{video_frame}`\n* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate `(x_min, y_min)` of its lower left corner within the image together with its `width` and `height` in pixels --> (COCO format).Python„ÅßÁõ¥Êé•Ë©ï‰æ°ÂèØËÉΩ„Å™ÊñáÂ≠óÂàóÂΩ¢Âºè„ÅÆ„Éí„Éà„ÉáÊ§úÂá∫„ÅÆ„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÄÇÊèêÂá∫„Åô„Çã‰∫àÊ∏¨ÂÄ§„Å®Âêå„Åò„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇtest.csv„Åß„ÅØÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅØ„ÄÅÁîªÂÉèÂÜÖ„ÅÆÂ∑¶‰∏ãÈöÖ„ÅÆ„Éî„ÇØ„Çª„É´Â∫ßÊ®ô `(x_min, y_min)` „Å®„ÄÅ„Éî„ÇØ„Çª„É´Âçò‰Ωç„ÅÆ `width` „Å® `height` „ÅßË®òËø∞„Åï„Çå„Çã --> (COCO ÂΩ¢Âºè)„ÄÇ","metadata":{}},{"cell_type":"code","source":"FOLD      = 4 # which fold to train\nDIM       = 2016\nMODEL     = 'yolov5s'\nBATCH     = 4\nEPOCHS    = 10\nOPTIM     = 'Adam'\nAUG       = 'HE'\n\nPROJECT   = 'iforine/great-barrier-reef-public' # w&b in yolov5\nNAME      = f'{MODEL}-dim{DIM}-fold{FOLD}-bat{BATCH}-opt{OPTIM}-aug{AUG}-epch{EPOCHS}-addNoCot' # w&b for yolov5\n\nREMOVE_NOBBOX = False # remove images with no bbox\nADD_NOBBOX = True # bbox„ÅÆ„ÅÇ„ÇãÁîªÂÉè„Å®Âêå„ÅòÊûöÊï∞ÂàÜbbox„ÅÆÁÑ°„ÅÑÁîªÂÉè„ÇíÂ≠¶Áøí„Éá„Éº„Çø„Å´Âä†„Åà„Çã\nROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\nIMAGE_DIR = '/kaggle/working/images' # directory to save images\nLABEL_DIR = '/kaggle/working/labels' # directory to save labels\n\nWORKER = 4 # „Çà„Åè„Çè„Åã„Å£„Å¶„Å™„ÅÑ„ÄÇ„Çπ„É¨„ÉÉ„Éâ„ÅÆÊï∞„Å®„ÅãÔºü\n\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:13.147475Z","iopub.execute_input":"2022-02-03T12:23:13.148445Z","iopub.status.idle":"2022-02-03T12:23:13.155912Z","shell.execute_reply.started":"2022-02-03T12:23:13.148401Z","shell.execute_reply":"2022-02-03T12:23:13.155246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Directories","metadata":{}},{"cell_type":"code","source":"!mkdir -p {IMAGE_DIR}\n!mkdir -p {LABEL_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:13.159112Z","iopub.execute_input":"2022-02-03T12:23:13.159665Z","iopub.status.idle":"2022-02-03T12:23:14.477984Z","shell.execute_reply.started":"2022-02-03T12:23:13.159636Z","shell.execute_reply":"2022-02-03T12:23:14.477058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Paths","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', 10)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:14.480137Z","iopub.execute_input":"2022-02-03T12:23:14.480387Z","iopub.status.idle":"2022-02-03T12:23:14.486042Z","shell.execute_reply.started":"2022-02-03T12:23:14.480359Z","shell.execute_reply":"2022-02-03T12:23:14.485276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pandas.eval()\n\nÊßò„ÄÖ„Å™„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„Çí‰ΩøÁî®„Åó„Å¶„ÄÅPythonÂºè„ÇíÊñáÂ≠óÂàó„Å®„Åó„Å¶Ë©ï‰æ°„Åó„Åæ„Åô„ÄÇ","metadata":{}},{"cell_type":"code","source":"# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf['old_image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\ndf['image_path']  = f'{IMAGE_DIR}/'+df.image_id+'.jpg' # '/kaggle/working/images'\ndf['label_path']  = f'{LABEL_DIR}/'+df.image_id+'.txt' # '/kaggle/working/labels'\ndf['annotations'] = df['annotations'].progress_apply(eval) # apply(ÂêÑË¶ÅÁ¥†„Å´Èñ¢Êï∞„ÇíÈÅ©Áî®„Åô„Çã)„ÅÆÈÄ≤Êçó„ÇíË°®Á§∫„Åô„Çã„ÄÇeval„ÅØ‰ΩïÔºü\ndisplay(df.head(100))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:14.48773Z","iopub.execute_input":"2022-02-03T12:23:14.488296Z","iopub.status.idle":"2022-02-03T12:23:15.063406Z","shell.execute_reply.started":"2022-02-03T12:23:14.48826Z","shell.execute_reply":"2022-02-03T12:23:15.062616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of BBoxes\n> Nearly 80% images are without any bbox.","metadata":{}},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts(normalize=True)*100 # „É¶„Éã„Éº„ÇØ„Å™Ë¶ÅÁ¥†„ÅÆÂá∫ÁèæÈ†ªÂ∫¶„ÇíÁÆóÂá∫„ÄÇnormalize=True„Å´„Åô„Çã„Å®ÂêàË®à„Åå1„Å´„Å™„Çã„Çà„ÅÜ„Å´Ê≠£Ë¶èÂåñ„Åï„Çå„Çã(Ââ≤Âêà„Å´„Å™„Çã)\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:15.064874Z","iopub.execute_input":"2022-02-03T12:23:15.065134Z","iopub.status.idle":"2022-02-03T12:23:15.163444Z","shell.execute_reply.started":"2022-02-03T12:23:15.065097Z","shell.execute_reply":"2022-02-03T12:23:15.159608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Error displaying widget: model not found`\n\n‰Ωï„Çâ„Åã„ÅÆÂéüÂõ†„Åßprogress„ÇíÂá∫„Åõ„Å™„ÅÑ„ÅÆ„Åã„ÇÇ„ÄÇ„É¢„Éá„É´„Åå„Å™„ÅÑ„ÄÅ„Å®„ÅØ„Å©„ÅÜ„ÅÑ„ÅÜ„Åì„Å®Ôºü","metadata":{}},{"cell_type":"markdown","source":"ÂÆüÈöõ„Å´bbox„ÅÆÂ≠òÂú®„Åô„ÇãÁîªÂÉè„ÇíË¶ã„Å¶„Åø„Çã","metadata":{}},{"cell_type":"markdown","source":"# üßπ Clean Data\n* In this notebook, we use only **bboxed-images** (`~5k`). We can use all `~23K` images for train but most of them don't have any labels. So it would be easier to carry out experiments using only **bboxed images**.\n* „Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„Åß„ÅØ„ÄÅbboxed-imagesÔºà„Äú5kÔºâ„ÅÆ„Åø„Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇtrain„Å´„ÅØÁ¥Ñ23K„ÅÆÁîªÂÉè„Çí„Åô„Åπ„Å¶‰ΩøÁî®„Åß„Åç„Åæ„Åô„Åå„ÄÅ„Åª„Å®„Çì„Å©„ÅÆÁîªÂÉè„Å´„ÅØ„É©„Éô„É´„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Åó„Åü„Åå„Å£„Å¶„ÄÅbboxedÁîªÂÉè„ÅÆ„Åø„Çí‰ΩøÁî®„Åó„Å¶ÂÆüÈ®ì„ÇíÂÆüË°å„Åô„ÇãÊñπ„ÅåÁ∞°Âçò„Åß„Åô","metadata":{}},{"cell_type":"code","source":"if REMOVE_NOBBOX:\n    df = df.query(\"num_bbox>0\") # df[df['num_bbox'] > 0]„Å®ÂêåÁ≠â„ÄÇÁõ¥Ë¶≥ÁöÑ„Åß‰æøÂà©„Å†„Éª„Éª„Éª","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:15.164541Z","iopub.execute_input":"2022-02-03T12:23:15.164934Z","iopub.status.idle":"2022-02-03T12:23:15.169313Z","shell.execute_reply.started":"2022-02-03T12:23:15.164895Z","shell.execute_reply":"2022-02-03T12:23:15.168461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ËÉåÊôØÁîªÂÉè„ÇíÂÖ®‰Ωì„ÅÆ10%Âê´„ÇÅ„Çã\nif ADD_NOBBOX:\n    df = pd.concat([df.query(\"num_bbox>0\"), df.query(\"num_bbox==0\").sample(int(len(df.query(\"num_bbox>0\")) * 0.1))])","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:15.170638Z","iopub.execute_input":"2022-02-03T12:23:15.171465Z","iopub.status.idle":"2022-02-03T12:23:15.201889Z","shell.execute_reply.started":"2022-02-03T12:23:15.171427Z","shell.execute_reply":"2022-02-03T12:23:15.201164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚úèÔ∏è Write Images\n* We need to copy the Images to Current Directory(`/kaggle/working`) as `/kaggle/input` doesn't have **write access** which is needed for **YOLOv5**.\n* We can make this process faster using **Joblib** which uses **Parallel** computing.\n\n* / kaggle / input„Å´„ÅØYOLOv5„Å´ÂøÖË¶Å„Å™Êõ∏„ÅçËæº„Åø„Ç¢„ÇØ„Çª„ÇπÊ®©„Åå„Å™„ÅÑ„Åü„ÇÅ„ÄÅ„Ç§„É°„Éº„Ç∏„ÇíÁèæÂú®„ÅÆ„Éá„Ç£„É¨„ÇØ„Éà„É™Ôºà/ kaggle / workingÔºâ„Å´„Ç≥„Éî„Éº„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n* „Åì„ÅÆÂá¶ÁêÜ„ÇíÈ´òÈÄüÂåñ„Åô„Çã„Å´„ÅØ„ÄÅ**‰∏¶Âàó**Ë®àÁÆó„ÇíÂà©Áî®„Åô„Çã**Joblib**„Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ","metadata":{}},{"cell_type":"markdown","source":"shutil.copyfile(src, dst, *, follow_symlinks=True)\n\nsrc „Å®„ÅÑ„ÅÜÂêçÂâç„ÅÆ„Éï„Ç°„Ç§„É´„ÅÆÂÜÖÂÆπ („É°„Çø„Éá„Éº„Çø„ÇíÂê´„Åæ„Å™„ÅÑ) „Çí dst „Å®„ÅÑ„ÅÜÂêçÂâç„ÅÆ„Éï„Ç°„Ç§„É´„Å´„Ç≥„Éî„Éº„Åó„ÄÅÊúÄ„ÇÇÂäπÁéáÁöÑ„Å™ÊñπÊ≥ï„Åß dst „ÇíËøî„Åó„Åæ„Åô„ÄÇ src „Å® dst „ÅØ path-like object „Åæ„Åü„ÅØÊñáÂ≠óÂàó„Åß„Éë„ÇπÂêç„ÇíÊåáÂÆö„Åó„Åæ„Åô„ÄÇ","metadata":{}},{"cell_type":"code","source":"def make_copy(row):\n    shutil.copyfile(row.old_image_path, row.image_path)\n    return","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:15.204028Z","iopub.execute_input":"2022-02-03T12:23:15.20446Z","iopub.status.idle":"2022-02-03T12:23:15.208881Z","shell.execute_reply.started":"2022-02-03T12:23:15.204423Z","shell.execute_reply":"2022-02-03T12:23:15.20811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"‰∏¶ÂàóÂá¶ÁêÜ\n\n```\njoblib.Parallel(<Parallel„Å∏„ÅÆÂºïÊï∞>)(\n    joblib.delayed(<ÂÆüË°å„Åô„ÇãÈñ¢Êï∞>)(<Èñ¢Êï∞„Å∏„ÅÆÂºïÊï∞>) for Â§âÊï∞Âêç in „Ç§„ÉÜ„É©„Éñ„É´\n)\n```","metadata":{}},{"cell_type":"markdown","source":"iterrows()„É°„ÇΩ„ÉÉ„Éâ„Çí‰Ωø„ÅÜ„Å®„ÄÅ1Ë°å„Åö„Å§„ÄÅ„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÂêçÔºàË°åÂêçÔºâ„Å®„Åù„ÅÆË°å„ÅÆ„Éá„Éº„ÇøÔºàpandas.SeriesÂûãÔºâ„ÅÆ„Çø„Éó„É´(index, Series)„ÇíÂèñÂæó„Åß„Åç„Çã„ÄÇ","metadata":{}},{"cell_type":"code","source":"image_paths = df.old_image_path.tolist()\n_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(row) for _, row in tqdm(df.iterrows(), total=len(df)))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:15.210201Z","iopub.execute_input":"2022-02-03T12:23:15.210642Z","iopub.status.idle":"2022-02-03T12:23:47.548148Z","shell.execute_reply.started":"2022-02-03T12:23:15.210604Z","shell.execute_reply":"2022-02-03T12:23:47.547506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî® Helper","metadata":{}},{"cell_type":"code","source":"# check https://github.com/awsaf49/bbox for source code of following utility functions\n# ‰ΩúËÄÖ„Åå‰Ωú„Å£„Åü„Éò„É´„Éë„ÉºÈñ¢Êï∞\nfrom bbox.utils import coco2yolo, coco2voc, voc2yolo\nfrom bbox.utils import draw_bboxes, load_image\nfrom bbox.utils import clip_bbox, str2annot, annot2str\n\n# bbox„Çí„É™„Çπ„Éà„Å´„Åó„Å¶Ëøî„Åô\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\n# row„Å´ÂπÖ„Å®È´ò„Åï„ÅÆÂàó„ÇíËøΩÂä†„Åô„Çã\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path']) # „Åì„ÅÆimagesize„Å£„Å¶‰ΩïÔºü\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-03T12:23:47.54924Z","iopub.execute_input":"2022-02-03T12:23:47.549434Z","iopub.status.idle":"2022-02-03T12:23:48.177477Z","shell.execute_reply.started":"2022-02-03T12:23:47.549409Z","shell.execute_reply":"2022-02-03T12:23:48.176769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create BBox","metadata":{}},{"cell_type":"code","source":"# annotions„Åã„ÇâbboxesÂàó„Çí‰ΩúÊàê\ndf['bboxes'] = df.annotations.progress_apply(get_bbox)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:48.181172Z","iopub.execute_input":"2022-02-03T12:23:48.181406Z","iopub.status.idle":"2022-02-03T12:23:49.823586Z","shell.execute_reply.started":"2022-02-03T12:23:48.18138Z","shell.execute_reply":"2022-02-03T12:23:49.8229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Image-Size\n> All Images have same dimension, [Width, Height] =  `[1280, 720]`","metadata":{}},{"cell_type":"code","source":"df['width']  = 1280\ndf['height'] = 720\ndisplay(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:49.824893Z","iopub.execute_input":"2022-02-03T12:23:49.825291Z","iopub.status.idle":"2022-02-03T12:23:49.843354Z","shell.execute_reply.started":"2022-02-03T12:23:49.825254Z","shell.execute_reply":"2022-02-03T12:23:49.842622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üè∑Ô∏è Create Labels\nWe need to export our labels to **YOLO** format, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required). The *.txt file specifications are:\n\n* One row per object\n* Each row is class `[x_center, y_center, width, height]` format.\n* Box coordinates must be in **normalized** `xywh` format (from `0 - 1`). If your boxes are in pixels, divide `x_center` and `width` by `image width`, and `y_center` and `height` by `image height`.\n* Class numbers are **zero-indexed** (start from `0`).\n\n> Competition bbox format is **COCO** hence `[x_min, y_min, width, height]`. So, we need to convert form **COCO** to **YOLO** format.\n\nÂêÑÁîªÂÉè„Å´ÂØæ„Åó„Å¶.txt„Éï„Ç°„Ç§„É´„Çí‰Ωú„Å£„Å¶YOLO„Å´ÂØæÂøú„Åô„ÇãÂΩ¢Âºè„Å´„Åô„Çã\n\n* „Ç™„Éñ„Ç∏„Çß„ÇØ„Éà‰∏Ä„Å§„Å´„Å§„Åç1Ë°å\n* ÂêÑË°å‰ª•‰∏ã„ÅÆ„Éï„Ç©„Éº„Éû„ÉÉ„Éà`[x_center, y_center, width, height]`\n* boxÂ∫ßÊ®ô„ÅØ**0-1„ÅßÊ≠£Ë¶èÂåñ„Åï„Çå„Åü**xywh„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÄÇ„Éî„ÇØ„Çª„É´Âçò‰Ωç„ÅÆÂ†¥Âêà„ÅØ„ÄÅ `x_center` „Å® `width` „Çí `image width` „ÅßÂâ≤„Å£„Å¶„ÄÅ `y_center` „Å® `height` „Çí `image height` „ÅßÂâ≤„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n* „ÇØ„É©„ÇπÁï™Âè∑„ÅØ **0„Åã„ÇâÂßã„Åæ„Çã„Çº„É≠„Éª„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ** „Åß„Åô„ÄÇ\n\n> „Ç≥„É≥„Éö„ÅÆbboxÂΩ¢Âºè„ÅØCOCO„Åß„ÅÇ„Çã„Åü„ÇÅ„ÄÅ[x_min„ÄÅy_min„ÄÅwidth„ÄÅheight]„Åß„Åô„ÄÇ„Åó„Åü„Åå„Å£„Å¶„ÄÅ„Éï„Ç©„Éº„É†COCO„ÇíYOLOÂΩ¢Âºè„Å´Â§âÊèõ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', 100) # Âàó„ÅåÂ§ö„ÅÑ„Å®ÁúÅÁï•„Åï„Çå„Çã„ÅÆ„ÇíÈò≤„Åê","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:49.84488Z","iopub.execute_input":"2022-02-03T12:23:49.845369Z","iopub.status.idle":"2022-02-03T12:23:49.851906Z","shell.execute_reply.started":"2022-02-03T12:23:49.845331Z","shell.execute_reply":"2022-02-03T12:23:49.851253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"„Åì„Çå„Åå„É©„Éô„É´Ôºü3„ÅÆ„Å®„Åì„Å´„ÅØbbox„ÅÆÊï∞„ÅåÂÖ•„Çã","metadata":{}},{"cell_type":"markdown","source":"coco => **[xmin, ymin, w, h]**\n\nvoc  => **[xmin, ymin, xmax, ymax]**\n\nyolo => **[xmid, ymid, w, h]** (normalized)\n\n```\ndef clip_bbox(bboxes_voc, height=720, width=1280):\n\n    Clip bounding boxes to image boundaries.\n    Args:\n        bboxes_voc (np.ndarray): bboxes in [xmin, ymin, xmax, ymax] format.\n        height (int, optional): height of bbox. Defaults to 720.\n        width (int, optional): width of bbox. Defaults to 1280.\n    Returns:\n        np.ndarray : clipped bboxes in [xmin, ymin, xmax, ymax] format.\n```","metadata":{}},{"cell_type":"markdown","source":"## label.txt„Çí‰ΩúÊàê","metadata":{}},{"cell_type":"code","source":"cnt = 0\nall_bboxes = []\nbboxes_info = []\nfor row_idx in tqdm(range(df.shape[0])):\n    row = df.iloc[row_idx]\n    image_height = row.height\n    image_width  = row.width\n    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy() #bboxes„ÇínumpyÂΩ¢Âºè„Å´Â§âÊèõ\n    num_bbox     = len(bboxes_coco)\n    names        = ['cots']*num_bbox\n    labels       = np.array([0]*num_bbox)[..., None].astype(str) # Ê¨°ÂÖÉ„Çí++(„É™„Çπ„Éà„Åã„Çâshape:(N, 1)„ÅÆË°åÂàó„Å∏)\n    ## Create Annotation(YOLO)\n    with open(row.label_path, 'w') as f:\n        if num_bbox<1:\n            annot = ''\n            f.write(annot)\n            cnt+=1\n            continue\n        bboxes_voc  = coco2voc(bboxes_coco, image_height, image_width)\n        bboxes_voc  = clip_bbox(bboxes_voc, image_height, image_width)\n        bboxes_yolo = voc2yolo(bboxes_voc, image_height, image_width).astype(str)\n        all_bboxes.extend(bboxes_yolo.astype(float)) # all_bboxes„Å´bboxes_yolo„ÇíËøΩÂä†\n        bboxes_info.extend([[row.image_id, row.video_id, row.sequence]]*len(bboxes_yolo)) # bboxes_info„Å´bbox„ÅÆÊï∞„Å†„Åë[image_id, video_id, sequence]„ÇíËøΩÂä†\n        annots = np.concatenate([labels, bboxes_yolo], axis=1) # labels„ÅÆÊ®™„Å´bboxes_yolo„Çí„Åè„Å£„Å§„Åë„Çã(bbox„ÅÆÊï∞„Å†„ÅëË°å„Åå„Åß„Åç„Çã)\n        string = annot2str(annots) # annotation„Çístr„Å´„Åó„Å¶„Çã\n        f.write(string)\nprint('Missing:',cnt)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-03T12:23:49.853375Z","iopub.execute_input":"2022-02-03T12:23:49.853931Z","iopub.status.idle":"2022-02-03T12:23:54.691956Z","shell.execute_reply.started":"2022-02-03T12:23:49.853893Z","shell.execute_reply":"2022-02-03T12:23:54.69119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ÊåáÂÆö„Åó„ÅüÂ†¥ÊâÄ`kaggle/image„Å®„Åãlabel`„Å´„Éï„Ç°„Ç§„É´„Åå„Åß„Åç„Å™„ÅÑ„ÅûÔºüÔºüÔºü","metadata":{}},{"cell_type":"markdown","source":"# üìÅ Create Folds\n> Number of samples aren't same in each fold which can create large variance in **Cross-Validation**.\n\n> ÂêÑ„Éï„Ç©„Éº„É´„Éâ„ÅÆ„Çµ„É≥„Éó„É´Êï∞„ÅåÂêå„Åò„Åß„Å™„ÅÑ„Åü„ÇÅ„ÄÅ„ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„ÅßÂ§ß„Åç„Å™„Å∞„Çâ„Å§„Åç„ÅåÁîü„Åò„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n\nGroupKFold: Âêå„Åò„Ç∞„É´„Éº„Éó„ÅåÁï∞„Å™„ÇãÂàÜÂâ≤„Éë„Çø„Éº„É≥„Å´Âá∫Áèæ„Åó„Å™„ÅÑ„Çà„ÅÜ„Å´„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂàÜÂâ≤„Åô„Çã„ÄÇ\nÂèÇËÄÉÔºöhttps://upura.hatenablog.com/entry/2018/12/04/224436\n\n> „ÇØ„É©„Çπ„Å®„ÅØÂà•„ÅÆÊ¶ÇÂøµ„Å®„Åó„Å¶„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰Ωì„ÅØÂùáÁ≠â„Å™10„Ç∞„É´„Éº„Éó„Å´ÂàÜÂâ≤„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Ç∞„É´„Éº„Éó„ÅØ„Å™„Åã„Å™„Åã„Ç§„É°„Éº„Ç∏„Åå‰ªò„Åç„Å•„Çâ„ÅÑ„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„Åå„ÄÅ‰æã„Åà„Å∞„ÄåÂêå„Åò„É¶„Éº„Ç∂„ÅÆ„Éá„Éº„Çø„Çí‰∏Ä„Å§„ÅÆ„Ç∞„É´„Éº„Éó„Å´„Åæ„Å®„ÇÅ„Å¶„Åä„Åè„Äç„Å®„ÅÑ„Å£„Åü‰Ωø„ÅÑÊñπ„ÅåÊÉ≥ÂÆö„Åß„Åç„Åæ„Åô„ÄÇ**Âêå„Åò„É¶„Éº„Ç∂„ÅÆ„Éá„Éº„Çø„Ååtrain„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å®validation„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ‰∏°ËÄÖ„Å´Â≠òÂú®„Åô„Çã„Å®„ÄÅ‰∏çÂΩì„Å´Á≤æÂ∫¶„ÅåÈ´ò„Åè„Å™„ÇãÊÅê„Çå„Åå„ÅÇ„Çã**„Åü„ÇÅ„Åß„Åô„ÄÇ\n\n‰ªäÂõû„ÅØÂãïÁîª„ÅÆÊï∞(`len(df['sequence'].unique())`„ÅÆ‰∫ã)","metadata":{}},{"cell_type":"markdown","source":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold\n\n> class sklearn.model_selection.GroupKFold(n_splits=5)[source]\n>\n> „Ç™„Éº„Éê„Éº„É©„ÉÉ„Éó„Åó„Å™„ÅÑ„Ç∞„É´„Éº„Éó„ÇíÊåÅ„Å§K-fold„Ç§„ÉÜ„É¨„Éº„Çø„ÅÆÂ§âÂΩ¢„ÄÇ\n>\n> Âêå„Åò„Ç∞„É´„Éº„Éó„Åå2„Å§„ÅÆÁï∞„Å™„Çã„Éï„Ç©„Éº„É´„Éâ„Å´Áèæ„Çå„Çã„Åì„Å®„ÅØ„ÅÇ„Çä„Åæ„Åõ„ÇìÔºàÁï∞„Å™„Çã„Ç∞„É´„Éº„Éó„ÅÆÊï∞„ÅØ„ÄÅÂ∞ë„Å™„Åè„Å®„ÇÇ„Éï„Ç©„Éº„É´„Éâ„ÅÆÊï∞„Å®Âêå„Åò„Åß„Å™„Åë„Çå„Å∞„Å™„Çä„Åæ„Åõ„ÇìÔºâ„ÄÇ\n>\n> ÂêÑ„Éï„Ç©„Éº„É´„Éâ„ÅØ„ÄÅ„Åù„Çå„Åû„Çå„ÅÆ„Éï„Ç©„Éº„É´„Éâ„ÅßÁï∞„Å™„Çã„Ç∞„É´„Éº„Éó„ÅÆÊï∞„Åå„Åª„ÅºÂêå„Åò„Å®„ÅÑ„ÅÜÊÑèÂë≥„Åß„ÄÅ„Åª„Åº„Éê„É©„É≥„Çπ„ÅåÂèñ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nkf = GroupKFold(n_splits = 5) # n_split: train,val„ÅÆ„Éë„Çø„Éº„É≥„ÅÆÊï∞„ÄÇÂÖÉ„Éá„Éº„Çø„Çí5„Éë„Çø„Éº„É≥„ÅÆtrain,val„Å´ÂàÜ„Åë„Çã\ndf = df.reset_index(drop=True)\ndf['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, y = df.video_id.tolist(), groups=df.sequence)): # sequence: ÂãïÁîª„ÅÆ„Çµ„Éñ„Çª„ÉÉ„ÉàID(Âêå„ÅòID„ÅÆÁîªÂÉß„ÅØÂêå„ÅòÂãïÁîª)\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.fold.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:54.693614Z","iopub.execute_input":"2022-02-03T12:23:54.694126Z","iopub.status.idle":"2022-02-03T12:23:55.216792Z","shell.execute_reply.started":"2022-02-03T12:23:54.694084Z","shell.execute_reply":"2022-02-03T12:23:55.216052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚öôÔ∏è Configuration\nThe dataset config file requires\n1. The dataset root directory path and relative paths to `train / val / test` image directories (or *.txt files with image paths)\n2. The number of classes `nc` and \n3. A list of class `names`:`['cots']`\n\n„Éá„Éº„Çø„Çª„ÉÉ„ÉàË®≠ÂÆö„Éï„Ç°„Ç§„É´„Å´„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„ÇÇ„ÅÆ„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ\n1. 1. „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„É´„Éº„Éà„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆ„Éë„Çπ„Å®Ôºå`train / val / test` ÁîªÂÉè„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆÁõ∏ÂØæ„Éë„Çπ („Åæ„Åü„ÅØÁîªÂÉè„Éë„Çπ„ÇíÂê´„ÇÄ *.txt „Éï„Ç°„Ç§„É´)\n2. „ÇØ„É©„ÇπÊï∞ `nc` „Å® \n3. „ÇØ„É©„ÇπÂêç`:`['cots']`„ÅÆ„É™„Çπ„Éà„ÄÇ","metadata":{}},{"cell_type":"code","source":"train_files = []\nval_files   = []\ntrain_df = df.query(\"fold!=@FOLD\")\nvalid_df = df.query(\"fold==@FOLD\")\ntrain_files += list(train_df.image_path.unique())\nval_files += list(valid_df.image_path.unique())\nlen(train_files), len(val_files)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:55.21797Z","iopub.execute_input":"2022-02-03T12:23:55.2183Z","iopub.status.idle":"2022-02-03T12:23:55.237717Z","shell.execute_reply.started":"2022-02-03T12:23:55.218263Z","shell.execute_reply":"2022-02-03T12:23:55.236713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Augmentation**","metadata":{"execution":{"iopub.status.busy":"2022-01-31T13:27:35.106346Z","iopub.execute_input":"2022-01-31T13:27:35.106754Z","iopub.status.idle":"2022-01-31T13:27:35.145508Z","shell.execute_reply.started":"2022-01-31T13:27:35.106708Z","shell.execute_reply":"2022-01-31T13:27:35.144592Z"}}},{"cell_type":"markdown","source":"train_df„Å´ÂØæ„Åó„Å¶DA„Çí„Åã„Åë„Çã\n\n‰ªäÂõû„ÅØbbox„ÅÆ‰ΩçÁΩÆ„ÅåÂ§â„Çè„ÇãÂá¶ÁêÜ„ÅØ„Åó„Å™„ÅÑ(label.txt„ÇíÊµÅÁî®„Åó„Åü„ÅÑ„Åü„ÇÅ)\n\n„Åß„Åç„ÅüÁîªÂÉè„ÅØ`working/images/{video_id}-{video_frame}-aug.jpg`„Å´ÂÖ•„Çå„Çã„ÄÇ\n\n„Åù„ÅÆÁîªÂÉè„Å´ÂØæ„Åô„Çã„É©„Éô„É´„ÅØÂÖÉÁîªÂÉè„ÅÆlabel.txt„Åã„ÇâÊµÅÁî®`working/labels/{video_id}-{video_frame}-aug.txt`„Å´ÂÖ•„Çå„Çã„ÄÇ\n\ntrain_df„Å´Ë°å„ÇíËøΩÂä†„ÄÇ(ÂÖÉÁîªÂÉè„ÅÆË°å„Çí„Ç≥„Éî„Éº„ÄÇimage_path, label_path„Çí‚Üë„ÅÆ„ÇÇ„ÅÆ„Å´Â§â„Åà„Çå„Å∞OK„ÅÆ„ÅØ„Åö)","metadata":{}},{"cell_type":"code","source":"import albumentations as A","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:55.238985Z","iopub.execute_input":"2022-02-03T12:23:55.239447Z","iopub.status.idle":"2022-02-03T12:23:56.344226Z","shell.execute_reply.started":"2022-02-03T12:23:55.239412Z","shell.execute_reply":"2022-02-03T12:23:56.343413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HE_HSV(A.ImageOnlyTransform):\n    def __init__(self, p: float = 0.5, always_apply=False):\n        super().__init__(always_apply, p)\n        \n    def apply(self, image,**params):\n        img_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n        # Histogram equalisation on the V-channel\n        img_hsv[:, :, 2] = cv2.equalizeHist(img_hsv[:, :, 2])\n\n        # convert image back from HSV to RGB\n        image_hsv = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2RGB)\n\n        return image_hsv","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:56.345693Z","iopub.execute_input":"2022-02-03T12:23:56.34597Z","iopub.status.idle":"2022-02-03T12:23:56.352854Z","shell.execute_reply.started":"2022-02-03T12:23:56.345932Z","shell.execute_reply":"2022-02-03T12:23:56.351452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AUG_DATASET(Dataset):\n    \n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        \n    def coco2yolo(self, bboxes, image_height=720, image_width=1280):\n        \"\"\"\n        coco => [xmin, ymin, w, h]\n        yolo => [xmid, ymid, w, h] (normalized)\n        \"\"\"\n        bboxes = np.array(bboxes)\n        bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n        # normolizinig\n        bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n        bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n\n        # converstion (xmin, ymin) => (xmid, ymid)\n        bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n\n        return bboxes\n    \n    def coord_to_box(self, bouding_box, image):\n        box_yolo_format = []\n        height, width = image.shape[0], image.shape[1]\n        \n        if False: # CFG.use_coco2yolo\n            box_yolo_format = self.coco2yolo(bouding_box)\n            box_yolo_format = np.clip(box_yolo_format,0,1)\n            label = np.repeat([0],box_yolo_format.shape[0]).reshape(-1,1)\n            box_yolo_format = np.append(box_yolo_format,label, axis=1)\n        else:\n            for bb in bouding_box:\n                label = [max(0,bb[0]), max(0,bb[1]), min(bb[0]+bb[2], 1280), min(720,bb[1]+bb[3]), '0']\n                bbox_albu = A.convert_bbox_to_albumentations(label, source_format='pascal_voc', rows=height, cols=width)\n                bbox_yolo = A.convert_bbox_from_albumentations(bbox_albu, target_format='yolo', rows=height, cols=width, check_validity=True)\n                clip_box = [np.clip(value,0,1) for value in bbox_yolo[:-1]] + [bbox_yolo[-1]]\n                box_yolo_format.append(clip_box)\n        return box_yolo_format\n\n    def bbox_to_txt(self, bboxes):\n        \"\"\"\n        Convert a list of bbox into a string in YOLO format (to write a file).\n        @bboxes : numpy array of bounding boxes \n        return : a string for each object in new line: <object-class> <x> <y> <width> <height>\n        \"\"\"\n        txt=''\n        for index,l in enumerate(bboxes):\n            l = [str(x) for x in l[:4]]\n            l = ' '.join(l)\n            txt +=  '0' +' ' + l + '\\n'\n        return txt\n\n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self,index):\n        row = self.df.iloc[index]\n        path = row['image_path']\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        #aug_index = row['aug_index']\n        list_info = path.split('/')\n        image_name = list_info[-2] + '_' + list_info[-1].split('.')[0]\n        box = row['bboxes']\n        bounding_box = self.coord_to_box(box, img)\n\n        if self.transform is not None:\n            res = self.transform(image=img, bboxes=bounding_box)\n            img = res['image']\n            bounding_box = res['bboxes']\n            \n        box_yolo_format = self.bbox_to_txt(bounding_box)\n        return img, box_yolo_format, image_name","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:56.354162Z","iopub.execute_input":"2022-02-03T12:23:56.354614Z","iopub.status.idle":"2022-02-03T12:23:57.409139Z","shell.execute_reply.started":"2022-02-03T12:23:56.354579Z","shell.execute_reply":"2022-02-03T12:23:57.408243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms():\n    return A.Compose([\n            HE_HSV(always_apply=True) # ÁèæÂú®„ÅØÂÖ®„Éá„Éº„Çø„Å´ÂØæ„Åó„Å¶DA„Åã„Åë„ÅüÂâçÊèê„Åßtrain_df„ÅÆË°å„ÇíËøΩÂä†„Åó„Å¶„ÅÑ„Çã\n            ], bbox_params=A.BboxParams(format='yolo' , min_visibility=0.4,min_area=500))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:57.411024Z","iopub.execute_input":"2022-02-03T12:23:57.411587Z","iopub.status.idle":"2022-02-03T12:23:57.443381Z","shell.execute_reply.started":"2022-02-03T12:23:57.411543Z","shell.execute_reply":"2022-02-03T12:23:57.442472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"class AUGDATA„ÄÄ„ÅÆimage_pathÂë®„Çä„ÅÆË®≠ÂÆö„ÅÑ„Åò„ÇãÂøÖË¶Å„ÅÇ„Çä","metadata":{}},{"cell_type":"markdown","source":"„Åß„Åç„ÅüÁîªÂÉè„ÅØ`working/images/{video_id}-{video_frame}-aug.jpg`„Å´ÂÖ•„Çå„Çã„ÄÇ\n\n„Åù„ÅÆÁîªÂÉè„Å´ÂØæ„Åô„Çã„É©„Éô„É´„ÅØÂÖÉÁîªÂÉè„ÅÆlabel.txt„Åã„ÇâÊµÅÁî®`working/labels/{video_id}-{video_frame}-aug.txt`„Å´ÂÖ•„Çå„Çã„ÄÇ\n\ntrain_df„Å´Ë°å„ÇíËøΩÂä†„ÄÇ(ÂÖÉÁîªÂÉè„ÅÆË°å„Çí„Ç≥„Éî„Éº„ÄÇimage_path, label_path„Çí‚Üë„ÅÆ„ÇÇ„ÅÆ„Å´Â§â„Åà„Çå„Å∞OK„ÅÆ„ÅØ„Åö)\n\n---\n\nÊúÄÂàù„Åã„Çâtrain_df„Çí„Ç≥„Éî„Éº„Åô„Çã„ÄÇ=tran_aug_df\n\ntrain_aug_df„Å´ÂØæ„Åó„Å¶DA„Çí„Åã„Åë„Çã\n\nÂêå„Åòtrain„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´‰øùÂ≠ò„Åô„Çå„Å∞ok","metadata":{}},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:57.444839Z","iopub.execute_input":"2022-02-03T12:23:57.454692Z","iopub.status.idle":"2022-02-03T12:23:58.086109Z","shell.execute_reply.started":"2022-02-03T12:23:57.454648Z","shell.execute_reply":"2022-02-03T12:23:58.085333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if AUG is not None:\n    \n    train_df_he = train_df.copy(deep=True)\n    \n    # dataloader„Çí‰Ωø„Å£„Å¶„Éá„Éº„ÇøÊã°Âºµ\n    dataset = AUG_DATASET(train_df_he, transform = get_transforms())\n    dataloader = Data.DataLoader(dataset=dataset, num_workers=WORKER, batch_size=BATCH, shuffle=False, drop_last=False,\\\n                               pin_memory = False)\n    \n    for aug_img, aug_box, image_name in progress_bar(dataloader):\n        for idx, image in enumerate(aug_img):\n            name = image_name[idx]\n            new_name = \"{}_HE\".format(name)\n            image = aug_img[idx]\n            box = aug_box[idx]\n            \n            path_txt = LABEL_DIR + \"/\" + new_name + \".txt\"\n            path_jpg = IMAGE_DIR + \"/\" + new_name + \".jpg\"\n            is_path = os.path.exists(path_jpg)\n            image = image.numpy()\n            cv2.imwrite(path_jpg, image[...,::-1])\n            txt_file = open(path_txt, \"w\")\n            txt_file.write(box)\n            txt_file.close()\n        break\n    \n    # train_df„Å´DA„Åó„ÅüË°å„ÇíËøΩÂä†\n    func_he_path = lambda x: '{}_HE'.format(x)\n    train_df_he.image_path = train_df_he.image_path.map(func_he_path) # image_path„ÇíÊõ¥Êñ∞\n    train_df_he.label_path = train_df_he.label_path.map(func_he_path) # label_path„ÇíÊõ¥Êñ∞\n    train_df = pd.concat([train_df, train_df_he], axis=0, ignore_index=True) #indexÂÜçÂ∫¶Èôç„ÇäÁõ¥„Åó","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:58.09462Z","iopub.execute_input":"2022-02-03T12:23:58.102573Z","iopub.status.idle":"2022-02-03T12:23:59.570004Z","shell.execute_reply.started":"2022-02-03T12:23:58.102526Z","shell.execute_reply":"2022-02-03T12:23:59.569143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df.image_path.unique())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:59.571871Z","iopub.execute_input":"2022-02-03T12:23:59.57214Z","iopub.status.idle":"2022-02-03T12:23:59.581864Z","shell.execute_reply.started":"2022-02-03T12:23:59.572102Z","shell.execute_reply":"2022-02-03T12:23:59.580969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train.txt, val.txt„Çí‰Ωú„Çã(Â≠¶Áøí„Å®Ê§úË®º„Éá„Éº„Çø„ÅÆ„Éë„Çπ„ÅåË®òËø∞„Åï„Çå„Å¶„ÅÑ„Çã)","metadata":{}},{"cell_type":"code","source":"import yaml\n\ncwd = '/kaggle/working/'\n\nwith open(os.path.join( cwd , 'train.txt'), 'w') as f:\n    for path in train_df.image_path.tolist():\n        f.write(path+'\\n')\n            \nwith open(os.path.join(cwd , 'val.txt'), 'w') as f:\n    for path in valid_df.image_path.tolist():\n        f.write(path+'\\n')\n\ndata = dict(\n    path  = '/kaggle/working',\n    train =  os.path.join( cwd , 'train.txt') ,\n    val   =  os.path.join( cwd , 'val.txt' ),\n    nc    = 1,\n    names = ['cots'],\n    )\n\nwith open(os.path.join( cwd , 'gbr.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(os.path.join( cwd , 'gbr.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:23:59.583114Z","iopub.execute_input":"2022-02-03T12:23:59.583614Z","iopub.status.idle":"2022-02-03T12:23:59.600709Z","shell.execute_reply.started":"2022-02-03T12:23:59.583579Z","shell.execute_reply":"2022-02-03T12:23:59.599851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/hyp.yaml\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+/- deg)\ntranslate: 0.10  # image translation (+/- fraction)\nscale: 0.5  # image scale (+/- gain)\nshear: 0.0  # image shear (+/- deg)\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\nflipud: 0.5  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 0.5  # image mosaic (probability)\nmixup: 0.5 # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:23:59.602822Z","iopub.execute_input":"2022-02-03T12:23:59.603311Z","iopub.status.idle":"2022-02-03T12:23:59.610246Z","shell.execute_reply.started":"2022-02-03T12:23:59.603276Z","shell.execute_reply":"2022-02-03T12:23:59.609483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üì¶ [YOLOv5](https://github.com/ultralytics/yolov5/)","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!rm -r /kaggle/working/yolov5\n# !git clone https://github.com/ultralytics/yolov5 # clone\n!cp -r /kaggle/input/yolov5-lib-ds /kaggle/working/yolov5\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nfrom yolov5 import utils\ndisplay = utils.notebook_init()  # check","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:59.611601Z","iopub.execute_input":"2022-02-03T12:23:59.612073Z","iopub.status.idle":"2022-02-03T12:24:12.291077Z","shell.execute_reply.started":"2022-02-03T12:23:59.612038Z","shell.execute_reply":"2022-02-03T12:24:12.290195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üöÖ Training","metadata":{}},{"cell_type":"code","source":"!python train.py --img {DIM}\\\n--batch {BATCH}\\\n--epochs {EPOCHS}\\\n--data /kaggle/working/gbr.yaml\\\n--hyp /kaggle/working/hyp.yaml\\\n--weights {MODEL}.pt\\\n--optimizer {OPTIM}\\\n--project {PROJECT} --name {NAME}\\\n--exist-ok","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-03T12:24:12.292782Z","iopub.execute_input":"2022-02-03T12:24:12.293699Z","iopub.status.idle":"2022-02-03T12:27:53.010773Z","shell.execute_reply.started":"2022-02-03T12:24:12.29365Z","shell.execute_reply":"2022-02-03T12:27:53.009887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚ú® Overview","metadata":{}},{"cell_type":"markdown","source":"## Output Files","metadata":{}},{"cell_type":"code","source":"OUTPUT_DIR = '{}/{}'.format(PROJECT, NAME)\n!ls {OUTPUT_DIR}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:53.014875Z","iopub.execute_input":"2022-02-03T12:27:53.015386Z","iopub.status.idle":"2022-02-03T12:27:53.778407Z","shell.execute_reply.started":"2022-02-03T12:27:53.015347Z","shell.execute_reply":"2022-02-03T12:27:53.777615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìà Class Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/labels_correlogram.jpg'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:53.781368Z","iopub.execute_input":"2022-02-03T12:27:53.781593Z","iopub.status.idle":"2022-02-03T12:27:54.602482Z","shell.execute_reply.started":"2022-02-03T12:27:53.781564Z","shell.execute_reply":"2022-02-03T12:27:54.601553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/labels.jpg'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:54.604338Z","iopub.execute_input":"2022-02-03T12:27:54.605045Z","iopub.status.idle":"2022-02-03T12:27:55.31483Z","shell.execute_reply.started":"2022-02-03T12:27:54.605003Z","shell.execute_reply":"2022-02-03T12:27:55.314145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî≠ Batch Image","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/train_batch0.jpg'))\n\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/train_batch1.jpg'))\n\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/train_batch2.jpg'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:55.316108Z","iopub.execute_input":"2022-02-03T12:27:55.316497Z","iopub.status.idle":"2022-02-03T12:27:57.813941Z","shell.execute_reply.started":"2022-02-03T12:27:55.316461Z","shell.execute_reply":"2022-02-03T12:27:57.813289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GT Vs Pred","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 2, figsize = (2*9,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'{OUTPUT_DIR}/val_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'{OUTPUT_DIR}/val_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'{OUTPUT_DIR}/val_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'{OUTPUT_DIR}/val_batch{row}_pred.jpg', fontsize = 12)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:57.815155Z","iopub.execute_input":"2022-02-03T12:27:57.815699Z","iopub.status.idle":"2022-02-03T12:27:58.923098Z","shell.execute_reply.started":"2022-02-03T12:27:57.81566Z","shell.execute_reply":"2022-02-03T12:27:58.921183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üîç Result","metadata":{}},{"cell_type":"markdown","source":"## Score Vs Epoch","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/results.png'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:58.925154Z","iopub.status.idle":"2022-02-03T12:27:58.925502Z","shell.execute_reply.started":"2022-02-03T12:27:58.92533Z","shell.execute_reply":"2022-02-03T12:27:58.925352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/confusion_matrix.png'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:58.929393Z","iopub.status.idle":"2022-02-03T12:27:58.929686Z","shell.execute_reply.started":"2022-02-03T12:27:58.929532Z","shell.execute_reply":"2022-02-03T12:27:58.929553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"for metric in ['F1', 'PR', 'P', 'R']:\n    print(f'Metric: {metric}')\n    plt.figure(figsize=(12,10))\n    plt.axis('off')\n    plt.imshow(plt.imread(f'{OUTPUT_DIR}/{metric}_curve.png'));\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:58.931083Z","iopub.status.idle":"2022-02-03T12:27:58.93149Z","shell.execute_reply.started":"2022-02-03T12:27:58.931267Z","shell.execute_reply":"2022-02-03T12:27:58.931289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please Upvote if you find this Helpful","metadata":{}},{"cell_type":"markdown","source":"# ‚úÇÔ∏è Remove Files","metadata":{}},{"cell_type":"code","source":"!rm -r {IMAGE_DIR}\n!rm -r {LABEL_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:27:58.93316Z","iopub.status.idle":"2022-02-03T12:27:58.933588Z","shell.execute_reply.started":"2022-02-03T12:27:58.933364Z","shell.execute_reply":"2022-02-03T12:27:58.933388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://www.pngall.com/wp-content/uploads/2018/04/Under-Construction-PNG-File.png\">","metadata":{}}]}