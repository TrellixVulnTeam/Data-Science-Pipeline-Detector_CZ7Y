{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-20T06:11:11.471408Z","iopub.status.idle":"2022-02-20T06:11:11.471718Z","shell.execute_reply.started":"2022-02-20T06:11:11.471552Z","shell.execute_reply":"2022-02-20T06:11:11.471572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"referance notebook -https://www.kaggle.com/soumya9977/learning-to-sea-underwater-img-enhancement-eda","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport re\nimport os\nimport random\n! pip install tensorflow_addons\nimport tensorflow_addons as tfa\nfrom matplotlib import animation, rc\nimport tensorflow as tf\nimport seaborn as sns\nimport random\nimport shutil\nimport ast\n\nfrom IPython.display import HTML\nfrom base64 import b64encode\nimport subprocess\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndf=pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\ntest=pd.read_csv(\"../input/tensorflow-great-barrier-reef/test.csv\")\nsample=pd.read_csv(\"../input/tensorflow-great-barrier-reef/example_sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:42:55.71903Z","iopub.execute_input":"2022-02-24T12:42:55.720087Z","iopub.status.idle":"2022-02-24T12:43:13.262545Z","shell.execute_reply.started":"2022-02-24T12:42:55.720042Z","shell.execute_reply":"2022-02-24T12:43:13.261592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df.head(5) , test.head(5) , sample)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:13.264682Z","iopub.execute_input":"2022-02-24T12:43:13.265015Z","iopub.status.idle":"2022-02-24T12:43:13.302932Z","shell.execute_reply.started":"2022-02-24T12:43:13.264973Z","shell.execute_reply":"2022-02-24T12:43:13.302013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:13.30421Z","iopub.execute_input":"2022-02-24T12:43:13.304647Z","iopub.status.idle":"2022-02-24T12:43:13.31049Z","shell.execute_reply.started":"2022-02-24T12:43:13.304617Z","shell.execute_reply":"2022-02-24T12:43:13.309819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_img(img_arr):\n  '''function take input as array and plot image'''\n  plt.figure(figsize = (15 , 5))\n  plt.imshow(img_arr)\n  plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:13.312363Z","iopub.execute_input":"2022-02-24T12:43:13.314561Z","iopub.status.idle":"2022-02-24T12:43:13.319418Z","shell.execute_reply.started":"2022-02-24T12:43:13.314523Z","shell.execute_reply":"2022-02-24T12:43:13.318742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=df['video_id'].count()\nprint(f\"we have total  {count} no of images in dataframe\")","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:13.320298Z","iopub.execute_input":"2022-02-24T12:43:13.320849Z","iopub.status.idle":"2022-02-24T12:43:13.336536Z","shell.execute_reply.started":"2022-02-24T12:43:13.320819Z","shell.execute_reply":"2022-02-24T12:43:13.33575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Dataframe have  3 different videos images.\n> \n> those video images can seperated by varable video_id","metadata":{}},{"cell_type":"code","source":"for i , j in enumerate(df['video_id'].value_counts().sort_values()):\n    print(f\"video {i} have {j} no. of images\")\n    \n########################################################################\n\nplt.figure(figsize=(8,5))\nsns.countplot(df['video_id'], color='#2196F3')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:13.339925Z","iopub.execute_input":"2022-02-24T12:43:13.340404Z","iopub.status.idle":"2022-02-24T12:43:13.578895Z","shell.execute_reply.started":"2022-02-24T12:43:13.340369Z","shell.execute_reply":"2022-02-24T12:43:13.578012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annot=[]\nfor i  in df[\"annotations\"]:\n    if i==\"[]\": annot.append(1)\n    else:annot.append(0)\n\nannot_True = annot.count(1)\nannot_False = annot.count(0)\n\nprint(f\"we have total {annot_True} no. of images which contain COTS in it\")\nprint(f\"we have total {annot_False} no. of images which dont have  COTS in it\")\n\ndf[\"bounding_box_or_not\"] = annot\n\nsns.countplot(x ='bounding_box_or_not', data = df)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:13.580189Z","iopub.execute_input":"2022-02-24T12:43:13.580904Z","iopub.status.idle":"2022-02-24T12:43:13.770071Z","shell.execute_reply.started":"2022-02-24T12:43:13.580865Z","shell.execute_reply":"2022-02-24T12:43:13.769407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting string annotations into list\ndf['annotations'] =df['annotations'].apply(eval)\n########################################################################\n\n#counting number of bounding boxes in each img  and adding it to new variable no_of_boundingBox\n\nno_of_BoundingBox=[]\nfor i in tqdm(df[\"annotations\"]):\n    no_of_BoundingBox.append(len(i))\ndf[\"no_of_BoundingBox\"]=no_of_BoundingBox\n#######################################################################\n\n#plot histogram\nsns.countplot(df[\"no_of_BoundingBox\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:13.771022Z","iopub.execute_input":"2022-02-24T12:43:13.771363Z","iopub.status.idle":"2022-02-24T12:43:14.283265Z","shell.execute_reply.started":"2022-02-24T12:43:13.771323Z","shell.execute_reply":"2022-02-24T12:43:14.282439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df[\"no_of_BoundingBox\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:14.284495Z","iopub.execute_input":"2022-02-24T12:43:14.28472Z","iopub.status.idle":"2022-02-24T12:43:14.562259Z","shell.execute_reply.started":"2022-02-24T12:43:14.284692Z","shell.execute_reply":"2022-02-24T12:43:14.561723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> *18000 images dont have bounding box in it*\n> \n> *there are over 2500 images which have single bounding box in image*\n> \n> *very few images have 2+ bounding boxes in image*","metadata":{}},{"cell_type":"code","source":"# removing images which dont have COTS/bounding_boxes \ndf=df[df[\"no_of_BoundingBox\"]>=1]\ndf.reset_index(drop=True, inplace=True)\ndf.head(5)\n\n#plotting no_of_BoundingBox\nfig, ax = plt.subplots()\nsns.countplot(data=df, x=\"no_of_BoundingBox\" , ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:14.564367Z","iopub.execute_input":"2022-02-24T12:43:14.564676Z","iopub.status.idle":"2022-02-24T12:43:14.798748Z","shell.execute_reply.started":"2022-02-24T12:43:14.56465Z","shell.execute_reply":"2022-02-24T12:43:14.798202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing images which dont have COTS/bounding_boxes \ndf=df[df[\"no_of_BoundingBox\"]>=1]\ndf.reset_index(drop=True, inplace=True)\ndf.head(5)\n\n#plotting no_of_BoundingBox\nfig, ax = plt.subplots()\nsns.countplot(data=df, x=\"no_of_BoundingBox\" , ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:14.799711Z","iopub.execute_input":"2022-02-24T12:43:14.800009Z","iopub.status.idle":"2022-02-24T12:43:15.033704Z","shell.execute_reply.started":"2022-02-24T12:43:14.799983Z","shell.execute_reply":"2022-02-24T12:43:15.03306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> 2500+ images have 2 bounding box in image\n> \n> 900+ images have 3 bounding box in image","metadata":{}},{"cell_type":"code","source":"for i in range(90 , 100 , 1):\n    value = round(np.percentile(df[\"no_of_BoundingBox\"], i) , 0)\n    print(f\" {i}th percentile of data  contain {value} no.of COTS in image\")","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:15.034717Z","iopub.execute_input":"2022-02-24T12:43:15.035024Z","iopub.status.idle":"2022-02-24T12:43:15.045956Z","shell.execute_reply.started":"2022-02-24T12:43:15.034997Z","shell.execute_reply":"2022-02-24T12:43:15.04501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> we have 50% of images which have 1 bounding box in image \n> \n> 20% images have 2 bounding boxes in image\n> \n> we will go further now to check how many bounding boxes in  90 to 100 percentile\n","metadata":{}},{"cell_type":"code","source":"for i in range(90 , 100 , 1):\n    k=round(np.percentile(df[\"no_of_BoundingBox\"], i) , 0)\n    print(f\"{i} percentile values have {k} no. of bounding boxes in image\")","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:15.047281Z","iopub.execute_input":"2022-02-24T12:43:15.047664Z","iopub.status.idle":"2022-02-24T12:43:15.057872Z","shell.execute_reply.started":"2022-02-24T12:43:15.047635Z","shell.execute_reply":"2022-02-24T12:43:15.057205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> 1% of image have 16 bounding box in image\n> \n> 1% of image have 14 bounding box in image\n> \n> 1% of image have 11 bounding box in image\n","metadata":{}},{"cell_type":"code","source":"# adding image path to data frame\ndir=\"../input/tensorflow-great-barrier-reef/train_images\"\ndf['image_path'] =dir + \"/video_\" + df['video_id'].astype(str) + \"/\" + df['video_frame'].astype(str) + \".jpg\"","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:15.059155Z","iopub.execute_input":"2022-02-24T12:43:15.059777Z","iopub.status.idle":"2022-02-24T12:43:15.081087Z","shell.execute_reply.started":"2022-02-24T12:43:15.059737Z","shell.execute_reply":"2022-02-24T12:43:15.080144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['image_path'][0])\nprint(df['image_path'][1])\nprint(df['image_path'][2])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:15.083194Z","iopub.execute_input":"2022-02-24T12:43:15.084299Z","iopub.status.idle":"2022-02-24T12:43:15.095037Z","shell.execute_reply.started":"2022-02-24T12:43:15.084259Z","shell.execute_reply":"2022-02-24T12:43:15.09441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding two new variable to data frame image height and img_width\nimg = mpimg.imread(df[\"image_path\"][0]) \n\ndf[\"img_width\"] = img.shape[1]\ndf[\"img_height\"] = img.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:15.09651Z","iopub.execute_input":"2022-02-24T12:43:15.097055Z","iopub.status.idle":"2022-02-24T12:43:15.171264Z","shell.execute_reply.started":"2022-02-24T12:43:15.09701Z","shell.execute_reply":"2022-02-24T12:43:15.170553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = mpimg.imread(df[\"image_path\"][19]) \nimg=cv2.rectangle(img, (520 , 151), (598 , 213), (0,0,0), 2)\nimg=cv2.rectangle(img, (598 , 204), (656 , 236), (0,0,0), 2)\nplot_img(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:15.172897Z","iopub.execute_input":"2022-02-24T12:43:15.173513Z","iopub.status.idle":"2022-02-24T12:43:15.632242Z","shell.execute_reply.started":"2022-02-24T12:43:15.17347Z","shell.execute_reply":"2022-02-24T12:43:15.631436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"annotations\"][19]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:15.633424Z","iopub.execute_input":"2022-02-24T12:43:15.634158Z","iopub.status.idle":"2022-02-24T12:43:15.640122Z","shell.execute_reply.started":"2022-02-24T12:43:15.634077Z","shell.execute_reply":"2022-02-24T12:43:15.639562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id=35\nbb=df[\"annotations\"][id][0]\nbb2=df[\"annotations\"][id][1]\n########################################################################\n\n#calculating x_min , y_min , x_max , y_max for bounding box 1 \n\nx_min=bb.get(\"x\")\ny_min=bb.get(\"y\")\nx_max=bb.get(\"x\")+bb.get(\"width\")\ny_max=bb.get(\"y\")+bb.get(\"height\")\n\n########################################################################\n\n#calculating x_min , y_min , x_max , y_max for bounding box 2\n\nx_min2=bb2.get(\"x\")\ny_min2=bb2.get(\"y\")\nx_max2=bb2.get(\"x\")+bb2.get(\"width\")\ny_max2=bb2.get(\"y\")+bb2.get(\"height\")\nprint(x_min , y_min , x_max , y_max)\nprint(x_min2 , y_min2 , x_max2 , y_max2)\n########################################################################\n\nimg = mpimg.imread(df[\"image_path\"][id]) \n\n# drawwing rectangle bounding box on image for bounding box 1 & 2\n\nimg=cv2.rectangle(img, (x_min , y_min), (x_max , y_max), (255,0,0), 2)\nimg=cv2.rectangle(img,  (x_min2 , y_min2), (x_max2 , y_max2), (255,0,0), 2)\nplot_img(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:15.641197Z","iopub.execute_input":"2022-02-24T12:43:15.641787Z","iopub.status.idle":"2022-02-24T12:43:16.04622Z","shell.execute_reply.started":"2022-02-24T12:43:15.641758Z","shell.execute_reply":"2022-02-24T12:43:16.045481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting dict annotation to [ x , y , h , w ]\nbounding_box=[]\nfor annot in tqdm(df[\"annotations\"]):\n    bb=[]\n    for i in annot:\n        bb_cordinate=[]\n        bb_cordinate.append(i.get(\"x\"))\n        bb_cordinate.append(i.get(\"y\"))\n        bb_cordinate.append(i.get(\"x\")+i.get(\"width\"))\n        bb_cordinate.append(i.get(\"y\")+i.get(\"height\"))\n        bb.append(bb_cordinate)\n    bounding_box.append(bb)\ndf[\"Bounding_Box\"]=bounding_box","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:16.047366Z","iopub.execute_input":"2022-02-24T12:43:16.04758Z","iopub.status.idle":"2022-02-24T12:43:16.264596Z","shell.execute_reply.started":"2022-02-24T12:43:16.047555Z","shell.execute_reply":"2022-02-24T12:43:16.263629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Bounding_Box\"][30:40]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:16.266129Z","iopub.execute_input":"2022-02-24T12:43:16.266527Z","iopub.status.idle":"2022-02-24T12:43:16.278701Z","shell.execute_reply.started":"2022-02-24T12:43:16.266482Z","shell.execute_reply":"2022-02-24T12:43:16.277635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:16.280633Z","iopub.execute_input":"2022-02-24T12:43:16.281005Z","iopub.status.idle":"2022-02-24T12:43:16.304112Z","shell.execute_reply.started":"2022-02-24T12:43:16.28096Z","shell.execute_reply":"2022-02-24T12:43:16.303167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_bounding_box(id , data , img_arr=False ):\n    '''given function draw bounding box in image , \n    it take input as image id and it output image with bounding box  '''\n    if img_arr is not False:\n        img=img_arr\n    else: img=mpimg.imread(data[\"image_path\"][id]) \n    bb=data[\"Bounding_Box\"][id]\n    for i in bb:\n        \n        x_min=i[0]\n        y_min=i[1]\n        x_max=i[2]\n        y_max=i[3]\n        img=cv2.rectangle(img, (x_min , y_min), (x_max , y_max), (255,0,0), 2)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:16.305331Z","iopub.execute_input":"2022-02-24T12:43:16.305765Z","iopub.status.idle":"2022-02-24T12:43:16.315682Z","shell.execute_reply.started":"2022-02-24T12:43:16.305649Z","shell.execute_reply":"2022-02-24T12:43:16.314771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id=35 \nimg=draw_bounding_box(id , df)\nplot_img(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:16.316838Z","iopub.execute_input":"2022-02-24T12:43:16.317728Z","iopub.status.idle":"2022-02-24T12:43:16.707199Z","shell.execute_reply.started":"2022-02-24T12:43:16.317692Z","shell.execute_reply":"2022-02-24T12:43:16.7064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> visualizing image after drawing bounding box on image with the help of function draw_bounding_box","metadata":{}},{"cell_type":"code","source":"# creating dataframe which just have 13 bounding boxes in each image\ndf_13=df[df[\"no_of_BoundingBox\"]>=13]    \ndf_13.sort_values(\"no_of_BoundingBox\").head(5)\ndf_13.reset_index(drop=True, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:16.708556Z","iopub.execute_input":"2022-02-24T12:43:16.709537Z","iopub.status.idle":"2022-02-24T12:43:16.719915Z","shell.execute_reply.started":"2022-02-24T12:43:16.70949Z","shell.execute_reply":"2022-02-24T12:43:16.719055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**why we are creating seperate data frame for 13 boxes?**\n\n> because what we notice is that as we go deep into sea , we cant visualize COTS properly because of blue color saturation is more . as we go deep we see more COTS in depth\n> \n> \n> we need to clear image such a way  we can clearly visualize COTS in images / we need to remove water from image so we can clearly visualize COTS in image","metadata":{}},{"cell_type":"code","source":"id=30\nimg=draw_bounding_box( id , df_13)\nplot_img(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:16.72106Z","iopub.execute_input":"2022-02-24T12:43:16.721302Z","iopub.status.idle":"2022-02-24T12:43:17.159947Z","shell.execute_reply.started":"2022-02-24T12:43:16.721273Z","shell.execute_reply":"2022-02-24T12:43:17.158868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> above image have 13 COTS in it , but we cant visualize most of them by eye so we need to do feature eng","metadata":{}},{"cell_type":"code","source":"def make_video(data ,start , end ,  video_id ,fun_name ,  fun=False):\n    # partly borrowed from https://github.com/RobMulla/helmet-assignment/blob/main/helmet_assignment/video.py\n    fps = 15 # don't know exact value\n    width = 1280\n    height = 720\n    df=data[data[\"video_id\"]==video_id][start : end]\n    save_path = f'video{video_id}-{start}-{end}-{fun_name}.mp4'\n    tmp_path = \"tmp_\" + save_path\n    output_video = cv2.VideoWriter(tmp_path, cv2.VideoWriter_fourcc(*\"MP4V\"), fps, (width, height))\n    \n    video_df = df.query('video_id == @video_id')\n    for i  in tqdm(df.index):\n      img=draw_bounding_box(i , df)\n      img=cv2.cvtColor(img , cv2.COLOR_BGR2RGB)\n      if fun!=False:\n        img = fun(img)\n      output_video.write(img)\n    \n    output_video.release()\n    # Not all browsers support the codec, we will re-load the file at tmp_output_path\n    # and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(save_path):\n        os.remove(save_path)\n    subprocess.run(\n        [\"ffmpeg\", \"-i\", tmp_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-vcodec\", \"libx264\", save_path]\n    )\n    os.remove(tmp_path)\n\ndef plot_img(img_arr):\n  '''function take input as array and plot image'''\n  plt.figure(figsize = (15 , 5))\n  plt.imshow(img_arr)\n  plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:17.161245Z","iopub.execute_input":"2022-02-24T12:43:17.161526Z","iopub.status.idle":"2022-02-24T12:43:17.170778Z","shell.execute_reply.started":"2022-02-24T12:43:17.161495Z","shell.execute_reply":"2022-02-24T12:43:17.169754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fun_name = \"none\"\nstart=15\nend=100\nvideo_id=0\nmake_video(df ,start , end , video_id ,fun_name)\n\n\nmp4 = open(f'./video{video_id}-{start}-{end}-{fun_name}.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=1000 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:44:55.082717Z","iopub.execute_input":"2022-02-24T12:44:55.083141Z","iopub.status.idle":"2022-02-24T12:45:04.100406Z","shell.execute_reply.started":"2022-02-24T12:44:55.083098Z","shell.execute_reply":"2022-02-24T12:45:04.099491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating dataframe which just have 13 bounding boxes in each image\ndf_13=df[df[\"no_of_BoundingBox\"]>=13]    \ndf_13.sort_values(\"no_of_BoundingBox\").head(5)\ndf_13.reset_index(drop=True, inplace=True)\n\n#creating video \nfun_name = \"none\"\nstart=0\nend=100\nvideo_id= 1\nmake_video(df_13 ,start , end , video_id ,fun_name)\n\n\nmp4 = open(f'./video{video_id}-{start}-{end}-{fun_name}.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=1000 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:45:04.102131Z","iopub.execute_input":"2022-02-24T12:45:04.103008Z","iopub.status.idle":"2022-02-24T12:45:11.0271Z","shell.execute_reply.started":"2022-02-24T12:45:04.102964Z","shell.execute_reply":"2022-02-24T12:45:11.026346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# below both fucntion are use to convert set of images into animation \nrc('animation', html='jshtml')\n\ndef create_animation(ims):\n    fig = plt.figure(figsize=(9, 9))\n    plt.axis('off')\n    im = plt.imshow(ims[0])\n\n    def animate_func(i):\n        im.set_array(ims[i])\n        return [im]\n\n    return animation.FuncAnimation(fig, animate_func, frames = len(ims), interval = 1000//12)\n\n###############################################################################################\n\ndef animate_images(start , no_of_img , data  ,function):\n  images=[]\n  for i in tqdm(range(start , start+no_of_img)):\n    img=mpimg.imread(data[\"image_path\"][i])\n    img1=function(img)\n\n    bb= data[\"Bounding_Box\"][i]\n    for i in bb:\n      x_min=i[0]\n      y_min=i[1]\n      x_max=i[2]\n      y_max=i[3]\n      img1=cv2.rectangle(img1, (x_min , y_min), (x_max , y_max), (255,0,0), 2)\n    images.append(img1)\n  return create_animation(images)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:45:19.122226Z","iopub.execute_input":"2022-02-24T12:45:19.122572Z","iopub.status.idle":"2022-02-24T12:45:19.132391Z","shell.execute_reply.started":"2022-02-24T12:45:19.122536Z","shell.execute_reply":"2022-02-24T12:45:19.131247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images=[]\nfor i in tqdm(range(25 , 60)):\n    img=mpimg.imread(df[\"image_path\"][i])\n\n    bb=df[\"Bounding_Box\"][i]\n    for i in bb:\n      x_min=i[0]\n      y_min=i[1]\n      x_max=i[2]\n      y_max=i[3]\n      img=cv2.rectangle(img, (x_min , y_min), (x_max , y_max), (255,0,0), 2)\n    images.append(img)\n\ncreate_animation(images)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:45:20.529132Z","iopub.execute_input":"2022-02-24T12:45:20.530098Z","iopub.status.idle":"2022-02-24T12:45:29.19999Z","shell.execute_reply.started":"2022-02-24T12:45:20.530041Z","shell.execute_reply":"2022-02-24T12:45:29.199342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> n above animation we can see COTS properly because camera is close to COTS and video captured not in too depth","metadata":{}},{"cell_type":"code","source":"images=[]\nfor i in tqdm(range(25 , 60)):\n    img=mpimg.imread(df_13[\"image_path\"][i])\n\n    bb=df_13[\"Bounding_Box\"][i]\n    for i in bb:\n      x_min=i[0]\n      y_min=i[1]\n      x_max=i[2]\n      y_max=i[3]\n      img=cv2.rectangle(img, (x_min , y_min), (x_max , y_max), (255,0,0), 2)\n    images.append(img)\n\ncreate_animation(images)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:45:29.201449Z","iopub.execute_input":"2022-02-24T12:45:29.201847Z","iopub.status.idle":"2022-02-24T12:45:37.928739Z","shell.execute_reply.started":"2022-02-24T12:45:29.201801Z","shell.execute_reply":"2022-02-24T12:45:37.927749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> as we go in depth of sea we found more number of COTS in image but we cant properly visualize image","metadata":{}},{"cell_type":"markdown","source":"Learning to Sea: Underwater img Enhancement + EDA\nhttps://www.kaggle.com/soumya9977/learning-to-sea-underwater-img-enhancement-eda","metadata":{}},{"cell_type":"markdown","source":"# HSV - Hue Saturation Value\n* HSV color space: It stores color information in a cylindrical representation of RGB color points.\n* It attempts to depict the colors as perceived by the human eye.\n* Hue value varies from 0-179, Saturation value varies from 0-255 and Value value varies from 0-255.","metadata":{}},{"cell_type":"markdown","source":"# ***histogram equalization***\n>  the pixel values are are between 0-255, but we will not find any pixel values which are exactly 0 or 255, \n> it does not have any image which is pure white or pure black.\n> If we apply the histogram equalization then it will reduce the color depth.Currently the minimum pixel value is 52 and the highest is 255. After you apply histogram equalization, you will find the the min pixel value now got transformed to zero and the max got converted to 255","metadata":{}},{"cell_type":"code","source":"id=45\nimg = mpimg.imread(df_13[\"image_path\"][id])\nprint(\"Original image\")\nplot_img(img)\n######################################################\nprint(\"transformed image\")\nimg = mpimg.imread(df_13[\"image_path\"][id])\n#converting RGB image to HSV image \nimg_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n#value channnel \nvalue=img_hsv[: ,: ,2]\n#applying histogram equalization on value channnel\nimg_hsv[:, :, 2] = cv2.equalizeHist(value)\n#converting HSV to RGB\nimage_RGB = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2RGB)\nplot_img(image_RGB)\n#######################################################\nprint(\"transformed image+bounding_Box\")\nimg=draw_bounding_box(id  ,df_13 , image_RGB)\nplot_img(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:45:52.397346Z","iopub.execute_input":"2022-02-24T12:45:52.39765Z","iopub.status.idle":"2022-02-24T12:45:53.545978Z","shell.execute_reply.started":"2022-02-24T12:45:52.397615Z","shell.execute_reply":"2022-02-24T12:45:53.545195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hist_equalization(img):\n    #converting RGB image to HSV image \n    img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    #value channnel\n    value=img_hsv[: ,: ,2]\n    #applying histogram equalization on value channnel\n    img_hsv[:, :, 2] = cv2.equalizeHist(value)\n    #converting HSV to RGB\n    image_RGB = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2RGB)\n    return image_RGB\n\n####################################################################\n\ndef plot_original_transformed_img(sample ,function  ):\n    for k in range(0 , sample):\n       #random smaple\n       i=random.choice(list(df_13.index))\n       path=df_13[\"image_path\"][i]\n       plt.figure(figsize = (10 , 3))\n       #reading original imaage \n       img = mpimg.imread(path)\n       plt.subplot(1,2,1)\n       plt.imshow(img)\n       #transforming image\n       img=function(img)\n       plt.subplot(1,2,2)\n       plt.imshow(img)\n       plt.show()\n    return","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:45:53.547436Z","iopub.execute_input":"2022-02-24T12:45:53.548262Z","iopub.status.idle":"2022-02-24T12:45:53.559079Z","shell.execute_reply.started":"2022-02-24T12:45:53.548216Z","shell.execute_reply":"2022-02-24T12:45:53.558114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_original_transformed_img(3 , hist_equalization )","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:45:54.763076Z","iopub.execute_input":"2022-02-24T12:45:54.763702Z","iopub.status.idle":"2022-02-24T12:45:56.191454Z","shell.execute_reply.started":"2022-02-24T12:45:54.763664Z","shell.execute_reply":"2022-02-24T12:45:56.190582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating video \nfun_name = hist_equalization\nstart=0\nend=100\nvideo_id= 1\nmake_video(df_13 ,start , end , video_id ,fun_name ,fun_name)\n\n\nmp4 = open(f'./video{video_id}-{start}-{end}-{fun_name}.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=1000 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:45:56.192879Z","iopub.execute_input":"2022-02-24T12:45:56.193113Z","iopub.status.idle":"2022-02-24T12:46:02.14456Z","shell.execute_reply.started":"2022-02-24T12:45:56.193085Z","shell.execute_reply":"2022-02-24T12:46:02.143215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> histogram equalization not giving good results ,  we need to try another technique to clearify images ","metadata":{}},{"cell_type":"markdown","source":"# **Histogram Equalizers on each channel**","metadata":{}},{"cell_type":"code","source":"def RecoverHE(img):\n  #converting image to numpy array\n    img= np.array(img)\n    #appying histogram equaalization on each channel\n    for i in range(3):\n        img[:, :, i] =  cv2.equalizeHist(img[:, :, i])\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:46:04.787134Z","iopub.execute_input":"2022-02-24T12:46:04.787455Z","iopub.status.idle":"2022-02-24T12:46:04.793443Z","shell.execute_reply.started":"2022-02-24T12:46:04.787423Z","shell.execute_reply":"2022-02-24T12:46:04.792408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_original_transformed_img(3 , RecoverHE)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:46:05.010186Z","iopub.execute_input":"2022-02-24T12:46:05.01049Z","iopub.status.idle":"2022-02-24T12:46:06.583499Z","shell.execute_reply.started":"2022-02-24T12:46:05.01046Z","shell.execute_reply":"2022-02-24T12:46:06.582785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# animate_images(30 ,60 , df_13 ,RecoverHE )","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:17.44203Z","iopub.status.idle":"2022-02-24T12:43:17.442869Z","shell.execute_reply.started":"2022-02-24T12:43:17.442671Z","shell.execute_reply":"2022-02-24T12:43:17.4427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating video \nfun_name = RecoverHE\nstart=0\nend=500\nvideo_id=1\nmake_video(df_13 ,start , end , video_id ,fun_name ,fun_name)\n\n\nmp4 = open(f'./video{video_id}-{start}-{end}-{fun_name}.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=1000 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:46:07.29908Z","iopub.execute_input":"2022-02-24T12:46:07.299998Z","iopub.status.idle":"2022-02-24T12:46:13.642696Z","shell.execute_reply.started":"2022-02-24T12:46:07.299939Z","shell.execute_reply":"2022-02-24T12:46:13.637368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> after applying Histogram Equalizers on each channel we  see  good results , we can use this transformation \n\n> lets try some more transformation technique","metadata":{}},{"cell_type":"markdown","source":"# **CLAHE Histogram Equalization**\n> CLAHE operates on small regions in the image, called tiles, rather than the entire image. The neighboring tiles are then combined using bilinear interpolation to remove the artificial boundaries. \n> This algorithm can be applied to improve the contrast of images.","metadata":{}},{"cell_type":"code","source":"img = mpimg.imread(df_13[\"image_path\"][0])\nplt.figure(figsize = (15 , 5))\nplt.imshow(img)\nplt.show()\n#converting RGB to HSV\nhsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n#extracting H S V values\nh, s, v = hsv_img[:,:,0], hsv_img[:,:,1], hsv_img[:,:,2]\n#clipLimit=Threshold for contrast limiting. , \n#tileGridSize =Size of grid for histogram equalization. Input image will be divided into equally sized rectangular tiles. \nclahe = cv2.createCLAHE(clipLimit = 20, tileGridSize = (30 , 30))\n#applying clache on value\nv = clahe.apply(v)\n#stacking HSV\nhsv_img = np.dstack((h,s,v))\n#converting HSV image back to RGB\nrgb_img = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)\n#plot image\nplt.figure(figsize = (15 , 5))\nplt.imshow(rgb_img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:46:14.853804Z","iopub.execute_input":"2022-02-24T12:46:14.854095Z","iopub.status.idle":"2022-02-24T12:46:15.625421Z","shell.execute_reply.started":"2022-02-24T12:46:14.854066Z","shell.execute_reply":"2022-02-24T12:46:15.624475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clahe_hsv(img):\n    #converting RGB to HSV\n    hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    #extracting H S V values\n    h, s, v = hsv_img[:,:,0], hsv_img[:,:,1], hsv_img[:,:,2]\n    #clipLimit=Threshold for contrast limiting. , \n    #tileGridSize =Size of grid for histogram equalization. Input image will be divided into equally sized rectangular tiles.\n    clahe = cv2.createCLAHE(clipLimit = 15, tileGridSize = (10 , 10))\n    #applying clache on value\n    v = clahe.apply(v)\n    #stacking HSV\n    hsv_img = np.dstack((h,s,v))\n    #converting HSV image back to RGB\n    rgb_img = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)\n    return rgb_img","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:46:27.686904Z","iopub.execute_input":"2022-02-24T12:46:27.687204Z","iopub.status.idle":"2022-02-24T12:46:27.694572Z","shell.execute_reply.started":"2022-02-24T12:46:27.687169Z","shell.execute_reply":"2022-02-24T12:46:27.693898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_original_transformed_img(3 ,clahe_hsv )","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:46:42.839455Z","iopub.execute_input":"2022-02-24T12:46:42.839825Z","iopub.status.idle":"2022-02-24T12:46:44.669243Z","shell.execute_reply.started":"2022-02-24T12:46:42.839789Z","shell.execute_reply":"2022-02-24T12:46:44.668585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating video \nfun_name = clahe_hsv\nstart=0\nend=500\nvideo_id=2\nmake_video(df_13 ,start , end , video_id ,fun_name ,fun_name)\n\n\nmp4 = open(f'./video{video_id}-{start}-{end}-{fun_name}.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=1000 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:46:49.918434Z","iopub.execute_input":"2022-02-24T12:46:49.920687Z","iopub.status.idle":"2022-02-24T12:46:53.909495Z","shell.execute_reply.started":"2022-02-24T12:46:49.920646Z","shell.execute_reply":"2022-02-24T12:46:53.907819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# animate_images(30 ,60 , df_13 ,clahe_hsv)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:43:17.454081Z","iopub.status.idle":"2022-02-24T12:43:17.454427Z","shell.execute_reply.started":"2022-02-24T12:43:17.454227Z","shell.execute_reply":"2022-02-24T12:43:17.454248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> CLAHE Histogram Equalization is not giving me good result so i will skip this method","metadata":{}},{"cell_type":"markdown","source":"# RecoverCLAHE","metadata":{}},{"cell_type":"code","source":"def RecoverCLAHE(img):\n    clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(15 , 15))\n    img= np.array(img)\n    for i in range(3):\n\n        img[:, :, i] = clahe.apply(img[:, :, i])\n\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:47:03.129472Z","iopub.execute_input":"2022-02-24T12:47:03.130145Z","iopub.status.idle":"2022-02-24T12:47:03.135524Z","shell.execute_reply.started":"2022-02-24T12:47:03.130105Z","shell.execute_reply":"2022-02-24T12:47:03.134574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_original_transformed_img(2 ,RecoverCLAHE )","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:47:03.556994Z","iopub.execute_input":"2022-02-24T12:47:03.559355Z","iopub.status.idle":"2022-02-24T12:47:04.605478Z","shell.execute_reply.started":"2022-02-24T12:47:03.559286Z","shell.execute_reply":"2022-02-24T12:47:04.604774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# animate_images(30 ,50 , df_13 ,RecoverCLAHE)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:47:04.606625Z","iopub.execute_input":"2022-02-24T12:47:04.607202Z","iopub.status.idle":"2022-02-24T12:47:04.610265Z","shell.execute_reply.started":"2022-02-24T12:47:04.607162Z","shell.execute_reply":"2022-02-24T12:47:04.609698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating video \nfun_name = RecoverCLAHE\nstart=0\nend=500\nvideo_id=2\nmake_video(df_13 ,start , end , video_id ,fun_name ,fun_name)\n\n\nmp4 = open(f'./video{video_id}-{start}-{end}-{fun_name}.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=1000 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:47:04.611252Z","iopub.execute_input":"2022-02-24T12:47:04.611733Z","iopub.status.idle":"2022-02-24T12:47:09.040818Z","shell.execute_reply.started":"2022-02-24T12:47:04.61169Z","shell.execute_reply":"2022-02-24T12:47:09.039429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we cant use RecoverCLAHE  because its gving worst result","metadata":{}},{"cell_type":"markdown","source":"# tfa.image.equalize","metadata":{}},{"cell_type":"code","source":"def tfimageEqualizer(img):\n    img=tfa.image.equalize(img)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:47:11.45348Z","iopub.execute_input":"2022-02-24T12:47:11.453956Z","iopub.status.idle":"2022-02-24T12:47:11.458658Z","shell.execute_reply.started":"2022-02-24T12:47:11.453914Z","shell.execute_reply":"2022-02-24T12:47:11.457803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images=[]\nfor i in tqdm(range(10  ,30+80)):\n    img=mpimg.imread(df_13[\"image_path\"][i])\n    img=tfa.image.equalize(img)\n    images.append(img)\n    \nfinal_images=[]   \nfor k in tqdm(range(0 , len(images))):\n    img=np.array(images[k])\n    m=10+k\n    bb=df_13[\"Bounding_Box\"][m]\n    for i in bb:\n       x_min=i[0]\n       y_min=i[1]\n       x_max=i[2]\n       y_max=i[3]\n       img=cv2.rectangle(img, (x_min , y_min), (x_max , y_max), (255,0,0), 2) \n    final_images.append(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:47:15.149871Z","iopub.execute_input":"2022-02-24T12:47:15.150137Z","iopub.status.idle":"2022-02-24T12:47:20.779459Z","shell.execute_reply.started":"2022-02-24T12:47:15.150108Z","shell.execute_reply":"2022-02-24T12:47:20.778378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create_animation(final_images)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:47:20.7811Z","iopub.execute_input":"2022-02-24T12:47:20.781453Z","iopub.status.idle":"2022-02-24T12:47:20.785559Z","shell.execute_reply.started":"2022-02-24T12:47:20.781419Z","shell.execute_reply":"2022-02-24T12:47:20.784649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images=[]\nfor i in tqdm(range(10  ,30+80)):\n    img=mpimg.imread(df_13[\"image_path\"][i])\n    img=tfa.image.equalize(img)\n    images.append(img)\n    \n###############################################################################################     \nfinal_images=[]   \nfor k in tqdm(range(0 , len(images))):\n    img=np.array(images[k])\n    m=10+k\n    bb=df_13[\"Bounding_Box\"][m]\n    for i in bb:\n       x_min=i[0]\n       y_min=i[1]\n       x_max=i[2]\n       y_max=i[3]\n       img=cv2.rectangle(img, (x_min , y_min), (x_max , y_max), (255,0,0), 2) \n    final_images.append(img)\n############################################################################################### \n#creating video from Final Image list\n\nfps = 15 # don't know exact value\nwidth = 1280\nheight = 720\nsave_path = f'video_tfaEqualizer.mp4'\ntmp_path = \"tmp_\" + save_path\noutput_video = cv2.VideoWriter(tmp_path, cv2.VideoWriter_fourcc(*\"MP4V\"), fps, (width, height))\n\nfor img  in tqdm(final_images):\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    output_video.write(img)\noutput_video.release()\n\nif os.path.exists(save_path):\n        os.remove(save_path)\nsubprocess.run(\n        [\"ffmpeg\", \"-i\", tmp_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-vcodec\", \"libx264\", save_path]\n    )\nos.remove(tmp_path)\n\n############################################################################################### \nmp4 = open(f'./video_tfaEqualizer.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=1000 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:47:20.786912Z","iopub.execute_input":"2022-02-24T12:47:20.787214Z","iopub.status.idle":"2022-02-24T12:47:31.083796Z","shell.execute_reply.started":"2022-02-24T12:47:20.787175Z","shell.execute_reply":"2022-02-24T12:47:31.082144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> tfa.image.equalize give us good result \n> \n> we can properly visualize COTS in image \n> \n> so we will use tfa.img.equalize as transformation","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}