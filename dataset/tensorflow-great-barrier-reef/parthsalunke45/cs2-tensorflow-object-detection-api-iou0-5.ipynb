{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook contains code to train a crown-of-thorns starfish (COTS) detection model to serve as a baseline model for [this competition](https://www.kaggle.com/c/tensorflow-great-barrier-reef/overview). We use [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to apply transfer learning on an [EfficientDet-D0](https://arxiv.org/abs/1911.09070) pretrained model. ","metadata":{"papermill":{"duration":0.023199,"end_time":"2021-11-19T08:38:07.360759","exception":false,"start_time":"2021-11-19T08:38:07.33756","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Install TensorFlow Object Detection API\n\nPip may report some dependency errors. You can safely ignore these errors and proceed if all tests in `model_builder_tf2_test.py` passed. ","metadata":{"papermill":{"duration":0.021045,"end_time":"2021-11-19T08:38:07.403389","exception":false,"start_time":"2021-11-19T08:38:07.382344","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!git clone https://github.com/tensorflow/models\n    \n# Check out a certain commit to ensure that future changes in the TF ODT API codebase won't affect this notebook.\n!cd models && git checkout ac8d06519","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":25.822618,"end_time":"2021-11-19T08:38:33.247836","exception":false,"start_time":"2021-11-19T08:38:07.425218","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T04:32:33.1632Z","iopub.execute_input":"2022-02-06T04:32:33.163563Z","iopub.status.idle":"2022-02-06T04:32:55.612382Z","shell.execute_reply.started":"2022-02-06T04:32:33.163477Z","shell.execute_reply":"2022-02-06T04:32:55.611593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\ncd models/research\n\n# Compile protos.\nprotoc object_detection/protos/*.proto --python_out=.\n\n# Install TensorFlow Object Detection API.\n# Note: I fixed the version of some dependencies to make it work on Kaggle notebook. In particular:\n# * scipy==1.6.3 to avoid the missing GLIBCXX_3.4.26 error\n# * tensorflow to 2.6.0 to make it compatible with the CUDA version preinstalled on Kaggle.\n# When Kaggle notebook upgrade to TF 2.7, you can use the default setup.py script:\n# cp object_detection/packages/tf2/setup.py .\nwget https://storage.googleapis.com/odml-dataset/others/setup.py\npip install -q --user .\n\n# Test if the Object Dectection API is working correctly\npython object_detection/builders/model_builder_tf2_test.py","metadata":{"papermill":{"duration":79.154739,"end_time":"2021-11-19T08:39:52.448588","exception":false,"start_time":"2021-11-19T08:38:33.293849","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T04:32:55.614647Z","iopub.execute_input":"2022-02-06T04:32:55.614916Z","iopub.status.idle":"2022-02-06T04:34:23.635517Z","shell.execute_reply.started":"2022-02-06T04:32:55.614878Z","shell.execute_reply":"2022-02-06T04:34:23.634697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import dependencies","metadata":{}},{"cell_type":"code","source":"import contextlib2\nimport io\nimport IPython\nimport json\nimport numpy as np\nimport os\nimport pathlib\nimport pandas as pd\nimport sys\nimport tensorflow as tf\nimport time\n\nfrom PIL import Image, ImageDraw\n\n# Import the library that is used to submit the prediction result.\nINPUT_DIR = '/kaggle/input/tensorflow-great-barrier-reef/'\nsys.path.insert(0, INPUT_DIR)\nimport greatbarrierreef","metadata":{"papermill":{"duration":1.526137,"end_time":"2021-11-19T08:39:54.022253","exception":false,"start_time":"2021-11-19T08:39:52.496116","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T04:34:23.637507Z","iopub.execute_input":"2022-02-06T04:34:23.637797Z","iopub.status.idle":"2022-02-06T04:34:25.155061Z","shell.execute_reply.started":"2022-02-06T04:34:23.637758Z","shell.execute_reply":"2022-02-06T04:34:25.154304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The notebook is supposed to run with TF 2.6.0\nprint(tf.__version__)\nprint(tf.test.is_gpu_available())\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"papermill":{"duration":0.729459,"end_time":"2021-11-19T08:39:54.798333","exception":false,"start_time":"2021-11-19T08:39:54.068874","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T04:34:25.157375Z","iopub.execute_input":"2022-02-06T04:34:25.157652Z","iopub.status.idle":"2022-02-06T04:34:25.839103Z","shell.execute_reply.started":"2022-02-06T04:34:25.157599Z","shell.execute_reply":"2022-02-06T04:34:25.838355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **TensorFlow Object Detection API 2**","metadata":{}},{"cell_type":"code","source":"from object_detection.utils import dataset_util\nfrom object_detection.dataset_tools import tf_record_creation_util\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:34:25.842036Z","iopub.execute_input":"2022-02-06T04:34:25.842239Z","iopub.status.idle":"2022-02-06T04:34:25.899474Z","shell.execute_reply.started":"2022-02-06T04:34:25.842216Z","shell.execute_reply":"2022-02-06T04:34:25.898842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\ntest=pd.read_csv(\"../input/tensorflow-great-barrier-reef/test.csv\")\nsample=pd.read_csv(\"../input/tensorflow-great-barrier-reef/example_sample_submission.csv\")\n\n# # adding image path to data frame\n# dir= \"../input/tensorflow-great-barrier-reef/train_images\"\n# df['image_path'] = dir + \"/video_\" + df['video_id'].astype(str) + \"/\" + df['video_frame'].astype(str) + \".jpg\"\n\n# #######################################################################\n\n# # converting string annotations into list\n# df['annotations'] =df['annotations'].apply(eval)\n# ########################################################################\n\n# #counting number of bounding boxes in each img  and adding it to new variable no_of_boundingBox\n\n# no_of_BoundingBox=[]\n# for i in tqdm(df[\"annotations\"]):\n#     no_of_BoundingBox.append(len(i))\n# df[\"no_of_BoundingBox\"]=no_of_BoundingBox\n\n# ########################################################################\n\n# # removing images which dont have COTS/bounding_boxes \n\n# df=df[df[\"no_of_BoundingBox\"]>=1]\n# df.reset_index(drop=True, inplace=True)\n\n# #converting annotation into yolo format\n# yolo_annotation=[]\n\n# # img_w , img_h = 1280, 720\n# df[\"width\"]=1280\n# df[\"height\"]=720\n# df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:34:25.900858Z","iopub.execute_input":"2022-02-06T04:34:25.901116Z","iopub.status.idle":"2022-02-06T04:34:25.96395Z","shell.execute_reply.started":"2022-02-06T04:34:25.901081Z","shell.execute_reply":"2022-02-06T04:34:25.963261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\ndf = df[df.annotations != '[]'].reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:34:25.965329Z","iopub.execute_input":"2022-02-06T04:34:25.965654Z","iopub.status.idle":"2022-02-06T04:34:26.008175Z","shell.execute_reply.started":"2022-02-06T04:34:25.965603Z","shell.execute_reply":"2022-02-06T04:34:26.007542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nkf = GroupKFold(n_splits = 3)\ndf = df.reset_index(drop=True)\ndf['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, groups=df.video_id.tolist())):\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.fold.value_counts())\n\nFOLD=1\ntrain_files = []\nval_files   = []\ntrain_df = df.query(\"fold!=@FOLD\")\nval_df = df.query(\"fold==@FOLD\")\nval_df.reset_index(drop=True, inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:34:26.010734Z","iopub.execute_input":"2022-02-06T04:34:26.010918Z","iopub.status.idle":"2022-02-06T04:34:26.275442Z","shell.execute_reply.started":"2022-02-06T04:34:26.010896Z","shell.execute_reply":"2022-02-06T04:34:26.274767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from object_detection.utils import dataset_util\nfrom object_detection.dataset_tools import tf_record_creation_util\n\n\ndef create_tf_example(video_id, video_frame, data_df, image_path):\n    \"\"\"Create a tf.Example entry for a given training image.\"\"\"\n    full_path = os.path.join(image_path, os.path.join(f'video_{video_id}', f'{video_frame}.jpg'))\n    with tf.io.gfile.GFile(full_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n\n    height = image.size[1] # Image height\n    width = image.size[0] # Image width\n    filename = f'{video_id}:{video_frame}'.encode('utf8') # Unique id of the image.\n    encoded_image_data = None # Encoded image bytes\n    image_format = 'jpeg'.encode('utf8') # b'jpeg' or b'png'\n\n    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = [] # List of normalized right x coordinates in bounding box\n             # (1 per box)\n    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n             # (1 per box)\n    classes_text = [] # List of string class name of bounding box (1 per box)\n    classes = [] # List of integer class id of bounding box (1 per box)\n\n    rows = data_df[(data_df.video_id == video_id) & (data_df.video_frame == video_frame)]\n    for _, row in rows.iterrows():\n        annotations = json.loads(row.annotations.replace(\"'\", '\"'))\n        for annotation in annotations:\n            xmins.append(annotation['x'] / width) \n            xmaxs.append((annotation['x'] + annotation['width']) / width) \n            ymins.append(annotation['y'] / height) \n            ymaxs.append((annotation['y'] + annotation['height']) / height) \n\n            classes_text.append('COTS'.encode('utf8'))\n            classes.append(1)\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image/height': dataset_util.int64_feature(height),\n      'image/width': dataset_util.int64_feature(width),\n      'image/filename': dataset_util.bytes_feature(filename),\n      'image/source_id': dataset_util.bytes_feature(filename),\n      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image/format': dataset_util.bytes_feature(image_format),\n      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n      'image/object/class/label': dataset_util.int64_list_feature(classes),\n    }))\n    \n    return tf_example\n\ndef convert_to_tfrecord(data_df, tfrecord_filebase, image_path, num_shards = 10):\n    \"\"\"Convert the object detection dataset to TFRecord as required by the TF ODT API.\"\"\"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n            tf_record_close_stack, tfrecord_filebase, num_shards)\n        \n        for index, row in data_df.iterrows():\n            if index % 500 == 0:\n                print('Processed {0} images.'.format(index))\n            tf_example = create_tf_example(row.video_id, row.video_frame, data_df, image_path)\n            output_shard_index = index % num_shards\n            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())\n        \n        print('Completed processing {0} images.'.format(len(data_df)))\n\n!mkdir dataset\nimage_path = os.path.join(INPUT_DIR, 'train_images')\n\n# Convert train images to TFRecord\nprint('Converting TRAIN images...')\nconvert_to_tfrecord(\n  train_df,\n  '/kaggle/working/dataset/cots_train',\n  image_path,\n  num_shards = 4\n)\n\n# Convert validation images to TFRecord\nprint('Converting VALIDATION images...')\nconvert_to_tfrecord(\n  val_df,\n  '/kaggle/working/dataset/cots_val',\n  image_path,\n  num_shards = 4\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:34:26.276757Z","iopub.execute_input":"2022-02-06T04:34:26.277127Z","iopub.status.idle":"2022-02-06T04:35:41.572774Z","shell.execute_reply.started":"2022-02-06T04:34:26.277088Z","shell.execute_reply":"2022-02-06T04:35:41.571905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a label map to map between label index and human-readable label name.\n\nlabel_map_str = \"\"\"item {\n  id: 1\n  name: 'COTS'\n}\"\"\"\n\nwith open('dataset/label_map.pbtxt', 'w') as f:\n  f.write(label_map_str)\n\n!more dataset/label_map.pbtxt","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:35:41.576071Z","iopub.execute_input":"2022-02-06T04:35:41.576281Z","iopub.status.idle":"2022-02-06T04:35:42.249479Z","shell.execute_reply.started":"2022-02-06T04:35:41.576254Z","shell.execute_reply":"2022-02-06T04:35:42.248666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download the pretrained EfficientDet-D0 checkpoint\n!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n!tar -xvzf efficientdet_d0_coco17_tpu-32.tar.gz","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:35:42.251754Z","iopub.execute_input":"2022-02-06T04:35:42.252495Z","iopub.status.idle":"2022-02-06T04:35:44.64285Z","shell.execute_reply.started":"2022-02-06T04:35:42.252456Z","shell.execute_reply":"2022-02-06T04:35:44.642036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from string import Template\n\nconfig_file_template = \"\"\"\n# SSD with EfficientNet-b0 + BiFPN feature extractor,\n# shared box predictor and focal loss (a.k.a EfficientDet-d0).\n# See EfficientDet, Tan et al, https://arxiv.org/abs/1911.09070\n# See Lin et al, https://arxiv.org/abs/1708.02002\n# Initialized from an EfficientDet-D0 checkpoint.\n#\n# Train on GPU\n\nmodel {\n  ssd {\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n    num_classes: 1\n    add_background_class: false\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    encode_background_as_zeros: true\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: [1.0, 2.0, 0.5]\n        scales_per_octave: 3\n      }\n    }\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 1280\n        max_dimension: 1280\n        pad_to_max_dimension: true\n        }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        depth: 64\n        class_prediction_bias_init: -4.6\n        conv_hyperparams {\n          force_use_bias: true\n          activation: SWISH\n          regularizer {\n            l2_regularizer {\n              weight: 0.00004\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              stddev: 0.01\n              mean: 0.0\n            }\n          }\n          batch_norm {\n            scale: true\n            decay: 0.99\n            epsilon: 0.001\n          }\n        }\n        num_layers_before_predictor: 3\n        kernel_size: 3\n        use_depthwise: true\n      }\n    }\n    feature_extractor {\n      type: 'ssd_efficientnet-b0_bifpn_keras'\n      bifpn {\n        min_level: 3\n        max_level: 7\n        num_iterations: 3\n        num_filters: 64\n      }\n      conv_hyperparams {\n        force_use_bias: true\n        activation: SWISH\n        regularizer {\n          l2_regularizer {\n            weight: 0.00004\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            stddev: 0.03\n            mean: 0.0\n          }\n        }\n        batch_norm {\n          scale: true,\n          decay: 0.99,\n          epsilon: 0.001,\n        }\n      }\n    }\n    loss {\n      classification_loss {\n        weighted_sigmoid_focal {\n          alpha: 0.25\n          gamma: 1.5\n        }\n      }\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    normalize_loss_by_num_matches: true\n    normalize_loc_loss_by_codesize: true\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 1e-8\n        iou_threshold: 0.5\n        max_detections_per_class: 100\n        max_total_detections: 100\n      }\n      score_converter: SIGMOID\n    }\n  }\n}\n\ntrain_config: {\n  fine_tune_checkpoint: \"efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\"\n  fine_tune_checkpoint_version: V2\n  fine_tune_checkpoint_type: \"detection\"\n  batch_size: 2\n  sync_replicas: false\n  startup_delay_steps: 0\n  replicas_to_aggregate: 1\n  use_bfloat16: false\n  num_steps: $training_steps\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_scale_crop_and_pad_to_square {\n      output_size: 1280\n      scale_min: 0.5\n      scale_max: 2.0\n    }\n  }\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        cosine_decay_learning_rate {\n          learning_rate_base: 5e-3\n          total_steps: $training_steps\n          warmup_learning_rate: 5e-4\n          warmup_steps: $warmup_steps\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n}\n\ntrain_input_reader: {\n  label_map_path: \"dataset/label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"dataset/cots_train-?????-of-00004\"\n  }\n}\n\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 2;\n}\n\neval_input_reader: {\n  label_map_path: \"dataset/label_map.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"dataset/cots_val-?????-of-00004\"\n  }\n}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:35:44.644902Z","iopub.execute_input":"2022-02-06T04:35:44.645194Z","iopub.status.idle":"2022-02-06T04:35:44.653216Z","shell.execute_reply.started":"2022-02-06T04:35:44.645155Z","shell.execute_reply":"2022-02-06T04:35:44.652456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the training pipeline\n\nTRAINING_STEPS = 20000\nWARMUP_STEPS = 2000\nPIPELINE_CONFIG_PATH='dataset/pipeline.config'\n\npipeline = Template(config_file_template).substitute(\n    training_steps=TRAINING_STEPS, warmup_steps=WARMUP_STEPS)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:35:44.654506Z","iopub.execute_input":"2022-02-06T04:35:44.654935Z","iopub.status.idle":"2022-02-06T04:35:44.666056Z","shell.execute_reply.started":"2022-02-06T04:35:44.654898Z","shell.execute_reply":"2022-02-06T04:35:44.665307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_DIR='cots_efficientdet_d0'\n!mkdir {MODEL_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:35:44.667576Z","iopub.execute_input":"2022-02-06T04:35:44.668029Z","iopub.status.idle":"2022-02-06T04:35:45.408621Z","shell.execute_reply.started":"2022-02-06T04:35:44.667983Z","shell.execute_reply":"2022-02-06T04:35:45.407402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL_DIR='cots_efficientdet_d0'\n# !mkdir {MODEL_DIR}\n!python models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --alsologtostderr","metadata":{"execution":{"iopub.status.busy":"2022-02-06T04:35:45.410073Z","iopub.execute_input":"2022-02-06T04:35:45.410875Z","iopub.status.idle":"2022-02-06T09:41:37.324835Z","shell.execute_reply.started":"2022-02-06T04:35:45.410829Z","shell.execute_reply":"2022-02-06T09:41:37.323949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --checkpoint_dir={MODEL_DIR} \\\n    --eval_timeout=0 \\\n    --alsologtostderr","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:49:37.140975Z","iopub.execute_input":"2022-02-06T09:49:37.142461Z","iopub.status.idle":"2022-02-06T09:57:08.87753Z","shell.execute_reply.started":"2022-02-06T09:49:37.142404Z","shell.execute_reply":"2022-02-06T09:57:08.876677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"import time\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as viz_utils\nimport matplotlib.image as mpimg\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n\ndef load_image_into_numpy_array(path):\n    return np.array(Image.open(path))\n\ndef plot_img(img_arr):\n  '''function take input as array and plot image'''\n  plt.figure(figsize = (15 , 5))\n  plt.imshow(img_arr)\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:57:51.808275Z","iopub.execute_input":"2022-02-06T09:57:51.808581Z","iopub.status.idle":"2022-02-06T09:57:51.867267Z","shell.execute_reply.started":"2022-02-06T09:57:51.808549Z","shell.execute_reply":"2022-02-06T09:57:51.86656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH_TO_LABELS = '/kaggle/working/dataset/label_map.pbtxt'\nIMAGE_PATHS=\"/kaggle/input/tensorflow-great-barrier-reef/train_images/video_0/45.jpg\"","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:57:54.126601Z","iopub.execute_input":"2022-02-06T09:57:54.127397Z","iopub.status.idle":"2022-02-06T09:57:54.13236Z","shell.execute_reply.started":"2022-02-06T09:57:54.127356Z","shell.execute_reply":"2022-02-06T09:57:54.131506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,use_display_name=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:57:54.921747Z","iopub.execute_input":"2022-02-06T09:57:54.922425Z","iopub.status.idle":"2022-02-06T09:57:54.927419Z","shell.execute_reply.started":"2022-02-06T09:57:54.922388Z","shell.execute_reply":"2022-02-06T09:57:54.926407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Loading model...', end='')\nstart_time = time.time()\n\n# LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\ndetect_fn = tf.saved_model.load(\"/kaggle/working/efficientdet_d0_coco17_tpu-32/saved_model\")\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint('Done! Took {} seconds'.format(elapsed_time))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:57:55.618849Z","iopub.execute_input":"2022-02-06T09:57:55.61941Z","iopub.status.idle":"2022-02-06T09:58:22.865924Z","shell.execute_reply.started":"2022-02-06T09:57:55.61937Z","shell.execute_reply":"2022-02-06T09:58:22.865136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image =  mpimg.imread(\"/kaggle/input/tensorflow-great-barrier-reef/train_images/video_0/85.jpg\")\n# image_rgb =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage_expande = np.expand_dims(image, axis=0)\ninput_tensor = tf.convert_to_tensor(image)\ninput_tensor = input_tensor[tf.newaxis, ...]\ndetections = detect_fn(input_tensor)\nnum_detections = int(detections.pop('num_detections'))\ndetections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\ndetections['num_detections'] = num_detections\n\ndetections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n\nimage_with_detections = image.copy()\n\nviz_utils.visualize_boxes_and_labels_on_image_array(\n      image_with_detections,\n      detections['detection_boxes'],\n      detections['detection_classes'],\n      detections['detection_scores'],\n      category_index,\n      use_normalized_coordinates=True,\n      max_boxes_to_draw=100,\n      min_score_thresh=0.4,\n      agnostic_mode=False)\n# ​\n# print('Done')\n# # # DISPLAYS OUTPUT IMAGE\nprint(\"original img\")\n%pylab inline\nplot_img(image)\nprint(\"predicted output\")\n%pylab inline\nplot_img(image_with_detections)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:58:22.867809Z","iopub.execute_input":"2022-02-06T09:58:22.868095Z","iopub.status.idle":"2022-02-06T09:58:29.534071Z","shell.execute_reply.started":"2022-02-06T09:58:22.868057Z","shell.execute_reply":"2022-02-06T09:58:29.53347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image =  mpimg.imread(\"/kaggle/input/tensorflow-great-barrier-reef/train_images/video_0/60.jpg\")\n# image_rgb =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage_expande = np.expand_dims(image, axis=0)\ninput_tensor = tf.convert_to_tensor(image)\ninput_tensor = input_tensor[tf.newaxis, ...]\ndetections = detect_fn(input_tensor)\nnum_detections = int(detections.pop('num_detections'))\ndetections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\ndetections['num_detections'] = num_detections\n\ndetections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n\nimage_with_detections = image.copy()\n\nviz_utils.visualize_boxes_and_labels_on_image_array(\n      image_with_detections,\n      detections['detection_boxes'],\n      detections['detection_classes'],\n      detections['detection_scores'],\n      category_index,\n      use_normalized_coordinates=True,\n      max_boxes_to_draw=100,\n      min_score_thresh=0.4,\n      agnostic_mode=False)\n# ​\n# print('Done')\n# # # DISPLAYS OUTPUT IMAGE\nprint(\"original img\")\n%pylab inline\nplot_img(image)\nprint(\"predicted output\")\n%pylab inline\nplot_img(image_with_detections)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:58:38.230167Z","iopub.execute_input":"2022-02-06T09:58:38.230453Z","iopub.status.idle":"2022-02-06T09:58:39.180309Z","shell.execute_reply.started":"2022-02-06T09:58:38.230424Z","shell.execute_reply":"2022-02-06T09:58:39.179481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image =  mpimg.imread(\"/kaggle/input/tensorflow-great-barrier-reef/train_images/video_0/85.jpg\")\n# image_rgb =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage_expande = np.expand_dims(image, axis=0)\ninput_tensor = tf.convert_to_tensor(image)\ninput_tensor = input_tensor[tf.newaxis, ...]\ndetections = detect_fn(input_tensor)\nnum_detections = int(detections.pop('num_detections'))\ndetections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\ndetections['num_detections'] = num_detections\n\ndetections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n\nimage_with_detections = image.copy()\n\nviz_utils.visualize_boxes_and_labels_on_image_array(\n      image_with_detections,\n      detections['detection_boxes'],\n      detections['detection_classes'],\n      detections['detection_scores'],\n      category_index,\n      use_normalized_coordinates=True,\n      max_boxes_to_draw=100,\n      min_score_thresh=0.4,\n      agnostic_mode=False)\n# ​\n# print('Done')\n# # # DISPLAYS OUTPUT IMAGE\nprint(\"original img\")\n%pylab inline\nplot_img(image)\nprint(\"predicted output\")\n%pylab inline\nplot_img(image_with_detections)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:59:32.083308Z","iopub.execute_input":"2022-02-06T09:59:32.083601Z","iopub.status.idle":"2022-02-06T09:59:33.163171Z","shell.execute_reply.started":"2022-02-06T09:59:32.083568Z","shell.execute_reply":"2022-02-06T09:59:33.162544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image =  mpimg.imread(\"/kaggle/input/tensorflow-great-barrier-reef/train_images/video_2/5765.jpg\")\n# image_rgb =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage_expande = np.expand_dims(image, axis=0)\ninput_tensor = tf.convert_to_tensor(image)\ninput_tensor = input_tensor[tf.newaxis, ...]\ndetections = detect_fn(input_tensor)\nnum_detections = int(detections.pop('num_detections'))\ndetections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\ndetections['num_detections'] = num_detections\n\ndetections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n\nimage_with_detections = image.copy()\n\nviz_utils.visualize_boxes_and_labels_on_image_array(\n      image_with_detections,\n      detections['detection_boxes'],\n      detections['detection_classes'],\n      detections['detection_scores'],\n      category_index,\n      use_normalized_coordinates=True,\n      max_boxes_to_draw=100,\n      min_score_thresh=0.4,\n      agnostic_mode=False)\n# ​\n# print('Done')\n# # # DISPLAYS OUTPUT IMAGE\nprint(\"original img\")\n%pylab inline\nplot_img(image)\nprint(\"predicted output\")\n%pylab inline\nplot_img(image_with_detections)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:00:34.142261Z","iopub.execute_input":"2022-02-06T10:00:34.142547Z","iopub.status.idle":"2022-02-06T10:00:35.098014Z","shell.execute_reply.started":"2022-02-06T10:00:34.142517Z","shell.execute_reply":"2022-02-06T10:00:35.093898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(55 , 70):\n    image =  mpimg.imread(f\"/kaggle/input/tensorflow-great-barrier-reef/train_images/video_0/{i}.jpg\")\n    # image_rgb =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image_expande = np.expand_dims(image, axis=0)\n    input_tensor = tf.convert_to_tensor(image)\n    input_tensor = input_tensor[tf.newaxis, ...]\n    detections = detect_fn(input_tensor)\n    num_detections = int(detections.pop('num_detections'))\n    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n    detections['num_detections'] = num_detections\n\n    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n\n    image_with_detections = image.copy()\n\n    viz_utils.visualize_boxes_and_labels_on_image_array(\n      image_with_detections,\n      detections['detection_boxes'],\n      detections['detection_classes'],\n      detections['detection_scores'],\n      category_index,\n      use_normalized_coordinates=True,\n      max_boxes_to_draw=100,\n      min_score_thresh=0.4,\n      agnostic_mode=False)\n# ​\n# print('Done')\n# # # DISPLAYS OUTPUT IMAGE\n    print(\"original img\")\n    %pylab inline\n    plot_img(image)\n    print(\"predicted output\")\n    %pylab inline\n    plot_img(image_with_detections)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:11:03.249951Z","iopub.execute_input":"2022-02-06T10:11:03.250763Z","iopub.status.idle":"2022-02-06T10:11:18.199129Z","shell.execute_reply.started":"2022-02-06T10:11:03.250713Z","shell.execute_reply":"2022-02-06T10:11:18.198427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(90 , 110):\n    image =  mpimg.imread(f\"/kaggle/input/tensorflow-great-barrier-reef/train_images/video_0/{i}.jpg\")\n    # image_rgb =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image_expande = np.expand_dims(image, axis=0)\n    input_tensor = tf.convert_to_tensor(image)\n    input_tensor = input_tensor[tf.newaxis, ...]\n    detections = detect_fn(input_tensor)\n    num_detections = int(detections.pop('num_detections'))\n    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n    detections['num_detections'] = num_detections\n\n    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n\n    image_with_detections = image.copy()\n\n    viz_utils.visualize_boxes_and_labels_on_image_array(\n      image_with_detections,\n      detections['detection_boxes'],\n      detections['detection_classes'],\n      detections['detection_scores'],\n      category_index,\n      use_normalized_coordinates=True,\n      max_boxes_to_draw=100,\n      min_score_thresh=0.4,\n      agnostic_mode=False)\n# ​\n# print('Done')\n# # # DISPLAYS OUTPUT IMAGE\n    print(\"original img\")\n    %pylab inline\n    plot_img(image)\n    print(\"predicted output\")\n    %pylab inline\n    plot_img(image_with_detections)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T10:18:13.115981Z","iopub.execute_input":"2022-02-06T10:18:13.116585Z","iopub.status.idle":"2022-02-06T10:18:32.410007Z","shell.execute_reply.started":"2022-02-06T10:18:13.116534Z","shell.execute_reply":"2022-02-06T10:18:32.406105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}