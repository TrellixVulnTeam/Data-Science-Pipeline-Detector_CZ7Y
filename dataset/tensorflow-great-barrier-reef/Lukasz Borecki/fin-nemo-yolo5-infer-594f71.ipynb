{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\nDATASET_PATH = '/kaggle/input/tensorflow-great-barrier-reef/train_images/'\nCKPT_PATH = '/kaggle/input/cots-nemo-team/f0-10.pt'\nCKPT_PATH1 = '/kaggle/input/reef-inference/f2_sub2.pt'\n\nIMG_SIZE  = 2400\nCONF      = 0.40\nIOU       = 0.95\n\nIMG_SIZE1  = 6400\nCONF1      = 0.3\nIOU1       = 0.95\n\nimport numpy as np\nimport pandas as pd\nimport sys\nimport cv2\nimport torch\nfrom PIL import Image\n\nsys.path.append('../input/tensorflow-great-barrier-reef')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T16:43:03.639886Z","iopub.execute_input":"2022-02-04T16:43:03.640551Z","iopub.status.idle":"2022-02-04T16:43:05.171131Z","shell.execute_reply.started":"2022-02-04T16:43:03.640466Z","shell.execute_reply":"2022-02-04T16:43:05.1696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/input/augs-tweaked/albumentations\n%pip install -U .\n%cd /kaggle/working","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:20:41.358665Z","iopub.execute_input":"2022-01-25T05:20:41.358902Z","iopub.status.idle":"2022-01-25T05:20:41.611706Z","shell.execute_reply.started":"2022-01-25T05:20:41.358869Z","shell.execute_reply":"2022-01-25T05:20:41.610941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/reef-inference/ensemble_boxes-1.0.7-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:20:43.115715Z","iopub.execute_input":"2022-01-25T05:20:43.115969Z","iopub.status.idle":"2022-01-25T05:21:11.403401Z","shell.execute_reply.started":"2022-01-25T05:20:43.115938Z","shell.execute_reply":"2022-01-25T05:21:11.402502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ensemble_boxes import *","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:21:11.405566Z","iopub.execute_input":"2022-01-25T05:21:11.405837Z","iopub.status.idle":"2022-01-25T05:21:12.044231Z","shell.execute_reply.started":"2022-01-25T05:21:11.405801Z","shell.execute_reply":"2022-01-25T05:21:12.043486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp ../input/yolov5-font/Arial.ttf /root/.config/Ultralytics/","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-25T05:21:17.222414Z","iopub.execute_input":"2022-01-25T05:21:17.222869Z","iopub.status.idle":"2022-01-25T05:21:18.568Z","shell.execute_reply.started":"2022-01-25T05:21:17.222832Z","shell.execute_reply":"2022-01-25T05:21:18.567078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-25T05:21:18.799415Z","iopub.execute_input":"2022-01-25T05:21:18.800151Z","iopub.status.idle":"2022-01-25T05:22:32.16929Z","shell.execute_reply.started":"2022-01-25T05:21:18.8001Z","shell.execute_reply":"2022-01-25T05:22:32.168418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:22:32.173036Z","iopub.execute_input":"2022-01-25T05:22:32.173477Z","iopub.status.idle":"2022-01-25T05:22:32.179149Z","shell.execute_reply.started":"2022-01-25T05:22:32.173444Z","shell.execute_reply":"2022-01-25T05:22:32.178424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RecoverCLAHE(sceneRadiance):\n    clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(4, 4))\n    for i in range(3):\n        sceneRadiance[:, :, i] = clahe.apply((sceneRadiance[:, :, i]))\n\n    return sceneRadiance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(ckpt_path, conf=0.15, iou=0.30):\n    model = torch.hub.load('/kaggle/input/yolo-lib',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload = True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 100  # maximum number of detections per image\n    return model\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-25T05:22:32.18073Z","iopub.execute_input":"2022-01-25T05:22:32.181403Z","iopub.status.idle":"2022-01-25T05:22:32.189864Z","shell.execute_reply.started":"2022-01-25T05:22:32.181365Z","shell.execute_reply":"2022-01-25T05:22:32.189156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(models, img, size = 1280, augment = False):\n    bboxes = []\n    scores = []\n    box_list = []\n    score_list = []\n    label_list = []\n    for i, model in enumerate(models):\n        if i==0:\n            results = model(img, size=6400, augment = False)  # custom inference size\n        elif i==1:\n            results = model(img, size=4800, augment = False)\n        elif i==2:\n            results = model(img, size=3200, augment = False)\n        else:\n            results = model(img, size=2400, augment = True)\n        preds   = results.pandas().xyxy[0].values\n        preds[:,[0,2]]=preds[:,[0,2]]/1280\n        preds[:,[1,3]]=preds[:,[1,3]]/720\n        bboxes, scores, labels = weighted_boxes_fusion([preds[:,:4]], [preds[:,4]], [preds[:,5]], weights=None, iou_thr=0.25, skip_box_thr=0.001)\n        box_list.append(bboxes)\n        score_list.append(scores)\n        label_list.append(labels)\n    \n\n    bboxes, scores, labels = weighted_boxes_fusion(box_list, score_list, label_list, weights=None, iou_thr=0.25, skip_box_thr=0.001)\n    bboxes[:,[0,2]]=bboxes[:,[0,2]]*1280\n    bboxes[:,[1,3]]=bboxes[:,[1,3]]*720\n    bboxes1, scores1, labels1 = [],[], []\n    for b,s,l in zip(bboxes,scores,labels):\n        if s >= 0.23:\n            bboxes1.append(b)\n            scores1.append(s)\n            labels1.append(l)\n\n\n    return bboxes1, scores1","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:38:04.656711Z","iopub.execute_input":"2022-01-25T05:38:04.656963Z","iopub.status.idle":"2022-01-25T05:38:04.666347Z","shell.execute_reply.started":"2022-01-25T05:38:04.656933Z","shell.execute_reply":"2022-01-25T05:38:04.665535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from norfair import Detection, Tracker\n\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        ratio = w/h\n        if ratio > 0.45 and ratio < 2.22:\n            result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:23:39.991925Z","iopub.execute_input":"2022-01-25T05:23:39.992305Z","iopub.status.idle":"2022-01-25T05:23:40.71076Z","shell.execute_reply.started":"2022-01-25T05:23:39.992268Z","shell.execute_reply":"2022-01-25T05:23:40.709876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=25,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\nframe_id = 0","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:23:41.367879Z","iopub.execute_input":"2022-01-25T05:23:41.368384Z","iopub.status.idle":"2022-01-25T05:23:41.376443Z","shell.execute_reply.started":"2022-01-25T05:23:41.368346Z","shell.execute_reply":"2022-01-25T05:23:41.37528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_prediction(img, bboxes):\n    colors = [(0, 0, 255)]\n\n    obj_names = [\"s\"]\n\n    for box in bboxes:\n        cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255,0,0), 2)\n    \n    img = Image.fromarray(img).resize((800, 400))\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:23:42.333979Z","iopub.execute_input":"2022-01-25T05:23:42.334582Z","iopub.status.idle":"2022-01-25T05:23:42.341139Z","shell.execute_reply.started":"2022-01-25T05:23:42.334542Z","shell.execute_reply":"2022-01-25T05:23:42.340169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = load_model(CKPT_PATH1,CONF1,IOU1)\nmodel2 = load_model(CKPT_PATH1,0.4,IOU1)\nmodel3 = load_model(CKPT_PATH1,0.4,IOU1)\nmodel4 = load_model(CKPT_PATH,0.4,IOU)\nmodels = [model1,model2,model3,model4]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:23:44.129724Z","iopub.execute_input":"2022-01-25T05:23:44.13075Z","iopub.status.idle":"2022-01-25T05:23:52.494304Z","shell.execute_reply.started":"2022-01-25T05:23:44.130703Z","shell.execute_reply":"2022-01-25T05:23:52.493482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir = f'{DATASET_PATH}'\nimgs = [dir + f for f in ('video_0/9674.jpg',\n                          'video_0/20.jpg', \n                          'video_0/17.jpg', \n                          'video_0/100.jpg',\n                          'video_0/246.jpg',\n                          'video_0/337.jpg',\n                          'video_0/358.jpg',\n                          'video_0/1566.jpg',\n                          'video_0/1854.jpg',\n                          'video_0/1884.jpg',\n                          'video_0/4224.jpg',\n                          'video_0/4540.jpg',\n                          'video_0/4582.jpg',\n                          'video_0/4664.jpg',\n                          'video_0/8215.jpg',\n                          'video_0/8897.jpg',\n                          'video_0/8928.jpg',\n                          'video_0/9441.jpg',\n                          'video_0/9859.jpg',\n                          'video_0/12267.jpg',\n                          'video_2/5748.jpg',\n                          'video_2/5772.jpg',\n                          'video_2/5820.jpg',\n                          'video_2/5409.jpg',\n                          'video_2/5482.jpg',\n                          'video_2/5679.jpg',\n                          'video_2/5712.jpg',\n                          'video_2/5730.jpg',\n                          'video_2/5751.jpg',\n                          'video_2/5817.jpg',\n                          'video_2/5868.jpg',\n                          'video_2/6254.jpg',\n                          'video_2/6339.jpg',\n                          'video_2/10622.jpg',\n                          'video_1/4159.jpg', \n                          'video_1/4183.jpg', \n                          'video_1/4501.jpg',\n                          'video_1/5474.jpg',\n                          'video_1/625.jpg',\n                          'video_1/616.jpg',\n                          'video_1/672.jpg',\n                          'video_1/684.jpg',\n                          'video_1/850.jpg',\n                          'video_1/1927.jpg',\n                          'video_1/2000.jpg',\n                          'video_1/3903.jpg',\n                          'video_1/3945.jpg',\n                          'video_1/4051.jpg',\n                          'video_1/4078.jpg',\n                          'video_1/4096.jpg',\n                          'video_1/4126.jpg',\n                          'video_1/4456.jpg',\n                          'video_1/4525.jpg',\n                          'video_1/5267.jpg',\n                          'video_1/5366.jpg',\n                          'video_1/5411.jpg',\n                          'video_1/5429.jpg',\n                          'video_1/5492.jpg',\n                          'video_1/5661.jpg',\n                          'video_1/5892.jpg',\n                          'video_1/6747.jpg',\n                          'video_1/9082.jpg',)]\n\nfor img in(imgs):\n    im = cv2.imread(img)\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    bboxes, scores = predict(models, im, size = IMG_SIZE, augment = True)\n    display(show_prediction(im, bboxes))","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:38:08.146515Z","iopub.execute_input":"2022-01-25T05:38:08.146771Z","iopub.status.idle":"2022-01-25T05:38:12.431254Z","shell.execute_reply.started":"2022-01-25T05:38:08.146742Z","shell.execute_reply":"2022-01-25T05:38:12.430444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-25T05:36:10.811824Z","iopub.execute_input":"2022-01-25T05:36:10.81231Z","iopub.status.idle":"2022-01-25T05:36:10.844202Z","shell.execute_reply.started":"2022-01-25T05:36:10.81227Z","shell.execute_reply":"2022-01-25T05:36:10.843414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n\nfor (image_np, sample_prediction_df) in iter_test:\n    \n    bboxes, scores = predict(models, image_np, size = IMG_SIZE, augment = True)\n    \n    predictions = []\n    detects = []\n    \n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        score = scores[i]\n        x_min = int(box[0])\n        y_min = int(box[1])\n        x_max = int(box[2])\n        y_max = int(box[3])\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        detects.append([x_min, y_min, x_max, y_max, score])\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        \n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    \n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n        if last_detected_frame_id <= frame_id-2:  continue\n            \n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n        ratio = bbox_width/bbox_height\n        if ratio > 0.45 and ratio < 2.22:\n            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n    frame_id += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:36:11.614422Z","iopub.execute_input":"2022-01-25T05:36:11.615239Z","iopub.status.idle":"2022-01-25T05:36:14.638292Z","shell.execute_reply.started":"2022-01-25T05:36:11.615189Z","shell.execute_reply":"2022-01-25T05:36:14.637529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T05:36:15.815163Z","iopub.execute_input":"2022-01-25T05:36:15.815695Z","iopub.status.idle":"2022-01-25T05:36:15.830716Z","shell.execute_reply.started":"2022-01-25T05:36:15.815657Z","shell.execute_reply":"2022-01-25T05:36:15.829973Z"},"trusted":true},"execution_count":null,"outputs":[]}]}