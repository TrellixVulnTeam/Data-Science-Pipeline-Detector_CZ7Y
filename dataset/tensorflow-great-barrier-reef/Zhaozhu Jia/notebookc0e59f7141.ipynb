{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!cp -r /kaggle/input/yolov5sjzz/models/ .\n!cp -r /kaggle/input/yolov5sjzz/utils/ .\n!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/arialttf-2/Arial.ttf /root/.config/Ultralytics/\n!ls\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-02T03:23:33.408958Z","iopub.execute_input":"2021-12-02T03:23:33.409532Z","iopub.status.idle":"2021-12-02T03:23:37.210808Z","shell.execute_reply.started":"2021-12-02T03:23:33.409495Z","shell.execute_reply":"2021-12-02T03:23:37.209839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport torch\nimport os\nimport time\nimport torchvision\nimport cv2\nfrom PIL import Image\n\nfrom utils.general import non_max_suppression,scale_coords\nfrom utils.augmentations import letterbox\nfrom models.experimental import attempt_load\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input/yolov5mmodel/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-02T03:23:39.403248Z","iopub.execute_input":"2021-12-02T03:23:39.404096Z","iopub.status.idle":"2021-12-02T03:23:40.148958Z","shell.execute_reply.started":"2021-12-02T03:23:39.404053Z","shell.execute_reply":"2021-12-02T03:23:40.148196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n@torch.no_grad()\ndef predict(model,\n        device,\n        im0s,  # file/dir/URL/glob, 0 for webcam\n        classes=None,  # filter by class: --class 0, or --class 0 2 3\n        agnostic_nms=False,  # class-agnostic NMS\n        augment=False,  # augmented inference\n        visualize=False,  # visualize features\n        half=False,  # use FP16 half-precision inference\n        ):\n    pt = True\n    stride = int(model.stride.max())  # model stride\n#     print(\"ssssss\",stride,pt)\n\n    # Dataloader\n    \n    # dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n    bs = 1  # batch_size\n    \n    # torch.save(model, 'yolov5s.pth')\n\n    # Run inference\n    if device.type != 'cpu':\n        model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(model.parameters())))  # run once\n\n    img = letterbox(im0s, imgsz, stride, pt)[0]\n    img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n    img = np.ascontiguousarray(img)\n\n\n\n    img = torch.from_numpy(img).to(device)\n    img = img.half() if half else img.float()  # uint8 to fp16/32\n    img = img / 255.0  # 0 - 255 to 0.0 - 1.0\n    if len(img.shape) == 3:\n        img = img[None]  # expand for batch dim\n\n    # Inference\n\n    visualize = False\n    pred = model(img, augment=augment, visualize=visualize)[0]\n\n    # NMS\n    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n    if len(pred)>0:\n        pred[0][:, :4] = scale_coords(img.shape[2:], pred[0][:, :4], im0s.shape).round()\n        return im0s,pred[0].cpu().numpy()\n    else:\n        return im0s,[]\n\n    \ndef format_prediction(bboxes):\n    annot = ''\n    if bboxes.shape[0]>0:\n        for idx in range(bboxes.shape[0]):\n            xmin, ymin, xmax, ymax = bboxes[idx,:4]\n            conf             = bboxes[idx,4]\n            w = int(xmax-xmin)\n            h = int(ymax-ymin)\n            xmin = int(xmin)\n            ymin = int(ymin)\n            \n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:17:37.472154Z","iopub.execute_input":"2021-12-01T07:17:37.473636Z","iopub.status.idle":"2021-12-01T07:17:37.487241Z","shell.execute_reply.started":"2021-12-01T07:17:37.473595Z","shell.execute_reply":"2021-12-01T07:17:37.486532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 1. ","metadata":{}},{"cell_type":"code","source":"\nCKPT_PATH = '/kaggle/input/yolov5mmodel/yolov5m.pt'\nimgsz=[768,768]  # inference size (pixels)\nconf_thres=0.15  # confidence threshold\niou_thres=0.45  # NMS IOU threshold\nmax_det=1000  # maximum detections per image\n\ndevice = torch.device('cuda:0')\n# model = torch.load(CKPT_PATH,map_location=device)\nsource = '/kaggle/input/yolov5sjzz/test_pic/100.jpg'\nimage = cv2.imread(source)\nimages,dets = predict(model,  # model.pt path(s)\n                  device,\n                 image,  # file/dir/URL/glob, 0 for webcam\n                classes=None,  # filter by class: --class 0, or --class 0 2 3\n                agnostic_nms=False,  # class-agnostic NMS\n                augment=False,  # augmented inference\n                visualize=False,  # visualize features\n                half=False)  # use FP16 half-precision inference)\n    \n#     if dets.shape[0]>0:\n#         for i in range(dets.shape[0]):\n#             cv2.rectangle(images,(int(dets[i,0]),int(dets[i,1])),(int(dets[i,2]),int(dets[i,3])),(255,0,0),2)\n#         cv2.imwrite(\"shjdcugs.jpg\",images)\n\n\nprint(dets.shape,dets)\nprint(format_prediction(dets))","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:17:37.48861Z","iopub.execute_input":"2021-12-01T07:17:37.488871Z","iopub.status.idle":"2021-12-01T07:17:47.147833Z","shell.execute_reply.started":"2021-12-01T07:17:37.488836Z","shell.execute_reply":"2021-12-01T07:17:47.147077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import greatbarrierreef\nimport tqdm\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:17:47.149768Z","iopub.execute_input":"2021-12-01T07:17:47.150499Z","iopub.status.idle":"2021-12-01T07:17:47.178653Z","shell.execute_reply.started":"2021-12-01T07:17:47.150443Z","shell.execute_reply":"2021-12-01T07:17:47.177951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport cv2\nfrom PIL import Image\n\nfor (pixel_array, sample_prediction_df) in iter_test:\n    img = cv2.cvtColor(pixel_array,cv2.COLOR_RGB2BGR)\n    images,dets = predict(model,  # model.pt path(s)\n                  device,\n                 img,  # file/dir/URL/glob, 0 for webcam\n                classes=None,  # filter by class: --class 0, or --class 0 2 3\n                agnostic_nms=False,  # class-agnostic NMS\n                augment=False,  # augmented inference\n                visualize=False,  # visualize features\n                half=False)  # use FP16 half-precision inference)\n    res_str = format_prediction(dets)\n    sample_prediction_df['annotations'] = res_str  # make your predictions here\n    print(sample_prediction_df)\n    env.predict(sample_prediction_df)   # register your predictions\n#     display(Image.fromarray(pixel_array))\n\n\n\n# for (pixel_array, sample_prediction_df) in iter_test:\n#     sample_prediction_df['annotations'] = '0.5 0 0 100 100'  # make your predictions here\n#     env.predict(sample_prediction_df)   # register your predictions\n# #     display(Image.fromarray(pixel_array))\n#     img = cv2.cvtColor(pixel_array,cv2.COLOR_RGB2BGR)\n#     print(img)\n\n\n    \n# sub_df = pd.read_csv('submission.csv')\n# sub_df.head()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:17:47.179795Z","iopub.execute_input":"2021-12-01T07:17:47.18047Z","iopub.status.idle":"2021-12-01T07:17:47.599387Z","shell.execute_reply.started":"2021-12-01T07:17:47.180429Z","shell.execute_reply":"2021-12-01T07:17:47.598679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -lh\n!cat submission.csv\n# !rm submission.csv","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:17:47.600803Z","iopub.execute_input":"2021-12-01T07:17:47.601213Z","iopub.status.idle":"2021-12-01T07:17:48.972644Z","shell.execute_reply.started":"2021-12-01T07:17:47.601176Z","shell.execute_reply":"2021-12-01T07:17:48.971792Z"},"trusted":true},"execution_count":null,"outputs":[]}]}