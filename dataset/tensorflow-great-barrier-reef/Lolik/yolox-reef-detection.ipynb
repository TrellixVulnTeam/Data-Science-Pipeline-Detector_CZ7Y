{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision.transforms import ToTensor, Normalize, Compose, Resize\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nORIGIN_W = 1280\nORIGIN_H = 720\nMODEL_W = 640\nMODEL_H = 640\n\nw_scale = ORIGIN_W / MODEL_W\nh_scale = ORIGIN_H / MODEL_H\n\nIOU_TH = 0.65\nCONFIDANCE_TH = 0.16\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntransform = Compose([\n        ToTensor(),\n        Resize((640, 640)),\n        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copyright (c) OpenMMLab. All rights reserved.\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.utils import _pair\nfrom torchvision.ops import batched_nms\n\nclass MlvlPointGenerator(nn.Module):\n    \"\"\"Standard points generator for multi-level (Mlvl) feature maps in 2D\n    points-based detectors.\n\n    Args:\n        strides (list[int] | list[tuple[int, int]]): Strides of anchors\n            in multiple feature levels in order (w, h).\n        offset (float): The offset of points, the value is normalized with\n            corresponding stride. Defaults to 0.5.\n\n    Generate grid points of multiple feature levels.\n\n        Args:\n            featmap_sizes (list[tuple]): List of feature map sizes in\n                multiple feature levels, each size arrange as\n                as (h, w).\n            dtype (:obj:`dtype`): Dtype of priors. Default: torch.float32.\n            device (str): The device where the anchors will be put on.\n            with_stride (bool): Whether to concatenate the stride to\n                the last dimension of points.\n\n        Return:\n            list[torch.Tensor]: Points of  multiple feature levels.\n            The sizes of each tensor should be (N, 2) when with stride is\n            ``False``, where N = width * height, width and height\n            are the sizes of the corresponding feature level,\n            and the last dimension 2 represent (coord_x, coord_y),\n            otherwise the shape should be (N, 4),\n            and the last dimension 4 represent\n            (coord_x, coord_y, stride_w, stride_h).\n        \n    \"\"\"\n\n    def __init__(self, strides, \n                    featmap_sizes,\n                    offset=0.5,\n                    dtype=torch.float16,\n                    with_stride=False):\n        super(MlvlPointGenerator, self).__init__()\n        self.strides = [_pair(stride) for stride in strides]\n        self.offset = offset\n        assert self.num_levels == len(featmap_sizes)\n        multi_level_priors = []\n        for i in range(self.num_levels):\n            priors = self.single_level_grid_priors(\n                featmap_sizes[i],\n                level_idx=i,\n                dtype=dtype,\n                with_stride=with_stride)\n            multi_level_priors.append(priors)\n\n        flatten_priors = torch.cat(multi_level_priors).float()\n        self.register_buffer(\"flatten_priors\", flatten_priors)\n        \n\n    @property\n    def num_levels(self):\n        \"\"\"int: number of feature levels that the generator will be applied\"\"\"\n        return len(self.strides)\n\n    @property\n    def num_base_priors(self):\n        \"\"\"list[int]: The number of priors (points) at a point\n        on the feature grid\"\"\"\n        return [1 for _ in range(len(self.strides))]\n\n    def _meshgrid(self, x, y, row_major=True):\n        yy, xx = torch.meshgrid(y, x)\n        if row_major:\n            # warning .flatten() would cause error in ONNX exporting\n            # have to use reshape here\n            return xx.reshape(-1), yy.reshape(-1)\n        else:\n            return yy.reshape(-1), xx.reshape(-1)\n\n    def single_level_grid_priors(self,\n                                 featmap_size,\n                                 level_idx,\n                                 dtype=torch.float16,\n                                 with_stride=False):\n        \"\"\"Generate grid Points of a single level.\n\n        Note:\n            This function is usually called by method ``self.grid_priors``.\n\n        Args:\n            featmap_size (tuple[int]): Size of the feature maps, arrange as\n                (h, w).\n            level_idx (int): The index of corresponding feature map level.\n            dtype (:obj:`dtype`): Dtype of priors. Default: torch.float32.\n            device (str, optional): The device the tensor will be put on.\n                Defaults to 'cuda'.\n            with_stride (bool): Concatenate the stride to the last dimension\n                of points.\n\n        Return:\n            Tensor: Points of single feature levels.\n            The shape of tensor should be (N, 2) when with stride is\n            ``False``, where N = width * height, width and height\n            are the sizes of the corresponding feature level,\n            and the last dimension 2 represent (coord_x, coord_y),\n            otherwise the shape should be (N, 4),\n            and the last dimension 4 represent\n            (coord_x, coord_y, stride_w, stride_h).\n        \"\"\"\n        feat_h, feat_w = featmap_size\n        stride_w, stride_h = self.strides[level_idx]\n        shift_x = (torch.arange(0, feat_w) + self.offset) * stride_w\n        # keep featmap_size as Tensor instead of int, so that we\n        # can convert to ONNX correctly\n        shift_x = shift_x.to(dtype)\n\n        shift_y = (torch.arange(0, feat_h) + self.offset) * stride_h\n        # keep featmap_size as Tensor instead of int, so that we\n        # can convert to ONNX correctly\n        shift_y = shift_y.to(dtype)\n        shift_xx, shift_yy = self._meshgrid(shift_x, shift_y)\n        if not with_stride:\n            shifts = torch.stack([shift_xx, shift_yy], dim=-1)\n        else:\n            # use `shape[0]` instead of `len(shift_xx)` for ONNX export\n            stride_w = shift_xx.new_full((shift_xx.shape[0], ),\n                                         stride_w).to(dtype)\n            stride_h = shift_xx.new_full((shift_yy.shape[0], ),\n                                         stride_h).to(dtype)\n            shifts = torch.stack([shift_xx, shift_yy, stride_w, stride_h],\n                                 dim=-1)\n        all_points = shifts\n        return all_points\n\n    def valid_flags(self, featmap_sizes, pad_shape):\n        \"\"\"Generate valid flags of points of multiple feature levels.\n\n        Args:\n            featmap_sizes (list(tuple)): List of feature map sizes in\n                multiple feature levels, each size arrange as\n                as (h, w).\n            pad_shape (tuple(int)): The padded shape of the image,\n                 arrange as (h, w).\n            device (str): The device where the anchors will be put on.\n\n        Return:\n            list(torch.Tensor): Valid flags of points of multiple levels.\n        \"\"\"\n        assert self.num_levels == len(featmap_sizes)\n        multi_level_flags = []\n        for i in range(self.num_levels):\n            point_stride = self.strides[i]\n            feat_h, feat_w = featmap_sizes[i]\n            h, w = pad_shape[:2]\n            valid_feat_h = min(int(np.ceil(h / point_stride[1])), feat_h)\n            valid_feat_w = min(int(np.ceil(w / point_stride[0])), feat_w)\n            flags = self.single_level_valid_flags((feat_h, feat_w),\n                                                  (valid_feat_h, valid_feat_w))\n            multi_level_flags.append(flags)\n        return multi_level_flags\n\n    def single_level_valid_flags(self,\n                                 featmap_size,\n                                 valid_size):\n        \"\"\"Generate the valid flags of points of a single feature map.\n\n        Args:\n            featmap_size (tuple[int]): The size of feature maps, arrange as\n                as (h, w).\n            valid_size (tuple[int]): The valid size of the feature maps.\n                The size arrange as as (h, w).\n            device (str, optional): The device where the flags will be put on.\n                Defaults to 'cuda'.\n\n        Returns:\n            torch.Tensor: The valid flags of each points in a single level \\\n                feature map.\n        \"\"\"\n        feat_h, feat_w = featmap_size\n        valid_h, valid_w = valid_size\n        assert valid_h <= feat_h and valid_w <= feat_w\n        valid_x = torch.zeros(feat_w, dtype=torch.bool)\n        valid_y = torch.zeros(feat_h, dtype=torch.bool)\n        valid_x[:valid_w] = 1\n        valid_y[:valid_h] = 1\n        valid_xx, valid_yy = self._meshgrid(valid_x, valid_y)\n        valid = valid_xx & valid_yy\n        return valid\n\n    def sparse_priors(self,\n                      prior_idxs,\n                      featmap_size,\n                      level_idx,\n                      dtype=torch.float16,\n                      ):\n        \"\"\"Generate sparse points according to the ``prior_idxs``.\n\n        Args:\n            prior_idxs (Tensor): The index of corresponding anchors\n                in the feature map.\n            featmap_size (tuple[int]): feature map size arrange as (w, h).\n            level_idx (int): The level index of corresponding feature\n                map.\n            dtype (obj:`torch.dtype`): Date type of points. Defaults to\n                ``torch.float32``.\n            device (obj:`torch.device`): The device where the points is\n                located.\n        Returns:\n            Tensor: Anchor with shape (N, 2), N should be equal to\n            the length of ``prior_idxs``. And last dimension\n            2 represent (coord_x, coord_y).\n        \"\"\"\n        height, width = featmap_size\n        x = (prior_idxs % width + self.offset) * self.strides[level_idx][0]\n        y = ((prior_idxs // width) % height +\n             self.offset) * self.strides[level_idx][1]\n        prioris = torch.stack([x, y], 1).to(dtype)\n        return prioris\n\nstrides=[8, 16, 32]\nfeatmap_sizes = [torch.Size([int(MODEL_H/stride), int(MODEL_W/stride)]) for stride in strides]\nprior_generator = MlvlPointGenerator(strides, featmap_sizes, offset=0, with_stride=True)\nflatten_priors = prior_generator.flatten_priors\nflatten_priors\n\nclass InferModule:\n    def __init__(self, model, flatten_priors=flatten_priors, iou_th=IOU_TH, conf_th=CONFIDANCE_TH):\n        self.model = model\n        self.flatten_priors=flatten_priors\n        self.iou_th = iou_th\n        self.conf_th = conf_th\n        \n    @staticmethod\n    def _bbox_decode(priors, bbox_preds):\n        xys = (bbox_preds[..., :2] * priors[:, 2:]) + priors[:, :2]\n        whs = bbox_preds[..., 2:].exp() * priors[:, 2:]\n\n        tl_x = (xys[..., 0] - whs[..., 0] / 2)\n        tl_y = (xys[..., 1] - whs[..., 1] / 2)\n        br_x = (xys[..., 0] + whs[..., 0] / 2)\n        br_y = (xys[..., 1] + whs[..., 1] / 2)\n        decoded_bboxes = torch.stack([tl_x, tl_y, br_x, br_y], -1)\n        return decoded_bboxes\n    \n    def _bboxes_nms(self, cls_scores, bboxes, score_factor, confidance_th = 0.25, iou_threshold = 0.65):\n\n        cls_scores = torch.cat([1 - cls_scores, cls_scores], -1)\n        max_scores, labels = torch.max(cls_scores, 1)\n        valid_mask = score_factor * max_scores >= confidance_th\n        \n        bboxes = bboxes[valid_mask].float()\n        scores = max_scores[valid_mask] * score_factor[valid_mask]\n        labels = labels[valid_mask]\n\n        if labels.numel() == 0:\n            return bboxes, labels\n        else:\n            indexes = batched_nms(bboxes.double(), scores.double(), labels, iou_threshold=self.iou_th)\n            return bboxes[indexes], scores[indexes]\n    \n    @staticmethod\n    def _get_flatten_output(\n                            cls_scores,\n                            bbox_preds,\n                            objectnesses,\n                            num_classes=1\n                            ):\n\n        num_imgs = bbox_preds[0].shape[0]\n\n        flatten_cls_preds = [\n            cls_pred.permute(0, 2, 3, 1).reshape(num_imgs, -1,\n                                                    num_classes)\n            for cls_pred in cls_scores\n        ]\n        flatten_bbox_preds = [\n            bbox_pred.permute(0, 2, 3, 1).reshape(num_imgs, -1, 4)\n            for bbox_pred in bbox_preds\n        ]\n        flatten_objectness = [\n            objectness.permute(0, 2, 3, 1).reshape(num_imgs, -1)\n            for objectness in objectnesses\n        ]\n\n        flatten_cls_preds = torch.cat(flatten_cls_preds, dim=1).double().detach().cpu()\n        flatten_bbox_preds = torch.cat(flatten_bbox_preds, dim=1).double().detach().cpu()\n        flatten_objectness = torch.cat(flatten_objectness, dim=1).double().detach().cpu()\n        flatten_bboxes = InferModule._bbox_decode(flatten_priors, flatten_bbox_preds).double().detach().cpu()\n\n        return flatten_cls_preds, flatten_objectness, flatten_bboxes, flatten_bbox_preds\n\n    def forward(self, x, confidance=0.25):\n        cls_score, bbox_pred, objectness = model(x)\n        flatten_cls_preds, flatten_objectness, flatten_bboxes, flatten_bbox_preds = InferModule._get_flatten_output(\n                                                                                            cls_scores=cls_score,\n                                                                                            bbox_preds=bbox_pred,\n                                                                                            objectnesses=objectness\n                                                                                            )\n        result_list = []\n        flatten_cls_preds = flatten_cls_preds.sigmoid()\n        flatten_objectness = flatten_objectness.sigmoid()\n\n        for img_id in range(len(flatten_cls_preds)):\n            _cls_scores = flatten_cls_preds[img_id].double().detach().cpu()\n            _score_factor = flatten_objectness[img_id]\n            _bboxes = flatten_bboxes[img_id]\n\n            result_list.append(self._bboxes_nms(cls_scores=_cls_scores, bboxes=_bboxes, score_factor=_score_factor, confidance_th=confidance))\n            \n        result_list = [list(elem) for elem in result_list]\n        #change tuple to list        \n        model_bboxes = []\n        for elem in result_list:\n            bboxes = elem[0]\n            scores = elem[1]\n            if len(bboxes) == 0:\n                continue\n            for index, box in enumerate(elem[0]):\n                if len(box) == 0:\n                    continue\n                x_min = float(box[0]*w_scale)\n                y_min = float(box[1]*h_scale)\n                w = float(box[2]*w_scale) - x_min\n                h = float(box[3]*h_scale) - y_min\n                model_bboxes.append([float(scores[index]), x_min, y_min, w, h])\n        return model_bboxes","metadata":{"execution":{"iopub.status.busy":"2022-01-26T16:24:27.144711Z","iopub.execute_input":"2022-01-26T16:24:27.144971Z","iopub.status.idle":"2022-01-26T16:24:27.242972Z","shell.execute_reply.started":"2022-01-26T16:24:27.144939Z","shell.execute_reply":"2022-01-26T16:24:27.242214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.jit.load('../input/d/lolik228/yolox-pt/yolox.pt')\ninfer_module = InferModule(model)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T16:24:34.190214Z","iopub.execute_input":"2022-01-26T16:24:34.1905Z","iopub.status.idle":"2022-01-26T16:24:37.493706Z","shell.execute_reply.started":"2022-01-26T16:24:34.19047Z","shell.execute_reply":"2022-01-26T16:24:37.492925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport cv2\nfrom tqdm import tqdm\nimport random\n\nnames = []\nfor i in range(3):\n    path = '../input/tensorflow-great-barrier-reef/train_images/video_' + str(i) + '/'\n    path_names = os.listdir(path)\n    for name in path_names:\n        names.append(path + name)\n        \nrandom.shuffle(names)\n\nbad_names = []\nname = np.random.choice(names)\nimage_np = cv2.imread(name)\nimage_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\ntensor = transform(image_np)\ntensor = torch.unsqueeze(tensor, 0).half().to(device)\ninfer_module.forward(tensor)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T16:24:37.495135Z","iopub.execute_input":"2022-01-26T16:24:37.495401Z","iopub.status.idle":"2022-01-26T16:24:42.975863Z","shell.execute_reply.started":"2022-01-26T16:24:37.495365Z","shell.execute_reply":"2022-01-26T16:24:42.975189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_predict(img_path, confidance_th):\n    image_np = cv2.imread(img_path)\n    image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n    tensor = transform(image_np)\n    tensor = torch.unsqueeze(tensor, 0).half().to(device)\n    predict = infer_module.forward(tensor, confidance_th)\n    return predict","metadata":{"execution":{"iopub.status.busy":"2022-01-26T16:24:45.472182Z","iopub.execute_input":"2022-01-26T16:24:45.472708Z","iopub.status.idle":"2022-01-26T16:24:45.477482Z","shell.execute_reply.started":"2022-01-26T16:24:45.472669Z","shell.execute_reply":"2022-01-26T16:24:45.476532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_iou(bboxes1, bboxes2, bbox_mode='xywh'):\n    assert len(bboxes1.shape) == 2 and bboxes1.shape[1] == 4\n    assert len(bboxes2.shape) == 2 and bboxes2.shape[1] == 4\n    \n    bboxes1 = bboxes1.copy()\n    bboxes2 = bboxes2.copy()\n    \n    if bbox_mode == 'xywh':\n        bboxes1[:, 2:] += bboxes1[:, :2]\n        bboxes2[:, 2:] += bboxes2[:, :2]\n\n    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\n    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\n    xA = np.maximum(x11, np.transpose(x21))\n    yA = np.maximum(y11, np.transpose(y21))\n    xB = np.minimum(x12, np.transpose(x22))\n    yB = np.minimum(y12, np.transpose(y22))\n    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n    iou = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n    return iou\n\ndef f_beta(tp, fp, fn, beta=2):\n    return (1+beta**2)*tp / ((1+beta**2)*tp + beta**2*fn+fp)\n\ndef calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th, verbose=False):\n    gt_bboxes = gt_bboxes.copy()\n    pred_bboxes = pred_bboxes.copy()\n    \n    tp = 0\n    fp = 0\n    for k, pred_bbox in enumerate(pred_bboxes): # fixed in ver.7\n        ious = calc_iou(gt_bboxes, pred_bbox[None, 1:])\n        max_iou = ious.max()\n        if max_iou > iou_th:\n            tp += 1\n            gt_bboxes = np.delete(gt_bboxes, ious.argmax(), axis=0)\n        else:\n            fp += 1\n        if len(gt_bboxes) == 0:\n            fp += len(pred_bboxes) - (k + 1) # fix in ver.7\n            break\n\n    fn = len(gt_bboxes)\n    return tp, fp, fn\n\ndef calc_is_correct(gt_bboxes, pred_bboxes):\n    \"\"\"\n    gt_bboxes: (N, 4) np.array in xywh format\n    pred_bboxes: (N, 5) np.array in conf+xywh format\n    \"\"\"\n    if len(gt_bboxes) == 0 and len(pred_bboxes) == 0:\n        tps, fps, fns = 0, 0, 0\n        return tps, fps, fns\n    \n    elif len(gt_bboxes) == 0:\n        tps, fps, fns = 0, len(pred_bboxes), 0\n        return tps, fps, fns\n    \n    elif len(pred_bboxes) == 0:\n        tps, fps, fns = 0, 0, len(gt_bboxes)\n        return tps, fps, fns\n    \n    pred_bboxes = pred_bboxes[pred_bboxes[:,0].argsort()[::-1]] # sort by conf\n    \n    tps, fps, fns = 0, 0, 0\n    for iou_th in np.arange(0.3, 0.85, 0.05):\n        tp, fp, fn = calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th)\n        tps += tp\n        fps += fp\n        fns += fn\n    return tps, fps, fns\n\ndef calc_f2_score(gt_bboxes_list, pred_bboxes_list, verbose=False):\n    \"\"\"\n    gt_bboxes_list: list of (N, 4) np.array in xywh format\n    pred_bboxes_list: list of (N, 5) np.array in conf+xywh format\n    \"\"\"\n    tps, fps, fns = 0, 0, 0\n    for gt_bboxes, pred_bboxes in zip(gt_bboxes_list, pred_bboxes_list):\n        tp, fp, fn = calc_is_correct(gt_bboxes, pred_bboxes)\n        tps += tp\n        fps += fp\n        fns += fn\n        if verbose:\n            num_gt = len(gt_bboxes)\n            num_pred = len(pred_bboxes)\n            print(f'num_gt:{num_gt:<3} num_pred:{num_pred:<3} tp:{tp:<3} fp:{fp:<3} fn:{fn:<3}')\n    return f_beta(tps, fps, fns, beta=2)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T16:24:47.328138Z","iopub.execute_input":"2022-01-26T16:24:47.328713Z","iopub.status.idle":"2022-01-26T16:24:47.350579Z","shell.execute_reply.started":"2022-01-26T16:24:47.328675Z","shell.execute_reply":"2022-01-26T16:24:47.349649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/d/lolik228/yolox-pt/valid_full_2_video.csv')\ndf['annotation'] = df.annotation.apply(eval)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T16:25:23.895824Z","iopub.execute_input":"2022-01-26T16:25:23.896335Z","iopub.status.idle":"2022-01-26T16:25:23.980799Z","shell.execute_reply.started":"2022-01-26T16:25:23.8963Z","shell.execute_reply":"2022-01-26T16:25:23.979976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path = '../input/tensorflow-great-barrier-reef/'\n# confidance_greed_search = []\n# for confidance in tqdm(np.arange(0.01, 0.85, 0.05)):\n#     predictions = []\n#     targets = []\n#     for i in range(len(df)):\n#         row = df.iloc[i]\n#         img_path = path + row['image_path']\n#         annot = row['annotation']\n#         target = []\n#         for an in annot:\n#             target.append([an['x_min'], an['y_min'], an['x_max'] - an['x_min'], an['y_max'] - an['y_min']])\n\n#         predictions.append(np.array(model_predict(img_path, confidance)))\n#         targets.append(np.array(target))\n\n#     confidance_greed_search.append(calc_f2_score(targets, predictions))\n    \n# confidance_greed_search","metadata":{"execution":{"iopub.status.busy":"2022-01-26T16:25:44.511129Z","iopub.execute_input":"2022-01-26T16:25:44.511389Z","iopub.status.idle":"2022-01-26T18:28:38.153287Z","shell.execute_reply.started":"2022-01-26T16:25:44.511359Z","shell.execute_reply":"2022-01-26T18:28:38.152604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for confidance in tqdm(np.arange(0.01, 0.85, 0.05)):\n#     print(confidance)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T18:30:49.925004Z","iopub.execute_input":"2022-01-26T18:30:49.925264Z","iopub.status.idle":"2022-01-26T18:30:49.936953Z","shell.execute_reply.started":"2022-01-26T18:30:49.925235Z","shell.execute_reply":"2022-01-26T18:30:49.936104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import os\n# import cv2\n# from tqdm import tqdm\n# import random\n\n\n# # df = pd.read_csv('../input/tensorflow-great-barrier-reef/train.csv')\n# names = []\n# for i in range(3):\n#     path = '../input/tensorflow-great-barrier-reef/train_images/video_' + str(i) + '/'\n#     path_names = os.listdir(path)\n#     for name in path_names:\n#         names.append(path + name)\n        \n# random.shuffle(names)\n\n# bad_names = []\n# for name in tqdm(names):\n#     image_np = cv2.imread(name)\n#     image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n#     tensor = transform(image_np)\n#     tensor = tensor.unsqueeze(0).half().to(device)\n    \n#     bboxes = inder_module.forward(tensor)\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T21:19:29.689632Z","iopub.execute_input":"2022-01-25T21:19:29.689896Z","iopub.status.idle":"2022-01-25T21:19:29.694289Z","shell.execute_reply.started":"2022-01-25T21:19:29.689864Z","shell.execute_reply":"2022-01-25T21:19:29.693337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import greatbarrierreef\n\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()  ","metadata":{"execution":{"iopub.status.busy":"2022-01-26T18:33:11.633543Z","iopub.execute_input":"2022-01-26T18:33:11.633871Z","iopub.status.idle":"2022-01-26T18:33:11.691345Z","shell.execute_reply.started":"2022-01-26T18:33:11.633835Z","shell.execute_reply":"2022-01-26T18:33:11.690634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\nconfthre = 0.01\nfor (image_np, sample_prediction_df) in iter_test:\n    tensor = transform(image_np)\n    tensor = tensor.unsqueeze(0).half().to(device)\n    \n    bboxes = infer_module.forward(tensor, 0.65)\n    predictions = []\n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        predictions.append('{:.2f} {} {} {} {}'.format(box[0], box[1], box[2], box[3], box[4]))\n        \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n    print('Prediction:', prediction_str)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T18:33:11.89259Z","iopub.execute_input":"2022-01-26T18:33:11.892843Z","iopub.status.idle":"2022-01-26T18:33:12.298856Z","shell.execute_reply.started":"2022-01-26T18:33:11.892814Z","shell.execute_reply":"2022-01-26T18:33:12.298178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T21:19:37.311665Z","iopub.execute_input":"2022-01-25T21:19:37.312223Z","iopub.status.idle":"2022-01-25T21:19:37.327512Z","shell.execute_reply.started":"2022-01-25T21:19:37.312184Z","shell.execute_reply":"2022-01-25T21:19:37.326769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}