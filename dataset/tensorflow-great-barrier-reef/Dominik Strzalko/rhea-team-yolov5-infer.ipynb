{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Tensorflow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef) [INFER]\n> Wykrywanie Koron cierniowych (rozgwiazd) na rafie koralowej\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/31703/logos/header.png?t=2021-10-29-00-30-04\">","metadata":{}},{"cell_type":"markdown","source":"# ðŸ›  Skopiowanie kodu niektÃ³rych sryptÃ³w z biblioteki bbox (problemy z importem biblioteki przy pomocy kaggle)","metadata":{}},{"cell_type":"code","source":"#I have a problem with importing the bbox-lib-ds via \"Input\". Code from: https://github.com/awsaf49/bbox/blob/main/bbox/utils.py\n\nimport cv2\nimport numpy as np\nimport random\n\ndef voc2yolo(bboxes, height=720, width=1280):\n    \"\"\"\n    voc  => [x1, y1, x2, y1]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n#     bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., 0::2] /= width\n    bboxes[..., 1::2] /= height\n    \n    bboxes[..., 2] -= bboxes[..., 0]\n    bboxes[..., 3] -= bboxes[..., 1]\n    \n    bboxes[..., 0] += bboxes[..., 2]/2\n    bboxes[..., 1] += bboxes[..., 3]/2\n    \n    return bboxes\n\ndef yolo2voc(bboxes, height=720, width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n    \n    \"\"\" \n#     bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., 0::2] *= width\n    bboxes[..., 1::2] *= height\n    \n    bboxes[..., 0:2] -= bboxes[..., 2:4]/2\n    bboxes[..., 2:4] += bboxes[..., 0:2]\n    \n    return bboxes\n\ndef coco2yolo(bboxes, height=720, width=1280):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n#     bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., 0::2] /= width\n    bboxes[..., 1::2] /= height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., 0:2] += bboxes[..., 2:4]/2\n    \n    return bboxes\n\ndef yolo2coco(bboxes, height=720, width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n#     bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., 0::2] *= width\n    bboxes[..., 1::2] *= height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., 0:2] -= bboxes[..., 2:4]/2\n    \n    return bboxes\n\ndef voc2coco(bboxes, height=720, width=1280):\n    \"\"\"\n    voc  => [xmin, ymin, xmax, ymax]\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n#     bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # converstion (xmax, ymax) => (w, h) \n    bboxes[..., 2:4] -= bboxes[..., 0:2]\n    \n    return bboxes\n\ndef coco2voc(bboxes, height=720, width=1280):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    voc  => [xmin, ymin, xmax, ymax]\n    \n    \"\"\" \n#     bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # converstion (w, h) => (w, h) \n    bboxes[..., 2:4] += bboxes[..., 0:2]\n    \n    return bboxes\n\ndef bbox_iou(b1, b2):\n    \"\"\"Calculate the Intersection of Unions (IoUs) between bounding boxes.\n    Args:\n        b1 (np.ndarray): An ndarray containing N(x4) bounding boxes of shape (N, 4) in [xmin, ymin, xmax, ymax] format.\n        b2 (np.ndarray): An ndarray containing M(x4) bounding boxes of shape (N, 4) in [xmin, ymin, xmax, ymax] format.\n    Returns:\n        np.ndarray: An ndarray containing the IoUs of shape (N, 1)\n    \"\"\"\n#     0 = np.convert_to_tensor(0.0, b1.dtype)\n    # b1 = b1.astype(np.float32)\n    # b2 = b2.astype(np.float32)\n    b1_xmin, b1_ymin, b1_xmax, b1_ymax = np.split(b1, 4, axis=-1)\n    b2_xmin, b2_ymin, b2_xmax, b2_ymax = np.split(b2, 4, axis=-1)\n    b1_height = np.maximum(0, b1_ymax - b1_ymin)\n    b1_width  = np.maximum(0, b1_xmax - b1_xmin)\n    b2_height = np.maximum(0, b2_ymax - b2_ymin)\n    b2_width  = np.maximum(0, b2_xmax - b2_xmin)\n    b1_area = b1_height * b1_width\n    b2_area = b2_height * b2_width\n\n    intersect_xmin = np.maximum(b1_xmin, b2_xmin)\n    intersect_ymin = np.maximum(b1_ymin, b2_ymin)\n    intersect_xmax = np.minimum(b1_xmax, b2_xmax)\n    intersect_ymax = np.minimum(b1_ymax, b2_ymax)\n    intersect_height = np.maximum(0, intersect_ymax - intersect_ymin)\n    intersect_width  = np.maximum(0, intersect_xmax - intersect_xmin)\n    intersect_area   = intersect_height * intersect_width\n\n    union_area = b1_area + b2_area - intersect_area\n    iou = np.nan_to_num(intersect_area/union_area).squeeze()\n    \n    return iou\n\ndef clip_bbox(bboxes_voc, height=720, width=1280):\n    \"\"\"Clip bounding boxes to image boundaries.\n    Args:\n        bboxes_voc (np.ndarray): bboxes in [xmin, ymin, xmax, ymax] format.\n        height (int, optional): height of bbox. Defaults to 720.\n        width (int, optional): width of bbox. Defaults to 1280.\n    Returns:\n        np.ndarray : clipped bboxes in [xmin, ymin, xmax, ymax] format.\n    \"\"\"\n    bboxes_voc[..., 0::2] = bboxes_voc[..., 0::2].clip(0, width)\n    bboxes_voc[..., 1::2] = bboxes_voc[..., 1::2].clip(0, height)\n    return bboxes_voc\n\ndef str2annot(data):\n    \"\"\"Generate annotation from string.\n    \n    Args:\n        data (str): string of annotation.\n    \n    Returns:\n        np.ndarray: annotation in array format.\n    \"\"\"\n    data  = data.replace('\\n', ' ')\n    data  = data.strip().split(' ')\n    data  = np.array(data)\n    annot = data.astype(float).reshape(-1, 5)\n    return annot\n\ndef annot2str(data):\n    \"\"\"Generate string from annotation.\n    \n    Args:\n        data (np.ndarray): annotation in array format.\n    \n    Returns:\n        str: annotation in string format.\n    \"\"\"\n    data   = data.astype(str)\n    string = '\\n'.join([' '.join(annot) for annot in data])\n    return string\n\ndef load_image(image_path):\n    return cv2.imread(image_path)[..., ::-1]\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n                h  = round(float(bbox[3])*image.shape[0]/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:24:45.041126Z","iopub.execute_input":"2022-02-12T08:24:45.041566Z","iopub.status.idle":"2022-02-12T08:24:45.539036Z","shell.execute_reply.started":"2022-02-12T08:24:45.041417Z","shell.execute_reply":"2022-02-12T08:24:45.537916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“š Importowanie bibliotek","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:24:45.540967Z","iopub.execute_input":"2022-02-12T08:24:45.541862Z","iopub.status.idle":"2022-02-12T08:24:47.249901Z","shell.execute_reply.started":"2022-02-12T08:24:45.541816Z","shell.execute_reply":"2022-02-12T08:24:47.248864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“– Meta Data","metadata":{}},{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\n# CKPT_DIR  = '/kaggle/input/greatbarrierreef-yolov5-train-ds'\nCKPT_PATH = '/kaggle/input/rhea-team-yolov5-trening/yolov5/great-barrier-reef-public/yolov5s6-dim4000-fold1-bat2-optAdam-epch5/weights/best.pt'\nIMG_SIZE  = 4000\nCONF      = 0.20\nIOU       = 0.40\nAUGMENT   = True","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:24:47.254004Z","iopub.execute_input":"2022-02-12T08:24:47.254275Z","iopub.status.idle":"2022-02-12T08:24:47.261665Z","shell.execute_reply.started":"2022-02-12T08:24:47.254242Z","shell.execute_reply":"2022-02-12T08:24:47.260492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import zbioru treningowego","metadata":{}},{"cell_type":"code","source":"# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf['image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\ndf['annotations'] = df['annotations'].progress_apply(eval)\ndisplay(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:24:47.264508Z","iopub.execute_input":"2022-02-12T08:24:47.264834Z","iopub.status.idle":"2022-02-12T08:24:47.792939Z","shell.execute_reply.started":"2022-02-12T08:24:47.264789Z","shell.execute_reply":"2022-02-12T08:24:47.791762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts()/len(df)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:24:47.794902Z","iopub.execute_input":"2022-02-12T08:24:47.795244Z","iopub.status.idle":"2022-02-12T08:24:47.917554Z","shell.execute_reply.started":"2022-02-12T08:24:47.795186Z","shell.execute_reply":"2022-02-12T08:24:47.916487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Funkcje Pomocnicze","metadata":{}},{"cell_type":"code","source":"\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:24:47.919259Z","iopub.execute_input":"2022-02-12T08:24:47.919776Z","iopub.status.idle":"2022-02-12T08:24:47.928784Z","shell.execute_reply.started":"2022-02-12T08:24:47.919731Z","shell.execute_reply":"2022-02-12T08:24:47.927592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:24:47.930397Z","iopub.execute_input":"2022-02-12T08:24:47.930942Z","iopub.status.idle":"2022-02-12T08:24:49.602599Z","shell.execute_reply.started":"2022-02-12T08:24:47.93089Z","shell.execute_reply":"2022-02-12T08:24:49.60122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ZaÅ‚adowanie modelu yolov5","metadata":{}},{"cell_type":"code","source":"def load_model(ckpt_path, conf=0.25, iou=0.50):\n    model = torch.hub.load('/kaggle/input/yolov5-lib-ds',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 1000  # maximum number of detections per image\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:24:49.605066Z","iopub.execute_input":"2022-02-12T08:24:49.605689Z","iopub.status.idle":"2022-02-12T08:24:49.615408Z","shell.execute_reply.started":"2022-02-12T08:24:49.605614Z","shell.execute_reply":"2022-02-12T08:24:49.614362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Funkcje pomocnicze","metadata":{}},{"cell_type":"code","source":"def predict(model, img, size=768, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:24:49.61764Z","iopub.execute_input":"2022-02-12T08:24:49.618272Z","iopub.status.idle":"2022-02-12T08:24:49.637987Z","shell.execute_reply.started":"2022-02-12T08:24:49.618217Z","shell.execute_reply":"2022-02-12T08:24:49.636923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inicjalizacja modelu z pobranymi wagami ze notatnika treningowgo (na zbiorze treningowym)","metadata":{}},{"cell_type":"code","source":"model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\nimage_paths = df[df.num_bbox>1].sample(100).image_path.tolist()\nfor idx, path in enumerate(image_paths):\n    img = cv2.imread(path)[...,::-1]\n    bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    display(show_img(img, bboxes, bbox_format='coco'))\n    if idx>5:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:24:49.646331Z","iopub.execute_input":"2022-02-12T08:24:49.646994Z","iopub.status.idle":"2022-02-12T08:25:03.875787Z","shell.execute_reply.started":"2022-02-12T08:24:49.646941Z","shell.execute_reply":"2022-02-12T08:25:03.874504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inicjalizacja modelu z pobranymi wagami ze notatnika treningowgo (na zbiorze testowym)","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:25:03.877803Z","iopub.execute_input":"2022-02-12T08:25:03.878505Z","iopub.status.idle":"2022-02-12T08:25:03.905228Z","shell.execute_reply.started":"2022-02-12T08:25:03.878457Z","shell.execute_reply":"2022-02-12T08:25:03.904037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\nfor idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    annot          = format_prediction(bboxes, confs)\n    pred_df['annotations'] = annot\n    env.predict(pred_df)\n    if idx<3:\n        display(show_img(img, bboxes, bbox_format='coco'))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:25:03.906874Z","iopub.execute_input":"2022-02-12T08:25:03.907312Z","iopub.status.idle":"2022-02-12T08:25:05.306331Z","shell.execute_reply.started":"2022-02-12T08:25:03.907262Z","shell.execute_reply":"2022-02-12T08:25:05.305275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Zapisanie wynikÃ³w do pliku .csv","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T08:25:05.308744Z","iopub.execute_input":"2022-02-12T08:25:05.309337Z","iopub.status.idle":"2022-02-12T08:25:05.326516Z","shell.execute_reply.started":"2022-02-12T08:25:05.30929Z","shell.execute_reply":"2022-02-12T08:25:05.325451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}