{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Feature extraction and XGBoost\n\nCompetition: https://www.kaggle.com/c/plant-pathology-2020-fgvc7/\n\nThe sensible approach here seems to be using transfer learning and fine-tuning to adapt a pretrained CNN to the task. \n\nInstead, I'll see how well I can do without training a neural network and maybe learn something in the process.\n\n### This notebook in a nutshell:\n- Taking a look at the dataset\n- Using pretrained neural network for feature extraction\n- Training and predicting with decision-tree based classifier (e.g. XGBoost)\n- Hyperparameter tuning with a grid search\n- Separate classifiers for rust/scab classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q efficientnet\nimport efficientnet.tfkeras as efn\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom scipy.special import softmax\n\nIMAGE_SIZE = [256, 256]\nDO_PARAMETER_TUNING = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data prep & inspection"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/plant-pathology-2020-fgvc7/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/plant-pathology-2020-fgvc7/test.csv\")\nimage_dir = \"/kaggle/input/plant-pathology-2020-fgvc7/images/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = train_df.columns\ntarget_columns = train_df.columns.drop(\"image_id\")\ntrain_target = train_df[target_columns]\ndisplay(train_target.head())\n\nclass_names = list(target_columns.values)\ntrain_labels = train_target.idxmax(axis=1)\nsns.countplot(train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting binary columns to labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files_labels = pd.concat([train_df[\"image_id\"], train_labels], axis=1)\ntrain_files_labels.columns = ['file', 'label']\ntrain_files_labels['file'] = train_files_labels['file'].apply(lambda x : x+\".jpg\")\n\ndisplay(train_files_labels.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking the training data\n\nReading jpgs with ImageDataGenerator and flow_from_dataframe.\n\nDisplay training samples and labels to check that we are reading them correctly."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ident_gen = ImageDataGenerator(rescale=1./255)\n\ntrain_gen = ident_gen.flow_from_dataframe(\n    train_files_labels, \n    directory=image_dir, \n    x_col='file', \n    y_col='label', \n    target_size=IMAGE_SIZE, \n    classes=class_names,\n    class_mode='categorical', \n    batch_size=25, \n    shuffle=True)\n\ndef show_batch(image_batch, label_batch, true_label_batch=[]):\n    plt.figure(figsize=(15,15))\n    for n in range(min(25, len(image_batch))):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        title = class_names[label_batch[n].argmax()]\n        if len(true_label_batch):\n            title += f\" ({class_names[true_label_batch[n].argmax()]})\"\n        plt.title(title)\n        plt.axis('off')\n\nimage_batch, label_batch = next(train_gen)\nshow_batch(image_batch, label_batch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading a pretrained CNN for feature extraction\n\nUsing an EfficientNet pretrained on the imagenet dataset. We drop the prediction layer and add global average pooling to get a model which converts the input image into a vector of feature activations."},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_extractor_model():\n    pretrained_model = efn.EfficientNetB5(\n        input_shape=[*IMAGE_SIZE, 3],\n        weights='imagenet',\n        include_top=False)\n    pretrained_model.trainable = False\n    return tf.keras.Sequential([\n        pretrained_model,\n        tf.keras.layers.GlobalAveragePooling2D()\n    ])\n\nfeature_model = feature_extractor_model()\nfeature_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the training data\n\nThe XGBoost model doesn't support training via a generator, so we use flow_from_dataframe to create a single batch containing all training images."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Load all the images in a single batch\ntrain_batch_gen = ident_gen.flow_from_dataframe(\n    train_files_labels, \n    directory=image_dir, \n    x_col='file', \n    y_col='label', \n    target_size=IMAGE_SIZE, \n    color_mode='rgb', \n    classes=class_names,\n    class_mode='categorical', \n    batch_size=len(train_files_labels), \n    shuffle=True)\n\ntrain_X_all, train_y_all = next(train_batch_gen)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the training data\n\nThis converts class information from one-hot columns to integer labels and uses the feature extraction model to convert input images to feature vectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_y_all = train_y_all.argmax(axis=-1) # onehot -> labels\ntrain_X, valid_X, train_y, valid_y = train_test_split(train_X_all, train_y_all, test_size=0.1, random_state=99)\n\ntrain_X_feat = feature_model.predict(train_X)\nvalid_X_feat = feature_model.predict(valid_X)\n\nprint(train_X_feat.shape)\nprint(train_y.shape)\n\nprint(train_X.shape)\nprint(train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, training. \n\nBeing careful to use the proper objective and number of classes with XGBRegressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nxgb_model = XGBRegressor(objective='multi:softmax', \n                         num_class=4)\nxgb_model.fit(train_X_feat, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the model\n\nFirst, some helper functions for scoring and dealing with categorical predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, accuracy_score\n\ndef to_onehot(labels, n_classes=4):\n    m = np.zeros(shape=(labels.size, n_classes))\n    m[np.arange(labels.size), labels.astype('int')] = 1\n    return m\n\ndef get_acc_scores(y_true_labels, y_pred_labels):\n    y_true = to_onehot(y_true_labels)\n    y_pred = to_onehot(y_pred_labels)\n    return {\n        cat : accuracy_score(y_true.T[i], y_pred.T[i]) for \n        (i, cat) in enumerate(class_names)\n    }\n\ndef get_auc_scores(y_true_labels, y_pred_labels):\n    y_true = to_onehot(y_true_labels)\n    y_pred = to_onehot(y_pred_labels)\n    return {\n        cat : roc_auc_score(y_true.T[i], y_pred.T[i]) for \n        (i, cat) in enumerate(class_names)\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluating the model on the training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = xgb_model.predict(train_X_feat)\n\nprint(\"Accuracy on training set:\", get_acc_scores(train_y, pred_y))\nprint(\"ROC AUC on training set:\", get_auc_scores(train_y, pred_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluating the model on the validation set:\n\nAs per the evaluation page, \"Submissions are evaluated on mean column-wise ROC AUC.\" so we take the average of each predicted column's AUC score."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = xgb_model.predict(valid_X_feat)\n\nprint(valid_y.shape)\nprint(train_y.shape)\nprint(pred_y[:3])\n\nval_auc_scores = get_auc_scores(valid_y, pred_y)\nprint(\"Accuracy on validation set:\", get_acc_scores(valid_y, pred_y))\nprint(\"ROC AUC on validation set:\", val_auc_scores)\n\nprint(\"\\nMean:\",np.mean(list(val_auc_scores.values())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Viewing the mistakes"},{"metadata":{"trusted":true},"cell_type":"code","source":"correct_preds = valid_y == pred_y\nbad_preds = valid_y != pred_y\n\nprint(\"#Mistakes\", sum(bad_preds))\n\nmistakes_X = valid_X[bad_preds]\nmistakes_y = pred_y[bad_preds]\nmistakes_y_true = valid_y[bad_preds]\n\nprint(mistakes_X.shape)\n\nshow_batch(mistakes_X, to_onehot(mistakes_y), to_onehot(mistakes_y_true))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# see: https://www.kaggle.com/agungor2/various-confusion-matrix-plots\nfrom sklearn.metrics import confusion_matrix\n\ndata = confusion_matrix(pred_y, valid_y)\n\ndf_cm = pd.DataFrame(data, columns = class_names, index = class_names)\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\n\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16})# font size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mistake counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,2, figsize=(12,6))\n\nmistake_pred_data = {\n    'healthy': mistakes_y == 0,\n    'multiple_diseases': mistakes_y == 1,\n    'rust': mistakes_y == 2,\n    'scab': mistakes_y == 3}\nmistakes_pred_df = pd.DataFrame(data=mistake_pred_data, dtype='int')\nax = sns.countplot(mistakes_pred_df.idxmax(axis=1), ax=axes[0])\nax.set_title(\"False positives\")\n\nmistake_data_true = {\n    'healthy': mistakes_y_true == 0,\n    'multiple_diseases': mistakes_y_true == 1,\n    'rust': mistakes_y_true == 2,\n    'scab': mistakes_y_true == 3}\nmistakes_true_df = pd.DataFrame(data=mistake_data_true, dtype='int')\nax = sns.countplot(mistakes_true_df.idxmax(axis=1), ax=axes[1])\nax.set_title(\"False negatives\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Parameter tuning\n\nTrying out the sklearn library grid search implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef get_mean_auc(estimator, X, y_true_labels):\n    y_pred_labels = estimator.predict(X)\n    y_true = to_onehot(y_true_labels)\n    y_pred = to_onehot(y_pred_labels)\n    return np.mean([roc_auc_score(y_true.T[i], y_pred.T[i]) for i in range(len(class_names))])\n\nif DO_PARAMETER_TUNING:\n    parameters = {\n        \"n_estimators\": [5,25,50,75],\n        \"max_depth\": [2,4,6],\n        \"learning_rate\":  [0.2, 0.3, 0.4]\n    }\n\n    xgb = XGBRegressor(objective='multi:softmax', num_class=4, threads=1)\n    search = GridSearchCV(xgb, parameters, scoring=get_mean_auc, n_jobs=-1, cv=3, verbose=3)\n    search.fit(train_X_feat, train_y)\n    tuned_params = search.best_params_\nelse:\n    tuned_params = {'learning_rate': 0.4, 'max_depth': 2, 'n_estimators': 75}\n\nprint(tuned_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multiple classifiers\n\nWhat if instead of predicting over the four classes, we train two models: one to predict 'rust' and another to predict 'scab'?\n\nThe 'healthy' and 'multiple_diseases' classes could then be inferred from the results of the two classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nxgb_model_rust = XGBRegressor(n_estimators=tuned_params['n_estimators'],\n                              max_depth=tuned_params['max_depth'],\n                              learning_rate=tuned_params['learning_rate'],\n                              objective=\"binary:logistic\")\nxgb_model_scab = XGBRegressor(n_estimators=tuned_params['n_estimators'],\n                              max_depth=tuned_params['max_depth'],\n                              learning_rate=tuned_params['learning_rate'],\n                              objective=\"binary:logistic\")\n\n# need to split our data into two sets:\n# - one with combined 'rust' and 'multiple_diseases' as the prediction target\n# - another with 'scab' and 'multiple_diseases'\n\nhealthy_idx = train_y == class_names.index(\"healthy\")\nmultiple_idx = train_y == class_names.index(\"multiple_diseases\")\nrust_idx = train_y == class_names.index(\"rust\")\nscab_idx = train_y == class_names.index(\"scab\")\n\nhas_rust_idx = rust_idx + multiple_idx\nhas_scab_idx = scab_idx + multiple_idx\n\nxgb_model_rust.fit(train_X_feat, has_rust_idx)\nxgb_model_scab.fit(train_X_feat, has_scab_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rust_pred_y = xgb_model_rust.predict(valid_X_feat)\nscab_pred_y = xgb_model_scab.predict(valid_X_feat)\n\nval_multiple_idx = valid_y == class_names.index(\"multiple_diseases\")\nval_healthy_idx = valid_y == class_names.index(\"healthy\")\nval_rust_idx = valid_y == class_names.index(\"rust\")\nval_scab_idx = valid_y == class_names.index(\"scab\")\n\nrust_auc = roc_auc_score(val_rust_idx, rust_pred_y)\nscab_auc = roc_auc_score(val_scab_idx, scab_pred_y)\nmult_auc = roc_auc_score(val_multiple_idx, np.minimum(scab_pred_y, rust_pred_y))\nheal_auc = roc_auc_score(val_healthy_idx, 1 - np.maximum(scab_pred_y, rust_pred_y))\n\nprint(\"Rust AUC\", rust_auc)\nprint(\"Scab AUC\", scab_auc)\nprint(\"Multiple AUC\", mult_auc)\nprint(\"Healthy AUC\", heal_auc)\n\nprint(\"\\nMean AUC\", (rust_auc+scab_auc+mult_auc+heal_auc)/4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, but making the predictions was a hassle. Lets wrap it in a function..."},{"metadata":{"trusted":true},"cell_type":"code","source":"def double_model_predict(X):\n    rust_pred_y = xgb_model_rust.predict(X)\n    scab_pred_y = xgb_model_scab.predict(X)\n    \n    multiple_pred_y = np.minimum(scab_pred_y, rust_pred_y)\n    healthy_pred_y = 1 - np.maximum(scab_pred_y, rust_pred_y)\n    \n    return np.stack((healthy_pred_y,multiple_pred_y,rust_pred_y,scab_pred_y)).T\n    \nprint(double_model_predict(valid_X_feat)[:3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_files = pd.concat([test_df[\"image_id\"], \n                        test_df[\"image_id\"].apply(lambda x : x+\".jpg\")],\n                        axis=1)\ntest_files.columns = [\"image_id\", \"file\"]\n\ntest_batch_gen = ident_gen.flow_from_dataframe(\n    test_files, \n    directory=image_dir, \n    x_col='file', \n    target_size=IMAGE_SIZE, \n    color_mode='rgb',\n    class_mode=None,\n    batch_size=len(test_files),\n    shuffle=False)\n\ntest_X_img = next(test_batch_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_X_feat = feature_model.predict(test_X_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_y = double_model_predict(test_X_feat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visual check of predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_batch(test_X_img, to_onehot(np.argmax(test_y, axis=-1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saving the predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_y.shape)\n#test_y = softmax(test_y, axis=1)\nids = test_files['image_id'].to_numpy()\nprint(ids.shape)\n\nnp.savetxt('submission.csv', \n           np.rec.fromarrays([ids] + [test_y[:,i] for i in range(4)]), \n           fmt=['%s', '%.2f', '%.2f', '%.2f', '%.2f'], \n           delimiter=',', \n           header='image_id,healthy,multiple_diseases,rust,scab', \n           comments='')\n\n!head submission.csv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}