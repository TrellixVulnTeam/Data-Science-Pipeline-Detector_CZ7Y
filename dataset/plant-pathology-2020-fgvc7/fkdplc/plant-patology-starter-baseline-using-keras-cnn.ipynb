{"cells":[{"metadata":{},"cell_type":"markdown","source":"in this notebook you can see a **starter model** for this competition"},{"metadata":{},"cell_type":"markdown","source":"first of all let's read train and test csv"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nPATH = '/kaggle/input/plant-pathology-2020-fgvc7/'\n\ntrain = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'test.csv')\n\ntarget = train[['healthy', 'multiple_diseases', 'rust', 'scab']]\ntest_ids = test['image_id']\n\ntrain_len = train.shape[0]\ntest_len = test.shape[0]\n\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looking at mean, we can see that classes are imbalanced for all 4 target features so i will use oversampling later"},{"metadata":{},"cell_type":"markdown","source":"this code performs reading images and resizing them into 224x224"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom tqdm.notebook import tqdm\n\nSIZE = 224\n\ntrain_images = np.empty((train_len, SIZE, SIZE, 3))\nfor i in tqdm(range(train_len)):\n    train_images[i] = np.uint8(Image.open(PATH + f'images/Train_{i}.jpg').resize((SIZE, SIZE)))\n    \ntest_images = np.empty((test_len, SIZE, SIZE, 3))\nfor i in tqdm(range(test_len)):\n    test_images[i] = np.uint8(Image.open(PATH + f'images/Test_{i}.jpg').resize((SIZE, SIZE)))\n\ntrain_images.shape, test_images.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here goes the separation train images into train and validation datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_images, target.to_numpy(), test_size=0.2, random_state=289) \n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"i use imblearn to perform oversampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=289)\n\nx_train, y_train = ros.fit_resample(x_train.reshape((-1, SIZE * SIZE * 3)), y_train)\nx_train = x_train.reshape((-1, SIZE, SIZE, 3))\nx_train.shape, y_train.sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it would be better to release the data that won't be used no more"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ndel train_images\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this is a simple convolutional neural network to solve this task. i use three callbacks:\n* learning rate reducing by 0.1 every 10 epochs on plateau\n* early stopping to stop learning after 24 epochs on plateau (with restoring best model)\n* model checkpoint to save best model to file\n\ni also add a l2 regularization to decrease an impact of overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, Sequential, load_model, Input\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, LeakyReLU\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.utils import plot_model\nfrom keras.regularizers import l2\n\nrlr = ReduceLROnPlateau(patience=15, verbose=1)\nes = EarlyStopping(patience=35, restore_best_weights=True, verbose=1)\nmc = ModelCheckpoint('model.hdf5', save_best_only=True, verbose=0)\n\nfilters = 32\nreg = .0005\n\nmodel = Sequential()\n\nfor i in range(5):\n    model.add(Conv2D(filters, 3, kernel_regularizer=l2(reg), input_shape=(SIZE, SIZE, 3)))\n    model.add(LeakyReLU())\n    \n    model.add(Conv2D(filters, 3, kernel_regularizer=l2(reg)))\n    model.add(LeakyReLU())\n    \n    if i != 4:\n        model.add(Conv2D(filters, 5, kernel_regularizer=l2(reg)))\n        model.add(LeakyReLU())\n        \n    model.add(MaxPooling2D())\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n\n    filters *= 2\n\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(4, activation='softmax'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"preparing data for training and training! data augmentation helps to enlarge image set by flipping, moving, zomming etc. \ni will track model's accuracy because i applied imblearn's balancing earlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['acc']\n)\n\nimagegen = ImageDataGenerator(\n    rotation_range=20,\n    zoom_range=0.2,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True\n)\n\nhistory = model.fit_generator(\n    imagegen.flow(x_train, y_train, batch_size=32),\n    epochs=400,\n    steps_per_epoch=x_train.shape[0] // 32,\n    verbose=0,\n    callbacks=[rlr, es, mc],\n    validation_data=(x_test, y_test)\n)\n# load best model\nmodel = load_model('model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here is a history: losses and accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nh = history.history\n\noffset = 5\nepochs = range(offset, len(h['loss']))\n\nplt.figure(1, figsize=(20, 6))\n\nplt.subplot(121)\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.plot(epochs, h['loss'][offset:], label='train')\nplt.plot(epochs, h['val_loss'][offset:], label='val')\nplt.legend()\n\nplt.subplot(122)\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.plot(h[f'acc'], label='train')\nplt.plot(h[f'val_acc'], label='val')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the competition's eval metric is mean of column-wise roc auc so let's check it here"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\npred_test = model.predict(x_test)\nroc_sum = 0\nfor i in range(4):\n    score = roc_auc_score(y_test[:, i], pred_test[:, i])\n    roc_sum += score\n    print(f'{score:.3f}')\n\nroc_sum /= 4\nprint(f'totally:{roc_sum:.3f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"finally, predict and save!"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(test_images)\n\nres = pd.DataFrame()\nres['image_id'] = test_ids\nres['healthy'] = pred[:, 0]\nres['multiple_diseases'] = pred[:, 1]\nres['rust'] = pred[:, 2]\nres['scab'] = pred[:, 3]\nres.to_csv('submission.csv', index=False)\nres.head(40)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}