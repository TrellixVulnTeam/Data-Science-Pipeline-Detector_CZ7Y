{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nPATH = '/kaggle/input/plant-pathology-2020-fgvc7/'\n\ntrain = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'test.csv')\n\ntarget = train[['healthy', 'multiple_diseases', 'rust', 'scab']]\ntest_ids = test['image_id']\n\ntrain_len = train.shape[0]\ntest_len = test.shape[0]\n\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom tqdm.notebook import tqdm\n\nSIZE = 224\n\ntrain_images = np.empty((train_len, SIZE, SIZE, 3))\nfor i in tqdm(range(train_len)):\n    train_images[i] = np.uint8(Image.open(PATH + f'images/Train_{i}.jpg').resize((SIZE, SIZE)))\n    \ntest_images = np.empty((test_len, SIZE, SIZE, 3))\nfor i in tqdm(range(test_len)):\n    test_images[i] = np.uint8(Image.open(PATH + f'images/Test_{i}.jpg').resize((SIZE, SIZE)))\n\ntrain_images.shape, test_images.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_images, target.to_numpy(), test_size=0.2, random_state=289) \n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=289)\n\nx_train, y_train = ros.fit_resample(x_train.reshape((-1, SIZE * SIZE * 3)), y_train)\nx_train = x_train.reshape((-1, SIZE, SIZE, 3))\nx_train.shape, y_train.sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ndel train_images\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import Callback\nfrom keras import backend\nfrom keras.models import load_model\nimport math\n\n# this callback applies cosine annealing, saves snapshots and allows to load them\nclass SnapshotEnsemble(Callback):\n    \n    __snapshot_name_fmt = \"snapshot_%d.hdf5\"\n    \n    def __init__(self, n_models, n_epochs_per_model, lr_max, verbose=1):\n        \"\"\"\n        n_models -- quantity of models (snapshots)\n        n_epochs_per_model -- quantity of epoch for every model (snapshot)\n        lr_max -- maximum learning rate (snapshot starter)\n        \"\"\"\n        self.n_epochs_per_model = n_epochs_per_model\n        self.n_models = n_models\n        self.n_epochs_total = self.n_models * self.n_epochs_per_model\n        self.lr_max = lr_max\n        self.verbose = verbose\n        self.lrs = []\n \n    # calculate learning rate for epoch\n    def cosine_annealing(self, epoch):\n        cos_inner = (math.pi * (epoch % self.n_epochs_per_model)) / self.n_epochs_per_model\n        return self.lr_max / 2 * (math.cos(cos_inner) + 1)\n\n    # when epoch begins update learning rate\n    def on_epoch_begin(self, epoch, logs={}):\n        # update learning rate\n        lr = self.cosine_annealing(epoch)\n        backend.set_value(self.model.optimizer.lr, lr)\n        # log value\n        self.lrs.append(lr)\n\n    # when epoch ends check if there is a need to save a snapshot\n    def on_epoch_end(self, epoch, logs={}):\n        if (epoch + 1) % self.n_epochs_per_model == 0:\n            # save model to file\n            filename = self.__snapshot_name_fmt % ((epoch + 1) // self.n_epochs_per_model)\n            self.model.save(filename)\n            if self.verbose:\n                print('Epoch %d: snapshot saved to %s' % (epoch, filename))\n                \n    # load all snapshots after training\n    def load_ensemble(self):\n        models = []\n        for i in range(self.n_models):\n            models.append(load_model(self.__snapshot_name_fmt % (i + 1)))\n        return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, Sequential, load_model, Input\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, LeakyReLU\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.utils import plot_model\nfrom keras.regularizers import l2\n\nfilters = 32\nreg = .0005\n\nmodel = Sequential()\n\nfor i in range(5):\n    model.add(Conv2D(filters, 3, kernel_regularizer=l2(reg), input_shape=(SIZE, SIZE, 3)))\n    model.add(LeakyReLU())\n    \n    model.add(Conv2D(filters, 3, kernel_regularizer=l2(reg)))\n    model.add(LeakyReLU())\n    \n    if i != 4:\n        model.add(Conv2D(filters, 5, kernel_regularizer=l2(reg)))\n        model.add(LeakyReLU())\n        \n    model.add(MaxPooling2D())\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n\n    filters *= 2\n\nmodel.add(Flatten())\nmodel.add(Dense(4, activation='softmax'))\n\nmodel.summary()\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['acc']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\nimagegen = ImageDataGenerator(\n    rotation_range=20,\n    zoom_range=0.2,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True\n)\n\nse_callback = SnapshotEnsemble(n_models=3, n_epochs_per_model=300, lr_max=.005)\n\nhistory = model.fit_generator(\n    imagegen.flow(x_train, y_train, batch_size=32),\n    epochs=se_callback.n_epochs_total,\n    steps_per_epoch=x_train.shape[0] // 32,\n    verbose=1,\n    callbacks=[se_callback],\n    validation_data=(x_test, y_test)\n)\n\n# load list of snapshots\nmodels = se_callback.load_ensemble()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nh = history.history\nplt.figure(1, figsize=(16, 10))\n\nplt.subplot(121)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.plot(h['loss'], label='training')\nplt.plot(h['val_loss'], label='validation')\nplt.legend()\n\nplt.subplot(122)\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.plot(h['acc'], label='training')\nplt.plot(h['val_acc'], label='validation')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\n# makes prediction according to given models and given weights\ndef predict(models, data, weights=None):\n    if weights is None:\n        # default weights provide voting equality\n        weights = [1 / (len(models))] * len(models)\n    pred = np.zeros((data.shape[0], 4))\n    for i, model in enumerate(models):\n        pred += model.predict(data) * weights[i]\n    return pred\n    \n# returns roc auc for given predictions\ndef evaluate(preds, weights=None):\n    if weights is None:\n        weights = [1 / len(preds)] * len(preds)\n    y_pred = np.zeros((y_test.shape[0], 4))\n    for i, pred in enumerate(preds):\n        y_pred += pred * weights[i]\n    return roc_auc_score(y_test, y_pred)\n\n# load list of snapshots\nmodels = se_callback.load_ensemble()\n# precalculated predictions of all models\npreds = []\n# evaluate every model as single\nfor i, model in enumerate(models):\n    pred = predict([model], x_test)\n    preds.append(pred)\n    score = evaluate([pred])\n    print(f'model {i + 1}: roc auc = {score:.4f}')\n\n# evaluate ensemble (with voting equality)\nensemble_score = evaluate(preds)\nprint(f'ensemble: roc auc = {ensemble_score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_score = ensemble_score\nbest_weights = None\nno_improvements = 0\nwhile no_improvements < 5000: #patience\n    \n    # generate normalized weights\n    new_weights = np.random.uniform(size=(len(models), ))\n    new_weights /= new_weights.sum()\n    \n    # get the score without predicting again\n    new_score = evaluate(preds, new_weights)\n    \n    # check (and save)\n    if new_score > best_score:\n        no_improvements = 0\n        best_score = new_score\n        best_weights = new_weights\n        print(f'improvement: {best_score:.4f}')\n    else:\n        no_improvements += 1\n\nprint(f'best weights are {best_weights}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = predict(models, test_images, best_weights)\n\nres = pd.DataFrame()\nres['image_id'] = test_ids\nres['healthy'] = pred[:, 0]\nres['multiple_diseases'] = pred[:, 1]\nres['rust'] = pred[:, 2]\nres['scab'] = pred[:, 3]\nres.to_csv('submission.csv', index=False)\nres.head(40)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}