{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Librerias\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport os\nimport seaborn as sns\nimport cv2\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten,  Dropout, BatchNormalization, LeakyReLU,Input\nfrom keras.optimizers import SGD, Adam\nfrom keras.utils import np_utils\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom kaggle_datasets import KaggleDatasets\nfrom keras.models import Model\nfrom tensorflow import keras\n!pip install -q efficientnet\nimport efficientnet.tfkeras as efn\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TPU Configuration\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \ndef seed_everything(seed=0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 2048\nseed_everything(seed)\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\nAUTO = tf.data.experimental.AUTOTUNE\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parametros\nEPOCAS = 40\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMG_SIZE = 700\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Datos de entrenamiento y test\ntrain = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/train.csv')\ntest = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------PREPARACION DE LA DATA----------------------------------------\n\n#definicion de la funcion de ruta de las imagenes\ndef format_path(st):\n    return GCS_DS_PATH + '/images/' + st + '.jpg'\n\ntrain_paths = train.image_id.apply(format_path).values\ntest_paths = test.image_id.apply(format_path).values\ntrain_labels = train.loc[:, 'healthy':].values\nSPLIT_VALIDATION =True\nif SPLIT_VALIDATION:\n    train_paths, valid_paths, train_labels, valid_labels =train_test_split(train_paths, train_labels, test_size=0.15, \n                                                                           random_state=seed)\n\n#funcion para decodificar\ndef decode_image(filename, label=None, IMG_SIZE=(IMG_SIZE, IMG_SIZE)):\n\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, IMG_SIZE)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n    \n#funcion de aumento de la data\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\ntf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\ntrain_dataset_1 = (\ntf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(64)\n    .prefetch(AUTO)\n)\n\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(list(test_dataset))\n#print(list(valid_dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------CONFIGURACION DEL MODELO----------------------------------------\n\n\n\n#Definicion de la tasa de aprendizaje\nTasa_inicio = 0.0001\nTasa_max = 0.00005 * strategy.num_replicas_in_sync\nTasa_min = 0.0001\nEtapa2 = 4\nEtapa3 = 6\nTasa_decaexpo = .8\n\ndef Tasa_aprend(epoca):\n    if epoca < Etapa2:\n        lr = (Tasa_max - Tasa_inicio) / Etapa2 * epoca + Tasa_inicio\n    elif epoca < Etapa2 + Etapa3:\n        lr = Tasa_max\n    else:\n        lr = (Tasa_max - Tasa_min) * Tasa_decaexpo**(epoca - Etapa2 - Etapa3) + Tasa_min\n    return lr\n    \nTA = tf.keras.callbacks.LearningRateScheduler(Tasa_aprend, verbose=True)\n\n#Grafica de la tasa de aprendizaje\nrng = [i for i in range(EPOCAS)]\ny = [Tasa_aprend(x) for x in rng]\nplt.plot(rng, y)\nprint(\"La Tasa de aprendizaje esta entre : {:.3g} to {:.3g} \".format(y[0], max(y)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\n\n#-------------------------------CONFIGURACION DEL MODELO----------------------------------------\n\nwith strategy.scope():    \n    \n    #Arquitectura seleccionada\n        #image_shape : forma de la imagen , 700x700x3\n        #weights : Pesos iniciales , \n        #con el parametro imagenet ,se utiliza un modelo preentrenado en la base de datos imagennet\n        #include_top: incluir o no la capa de conexion completa al final de la red\n        \n    efficient_net = efn.EfficientNetB7(\n                    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n                    weights='imagenet',\n                    include_top=False\n                    )\n    x = efficient_net.output\n    \n    #Capas de agrupamiento o polling\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n\n    #unidades: Entero positivo, dimensionalidad del espacio de salida.\n    #activation = funcion de activacion,Relu ,softmax \n    x = tf.keras.layers.Dense(4, activation='softmax')(x)\n    \n    model =  keras.Model(inputs = efficient_net.input,outputs=x)\n    \n    #funciones de pérdida : funcion de pérdida de entropía cruzada \"crossentropy\"\n    #La funcion se utiliza durante el entrenamiento para medir el desempeño de la red\n    #optimizador adam \n    \n    #Compilacion de un modelo en keras\n    model.compile(loss=\"categorical_crossentropy\", optimizer= 'adam', metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------ENTRENAMIENTO----------------------------------------\npasos_por_epoca= train_labels.shape[0] // BATCH_SIZE\n\n#Entrenamiento del modelo con los siguente parametros\n#train_dataset= Data de entrenamiento\n#epochs= numero de epocas que va tener el experimento , 40 epocas\n#callbacks = Tasa de aprendizaje en cada epoca , TA(epoca)\n#steps_per_epoch=pasos por cada epoca \n\nhistory = model.fit(\n    train_dataset, \n    epochs=EPOCAS, \n    callbacks=[TA],\n    steps_per_epoch=pasos_por_epoca,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None,\n)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#-------------------------------TEST----------------------------------------\n\n#Prediccion del modelo\nprint(\"La data de pruebas contiene \",test.shape[0],\" imagenes\")\nprediccion= model.predict(test_dataset)\nprint(prediccion)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------RESULTADOS----------------------------------------\n\n\n#Matriz de resultados\nresultados = np.ndarray(shape = (test.shape[0],4), dtype = np.float32)\n\n#Bucle para predecir la clase de cada imagen en la data de pruebas.\nfor row in range(test.shape[0]):\n    for col in range(4):\n        if prediccion[row][col] == max(prediccion[row]):\n            resultados[row][col] = 1\n        else:\n            resultados[row][col] = 0\n            \n\n#rust = oxido\n#scab = costra\n\n#Mostrar Resultados del test        \nresultados = pd.DataFrame(resultados)\nresultados.columns = ['Sana', 'Multiple', 'oxido', 'costra']\nresultados2= pd.concat([test.image_id, resultados ], axis = 1)\n\nprint(resultados2[0:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_dataset)\nprint(train_dataset)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CATEGORY_NAMES = ['healthy','multiple_diseases','rust','scab']\nnb_classes = 4\n\ndef compute_confusion_matrix(true, pred):\n    result = np.zeros((nb_classes,nb_classes))\n    \n    true = true.argmax(axis=1)\n    pred = pred.argmax(axis=1)\n    \n    for i in range(len(true)):\n        result[true[i]][pred[i]] += 1\n        \n    return result.astype('uint16')\n\ndef plot_confusion_matrix(matrix, labels_x=CATEGORY_NAMES, labels_y=CATEGORY_NAMES):\n    \n    # vertical axis = true, horizontal axis = pred\n    \n    plt.title(\"Matrix de confusion\")\n    ax = sns.heatmap(matrix, annot=True, fmt='d', xticklabels = labels_x, yticklabels = labels_y)\n    ax.set(ylabel=\"Real\", xlabel=\"Predicho\")\n    \n#val_pred = get_predict(model,val_dataset,USE_TTA = False)\ny_pred = np.array()\ny_val = np.array()\nval_conf_matrix = compute_confusion_matrix(y_val,y_pred)\nplot_confusion_matrix(val_conf_matrix)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}