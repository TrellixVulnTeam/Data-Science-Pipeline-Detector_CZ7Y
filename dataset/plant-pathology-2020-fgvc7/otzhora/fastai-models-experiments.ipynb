{"cells":[{"metadata":{},"cell_type":"markdown","source":" # Overview ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I used progressive resizing and transfer-learning (ofc). My learning pipeline is very minimalistic: 1) create two databunches (with default transformations, except for flip_vert) one of size 224 and second of size 448, 2) train model on 224, then unfreeze and train a little bit more, then freeze and train on 448 and then unfreeze and fine tune. So the interesting part of my work was experementing with hyperparametrs, so here are my results\n\nHere i would just list training times on my best attempts\n\n* resnet50 public score 0.969 training time on gpu 3 hours \n* efficientnet-b3 public score 0.964 training time on gpu 1.18 hours (71 minutes)\n* densenet121 public score 0.958 training time on gpu 1.43 hours (86 minutes)\n* efficientnet-b4 public score 0.955 training time on gpu 1.93 hours (115 minutes)\n\nMy best ensemble solution got 0.970\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Ways to imporve my result \nCouple of ideas that you can implement:\n1. Create third databunch of size 768 and fine tune models on it\n2. densenet161 is really underfitted, so you might train it a few extra epochs and/or increase lr\n3. Adding one more split in efficientnets will help them train better","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q efficientnet_pytorch","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom pathlib import Path\n\nfrom fastai.imports import *\nfrom fastai import *\nfrom fastai.vision import *\n\nfrom tqdm import tqdm_notebook as tqdm\n\nbase_path = Path('/kaggle/input/plant-pathology-2020-fgvc7/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get tags from train.csv","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tag(row):\n    if row.healthy:\n        return \"healthy\"\n    if row.multiple_diseases:\n        return \"multiple_diseases\"\n    if row.rust:\n        return \"rust\"\n    if row.scab:\n        return \"scab\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_data(train_labels):\n    train_labels.image_id = [image_id+'.jpg' for image_id in train_labels.image_id]\n    train_labels['tag'] = [get_tag(train_labels.iloc[idx]) for idx in train_labels.index]\n    train_labels.drop(columns=['healthy', 'multiple_diseases', 'rust', 'scab'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv(base_path/\"train.csv\")\npath = base_path/\"images\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_data(train_labels)\ntrain_labels = train_labels.set_index(\"image_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create DataBunches ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I want first to train my models on small resolution images and then using transfer learning fine-tune them on high resolution images. This approach speeds up learning dramatically and reduces amount of data needet for training. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfms = get_transforms(max_rotate=30.,flip_vert=True) # all default except for fliping images vertically since these images of leaves, they have no top or bottom","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"src = (ImageList.from_folder(path)\n      .filter_by_func(lambda fname: \"Train\" in fname.name) \n      .split_by_rand_pct()\n      .label_from_func(lambda o: train_labels.loc[o.name]['tag']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next two cells actually needs to become single function that takes size and bs as arguments. Since for different models we need different batchsizes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(size, bs):\n    data = (src.transform(tfms, size=size)\n       .databunch(bs=bs) \n       .normalize())\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I trained 4 models (densenet121, resnet50, efficientnet-b3, efficientnet-b4) and then combined results. Also i used half-precision floating-point format (learner.to_fp16), more info here https://www.quora.com/What-is-the-difference-between-FP16-and-FP32-when-doing-deep-learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet\n\ndef getModelEff(model_name = 'efficientnet-b4'):\n    model = EfficientNet.from_pretrained(model_name)\n    if model_name == 'efficientnet-b4':\n        model._fc = nn.Linear(1792,4)\n    elif model_name == 'efficientnet-b3':\n        model._fc = nn.Linear(1536,4)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are hyperparamentrs that i found out work best, but you should read comments ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"archs = {\n    \"resnet50\": {\n        \"model\": models.resnet50,\n        \"epochs_1\": 10,\n        \"epochs_2\": 10,\n        \"epochs_3\": 10,\n        \"epochs_4\": 10,\n        \"max_lr_s1\": 0.003,\n        \"max_lr_s2\": slice(1e-5, 3e-4),\n        \"max_lr_s1_448\": 0.003,\n        \"max_lr_s2_448\":slice(1e-5, 3e-4)\n    }, \n    \"dense121\": {\n        \"model\": models.densenet121,\n        \"epochs_1\": 6,\n        \"epochs_2\": 5,\n        \"epochs_3\": 6,\n        \"epochs_4\": 3,\n        \"max_lr_s1\": 0.003,\n        \"max_lr_s2\": slice(1e-5, 3e-4),\n        \"max_lr_s1_448\": 0.003,\n        \"max_lr_s2_448\":slice(1e-5, 1e-4)\n    }, \n    \"dense161\": { # it really underfits, so i need to try more epochs/higher lr\n        \"model\": models.densenet161,\n        \"epochs_1\": 8,\n        \"epochs_2\": 5,\n        \"epochs_3\": 7,\n        \"epochs_4\": 4,\n        \"max_lr_s1\": 1e-3,\n        \"max_lr_s2\": slice(1e-5, 1e-4),\n        \"max_lr_s1_448\": 0.003,\n        \"max_lr_s2_448\":slice(1e-5, 3e-4)\n    },  \n    \"eff-b3\": {\n        \"model\": getModelEff('efficientnet-b3'),\n        \"epochs_1\": 7,\n        \"epochs_2\": 3,\n        \"epochs_3\": 5,\n        \"epochs_4\": 2,\n        \"max_lr_s1\": 1e-3,\n        \"max_lr_s2\": slice(1e-5, 1e-4),\n        \"max_lr_s1_448\": 1e-3,\n        \"max_lr_s2_448\":slice(1e-5, 3e-4)\n    },\n    \"eff-b4\": {\n        \"model\": getModelEff('efficientnet-b4'),\n        \"epochs_1\": 10,\n        \"epochs_2\": 5,\n        \"epochs_3\": 8,\n        \"epochs_4\": 5,\n        \"max_lr_s1\": 3e-3,\n        \"max_lr_s2\": slice(1e-5, 3e-4),\n        \"max_lr_s1_448\": 1e-3,\n        \"max_lr_s2_448\":slice(1e-5, 1e-4)\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"archs = {\n    \"resnet152\": {\n        \"model_name\": \"resnet152\",\n        \"model\": models.resnet152,\n        \"stages\": [{\n            \"name_s1\": \"stage-1_lr\",\n            \"epochs_s1\": 10,\n            \"max_lr_s1\": 1e-3,\n            \"name_s2\": \"stage-2_lr\",\n            \"epochs_s2\": 10,\n            \"max_lr_s2\": slice(1e-5, 1e-4),\n            \"size\": 224,\n            \"bs\": 20\n        },\n        {\n            \"name_s1\": \"stage-1_mr\",\n            \"epochs_s1\": 10,\n            \"max_lr_s1\": 1e-3,\n            \"name_s2\": \"stage-2_mr\",\n            \"epochs_s2\": 10,\n            \"max_lr_s2\": slice(1e-5, 1e-4),\n            \"size\": 448,\n            \"bs\": 10\n        },\n        {\n            \"name_s1\": \"stage-1_hr\",\n            \"epochs_s1\": 10,\n            \"max_lr_s1\": 1e-3,\n            \"name_s2\": \"stage-2_hr\",\n            \"epochs_s2\": 10,\n            \"max_lr_s2\": slice(1e-5, 1e-4),\n            \"size\": 896,\n            \"bs\": 5\n        }]\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ndef train_model(arch): \n    model_name = arch[\"model_name\"]\n    \n    print(arch)\n    for i, stage in enumerate(arch[\"stages\"]):\n        print(f\"traing {model_name} on {stage['name_s1']}\")\n        data = get_data(size=stage[\"size\"], bs=stage[\"bs\"])\n        if model_name.startswith(\"eff\"):\n            learner = Learner(data, arch[\"model\"], metrics=error_rate).to_fp16() # if we create learner from pytorch model we can't use cnn_learner\n            learner.split( lambda m: (arch._conv_head,) ) # we need to tell fastai where to split model to use different lr (from slice)\n        else:\n            learner = cnn_learner(data, arch[\"model\"], metrics=error_rate).to_fp16()\n        learner.model_dir = \"/kaggle/working\"\n        if i != 0:\n            learner.load(f\"{arch['stages'][i-1]['name_s2']}_{model_name}\")\n                                             \n        print(f\"lr for {model_name} {stage['name_s1']}\")\n        learner.lr_find()\n        learner.recorder.plot(suggestion=True)\n\n        learner.fit_one_cycle(stage['epochs_s1'], max_lr=stage['max_lr_s1'])\n        learner.save(f\"{stage['name_s1']}_{model_name}\")\n                                             \n        print(f\"lr for {model_name} {stage['name_s2']}\")\n        learner.unfreeze()\n\n        learner.lr_find()\n        learner.recorder.plot(suggestion=True)\n\n        learner.fit_one_cycle(stage['epochs_s2'], max_lr=stage['max_lr_s2'])\n        learner.save(f\"{stage['name_s2']}_{model_name}\")\n        del data\n        gc.collect()\n        if i == len(arch[\"stages\"])-1:\n                return learner\n        learner.destroy()\n        torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"create_results will train model with specified hyper-parametrs and save predictions in csv files, which you can combine how you like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_results():\n    test_images = ImageList.from_folder(base_path/\"images\")\n    test_images.filter_by_func(lambda x: x.name.startswith(\"Test\"))\n    \n    for model_name in archs:\n        arch = archs[model_name]\n        learner = train_model(arch)\n        \n        test_df = pd.read_csv(base_path/\"test.csv\")\n        test_df['healthy'] = [0.0 for _ in test_df.index]\n        test_df['multiple_diseases'] = [0.0 for _ in test_df.index]\n        test_df['rust'] = [0.0 for _ in test_df.index]\n        test_df['scab'] = [0.0 for _ in test_df.index]\n        test_df = test_df.set_index('image_id')\n        \n        for item in tqdm(test_images.items):\n            name = item.name[:-4]\n            img = open_image(item)\n            preds = learner.predict(img)[2]\n\n            test_df.loc[name]['healthy'] = preds[0]\n            test_df.loc[name]['multiple_diseases'] = preds[1]\n            test_df.loc[name]['rust'] = preds[2]\n            test_df.loc[name]['scab'] = preds[3]\n            \n        test_df.to_csv(f\"/kaggle/working/{model_name}_result.csv\")\n        \n        learner.destroy()\n        torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}