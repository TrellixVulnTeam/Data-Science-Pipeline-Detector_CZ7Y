{"cells":[{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"!pip install knockknock #For sending message to telegram\n!pip install -q efficientnet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"import os\nimport cv2\nimport math\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,precision_score,recall_score,ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split,KFold,StratifiedKFold\nfrom transformers import get_cosine_schedule_with_warmup\nfrom albumentations import *\nfrom kaggle_datasets import KaggleDatasets\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras import optimizers\nimport efficientnet.tfkeras as efn\nfrom albumentations import *\n\n\nfrom knockknock import telegram_sender \nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"token\")\nchat_id = user_secrets.get_secret(\"chat_id\")\n\nimport warnings  \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"SEED = 42\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nSIZE = [720,720]\nLR = 0.0008\nWEIGHT_DECAY = 0\nEPOCHS = 35\nWARMUP = 15\nTTA = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def seed_everything(SEED):\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"DIR_INPUT = '/kaggle/input/plant-pathology-2020-fgvc7'\ntrain_df = pd.read_csv(DIR_INPUT + '/train.csv')\ntest_df = pd.read_csv(DIR_INPUT + '/test.csv')\ncols = list(train_df.columns[1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train,valid = train_test_split(train_df,test_size = 0.2,random_state = SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"transform = {\n    'train' :Compose([\n        Resize(SIZE[0],SIZE[1],always_apply=True),\n        HorizontalFlip(p=0.5),\n        VerticalFlip(p=0.5),\n        RandomRotate90(p=0.5),\n        Rotate(limit=25.0,p=0.8)]) }\n\ndef preprocess(df,test=False):\n    paths = df.image_id.apply(lambda x: GCS_DS_PATH + '/images/' + x + '.jpg').values\n    labels = df.loc[:,'healthy':].values\n    if test==False:\n        return paths,labels\n    else:\n        return paths\n    \ndef decode_image(filename, label=None, image_size=(SIZE[0], SIZE[1])):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3) \n    image = tf.image.resize(image, image_size)\n    image = tf.cast(image, tf.float32)\n    image = tf.image.per_image_standardization(image)\n    if label is None:\n        return image\n    else:\n        return image, label\n    \ndef data_augment(image, label=None, seed=SEED):\n    image = tf.image.rot90(image,k=np.random.randint(4))\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.random_flip_up_down(image, seed=seed)\n    if label is None:\n        return image\n    else:\n        return image, label\n    \ndef albu(image):\n    transforms = transform['train']\n    image = transforms(image=image.numpy())['image']\n    image = tf.cast(image, tf.float32)\n    return image\n    \ndef albu_fn(image,label=None):\n    [image,] = tf.py_function(albu, [image], [tf.float32])\n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_dataset = (tf.data.Dataset\n    .from_tensor_slices(preprocess(train))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .shuffle(SEED)\n    .batch(BATCH_SIZE,drop_remainder=True)\n    .repeat()\n    .prefetch(AUTO))\n\nvalid_dataset = (tf.data.Dataset\n    .from_tensor_slices(preprocess(valid))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO))\n\ntest_dataset = (tf.data.Dataset\n    .from_tensor_slices(preprocess(test_df,test=True))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def plot_transform(image_id,num_images=7):\n    plt.figure(figsize=(30,10))\n    path,_ = preprocess(train.iloc[image_id:image_id+1])\n    for i in range(1,num_images+1):\n        plt.subplot(1,num_images+1,i)\n        plt.axis('off')\n        image = decode_image(filename=path[0])\n        image = data_augment(image=image)\n        plt.imshow(image)\n\nplot_transform(9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([\n        efn.EfficientNetB6(input_shape=(SIZE[0], SIZE[1], 3),weights='imagenet',pooling='avg',include_top=False),\n        Dense(4, activation='softmax')\n    ])\n        \n    model.compile(\n        optimizer='adam',\n        loss = 'categorical_crossentropy',\n        metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def get_cosine_schedule_with_warmup(lr,num_warmup_steps, num_training_steps, num_cycles=0.5):\n    \"\"\"\n    Modified the get_cosine_schedule_with_warmup from huggingface for tenserflow\n    (https://huggingface.co/transformers/_modules/transformers/optimization.html#get_cosine_schedule_with_warmup)\n\n    Create a schedule with a learning rate that decreases following the\n    values of the cosine function between 0 and `pi * cycles` after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"\n\n    def lrfn(epoch):\n        if epoch < num_warmup_steps:\n            return float(epoch) / float(max(1, num_warmup_steps)) * lr\n        progress = float(epoch - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nlr_schedule= get_cosine_schedule_with_warmup(lr=LR,num_warmup_steps=WARMUP,num_training_steps=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"STEPS_PER_EPOCH = train.shape[0] // BATCH_SIZE\n@telegram_sender(token=token, chat_id=int(chat_id))\ndef train():\n    history = model.fit(\n        train_dataset, \n        epochs=EPOCHS, \n        callbacks=[lr_schedule],\n        steps_per_epoch=STEPS_PER_EPOCH,\n        validation_data=valid_dataset)\n    \n    string = 'Train acc:{:.4f} Train loss:{:.4f},Val acc:{:.4f} Val loss:{:.4f}'.format( \\\n        model.history.history['categorical_accuracy'][-1],model.history.history['loss'][-1],\\\n        model.history.history['val_categorical_accuracy'][-1],model.history.history['val_loss'][-1])\n    \n    return string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def display_training_curves(training, validation, title, subplot):\n    \"\"\"\n    Source: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n    \"\"\"\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(15,15), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"display_training_curves(\n    model.history.history['loss'], \n    model.history.history['val_loss'], \n    'loss', 211)\ndisplay_training_curves(\n    model.history.history['categorical_accuracy'], \n    model.history.history['val_categorical_accuracy'], \n    'accuracy', 212)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"test_pred = model.predict(test_dataset, verbose=1)\nsubmission_df = pd.read_csv(DIR_INPUT + '/sample_submission.csv')\nsubmission_df[['healthy', 'multiple_diseases', 'rust', 'scab']] = test_pred\nsubmission_df.to_csv('submission.csv', index=False)\npd.Series(np.argmax(submission_df[cols].values,axis=1)).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_tta = np.zeros((len(test_df),4))\nfor i in range(TTA):\n    test_dataset_tta = (tf.data.Dataset\n    .from_tensor_slices(preprocess(test_df,test=True))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)    \n    .batch(BATCH_SIZE))\n    test_pred_tta += model.predict(test_dataset_tta, verbose=1)\nsubmission_df = pd.read_csv(DIR_INPUT + '/sample_submission.csv')\nsubmission_df[['healthy', 'multiple_diseases', 'rust', 'scab']] = test_pred_tta/TTA\nsubmission_df.to_csv('submission_tta.csv', index=False)\npd.Series(np.argmax(submission_df[cols].values,axis=1)).value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}