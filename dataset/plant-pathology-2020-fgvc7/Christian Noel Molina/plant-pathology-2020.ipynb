{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\n\nimport os\nimport gc\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X_train = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/train.csv\")\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.healthy.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.multiple_diseases.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.rust.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.scab.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [\"healthy\", \"multiple_diseases\", \"rust\", \"scab\"]\ntarget_sum = 0\nfor target in targets:\n    target_sum = target_sum + X_train[target].value_counts()[1] \ntarget_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_class(X_train):\n    X_train[\"class\"] = \"healthy\"\n    X_train.loc[X_train.multiple_diseases==1, \"class\"] = \"multiple_diseases\"\n    X_train.loc[X_train.rust==1, \"class\"] = \"rust\"\n    X_train.loc[X_train.scab==1, \"class\"] = \"scab\"\n    return X_train\n\nX_train = add_class(X_train)\nsns.countplot(x=\"class\", data=X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nrandom_state=10\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, X_train[targets], train_size=0.75, random_state=random_state, stratify=X_train[\"class\"])\nX_train[\"image_id\"] = X_train[\"image_id\"].map(lambda x: \"\".join([x, \".jpg\"]))\nX_valid[\"image_id\"] = X_valid[\"image_id\"].map(lambda x: \"\".join([x, \".jpg\"]))\n\nbase_path = \"../input/plant-pathology-2020-fgvc7/images/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_leaf(full_img, border_percentage=0.1, min_percentage_width=0.7, min_percentage_height=0.5,\n             base_kernel=61, kernel_step=6, base_threshold1=50, threshold2_params=[150,100,50],\n             return_crop_img=False):\n\n    edges = np.zeros((1,1,3))\n    boundary_box_shape = (0, 0) \n    threshold_iter = 0\n    boundary_img = None\n\n    while (base_kernel > kernel_step+1 or threshold_iter != len(threshold2_params)):\n\n        if (boundary_box_shape[0] > full_img.shape[0]*min_percentage_height and \n               boundary_box_shape[1] > full_img.shape[1]*min_percentage_width):\n            break\n        \n        if threshold_iter == len(threshold2_params):\n            threshold_iter = 0\n            base_kernel = base_kernel - kernel_step\n#         print(\"Finding object with kernel:\", base_kernel, \"and thresholds:\", base_threshold1, threshold2_params[threshold_iter])\n\n        img_blur = cv2.medianBlur(full_img, ksize=base_kernel)\n        img_gray = cv2.cvtColor(img_blur, cv2.COLOR_RGB2GRAY)\n        edges = cv2.Canny(img_gray, base_threshold1, threshold2_params[threshold_iter], L2gradient=True)\n\n        height, width = edges.shape\n\n        border_height_top = int(height*border_percentage)\n        border_height_bottom = height-border_height_top\n        border_width_left = int(width*border_percentage)\n        border_width_right = width-border_width_left\n\n        edges_along_y = np.nonzero(np.argmax(edges[border_height_top:border_height_bottom, \n                                                 border_width_left:border_width_right], axis=1))[0]\n        edges_along_x = np.nonzero(np.argmax(edges[border_height_top:border_height_bottom, \n                                                 border_width_left:border_width_right], axis=0))[0]\n\n        y_start = border_height_top\n        y_end = border_height_top\n\n        x_start = border_width_left\n        x_end = border_width_left\n        \n        if len(edges_along_y) > 0:\n            y_start = y_start + edges_along_y[0] \n            y_end = y_end + edges_along_y[-1]\n            \n        if len(edges_along_x) > 0:\n            x_start = x_start + edges_along_x[0] \n            x_end = x_end + edges_along_x[-1]\n\n        boundary_box_shape = full_img[y_start:y_end, x_start:x_end].shape\n        threshold_iter = threshold_iter + 1\n        \n    if return_crop_img:\n        return full_img[y_start:y_end, x_start:x_end]\n    else:\n        return x_start,y_start, x_end,y_end,edges","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.image_id.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid.image_id.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_img(img):\n    print(\"Preprocessing\")\n    x_start,y_start, x_end,y_end,edges = crop_leaf(img.numpy(),threshold2_params=[100],kernel_step=8)\n    img = cv2.cvtColor(img.numpy(), cv2.COLOR_RGB2GRAY)\n    return tf.convert_to_tensor(img[y_start:y_end, x_start:x_end])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input\n# from tensorflow.keras.applications.vgg16 import preprocess_input\n# from tensorflow.keras.applications.nasnet import preprocess_input\nfrom tensorflow.keras.applications.densenet import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nBATCH_SIZE = 64\ntarget_size = 512\ntransform_range=0.10\nimage_gen = ImageDataGenerator(\n#                       featurewise_center=True, featurewise_std_normalization=True,\n                      rotation_range=30, \n                      samplewise_center=True, samplewise_std_normalization=True, \n                      height_shift_range=transform_range,\n                      zoom_range=[0.8, 0.9], horizontal_flip=True, vertical_flip=True, rescale=1./255,\n                      preprocessing_function=preprocess_input)\n\ndef load_img(id_):\n    img = cv2.imread(\"\".join([base_path, id_]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return cv2.resize(img, dsize=(target_size,target_size))\n\n# sample_set = X_train.groupby([\"class\"]).apply(lambda x: x.sample(50))\n# image_set = np.asarray(list(map(load_img, X_train.image_id.tolist())))\n# image_gen.fit(image_set)\n\n\nimage_gen_train = image_gen.flow_from_dataframe(X_train, directory=base_path, x_col=\"image_id\", y_col=\"class\",\n                             class_mode=\"categorical\", seed=random_state, target_size=(target_size,target_size), shuffle=True,\n                                               batch_size=BATCH_SIZE)\n\nvalid_gen = ImageDataGenerator(rescale=1./255)\nimage_gen_valid = valid_gen.flow_from_dataframe(X_valid, directory=base_path, x_col=\"image_id\", y_col=\"class\",\n                             class_mode=\"categorical\", seed=random_state, target_size=(target_size,target_size), shuffle=False,\n                                               batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_image(class_, count, channel=None, display=True):\n    plt.figure(figsize=(10,10))\n    id_ = X_train[X_train[\"class\"]==class_].image_id.reset_index(drop=True).iloc[count]\n    img = cv2.imread(\"\".join([base_path, id_]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    if channel is not None:\n        img = img[:,:,channel]\n    \n    if display:\n        if channel is not None:\n            plt.imshow(img, cmap=\"gray\")\n        else:\n            plt.imshow(img)\n        \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = display_image(\"scab\", 10, display=False)\nplt.imshow(image_gen.random_transform(img, seed=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.nasnet import NASNetMobile\nfrom tensorflow.keras.applications.densenet import DenseNet121\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.keras.models import Model\n\ndef create_model(last_layers_to_unfreeze=None):\n    model = DenseNet121(include_top=False, input_shape=(target_size,target_size,3), weights=\"imagenet\", pooling=\"max\")\n\n    print(\"Number of layers:\", len(model.layers))\n    if last_layers_to_unfreeze is not None:\n        for layer in model.layers[:-last_layers_to_unfreeze]:\n            layer.trainable = False\n        print(\"Number of frozen layers:\", len(model.layers[:-last_layers_to_unfreeze]))\n\n    model_output = Flatten()(model.output)\n    model_output = Dense(512, activation=\"relu\")(model_output)\n    model_output = Dropout(0.5)(model_output)\n    model_output = Dense(256, activation=\"relu\")(model_output)\n    model_output = Dropout(0.5)(model_output)\n    model_output = Dense(4, activation=\"softmax\")(model_output)\n    model = Model(inputs=model.input, outputs=model_output)\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(114)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nencoder.fit(X_train[\"class\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nencoded_class = encoder.transform(X_train[\"class\"])\nc_weight = compute_class_weight(\"balanced\", np.unique(encoded_class), encoded_class)\nc_weight = dict(zip(range(4), c_weight))\n\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=3)\n# history = model.fit_generator(image_gen_train, epochs=20, callbacks=[early_stop], validation_data=image_gen_valid)\nhistory = model.fit_generator(image_gen_train, epochs=20, callbacks=[early_stop], validation_data=image_gen_valid, class_weight=c_weight)\nmodel.save(\"model_with_validation.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(list(zip(history.history[\"loss\"], history.history[\"val_loss\"])))\\\n    .rename(columns={0: \"loss\", 1: \"val_loss\", 2: \"categorical_accuracy\", 3: \"val_categorical_accuracy\", \"index\": \"epochs\"})\n    \nplt.figure(figsize=(10,10))\nhistory_df.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(list(zip(history.history[\"categorical_accuracy\"],  history.history[\"val_categorical_accuracy\"])))\\\n    .rename(columns={0: \"categorical_accuracy\", 1: \"val_categorical_accuracy\", \"index\": \"epochs\"})\nplt.figure(figsize=(10,10))\nhistory_df.plot()\n# sns.lineplot(x=\"index\", y=\"categorical_accuracy\", data=history_df).set_label(\"sasa\")\n# sns.lineplot(x=\"index\", y=\"val_categorical_accuracy\", data=history_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom sklearn.metrics import classification_report\n\ndef eval_model(model, image_gen_valid, y_valid):\n    y_valid = add_class(y_valid)\n    y_true_pred = encoder.transform(y_valid[\"class\"])\n    \n    valid_probs = model.predict_generator(image_gen_valid)\n    valid_preds = np.argmax(valid_probs, axis=1)\n    print(classification_report(y_true_pred, valid_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_model(model, image_gen_valid, y_valid) # with class_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluated model macro f1 score is around 0.69 with higher precision and recall scores for each class compared to last version","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/train.csv\")\nX_train = add_class(X_train)\nX_train[\"image_id\"] = X_train[\"image_id\"].map(lambda x: \"\".join([x, \".jpg\"]))\n\n# reinitialize for the whole dataset\nimage_gen = ImageDataGenerator(\n#                              featurewise_center=True, featurewise_std_normalization=True,\n                             rotation_range=30, samplewise_center=True, samplewise_std_normalization=True, \n                             height_shift_range=transform_range,\n                             zoom_range=[0.8, 0.9], horizontal_flip=True, vertical_flip=True, rescale=1./255,\n                             preprocessing_function=preprocess_input)\n# image_set = np.asarray(list(map(load_img, X_train.image_id.tolist())))\n# image_gen.fit(image_set)\n\nimage_gen_final = image_gen.flow_from_dataframe(X_train, directory=base_path, x_col=\"image_id\", y_col=\"class\",\n                             class_mode=\"categorical\", seed=random_state, target_size=(target_size,target_size), batch_size=BATCH_SIZE)\n\nmodel = create_model(114)\nmodel.fit_generator(image_gen_final, epochs=3)\n\n# predict and submit test data\nX_test = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/test.csv\")\nX_test[\"image_id\"] = X_test[\"image_id\"].map(lambda x: \"\".join([x, \".jpg\"]))\ntest_gen = ImageDataGenerator(rescale=1./255)\nimage_test_gen = test_gen.flow_from_dataframe(X_test, directory=base_path, x_col=\"image_id\",\n                             class_mode=None, target_size=(target_size,target_size), shuffle=False)\nprobs = model.predict_generator(image_test_gen)\n\nsubmission = X_test.join(pd.DataFrame(probs))\nsubmission[\"image_id\"] = submission[\"image_id\"].map(lambda x: x.replace(\".jpg\",\"\"))\nsubmission = submission.rename(columns=dict(zip(range(4), targets)))\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.history.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 100))\nfull_img = display_image(\"scab\", 10)\nx_start,y_start, x_end,y_end,edges = crop_leaf(full_img)\ncv2.rectangle(full_img, (x_start,y_start), (x_end,y_end), color=(255,0,0), thickness=10)\n# plt.figure(figsize=(10,10))\nax[0].imshow(cv2.bitwise_not(edges), cmap=\"gray\")\nax[1].imshow(full_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# full_img = display_image(\"scab\", 10)\nhist = cv2.calcHist([full_img], channels=[0], mask=None, histSize=[256], ranges=[0,256])\nplt.plot(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(cv2.distTransform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"src = cv2.cvtColor(display_image(\"scab\", 15), cv2.COLOR_RGB2GRAY)\n# clahe = cv2.createCLAHE()\n\nkernel = np.array([[-1, -1, -1], \n                   [-1, 9, -1], \n                   [-1, -1, -1]], dtype=np.float32)\nprint(np.sum(kernel))\n# do the laplacian filtering as it is\n# well, we need to convert everything in something more deeper then CV_8U\n# because the kernel has some negative values,\n# and we can expect in general to have a Laplacian image with negative values\n# BUT a 8bits unsigned int (the one we are working with) can contain values from 0 to 255\n# so the possible negative number will be truncated\nimgLaplacian = cv2.filter2D(src, cv2.CV_32F, kernel)\nsharp = np.float32(src)\nimgResult = sharp - imgLaplacian\n# # convert back to 8bits gray scale\nimgResult = np.clip(imgResult, 0, 255)\nimgResult = imgResult.astype('uint8')\nimgLaplacian = np.clip(imgLaplacian, 0, 255)\nimgLaplacian = np.uint8(imgLaplacian)\n\nimg = cv2.threshold(imgLaplacian, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\ndist = cv2.distanceTransform(img, cv2.DIST_L2, 3)\nplt.imshow(dist, cmap=\"gray\")\n# plt.imshow(clahe.apply)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(cv2.normalize(full_img, None, 0, 255, cv2.NORM_MINMAX))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(cv2.normalize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=20, ncols=2, figsize=(30, 100))\nfor i in range(10,20):\n    print(\"item\", i)\n    full_img = display_image(\"rust\", i)\n    x_start,y_start, x_end,y_end,edges = crop_leaf(full_img)\n    cv2.rectangle(full_img, (x_start,y_start), (x_end,y_end), color=(255,0,0), thickness=10)\n    # plt.figure(figsize=(10,10))\n    ax[i,0].imshow(cv2.bitwise_not(edges), cmap=\"gray\")\n    ax[i,1].imshow(full_img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}