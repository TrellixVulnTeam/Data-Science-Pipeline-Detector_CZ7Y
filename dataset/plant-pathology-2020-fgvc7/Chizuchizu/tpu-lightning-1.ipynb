{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.7 > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev\n\n!pip install pytorch-lightning == 1.8\n\n!pip install omegaconf\n\n# !git clone https://github.com/rwightman/pytorch-image-models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/rwightman/pytorch-image-models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('pytorch-image-models')\n# sys.path.append(\"../input/mytorchlightning/pytorch-lightning\")\n# sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport time\nimport random\nfrom contextlib import contextmanager\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn import model_selection\nfrom collections import defaultdict, Counter\nimport sys\nfrom typing import Tuple\nimport PIL\nfrom omegaconf import OmegaConf\nfrom torch.utils.data import Dataset\nfrom pathlib import Path\nfrom PIL import Image\nfrom PIL.Image import Image as PILImage\nfrom torch.utils.data.dataloader import DataLoader\nimport cv2\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport albumentations as A\nfrom sklearn.metrics import roc_auc_score\n\nfrom albumentations import (\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip,\n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout,\n    IAAAdditiveGaussianNoise, Transpose\n)\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\nimport matplotlib.pyplot as plt\nimport timm\nfrom torchvision import models\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom torch import optim\n\nimport torch_xla.core.xla_model as xm\n\n\nimport torch_xla","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_lightning import LightningDataModule\nfrom pytorch_lightning.core.lightning import LightningModule\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_PATH = '../input/plant-pathology-2020-fgvc7/images'\n# target_cols = \ntrain = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/train.csv\")\ntest = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/train.csv\")\nrand = random.randint(0, 100000)\n\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf = \"\"\"\nbase:\n  train_path: '../input/plant-pathology-2020-fgvc7/images'\n  print_freq: 100\n  num_workers: 4\n  seed: 42\n  target_size: 4\n  target_cols: [\n      \"healthy\",\n      \"multiple_diseases\",\n      \"rust\",\n      \"scab\"\n  ]\n\n  n_fold: 4\n  trn_fold: [0]\n  train: True\n  debug: True\n  oof: False\n  tpu: True\n\nsplit:\n  name: \"KFold\"\n  param: {\n           \"n_splits\": 4,\n           \"shuffle\": True,\n           \"random_state\": 1212\n  }\n\nmodel:\n  model_name: \"tf_efficientnet_b0_ns\"\n  size: 224  # 480\n  batch_size: 128\n  pretrained: true\n  epochs: 10\n\nloss:\n  name: \"BCEWithLogitsLoss\"\n  param: {}\n\noptimizer:\n  name: \"AdamW\"\n  param: {\n           \"lr\": 5e-3,\n           \"weight_decay\": 1e-6,\n           \"amsgrad\": False\n  }\n\nscheduler:\n  name: \"CosineAnnealingLR\"\n  param: {\n            \"T_max\": 6,\n            \"eta_min\": 0,\n            \"last_epoch\": -1\n  }\nwandb:\n  use: false\n  project: \"kaggle-tpu\"\n  name: \"1\"\n  tags: []\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_transforms(img_size, data):\n    if data == 'train':\n        return Compose([\n            Resize(img_size, img_size),\n            RandomResizedCrop(img_size, img_size, scale=(0.85, 1.0)),\n            HorizontalFlip(p=0.5),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n\n    elif data == 'valid':\n        return Compose([\n            Resize(img_size, img_size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\nclass TrainDataset(Dataset):\n    def __init__(self, cfg, df, transform=None, inference=False):\n        self.df = df\n        self.cfg = cfg\n        self.file_names = df['image_id'].values\n        self.labels = df[cfg.base.target_cols].values\n        self.transform = transform\n        self.inference = inference\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.file_names[idx]\n        file_path = f'{self.cfg.base.train_path}/{file_name}.jpg'\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n        if self.transform:\n            # print(image.shape)\n            # image = image.transpose(2, 0, 1)\n            augmented = self.transform(image=image)\n            # print(image)\n            # print(augmented)\n            image = augmented['image']\n        label = torch.tensor(self.labels[idx]).float()\n\n        if self.inference:\n            return image\n        else:\n            return image, label\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CHIZUDataModule(LightningDataModule):\n    def __init__(\n            self,\n            cfg,\n            train_df,\n            val_df,\n            aug_p: float = 0.5,\n            val_pct: float = 0.2,\n            img_sz: int = 224,\n            batch_size: int = 64,\n            num_workers: int = 4,\n            fold_id: int = 0,\n    ):\n        super().__init__()\n        self.cfg = cfg\n        self.aug_p = aug_p\n        self.val_pct = val_pct\n        self.img_sz = img_sz\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.fold_id = fold_id\n\n        self.train_df = train_df\n        self.val_df = val_df\n\n    def train_dataloader(self):\n        train_dataset = TrainDataset(self.cfg, self.train_df, transform=get_transforms(self.img_sz, data=\"train\"))\n        \n        sampler = None\n        if self.cfg.base.tpu:\n            sampler = torch.utils.data.distributed.DistributedSampler(\n            train_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True\n            )\n            \n            return DataLoader(\n                train_dataset,\n                batch_size=self.batch_size,\n                # num_workers=self.num_workers,\n                sampler = sampler,\n                # shuffle=True,\n                # pin_memory=True,\n                # drop_last=True\n            )\n        else:\n            return DataLoader(\n                train_dataset,\n                batch_size=self.batch_size,\n                num_workers=self.num_workers,\n                # sampler = sampler,\n                shuffle=True,\n                pin_memory=True,\n                drop_last=True\n            )\n\n    def val_dataloader(self):\n        valid_dataset = TrainDataset(self.cfg, self.val_df, transform=get_transforms(self.img_sz, data=\"valid\"))\n        \n        sampler = None\n        if self.cfg.base.tpu:\n            sampler = torch.utils.data.distributed.DistributedSampler(\n            valid_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=False\n        )\n                \n            return DataLoader(\n                valid_dataset,\n                batch_size=self.batch_size,\n                # num_workers=self.num_workers,\n                # shuffle=False,\n                # pin_memory=True,\n                # drop_last=False,\n                sampler=sampler\n            )\n        else:\n            return DataLoader(\n                valid_dataset,\n                batch_size=self.batch_size,\n                num_workers=self.num_workers,\n                shuffle=False,\n                pin_memory=True,\n                drop_last=False,\n                # sampler=sampler\n            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"__CRITERIONS__ = {\n    # \"BCEFocalLoss\": BCEFocalLoss\n}\n    \n\n__SPLITS__ = {\n    # \"MultilabelStratifiedKFold\": MultilabelStratifiedKFold\n}\n\n__OPTIMIZERS__ = {\n    # \"AdaBelief\": AdaBelief,\n    # \"RAdam\": torch_optimizer.RAdam\n}\n\ndef get_criterion(cfg):\n    if hasattr(nn, cfg.loss.name):\n        return nn.__getattribute__(cfg.loss.name)(**cfg.loss.param)\n    elif __CRITERIONS__.get(cfg.loss.name) is not None:\n        return __CRITERIONS__[cfg.loss.name](**cfg.loss.param)\n    else:\n        raise NotImplementedError\n\n        \ndef get_optimizer(cfg, model):\n    optimizer_name = cfg.optimizer.name\n\n    if __OPTIMIZERS__.get(optimizer_name) is not None:\n        return __OPTIMIZERS__[optimizer_name](model.parameters(), **cfg.optimizer.param)\n    else:\n        return optim.__getattribute__(optimizer_name)(model.parameters(), **cfg.optimizer.param)\n\n\ndef get_scheduler(cfg, optimizer):\n    scheduler_name = cfg.scheduler.name\n\n    if scheduler_name is None:\n        return\n    else:\n        return optim.lr_scheduler.__getattribute__(scheduler_name)(optimizer, **cfg.scheduler.param)\n\n\ndef get_split(cfg):\n    if hasattr(model_selection, cfg.split.name):\n        return model_selection.__getattribute__(cfg.split.name)(**cfg.split.param)\n    elif __SPLITS__.get(cfg.split.name) is not None:\n        return __SPLITS__[cfg.split.name](**cfg.split.param)\n    else:\n        raise NotImplementedError","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CHIZUModel(LightningModule):\n    def __init__(self, cfg, model_name=\"resnext50_32x4d\"):\n        super().__init__()\n\n        self.cfg = cfg\n        self.wd = 1e-6\n        self.model_name = model_name\n        self.model = timm.create_model(model_name, pretrained=cfg.model.pretrained)\n\n        if \"efficient\" not in self.model_name:\n            n_features = self.model.fc.in_features\n            self.model.fc = nn.Linear(n_features, cfg.base.target_size)\n        else:\n            \"efficient\"\n            self.model.classifier = nn.Linear(self.model.num_features, cfg.base.target_size)\n\n        # self.model.avg_pool = GeM()\n\n        self.optimizer = get_optimizer(cfg, self.model)\n        self.scheduler = get_scheduler(cfg, self.optimizer)\n        self.criterion = get_criterion(cfg)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.model(x)\n        # x = self.sigmoid(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        # self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        # self.log(\"valid_loss\", loss, prog_bar=True)\n        return loss, y_hat.cpu().numpy(), y.cpu().numpy()\n\n    def validation_epoch_end(self, input_):\n        auc_l = 0\n        acc_l = 0\n        acc_f = 0\n        for j in range(4):\n            loss_list, y_hat_list, y_list = np.array([]), np.array([]), np.array([])\n            for i, (loss, y_hat, y) in enumerate(input_):\n                y_hat_list = np.append(y_hat_list, y_hat[:, j])\n                y_list = np.append(y_list, y[:, j])\n\n            y_hat_list = sigmoid(y_hat_list)\n            try:\n                auc = roc_auc_score(y_list, y_hat_list)\n            except ValueError:\n                auc = 0\n            # acc = accuracy_score(y_list, np.round(y_hat_list))\n            auc_l += auc / 4\n            # acc_l += acc / 29\n\n            num = \"{0:01d}\".format(j + 1)\n            self.log(f\"{num}-auc\", auc, prog_bar=True)\n\n            if j == 0:\n                auc_f = auc\n\n        for i, (loss, y_hat, y) in enumerate(input_):\n            loss_list = np.append(loss_list, float(loss.cpu()))\n        self.log(\"valid_loss\", loss_list.mean(), prog_bar=True)\n        self.log(\"valid auc\", auc_l, prog_bar=True)\n        # self.log(\"valid Acc\", acc_l, prog_bar=True)\n        # self.log(\"auc-1st\", auc_f, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = self.optimizer\n        scheduler = self.scheduler\n\n        return [optimizer], [scheduler]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_loop(cfg, folds, fold):\n    global rand\n\n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n\n    data_module = CHIZUDataModule(\n        cfg,\n        train_folds,\n        valid_folds,\n        aug_p=0.5,\n        img_sz=cfg.model.size,\n        batch_size=cfg.model.batch_size,\n        num_workers=cfg.base.num_workers,\n        # fold_id=fold,\n    )\n    model = CHIZUModel(\n        cfg,\n        model_name=cfg.model.model_name,\n    )\n\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=f'../exp2/{rand}',\n        filename=f\"fold-{fold}\",\n        # save_top_k=3,\n        mode='min',\n    )\n    \n    if cfg.wandb.use:\n        wandb.init(\n            name=cfg.wandb.name + f\"-fold-{fold}-{rand}\",\n            project=cfg.wandb.project,\n            tags=cfg.wandb.tags + [str(rand)],\n            reinit=True\n        )\n        wandb_logger = WandbLogger(\n            name=cfg.wandb.name + f\"-fold-{fold}-{rand}\",\n            project=cfg.wandb.project,\n            tags=cfg.wandb.tags + [str(rand)]\n        )\n        wandb_logger.log_hyperparams(dict(cfg))\n        wandb_logger.log_hyperparams(dict({\"rand\": rand, \"fold\": fold, }))\n\n    \n    if cfg.base.tpu:\n        trainer = pl.Trainer(\n            # gpus=-1,\n            tpu_cores=8,\n            max_epochs=cfg.model.epochs,\n            # gradient_clip_val=0.1,\n            precision=16,\n            # logger=wandb_logger if \"wandb_logger\" in locals() else False,\n            # callbacks=[checkpoint_callback]\n        )\n    else:\n\n        trainer = pl.Trainer(\n            gpus=-1,\n            max_epochs=cfg.model.epochs,\n            gradient_clip_val=0.1,\n            precision=16,\n            # logger=wandb_logger if \"wandb_logger\" in locals() else False,\n            callbacks=[checkpoint_callback]\n        )\n\n    trainer.fit(model=model, datamodule=data_module)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def main(cfg):\n    seed_torch(seed=cfg.base.seed)\n\n    folds = train.copy()\n\n    if cfg.base.debug:\n        folds = folds.sample(n=1000, random_state=cfg.base.seed).reset_index(drop=True)\n        cfg.model.epochs = 1\n    \n    \n    Fold = get_split(cfg)\n    for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[cfg.base.target_cols])):\n        folds.loc[val_index, 'fold'] = int(n)\n    folds['fold'] = folds['fold'].astype(int)\n\n    oof_df = train.copy()\n    test_pred = test.copy()\n    test_pred.iloc[:, 1:] = 0\n\n    for fold in range(cfg.base.n_fold):\n        if fold in cfg.base.trn_fold:\n            train_loop(cfg, folds, fold)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main(OmegaConf.create(conf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}