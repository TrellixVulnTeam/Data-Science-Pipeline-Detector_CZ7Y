{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Basic Setup","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport cv2\nimport pylab\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import models\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.applications import DenseNet121","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Global variables\n\nSAMPLE_LEN = 20\nBASE_DIR_PATH = \"/kaggle/input/plant-pathology-2020-fgvc7\"\nIMAGE_PATH = \"/kaggle/input/plant-pathology-2020-fgvc7/images/\"\nTEST_PATH = \"/kaggle/input/plant-pathology-2020-fgvc7/test.csv\"\nTRAIN_PATH = \"/kaggle/input/plant-pathology-2020-fgvc7/train.csv\"\nSUB_PATH = \"/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv\"\n\nIMAGE_SIZE = 124\nBATCH_SIZE = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(SUB_PATH)\ntest_data = pd.read_csv(TEST_PATH)\ntrain_data = pd.read_csv(TRAIN_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_path(st):\n    return BASE_DIR_PATH + '/images/' + st + '.jpg'\n\ndef load_image(image_id):\n    file_path = image_id + \".jpg\"\n    image = cv2.imread(IMAGE_PATH + file_path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \ndef decode_image(filename, label=None, image_size=(IMAGE_SIZE, IMAGE_SIZE)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n    \ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_paths = test_data.image_id.apply(format_path).values\ntrain_paths = train_data.image_id.apply(format_path).values\ntrain_labels = np.float32(train_data.loc[:, 'healthy':'scab'].values)\ntrain_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, train_labels, test_size=0.15, random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualisations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = train_data[\"image_id\"][:SAMPLE_LEN].apply(load_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_x, mean_y = 0, 1\nfor image in train_images:\n    mean_x = mean_x + image.shape[0]\n    mean_y = mean_y + image.shape[1]\n    \nprint(mean_x/len(train_images), mean_y/len(train_images), mean_x/len(train_images) / 5, mean_y/len(train_images) / 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\nax[0].imshow(cv2.resize(train_images[5], (409, 273)))\nax[0].set_title('Original Image', fontsize=20)\nax[1].imshow(cv2.resize(train_images[5], (IMAGE_SIZE, IMAGE_SIZE)))\nax[1].set_title('Resized Image', fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(30, 10))\n\nax[0].imshow(cv2.resize(train_images[15][:,:,0], (IMAGE_SIZE, IMAGE_SIZE)), cmap='Reds')\nax[1].imshow(cv2.resize(train_images[15][:,:,1], (IMAGE_SIZE, IMAGE_SIZE)), cmap='Greens')\nax[2].imshow(cv2.resize(train_images[15][:,:,2], (IMAGE_SIZE, IMAGE_SIZE)), cmap='Blues')\nax[3].imshow(cv2.resize(train_images[15], (IMAGE_SIZE, IMAGE_SIZE)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"red seems to be the most important colors as far as detecting plant diseases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"red_values   = [np.mean(train_images[idx][:, :, 0]) for idx in range(len(train_images))]\ngreen_values = [np.mean(train_images[idx][:, :, 1]) for idx in range(len(train_images))]\nblue_values  = [np.mean(train_images[idx][:, :, 2]) for idx in range(len(train_images))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red_channel   =    [np.mean(train_images[idx][:, :, 0]) for idx in range(len(train_images))] # train_df['lenght_prop'][train_df['sentiment'] == 'positive'].to_numpy()\ngreen_channel =    [np.mean(train_images[idx][:, :, 1]) for idx in range(len(train_images))] # train_df['lenght_prop'][train_df['sentiment'] == 'negative'].to_numpy()\nblue_channel  =    [np.mean(train_images[idx][:, :, 2]) for idx in range(len(train_images))]  # train_df['lenght_prop'][train_df['sentiment'] == 'negative'].to_numpy()\n\nBoxName = ['red','green', 'blue']\ndata = [red_channel, green_channel, blue_channel ]\n\nplt.boxplot(data)\nplt.ylim(0,200)\npylab.xticks([1,2,3], BoxName)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE \nSTEPS_PER_EPOCH","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image)\n    .map(data_augment)\n    .repeat()\n    .shuffle(1)\n    .batch(BATCH_SIZE)\n    .prefetch(1)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image)\n    .batch(BATCH_SIZE)\n    .prefetch(1)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dense Net","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Efficient net are more Efficient than Dense net computationlesly but at the prize of eating way more memory which pushed me to downgrade the image size from around 300 per 300 pixels to 124 per 124 pixels  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([DenseNet121(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n                                         weights='imagenet',\n                                         include_top=False),\n                             L.GlobalAveragePooling2D(),\n                             L.Dense(train_labels.shape[1],\n                                     activation='softmax')])\n\nmodel.compile(optimizer='adam',\n              loss = 'categorical_crossentropy',\n              metrics=['categorical_accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n\nax[0].plot(np.linspace(1,EPOCHS,EPOCHS), history.history['val_loss'], color='red')\nax[0].plot(np.linspace(1,EPOCHS,EPOCHS), history.history['loss'], color='blue')\n\nax[0].set_xlabel('epoch')\nax[0].set_ylabel('loss')\nax[0].set_ylim(0, 1.3)\n\n\nax[1].plot(np.linspace(1,EPOCHS,EPOCHS), history.history['val_categorical_accuracy'], color='red')\nax[1].plot(np.linspace(1,EPOCHS,EPOCHS), history.history['categorical_accuracy'], color='blue')\n\nax[1].set_xlabel('epoch')\nax[1].set_ylabel('val_categorical_accuracy')\nax[1].set_ylim(0.8, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that mainly, the val loss is decreasing when the categorical accuracy is increasing. Actually we know that we are overfitting when the loss is decreasing as the categorical accuracy is incresing. Let's check when that is happening.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,len(history.history['val_loss'])):\n    decreasing_loss     = history.history['loss'][i] < history.history['loss'][i-1]\n    decreasing_accuracy = history.history['val_categorical_accuracy'][i] < history.history['val_categorical_accuracy'][i-1]\n    \n    if decreasing_loss and decreasing_accuracy:\n        print(' we overfit at epoch ', i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the more we fit our models, the more we overfit our training dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([DenseNet121(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n                                         weights='imagenet',\n                                         include_top=False),\n                             L.GlobalAveragePooling2D(),\n                             L.Dense(train_labels.shape[1],\n                                     activation='softmax')])\n\nmodel.compile(optimizer='adam',\n              loss = 'categorical_crossentropy',\n              metrics=['categorical_accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n\nax[0].plot(np.linspace(1,EPOCHS,EPOCHS), history.history['val_loss'], color='red')\nax[0].plot(np.linspace(1,EPOCHS,EPOCHS), history.history['loss'], color='blue')\n\nax[0].set_xlabel('epoch')\nax[0].set_ylabel('loss')\nax[0].set_ylim(0, 1.3)\n\n\nax[1].plot(np.linspace(1,EPOCHS,EPOCHS), history.history['val_categorical_accuracy'], color='red')\nax[1].plot(np.linspace(1,EPOCHS,EPOCHS), history.history['categorical_accuracy'], color='blue')\n\nax[1].set_xlabel('epoch')\nax[1].set_ylabel('val_categorical_accuracy')\nax[1].set_ylim(0.8, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### visualisation of what the convnet is learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"img_val = cv2.cvtColor(cv2.imread(valid_paths[7]), cv2.COLOR_BGR2RGB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(cv2.resize(img_val, (409, 273)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.layers[0].input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_outputs = [layer.output for layer in model.layers[0].layers] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activation_model = models.Model(inputs=model.layers[0].input, outputs=layer_outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activations = activation_model.predict(np.array([cv2.resize(img_val, (IMAGE_SIZE, IMAGE_SIZE))]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's display the transformed image after it has been around a third of the convnet ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=7, ncols=4, figsize=(30, 25))\nm = 0\nfor i in range(0,7):\n    for j in range(0,4):\n        ax[i][j].matshow(activations[79][0, :, :, m], cmap='viridis')\n        m = m+1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dispaying more advanced steps in the convnet is difficult as the more we go in depth in the convenet, the more the more the features extracted by the layers become abstract. Futhermore, ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of chhanels are able to specificly detect the infected area ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"val_images = train_data[\"image_id\"][:SAMPLE_LEN].apply(load_image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs_dnn = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = probs_dnn\nsub.to_csv('submission_enn.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}