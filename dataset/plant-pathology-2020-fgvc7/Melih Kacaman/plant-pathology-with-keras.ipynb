{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n\nimport os\nfrom PIL import Image\n\n\nimport IPython.display \nfrom keras.preprocessing.image import array_to_img \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\n\nfrom keras.callbacks import TensorBoard \nfrom time import strftime ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGES_PATH = '../input/plant-pathology-2020-fgvc7/images' \nTRAIN_PATH =  '../input/plant-pathology-2020-fgvc7/train.csv'\nTEST_PATH =  '../input/plant-pathology-2020-fgvc7/test.csv' \n\n\nIMG_WIDTH = IMG_HEIGHT = 300 \nNR_CHANNELS = 3\nTOTAL_INPUTS = NR_CHANNELS * IMG_HEIGHT * IMG_WIDTH\n\nLOG_DIR = '/kaggle/working/tensorboard_plant_logs/'\n\nCOLUMNS = [\"healthy\", \"multiple_diseases\", \"rust\", \"scab\"]\nLABEL_ENCODE = np.array([0,1,2,3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In train set; \n* image_id: the foreign key for the parquet files\n* combinations: one of the target labels\n* healthy: one of the target labels\n* rust: one of the target labels\n* scab: one of the target labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['healthy'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['multiple_diseases'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['rust'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['scab'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get the imgs"},{"metadata":{},"cell_type":"markdown","source":"* As you can see, we have large images. \n* We can't load all of them in main memory. \n* There is several ways to handle this problem but we just resize it. \n* For now its size is going to be fixed, at the end we can arrange it again. "},{"metadata":{"trusted":true},"cell_type":"code","source":"ex_image = os.path.join(IMAGES_PATH, train.image_id[0] + '.jpg')\nex_image = np.asanyarray(Image.open(ex_image)) \nex_image.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_images(dataset, folder): \n    arr = [] \n    for imageName in dataset:\n        path = os.path.join(folder, imageName + '.jpg') \n        img = Image.open(path) \n        img = img.resize((300, 300)) \n        arr.append(np.asanyarray(img)) \n        img.close() \n    \n    return arr ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_all = np.array(load_images(train.image_id, IMAGES_PATH)) \ntrain_images_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images_all = np.array(load_images(test.image_id, IMAGES_PATH))\ntest_images_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get target from train \ntrain_all_y = train.iloc[:, 1:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data for validation set and flatten data \nx_train = train_images_all[:1638] \nx_train = x_train.reshape(x_train.shape[0], -1)\ny_train = train_all_y[:1638] \ny_train = np.dot(y_train, LABEL_ENCODE.T) # to label encoding\n\nx_val = train_images_all[1638:] # %10 for validation set \nx_val = x_val.reshape(x_val.shape[0], -1)\ny_val = train_all_y[1638:]\ny_val = np.dot(y_val, LABEL_ENCODE.T) # to label encoding\n\nprint(\"x_val shape: \" , x_val.shape) \nprint(\"x_train shape: \",x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7)) \nfor i in range(1,10): \n    plt.subplot(1, 10, i) \n    plt.yticks([])\n    plt.xticks([]) \n    plt.imshow(train_images_all[i]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-process Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train_images_all[0][0][0][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_all, test_images_all = train_images_all / 255.0, test_images_all / 255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(train_images_all[0][0][0][0])) \nprint(train_images_all[0][0][0][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Flat Images to one single vector \ntrain_images_all = train_images_all.reshape(len(train_images_all), TOTAL_INPUTS)\ntest_images_all = test_images_all.reshape(len(test_images_all), TOTAL_INPUTS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_all.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define the Neural Network Using Keras "},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1 = Sequential([\n    Dense(units=32, input_dim=TOTAL_INPUTS, activation='relu'),  \n    Dense(units=16, activation='relu'), \n    Dense(units=4, activation='softmax') \n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(model_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) \nmodel_1.summary() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tensorboard"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tensorboard(model_name): \n    folder_name = f'{model_name} at {strftime(\"%H %M\")}'\n    dir_paths = os.path.join(LOG_DIR, folder_name) \n    \n    try: \n        os.makedirs(dir_paths) \n    except OSError as err:\n        print(err.strerror)\n    else: \n        print('Succesfully created.') \n    \n    return TensorBoard(log_dir=dir_paths), dir_paths ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit the Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"samples_per_batch = 200 \nnr_epoch = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \ntensorboard, logdir = get_tensorboard('model_1') \nhistory_model_1 = model_1.fit(x_train, y_train, batch_size=samples_per_batch, validation_data=(x_val, y_val), epochs= nr_epoch, \n           callbacks=[tensorboard], verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_model_hist(train_loss, train_accuracy, val_loss, val_accuracy): \n    \n    fig, axs = plt.subplots(2, 2)\n    fig.set_figheight(15)\n    fig.set_figwidth(15)\n    \n    axs[0,0].set_xlabel('Nr of Epochs', fontsize=18)\n    axs[0,0].set_ylabel('Train Loss', fontsize=18)\n    axs[0,0].plot(train_loss,color='green')\n\n    axs[0,1].set_xlabel('Nr of Epochs', fontsize=18)\n    axs[0,1].set_ylabel('Train Accuracy', fontsize=18)\n    axs[0,1].plot(train_accuracy, color='green') \n    \n    axs[1,0].set_xlabel('Nr of Epochs', fontsize=18)\n    axs[1,0].set_ylabel('Val Loss', fontsize=18)\n    axs[1,0].plot(val_loss,color='magenta')\n\n    axs[1,1].set_xlabel('Nr of Epochs', fontsize=18)\n    axs[1,1].set_ylabel('Val Accuracy', fontsize=18)\n    axs[1,1].plot(val_accuracy, color='magenta') \n    \n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history_model_1.history['loss'] \naccs = history_model_1.history['accuracy'] \n\nval_loss = history_model_1.history['val_loss']\nval_accuracy = history_model_1.history['val_accuracy']\n\ndisplay_model_hist(loss, accs, val_loss, val_accuracy) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Try early stopping, because we have obtained high accuracy and low loss previous epoch.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"nr_epoch = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \ntensorboard, logdir = get_tensorboard('model_2') \nhistory_model_2 = model_1.fit(x_train, y_train, batch_size=samples_per_batch, validation_data=(x_val, y_val), epochs= nr_epoch, \n           callbacks=[tensorboard], verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history_model_2.history['loss'] \naccs = history_model_2.history['accuracy'] \n\nval_loss = history_model_2.history['val_loss']\nval_accuracy = history_model_2.history['val_accuracy']\n\ndisplay_model_hist(loss, accs, val_loss, val_accuracy) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_3 = Sequential([\n    Dropout(0.2, seed=42, input_shape=(TOTAL_INPUTS,)),\n    Dense(units=64, activation='relu'),  \n    Dense(units=32, activation='relu'),\n    Dense(units=16, activation='relu'), \n    Dense(units=4, activation='softmax') \n])\n\nmodel_3.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) \nmodel_3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \ntensorboard, logdir = get_tensorboard('model_3') \nhistory_model_3 = model_3.fit(x_train, y_train, batch_size=samples_per_batch, validation_data=(x_val, y_val), epochs= nr_epoch, \n           callbacks=[tensorboard], verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history_model_3.history['loss'] \naccs = history_model_3.history['accuracy'] \n\nval_loss = history_model_3.history['val_loss']\nval_accuracy = history_model_3.history['val_accuracy']\n\ndisplay_model_hist(loss, accs, val_loss, val_accuracy) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction and Evaluate Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ph = np.expand_dims(test_images_all[0], axis=0)\ntest_ph.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_3.predict(test_ph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nresult = model_3.predict(test_images_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(columns=train.columns[1:], data=result)\nsubmission = pd.concat([test, submission], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head() ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}