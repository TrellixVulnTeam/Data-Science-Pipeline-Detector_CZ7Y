{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport cv2\nimport albumentations\nimport albumentations.pytorch\nimport os\nimport PIL\nimport copy\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Install pytorchtrainutils and efficientnet-pytorch","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Very simple yet useful set of util functions for training models in pytorch https://github.com/carloalbertobarbano/pytorch-train-utils","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install --upgrade git+git://github.com/carloalbertobarbano/pytorch-train-utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from pytorchtrainutils import trainer\nfrom pytorchtrainutils import metrics\nfrom pytorchtrainutils import utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utilities functions\nUnhide code cell","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_cm(logs):\n    accs = logs['top1-acc']\n    cms = logs['cm']\n\n    classes = ['healthy', 'multiple_diseases', 'rust', 'scab']\n\n    plt.figure(figsize=(20, 3))\n    plt.suptitle(f'CM')\n    for class_idx, class_name in enumerate(classes):\n        plt.subplot(1, 4, class_idx+1)\n        plt.title(f'{class_name}')\n        ax = sns.heatmap(\n            cms.class_cm(class_idx, normalized=True), \n            annot=True, fmt=\".2f\", vmin=0., vmax=1.\n        )\n\n    plt.savefig('cm.png')\n    plt.show()\n\n    plt.figure(figsize=(20, 3))\n    plt.suptitle(f'CM (best threshold)')\n    for class_idx, class_name in enumerate(classes):\n        plt.subplot(1, 4, class_idx+1)\n        plt.title(f'{class_name}')\n\n        best_threshold = accs.get_best_threshold(class_idx)\n        ax = sns.heatmap(\n            cms.class_cm(class_idx, normalized=True, threshold=best_threshold), \n            annot=True, fmt=\".2f\", vmin=0., vmax=1.\n        )\n\n    plt.savefig('cm-t')\n    plt.show()\n    \n\ndef plot_roc_auc(logs):\n    aucs = logs['col-auc']\n    accs = logs['top1-acc']\n    classes = ['healthy', 'multiple_diseases', 'rust', 'scab']\n\n    plt.figure(figsize=(20, 5))\n    plt.suptitle(f'Classification report - average AUC: {aucs.get():.4f}')\n    for class_idx, class_name in enumerate(classes):\n        plt.subplot(1, 4, class_idx+1)\n        plt.title(f'{class_name} BA={accs.class_ba(class_idx):.4f}')\n        fpr, tpr, _ = aucs.class_curve(class_idx)\n        plt.plot(fpr, tpr, label=f'AUC: {aucs.class_auc(class_idx):.4f}')\n        plt.plot([0, 1], [0, 1], color='navy', lw=0.5, linestyle='--')\n        plt.legend(loc='lower right')\n    plt.savefig('auc.png')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparams & values","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"seed = 42\nutils.set_seed(seed)\ndevice = torch.device('cuda')\n\nlr = 1e-2\nbatch_size = 8\nn_epochs_224 = 30\nn_epochs_448 = 20\n\narch = 'resnet18'\n\nmean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225] #Imagenet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loading\n\nI converted the competition dataset to pickles (.npy). \nImages are already downscaled to 600x600. You can find the dataset (along with the generating script) [here](https://www.kaggle.com/carloalbertobarbano/plantpathology2020fgvc7pickles).\nThe speedup is noticeable.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"!ls /kaggle/input","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset_path = '/kaggle/input/plantpathology2020fgvc7pickles/plant-pathology-2020-fgvc7-pickles'\ntrain_df = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(dataset_path, 'test.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def preprocess_df(df):\n    df['label'] =  df.multiple_diseases * 2 + \\\n                df.rust * 3 + \\\n                df.scab * 4 + \\\n                df.healthy\n    df.label -= 1\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = preprocess_df(train_df)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.iloc[:, 1:].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see the *multiple_diseases* class is very unbalanced, so we will apply weights to the loss function. Undersampling of the other classes is not really an option here","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"class PlantDataset(torch.utils.data.dataset.Dataset):\n    def __init__(self, df, path, transform):\n        super().__init__()\n\n        self.df = df\n        self.path = os.path.join(path, 'images')\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        entry = self.df.iloc[index]\n        fname = entry.image_id + '.npy'\n        fname = os.path.join(self.path, fname)\n\n        img = np.load(fname)\n        img = self.transform(img)\n\n        return img, entry.values[1:5].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=seed, stratify=train_df.label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll treat this task as a multilabel classification problem; even if the given data is actually a single-class classification. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_weight = val_weight = torch.tensor([\n    [1., 1.],\n    [1.5, 2.],\n    [1., 1.],\n    [1., 1.]\n]).to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will also apply a slight oversampling on the minority class, by giving it a higher weight.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"class_weights = torch.tensor([1., 1.3, 1.1, 1.])\nsampler_weights = class_weights[train_df.label.values]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data transforms & aug","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Applying TTA with albumentations and [pytorchtrainutils](https://github.com/carloalbertobarbano/pytorch-train-utils) is very easy","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def stack_image(image, **kwargs):\n    to_tensor = albumentations.pytorch.ToTensor()\n    vflip = albumentations.VerticalFlip(always_apply=True)\n    hflip = albumentations.HorizontalFlip(always_apply=True)\n    \n    return torch.stack([\n        to_tensor(image=vflip(image=image)['image'])['image'],\n        to_tensor(image=hflip(image=image)['image'])['image'],\n        to_tensor(image=vflip(image=hflip(image=image)['image'])['image'])['image'],\n        to_tensor(image=image)['image']\n    ])\n\ndef get_transform(img_size, crop_size):\n    train_transform = albumentations.Compose([\n        albumentations.Resize(img_size, img_size, always_apply=True),\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.VerticalFlip(p=0.5),\n        albumentations.ShiftScaleRotate(rotate_limit=30.0, scale_limit=0.2, shift_limit=0.15, p=0.7),\n        albumentations.CenterCrop(crop_size, crop_size, always_apply=True),\n        albumentations.Normalize(mean, std),\n        albumentations.pytorch.ToTensor()\n    ])\n\n    transform = albumentations.Compose([\n        albumentations.Resize(img_size, img_size, always_apply=True),\n        albumentations.CenterCrop(crop_size, crop_size, always_apply=True),\n        albumentations.Normalize(mean, std),\n        albumentations.pytorch.ToTensor()\n    ])\n\n    tta_transform = albumentations.Compose([\n        albumentations.Resize(img_size, img_size, always_apply=True),\n        albumentations.CenterCrop(crop_size, crop_size, always_apply=True),\n        albumentations.Normalize(mean, std),\n        albumentations.Lambda(stack_image, always_apply=True) \n    ])\n    \n    lambda_train = lambda image: train_transform(image=image)['image']\n    lambda_valid = lambda image: transform(image=image)['image']\n    lambda_tta = lambda image: tta_transform(image=image)['image']\n    \n    return lambda_train, lambda_valid, lambda_tta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model creation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We actually don't need a very large or sophisticated model to reach LB ~0.963. A resnet18 is enough, though you can probably do better with a slightly bigger model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"model = torchvision.models.resnet18(pretrained=True)\nnum_ft = model.fc.in_features\nmodel.fc = torch.nn.Linear(in_features=num_ft, out_features=4, bias=True)\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class Softmaxer(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        \n    def forward(self, x):\n        x = self.model(x)\n        return F.softmax(x, dim=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss function","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The loss function is a normal binary cross entropy (with logits), we only need some indexing magic to correctly apply the weight","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def bce(preds, targets, weight=None):    \n    loss = F.binary_cross_entropy_with_logits(preds, targets.type(preds.dtype), reduction='none')\n    if weight is not None:\n        weight = weight[:, targets.T.long()]\n        idx = np.diag_indices(weight.shape[0])\n        weight = weight[idx[0], idx[1], :]\n        loss *= weight.T\n    return loss.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train on 224","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"img_size = 250\ncrop_size = 224\nname = f'{arch}-{crop_size}'\nprint(f'{name} image size: {img_size}, crop size: {crop_size}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_transform, valid_transform, tta_transform = get_transform(img_size=img_size, crop_size=crop_size)\n\ntrain_dataset = PlantDataset(train_df, dataset_path, train_transform)\ntrain_sampler = torch.utils.data.sampler.WeightedRandomSampler(sampler_weights, len(train_df))\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, num_workers=4, \n    sampler=train_sampler, shuffle=False\n)\n\nval_dataset = PlantDataset(val_df, dataset_path, valid_transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, num_workers=4, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"criterion = bce\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=1e-5)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pytorchtrainutils provides utilities functions and classes (i.e.) metrics","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"tracked_metrics = [\n    metrics.MultilabelAccuracy(),\n    metrics.MultilabelRocAuc(),\n    metrics.MultilabelConfusionMatrix()\n]\n\nbest_model = trainer.fit(\n    model, train_dataloader=train_loader, val_dataloader=val_loader,\n    test_dataloader=None, test_every=0, criterion=criterion,\n    optimizer=optimizer, scheduler=lr_scheduler, metrics=tracked_metrics, n_epochs=n_epochs_224,\n    metric_choice='col-auc', mode='max',\n    name=name, device=device, weight={'train': train_weight, 'val': val_weight}\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how the best model does on the validation set, with 4xTTA.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"tracked_metrics = [\n    metrics.MultilabelAccuracy(metric='top1-acc', apply_sigmoid=False),\n    metrics.MultilabelRocAuc(apply_sigmoid=False),\n    metrics.MultilabelConfusionMatrix()\n]\n\nsoftmaxer = Softmaxer(model)\nsoftmaxer_best = Softmaxer(best_model)\n\nval_dataset = PlantDataset(val_df, dataset_path, tta_transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, num_workers=4, shuffle=False)\n\nval_logs = trainer.test(softmaxer, criterion=criterion, test_dataloader=val_loader, metrics=tracked_metrics, weight=val_weight, device=device, tta=True)\nbest_val_logs = trainer.test(softmaxer_best, criterion=criterion, test_dataloader=val_loader, metrics=tracked_metrics, weight=val_weight, device=device, tta=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'Final {name} val:', trainer.summarize_metrics(val_logs))\nprint(f'Best {name} val:', trainer.summarize_metrics(best_val_logs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ax = sns.heatmap(val_logs['cm'].get(normalized=True), annot=True, fmt=\".2f\", vmin=0., vmax=1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ax = sns.heatmap(best_val_logs['cm'].get(normalized=True), annot=True, fmt=\".2f\", vmin=0., vmax=1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_cm(best_val_logs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_roc_auc(best_val_logs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train on 448","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's increase the resolution up to 448 and apply transfer learning","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"img_size *= 2\ncrop_size *= 2\nname = f'{arch}-{crop_size}'\nprint(f'{name} image size: {img_size}, crop size: {crop_size}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_transform, valid_transform, tta_transform = get_transform(img_size=img_size, crop_size=crop_size)\n\ntrain_dataset = PlantDataset(train_df, dataset_path, train_transform)\ntrain_sampler = torch.utils.data.sampler.WeightedRandomSampler(sampler_weights, len(train_df))\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, num_workers=4, \n    sampler=train_sampler, shuffle=False\n)\n\nval_dataset = PlantDataset(val_df, dataset_path,  valid_transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, num_workers=4, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"criterion = bce\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=1e-5)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"tracked_metrics = [\n    metrics.MultilabelAccuracy(apply_sigmoid=True),\n    metrics.MultilabelRocAuc(apply_sigmoid=True),\n    metrics.MultilabelConfusionMatrix()\n]\n\nbest_model = trainer.fit(\n    model, train_dataloader=train_loader, val_dataloader=val_loader,\n    test_dataloader=None, test_every=0, criterion=criterion,\n    optimizer=optimizer, scheduler=lr_scheduler, metrics=tracked_metrics, n_epochs=n_epochs_448,\n    metric_choice='col-auc', mode='max',\n    name=name, device=device, weight={'train': train_weight, 'val': val_weight}\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tracked_metrics = [\n    metrics.MultilabelAccuracy(metric='top1-acc', apply_sigmoid=False),\n    metrics.MultilabelRocAuc(apply_sigmoid=False),\n    metrics.MultilabelConfusionMatrix()\n]\n\nsoftmaxer = Softmaxer(model)\nsoftmaxer_best = Softmaxer(best_model)\n\nval_dataset = PlantDataset(val_df, dataset_path, tta_transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, num_workers=4, shuffle=False)\n\nval_logs = trainer.test(softmaxer, criterion=criterion, test_dataloader=val_loader, metrics=tracked_metrics, weight=val_weight, device=device, tta=True)\nbest_val_logs = trainer.test(softmaxer_best, criterion=criterion, test_dataloader=val_loader, metrics=tracked_metrics, weight=val_weight, device=device, tta=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'Final {name} val:', trainer.summarize_metrics(val_logs))\nprint(f'Best {name} val:', trainer.summarize_metrics(best_val_logs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ax = sns.heatmap(val_logs['cm'].get(normalized=True), annot=True, fmt=\".2f\", vmin=0., vmax=1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ax = sns.heatmap(best_val_logs['cm'].get(normalized=True), annot=True, fmt=\".2f\", vmin=0., vmax=1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_cm(best_val_logs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_roc_auc(best_val_logs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference & submission","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"test_dataset = PlantDataset(test_df, dataset_path, tta_transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_test_preds(model, test_loader):\n    outputs = []\n    for batch_idx, (data, labels) in enumerate(tqdm(test_loader)):\n        batch_size, n_crops, c, h, w = data.size()\n        data = data.view(-1, c, h, w)\n        with torch.no_grad():\n            output = model(data.to(device))\n        output = output.view(batch_size, n_crops, -1).mean(1)\n        outputs.append(output.cpu())\n\n    return torch.cat(outputs, dim=0).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def make_submission_df(df, preds):\n    classes = ['healthy', 'multiple_diseases', 'rust', 'scab']\n    for idx, name in enumerate(classes):\n        df[name] = pd.Series(preds[:, idx])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"softmaxer.eval()\ntest_preds = get_test_preds(softmaxer, test_loader)\ntest_submission = make_submission_df(test_df, test_preds)\ntest_submission.to_csv(f'submission-{name}.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"softmaxer_best.eval()\ntest_preds = get_test_preds(softmaxer_best, test_loader)\ntest_submission = make_submission_df(test_df, test_preds)\ntest_submission.to_csv(f'submission-best-{name}.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.read_csv(f'submission-{name}.csv').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.read_csv(f'submission-best-{name}.csv').head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}