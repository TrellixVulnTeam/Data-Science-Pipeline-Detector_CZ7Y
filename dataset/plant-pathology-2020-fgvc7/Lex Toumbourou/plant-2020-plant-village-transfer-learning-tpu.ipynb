{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Plant Pathology 2020\n\nThis kernel attempts to improve upon the current [highest scoring single model public kernel](https://www.kaggle.com/ateplyuk/fork-of-plant-2020-tpu-915e9c) by pretraining on the Plant Village Apple dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from pathlib import Path\nfrom functools import partial\n\nimport gc\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport os\nimport random, re, math\nimport tensorflow as tf, tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import load_model\nfrom kaggle_datasets import KaggleDatasets\nimport efficientnet.tfkeras as efn\nfrom PIL import Image\nfrom keras.backend.tensorflow_backend import clear_session\n\nSEED = 420\n\ndef decode_image(filename, label=None, image_size=None):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    if image_size:\n        image = tf.image.resize(image, image_size)\n\n    if label is None:\n        return image\n    else:\n        return image, label\n\n\ndef data_augment(image, label=None, seed=SEED):\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.random_flip_up_down(image, seed=seed)\n           \n    if label is None:\n        return image\n    else:\n        return image, label\n\n    \ndef show_batch(ds, labels):\n    row = 6; col = 4;\n    row = min(row, BATCH_SIZE//col)\n\n    for (img, label) in ds:\n        plt.figure(figsize=(15,int(15*row/col)))\n        for j in range(row*col):\n            plt.subplot(row,col,j+1)\n            plt.axis('off')\n            plt.imshow(img[j,])\n            plt.title(labels[label[j,].numpy().argmax()])\n        plt.show()\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\ndef get_strategy():\n    # Detect hardware, return appropriate distribution strategy\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    return strategy\n\nstrategy = get_strategy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plant Village"},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"LABELS = [\n    'healthy', 'Cedar_apple_rust', 'Black_rot', 'Apple_scab', \n]\n\nIMG_PATH = Path('../input/plantvillageapplecolor')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"all_img_ids = []\nall_img_labels = []\nfor l in LABELS:\n    img_ids = [\n        '/'.join(l.parts[-2:])\n        for l in (IMG_PATH / ('Apple___' + l)).iterdir()\n    ]\n    all_img_ids.extend(img_ids)\n    all_img_labels.extend([l] * len(img_ids))\n\ntrain_df = pd.DataFrame({'img_id': all_img_ids, 'img_label': all_img_labels})\ntrain_df.img_label = pd.Categorical(train_df.img_label, categories=LABELS, ordered=False)\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df.img_label)], axis=1)\ntrain_df = train_df.sample(frac=1., random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.img_label.value_counts().plot.bar(title=\"Plant village label distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hs, ws = [], []\nfor path in tqdm(all_img_ids, total=len(all_img_ids)):\n    img = Image.open(Path('../input/plantvillageapplecolor')/path)\n    h, w = img.size\n    hs.append(h)\n    ws.append(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, axes = plt.subplots(ncols=2, nrows=1, constrained_layout=True, figsize=(10, 3))\nfor ax, column, vals in zip(axes, ['heights', 'widths'], [hs, ws]):\n    ax.hist(vals, bins=100)\n    ax.set_title(f'{column} distribution')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pretrain"},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_CLASSES = 4\nIMG_SIZE = 256\nEPOCHS = 40\nBATCH_SIZE = 128 * strategy.num_replicas_in_sync\nN_FOLDS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('plantvillageapplecolor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_df[LABELS].values.astype(np.int64)\ntrain_paths = train_df.img_id.apply(lambda x: GCS_DS_PATH + '/' + x).values\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(partial(decode_image, image_size=(IMG_SIZE, IMG_SIZE)), num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(SEED)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_batch(train_dataset, LABELS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_START = 0.00001\nLR_MAX = 0.0001 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 15\nLR_SUSTAIN_EPOCHS = 3\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n\ndef get_model():\n    # Note the input shape needs to be a variable dimension: (None, None, 3).\n    base_model =  efn.EfficientNetB7(weights='imagenet', include_top=False, pooling='avg', input_shape=(None, None, 3))\n    x = base_model.output\n    predictions = Dense(NB_CLASSES, activation=\"softmax\")(x)\n    return Model(inputs=base_model.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strategy = get_strategy()\n\nwith strategy.scope():\n    model = get_model()\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=[tf.keras.metrics.AUC()])\n\nmodel.fit(\n    train_dataset,\n    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,\n    callbacks=[tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)],\n    epochs=EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"plant_village_pretrain_b7.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clear_session()\n\ndel model\ndel train_dataset\ndel train_labels\ndel strategy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transfer learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"strategy = get_strategy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_CLASSES = 4\nIMG_SIZE = 768\nEPOCHS = 40\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nLABEL_COLS = ['healthy', 'multiple_diseases', 'rust', 'scab']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('plant-pathology-2020-fgvc7')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/plant-pathology-2020-fgvc7/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsub = pd.read_csv(path + 'sample_submission.csv')\n\ntrain_paths = train.image_id.apply(lambda x: GCS_DS_PATH + '/images/' + x + '.jpg').values\ntest_paths = test.image_id.apply(lambda x: GCS_DS_PATH + '/images/' + x + '.jpg').values\n\ntrain_labels = train.loc[:, 'healthy':].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(partial(decode_image, image_size=(IMG_SIZE, IMG_SIZE)), num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(SEED)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(partial(decode_image, image_size=(IMG_SIZE, IMG_SIZE)), num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_batch(train_dataset, LABEL_COLS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\ndef get_model():\n    base_model = load_model('./plant_village_pretrain_b7.h5', compile=False)\n    x = base_model.layers[-2].output\n    predictions = Dense(NB_CLASSES, activation=\"softmax\")(x)\n    return Model(inputs=base_model.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = get_model()\n    \nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprobs = model.predict(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[:, 'healthy':] = probs\nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}