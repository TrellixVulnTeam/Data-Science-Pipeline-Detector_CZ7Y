{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Plant Pathology 2020\n\n## Adversial Validation\n\nSince it appears quite common to find leaderboard scores that are significantly lower than validation, this notebook explores the test set, trying to understand what makes it different from the training set.\n\nI use fastai2 for the model training again, forking the notebook from my [initial fastai2 experiment](https://www.kaggle.com/lextoumbourou/plant-pathology-2020-eda-training-fastai2)."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/fastai/fastcore > /dev/null\n!pip install git+https://github.com/fastai/fastai2 > /dev/null\n!pip install iterative-stratification > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nimport os\nimport pandas as pd\nimport sys\nfrom collections import Counter\nfrom pathlib import Path\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom tqdm.notebook import tqdm\nfrom torchvision.models import densenet121\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data.sampler import WeightedRandomSampler\n\nfrom fastai2.basics import *\nfrom fastai2.callback.all import *\nfrom fastai2.vision.all import *\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Params"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = Path('../input/plant-pathology-2020-fgvc7')\nIMG_PATH = DATA_PATH / 'images'\nLABEL_COLS = ['healthy', 'multiple_diseases', 'rust', 'scab']\n\nIMG_SIZE = 448\nSEED = 420\nN_FOLDS = 5\nBS = 16\nN_FOLDS = 5\n\nARCH = densenet121","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/plant-pathology-2020-fgvc7/train.csv')\ntest_df = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore the test set"},{"metadata":{},"cell_type":"markdown","source":"Firstly let's understand how the test set distribution compares to the train distribution. I'll use the predictions from the current [highest scoring kernel](https://www.kaggle.com/seefun/ensemble-top-kernels)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['label'] = train_df[['healthy', 'multiple_diseases', 'rust', 'scab']].idxmax(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_csv = pd.read_csv('../input/ensemble-top-kernels/submission.csv')\nsubmission_csv['label'] = submission_csv[['healthy', 'multiple_diseases', 'rust', 'scab']].idxmax(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes = axes.flatten()\ntrain_df.label.value_counts().plot.bar(ax=axes[0], title='Train')\nsubmission_csv.label.value_counts().plot.bar(ax=axes[1], title='Test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the major difference seems to be a higher frequency of `healthy` occurances in the train set."},{"metadata":{},"cell_type":"markdown","source":"## Adversarial Validation"},{"metadata":{},"cell_type":"markdown","source":"I'm going to train a model to classify whether the example came from the train or test set. If the distribution in the train and test set is exactly the same, we expect an ROC of about 0.5. Any higher than that suggests that there is something quite different about the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['is_test'] = False\ntest_df['is_test'] = True\n\nall_df = pd.concat([\n    train_df[['image_id', 'is_test']], test_df[['image_id', 'is_test']]]\n).reset_index(drop=True).sample(frac=1., random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df.is_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datablock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    getters=[\n        ColReader('image_id', pref=IMG_PATH, suff='.jpg'), ColReader('is_test')\n    ],\n    splitter=RandomSplitter(seed=SEED),\n    item_tfms=Resize(IMG_SIZE),\n    batch_tfms=aug_transforms(size=IMG_SIZE, max_rotate=30., min_scale=0.75, flip_vert=True, do_flip=True)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls = datablock.dataloaders(source=all_df, bs=BS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_learner(dls, lr=1e-3):\n    opt_func = partial(Adam, lr=lr, wd=0.01, eps=1e-8)\n\n    learn = cnn_learner(\n        dls, ARCH, opt_func=opt_func,\n        metrics=[RocAuc()]).to_fp16()\n\n    return learn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = get_learner(dls)\nlearn.fit_one_cycle(1)\nlearn.unfreeze()\nlearn.fit_one_cycle(4, slice(1e-4, 1e-3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, metric = learn.validate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final score"},{"metadata":{"trusted":true},"cell_type":"code","source":"metric","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC is pretty average - that's a good sign. It indicates the train and test set are pretty similar. However, it's not 0.5, so there does appear to be some distribution difference between the 2 sets."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}