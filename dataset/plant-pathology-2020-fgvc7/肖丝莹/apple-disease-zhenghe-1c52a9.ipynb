{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import datetime\ntime1 = datetime.datetime.now()\nprint(time1)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.406309Z","iopub.execute_input":"2021-09-08T13:08:38.406675Z","iopub.status.idle":"2021-09-08T13:08:38.411977Z","shell.execute_reply.started":"2021-09-08T13:08:38.406643Z","shell.execute_reply":"2021-09-08T13:08:38.411081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.metrics import categorical_accuracy\n\nfrom tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n                                        ModelCheckpoint, CSVLogger, LearningRateScheduler)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.413869Z","iopub.execute_input":"2021-09-08T13:08:38.414528Z","iopub.status.idle":"2021-09-08T13:08:38.425455Z","shell.execute_reply.started":"2021-09-08T13:08:38.414382Z","shell.execute_reply":"2021-09-08T13:08:38.424623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_HEIGHT = 224\nIMAGE_WIDTH = 224\nIMAGE_CHANNELS = 3\nTRAIN_BATCH_SIZE = 8\nVAL_BATCH_SIZE = 5\nEPOCHS = 50\nBALANCE = False #True  False\nAUG = False #True  False\nLR = 0.005#0.001  0.005  0.01\nH5_FILE_NAME = '_'.join(['model',\n                        'balance' if BALANCE else 'nobalance',\n                        'aug' if AUG else 'noaug',\n                        str(LR)]) + '.h5'\nprint(H5_FILE_NAME)\nTRAINING_LOG_FILE_NAME = '_'.join(['training_log',\n                        'balance' if BALANCE else 'nobalance',\n                        'aug' if AUG else 'noaug',\n                        str(LR)]) + '.csv'\nprint(TRAINING_LOG_FILE_NAME)\nSUBMISSION_FILE_NAME = '_'.join(['submission',\n                        'balance' if BALANCE else 'nobalance',\n                        'aug' if AUG else 'noaug',\n                        str(LR)]) + '.csv'\nprint(SUBMISSION_FILE_NAME)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.427832Z","iopub.execute_input":"2021-09-08T13:08:38.428072Z","iopub.status.idle":"2021-09-08T13:08:38.440812Z","shell.execute_reply.started":"2021-09-08T13:08:38.428048Z","shell.execute_reply":"2021-09-08T13:08:38.439881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/plant-pathology-2020-fgvc7/train.csv'\ndf_train_all = pd.read_csv(path)\n\npath = '../input/plant-pathology-2020-fgvc7/test.csv'\ndf_test = pd.read_csv(path)\n\npath = '../input/plant-pathology-2020-fgvc7/sample_submission.csv'\ndf_sample = pd.read_csv(path)\n\n\nprint(df_train_all.shape)\nprint(df_test.shape)\nprint(df_sample.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.442383Z","iopub.execute_input":"2021-09-08T13:08:38.442703Z","iopub.status.idle":"2021-09-08T13:08:38.471421Z","shell.execute_reply.started":"2021-09-08T13:08:38.442663Z","shell.execute_reply":"2021-09-08T13:08:38.470503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify the target class of each row in the train set\n\ndef get_class(row):\n    \n    if row['multiple_diseases'] == 1:\n        return 'multiple_diseases'\n    \n    elif row['rust'] == 1:\n        return 'rust'\n    \n    elif row['scab'] == 1:\n        return 'scab'\n    \n    else:\n        return 'healthy'\n    \ndf_train_all['target'] = df_train_all.apply(get_class, axis=1)\n\ndf_train_all.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.472766Z","iopub.execute_input":"2021-09-08T13:08:38.473181Z","iopub.status.idle":"2021-09-08T13:08:38.520709Z","shell.execute_reply.started":"2021-09-08T13:08:38.473115Z","shell.execute_reply":"2021-09-08T13:08:38.519982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_all['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.521853Z","iopub.execute_input":"2021-09-08T13:08:38.522185Z","iopub.status.idle":"2021-09-08T13:08:38.529631Z","shell.execute_reply.started":"2021-09-08T13:08:38.522137Z","shell.execute_reply":"2021-09-08T13:08:38.528776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle\ndf_train_all_shuffle = shuffle(df_train_all, random_state=101)\n# select the column that we will use for stratification\ny = df_train_all_shuffle['target']\n\ndf_train, df_val = train_test_split(df_train_all_shuffle, test_size=0.2, random_state=101, stratify=y)\n\n\nprint(df_train.shape)\nprint(df_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.531764Z","iopub.execute_input":"2021-09-08T13:08:38.532329Z","iopub.status.idle":"2021-09-08T13:08:38.546551Z","shell.execute_reply.started":"2021-09-08T13:08:38.532292Z","shell.execute_reply":"2021-09-08T13:08:38.545491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.547851Z","iopub.execute_input":"2021-09-08T13:08:38.548213Z","iopub.status.idle":"2021-09-08T13:08:38.556697Z","shell.execute_reply.started":"2021-09-08T13:08:38.548177Z","shell.execute_reply":"2021-09-08T13:08:38.555622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.596497Z","iopub.execute_input":"2021-09-08T13:08:38.596769Z","iopub.status.idle":"2021-09-08T13:08:38.604731Z","shell.execute_reply.started":"2021-09-08T13:08:38.596745Z","shell.execute_reply":"2021-09-08T13:08:38.603684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_balancer(df_train):\n    df_1 = df_train[df_train['target'] != 'multiple_diseases']\n    df_2 = df_train[df_train['target'] == 'multiple_diseases']\n    df_train_up = pd.concat([df_1, df_2,  df_2,  df_2,  df_2,  df_2,  df_2], axis=0).reset_index(drop=True)\n\n    df_train_ret = shuffle(df_train_up, random_state=101)\n    return df_train_ret","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.606551Z","iopub.execute_input":"2021-09-08T13:08:38.607034Z","iopub.status.idle":"2021-09-08T13:08:38.613859Z","shell.execute_reply.started":"2021-09-08T13:08:38.607Z","shell.execute_reply":"2021-09-08T13:08:38.612751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the new class distribution of the train set\nif BALANCE:\n    df_train = train_balancer(df_train)\ndf_train['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.615959Z","iopub.execute_input":"2021-09-08T13:08:38.616481Z","iopub.status.idle":"2021-09-08T13:08:38.627071Z","shell.execute_reply.started":"2021-09-08T13:08:38.616395Z","shell.execute_reply":"2021-09-08T13:08:38.625976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.to_csv('df_train.csv.gz', compression='gzip', index=False)\ndf_val.to_csv('df_val.csv.gz', compression='gzip', index=False)\ndf_test.to_csv('df_test.csv.gz', compression='gzip', index=False)\n!ls","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:38.628908Z","iopub.execute_input":"2021-09-08T13:08:38.629453Z","iopub.status.idle":"2021-09-08T13:08:39.347434Z","shell.execute_reply.started":"2021-09-08T13:08:38.629417Z","shell.execute_reply":"2021-09-08T13:08:39.34642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Albumentations\n\nimport albumentations as albu\n\n\ndef augment_image(augmentation, image):\n    \n    \"\"\"\n    Uses the Albumentations library.\n    \n    Inputs: \n    1. augmentation - this is the instance of type of augmentation to do \n    e.g. aug_type = HorizontalFlip(p=1) \n    # p=1 is the probability of the transform being executed.\n    \n    2. image - image with shape (h,w)\n    \n    Output:\n    Augmented image as a numpy array.\n    \n    \"\"\"\n    # get the transform as a dict\n    aug_image_dict =  augmentation(image=image)\n    # retrieve the augmented matrix of the image\n    image_matrix = aug_image_dict['image']\n    \n    \n    return image_matrix","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.350684Z","iopub.execute_input":"2021-09-08T13:08:39.350955Z","iopub.status.idle":"2021-09-08T13:08:39.357281Z","shell.execute_reply.started":"2021-09-08T13:08:39.350927Z","shell.execute_reply":"2021-09-08T13:08:39.356502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the transforms\n\n# Modified from --> Pneumothorax - 1st place solution\n# Source: https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/discussion/107824#latest-620521\n\n\naug_types1 = albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, \n                  interpolation=1, border_mode=4, value=None, mask_value=None, \n                  shift_limit_x=None, shift_limit_y=None, always_apply=False, \n                  p=1)\n\naug_types2 = albu.Flip(p=1)\n\naug_types3 = albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2,\n                                           brightness_by_max=True, always_apply=False,p=1)\n\naug_types4 = albu.Blur(blur_limit=(3,3.5), always_apply=False, p=1)\n\naug_types5 = albu.OneOf([\n                albu.ElasticTransform(alpha=1, sigma=50, alpha_affine=50,\n                                       interpolation=1, border_mode=4, value=None,mask_value=None,\n                                       always_apply=False, approximate=False, p=1),\n                albu.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=1, border_mode=4, \n                                 value=None, mask_value=None, always_apply=False, p=1)\n                        ], p=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.3588Z","iopub.execute_input":"2021-09-08T13:08:39.35947Z","iopub.status.idle":"2021-09-08T13:08:39.368834Z","shell.execute_reply.started":"2021-09-08T13:08:39.359389Z","shell.execute_reply":"2021-09-08T13:08:39.368045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Build the Data Generators\n\n\n[ 1 ] Train Generator","metadata":{}},{"cell_type":"code","source":"def train_generator_aug(batch_size=8,random_seed=None):\n    \n    while True:\n        \n        if random_seed:\n            random.seed(random_seed)\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_train.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['image_id'])\n            \n            # Create empty X matrix - 3 channels\n            X_train = np.zeros((6*len(df), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.uint8)\n            \n\n        \n            \n            # Create X_train\n            #================\n            \n            for i in range(0, len(image_id_list)):\n              \n              \n                # get the image and mask\n                image_id = image_id_list[i] + '.jpg'\n\n\n                # set the path to the image\n                path = '../input/plant-pathology-2020-fgvc7/images/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # convert to from BGR to RGB\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # resize the image\n                image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n                \n                \n            \n            \n            # Create y_train\n            # ===============\n                cols = ['healthy', 'multiple_diseases', 'rust', 'scab']\n                y_train = df[cols]\n                y_train = pd.concat([y_train, y_train, y_train, y_train, y_train, y_train], axis=0).reset_index(drop=True)\n                y_train = np.asarray(y_train) \n\n\n       \n                X_train[i] = image\n                X_train[i+1*len(image_id_list)] = augment_image(aug_types1, image)\n                X_train[i+2*len(image_id_list)] = augment_image(aug_types2, image)\n                X_train[i+3*len(image_id_list)] = augment_image(aug_types3, image)\n                X_train[i+4*len(image_id_list)] = augment_image(aug_types4, image)\n                X_train[i+5*len(image_id_list)] = augment_image(aug_types5, image)\n                \n            # Normalize the images\n            X_train = X_train/255\n\n            yield X_train, y_train\n            ","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.370319Z","iopub.execute_input":"2021-09-08T13:08:39.370732Z","iopub.status.idle":"2021-09-08T13:08:39.384707Z","shell.execute_reply.started":"2021-09-08T13:08:39.370697Z","shell.execute_reply":"2021-09-08T13:08:39.383912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_generator_no_aug(batch_size=8):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_train.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['image_id'])\n            \n            # Create empty X matrix - 3 channels\n            X_train = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.uint8)\n            \n\n        \n            \n            # Create X_train\n            #================\n            \n            for i in range(0, len(image_id_list)):\n              \n              \n                # get the image and mask\n                image_id = image_id_list[i] + '.jpg'\n\n\n                # set the path to the image\n                path = '../input/plant-pathology-2020-fgvc7/images/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # convert to from BGR to RGB\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # resize the image\n                image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n                \n                \n            \n            \n            # Create y_train\n            # ===============\n                cols = ['healthy', 'multiple_diseases', 'rust', 'scab']\n                y_train = df[cols]\n                y_train = np.asarray(y_train) \n                \n                # insert the image into X_train\n                X_train[i] = image\n                \n                          \n                \n            # Normalize the images\n            X_train = X_train/255\n\n            yield X_train, y_train","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.3888Z","iopub.execute_input":"2021-09-08T13:08:39.38905Z","iopub.status.idle":"2021-09-08T13:08:39.399011Z","shell.execute_reply.started":"2021-09-08T13:08:39.389022Z","shell.execute_reply":"2021-09-08T13:08:39.39824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the generator\n\n# initialize\nif AUG:\n    train_gen = train_generator_aug(batch_size=8,random_seed=123)\nelse:\n    train_gen = train_generator_no_aug(batch_size=8)\n\n# run the generator\nX_train, y_train = next(train_gen)\n\nprint(X_train.shape)\nprint(y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.402488Z","iopub.execute_input":"2021-09-08T13:08:39.402729Z","iopub.status.idle":"2021-09-08T13:08:39.672618Z","shell.execute_reply.started":"2021-09-08T13:08:39.4027Z","shell.execute_reply":"2021-09-08T13:08:39.671434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[ 2 ] Val Generator","metadata":{}},{"cell_type":"code","source":"def val_generator(batch_size=5):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_val.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['image_id'])\n            \n            # Create empty X matrix - 3 channels\n            X_val = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.uint8)\n            \n\n        \n            \n            # Create X_val\n            #================\n            \n            for i in range(0, len(image_id_list)):\n              \n              \n                # get the image and mask\n                image_id = image_id_list[i] + '.jpg'\n\n\n                # set the path to the image\n                path = '../input/plant-pathology-2020-fgvc7/images/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # convert to from BGR to RGB\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # resize the image\n                image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n\n                # insert the image into X_train\n                X_val[i] = image\n                \n                \n            \n            \n            # Create y_val\n            # ===============\n\n                cols = ['healthy', 'multiple_diseases', 'rust', 'scab']\n                y_val = df[cols]\n                y_val = np.asarray(y_val) \n\n                       \n                \n            # Normalize the images\n            X_val = X_val/255\n\n            yield X_val, y_val","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.674393Z","iopub.execute_input":"2021-09-08T13:08:39.674976Z","iopub.status.idle":"2021-09-08T13:08:39.684205Z","shell.execute_reply.started":"2021-09-08T13:08:39.674935Z","shell.execute_reply":"2021-09-08T13:08:39.683229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the generator\n\n# initialize\nval_gen = val_generator(batch_size=5)\n\n# run the generator\nX_val, y_val = next(val_gen)\n\nprint(X_val.shape)\nprint(y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.685608Z","iopub.execute_input":"2021-09-08T13:08:39.685983Z","iopub.status.idle":"2021-09-08T13:08:39.854179Z","shell.execute_reply.started":"2021-09-08T13:08:39.685946Z","shell.execute_reply":"2021-09-08T13:08:39.853107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[ 3 ] Test Generator","metadata":{}},{"cell_type":"code","source":"def test_generator(batch_size=1):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_test.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['image_id'])\n            \n            # Create empty X matrix - 3 channels\n            X_test = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.uint8)\n            \n\n        \n            \n            # Create X_test\n            #================\n            \n            for i in range(0, len(image_id_list)):\n              \n              \n                # get the image and mask\n                image_id = image_id_list[i] + '.jpg'\n\n\n                # set the path to the image\n                path = '../input/plant-pathology-2020-fgvc7/images/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # convert to from BGR to RGB\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # resize the image\n                image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n\n                # insert the image into X_train\n                X_test[i] = image\n                \n                 \n                \n            # Normalize the images\n            X_test = X_test/255\n\n            yield X_test","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.855487Z","iopub.execute_input":"2021-09-08T13:08:39.855825Z","iopub.status.idle":"2021-09-08T13:08:39.864074Z","shell.execute_reply.started":"2021-09-08T13:08:39.855787Z","shell.execute_reply":"2021-09-08T13:08:39.86312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the generator\n\n# initialize\ntest_gen = test_generator(batch_size=1)\n\n# run the generator\nX_test = next(test_gen)\n\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.865847Z","iopub.execute_input":"2021-09-08T13:08:39.866369Z","iopub.status.idle":"2021-09-08T13:08:39.909724Z","shell.execute_reply.started":"2021-09-08T13:08:39.866333Z","shell.execute_reply":"2021-09-08T13:08:39.908864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train_samples = len(df_train)\nnum_val_samples = len(df_val)\n\n# determine num train steps\ntrain_steps = np.ceil(num_train_samples / TRAIN_BATCH_SIZE)\n\n# determine num val steps\nval_steps = np.ceil(num_val_samples / VAL_BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.911096Z","iopub.execute_input":"2021-09-08T13:08:39.911464Z","iopub.status.idle":"2021-09-08T13:08:39.917025Z","shell.execute_reply.started":"2021-09-08T13:08:39.911427Z","shell.execute_reply":"2021-09-08T13:08:39.915842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\ntime3 = datetime.datetime.now()\nprint(time3)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.918882Z","iopub.execute_input":"2021-09-08T13:08:39.919334Z","iopub.status.idle":"2021-09-08T13:08:39.927456Z","shell.execute_reply.started":"2021-09-08T13:08:39.919296Z","shell.execute_reply":"2021-09-08T13:08:39.926697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras import layers as L\n# TPUの初期化\n# \ntry:\n#     TPUのハードウェア情報を獲得。TPUが利用できない環境ではエラー\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     TPU利用可能の確認\n    print('Running on TPU:', tpu.master())\n# 上記でエラー（例外）が出た場合の処理\nexcept ValueError:\n    tpu = None\n\n# TPUが利用できる場合（Accelerator TPU）\nif tpu:\n#   リモートクラスタに接続してTPUを初期化\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n#   データの並列処理を使用してトレーニングを分散する\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# TPUが利用できない場合（Accelerator None）\nelse:\n    strategy = tf.distribute.get_strategy()\n\n# 並列処理のレベルに関する決定をAUTOで行う\nAUTO = tf.data.experimental.AUTOTUNE\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\nfrom tensorflow.keras.applications import ResNet152\n\nwith strategy.scope():\n    rsn152 = ResNet152(include_top=False, weights='imagenet', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n\n    model = Sequential()\n    model.add(rsn152)\n    model.add(L.GlobalAveragePooling2D())\n    model.add(L.Dense(4, activation='softmax'))\n   # model.compile(optimizer='sgd',\n             # loss='mse',\n             # metrics=[tf.keras.metrics.Accuracy()])\n   # print(model.metrics_names)\n    model.compile(\n         Adam(lr=LR),\n         loss='categorical_crossentropy',\n         metrics=['accuracy'])\n    print(model.summary())\n    \n    \nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n                                        ModelCheckpoint, CSVLogger, LearningRateScheduler)\n# Initialize the generators\nif AUG:\n    train_gen = train_generator_aug(batch_size=TRAIN_BATCH_SIZE,random_seed=123)\nelse:\n    train_gen = train_generator_no_aug(batch_size=TRAIN_BATCH_SIZE)\n\nval_gen = val_generator(batch_size=VAL_BATCH_SIZE)\n\n\nfilepath = H5_FILE_NAME\n\n#earlystopper = EarlyStopping(patience=10, verbose=1)\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.3, patience=3, \n                                   verbose=1, mode='max')\n\n\n\nlog_fname = TRAINING_LOG_FILE_NAME\ncsv_logger = CSVLogger(filename=log_fname,\n                       separator=',',\n                       append=False)\n\ncallbacks_list = [checkpoint, csv_logger, reduce_lr]\n\nhistory = model.fit(train_gen, steps_per_epoch=train_steps, epochs=EPOCHS, \n                    validation_data=val_gen, validation_steps=val_steps,\n                    verbose=2,\n                    callbacks=callbacks_list)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:08:39.928573Z","iopub.execute_input":"2021-09-08T13:08:39.928823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\ntime4 = datetime.datetime.now()\nprint(time4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training time\nprint(time4-time3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(H5_FILE_NAME)\n\nval_gen = val_generator(batch_size=VAL_BATCH_SIZE)\n\nval_loss, val_acc = \\\nmodel.evaluate(val_gen, \n               steps=val_steps)\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the training log\n\ntrain_log = pd.read_csv(TRAINING_LOG_FILE_NAME)\n\ntrain_log.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = train_log['accuracy']\nval_acc = train_log['val_accuracy']\nloss = train_log['loss']\nval_loss = train_log['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc)\nplt.plot(epochs, val_acc)\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','val'],loc = 'upper left')\nplt.show()\n\nplt.plot(epochs, loss)\nplt.plot(epochs, val_loss)\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','val'],loc = 'upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make a prediction on the val set","metadata":{}},{"cell_type":"code","source":"model.load_weights(H5_FILE_NAME)\n\nval_gen = val_generator(batch_size=1)\n\npreds = model.predict(val_gen, steps=len(df_val), verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get y_pred as index values\n\ny_pred = np.argmax(preds, axis=1)\nprint(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get y_true as index values\n\ncols = ['healthy', 'multiple_diseases', 'rust', 'scab']\ny_true = df_val[cols]\ny_true = np.asarray(y_true) \n\ny_true = np.argmax(y_true, axis=1)\nprint(y_true)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classification Report","metadata":{}},{"cell_type":"markdown","source":"Make a test set prediction","metadata":{}},{"cell_type":"code","source":"model.load_weights(H5_FILE_NAME)\nval_gen = val_generator(batch_size=1)\n\npreds = model.predict(val_gen, steps=len(df_val), verbose=1)\n\n\ny_pred = np.argmax(preds, axis=1)\nprint(y_pred[:50])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put the preds into a dataframe\n\ndf_preds = pd.DataFrame(preds, columns=['healthy', 'multiple_diseases', 'rust', 'scab'])\n\ndf_preds['image_id'] = df_val['image_id'].copy().values\n\ndf_preds.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a submission csv file\n\ndf_results = pd.DataFrame({'image_id': df_preds.image_id,\n                            'healthy': df_preds.healthy,\n                               'multiple_diseases': df_preds.multiple_diseases,\n                               'rust': df_preds.rust,\n                               'scab': df_preds.scab,\n                           'target':df_val['target'].values\n                           }).set_index('image_id')\n\n\n# create a submission csv file\ndf_results.to_csv(SUBMISSION_FILE_NAME) \ndf_results.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time2 = datetime.datetime.now()\nprint(time2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total time\nprint(time2 - time1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}