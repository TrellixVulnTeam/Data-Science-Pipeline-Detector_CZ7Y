{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Plant Pathology 2020 - FGVC7\n\nIdentify the category of foliar diseases in apple trees\n\nKaggle competition - https://www.kaggle.com/c/plant-pathology-2020-fgvc7/submit\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nprint(tf.__version__)\nimport os\nimport shutil\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data and Preprocessing\n\nHere we load the data and take a look at what we're dealing with."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/plant-pathology-2020-fgvc7/train.csv')\ntest = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')\n\ntarget = train[['healthy', 'multiple_diseases', 'rust', 'scab']]\ntest_ids = test['image_id']\n\ntrain_len = train.shape[0]\ntest_len = test.shape[0]\n\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ah, we see the multiple_diseases label has drastically less images than the rest of the labels. Once we load the images in raw data form, we'll use scikitlearn to randomly over sample so we can fix this class imbalance.\n\nNow let's load the image data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of train data: \" + str(train.shape))\nprint(\"Shape of test data: \" + str(test.shape))\n\ntrain_len = train.shape[0]\ntest_len = test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom tqdm.notebook import tqdm\n\npath = '../input/plant-pathology-2020-fgvc7/images/'\nsize = 224\n\ntrain_images = np.ndarray(shape=(train_len, size, size, 3))\nfor i in tqdm(range(train_len)):\n  img = load_img(path + f'Train_{i}.jpg', target_size=(size, size))\n  train_images[i] = np.uint8(img_to_array(img))\n\ntest_images = np.ndarray(shape=(test_len, size, size, 3))\nfor i in tqdm(range(test_len)):\n  img = load_img(path + f'Test_{i}.jpg', target_size=(size, size))\n  test_images[i] = np.uint8(img_to_array(img))\n\ntrain_images.shape, test_images.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Let's take a look at what the images look like."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(4):\n\tplt.subplot(220 + 1 + i)\n\tplt.title(train['image_id'][i])\n\tplt.imshow(np.uint8(train_images[i]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(4):\n\tplt.subplot(220 + 1 + i)\n\tplt.title(test['image_id'][i])\n\tplt.imshow(np.uint8(test_images[i]))\nplt.show()\nplt.savefig('test_images.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's split out data into train and test sets for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_images, target.to_numpy(), test_size=0.1, random_state=289) \n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now use RandomOverSampler to fix our class imbalance in the multiple diseases class."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=289)\n\nx_train, y_train = ros.fit_resample(x_train.reshape((-1, size * size * 3)), y_train)\nx_train = x_train.reshape((-1, size, size, 3))\nx_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ndel train_images\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we prepare the data for going into a Keras deep learning model. Here I use the ImageDataGenerator to also give us more images by using the parameters to rotate, horizontally flip, and vertically flip. Also the image is rescaled by 1/255 to normalize the raw data so that the activation functions work properly."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_preprocessing.image import ImageDataGenerator\n\nbatch_size = 8\n\ntrain_datagen = ImageDataGenerator(samplewise_center = True,\n                                   samplewise_std_normalization = True,\n                                   horizontal_flip = True,\n                                   vertical_flip = True,\n                                   rotation_range=20)\n\ntrain_generator = train_datagen.flow(\n    x = x_train, \n    y = y_train,\n    batch_size = batch_size)\n\nvalidation_datagen = ImageDataGenerator(samplewise_center = True,\n                                        samplewise_std_normalization = True)\n\nvalidation_generator = validation_datagen.flow(\n    x = x_test, \n    y = y_test,\n    batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = np.random.randint(8)\nx, y = train_generator.__getitem__(idx)\nplt.title(y[idx])\nplt.imshow(x[idx])\nplt.savefig('processed_img.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keras Model\nLet's create the model. Here we use ResNet50 for image classification as our base model. The last two layers are not included and these two layers are trained. ResNet50 is built to classify between 1000 classes, but we only need 4. The rest of the layers use pre-trained weights, which we will fine-tune later on once we get a convergence on the last two layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = tf.keras.applications.ResNet50(include_top = False, weights='imagenet', input_shape=(size, size, 3))\n\ndef create_model():\n    model = tf.keras.Sequential([\n      base_model,\n      tf.keras.layers.GlobalAveragePooling2D(),\n      tf.keras.layers.Dense(4, activation='softmax')\n      ])\n    model.compile(\n        loss = 'kullback_leibler_divergence', \n        optimizer = 'adam', \n        metrics = ['accuracy'])\n    return model\n\nmodel = create_model()\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 100\nsteps_per_epoch = x_train.shape[0] // batch_size\nvalidation_steps = x_test.shape[0] // batch_size\nprint(steps_per_epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's set up some callbacks. \nCallbacks:\n\n**EarlyStopping** - stop early if the validation loss has stopped improving\n\n**ModelCheckpoint** - save the model every epoch and save the best weights\n\n**ReduceLROnPlateau** - reduce learning rate when validation loss has stopped improving\n\n**LearningRateScheduler** - set learning rate to ramp up during early epochs"},{"metadata":{"trusted":true},"cell_type":"code","source":"es = tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, verbose=1)\nmc = tf.keras.callbacks.ModelCheckpoint('model.hdf5', save_best_only=True, verbose=0)\nrlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, verbose=1)\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\nrampup_epochs = 20\nsustain_epochs = 15\nexp_decay = .8\n\ndef lrfn(epoch):\n  if epoch < rampup_epochs:\n    return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n  elif epoch < rampup_epochs + sustain_epochs:\n    return max_lr\n  else:\n    return min_lr\n    \nlr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n\nrang = np.arange(epochs)\ny = [lrfn(x) for x in rang]\nplt.plot(rang, y)\nprint('Learning rate per epoch:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    x = train_generator,  \n    validation_data = validation_generator,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch,\n    validation_steps = validation_steps,\n    verbose=1,\n    callbacks=[es, lr, mc, rlr])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\nWe plot the train and validation accuracy and loss to see how the model did over the epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_err = (1-history.history['accuracy'][-1])*100\nvalidation_err = (1-history.history['val_accuracy'][-1])*100\nprint(\"Train set error \" + str(train_err))\nprint(\"Validation set error \" + str(validation_err))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction\n\nHere we feed the test image set into the model.predict function and see how our model does."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_datagen = ImageDataGenerator(samplewise_center = True,\n                                 samplewise_std_normalization = True)\n\ntest_generator = test_datagen.flow(\n    x = test_images,\n    shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities = model.predict(test_generator, steps = len(test_generator))\nprint(probabilities[:,0].mean()*100)\nprint(probabilities[:,1].mean()*100)\nprint(probabilities[:,2].mean()*100)\nprint(probabilities[:,3].mean()*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine Tuning the Model\nHere we fine tune the model by unfreezing the layers in the pre-trained ResNet50 model by setting the base_model to trainable."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.trainable = True\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n        loss = 'kullback_leibler_divergence', \n        optimizer = tf.keras.optimizers.Adam(1e-5), \n        metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nhistory = model.fit(\n    x = train_generator,  \n    validation_data = validation_generator,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch,\n    validation_steps = validation_steps,\n    verbose=1,\n    callbacks = [es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_err = (1-history.history['accuracy'][-1])*100\nvalidation_err = (1-history.history['val_accuracy'][-1])*100\nprint(\"Train set error \" + str(train_err))\nprint(\"Validation set error \" + str(validation_err))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}