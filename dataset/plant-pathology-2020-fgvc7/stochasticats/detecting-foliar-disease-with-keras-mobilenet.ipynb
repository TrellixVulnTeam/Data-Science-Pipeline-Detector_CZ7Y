{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Plant Pathology 2020 - FGVC7\nIdentify the category of foliar diseases in apple trees\n\nKaggle competition - https://www.kaggle.com/c/plant-pathology-2020-fgvc7/submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nprint(tf.__version__)\nimport os\nimport shutil\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data and Preprocessing\n\nHere we load the data and take a look at what we're dealing with."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/plant-pathology-2020-fgvc7/train.csv')\ntest = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')\n\ntarget = train[['healthy', 'multiple_diseases', 'rust', 'scab']]\ntest_ids = test['image_id']\n\ntrain_len = train.shape[0]\ntest_len = test.shape[0]\n\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ah, we see the multiple_diseases label has drastically less images than the rest of the labels. Once we load the images in raw data form, we'll use scikitlearn to randomly over sample so we can fix this class imbalance.\n\nNow let's load the image data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of train data: \" + str(train.shape))\nprint(\"Shape of test data: \" + str(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_len = train.shape[0]\ntest_len = test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom tqdm.notebook import tqdm\n\npath = '../input/plant-pathology-2020-fgvc7/images/'\nsize = 224\n\ntrain_images = np.ndarray(shape=(train_len, size, size, 3))\nfor i in tqdm(range(train_len)):\n  img = load_img(path + f'Train_{i}.jpg', target_size=(size, size))\n  train_images[i] = np.uint8(img_to_array(img))\n\ntest_images = np.ndarray(shape=(test_len, size, size, 3))\nfor i in tqdm(range(test_len)):\n  img = load_img(path + f'Test_{i}.jpg', target_size=(size, size))\n  test_images[i] = np.uint8(img_to_array(img))\n\ntrain_images.shape, test_images.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at what the images look like."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(4):\n\tplt.subplot(220 + 1 + i)\n\tplt.title(train['image_id'][i])\n\tplt.imshow(np.uint8(train_images[i]), interpolation = 'nearest', aspect='auto')\nplt.show()\nplt.savefig('train_images.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(4):\n\tplt.subplot(220 + 1 + i)\n\tplt.title(test['image_id'][i])\n\tplt.imshow(np.uint8(test_images[i]), interpolation = 'nearest', aspect='auto')\nplt.show()\nplt.savefig('test_images.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's split out data into train and test sets for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_images, target.to_numpy(), test_size=0.1, random_state=289) \n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now use RandomOverSampler to fix our class imbalance in the multiple diseases class."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=289)\n\nx_train, y_train = ros.fit_resample(x_train.reshape((-1, size * size * 3)), y_train)\nx_train = x_train.reshape((-1, size, size, 3))\nx_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ndel train_images\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we prepare the data for going into a Keras deep learning model. Here I use the ImageDataGenerator to also give us more images by using the parameters to rotate, horizontally flip, and vertically flip. Also the image is samplewise standard normalized the raw data so that the activation functions work properly."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_preprocessing.image import ImageDataGenerator\n\nbatch_size = 8\n\ntrain_datagen = ImageDataGenerator(samplewise_center = True,\n                                   samplewise_std_normalization = True,\n                                   horizontal_flip = True,\n                                   vertical_flip = True,\n                                   rotation_range=70)\n\ntrain_generator = train_datagen.flow(\n    x = x_train, \n    y = y_train,\n    batch_size = batch_size)\n\nvalidation_datagen = ImageDataGenerator(samplewise_center = True,\n                                        samplewise_std_normalization = True)\n\nvalidation_generator = validation_datagen.flow(\n    x = x_test, \n    y = y_test,\n    batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what the images look like after processing and what they look like going into the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = np.random.randint(8)\nx, y = train_generator.__getitem__(idx)\nplt.title(y[idx])\nplt.imshow(x[idx])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keras Model\nHere we build the model. I will use a pre-trained MobileNet for deep CNN which will then be fed into a dense layer to predict 4 classes, since the original MobileNet predicts 1000. It will compile using the loss function KL Divergence, Adam optimizer, and accuracy metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    pre_trained = tf.keras.applications.MobileNet(input_shape=(size, size, 3), weights='imagenet', include_top=False)\n    for layer in pre_trained.layers:\n      layer.trainable = False\n    \n    #pretrained_model = tf.keras.applications.mobilenet.MobileNet(input_shape=(SIZE,SIZE,3), include_top=False)\n    model = tf.keras.Sequential([\n      pre_trained,\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dropout(0.3),\n      tf.keras.layers.Dense(4, activation='softmax')\n      ])\n    model.compile(\n        loss = 'kullback_leibler_divergence', \n        optimizer = 'adam', \n        metrics = ['accuracy'])\n    return model\n\nmodel = create_model()\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now define some model parameters and set up some callbacks."},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 150\nsteps_per_epoch = x_train.shape[0] // batch_size\nvalidation_steps = x_test.shape[0] // batch_size\nprint(steps_per_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, verbose=1)\nmc = tf.keras.callbacks.ModelCheckpoint('model.hdf5', save_best_only=True, verbose=0)\nrlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, verbose=1)\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\nrampup_epochs = 40\nsustain_epochs = 20\nexp_decay = .8\n\ndef lrfn(epoch):\n  if epoch < rampup_epochs:\n    return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n  elif epoch < rampup_epochs + sustain_epochs:\n    return max_lr\n  else:\n    return min_lr\n    \nlr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n\nrang = np.arange(epochs)\ny = [lrfn(x) for x in rang]\nplt.plot(rang, y)\nprint('Learning rate per epoch:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    x = train_generator,  \n    validation_data = validation_generator,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch,\n    validation_steps = validation_steps,\n    verbose=1,\n    callbacks=[es, lr, mc, rlr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_err = (1-history.history['accuracy'][-1])*100\nvalidation_err = (1-history.history['val_accuracy'][-1])*100\nprint(\"Train set error \" + str(train_err))\nprint(\"Validation set error \" + str(validation_err))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_datagen = ImageDataGenerator(samplewise_center = True,\n                                 samplewise_std_normalization = True)\n\ntest_generator = test_datagen.flow(\n    x = test_images,\n    shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities = model.predict(test_generator, steps = len(test_generator))\nprint(probabilities)\nprint(probabilities[:,0].mean()*100)\nprint(probabilities[:,1].mean()*100)\nprint(probabilities[:,2].mean()*100)\nprint(probabilities[:,3].mean()*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame()\nres['image_id'] = test['image_id']\nres['healthy'] = probabilities[:, 0]\nres['multiple_diseases'] = probabilities[:, 1]\nres['rust'] = probabilities[:, 2]\nres['scab'] = probabilities[:, 3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_probabilities = model.predict(validation_generator, steps = len(validation_generator))\nprint(valid_probabilities[:,0].mean()*100)\nprint(valid_probabilities[:,1].mean()*100)\nprint(valid_probabilities[:,2].mean()*100)\nprint(valid_probabilities[:,3].mean()*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, valid_probabilities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}