{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Leaf Disease Detection\nIn this notebook, you would need to implement a CNN classifier for leaf disease detection. Your goal is to submit your predictions to the competition! Feel free to use previous case studies, but make sure you understand what the code is doing before using it.","metadata":{}},{"cell_type":"code","source":"import os\nfrom os.path import join\n\nfrom tqdm.notebook import tqdm # progress bar\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\n\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.display import SVG\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"IMAGE_PATH = \"../input/plant-pathology-2020-fgvc7/images/\"\nTEST_PATH = \"../input/plant-pathology-2020-fgvc7/test.csv\"\nTRAIN_PATH = \"../input/plant-pathology-2020-fgvc7/train.csv\"\nSUB_PATH = \"../input/plant-pathology-2020-fgvc7/sample_submission.csv\"\nMODEL_PATH = \"models/plant_pathology_model.h5\"\n\nsub = pd.read_csv(SUB_PATH)\ndf_test = pd.read_csv(TEST_PATH)\ndf_train = pd.read_csv(TRAIN_PATH)\n\nEPOCHS = 50\n\n# Define size of the image to train on\n# Remember: large image size will probably lead to higher performance\n# at the expense of long training time and large memory use\nIMAGE_X = 200\nIMAGE_Y = 200\n\nlabels = ['healthy', 'multiple_diseases', 'rust', 'scab']","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore the data","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training set size:', len(df_train))\nfor label in labels:\n    print(f\"\\t{label}: {df_train[df_train[label]==1].shape[0]}\")\nprint('Test set size:', len(df_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## check if image only belong to one class","metadata":{}},{"cell_type":"code","source":"# Sum all of the labels together\ndf_train['number_of_classes'] = df_train['healthy'] + \\\n                                df_train['multiple_diseases'] + \\\n                                df_train['rust'] + df_train['scab']\n\n# mean should be 1, std should be 0\ndf_train['number_of_classes'].mean(), df_train['number_of_classes'].std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot some images\nnrows, ncols = 4, 4\n\nfig, axs = plt.subplots(nrows, ncols, figsize=(16,10))\naxs = axs.ravel() # make 1D array for easy plotting in for loop\n\nfor i in range(nrows*ncols):\n    # show an image\n    img = cv2.imread(f\"{IMAGE_PATH}/{df_train['image_id'][i]}.jpg\")\n\n    # reverse BGR to RGB of opencv imported image\n    axs[i].imshow(img[:,:,::-1])\n    axs[i].axis(False)\n    label = df_train.loc[:, 'healthy':].iloc[i, :].idxmax()\n    axs[i].set_title('{} : {}'.format(df_train['image_id'][i], label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = cv2.imread(f\"{IMAGE_PATH}/{df_train['image_id'][3]}.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nfig = px.imshow(image)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code thanks to https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\ndef load_image(image_id):\n    file_path = image_id + \".jpg\"\n    image = cv2.imread(IMAGE_PATH + file_path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\ntrain_images = df_train[\"image_id\"][:100].apply(load_image)\n\nred_values = [np.mean(train_images[idx][:, :, 0]) for idx in range(len(train_images))]\ngreen_values = [np.mean(train_images[idx][:, :, 1]) for idx in range(len(train_images))]\nblue_values = [np.mean(train_images[idx][:, :, 2]) for idx in range(len(train_images))]\nvalues = [np.mean(train_images[idx]) for idx in range(len(train_images))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = ff.create_distplot([red_values, green_values, blue_values],\n                         group_labels=[\"R\", \"G\", \"B\"],\n                         colors=[\"red\", \"green\", \"blue\"])\nfig.update_layout(title_text=\"Distribution of channel values\")\nfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Generators and Augmentation","metadata":{}},{"cell_type":"code","source":"#fig, axs = plt.subplots(1, 2, figsize=(16,10))\n#axs[0].imshow(cv2.imread(train_paths[0]))\n#axs[0].set_title('BGR')\n#axs[1].imshow(cv2.imread(train_paths[0])[:,:,::-1])\n#axs[1].set_title('RBG')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the names of the images into a correct path\ndef format_path(st):\n    return os.path.join(IMAGE_PATH, st + '.jpg')\n\n# Genereate train and test paths\ntrain_paths = df_train.image_id.apply(format_path).values\ntest_paths = df_test.image_id.apply(format_path).values\n\n# Convert the labels to floats\ntrain_labels = np.float32(df_train.loc[:,'healthy':'scab'].values)\n\n# Split the data into validation and training sets\ntrain_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, train_labels, test_size=0.2, random_state=2020)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_paths.shape, valid_paths.shape, train_labels.shape, valid_labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread(train_paths[0])\nimg_max = img.max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_image(filename, label=None, image_size=(IMAGE_X, IMAGE_Y), img_max = 255):\n    \"\"\"\n    Loads, normalizes and resizes the image\n    \"\"\"\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    \"\"\"\n    Define your data augmentations here\n    # only flip left/right up/down\n    \"\"\"\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 64\n\n# Create datasets\n# step 2: create a dataset returning slices of `filenames`\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n# step 3: apply every image in the dataset using `map`\ntrain_dataset = train_dataset.map(decode_image)\ntrain_dataset = train_dataset.map(data_augment) #only on training: helps model to become more robust\ntrain_dataset = train_dataset.batch(BATCH_SIZE)\n# step 4: shuffle random\ntrain_dataset = train_dataset.shuffle(500) # shuffle only impact on learning, random order of classes instead of in order\n# step 5: prefetch min time step during training\ntrain_dataset = train_dataset.prefetch(AUTO)\n\nvalid_dataset = tf.data.Dataset.from_tensor_slices((valid_paths, valid_labels))\nvalid_dataset = valid_dataset.map(decode_image)\nvalid_dataset = valid_dataset.batch(BATCH_SIZE)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_paths))\ntest_dataset = test_dataset.map(decode_image)\ntest_dataset = test_dataset.batch(BATCH_SIZE)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Definition","metadata":{}},{"cell_type":"markdown","source":"## try to find best fit hp","metadata":{}},{"cell_type":"code","source":"# Maker sure that keras-tuner is installed\n#!pip install -q -U keras-tuner\nimport kerastuner as kt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow import keras\n\n\ndef create_dense_block(inputs, n_depth, n_width):\n    \"\"\" Create Dense block.\"\"\"\n    x = layers.BatchNormalization()(inputs)\n    x = layers.Conv2D(n_width, (3,3), activation='relu', padding='same', kernel_initializer='he_normal', use_bias=False)(x)\n    x = layers.Dropout(0.2)(x)\n    x_list = [inputs]\n    for _ in range(n_depth-1):\n        x_list.append(x)\n        x = layers.Concatenate()(x_list)\n        x = layers.BatchNormalization()(x)\n        x = layers.Conv2D(n_width, (3,3), activation='relu', padding='same', kernel_initializer='he_normal', use_bias=False)(x)\n        x = layers.Dropout(0.2)(x)\n            \n    return x\n\n\ndef model_builder(hp):\n    # A hyperparameter search space needs to be defined. This is done by defining\n    # a number of variables whose value can change over some predefined value range.\n    # A variable can be an Int, Float, Boolean or Choice.\n    # tune number of feature maps, dense layer size, dropout rate and learning rate\n    hp_n_width = hp.Int('n_width', min_value = 16, max_value = 32, step = 8)\n    hp_n_depth = hp.Int('n_depth', min_value = 2, max_value = 4, step = 1)\n    hp_dense_units = hp.Int('dense_units', min_value = 128, max_value = 1024, step = 128)\n    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])\n    inputs = keras.Input(shape= (IMAGE_X, IMAGE_Y, 3))\n#     print('Model with n_width:{}, n_depth: {}, dense_units: {}, learning_rate: {}'.format(\n#     hp_n_width, hp_n_depth, hp_dense_units,hp_learning_rate\n#     ),)\n    x = create_dense_block(inputs, hp_n_depth, hp_n_width)\n    x = layers.Conv2D(hp_n_width, (1,1), activation='linear', padding='same')(x)\n    x = layers.MaxPooling2D((2, 2))(x)\n\n    x = create_dense_block(x, hp_n_depth, 2*hp_n_width)\n    x = layers.Conv2D(2*hp_n_width, (1,1), activation='linear', padding='same')(x)\n    x = layers.MaxPooling2D((2, 2))(x)\n\n    x = create_dense_block(x, hp_n_depth, 4*hp_n_width)\n    x = layers.Conv2D(4*hp_n_width, (1,1), activation='linear', padding='same')(x)\n    x = layers.MaxPooling2D((2, 2))(x)\n\n    n_dense = hp_dense_units\n    x = layers.Dropout(0.2)(x)    \n    x = layers.Flatten()(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(n_dense, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    outputs = layers.Dense(4, activation='softmax')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # define model optimization method\n    model.compile(optimizer=optimizers.Adam(lr=hp_learning_rate), \n                  loss='categorical_crossentropy', \n                  metrics=['categorical_accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner = kt.RandomSearch(model_builder,\n                        objective = 'val_categorical_accuracy', \n                        seed=42,\n                        max_trials=5,\n                        executions_per_trial=2,\n                        directory='random_search20',\n                        project_name='leafdisease_hp_2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This callback is going to clear the output of the cell when searching\nimport IPython\nclass ClearTrainingOutput(tf.keras.callbacks.Callback):\n      def on_train_end(*args, **kwargs):\n            IPython.display.clear_output(wait = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform hyper-parameter search\nsteps = int(df_train.shape[0] // BATCH_SIZE)\n\n# Pass the same paramters as in model.fit()\ntuner.search(\n    x=train_dataset,\n    steps_per_epoch=steps,\n    validation_data= valid_dataset,\n    epochs=15,\n    callbacks = [ClearTrainingOutput()],\n    verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n\nbest_hps.get('n_width'), best_hps.get('n_depth'), best_hps.get('dense_units'), best_hps.get('learning_rate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create model with best hyperparameters\nmodel = tuner.hypermodel.build(best_hps)\n\n# hp = kt.HyperParameters()\n# hp.Fixed('n_width', value=24)\n# hp.Fixed('n_depth', value=2)\n# hp.Fixed('dense_units', value=1024)\n# hp.Fixed('learning_rate', value=0.0001)\n# model = tuner.hypermodel.build(hp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## training with callbacks","metadata":{}},{"cell_type":"code","source":"best_w = tf.keras.callbacks.ModelCheckpoint(\n    'model_best.h5',\n    monitor='val_loss',\n    verbose=0,\n    save_best_only=True,\n    save_weights_only=True,\n    mode='auto',\n    period=1\n)\n\nlast_w = tf.keras.callbacks.ModelCheckpoint(\n    'model_last.h5',\n    monitor='val_loss',\n    verbose=0,\n    save_best_only=False,\n    mode='auto',\n    period=1\n)\ncallbacks = [best_w, last_w]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps = int(df_train.shape[0] // BATCH_SIZE)\n\n# model.fit() now supports generators as input!\nhistory = model.fit(\n    x=train_dataset,\n    steps_per_epoch=steps,\n    validation_data= valid_dataset,\n    epochs=100, \n    verbose=2,\n    callbacks=callbacks\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Final Training Accuracy: {:.4f}'.format(history.history['categorical_accuracy'][-1]))\nprint('Final Validation Accuracy: {:.4f}'.format(history.history['val_categorical_accuracy'][-1]))\nprint('Final Training Loss: {:.4f}'.format(history.history['loss'][-1]))\nprint('Final Validation Loss: {:.4f}'.format(history.history['val_loss'][-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model definition","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import MobileNet, DenseNet121\nfrom tensorflow.keras import layers\n\n# Define model architecture\nmodel = ... #---YOUR CODE HERE---\n\n# compile the model\nadam = optimizers.Adam(lr = 0.001)\nmodel.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Callbacks","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# Define checkpointing callback\nmcp = ... #---YOUR CODE HERE---\n\n# Define learnning rate schedule\ndef build_lrfn(lr_start=0.00001, lr_max=0.00005, \n               lr_min=0.00001, lr_rampup_epochs=5, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    #define your learning rate schedule\n    ... #---YOUR CODE HERE---\n\n# Create a learning rate schedule as keras callback\nlrfn = build_lrfn()\nlr_schedule = ... #---YOUR CODE HERE---","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the learning rate across epochs\nepochs_dummy = list(range(0, 50))\ny = [lrfn(e) for e in epochs_dummy]\nfig = go.Figure(go.Scatter(x=epochs_dummy, y=y, mode='lines+markers'))\nfig.update_layout(\n    yaxis = dict(\n        showexponent='all',\n        exponentformat='e'\n    ),\n    title='Learning rate schedule'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"# setup the callbacks\ncallbacks = ... #---YOUR CODE HERE---\n\n# train your model\nhistory = model.fit(... #---YOUR CODE HERE---","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\ndef visualize_training_process(history):\n    \"\"\" \n    Visualize loss and accuracy from training history\n    \n    :param history: A Keras History object\n    \"\"\"\n    history_df = pd.DataFrame(history.history)\n    epochs = np.arange(1, len(history_df) + 1)\n    fig = make_subplots(2, 1)\n    fig.append_trace(go.Scatter(x=epochs, y=history_df['categorical_accuracy'], mode='lines+markers', name='Accuracy Train'), row=1, col=1)\n    fig.append_trace(go.Scatter(x=epochs, y=history_df['val_categorical_accuracy'], mode='lines+markers', name='Accuracy Val'), row=1, col=1)\n    \n    fig.append_trace(go.Scatter(x=epochs, y=history_df['loss'], mode='lines+markers', name='Loss Train'), row=2, col=1)\n    fig.append_trace(go.Scatter(x=epochs, y=history_df['val_loss'], mode='lines+markers', name='Loss Val'), row=2, col=1)\n    \n    fig.update_layout( xaxis_title=\"Epochs\", template=\"plotly_white\")\n    \n    return fig\nvisualize_training_process(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate performance of model by plotting confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# see http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\n\ndef accuracy(y, y_pred):\n    return np.sum(y == y_pred)/len(y)\n\ndef plot_confusion_matrix(cm, labels=None, title='Confusion Matrix'):\n    import plotly.figure_factory as ff\n\n    x = labels\n    y = x\n\n    # change each element of z to type string for annotations\n    z_text = [[str(y) for y in x] for x in cm]\n\n    # set up figure \n    fig = ff.create_annotated_heatmap(cm, x=x, y=y, annotation_text=z_text, colorscale='YlGnBu', showscale=True)\n\n    # add title\n    fig.update_layout(title_text=title,\n                      #xaxis = dict(title='x'),\n                      #yaxis = dict(title='x')\n                     )\n\n    # add custom xaxis title\n    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n                            x=0.5,\n                            y=-0.15,\n                            showarrow=False,\n                            text=\"Predicted value\",\n                            xref=\"paper\",\n                            yref=\"paper\"))\n\n    # add custom yaxis title\n    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n                            x=-0.35,\n                            y=0.5,\n                            showarrow=False,\n                            text=\"Real value\",\n                            textangle=-90,\n                            xref=\"paper\",\n                            yref=\"paper\"))\n\n    # adjust margins to make room for yaxis title\n    fig.update_layout(margin=dict(t=100, l=200), width=700, height=600)\n    fig.show()\n    \n# predict labels from validation set\ny_pred = model.predict(valid_dataset)\n# convert data to label number\ny_pred = np.argmax(y_pred, axis=1) \ny_true = np.argmax(valid_labels, axis=1) \n\n# compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred) \n\nplot_confusion_matrix(cm, labels, title='Confusion_matrix Validation Set (acc={:.3f})'.format(accuracy(y_true, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save predictions","metadata":{}},{"cell_type":"code","source":"# Predict labels on the test set\npredictions = model.predict(test_dataset)\n\n# Prepare the submission file\nsub.loc[:, 'healthy':] = predictions\nsub.to_csv('submission_densenet.csv', index=False)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The last step is to submit your predictions!","metadata":{}}]}