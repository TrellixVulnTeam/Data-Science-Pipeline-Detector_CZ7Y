{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 常明杰的毕业设计作品","metadata":{}},{"cell_type":"markdown","source":"<h2>基于MobileNetV3的果树病虫害识别</h2>","metadata":{}},{"cell_type":"markdown","source":"首先，我们导入需要的包","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nprint(tf.__version__)\nimport os\nimport shutil\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>加载数据和预处理</h2>\n<h4>下面，加载数据，看一下我们需要处理什么</h4>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/plant-pathology-2020-fgvc7/train.csv')\ntest = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')\nprint(train)\ntarget = train[['healthy', 'multiple_diseases', 'rust', 'scab']]\nprint(target)\n\ntest_ids = test['image_id']\nprint(test_ids)\n\ntrain_len = train.shape[0]\ntest_len = test.shape[0]\nprint(train_len)\nprint(train.shape)\n\ntrain.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>count:每一列非空值的数量</h5>\n<h5>mean: 每一列的平均值</h5>\n<h5>std:每一列的标准差</h5>\n<h5>min：最小值</h5>\n<h5>25%：25%分位数，排序之后排在25%位置的数</h5>\n<h5>50%：50%分位数</h5>\n<h5>75%：75%分位数</h5>\n<h5>max:最大值</h5>\n<h5></h5>\n<h4>我们由mean行看到多重疾病标签的图像比其他标签的图像要少得多。因此不能以原始数据形式加载图像，这里将使用scikitlearn随机地过采样，这样就可以修复这个类的不平衡。</h4>\n<h5>现在来加载图片</h5>","metadata":{}},{"cell_type":"code","source":"print(\"Shape of train data: \" + str(train.shape))\nprint(\"Shape of test data: \" + str(test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom tqdm.notebook import tqdm\n\npath = '../input/plant-pathology-2020-fgvc7/images/'\nsize = 224\n\ntrain_images = np.ndarray(shape=(train_len, size, size, 3))\nfor i in tqdm(range(train_len)):\n  img = load_img(path + f'Train_{i}.jpg', target_size=(size, size))\n  train_images[i] = np.uint8(img_to_array(img))\n\ntest_images = np.ndarray(shape=(test_len, size, size, 3))\nfor i in tqdm(range(test_len)):\n  img = load_img(path + f'Test_{i}.jpg', target_size=(size, size))\n  test_images[i] = np.uint8(img_to_array(img))\n\ntrain_images.shape, test_images.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>现在让我们看一下分类好的图片</h4>\n<h5></h5>\n<h5>训练集的前四张图片：</h5>","metadata":{}},{"cell_type":"code","source":"for i in range(4):\n\tplt.subplot(220 + 1 + i)\n# \tplt.title(train['image_id'][i])\n\tplt.imshow(np.uint8(train_images[i]), interpolation = 'nearest', aspect='auto')\nplt.show()\nplt.savefig('train_images.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"测试集的前四张图片：","metadata":{}},{"cell_type":"code","source":"for i in range(4):\n\tplt.subplot(220 + 1 + i)\n\tplt.title(test['image_id'][i])\n\tplt.imshow(np.uint8(test_images[i]), interpolation = 'nearest', aspect='auto')\nplt.show()\nplt.savefig('test_images.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"下面，将训练集分成训练和验两部分数据","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_images, target.to_numpy(), test_size=0.1, random_state=289) \n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"现在利用RandomOverSampler解决样本不平衡的问题","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler #过采样处理\n\nros = RandomOverSampler()\nx_train, y_train = ros.fit_resample(x_train.reshape((-1, size * size * 3)), y_train)\nx_train = x_train.reshape((-1, size, size, 3))\nx_train.shape, y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>现在为进入MobileNetV3模型准备数据。</h5>\n<h5>在这里，我使用ImageDataGenerator通过使用参数来旋转、水平翻转和垂直翻转，也为我们提供了更多的图像。</h5>","metadata":{}},{"cell_type":"code","source":"# samplewise_center：布尔值，使输入数据的每个样本均值为0\n# samplewise_std_normalization：布尔值，将输入的每个样本除以其自身的标准差\nfrom keras_preprocessing.image import ImageDataGenerator\n\nbatch_size = 8\n\ntrain_datagen = ImageDataGenerator(samplewise_center = True,\n                                   samplewise_std_normalization = True,\n                                   horizontal_flip = True,\n                                   vertical_flip = True,\n                                   rotation_range=70)\n\ntrain_generator = train_datagen.flow(\n    x = x_train, \n    y = y_train,\n    batch_size = batch_size)\n\nvalidation_datagen = ImageDataGenerator(samplewise_center = True,\n                                        samplewise_std_normalization = True)\n\nvalidation_generator = validation_datagen.flow(\n    x = x_test, \n    y = y_test,\n    batch_size = batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"让我们看看图像处理后的样子，以及它们进入模型后的样子。","metadata":{}},{"cell_type":"code","source":"idx = np.random.randint(8)\nprint(idx)\nx, y = train_generator.__getitem__(idx)\nprint(len(x))\nprint(len(y))\nplt.title(y[idx])\nplt.imshow(x[idx])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>在这里建立模型。我将使用预先训练好的MobileNetV3Large进行深度CNN，然后将其输入到一个Dense层中预测4个类。</h5>\n<h5>这里使用损失函数KL散度、Adam优化器和精度度量进行编译。</h5>","metadata":{}},{"cell_type":"code","source":"def create_model():\n    pre_trained = tf.keras.applications.MobileNetV3Large(input_shape=(size, size, 3), weights='imagenet', include_top=False)\n\n    model = tf.keras.Sequential([\n      pre_trained,\n        tf.keras.layers.GlobalAveragePooling2D(),\n#       tf.keras.layers.Flatten(),\n      tf.keras.layers.Dropout(0.3),\n      tf.keras.layers.Dense(4, activation='softmax')\n      ])\n    model.compile(\n        loss = 'kullback_leibler_divergence', \n        optimizer = 'adam', \n        metrics = ['accuracy'])\n    return model\n\nmodel = create_model()\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"现在定义一些模型参数并设置一些回调,并开始训练模型","metadata":{}},{"cell_type":"code","source":"epochs = 150\nprint(batch_size)\nsteps_per_epoch = x_train.shape[0] // batch_size\nvalidation_steps = x_test.shape[0] // batch_size\nprint(steps_per_epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EarlyStopping是用于提前停止训练的callbacks。当训练集上的loss不在减小（即减小的程度小于某个阈值）的时候停止继续训练。\n# patience: 当early stop被激活(如发现loss相比上一个epoch训练没有下降)，则经过patience个epoch后停止训练\n# restore_best_weights：是否从时期以受监视数量的最佳值恢复模型权重。如果为False，则使用在训练的最后一步获得的模型权重。\nes = tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, verbose=1)\n# 该回调函数将在每个epoch后保存模型到filepath\n# verbose：信息展示模式，0或1。为1表示输出epoch模型保存信息，默认为0表示不输出该信息\nmc = tf.keras.callbacks.ModelCheckpoint('model.hdf5', save_best_only=True, verbose=0)\n# 在训练过程中如果出现了损失平台（loss plateau），即损失率不怎么变化时，改变学习率\nrlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, verbose=1)\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\nrampup_epochs = 40\nsustain_epochs = 20\nexp_decay = .8\n\ndef lrfn(epoch):\n  if epoch < rampup_epochs:\n    return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n  elif epoch < rampup_epochs + sustain_epochs:\n    return max_lr\n  else:\n    return min_lr\n# 学习率调度函数LearningRateScheduler\nlr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n\nrang = np.arange(epochs)\ny = [lrfn(x) for x in rang]\nplt.plot(rang, y)\nprint('Learning rate per epoch:')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    x = train_generator,  \n    validation_data = validation_generator,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch,\n    validation_steps = validation_steps,\n    verbose=1,\n    callbacks=[es, lr, mc, rlr])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"下面我们来看一看预测准确率和损失度","metadata":{}},{"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# samplewise_center：布尔值，使输入数据的每个样本均值为0\n# samplewise_std_normalization：布尔值，将输入的每个样本除以其自身的标准差\ntest_datagen = ImageDataGenerator(samplewise_center = True,\n                                 samplewise_std_normalization = True)\n\ntest_generator = test_datagen.flow(\n    x = test_images,\n    shuffle = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities = model.predict(test_generator, steps = len(test_generator))\nprint(probabilities)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_image(i):\n    plt.title(test['image_id'][i])\n    plt.imshow(np.uint8(test_images[i]),cmap=plt.cm.binary)\n\ndef plot_value_array(results):\n    r= results\n    c = ['healthy','multiple_diseases','rush','scab']\n    plt.xticks(rotation=90)\n    plt.bar(c,r)\n    plt.ylim([0, 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"让我们看一看前50张测试图片的预测结果","metadata":{}},{"cell_type":"code","source":"for i in range(50):\n    print(probabilities[i])\n    plt.figure(figsize=(6, 3))\n    plt.subplot(1, 2, 1)\n    plot_image(i)\n    plt.subplot(1, 2, 2)\n    plot_value_array(probabilities[i])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}