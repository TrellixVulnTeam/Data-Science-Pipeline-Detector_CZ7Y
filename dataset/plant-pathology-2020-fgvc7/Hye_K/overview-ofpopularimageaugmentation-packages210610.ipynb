{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%HTML\n<style type=\"text/css\">\n\ndiv.h2 {\n    background-color: steelblue; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 20px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\ndiv.h3 {\n    color: steelblue; \n    font-size: 14px; \n    margin-top: 20px; \n    margin-bottom:4px;\n}\ndiv.h4 {\n    font-size: 15px; \n    margin-top: 20px; \n    margin-bottom: 8px;\n}\nspan.note {\n    font-size: 5; \n    color: gray; \n    font-style: italic;\n}\nspan.captiona {\n    font-size: 5; \n    color: dimgray; \n    font-style: italic;\n    margin-left: 130px;\n    vertical-align: top;\n}\nhr {\n    display: block; \n    color: gray\n    height: 1px; \n    border: 0; \n    border-top: 1px solid;\n}\nhr.light {\n    display: block; \n    color: lightgray\n    height: 1px; \n    border: 0; \n    border-top: 1px solid;\n}\ntable.dataframe th \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n}\ntable.dataframe td \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 14px;\n    text-align: center;\n} \ntable.rules th \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 14px;\n}\ntable.rules td \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 13px;\n    text-align: center;\n} \ntable.rules tr.best\n{\n    color: green;\n}\n\n</style>","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-10T05:57:23.673769Z","iopub.execute_input":"2021-06-10T05:57:23.675984Z","iopub.status.idle":"2021-06-10T05:57:23.685194Z","shell.execute_reply.started":"2021-06-10T05:57:23.675913Z","shell.execute_reply":"2021-06-10T05:57:23.684045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h1> Increasing the amount and diversity of data. </div>\n\nDeep learning techniques have found great success when it comes to computer vision tasks namely image recognition, image segmentation, object detection, etc. Such deep learning techniques are heavily dependent on big data to avoid overfitting. So what do you do when you have limited data? Your go for Data Augmentation. [Data Augmentation techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them](https://link.springer.com/content/pdf/10.1186%2Fs40537-019-0197-0.pdf). In this notebook, we shall look at some of the common image augmentation techniques and some of the popular libraries which help to achieve these augmentations. \n\n<div class=h2>Types of Augmentations </div> \n \nThere are broadly two kinds of Augmentations","metadata":{}},{"cell_type":"markdown","source":"![](https://imgur.com/vz7gkdD.png)","metadata":{}},{"cell_type":"markdown","source":"\n<div class=h2>Commonly used Augmentation Techniques </div> \n \nSome of the commonly used Image data augmentations techniques are:\n* <div class=h3>Flipping  </div> \nThis means flipping the image horizontally or vertically\n* <div class=h3>Rotation  </div>\nThis means to rotate the image by a given angle in the clockwise or anticlockwise direction\n* <div class=h3>Cropping  </div>\nDuring cropping, a section of the image is sampled randomly\n* <div class=h3>Brightness  </div>\nIncrease or decrease the brightness of the image\n* <div class=h3>Scaling </div>Scaling\nImages can be scaled outward or inward. When scaled outward, the image size increases while the image size decreases when scaled inwards.\n* <div class=h3>Noise Addition </div>\nWe can also add gaussian noise to the existing images.\n\nBefore delving into the data augmentation packages let us understand some points about an image.For that we will need to import the necessary libraries and the image dataset","metadata":{}},{"cell_type":"markdown","source":"### Importing necessary libraries and the dataset","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport glob\n\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\n# skimage\nfrom skimage.io import imshow, imread, imsave\nfrom skimage.transform import rotate, AffineTransform, warp,rescale, resize, downscale_local_mean\nfrom skimage import color,data\nfrom skimage.exposure import adjust_gamma\nfrom skimage.util import random_noise\n\n#OpenCV-Python\nimport cv2\n\n# imgaug\nimport imageio\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\n# Albumentations\nimport albumentations as A\n\n# Augmentor\n!pip install augmentor\nimport Augmentor \n\n# Keras\nfrom keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img \n\n# SOLT\n!pip install solt\nimport solt\nimport solt.transforms as slt\n\n#visualisation\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport seaborn as sns\nfrom IPython.display import HTML, Image\n\n#source: https://www.kaggle.com/jpmiller/nfl-punt-analytics/edit\n# set additional display options for report\npd.set_option(\"display.max_columns\", 100)\nth_props = [('font-size', '13px'), ('background-color', 'white'), \n            ('color', '#666666')]\ntd_props = [('font-size', '15px'), ('background-color', 'white')]\nstyles = [dict(selector=\"td\", props=td_props), dict(selector=\"th\", \n            props=th_props)]\n\n#warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n#Helper function to display the images in a grid\n# Source: https://stackoverflow.com/questions/42040747/more-idiomatic-way-to-display-images-in-a-grid-with-numpy which was pointed by\n# this excellent article: https://towardsdatascience.com/data-augmentation-for-deep-learning-4fe21d1a4eb9\ndef gallery(array, ncols=3):\n    '''\n    Function to arange images into a grid.\n    INPUT:\n        array - numpy array containing images\n        ncols - number of columns in resulting imahe grid\n    OUTPUT:\n        result - reshaped array into a grid with given number of columns\n    '''\n    nindex, height, width, intensity = array.shape\n    nrows = nindex//ncols\n    assert nindex == nrows*ncols\n    result = (array.reshape(nrows, ncols, height, width, intensity)\n              .swapaxes(1,2)\n              .reshape(height*nrows, width*ncols, intensity))\n    return result\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining data path\nImage_Data_Path = \"../input/plant-pathology-2020-fgvc7/images/\"\n\ntrain_data = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/train.csv\")\ntest_data = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/test.csv\")\n\n# Loading the training images #refer: https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\ndef load_image(image_id):\n    file_path = image_id + \".jpg\"\n    image = imread(Image_Data_Path + file_path)\n    return image\n\ntrain_images = train_data[\"image_id\"][:50].apply(load_image)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A look at some of the images in the training set","metadata":{}},{"cell_type":"code","source":"\n# image titles\n#image_titles = ['Frame1', 'Frame2', 'Frame3']\n\n# plotting multiple images using subplots\nfig,ax = plt.subplots(nrows=2,ncols=3,figsize=(30,16))\nfor col in range(3):\n    for row in range(2):\n        ax[row,col].imshow(train_images.loc[train_images.index[row*3+col]])\n        #ax[row,col].set_title(image_titles[i])    \n        ax[row,col].set_xticks([])\n        ax[row,col].set_yticks([])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h2>Images as Arrays </div>\nAn image is nothing but a standard Numpy array containing pixels of data points. You can think of pixels to be tiny blocks of information arranged in the form of a 2 D grid, and the depth of a pixel refers to the colour information present in it","metadata":{}},{"cell_type":"code","source":"image = train_images[15]\nimshow(image)\nprint(image.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Coloured images** are represented as a combination of Red, Blue, and Green, and all the other colours can be achieved by mixing these primary colours in the correct proportions.","metadata":{}},{"cell_type":"code","source":"# red filter [R,G,B]\nred_filter = [1,0,0]\n# blue filter\nblue_filter = [0,0,1]\n# green filter\ngreen_filter = [0,1,0]\n\n\n# matplotlib code to display\nfig,ax = plt.subplots(nrows=1,ncols=3,figsize=(30,16))\nax[0].imshow(image*red_filter)\nax[0].set_title(\"Red Filter\",fontweight=\"bold\", size=30)\nax[1].imshow(image*blue_filter)\nax[1].set_title(\"BLue Filter\",fontweight=\"bold\", size=30)\nax[2].imshow(image*green_filter)\nax[2].set_title(\"Green Filter\",fontweight=\"bold\", size=30);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A **grayscale image** consists of 8 bits per pixel. This means it can have 256 different shades where 0 pixels will represent black colour while 255 denotes white. For example, the image below shows a grayscale image represented in the form of an array. A grayscale image has only 1 channel where the channel represents dimension.","metadata":{}},{"cell_type":"code","source":"# import color sub-module\nfrom skimage import color\n\n# converting image to grayscale\ngrayscale_image = color.rgb2gray(image)\ngrayscale_image.shape\nimshow(grayscale_image)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h1>Popular Image Augmentation packages </div> \n\nThere are a lot of image augmentations packages. However, we shall cover some of the popular ones like:\n\n* <div class=h3> skimage </div> \n* <div class=h3> opencv </div> \n* <div class=h3> imgaug </div> \n* <div class=h3> Albumentations </div> \n* <div class=h3> Augmentor </div> \n* <div class=h3> Keras(ImageDataGenerator class) </div> \n* <div class=h3> SOLT </div> \n\n\n# 1. Data Augmentation using skimage \n  \n[scikit-image](https://scikit-image.org/) is an open source Python package that works with numpy arrays. It implements algorithms and utilities for use in research, education and industry applications. It is a fairly simple and straightforward library even for those who are new to Python’s ecosystem. This code is of high-quality and peer-reviewed, written by an active community of volunteers.","metadata":{}},{"cell_type":"markdown","source":"<div class=h3> 1.1 Flipping with skimage </div> ","metadata":{}},{"cell_type":"code","source":"#Horizontally flipped\nhflipped_image= np.fliplr(image) #fliplr reverse the order of columns of pixels in matrix\n\n#Vertically flipped\nvflipped_image= np.flipud(image) #flipud reverse the order of rows of pixels in matrix\n\nfig,ax = plt.subplots(nrows=1,ncols=3,figsize=(30,16))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=30)\nax[1].imshow(hflipped_image)\nax[1].set_title(\"Horizontally flipped\", size=30)\nax[2].imshow(vflipped_image)\nax[2].set_title(\"Vertically flipped\", size=30);\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 1.2 Rotation with skimage </div> ","metadata":{}},{"cell_type":"code","source":"# clockwise rotation\nrot_clockwise_image = rotate(image, angle=45) \n# Anticlockwise rotation\nrot_anticlockwise_image = rotate(image, angle=-45)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig,ax = plt.subplots(nrows=1,ncols=3,figsize=(30,16))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=30)\nax[1].imshow(rot_clockwise_image)\nax[1].set_title(\"+45 degree Rotation\", size=30)\nax[2].imshow(rot_anticlockwise_image)\nax[2].set_title(\"-45 degree rotation\", size=30);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 1.3 Cropping with skimage </div> ","metadata":{}},{"cell_type":"code","source":"# source: https://www.kaggle.com/safavieh/image-augmentation-using-skimage\nimport random\nimport pylab as pl \ndef randRange(a, b):\n    '''\n    a utility function to generate random float values in desired range\n    '''\n    return pl.rand() * (b - a) + a\ndef randomCrop(im):\n    '''\n    croping the image in the center from a random margin from the borders\n    '''\n    margin = 1/3.5\n    start = [int(randRange(0, im.shape[0] * margin)),\n             int(randRange(0, im.shape[1] * margin))]\n    end = [int(randRange(im.shape[0] * (1-margin), im.shape[0])), \n           int(randRange(im.shape[1] * (1-margin), im.shape[1]))]\n    cropped_image = (im[start[0]:end[0], start[1]:end[1]])\n    return cropped_image\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,12))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=20)\nax[1].imshow(randomCrop(image))\nax[1].set_title(\"Cropped\", size=20)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 1.4 Brightness Manipulation </div> ","metadata":{}},{"cell_type":"code","source":"\nimage_bright = adjust_gamma(image, gamma=0.5,gain=1)\nimage_dark = adjust_gamma(image, gamma=2,gain=1)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(20,12))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=20)\nax[1].imshow(image_bright)\nax[1].set_title(\"Brightened Image\", size=20)\nax[2].imshow(image_dark)\nax[2].set_title(\"Darkened Image\", size=20)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 1.5 Scaling with skimage </div> ","metadata":{"trusted":true}},{"cell_type":"code","source":"\nimage_resized = resize(image, (image.shape[0] // 2, image.shape[1] // 2),\n                       anti_aliasing=True)\n#image_downscaled = downscale_local_mean(image, (4, 3))\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(30,16))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=20)\nax[1].imshow(image_resized)\nax[1].set_title(\"Resized image\",size=20)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 1.6 Noise Addition with skimage </div> ","metadata":{}},{"cell_type":"code","source":"noisy_image= random_noise(image)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig, ax = plt.subplots(nrows=1, ncols=2,figsize=(30,16))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=20)\nax[1].imshow(noisy_image)\nax[1].set_title(\"Image after adding noise\",size=20)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Augmentation using OpenCV-Python\n\n![](https://opencv-python-tutroals.readthedocs.io/en/latest/_static/opencv-logo-white.png)\n\n[OpenCV](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html) essentially stands for Open Source Computer Vision Library. Although it is written in optimized C/C++, it has interfaces for Python and Java along with C++. \nOpenCV-Python is the python API for OpenCV. You can think of it as a python wrapper around the C++ implementation of OpenCV. OpenCV-Python is not only fast (since the background consists of code written in C/C++) but is also easy to code and deploy(due to the Python wrapper in foreground). This makes it a great choice to perform computationally intensive programs.\n","metadata":{}},{"cell_type":"code","source":"# selecting a sample image\nimage13 = train_images[13]\nimshow(image13)\nprint(image13.shape)\nplt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div class=h3> 2.1 Flipping with opencv </div> \n\nThe image is flipped according to the value of flipCode as follows:\n\n* flipcode = 0: flip vertically\n* flipcode > 0: flip horizontally\n* flipcode < 0: flip vertically and horizontally","metadata":{}},{"cell_type":"code","source":"#vertical flip\nimg_flip_ud = cv2.flip(image13, 0)\nplt.imshow(img_flip_ud)\n\n#horizontal flip\nimg_flip_lr = cv2.flip(image13, 1)\nplt.imshow(img_flip_lr)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,12))\nax[0].imshow(img_flip_ud)\nax[0].set_title(\"vertical flip\", size=20)\nax[1].imshow(img_flip_lr)\nax[1].set_title(\"horizontal flip\", size=20)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 2.2 Rotation with opencv </div> \n\nThe OpenCV function that rotates the image is `cv2.rotate()`.The following three constants can be specified in rotateCode.\n\n```\ncv2.ROTATE_90_CLOCKWISE\ncv2.ROTATE_90_COUNTERCLOCKWISE\ncv2.ROTATE_180\n```\n","metadata":{}},{"cell_type":"code","source":"img_rotate_90_clockwise = cv2.rotate(image13, cv2.ROTATE_90_CLOCKWISE)\nimg_rotate_90_counterclockwise = cv2.rotate(image13, cv2.ROTATE_90_COUNTERCLOCKWISE)\nimg_rotate_180 = cv2.rotate(image13, cv2.ROTATE_180)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(20,12))\nax[0].imshow(img_rotate_90_clockwise)\nax[0].set_title(\"90 degrees clockwise\", size=20)\nax[1].imshow(img_rotate_90_counterclockwise)\nax[1].set_title(\"90 degrees anticlockwise\", size=20)\nax[2].imshow(img_rotate_180)\nax[2].set_title(\"180 degree rotation\", size=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 2.3 Scaling with opencv </div> \n\nScaling is just resizing of the image","metadata":{}},{"cell_type":"code","source":"#RESIZE\ndef resize_image(image,w,h):\n    resized_image = image=cv2.resize(image,(w,h))\n    return resized_image\n\nimshow(resize_image(image13, 500,500))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 2.4 Brightness Manipulation with opencv </div> \n","metadata":{}},{"cell_type":"code","source":"def add_light(image, gamma):\n    invGamma = 1.0 / gamma\n    table = np.array([((i / 255.0) ** invGamma) * 255\n                      for i in np.arange(0, 256)]).astype(\"uint8\")\n    image=cv2.LUT(image, table)\n    return image\n\nimshow(add_light(image13,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 2.5 Cropping with opencv </div> \n","metadata":{}},{"cell_type":"code","source":"#crop\ndef crop_image(image,y1,y2,x1,x2):\n    image=image[y1:y2,x1:x2]\n    return image\nimshow(crop_image(image13,200,800,250,1500))#(y1,y2,x1,x2)(bottom,top,left,right)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 2.6 Gaussian Blur with opencv </div> \n\nThe Gaussian filter is a low-pass filter that removes the high-frequency components are reduced.","metadata":{}},{"cell_type":"code","source":"def gaussian_blur(image,blur):\n    image = cv2.GaussianBlur(image,(5,5),blur)\n    return image\n\nimshow(gaussian_blur(image13,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Augmentation using imgaug  \n  \n\n[imgaug](https://imgaug.readthedocs.io/en/latest/) is a library for image augmentation in machine learning experiments. It supports a wide range of augmentation techniques, allows to easily combine these and to execute them in random order or on multiple CPU cores, has a simple yet powerful stochastic interface and can not only augment images, but also keypoints/landmarks, bounding boxes, heatmaps and segmentation maps.\n![](https://cdn-images-1.medium.com/max/800/1*QT3A5EZIp1EXSVIiB4SlCA.png)\n\n","metadata":{}},{"cell_type":"code","source":"# selecting a sample image\nimage2 = train_images[25]\nimshow(image2)\nprint(image2.shape)\nplt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 2.1 Flipping with imgaug </div> ","metadata":{}},{"cell_type":"code","source":"\n#Horizontally flipped\nhflip= iaa.Fliplr(p=1.0)\nhflipped_image2= hflip.augment_image(image2)\n\n#Vertically flipped\nvflip= iaa.Flipud(p=1.0) \nvflipped_image2= vflip.augment_image(image2)\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image=image2\nfig,ax = plt.subplots(nrows=1,ncols=3,figsize=(30,16))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=30)\nax[1].imshow(hflipped_image2)\nax[1].set_title(\"Horizontally flipped\", size=30)\nax[2].imshow(vflipped_image2)\nax[2].set_title(\"Vertically flipped\", size=30);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 2.2 Rotation with imgaug </div> ","metadata":{}},{"cell_type":"code","source":"# clockwise rotation\nrot = iaa.Affine(rotate=(-25,25))\nrot_clockwise_image2 = rot.augment_image(image2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image=image2\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(30,16))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=30)\nax[1].imshow(rot_clockwise_image2)\nax[1].set_title(\"Rotated Image\", size=30)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 2.3 Cropping with imgaug </div> ","metadata":{}},{"cell_type":"code","source":"\nimage=image2\ncrop = iaa.Crop(percent=(0, 0.2)) # crop image\ncorp_image=crop.augment_image(image)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,12))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=20)\nax[1].imshow(randomCrop(corp_image))\nax[1].set_title(\"Cropped\", size=20)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 2.4 Brightness Manipulation with imgaug </div> ","metadata":{}},{"cell_type":"code","source":"# 4. Brightness\n\nimage = image2\n# bright\n#contrast1=iaa.GammaContrast(gamma=0.5)\n#brightened_image = contrast1.augment_image(image)\n\n#dark\n#contrast2=iaa.GammaContrast(gamma=2)\n#darkened_image = contrast2.augment_image(image)\n\n#Somehow, this line of code gave me an error in the Kaggle notebook but worked fine in other IDEs. \n#So I have inserted the desired result.ULet me know if the above code works for you?","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(20,12))\n#ax[0].imshow(image)\n#ax[0].set_title(\"Original Image\", size=20)\n#ax[1].imshow(brightened_image)\n#ax[1].set_title(\"Brightened Image\", size=20)\n#ax[2].imshow(darkened_image)\n#ax[2].set_title(\"darkened_image\", size=20)\n\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://imgur.com/cwg7Oiy.png)","metadata":{}},{"cell_type":"markdown","source":"<div class=h3> 2.5 Scaling with imgaug </div> ","metadata":{}},{"cell_type":"code","source":"image= image2\nscale_im=iaa.Affine(scale={\"x\": (1.5, 1.0), \"y\": (0.5, 1.0)})\nscale_image =scale_im.augment_image(image)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,12))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=20)\nax[1].imshow(scale_image)\nax[1].set_title(\"Scaled\", size=20)","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3> 2.6 Noise addition with imgaug </div> ","metadata":{}},{"cell_type":"code","source":"image= image2\ngaussian_noise=iaa.AdditiveGaussianNoise(15,20)\nnoise_image=gaussian_noise.augment_image(image)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,12))\nax[0].imshow(image)\nax[0].set_title(\"Original Image\", size=20)\nax[1].imshow(noise_image)\nax[1].set_title(\"Gaussian Noise added\", size=20)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=h3>2.7 Augmentation pipeline </div> \n\nThe imgaug library provides a very useful feature called **Augmentation pipeline**. Such a pipeline is a sequence of steps that can be applied in a fixed or random order. This also gives the flexibility to apply certain transformations to a few images and other transformations to other images. In the following example, we are applying the flip, sharpen,crop etc transformations on some of the images. The blur and affline transformations will be applied sometimes and all these transformations will be applied in random order.","metadata":{"trusted":true}},{"cell_type":"code","source":"# Defining a pipeline.\n# The example has been taken from the documentation\naug_pipeline = iaa.Sequential([\n    iaa.SomeOf((0,3),[\n        iaa.Fliplr(1.0), # horizontally flip\n        iaa.Flipud(1.0),# Vertical flip\n        iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n        iaa.Crop(percent=(0, 0.4)),\n        iaa.Sometimes(0.5, iaa.Affine(rotate=5)),\n        iaa.Sometimes( 0.5,iaa.GaussianBlur(sigma=(0, 0.5))),\n        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n    ])\n], \nrandom_order=True # apply the augmentations in random order\n)\n\n# apply augmentation pipeline to sample image\nimages_aug = np.array([aug_pipeline.augment_image(image2) for _ in range(16)])\n\n# visualize the augmented images\nplt.figure(figsize=(30,10))\nplt.axis('off')\nplt.imshow(gallery(images_aug, ncols = 4))\nplt.title('Augmentation  examples')\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Augmentation using Albumentations \n\n![](https://albumentations.readthedocs.io/en/latest/_static/logo.png)\n\n[Albumentations](https://albumentations.readthedocs.io/en/latest/index.html#) is a fast image augmentation library and easy to use wrapper around other libraries.It is based on numpy, OpenCV, imgaug picking the best from each of them.It is written by Kagglers and was used to get top results in many DL competitions at Kaggle, topcoder, CVPR, MICCAI. Read more about it here: https://www.mdpi.com/2078-2489/11/2/125","metadata":{"trusted":true}},{"cell_type":"code","source":"# initialize augmentations\nhorizontal_flip = A.HorizontalFlip(p=1)\nrotate = A.ShiftScaleRotate(p=1)\ngaus_noise = A.GaussNoise() # gaussian noise\nbright_contrast = A.RandomBrightnessContrast(p=1) # random brightness and contrast\ngamma = A.RandomGamma(p=1) # random gamma\nblur = A.Blur()\n\n# apply augmentations to images\nimg_flip = horizontal_flip(image = image2)\nimg_gaus = gaus_noise(image = image2)\nimg_rotate = rotate(image = image2)\nimg_bc = bright_contrast(image = image2)\nimg_gamma = gamma(image = image2)\nimg_blur = blur(image = image2)\n\n# access the augmented image by 'image' key\nimg_list = [img_flip['image'],img_gaus['image'], img_rotate['image'], img_bc['image'], img_gamma['image'], img_blur['image']]\n\n# visualize the augmented images\nplt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(gallery(np.array(img_list), ncols = 3))\nplt.title('Augmentation examples')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Data Augmentation using Augmentor \n\n![](https://augmentor.readthedocs.io/en/master/_static/logo.png)\n","metadata":{"trusted":true}},{"cell_type":"code","source":"# Passing the path of the image directory \np = Augmentor.Pipeline(source_directory=\"/kaggle/input/plant-pathology-2020-fgvc7/images\",\n                      output_directory=\"/kaggle/output\")\n  \n# Defining augmentation parameters and generating 10 samples \np.flip_left_right(probability=0.4) \np.flip_top_bottom(probability=0.8)\np.rotate(probability=0.5, max_left_rotation=5, max_right_rotation=10)\np.skew(0.4, 0.5) \np.zoom(probability = 0.2, min_factor = 1.1, max_factor = 1.5) \np.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Keras Image Data Generator  \n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRgbD4KXC9PBSYSLHojvt-qcu99NCfy4AcN3eEGFM1YTmLIAJFo&usqp=CAU)\nThe Keras library has a built in class created just for the purpose of adding transformations to images.This class is called **ImageDataGenerator** and it generates batches of tensor image data with real-time data augmentations. ","metadata":{}},{"cell_type":"code","source":"# selecting a sample image\nimage5 = train_images[15]\nimshow(image5)\nprint(image5.shape)\nplt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a directory where the transformed images will be stored. The directory will be called keras_augmentation and its path is as follows:\n","metadata":{}},{"cell_type":"code","source":"# To delete any previously created directory\n#import shutil\n#shutil.rmtree(\"../output/keras_augmentations\")\n\n\n# Creating a new directory for placing augmented images\nimport os\nos.mkdir(\"../output/keras_augmentations\")\n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Augmentation process\ndatagen = ImageDataGenerator( \n        rotation_range = 40, \n        shear_range = 0.2, \n        zoom_range = 0.2, \n        horizontal_flip = True, \n        brightness_range = (0.5, 1.5)) \n\nimg_arr = img_to_array(image5)\nimg_arr = img_arr.reshape((1,) + img_arr.shape)\n\ni = 0\nfor batch in datagen.flow(\n    img_arr,\n    batch_size=1,\n    save_to_dir='../output/keras_augmentations',\n    save_prefix='Augmented_image',\n    save_format='jpeg'):\n    i += 1\n    if i > 20: # create 20 augmented images\n        break  # otherwise the generator would loop indefinitely","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These are just a few of the options available (for more, see the [documentation](https://keras.io/preprocessing/image/)). Let's see what do these transformation mean- Source: [Keras Blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n\n* **rotation_range** is a value in degrees (0-180), a range within which to randomly rotate pictures\n* shear_range is for randomly applying [shearing transformations](https://en.wikipedia.org/wiki/Shear_mapping)\n* zoom_range is for randomly zooming inside pictures\n* horizontal_flip is for randomly flipping half of the images horizontally --relevant when there are no assumptions of horizontal assymetry (e.g. real-world pictures).\n* fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n\nLet's look at the images which have been created in our directory","metadata":{}},{"cell_type":"code","source":"images = os.listdir(\"../output/keras_augmentations/\")\nimages","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at the augmented images\naug_images = []\nfor img_path in glob.glob(\"../output/keras_augmentations/*.jpeg\"):\n    aug_images.append(mpimg.imread(img_path))\n\nplt.figure(figsize=(20,10))\ncolumns = 5\nfor i, image in enumerate(aug_images):\n    plt.subplot(len(aug_images) / columns + 1, columns, i + 1)\n    plt.imshow(image)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selecting a sample image\nimage5 = train_images[25]\nimshow(image5)\nprint(image5.shape)\nplt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. SOLT : Streaming over lightweight data transformations \n\n![](https://github.com/MIPT-Oulu/solt/raw/master/doc/source/_static/logo.png)\n[SOLT](https://github.com/MIPT-Oulu/solt) is a fast data augmentation library, supporting arbitrary amount of images, segmentation masks, keypoints and data labels. It has OpenCV in its back-end, thus it works very fast.","metadata":{}},{"cell_type":"code","source":"h,w,c = image5.shape\nimg = image5[:w]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stream = solt.Stream([\n    slt.Rotate(angle_range=(-90, 90), p=1, padding='r'),\n    slt.Flip(axis=1, p=0.5),\n    slt.Flip(axis=0, p=0.5),\n    slt.Shear(range_x=0.3, range_y=0.8, p=0.5, padding='r'),\n    slt.Scale(range_x=(0.8, 1.3), padding='r', range_y=(0.8, 1.3), same=False, p=0.5),\n    slt.Pad((w, h), 'r'),\n    slt.Crop((w, w), 'r'),\n    slt.CvtColor('rgb2gs', keep_dim=True, p=0.2),\n    slt.HSV((0, 10), (0, 10), (0, 10)),\n    slt.Blur(k_size=7, blur_type='m'),\n    solt.SelectiveStream([\n        slt.CutOut(40, p=1),\n        slt.CutOut(50, p=1),\n        slt.CutOut(10, p=1),\n        solt.Stream(),\n        solt.Stream(),\n    ], n=3),\n], ignore_fast_mode=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(16,16))\nn_augs = 6\n\n\nrandom.seed(42)\nfor i in range(n_augs):\n    img_aug = stream({'image': img}, return_torch=False, ).data[0].squeeze()\n\n    ax = fig.add_subplot(1,n_augs,i+1)\n    if i == 0:\n        ax.imshow(img)\n    else:\n        ax.imshow(img_aug)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### References and Some great resources :\n\n- [Data Augmentation | How to use Deep Learning when you have Limited Data — Part 2](https://nanonets.com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/)\n- [Data Augmentation for Deep Learning](https://towardsdatascience.com/data-augmentation-for-deep-learning-4fe21d1a4eb9)\n\n\n","metadata":{"trusted":true}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}