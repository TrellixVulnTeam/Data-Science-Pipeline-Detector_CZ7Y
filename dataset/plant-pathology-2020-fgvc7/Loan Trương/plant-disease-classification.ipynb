{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Importing Libraries</h1>","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport re\n\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport tensorflow as tf\nfrom IPython.display import SVG\nfrom keras.utils import plot_model\nimport tensorflow.keras.layers as L\nfrom keras.utils import model_to_dot\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.applications import DenseNet201, InceptionV3, ResNet50V2, InceptionResNetV2, VGG19, VGG16\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\ntqdm.pandas()\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nnp.random.seed(0)\ntf.random.set_seed(0)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport cv2 as cv\nfrom skimage import filters\nfrom skimage import morphology\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport matplotlib.pyplot as plt\n\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nprint(f\"Tensorflow version: {tf.__version__}\")\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nSEED = 5","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-25T13:34:32.021709Z","iopub.execute_input":"2021-05-25T13:34:32.022082Z","iopub.status.idle":"2021-05-25T13:34:32.036795Z","shell.execute_reply.started":"2021-05-25T13:34:32.022053Z","shell.execute_reply":"2021-05-25T13:34:32.03571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Loading data</h1>","metadata":{}},{"cell_type":"code","source":"EPOCHS = 40\nSAMPLE_LEN = 1821\nINPUT_PATH = '/kaggle/input/plant-pathology-2020-fgvc7/'\nIMG_PATH = INPUT_PATH + 'images/'\nTRAIN_DATA = INPUT_PATH + 'train.csv'\nTEST_DATA = INPUT_PATH + 'test.csv'\nSAMPLE_SUB = INPUT_PATH + 'sample_submission.csv'\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:51:59.546964Z","iopub.execute_input":"2021-05-25T13:51:59.547343Z","iopub.status.idle":"2021-05-25T13:51:59.553852Z","shell.execute_reply.started":"2021-05-25T13:51:59.547312Z","shell.execute_reply":"2021-05-25T13:51:59.552797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_DATA)\ntest_df = pd.read_csv(TEST_DATA)\nsampleSubmission_df = pd.read_csv(SAMPLE_SUB)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:34:49.332362Z","iopub.execute_input":"2021-05-25T13:34:49.332914Z","iopub.status.idle":"2021-05-25T13:34:49.390351Z","shell.execute_reply.started":"2021-05-25T13:34:49.332879Z","shell.execute_reply":"2021-05-25T13:34:49.389416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>EDA</h1>","metadata":{}},{"cell_type":"code","source":"EDA_IMG_SHAPE = (512,256)\n\ndef getImage(image_id,SHAPE=EDA_IMG_SHAPE):\n    img = cv.imread(IMG_PATH + image_id + '.jpg')\n    img = cv.resize(img,SHAPE)\n    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n    \n    return img","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:34:56.492299Z","iopub.execute_input":"2021-05-25T13:34:56.492825Z","iopub.status.idle":"2021-05-25T13:34:56.497213Z","shell.execute_reply.started":"2021-05-25T13:34:56.49279Z","shell.execute_reply":"2021-05-25T13:34:56.496415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nhealthy = [getImage(image_id) for image_id in train_df[train_df['healthy']==1].iloc[:,0]]\n\nmultiple_diseases = [getImage(image_id) for image_id in train_df[train_df['multiple_diseases']==1].iloc[:,0]]\n\nrust = [getImage(image_id) for image_id in train_df[train_df['rust']==1].iloc[:,0]]\n\nscab = [getImage(image_id) for image_id in train_df[train_df['scab']==1].iloc[:,0]]","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:35:00.670548Z","iopub.execute_input":"2021-05-25T13:35:00.671074Z","iopub.status.idle":"2021-05-25T13:36:39.826493Z","shell.execute_reply.started":"2021-05-25T13:35:00.671041Z","shell.execute_reply":"2021-05-25T13:36:39.825243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = {'healthy':healthy, 'multiple_diseases':multiple_diseases, 'rust':rust, 'scab': scab} ","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:39.828284Z","iopub.execute_input":"2021-05-25T13:36:39.828665Z","iopub.status.idle":"2021-05-25T13:36:39.833155Z","shell.execute_reply.started":"2021-05-25T13:36:39.828622Z","shell.execute_reply":"2021-05-25T13:36:39.832085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploratory Data Analysis","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotlyDataFrame(df,title):\n    \n    fig = go.Figure(data=[go.Table(\n    header = dict(values = df.columns),\n    cells = dict(values = [df[col] for col in df.columns]))])\n    \n    fig.update_layout(\n        title = title)\n    \n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:39.835294Z","iopub.execute_input":"2021-05-25T13:36:39.835752Z","iopub.status.idle":"2021-05-25T13:36:39.848238Z","shell.execute_reply.started":"2021-05-25T13:36:39.835705Z","shell.execute_reply":"2021-05-25T13:36:39.847148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train DataFrame","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotlyDataFrame(train_df.iloc[:15,:],'Train Data')","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:39.850117Z","iopub.execute_input":"2021-05-25T13:36:39.850558Z","iopub.status.idle":"2021-05-25T13:36:39.935514Z","shell.execute_reply.started":"2021-05-25T13:36:39.85051Z","shell.execute_reply":"2021-05-25T13:36:39.934616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test DataFrame","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotlyDataFrame(test_df.iloc[:15,:],'Test Data')","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:39.936857Z","iopub.execute_input":"2021-05-25T13:36:39.937139Z","iopub.status.idle":"2021-05-25T13:36:39.94956Z","shell.execute_reply.started":"2021-05-25T13:36:39.937111Z","shell.execute_reply":"2021-05-25T13:36:39.948579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sample Submission File\nplotlyDataFrame(sampleSubmission_df.iloc[:15,:], 'Sample Submission')","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:39.951017Z","iopub.execute_input":"2021-05-25T13:36:39.951301Z","iopub.status.idle":"2021-05-25T13:36:39.973129Z","shell.execute_reply.started":"2021-05-25T13:36:39.951273Z","shell.execute_reply":"2021-05-25T13:36:39.972147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Class distributions</h2>","metadata":{}},{"cell_type":"code","source":"\n\nfig = go.Figure(data=[go.Pie(labels=train_df.columns[1:],\n                             values=[np.sum(train_df[col]) for col in train_df.columns[1:]])])\n\nfig.update_traces(hoverinfo='label+percent',\n                  textinfo='value',\n                  textfont_size=20,\n                  marker=dict(line=dict(color='#000000', width=2)))\n\nfig.update_layout(title_text=\"Target Distribution of Training-Data \")\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:39.974662Z","iopub.execute_input":"2021-05-25T13:36:39.974978Z","iopub.status.idle":"2021-05-25T13:36:40.023378Z","shell.execute_reply.started":"2021-05-25T13:36:39.974941Z","shell.execute_reply":"2021-05-25T13:36:40.022373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Image Augmentation\n#Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n\n#Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n\n#Transforms include a range of operations from the field of image manipulation, such as shifts, flips, zooms, and much more.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sample Image\n\ndef getRandomImage():\n    return random.choice(classes[random.choice(train_df.columns[2:])])","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:40.025409Z","iopub.execute_input":"2021-05-25T13:36:40.02573Z","iopub.status.idle":"2021-05-25T13:36:40.032228Z","shell.execute_reply.started":"2021-05-25T13:36:40.025699Z","shell.execute_reply":"2021-05-25T13:36:40.031153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = getRandomImage()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:40.033839Z","iopub.execute_input":"2021-05-25T13:36:40.034135Z","iopub.status.idle":"2021-05-25T13:36:40.044578Z","shell.execute_reply.started":"2021-05-25T13:36:40.034104Z","shell.execute_reply":"2021-05-25T13:36:40.043472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure(go.Image(z=img))\n\nfig.update_layout(title_text=\"Smaple Image\")\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:40.045948Z","iopub.execute_input":"2021-05-25T13:36:40.046259Z","iopub.status.idle":"2021-05-25T13:36:40.50369Z","shell.execute_reply.started":"2021-05-25T13:36:40.046227Z","shell.execute_reply":"2021-05-25T13:36:40.502345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Image Augmentation With ImageDataGenerator\n#We can use this to generate augmented images using different transformations like:\n\n#Vertical Flipping\n#Horizontal Flipping\n#Shifted Images\n#Rotated Images\n#Zoomed Images\n#Images with different Brightness levels etc.\n#Plus point of using this is that it will generate images in runtime (While training a model). It means we are not supposed to store this augmented images.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = np.expand_dims(img,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:40.505691Z","iopub.execute_input":"2021-05-25T13:36:40.506143Z","iopub.status.idle":"2021-05-25T13:36:40.51242Z","shell.execute_reply.started":"2021-05-25T13:36:40.506086Z","shell.execute_reply":"2021-05-25T13:36:40.510066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ngenerator = tf.keras.preprocessing.image.ImageDataGenerator(vertical_flip=True,\n                                                            horizontal_flip=True,\n                                                            brightness_range=[0.5,1.5],\n                                                            zoom_range=[0.5,1.1])\n\niterator = generator.flow(img,batch_size=1)\n\n# fig = make_subplots(10,3,horizontal_spacing=0.01,vertical_spacing=0.01)\n\n# for i in range(30):\n#     image = iterator.next()[0].astype('uint8')\n    \n#     fig.add_trace(go.Image(z=image),i//3 + 1,i%3 + 1)\n\n# fig.update_layout(title_text=\"Augmented Images of the sample image\",\n#                  height=128*10 + 20,\n#                  width=256*3 + 20)\n\n# fig.update_xaxes(showticklabels=False)\n# fig.update_yaxes(showticklabels=False)\n\n# fig.show()\n\nfig, ax = plt.subplots(nrows=5, ncols=3, figsize=(15,10))\n\nax[0,0].imshow(img[0])\nax[0,0].set_title(\"Sample Image\",fontsize=10)\nax[0,0].set_xticks([])\nax[0,0].set_yticks([])\n\n\nfor i in range(1,15):\n    \n    image = iterator.next()[0].astype('uint8')\n    \n    ax[i//3,i%3].imshow(image)\n    ax[i//3,i%3].set_xticks([])\n    ax[i//3,i%3].set_yticks([])\n\nfig.suptitle(\"Augmented Images of the sample image\",fontsize=20)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:40.514024Z","iopub.execute_input":"2021-05-25T13:36:40.514492Z","iopub.status.idle":"2021-05-25T13:36:42.015732Z","shell.execute_reply.started":"2021-05-25T13:36:40.514445Z","shell.execute_reply":"2021-05-25T13:36:42.014805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Image Processing\n\n#I'll use Open-CV for image processing. Image processing can help us enhance our Classification-Model.\n#In our case we just want the affected leaf hence I'll try to separate out that leaf from the unnecessary background. We can use image-segmentation techniques to do this.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampleImg = getRandomImage()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:42.016923Z","iopub.execute_input":"2021-05-25T13:36:42.017347Z","iopub.status.idle":"2021-05-25T13:36:42.020715Z","shell.execute_reply.started":"2021-05-25T13:36:42.017317Z","shell.execute_reply":"2021-05-25T13:36:42.019922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convertToHSV(img):\n    return cv.cvtColor(img,cv.COLOR_RGB2HSV_FULL)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:42.021813Z","iopub.execute_input":"2021-05-25T13:36:42.022198Z","iopub.status.idle":"2021-05-25T13:36:42.035486Z","shell.execute_reply.started":"2021-05-25T13:36:42.02217Z","shell.execute_reply":"2021-05-25T13:36:42.034104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ROI (Region Of Interest) Selection\n\n\n#To select the ROI, I have used Canny edge detection to detect the edges of the leaves \n#and to find the edges of the rectangle ROI, I have used this: getROI method.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef getROI(img):\n    # convert the image to the gray-scale image\n    gray = cv.cvtColor(img,cv.COLOR_RGB2GRAY)\n    \n    # Detect the edges in the image using canny edge detection\n    edged = cv.Canny(gray,150,200)\n    \n    xm = img.shape[1]//2    # Middle coordinate of the x-axis (width of the image)\n    ym = img.shape[0]//2    # Middle coordinate of the y-axis (height of the image)\n    \n    # to find the bottom-y coordinate to the Rectangle-ROI\n    for i in range(img.shape[0]-1,-1,-1):\n        if np.sum(edged[i,xm-5:xm+5])!=0:\n            y_bottom = np.where(i+10<img.shape[0]-1,i+10,img.shape[0]-2)\n            break\n            \n    # to find the top-y coordinate to the Rectangle-ROI\n    for i in range(img.shape[0]):\n        if np.sum(edged[i,xm-5:xm+5])!=0:\n            y_top = np.where(i-10>1,i-10,2)\n            break\n    \n    # to find the top-x coordinate to the Rectangle-ROI\n    for i in range(img.shape[1]):\n        if np.sum(edged[ym-5:ym+5,i])!=0:\n            x_top = np.where(i-10>1,i-10,2)\n            break\n            \n    # to find the bottom-x coordinate to the Rectangle-ROI\n    for i in range(img.shape[1]-1,-1,-1):\n        if np.sum(edged[ym-5:ym+5,i])!=0:\n            x_bottom = np.where(i+10<img.shape[1]-1,i+10,img.shape[1]-2)\n            break\n\n    return edged,(x_top,y_top,x_bottom,y_bottom)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:42.036859Z","iopub.execute_input":"2021-05-25T13:36:42.037214Z","iopub.status.idle":"2021-05-25T13:36:42.050734Z","shell.execute_reply.started":"2021-05-25T13:36:42.037181Z","shell.execute_reply":"2021-05-25T13:36:42.049381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(20, 13))\n\nfor i in range(3):\n    orignal = getRandomImage()\n    edged, coordinates = getROI(orignal)\n    \n    roi = orignal.copy()\n    \n    (x_top,y_top,x_bottom,y_bottom) = coordinates\n    \n    roi[y_top-2:y_top,x_top:x_bottom+1] = [255,0,0]        # Top-edge\n    roi[y_bottom:y_bottom+2,x_top:x_bottom+1] = [255,0,0]  # Bottom-edge\n    roi[y_top:y_bottom+1,x_top-2:x_top] = [255,0,0]        # Left-edge\n    roi[y_top:y_bottom+1,x_bottom:x_bottom+2] = [255,0,0]  # Right-edge\n    \n    ax[i,0].imshow(orignal)\n    ax[i,0].set_title('Original Image', fontsize=15)\n    ax[i,1].imshow(edged, cmap='gray')\n    ax[i,1].set_title('Detected Edges', fontsize=15)\n    ax[i,2].imshow(roi)\n    ax[i,2].set_title('ROI', fontsize=15)\n    \nfig.suptitle(\"ROI selection using Canny Edge Detection\",fontsize=20)\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:42.052Z","iopub.execute_input":"2021-05-25T13:36:42.052358Z","iopub.status.idle":"2021-05-25T13:36:43.614701Z","shell.execute_reply.started":"2021-05-25T13:36:42.052293Z","shell.execute_reply":"2021-05-25T13:36:43.613502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This method will only work if the target leaf is in the middle area of the image & \n#the images are of good quality. There could be many cases when it will not give the \n#desired outputs.\n#Also, we can see it is not giving accurate results for many of the images & we will \n#not get a good quality of images every time in real-world scenarios. Hence, I'll try \n#some other methods to accurately get the ROI, not as a rectangle but as the shape of \n#the target leaf.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ROI selection using Watershed Transformation\n\n#Any grayscale image can be viewed as a topographic surface where high intensity denotes \n#peaks and hills while low intensity denotes valleys. You start filling every isolated \n#valleys (local minima) with different colored water (labels). As the water rises, depending \n#on the peaks (gradients) nearby, water from different valleys, obviously with different \n#colors will start to merge. To avoid that, you build barriers in the locations where water \n#merges. You continue the work of filling water and building barriers until all the peaks are \n#under water. Then the barriers you created gives you the segmentation result. This is the \n#“philosophy” behind the watershed.\n\n#You can visit the CMM webpage on watershed to understand it with the help of some animations.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfig, ax = plt.subplots(nrows=6, ncols=3, figsize=(15,15))\n\nfor i in range(3):\n    orignal = getRandomImage()\n#     blur = cv.bilateralFilter(orignal,9,75,75)\n    \n    gray = cv.cvtColor(orignal,cv.COLOR_RGB2GRAY)\n    sobel = filters.sobel(gray)\n    \n#     sobel = cv.morphologyEx(sobel, cv.MORPH_OPEN, kernel)\n#     blurred = cv.bilateralFilter(sobel.astype('float32'),9,75,75)\n    blurred = filters.gaussian(sobel, sigma=2.0)\n    \n    ym = blurred.shape[0]//2\n    xm = blurred.shape[1]//2\n    \n    markers = np.zeros(blurred.shape,dtype=np.int)\n    # using corners of the image as background\n    markers[0,0:2*xm] = 1\n    markers[2*ym-1,0:2*xm] = 1\n    markers[0:2*ym,0] = 1\n    markers[0:2*ym,2*xm-1] = 1\n    \n    # using middle part of the image as foreground\n    markers[ym-50:ym+50,xm-20:xm+20] = 2\n    \n    mask = morphology.watershed(blurred, markers)\n    \n    ax[0,i].imshow(orignal)\n    ax[0,i].set_title('Original Image', fontsize=12)\n    \n    ax[1,i].imshow(gray, cmap='gray')\n    ax[1,i].set_title('Gray Image', fontsize=12)\n    \n    ax[2,i].imshow(sobel, cmap='gray')\n    ax[2,i].set_title('After Sobel Filter', fontsize=12)\n    \n    ax[3,i].imshow(blurred, cmap='gray')\n    ax[3,i].set_title('Blurred Image', fontsize=12)\n    \n    ax[4,i].imshow(mask, cmap='gray')\n    ax[4,i].set_title('Mask', fontsize=12)\n    \n    orignal[mask==1,:] = [0,0,0]\n    \n    ax[5,i].imshow(orignal)\n    ax[5,i].set_title('Segmented Image', fontsize=12)\n    \n\nfor i in range(6):\n    for j in range(3):\n        ax[i,j].set_xticks([])\n        ax[i,j].set_yticks([])\n    \nfig.suptitle(\"Image Segmentation (ROI selection) using Watershed Transformation\",fontsize=20)\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:36:43.616855Z","iopub.execute_input":"2021-05-25T13:36:43.617258Z","iopub.status.idle":"2021-05-25T13:36:45.06414Z","shell.execute_reply.started":"2021-05-25T13:36:43.617215Z","shell.execute_reply":"2021-05-25T13:36:45.062859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#As we can see it is able to extract the foreground from the image but it is not accurate \n#as there is so much noise (unnecessary edges) in the image. This method will not work if \n#the target leaf is not in the middle of the image. Maybe we can assign foreground & \n#background markers using some different technique to improve the performance of this method.\n#We can use Canny edge detection before applying this method to reduce the area of focus.\n\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### HSV Conversion\n\n#**HSV** is closer to how humans perceive color. It has three components: **Hue, Saturation, and Value**.  This color space describes colors (hue or tint) in terms of their shade (saturation or amount of gray) and their brightness value.  \n\n#The HSV color wheel sometimes appears as a cone or cylinder, but always with these three components:\n\n#1) Hue  \n#Hue is the color portion of the model, expressed as a number from 0 to 360 degrees:  \n#* Red falls between 0 and 60 degrees.\n#* Yellow falls between 61 and 120 degrees.\n#* Green falls between 121-180 degrees.\n#* Cyan falls between 181-240 degrees.\n#* Blue falls between 241-300 degrees.\n#* Magenta falls between 301-360 degrees.\n\n#2) Saturation  \n#Saturation describes the amount of gray in a particular color, from 0 to 100 percent. Reducing this component toward zero introduces more gray and produces a faded effect. Sometimes, saturation appears as a range from just 0-1, where 0 is gray, and 1 is a primary color.\n  \n#3) Value (or Brightness)  \n#Value works in conjunction with saturation and describes the brightness or intensity of the color, from 0-100 percent, where 0 is completely black, and 100 is the brightest and reveals the most color.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#HSV Conversion\n\nfig, ax = plt.subplots(nrows=3, ncols=2, figsize=(10, 10))\n\nfor i in range(3):\n    orignal = getRandomImage()\n    hsv = convertToHSV(orignal)\n    \n    ax[i,0].imshow(orignal)\n    ax[i,0].set_title('Original Image', fontsize=15)\n    ax[i,1].imshow(hsv, cmap='gray')\n    ax[i,1].set_title('HSV Image', fontsize=15)\n    \nfig.suptitle(\"RGB to HSV Conversion\",fontsize=20)\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Gray Scale Conversion\nfig, ax = plt.subplots(nrows=3, ncols=2, figsize=(10, 10))\n\nfor i in range(3):\n    orignal = getRandomImage()\n    gray = cv.cvtColor(orignal,cv.COLOR_RGB2GRAY)\n    \n    ax[i,0].imshow(orignal)\n    ax[i,0].set_title('Original Image', fontsize=15)\n    ax[i,1].imshow(gray, cmap='gray')\n    ax[i,1].set_title('Gray Image', fontsize=15)\n    \nfig.suptitle(\"RGB to Gray Scale Conversion\",fontsize=20)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Image Examples</h2>","metadata":{}},{"cell_type":"code","source":"#Function for showing image\ndef show_images(image_ids):\n    \n    col = 5\n    row = min(len(image_ids) // col, 5)\n    \n    fig, ax = plt.subplots(row, col, figsize=(16, 8))\n    ax = ax.flatten()\n\n    for i, image_id in enumerate(image_ids):\n        image = cv2.imread(IMAGE_PATH + '/{}.jpg'.format(image_id))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        ax[i].set_axis_off()\n        ax[i].imshow(image)\n        ax[i].set_title(image_id)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:38:28.635483Z","iopub.execute_input":"2021-05-25T13:38:28.635878Z","iopub.status.idle":"2021-05-25T13:38:28.643598Z","shell.execute_reply.started":"2021-05-25T13:38:28.635844Z","shell.execute_reply":"2021-05-25T13:38:28.642648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Random samples</h3>","metadata":{}},{"cell_type":"markdown","source":"<h1>Setup TPU Config</h1>","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:38:33.829026Z","iopub.execute_input":"2021-05-25T13:38:33.829393Z","iopub.status.idle":"2021-05-25T13:38:40.03224Z","shell.execute_reply.started":"2021-05-25T13:38:33.829343Z","shell.execute_reply":"2021-05-25T13:38:40.031151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Load labels and paths</h1>","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(SAMPLE_SUB)\ntest_data = pd.read_csv(TEST_DATA)\ntrain_data = pd.read_csv(TRAIN_DATA)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:38:40.72335Z","iopub.execute_input":"2021-05-25T13:38:40.723737Z","iopub.status.idle":"2021-05-25T13:38:40.748851Z","shell.execute_reply.started":"2021-05-25T13:38:40.723698Z","shell.execute_reply":"2021-05-25T13:38:40.747847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_path(st):\n    return GCS_DS_PATH + '/images/' + st + '.jpg'\n\ntest_paths = test_data.image_id.apply(format_path).values\ntrain_paths = train_data.image_id.apply(format_path).values\n\ntrain_labels = np.float32(train_data.loc[:, 'healthy':'scab'].values)\ntrain_paths, valid_paths, train_labels, valid_labels =\\\ntrain_test_split(train_paths, train_labels, test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:38:46.151817Z","iopub.execute_input":"2021-05-25T13:38:46.15236Z","iopub.status.idle":"2021-05-25T13:38:46.169212Z","shell.execute_reply.started":"2021-05-25T13:38:46.152324Z","shell.execute_reply":"2021-05-25T13:38:46.168095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_image(filename, label=None, image_size=(512, 512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None): # Data augmentations\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_saturation(image, 0.7, 1.3)\n    image = tf.image.random_contrast(image, 0.8, 1.2)\n    imgae = tf.image.random_brightness(image, 0.1)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:39:05.673943Z","iopub.execute_input":"2021-05-25T13:39:05.674363Z","iopub.status.idle":"2021-05-25T13:39:05.682967Z","shell.execute_reply.started":"2021-05-25T13:39:05.674328Z","shell.execute_reply":"2021-05-25T13:39:05.681697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Creating Dataset objects</h1>","metadata":{}},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:52:20.704628Z","iopub.execute_input":"2021-05-25T13:52:20.705154Z","iopub.status.idle":"2021-05-25T13:52:20.804416Z","shell.execute_reply.started":"2021-05-25T13:52:20.70512Z","shell.execute_reply":"2021-05-25T13:52:20.803387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Defining Learning-Rate Scheduler</h1>","metadata":{}},{"cell_type":"code","source":"def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n               lr_min=0.00001, lr_rampup_epochs=5, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:52:28.289132Z","iopub.execute_input":"2021-05-25T13:52:28.289802Z","iopub.status.idle":"2021-05-25T13:52:28.297615Z","shell.execute_reply.started":"2021-05-25T13:52:28.28975Z","shell.execute_reply":"2021-05-25T13:52:28.296704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Defining hyperparameters of fit</h1>","metadata":{}},{"cell_type":"code","source":"lrfn = build_lrfn()\nSTEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:52:32.446221Z","iopub.execute_input":"2021-05-25T13:52:32.446584Z","iopub.status.idle":"2021-05-25T13:52:32.451698Z","shell.execute_reply.started":"2021-05-25T13:52:32.446553Z","shell.execute_reply":"2021-05-25T13:52:32.450474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Function for visualizing training and validation accuracy</h1>","metadata":{}},{"cell_type":"code","source":"def display_training_curves(training, validation, yaxis):\n    if yaxis == \"loss\":\n        ylabel = \"Loss\"\n        title = \"Loss vs. Epochs\"\n    else:\n        ylabel = \"Accuracy\"\n        title = \"Accuracy vs. Epochs\"\n        \n    fig = go.Figure()\n        \n    fig.add_trace(\n        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=training, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"))\n    \n    fig.add_trace(\n        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=validation, marker=dict(color=\"darkorange\"),\n               name=\"Val\"))\n    \n    fig.update_layout(title_text=title, yaxis_title=ylabel, xaxis_title=\"Epochs\", template=\"plotly_white\")\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:52:36.81744Z","iopub.execute_input":"2021-05-25T13:52:36.81783Z","iopub.status.idle":"2021-05-25T13:52:36.825302Z","shell.execute_reply.started":"2021-05-25T13:52:36.817798Z","shell.execute_reply":"2021-05-25T13:52:36.824456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Modelling</h1>","metadata":{}},{"cell_type":"markdown","source":"<h2>1. DenseNet</h2>","metadata":{}},{"cell_type":"code","source":"#Setting the model to train in TPU\nwith strategy.scope():\n    model = tf.keras.Sequential([DenseNet201(input_shape=(512, 512, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:38:10.642516Z","iopub.execute_input":"2021-05-25T13:38:10.642921Z","iopub.status.idle":"2021-05-25T13:38:10.677889Z","shell.execute_reply.started":"2021-05-25T13:38:10.642885Z","shell.execute_reply":"2021-05-25T13:38:10.676147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fundamental Block\nSVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[13].output), dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model Architecture\nSVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training\nhistory = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Loss of the model is - \" , model.evaluate(valid_dataset)[0])\n#print(\"Accuracy of the model is - \" , model.evaluate(train_dataset)[1]*100 , \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing train and valid accuracy\ndisplay_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing train and valid loss\ndisplay_training_curves(\n    history.history['loss'], \n    history.history['val_loss'], \n    'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting training values\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='blue', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='red', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy with DenseNet')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='blue', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss with DenseNet')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sample predictions\n#Now, I will visualize some sample predictions made by the EfficientNet NoisyStudent model. The red bars represent the model's prediction (maximum probability), the green represent the ground truth (label), and the rest of the bars are blue. When the model predicts correctly, the prediction bar is green.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction\nprobs_dnn = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = probs_dnn\nsub.to_csv('submission_densenet.csv', index=False)\nsub.head()\n\n#LB:0.96792","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# submission.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf1 = pd.read_csv('./submission_densenet.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'submission_densenet.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotScatterMatrix(df1, 12, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>2. InceptionV3</h2>","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([InceptionV3(input_shape=(512, 512, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:52:51.222872Z","iopub.execute_input":"2021-05-25T13:52:51.223535Z","iopub.status.idle":"2021-05-25T13:53:08.489627Z","shell.execute_reply.started":"2021-05-25T13:52:51.223497Z","shell.execute_reply":"2021-05-25T13:53:08.488406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[11].output), dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:53:08.491429Z","iopub.execute_input":"2021-05-25T13:53:08.491897Z","iopub.status.idle":"2021-05-25T14:17:53.42406Z","shell.execute_reply.started":"2021-05-25T13:53:08.491848Z","shell.execute_reply":"2021-05-25T14:17:53.422747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-05-25T14:21:51.418058Z","iopub.execute_input":"2021-05-25T14:21:51.418472Z","iopub.status.idle":"2021-05-25T14:21:53.156689Z","shell.execute_reply.started":"2021-05-25T14:21:51.418437Z","shell.execute_reply":"2021-05-25T14:21:53.155632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing train and valid loss\ndisplay_training_curves(\n    history.history['loss'], \n    history.history['val_loss'], \n    'loss')","metadata":{"execution":{"iopub.status.busy":"2021-05-25T14:21:57.437911Z","iopub.execute_input":"2021-05-25T14:21:57.438416Z","iopub.status.idle":"2021-05-25T14:21:57.475621Z","shell.execute_reply.started":"2021-05-25T14:21:57.438383Z","shell.execute_reply":"2021-05-25T14:21:57.474625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting training values\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='blue', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='red', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy with InceptionV3')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='blue', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss with InceptionV3')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T14:22:02.897418Z","iopub.execute_input":"2021-05-25T14:22:02.897778Z","iopub.status.idle":"2021-05-25T14:22:03.386882Z","shell.execute_reply.started":"2021-05-25T14:22:02.897745Z","shell.execute_reply":"2021-05-25T14:22:03.385867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#print(\"[INFO] Calculating model accuracy\")\n#val_acc = model.evaluate(x_test, y_test)\n\n#val_acc = model.evaluate(valid_dataset)\n#print(f\"Test Accuracy: {scores[1]*100}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(valid_dataset, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T15:05:27.664956Z","iopub.execute_input":"2021-05-25T15:05:27.665304Z","iopub.status.idle":"2021-05-25T15:05:29.465592Z","shell.execute_reply.started":"2021-05-25T15:05:27.665274Z","shell.execute_reply":"2021-05-25T15:05:29.464641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Test Accuracy: {probs_incepv3[1]*100}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-25T15:04:44.018995Z","iopub.execute_input":"2021-05-25T15:04:44.019388Z","iopub.status.idle":"2021-05-25T15:04:44.025195Z","shell.execute_reply.started":"2021-05-25T15:04:44.019354Z","shell.execute_reply":"2021-05-25T15:04:44.024121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{"execution":{"iopub.status.busy":"2021-05-25T15:10:07.622231Z","iopub.execute_input":"2021-05-25T15:10:07.622598Z","iopub.status.idle":"2021-05-25T15:10:07.627797Z","shell.execute_reply.started":"2021-05-25T15:10:07.622567Z","shell.execute_reply":"2021-05-25T15:10:07.626689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model to disk\nprint(\"[INFO] Saving model...\")\npickle.dump(model,open('InceptionV3_model.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2021-05-25T15:10:10.629777Z","iopub.execute_input":"2021-05-25T15:10:10.630142Z","iopub.status.idle":"2021-05-25T15:10:10.666875Z","shell.execute_reply.started":"2021-05-25T15:10:10.630113Z","shell.execute_reply":"2021-05-25T15:10:10.665521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs_incepv3 = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = probs_incepv3\nsub.to_csv('submission_incepv3.csv', index=False)\nsub.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T14:22:10.694073Z","iopub.execute_input":"2021-05-25T14:22:10.694446Z","iopub.status.idle":"2021-05-25T14:23:22.946546Z","shell.execute_reply.started":"2021-05-25T14:22:10.694411Z","shell.execute_reply":"2021-05-25T14:23:22.945636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_dataset, verbose=1)\nprint(f\"Test Accuracy: {model.evaluate[1]*100}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-25T15:06:07.58991Z","iopub.execute_input":"2021-05-25T15:06:07.590294Z","iopub.status.idle":"2021-05-25T15:06:55.802507Z","shell.execute_reply.started":"2021-05-25T15:06:07.590261Z","shell.execute_reply":"2021-05-25T15:06:55.800762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_predictions = model.predict(test_dataset)\nprint(f\"Test Accuracy: {test_predictions[1]*100}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-25T14:54:38.0551Z","iopub.execute_input":"2021-05-25T14:54:38.055694Z","iopub.status.idle":"2021-05-25T14:55:26.864827Z","shell.execute_reply.started":"2021-05-25T14:54:38.055646Z","shell.execute_reply":"2021-05-25T14:55:26.863833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=5, ncols=2, figsize=(15, 18))\n\nfor i in range(5):\n    img_id = random.choice(np.arange(0,test_df.shape[0]))\n    test_image = getImage(test_df.image_id[img_id])\n    \n    ax[i,0].imshow(test_image)\n    ax[i,0].set_title(f'{test_df.image_id[img_id]}', fontsize=12)\n    ax[i,1].barh(y=train_df.columns[1:],width=test_predictions[img_id])\n    ax[i,1].set_title('Predictions', fontsize=12)\n    \nfig.suptitle(\"Test set Predictions\",fontsize=20)\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T16:09:41.347028Z","iopub.execute_input":"2021-05-25T16:09:41.347381Z","iopub.status.idle":"2021-05-25T16:09:41.368571Z","shell.execute_reply.started":"2021-05-25T16:09:41.347351Z","shell.execute_reply":"2021-05-25T16:09:41.366733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Test Accuracy: {scores[1]*100}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-25T14:50:31.725531Z","iopub.execute_input":"2021-05-25T14:50:31.725924Z","iopub.status.idle":"2021-05-25T14:50:31.772926Z","shell.execute_reply.started":"2021-05-25T14:50:31.725891Z","shell.execute_reply":"2021-05-25T14:50:31.771468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>3. ResNet</h2>","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([ResNet50V2(input_shape=(512, 512, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[11].output), dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing train and valid loss\ndisplay_training_curves(\n    history.history['loss'], \n    history.history['val_loss'], \n    'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting training values\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='blue', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='red', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy with ResNet')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='blue', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss with ResNet')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs_resnet = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = probs_resnet\nsub.to_csv('submission_resnet.csv', index=False)\nsub.head()\n\n#LB:-0.94379","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# submission.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf1 = pd.read_csv('./submission_resnet.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'submission_resnet.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotScatterMatrix(df1, 12, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>4. InceptionResNet</h2>","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([InceptionResNetV2(input_shape=(512, 512, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[11].output), dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing train and valid loss\ndisplay_training_curves(\n    history.history['loss'], \n    history.history['val_loss'], \n    'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting training values\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='blue', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='red', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy with InceptionResNet')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='blue', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss with InceptionResNet')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs_incepres = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = probs_incepres\nsub.to_csv('submission_incepres.csv', index=False)\nsub.head()\n\n#LB:-0.96181","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# submission.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf1 = pd.read_csv('./submission_incepres.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'submission_incepres.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotScatterMatrix(df1, 12, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#5. VGG19","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([VGG19(input_shape=(512, 512, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[11].output), dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(\n    history.history['loss'], \n    history.history['val_loss'], \n    'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting training values\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='blue', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='red', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy with VGG19')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='blue', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss with VGG19')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs_incepres = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = probs_incepres\nsub.to_csv('submission_vgg19.csv', index=False)\nsub.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# submission.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf1 = pd.read_csv('./submission_vgg19.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'submission_vgg19.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotScatterMatrix(df1, 12, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#6. VGG16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([VGG16(input_shape=(512, 512, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[11].output), dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(\n    history.history['loss'], \n    history.history['val_loss'], \n    'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting training values\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='blue', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='red', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy with VGG16')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='blue', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss with VGG16')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs_incepres = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = probs_incepres\nsub.to_csv('submission_vgg16.csv', index=False)\nsub.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# submission.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf1 = pd.read_csv('./submission_vgg16.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'submission_vgg16.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotScatterMatrix(df1, 12, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>5. Ensembling</h2>","metadata":{}},{"cell_type":"code","source":"ensemble_1, ensemble_2, ensemble_3 = [sub]*3\n\n# probs_incepv3\n\nensemble_1.loc[:, 'healthy':] = 0.50*probs_dnn + 0.50*probs_incepres\nensemble_2.loc[:, 'healthy':] = 0.75*probs_dnn + 0.20*probs_incepres + 0.05*probs_incepv3\nensemble_3.loc[:, 'healthy':] = 0.80*probs_dnn + 0.20*probs_incepres\n\nensemble_1.to_csv('submission_ensemble_1.csv', index=False) #LB :-0.96970\nensemble_2.to_csv('submission_ensemble_2.csv', index=False) #LB :-0.96970\nensemble_3.to_csv('submission_ensemble_3.csv', index=False) #LB :-0.96970","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#InceptionResNet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([InceptionResNetV2(input_shape=(512, 512, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[11].output), dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(\n    history.history['loss'], \n    history.history['val_loss'], \n    'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting training values\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='blue', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='red', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy with InceptionResNetV2')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='blue', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss with InceptionResNetV2')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process(img):\n    return cv2.resize(img/255.0, (512, 512)).reshape(-1, 512, 512, 3)\ndef predict(img):\n    return model.layers[2](model.layers[1](model.layers[0](process(img)))).numpy()[0]\n\nfig = make_subplots(rows=4, cols=2)\npreds = predict(train_images[2])\n\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n\ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Healthy\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[2], (205, 136))), row=1, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=1, col=2)\nfig.update_layout(height=1200, width=800, title_text=\"DenseNet Predictions\", showlegend=False)\n\npreds = predict(train_images[0])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Multiple diseases\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[0], (205, 136))), row=2, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=2, col=2)\n\npreds = predict(train_images[3])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Rust\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[3], (205, 136))), row=3, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=3, col=2)\n\npreds = predict(train_images[1])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Scab\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[1], (205, 136))), row=4, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=4, col=2)\n\nfig.update_layout(template=\"plotly_white\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs_incepres = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = probs_incepres\nsub.to_csv('submission_InceptionResNetV2.csv', index=False)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport re\nfrom tensorflow.keras.applications.resnet  import  ResNet50 as resNet\nfrom tensorflow.keras.models import Sequential\nfrom PIL import Image, ImageDraw, ImageEnhance\nimport albumentations as albu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_width,img_height = 512,512\n\nroot_path = \"../input/plant-pathology-2020-fgvc7/\"\ntrain_path = root_path+\"/images\"\ntest_path =  root_path+\"/images\"\ntrain_csv_path =  \"../input/plant-pathology-2020-fgvc7/train.csv\"\nsample_path =  \"../input/plant-pathology-2020-fgvc7/sample_submission.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df = pd.read_csv(train_csv_path)\ntotal_ids =  [i.split(\".\")[0] for i in  os.listdir(train_path)]\ntotal_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_bboxes(bboxs,img):\n    color = (255, 0, 0) \n    thickness = 3\n    for cur_box  in bboxs:\n        start_point = (cur_box[0],cur_box[1])\n        end_point = (cur_box[2]+cur_box[0],cur_box[3]+cur_box[1])\n   \n        \n        cv2.rectangle(img, start_point, end_point, color, thickness) \n\n    return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bboxes(image_id):\n    selected_df = total_df[total_df['image_id'] == image_id]\n    \n    image_bboxes = selected_df['bbox']\n    box_scores = selected_df['bbox']\n    bboxes = []\n    for row in image_bboxes:\n        row=row.replace(\" \", \"\")\n        x1y1x2y2 = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", row))\n        x1y1x2y2 = np.float32(x1y1x2y2)\n        x1y1x2y2 = x1y1x2y2\n        x1y1x2y2 =  np.int32(x1y1x2y2)\n  \n        bboxes.append(x1y1x2y2)\n           \n        \n    return bboxes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_image(image_id):\n    path = train_path+'/'+str(image_id)+'.jpg'\n    image = Image.open(path)\n    image = image.resize((img_width,img_height))\n    \n    return np.asarray(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###SEGMENTATION","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/jakeret/unet\n!pip install ../working/unet/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import minmax_scale\nimport random\nimport cv2\nfrom imgaug import augmenters as iaa\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Activation\nfrom tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nfrom unet import utils\nfrom unet.datasets import circles\nimport unet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_folder = '/kaggle/input/plant-2020-seg/plant_2020/train/'\nsamples_df = pd.read_csv(\"/kaggle/input/plant-2020-seg/plant_2020/train1.csv\")\nsamples_df[\"label\"] = samples_df[\"label\"].astype(\"str\")\nsamples_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples_df = samples_df.query(\"label=='2'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_percentage = 0.8\ntraining_item_count = int(len(samples_df)*training_percentage)\nvalidation_item_count = len(samples_df)-int(len(samples_df)*training_percentage)\ntraining_df = samples_df[:training_item_count]\nvalidation_df = samples_df[training_item_count:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ECI_band(img):\n    '''\n    Return the ECI band calculated from an RGB image between 0 and 255\n    using the formula below:    \n    ECI = (red_channel-1)^2 + green_channel^2/0.16\n    '''\n    img = img/255.\n    img = cv2.GaussianBlur(img,(35,35),0)\n    ECI_band = np.power(img[:,:,0]-1,2) + np.power(img[:,:,1],2)/0.16\n    normalized_ECI_band = (ECI_band/ECI_band.max()*255).astype(np.uint8)\n    return normalized_ECI_band\n\n\ndef get_CIVE_band(img):\n    '''\n    Return the CIVE band calculated from an RGB image between 0 and 255\n    using the formula below:\n    CIVE = 0.441*red_channel - 0.881*green_channel + 0.385*blue_channel + 18.787\n    '''\n    img = cv2.GaussianBlur(img,(35,35),0)\n    CIVE_band = 0.441*img[:,:,0] - 0.881*img[:,:,1] + 0.385*img[:,:,2] + 18.787\n    normalized_CIVE_band = (((CIVE_band+abs(CIVE_band.min()))/CIVE_band.max())).astype(np.uint8)\n    return normalized_CIVE_band\n\n\ndef apply_ECI_mask(img, vegetation_index_band):\n    '''\n    Apply a binary mask on an image and return the masked image\n    '''\n    ret, otsu = cv2.threshold(vegetation_index_band,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n    masked_img = cv2.bitwise_and(img,img,mask = otsu)\n    return masked_img\n\n\ndef apply_CIVE_mask(img, vegetation_index_band):\n    '''\n    Apply a binary mask on an image and return the masked image\n    '''\n    ret, otsu = cv2.threshold(vegetation_index_band,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n    masked_img = cv2.bitwise_and(img,img,mask = otsu)\n    return masked_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nitems = 6\nfor idx, image_id in enumerate(samples_df.image_id[:items]):\n    img_path = training_folder+image_id\n    img = np.array(Image.open(img_path))\n    ax = plt.subplot(items, 3, idx*3 + 1)\n    ax.set_title(\"original\")\n    plt.imshow(img)\n    \n    ECI_band =  get_ECI_band(img)\n    ax = plt.subplot(items, 3, idx*3 + 2)\n    ax.set_title(\"ECI band\")\n    plt.imshow(ECI_band)\n    \n    masked_img = apply_ECI_mask(img, ECI_band)\n    ax = plt.subplot(items, 3, idx*3 + 3)\n    ax.set_title(\"ECI+Otsu\")\n    plt.imshow(masked_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport cv2\n# load image\nimage = cv2.imread('../input/plant-2020-seg/plant_2020/train/Train_1000.jpg')\n# create hsv\nhsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n # set lower and upper color limits\nlow_val = (0,60,0)\nhigh_val = (179,255,255)\n# Threshold the HSV image \nmask = cv2.inRange(hsv, low_val,high_val)\n# remove noise\nmask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel=np.ones((8,8),dtype=np.uint8))\n# apply mask to original image\nresult = cv2.bitwise_and(image, image,mask=mask)\n\n#show image\nplt.imshow(\"Result\", result)\nplt.imshow(\"Mask\", mask)\nplt.imshow(\"Image\", image)\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>References</h1>\n\n[https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models/notebook](http://)\n\nhttps://www.kaggle.com/pestipeti/eda-plant-pathology-2020","metadata":{"trusted":true}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}