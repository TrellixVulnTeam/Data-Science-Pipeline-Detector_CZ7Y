{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd, os\nimport matplotlib.pyplot as plt, cv2\nimport tensorflow as tf, re, math","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/plant-pathology-2020-fgvc7/images/'\nall_imgs = os.listdir(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = [train for train in all_imgs if 'Train' in train]\ntest_images = [test for test in all_imgs if 'Test' in test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/train.csv')\ntest_df = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bytes_function(value):\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy()\n    return tf.train.Feature(bytes_list = tf.train.BytesList(value = [value]))\ndef float_function(value):\n    return tf.train.Feature(float_list = tf.train.FloatList(value = [value]))\n\ndef int64_function(value):\n    return tf.train.Feature(int64_list = tf.train.Int64List(value = [value]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def serialize_example(feature0, feature1, feature2, feature3, feature4, feature5):\n    feature = {\n        'image' : bytes_function(feature0),\n        'image_id': bytes_function(feature1),\n        'healthy': int64_function(feature2),\n        'multiple_diseases': int64_function(feature3),\n        'rust': int64_function(feature4),\n        'scab': int64_function(feature5)\n    }\n    example = tf.train.Example(features = tf.train.Features(feature = feature))\n    return example.SerializeToString()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SIZE = 2020\nCT = len(train_images)//SIZE+int(len(train_images)%SIZE!=0)\n\nfor j in range(CT):\n    print(); print('Writing TFRecord %i of %i...'%(j,CT))\n    CT2 = min(SIZE, len(train_images)-j*SIZE)\n    with tf.io.TFRecordWriter('train%.2i-%i.tfrec'%(j, CT2)) as writer:\n        for k in range(CT2):\n            img = cv2.imread(path+train_images[SIZE*j+k])\n#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (224, 224))\n            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tostring()\n            name = train_images[SIZE*j+k].split('.')[0]\n            row = train_df.loc[train_df.image_id == name]\n            example = serialize_example( \n                img, str.encode(name),\n                row.healthy.values[0],\n                row.multiple_diseases.values[0],\n                row.rust.values[0],\n                row.scab.values[0]\n            )\n            writer.write(example)\n            if k%100==0:\n                print(k,', ', end='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_serialize_example(feature0, feature1):\n    feature = {\n        'image' : bytes_function(feature0),\n        'image_id': bytes_function(feature1)\n    }\n    example = tf.train.Example(features = tf.train.Features(feature = feature))\n    return example.SerializeToString()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bytes_features(value):\n    if isinstance(value, tf.constant(0)):\n        value = value.numpy()\n    return tf.train.Features(bytes_list = (tf.train.Bytes_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SIZE = 2020\nCT = len(test_images)//SIZE+int(len(test_images)%SIZE!=0)\n\nfor j in range(CT):\n    print('Writing {} of {} files to records...'.format(j, CT))\n    CT2 = min(SIZE, len(test_images)-j*SIZE)\n    with tf.io.TFRecordWriter('test%.2i-%i.tfrec'%(j, CT2)) as writer:\n        for k in range(CT2):\n            img = cv2.imread(path+test_images[SIZE*j+k])\n#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (224, 224))\n            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tostring()\n            name = test_images[SIZE*j+k].split('.')[0]\n            row = test_df.loc[test_df.image_id == name]\n            example = test_serialize_example(\n                img, str.encode(name))\n            writer.write(example)\n            if k%100==0:\n                print(k,', ', end='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = tf.data.TFRecordDataset('/kaggle/working/train00-1821.tfrec')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for raw_record in raw_data.take(1):\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n    print(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('TPU found:',tpu.master())\nexcept ValueError:\n    tpu = None\n    print('No TPU was found')\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint('Number of replcias', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nbatch_size = 16\nimage_shape = [224, 224]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*image_shape, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),# tf.string means bytestring\n        'healthy':tf.io.FixedLenFeature([], tf.int64),\n        'multiple_diseases': tf.io.FixedLenFeature([], tf.int64),\n        'rust': tf.io.FixedLenFeature([], tf.int64),\n        'scab':tf.io.FixedLenFeature([], tf.int64),\n        \"image_id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    healthy = tf.cast(example['healthy'], tf.int32)\n    multiple_diseases = tf.cast(example['multiple_diseases'], tf.int32)\n    rust = tf.cast(example['rust'], tf.int32)\n    scab = tf.cast(example['scab'], tf.int32)\n    label = example['image_id']\n    target = [healthy, multiple_diseases, rust, scab]\n    return image, target # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_data(eg):\n    unlabeled_features = {\n        'image' : tf.io.FixedLenFeature([], tf.string),\n        'image_id': tf.io.FixedLenFeature([], tf.string)\n    }\n    eg = tf.io.parse_single_example(eg, features=unlabeled_features)\n    image = tf.image.decode_jpeg(eg['image'], channels=3)\n    image = tf.cast(image, tf.float32)/255.0\n    image = tf.reshape(image, [*image_shape, 3])\n    label = eg['image_id']\n    return image, label\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_data)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\ndef data_augment(image, target):\n    image = tf.image.random_flip_left_right(image, 101)\n    image = tf.image.random_flip_up_down(image, 101)\n    image = tf.image.random_brightness(image, .3, 101)\n    image = tf.image.random_crop(image, [224, 224, 3], 42)\n    return image, target\n\ndef get_training_dataset(do_aug = True):\n    dataset = load_dataset('/kaggle/working/train00-1821.tfrec', labeled=True)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    if do_aug: dataset = dataset.map(data_augment)\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_testing_dataset(ordered = True):\n    dataset = load_dataset('/kaggle/working/test00-1821.tfrec', labeled = False, ordered = ordered)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = get_training_dataset()\ntest_dataset = get_testing_dataset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor i in train_dataset:\n    count+=1\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image , target in cutmix_train.unbatch().take(1):\n    plt.imshow(image)\n    print(image.shape)\n    print(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cutmix(image, label, probability = 1.0):\n    DIMS = 512\n    CLASSES = 4\n    \n    imgs = []; labs = []\n    for j in range(batch_size):\n        P = tf.cast(tf.random.uniform([], 0, 1)<=probability, tf.int32)\n        k = tf.cast(tf.random.uniform([], 0, batch_size), tf.int32)\n        x = tf.cast(tf.random.uniform([], 0, DIMS), tf.int32)\n        y = tf.cast(tf.random.uniform([], 0, DIMS), tf.int32)\n        b = tf.random.uniform([], 0, 1)\n        WIDTH = tf.cast(DIMS * tf.math.sqrt(b), tf.int32)*P\n        ya = tf.math.maximum(0, y-WIDTH//2)\n        yb = tf.math.maximum(DIMS, y-WIDTH//2)\n        xa = tf.math.maximum(0, x-WIDTH//2)\n        xb = tf.math.maximum(DIMS, x-WIDTH//2)\n        one = image[j, ya:yb, 0:xa, :]\n        blank = np.zeros([*image_shape, 3])*tf.random.uniform([], 0, 255)\n        two = blank[ya:yb, xa:xb, :]\n        three = image[j, ya:yb, xb:DIMS, :]\n        middle = tf.concat([one, two, three], axis = 1)\n        img = tf.concat([image[j, 0:ya, :, :], middle, image[j, yb:DIMS, :]], axis = 0)\n        imgs.append(img)\n\n#         a = tf.cast(WIDTH*WIDTH/DIMS/DIMS, tf.int32)\n\n#         lab1 = label[j, ]\n#         lab2 = label[k, ]\n\n#         labs.append((1-a)*lab1+a*lab2)\n\n    images = tf.reshape(tf.stack(imgs), (batch_size, 224, 224, 3))\n#     labels = tf.reshape(tf.stack(labs), (batch_size, CLASSES))\n    return images, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cutmix_train = train_dataset.map(cutmix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.densenet import DenseNet201\nfrom tensorflow.keras.applications.xception import Xception","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    pretrained_model = Xception(input_shape=(224, 224, 3), include_top=False, weights='imagenet', pooling='avg')\n    model = tf.keras.models.Sequential([pretrained_model, tf.keras.layers.Dense(4, activation = 'softmax')])\n    \n    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['AUC'])\n    result = model.fit_generator(cutmix_train, epochs = 75, steps_per_epoch = 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = test_dataset.map(lambda image, name:image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(test_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_names = test_dataset.map(lambda image, label: label).unbatch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = next(iter(image_names.batch(1821))).numpy().astype('U')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_file = pd.concat([pd.DataFrame({'image_id':names}), pd.DataFrame(preds, columns = ['healthy','multiple_diseases', 'rust', 'scab'])], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_file.to_csv('Plants28.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numba import cuda\n\ncuda.select_device()\ncuda.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}