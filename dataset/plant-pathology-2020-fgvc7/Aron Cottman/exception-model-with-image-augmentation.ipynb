{"cells":[{"metadata":{},"cell_type":"markdown","source":"\nAfter some of the dismal performace I have had with my models, I decided to work on the overfitting my doing data augmentstion.  This is another area where I knew the theory behind what needed to be done, but actually coding it took a lot of time.  I ended up having to move this into kaggle since my little macbook could not proceess what I was asking it to do. (or I was doing something wrong)\nI boosted my accuracy up to about 74% using a CNN that I built. Check previous versions to see how that looked.  This removes the models I created and just appliess an Exception model to the data.  This is my last iteration of this project.  I was hoping I would be able to get better performance out of my own models I had built, but am pretty happy with what I did. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# load dependencies\nimport numpy as np \nimport pandas as pd\nimport os\nimport shutil\nimport cv2\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\n# load keras modules\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom tensorflow.keras.applications import Xception\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.getcwd()\n#os.chdir('/Users/Aron/Kaggle/plant_pathology')\nlocal_dir = '/Users/Aron/Kaggle/plant_pathology/plant-pathology-2020-fgvc7'\nkaggle_dir = '/kaggle/input/plant-pathology-2020-fgvc7/'\n\nsample_submission = pd.read_csv('../input/plant-pathology-2020-fgvc7/sample_submission.csv')\ntest = pd.read_csv(kaggle_dir + 'test.csv')\ntrain = pd.read_csv(kaggle_dir + 'train.csv')\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\nAUTO = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE = 300\ndef seed_everything(seed=0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 2048\nseed_everything(seed)\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\ndef format_path(st):\n    return GCS_DS_PATH + '/images/' + st + '.jpg'\n\n\nsub = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv')\n\ntrain_paths = train.image_id.apply(format_path).values\ntest_paths = test.image_id.apply(format_path).values\ntrain_labels = train.loc[:, 'healthy':].values\nSPLIT_VALIDATION =True\nif SPLIT_VALIDATION:\n    train_paths, valid_paths, train_labels, valid_labels =train_test_split(train_paths, train_labels, test_size=0.15, random_state=seed)\n\ndef decode_image(filename, label=None, IMG_SIZE=(IMG_SIZE, IMG_SIZE)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, IMG_SIZE)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\ntrain_dataset = (\ntf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\ntrain_dataset_1 = (\ntf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(64)\n    .prefetch(AUTO)\n)\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_START = 0.0001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.0001\nLR_RAMPUP_EPOCHS = 4\nLR_SUSTAIN_EPOCHS = 6\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The sample submission file, the training data and testing labels are read in.  ther eare 1821 training and testing images in the dataset.Next is to look at the distribution of the training set ategories.  \n\n#There are 4 categories.  check to make sure there is a fatir representation of each of the 4 categories.\n\n# since each image can only be represented in each column once, \n# the mean of the columns are the percentage each column is of the data.\nprint(train.sum())\npcts = train.mean()\npcts.plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import Xception\nfrom keras.models import Model\nfrom tensorflow import keras\nwith strategy.scope():\n    Dense_net = Xception(\n                    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n                    weights='imagenet',\n                    include_top=False\n                    )\n    x = Dense_net.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(4, activation='softmax')(x)\n    model =  keras.Model(inputs = Dense_net.input,outputs=x)\n    model.compile(loss=\"categorical_crossentropy\", optimizer= 'adam', metrics=[\"accuracy\"])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now create the data generator\ndatagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit the model\n\nmodel.fit(\n    train_dataset,\n    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,\n    epochs=50,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None,)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npredict= model.predict(test_dataset)\nprediction = np.ndarray(shape = (test.shape[0],4), dtype = np.float32)\nfor row in range(test.shape[0]):\n    for col in range(4):\n        if predict[row][col] == max(predict[row]):\n            prediction[row][col] = 1\n        else:\n            prediction[row][col] = 0\nprediction = pd.DataFrame(prediction)\nprediction.columns = ['healthy', 'multiple_diseases', 'rust', 'scab']\ndf = pd.concat([test.image_id, prediction], axis = 1)\ndf.to_csv('submission.csv', index = False)\nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}