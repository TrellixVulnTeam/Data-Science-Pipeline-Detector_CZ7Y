{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 導入","metadata":{"id":"WbY308Alxq_W"}},{"cell_type":"markdown","source":"# 参考文献\n\n1. [OpenCVのCanny関数によるエッジ検出について](http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_imgproc/py_canny/py_canny.html)\n2. [Flipについて](https://qiita.com/yu4u/items/855ff350e6d93c82afd5)","metadata":{"id":"unq0JDLC1d06"}},{"cell_type":"markdown","source":"# 目次\n\n* [<font size=4>EDA</font>](#1)\n    * [データの下準備](#1.1)\n    * [サンプル画像の読み込み](#1.2)\n    * [チャネル値調査](#1.3)\n    * [葉っぱの画像の読み込み](#1.4)\n    * [その他EDA](#1.5)\n\n\n* [<font size=4>画像処理</font>](#2)\n    * [Canny関数によるエッジ検出](#2.1)\n    * [Flipping（画像の回転と反転）](#2.2)\n    * [Convolution(畳み込み)](#2.3)\n    * [Blurring(ぼやけた画像)](#2.4)\n  \n\n* [<font size=4>モデル構築</font>](#3)\n    * [データの下準備](#3.1)\n    * [DenseNet](#3.2)\n    * [EfficientNet](#3.3)\n\n* [<font size=4>まとめと考察</font>](#4)","metadata":{"id":"UKO5BDl72a3B"}},{"cell_type":"markdown","source":"# EDA <a id=\"1\"></a>","metadata":{"id":"wRNLR-MQ2i90"}},{"cell_type":"markdown","source":"## データの下準備 <a id=\"1.1\"></a>","metadata":{"id":"ufcI1cah2o_T"}},{"cell_type":"markdown","source":"### 必要なライブラリのインストール","metadata":{"id":"1XvcaAzr2rjY"}},{"cell_type":"markdown","source":"画像認識モデルはkaggleではdensenetとefficientnetをよく使っているカーネルが多かったのでこの2つを使ってみる","metadata":{}},{"cell_type":"code","source":"# efficientnetのinstall\n!pip install -q efficientnet","metadata":{"id":"V8TgyWRwpnKH","execution":{"iopub.status.busy":"2022-01-12T02:02:55.759576Z","iopub.execute_input":"2022-01-12T02:02:55.75993Z","iopub.status.idle":"2022-01-12T02:03:05.54345Z","shell.execute_reply.started":"2022-01-12T02:02:55.75984Z","shell.execute_reply":"2022-01-12T02:03:05.54237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 基本\n\n## ターミナルの記述などを行える。\nimport os\n\n## numpy pandas\nimport numpy as np\nimport pandas as pd\n\n## 画像解析用のライブラリ\nimport cv2\n\n## 可視化\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm #完了までのバーを表示してくれる\n\ntqdm.pandas()\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nnp.random.seed(0)\n\n## ディープラーニング\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom IPython.display import SVG\nimport efficientnet.tfkeras as efn\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets #ダウンロードしなくてもaggleのデータにアクセスできるようにする\nfrom tensorflow.keras.applications import DenseNet121\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"y4ElXcLopnKO","outputId":"72756d56-48f2-46c7-d8d0-56ec9fc395ce","execution":{"iopub.status.busy":"2022-01-12T02:03:05.546126Z","iopub.execute_input":"2022-01-12T02:03:05.546497Z","iopub.status.idle":"2022-01-12T02:03:14.691727Z","shell.execute_reply.started":"2022-01-12T02:03:05.546449Z","shell.execute_reply":"2022-01-12T02:03:14.690815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### データのpathの設定","metadata":{"id":"DyZKLcDg2yRi"}},{"cell_type":"code","source":"EPOCHS = 20 #エポック数を20に設定\n\nSAMPLE_LEN = 100 #適当に画像を選別する際など\n\n## ここで設定したpathを使って画像データやテストデータにアクセスする\nimage_path = \"../input/plant-pathology-2020-fgvc7/images/\"\ntest_path = \"../input/plant-pathology-2020-fgvc7/test.csv\"\ntrain_path = \"../input/plant-pathology-2020-fgvc7/train.csv\"\nsubmission_path = \"../input/plant-pathology-2020-fgvc7/sample_submission.csv\"\n\ntest = pd.read_csv(test_path) #テストデータ\ntrain = pd.read_csv(train_path) #訓練データ\n\n## pathがちゃんと通っているか確認\ntrain.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"mneU8D9bpnKS","execution":{"iopub.status.busy":"2022-01-12T02:03:14.693418Z","iopub.execute_input":"2022-01-12T02:03:14.693712Z","iopub.status.idle":"2022-01-12T02:03:14.745326Z","shell.execute_reply.started":"2022-01-12T02:03:14.693674Z","shell.execute_reply":"2022-01-12T02:03:14.744612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"trainデータが読み込めていたので成功！","metadata":{}},{"cell_type":"markdown","source":"### 適当な一枚を読み込む","metadata":{"id":"O6DgMHJz293K"}},{"cell_type":"code","source":"## trainデータは読み込めていたのでフォルダ内の画像を認識できる形で読み込めるか調べる\n\n## 画像読み込みの関数\ndef load_image(image_id):\n    file_path = image_id + \".jpg\"\n    image = cv2.imread(image_path + file_path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# 100枚をランダムにimagesフォルダから取得\ntrain_images = train[\"image_id\"][:SAMPLE_LEN].progress_apply(load_image)","metadata":{"id":"uJD4JbtHpnKd","outputId":"afb14520-4a05-4f26-e7bc-96a86b3f478c","execution":{"iopub.status.busy":"2022-01-12T02:03:14.746408Z","iopub.execute_input":"2022-01-12T02:03:14.746639Z","iopub.status.idle":"2022-01-12T02:03:19.556248Z","shell.execute_reply.started":"2022-01-12T02:03:14.746606Z","shell.execute_reply":"2022-01-12T02:03:19.555333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## サンプル画像の読み込み <a id=\"1.2\"></a>","metadata":{"id":"aI1QC_K-3HjL"}},{"cell_type":"code","source":"leaf_fig = px.imshow(cv2.resize(train_images[42], (200, 150))) #サイズを調整した上でcv2を使って読み込み\nleaf_fig.show()","metadata":{"_kg_hide-input":true,"id":"LNXpowwwpnKg","outputId":"7a677746-3d36-4b54-d201-63474cefb493","execution":{"iopub.status.busy":"2022-01-12T02:03:19.558889Z","iopub.execute_input":"2022-01-12T02:03:19.559415Z","iopub.status.idle":"2022-01-12T02:03:20.460317Z","shell.execute_reply.started":"2022-01-12T02:03:19.559372Z","shell.execute_reply":"2022-01-12T02:03:20.459404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"読み込みは成功している！","metadata":{}},{"cell_type":"code","source":"## さらに適当な1枚をピックアップ\nleaf_fig_2 = px.imshow(cv2.resize(train_images[1],(450, 300))) #サイズを変更してみる\nleaf_fig_2.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T02:03:20.461263Z","iopub.execute_input":"2022-01-12T02:03:20.461482Z","iopub.status.idle":"2022-01-12T02:03:20.554287Z","shell.execute_reply.started":"2022-01-12T02:03:20.461457Z","shell.execute_reply":"2022-01-12T02:03:20.553522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"こちらは別の病状の葉の画像。読み込みは問題なさそうなのであとはどうやって病気を見分けていくかを考える","metadata":{}},{"cell_type":"markdown","source":"多くのカーネルで、channel distributionsというrgbで画像を見た際の赤、緑、青の割合を調べていたので真似してみる。\n\n健康な葉っぱは緑の割合が多く、rust（さび）やmultiple diseases（複合）は赤の割合が多いなどありそう。","metadata":{"id":"6iwEoQ90WnnA"}},{"cell_type":"markdown","source":"## チャネル値調査 <a id=\"1.3\"></a>","metadata":{"id":"JGNq3QwJ3vs9"}},{"cell_type":"code","source":"# 1つ目の次元が赤（[:,:,0]）\nred_values = [np.mean(train_images[idx][:,:,0]) for idx in range(len(train_images))] #取得した100枚全部の赤分布の割合の平均値を取得\n\n# 2つ目の次元が緑（[:,:,1]）\ngreen_values = [np.mean(train_images[idx][:,:,1]) for idx in range(len(train_images))]\n# 3つ目の次元が青（[:,:,2]）\nblue_values = [np.mean(train_images[idx][:,:,2]) for idx in range(len(train_images))]\n\n# 色彩全体の平均\nvalues = [np.mean(train_images[idx]) for idx in range(len(train_images))]","metadata":{"_kg_hide-input":true,"id":"SKlfmsiR3yVn","execution":{"iopub.status.busy":"2022-01-12T02:03:20.555511Z","iopub.execute_input":"2022-01-12T02:03:20.556193Z","iopub.status.idle":"2022-01-12T02:03:22.333943Z","shell.execute_reply.started":"2022-01-12T02:03:20.556153Z","shell.execute_reply":"2022-01-12T02:03:22.333158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RGB全てのチャネル","metadata":{"id":"cluIike23zBw"}},{"cell_type":"code","source":"fig = ff.create_distplot([values], group_labels=[\"Channel全体\"], colors=[\"purple\"])\nfig.update_layout(showlegend=False, template=\"simple_white\")\nfig.update_layout(title_text=\"チャネル全体の分布\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig","metadata":{"_kg_hide-input":true,"id":"Eb32WiwY34At","execution":{"iopub.status.busy":"2022-01-12T02:03:22.335052Z","iopub.execute_input":"2022-01-12T02:03:22.335284Z","iopub.status.idle":"2022-01-12T02:03:22.491676Z","shell.execute_reply.started":"2022-01-12T02:03:22.335257Z","shell.execute_reply":"2022-01-12T02:03:22.490843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 赤の分布","metadata":{"id":"utld_a6W37SB"}},{"cell_type":"code","source":"fig = ff.create_distplot([red_values], group_labels=[\"R\"], colors=[\"red\"])\nfig.update_layout(showlegend=False, template=\"simple_white\")\nfig.update_layout(title_text=\"赤の分布\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig","metadata":{"_kg_hide-input":true,"id":"jDIqxHgj3_zj","execution":{"iopub.status.busy":"2022-01-12T02:03:22.493076Z","iopub.execute_input":"2022-01-12T02:03:22.493318Z","iopub.status.idle":"2022-01-12T02:03:22.548703Z","shell.execute_reply.started":"2022-01-12T02:03:22.493277Z","shell.execute_reply":"2022-01-12T02:03:22.547805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"全体がおおよそな正規分布だったのに対して、赤は右に歪みがある。つまり、あまりメインの色ではない。","metadata":{"id":"r6HLx5fYVDIi"}},{"cell_type":"markdown","source":"### 緑の分布","metadata":{"id":"7Gr6JXgg4QAo"}},{"cell_type":"code","source":"fig = ff.create_distplot([green_values], group_labels=[\"G\"], colors=[\"green\"])\nfig.update_layout(showlegend=False, template=\"simple_white\")\nfig.update_layout(title_text=\"緑の分布\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig","metadata":{"_kg_hide-input":true,"id":"75DRD2Jy4Sp-","execution":{"iopub.status.busy":"2022-01-12T02:03:22.550679Z","iopub.execute_input":"2022-01-12T02:03:22.551009Z","iopub.status.idle":"2022-01-12T02:03:22.606978Z","shell.execute_reply.started":"2022-01-12T02:03:22.55097Z","shell.execute_reply":"2022-01-12T02:03:22.606399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"葉の画像なので当たり前だが赤よりも分布が多い。","metadata":{"id":"nkpWr3JTVTQe"}},{"cell_type":"markdown","source":"### 青の分布","metadata":{"id":"Dqs2zNPC4ZkG"}},{"cell_type":"code","source":"fig = ff.create_distplot([blue_values], group_labels=[\"B\"], colors=[\"blue\"])\nfig.update_layout(showlegend=False, template=\"simple_white\")\nfig.update_layout(title_text=\"青の分布\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig","metadata":{"_kg_hide-input":true,"id":"03Sp06S54bpy","execution":{"iopub.status.busy":"2022-01-12T02:03:22.607908Z","iopub.execute_input":"2022-01-12T02:03:22.608281Z","iopub.status.idle":"2022-01-12T02:03:22.663526Z","shell.execute_reply.started":"2022-01-12T02:03:22.608252Z","shell.execute_reply":"2022-01-12T02:03:22.662517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 3色の分布を箱ヒゲ図で見てみる\none_leaf = go.Figure()\n\nfor idx, values in enumerate([red_values, green_values, blue_values]):\n    if idx == 0:\n        color = \"Red\"\n    if idx == 1:\n        color = \"Green\"\n    if idx == 2:\n        color = \"Blue\"\n    one_leaf.add_trace(go.Box(x=[color]*len(values), y=values, name=color, marker=dict(color=color.lower())))\n\none_leaf.update_layout(yaxis_title=\"平均値\", xaxis_title=\"色\", title=\"色ごと平均値\", template=\"plotly_white\")","metadata":{"_kg_hide-input":true,"id":"iUJeF2Ae4oUl","execution":{"iopub.status.busy":"2022-01-12T02:03:22.667007Z","iopub.execute_input":"2022-01-12T02:03:22.667271Z","iopub.status.idle":"2022-01-12T02:03:22.728894Z","shell.execute_reply.started":"2022-01-12T02:03:22.667242Z","shell.execute_reply":"2022-01-12T02:03:22.728115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 255のrgbの振り分けで見ると緑が一番多く、ついで赤、青という順。\n- 病気によってはここの割合が違ったりしそう","metadata":{}},{"cell_type":"code","source":"## チャネル値を1つのグラフで可視化\n\nfig = ff.create_distplot([red_values, green_values, blue_values],group_labels=[\"R\", \"G\", \"B\"],colors=[\"red\", \"green\", \"blue\"])\nfig.update_layout(title_text=\"Distribution all channel values\", template=\"simple_white\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig.data[1].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[1].marker.line.width = 0.5\nfig.data[2].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[2].marker.line.width = 0.5\nfig","metadata":{"_kg_hide-input":true,"id":"2x8aqxw04o8u","execution":{"iopub.status.busy":"2022-01-12T02:03:22.730029Z","iopub.execute_input":"2022-01-12T02:03:22.730242Z","iopub.status.idle":"2022-01-12T02:03:22.816011Z","shell.execute_reply.started":"2022-01-12T02:03:22.730217Z","shell.execute_reply":"2022-01-12T02:03:22.815126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 今回見分ける病気\n- healthy\n- multiple_diseases\n- rust\n- scab\n\nこれらのrgbの割合を学習させれば良さそう","metadata":{"id":"7tUmWvzYWMSs"}},{"cell_type":"markdown","source":"## 葉っぱの画像の読み込み <a id=\"1.4\"></a>","metadata":{"id":"Ee2qutMw3FTP"}},{"cell_type":"code","source":"## 画像読み込みの関数を作成\n### 状態の名称をtrainデータのカラムから取得し、それを指定して画像を読み込ませる仕組みにする\ndef visualize_leaves(cond=[0, 0, 0, 0], cond_cols=[\"healthy\"], is_cond=True):\n    if not is_cond:\n        cols, rows = 3, min([3, len(train_images)//3])\n        fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(30, rows*20/3))\n        for col in range(cols):\n            for row in range(rows):\n                ax[row, col].imshow(train_images.loc[train_images.index[-row*3-col-1]])\n        return None\n        \n    cond_0 = \"healthy == {}\".format(cond[0])\n    cond_1 = \"scab == {}\".format(cond[1])\n    cond_2 = \"rust == {}\".format(cond[2])\n    cond_3 = \"multiple_diseases == {}\".format(cond[3])\n    \n    cond_list = []\n    for col in cond_cols:\n        if col == \"healthy\":\n            cond_list.append(cond_0)\n        if col == \"scab\":\n            cond_list.append(cond_1)\n        if col == \"rust\":\n            cond_list.append(cond_2)\n        if col == \"multiple_diseases\":\n            cond_list.append(cond_3)\n    \n    data = train.loc[:99]\n    for cond in cond_list:\n        data = data.query(cond)\n        \n    images = train_images.loc[list(data.index)]\n    cols, rows = 3, min([3, len(images)//3])\n    \n    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(30, rows*20/3))\n    for col in range(cols):\n        for row in range(rows):\n            ax[row, col].imshow(images.loc[images.index[row*3+col]])\n    plt.show()","metadata":{"_kg_hide-input":true,"id":"mQagns98pnKt","execution":{"iopub.status.busy":"2022-01-12T02:03:22.819915Z","iopub.execute_input":"2022-01-12T02:03:22.820209Z","iopub.status.idle":"2022-01-12T02:03:22.834017Z","shell.execute_reply.started":"2022-01-12T02:03:22.820179Z","shell.execute_reply":"2022-01-12T02:03:22.833021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Healthy","metadata":{"id":"6YBTzqs53VHW"}},{"cell_type":"code","source":"visualize_leaves(cond=[1, 0, 0, 0], cond_cols=[\"healthy\"])","metadata":{"_kg_hide-input":true,"id":"_Gw1CH7bpnLc","outputId":"81b8d1ed-231f-4008-9508-23df527bbc80","execution":{"iopub.status.busy":"2022-01-12T02:03:22.835406Z","iopub.execute_input":"2022-01-12T02:03:22.835855Z","iopub.status.idle":"2022-01-12T02:03:28.183283Z","shell.execute_reply.started":"2022-01-12T02:03:22.835811Z","shell.execute_reply":"2022-01-12T02:03:28.182377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"斑点や赤くなってしまっている箇所がほぼない、緑の割合が多い画像","metadata":{"id":"v7T9CAFoXt9A"}},{"cell_type":"markdown","source":"### scab","metadata":{"id":"lfWbbGfD5ACD"}},{"cell_type":"code","source":"visualize_leaves(cond=[0, 1, 0, 0], cond_cols=[\"scab\"])","metadata":{"_kg_hide-input":true,"id":"cWTEw_JZpnLj","outputId":"d34b44a9-f00b-4942-bd19-c055347fb186","execution":{"iopub.status.busy":"2022-01-12T02:03:28.184594Z","iopub.execute_input":"2022-01-12T02:03:28.184847Z","iopub.status.idle":"2022-01-12T02:03:33.202441Z","shell.execute_reply.started":"2022-01-12T02:03:28.184819Z","shell.execute_reply":"2022-01-12T02:03:33.20146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"葉のいくつかの箇所に黒ずんでしまっている箇所が存在する","metadata":{"id":"qHR_5GsKYQWz"}},{"cell_type":"markdown","source":"### rust","metadata":{"id":"1doTwr2S5Dxt"}},{"cell_type":"code","source":"visualize_leaves(cond=[0, 0, 1, 0], cond_cols=[\"rust\"])","metadata":{"_kg_hide-input":true,"id":"Xwn6GVaupnLl","outputId":"e109052f-3742-4212-c3bd-ec8fa82d2f0c","execution":{"iopub.status.busy":"2022-01-12T02:03:33.204138Z","iopub.execute_input":"2022-01-12T02:03:33.204642Z","iopub.status.idle":"2022-01-12T02:03:38.169744Z","shell.execute_reply.started":"2022-01-12T02:03:33.204555Z","shell.execute_reply":"2022-01-12T02:03:38.166546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"赤、黄色の斑点がいくつかの箇所に存在する。rustの画像は赤の割合が他より高くなっていそう","metadata":{"id":"l_0g20pmZk17"}},{"cell_type":"markdown","source":"### multiple diseases","metadata":{"id":"mKALD2vt5G78"}},{"cell_type":"code","source":"visualize_leaves(cond=[0, 0, 0, 1], cond_cols=[\"multiple_diseases\"])","metadata":{"_kg_hide-input":true,"id":"YQm_lj9TpnLo","outputId":"a0568b08-2df5-4275-ce41-1a461d8e0ebd","execution":{"iopub.status.busy":"2022-01-12T02:03:38.171322Z","iopub.execute_input":"2022-01-12T02:03:38.171937Z","iopub.status.idle":"2022-01-12T02:03:41.600539Z","shell.execute_reply.started":"2022-01-12T02:03:38.171892Z","shell.execute_reply":"2022-01-12T02:03:41.59962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"複合なのでrustかつscabなども見られる。これらをどう見分けるか","metadata":{"id":"qyG-NpVDahDN"}},{"cell_type":"markdown","source":"## その他EDA <a id=\"1.5\"></a>","metadata":{"id":"yTFIf7dZ5MNe"}},{"cell_type":"markdown","source":"### 円グラフ","metadata":{}},{"cell_type":"code","source":"## 円グラフでtrainデータの葉っぱの割合分布を見てみる\nfig = go.Figure([go.Pie(labels=train.columns[1:], values=train.iloc[:,1:].sum().values)])\nfig.update_layout(title_text=\"状態分布\", template=\"simple_white\")\nfig.data[0].marker.line.color = 'rgb(0,0,0)'\nfig.data[0].marker.line.width = 0.5\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-12T02:03:41.601916Z","iopub.execute_input":"2022-01-12T02:03:41.602405Z","iopub.status.idle":"2022-01-12T02:03:41.660115Z","shell.execute_reply.started":"2022-01-12T02:03:41.602372Z","shell.execute_reply":"2022-01-12T02:03:41.659579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- multipleの割合が少なく学習に影響があるかも\n- それ以外のものはおおよそ均等に画像データが与えられている","metadata":{}},{"cell_type":"markdown","source":"## 棒グラフ","metadata":{}},{"cell_type":"code","source":"## 棒グラフでtrainデータの葉っぱの割合分布を見てみる\nfig = go.Figure([go.Bar(x=train.columns[1:], y=train.iloc[:,1:].sum().values)])\nfig.update_layout(title_text=\"状態分布\", template=\"simple_white\")\nfig.data[0].marker.line.color = 'rgb(0,0,0)'\nfig.data[0].marker.line.width = 0.5\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T02:03:41.661297Z","iopub.execute_input":"2022-01-12T02:03:41.661559Z","iopub.status.idle":"2022-01-12T02:03:41.705551Z","shell.execute_reply.started":"2022-01-12T02:03:41.66153Z","shell.execute_reply":"2022-01-12T02:03:41.704945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- healthy, rust, scabは良いがmultiple_diseasesは枚数が少ないため水増しをしたい\n- 多くのカーネルで行われていた転置などを行うのが良さそう","metadata":{}},{"cell_type":"markdown","source":"# 画像処理 <a id=\"2\"></a>","metadata":{"id":"aiPKQDkh5hF5"}},{"cell_type":"markdown","source":"## Canny関数によるエッジ検出 <a id=\"2.1\"></a>","metadata":{"id":"jC9wp-8N5kU4"}},{"cell_type":"markdown","source":"多くのカーネルで「エッジ検出」という明るさ変化が大きい点を抽出する処理を行っているため、参考文献の記事を真似しながらエッジ検出を行ってみる。\n\nエッジ検出を行うことで、画像の中のどの部分が葉っぱなのかをAIが理解しやすくなり、モデルの精度が上がるらしい。\n\nここではOpenCVのCanny関数を使用。","metadata":{}},{"cell_type":"markdown","source":"### Cannyを使ったエッジ検出に関する参考文献を一部抜粋\n1. ノイズ削減\nエッジ検出は画像中のノイズに対して敏感なため，まず初めに画像を平滑化してノイズを削減します．具体的には5x5のサイズの Gaussianフィルタを使います．フィルタリングの方法は既に前のチュートリアルで見ているかと思います．\n\n2. 画像の輝度勾配を見つける\n次に，平滑化された画像からSobelフィルタを使って縦方向(G_y)と横方向(G_x)の1次微分を取得します．これら2つの微分画像から以下のようにエッジの勾配と方向を求めます:\n\n$$\nEdge\\_Gradient \\; (G) = \\sqrt{G_x^2 + G_y^2}\n$$\n\n$$\nAngle \\; (\\theta) = \\tan^{-1} \\bigg(\\frac{G_y}{G_x}\\bigg)\n$$\n\n勾配方向は常にエッジに対して直交します．勾配方向は横，縦，二つの対角方向の内どれか一つになります．\n\n3. 非極大値の抑制\n勾配の方向と強度を計算した後は，エッジと関係ない画素を取り除きます．具体的には，各画素に対してその画素が勾配方向に対して極大値であるかどうかを確認します\n\n4. ヒステリシス(Hysteresis)を使ったしきい値処理\n前処理で検出されたエッジの内，正しいエッジとそうでないものを区別します．この区別をするために， minVal と maxVal という二つのしきい値を使います．画素値の微分値が maxVal 以上であれば正しいエッジとみなし， minVal 以下の値であればエッジではないとみなし除外します．微分値が二つのしきい値の間であれば，正しいエッジとそうでないエッジとの隣接関係を基に区別します．正しいエッジと区別された画素につながっていれば正しいエッジとみなし，そうでなければエッジではない画素とみなします\n\nこの処理では，エッジは長い線であるという前提のもと，少数の画素で構成されるエッジも削除します．\n\n最終的に画像中の強いエッジを検出できます．","metadata":{}},{"cell_type":"markdown","source":"他のカーネルを参考にしながらエッジ検出の関数を記述する。","metadata":{}},{"cell_type":"code","source":"## エッジ検出の関数\ndef edge_and_cut(img):\n    emb_img = img.copy()\n    edges = cv2.Canny(img, 100, 200)\n    edge_coors = []\n    for i in range(edges.shape[0]):\n        for j in range(edges.shape[1]):\n            if edges[i][j] != 0:\n                edge_coors.append((i, j))\n    \n    row_min = edge_coors[np.argsort([coor[0] for coor in edge_coors])[0]][0]\n    row_max = edge_coors[np.argsort([coor[0] for coor in edge_coors])[-1]][0]\n    col_min = edge_coors[np.argsort([coor[1] for coor in edge_coors])[0]][1]\n    col_max = edge_coors[np.argsort([coor[1] for coor in edge_coors])[-1]][1]\n    new_img = img[row_min:row_max, col_min:col_max]\n    \n    emb_img[row_min-10:row_min+10, col_min:col_max] = [255, 0, 0]\n    emb_img[row_max-10:row_max+10, col_min:col_max] = [255, 0, 0]\n    emb_img[row_min:row_max, col_min-10:col_min+10] = [255, 0, 0]\n    emb_img[row_min:row_max, col_max-10:col_max+10] = [255, 0, 0]\n    \n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n    ax[0].imshow(img, cmap='gray')\n    ax[0].set_title('original', fontsize=24)\n    ax[1].imshow(edges, cmap='gray')\n    ax[1].set_title('canny edge', fontsize=24)\n    ax[2].imshow(emb_img, cmap='gray')\n    ax[2].set_title('trimming', fontsize=24)\n    plt.show()","metadata":{"_kg_hide-input":true,"id":"uU_iqYaCpnL7","execution":{"iopub.status.busy":"2022-01-12T02:03:41.706515Z","iopub.execute_input":"2022-01-12T02:03:41.707128Z","iopub.status.idle":"2022-01-12T02:03:41.722663Z","shell.execute_reply.started":"2022-01-12T02:03:41.707095Z","shell.execute_reply":"2022-01-12T02:03:41.721678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edge_and_cut(train_images[42])\nedge_and_cut(train_images[25])\nedge_and_cut(train_images[31])","metadata":{"_kg_hide-input":true,"id":"bYJ0t9kppnL9","outputId":"a2785c32-9151-4d8d-f837-ec9de9de49bf","execution":{"iopub.status.busy":"2022-01-12T02:03:41.724285Z","iopub.execute_input":"2022-01-12T02:03:41.724939Z","iopub.status.idle":"2022-01-12T02:04:14.246234Z","shell.execute_reply.started":"2022-01-12T02:03:41.724895Z","shell.execute_reply":"2022-01-12T02:04:14.245408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- エッジ検出によって葉の輪郭を光の勾配が急に変化する箇所を見分け、画像の中のどの部分が葉なのかを検出。\n- 赤枠の中を主に分析し、画像の病状を分類する","metadata":{}},{"cell_type":"markdown","source":"## Flipping（画像の回転と反転） <a id=\"2.2\"></a>\n\nここから先は画像の水増しを3パターンによって行う。まずは回転と反転","metadata":{"id":"Wh0WHTCC5sL_"}},{"cell_type":"code","source":"def invert(img):\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n    ax[0].imshow(img)\n    ax[0].set_title('original', fontsize=24)\n    ax[1].imshow(cv2.flip(img, 0))\n    ax[1].set_title('upside down', fontsize=24)\n    ax[2].imshow(cv2.flip(img, 1))\n    ax[2].set_title('horizontal flip', fontsize=24)\n    plt.show()","metadata":{"_kg_hide-input":true,"id":"XPZwEZAepnMA","execution":{"iopub.status.busy":"2022-01-12T02:04:14.247444Z","iopub.execute_input":"2022-01-12T02:04:14.247707Z","iopub.status.idle":"2022-01-12T02:04:14.254508Z","shell.execute_reply.started":"2022-01-12T02:04:14.24768Z","shell.execute_reply":"2022-01-12T02:04:14.253421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"invert(train_images[42])\ninvert(train_images[25])\ninvert(train_images[31])","metadata":{"_kg_hide-input":true,"id":"gaiLRUzopnMD","outputId":"dc8f2538-abb7-4449-f0c9-dedd8a393843","execution":{"iopub.status.busy":"2022-01-12T02:04:14.256012Z","iopub.execute_input":"2022-01-12T02:04:14.25623Z","iopub.status.idle":"2022-01-12T02:04:19.785672Z","shell.execute_reply.started":"2022-01-12T02:04:14.256202Z","shell.execute_reply":"2022-01-12T02:04:19.784809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 上下反転、左右反転した状態の画像に対しても学習を行うことで汎用性の高いモデルを作成する","metadata":{}},{"cell_type":"markdown","source":"## Convolution(畳み込み)<a id=\"2.3\"></a>","metadata":{"id":"PqS2I93A5u_R"}},{"cell_type":"code","source":"def conv(img):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n    kernel = np.ones((7, 7), np.float32)/25\n    conv = cv2.filter2D(img, -1, kernel)\n    ax[0].imshow(img)\n    ax[0].set_title('original', fontsize=24)\n    ax[1].imshow(conv)\n    ax[1].set_title('convolved', fontsize=24)\n    plt.show()","metadata":{"_kg_hide-input":true,"id":"aa81abmWpnMG","execution":{"iopub.status.busy":"2022-01-12T02:04:19.787084Z","iopub.execute_input":"2022-01-12T02:04:19.787609Z","iopub.status.idle":"2022-01-12T02:04:19.793988Z","shell.execute_reply.started":"2022-01-12T02:04:19.787576Z","shell.execute_reply":"2022-01-12T02:04:19.793272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv(train_images[42])\nconv(train_images[25])\nconv(train_images[31])","metadata":{"_kg_hide-input":true,"id":"2rqxzygspnMJ","outputId":"98092ab1-48d2-4ce6-e3a9-0342e31cf91c","execution":{"iopub.status.busy":"2022-01-12T02:04:19.795505Z","iopub.execute_input":"2022-01-12T02:04:19.795728Z","iopub.status.idle":"2022-01-12T02:04:23.441146Z","shell.execute_reply.started":"2022-01-12T02:04:19.795703Z","shell.execute_reply":"2022-01-12T02:04:23.440447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"こちらも水増しの役割。日光を浴びているような画像になった","metadata":{}},{"cell_type":"markdown","source":"## Blurring(ぼやけた画像) <a id=\"2.4\"></a>\n\n元の画像をぼやけさせるblur関数を使用","metadata":{"id":"xmmHZzUq5xu8"}},{"cell_type":"code","source":"def blur(img):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n    ax[0].imshow(img)\n    ax[0].set_title('original', fontsize=24)\n    ax[1].imshow(cv2.blur(img, (100, 100)))\n    ax[1].set_title('blurred', fontsize=24)\n    plt.show()","metadata":{"_kg_hide-input":true,"id":"OcXTa-cxpnMM","execution":{"iopub.status.busy":"2022-01-12T02:04:23.44218Z","iopub.execute_input":"2022-01-12T02:04:23.442702Z","iopub.status.idle":"2022-01-12T02:04:23.448617Z","shell.execute_reply.started":"2022-01-12T02:04:23.442668Z","shell.execute_reply":"2022-01-12T02:04:23.44782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blur(train_images[42])\nblur(train_images[25])\nblur(train_images[31])","metadata":{"_kg_hide-input":true,"id":"aH5qRDdupnMP","outputId":"5b649688-b7e6-419e-d060-ddb09a291e38","execution":{"iopub.status.busy":"2022-01-12T02:04:23.449824Z","iopub.execute_input":"2022-01-12T02:04:23.450066Z","iopub.status.idle":"2022-01-12T02:04:26.910468Z","shell.execute_reply.started":"2022-01-12T02:04:23.450001Z","shell.execute_reply":"2022-01-12T02:04:26.909608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"想像以上にぼやけたが、rustなどは特徴が掴めていそうなので学習は出来そう","metadata":{}},{"cell_type":"markdown","source":"# モデル構築 <a id=\"3\"></a>","metadata":{"id":"wyfDeAJo6lGL"}},{"cell_type":"markdown","source":"## データの下準備 <a id=\"3.1\"></a>\n\n今回は多くのカーネルで使用されていたDenseNetとEfficientNetの2つを作成し、精度を比べてみる。","metadata":{"id":"J5f4-lkS7lKs"}},{"cell_type":"markdown","source":"### TPUの下準備\nkaggleのカーネルで使われているTRUを使用するための下準備を真似る。（notebookのAcceleratorはTPUv3-8に変更済み）","metadata":{"id":"zOfbl73V6t3p"}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","metadata":{"id":"2ZC6VPQHpnMR","execution":{"iopub.status.busy":"2022-01-12T02:04:26.911668Z","iopub.execute_input":"2022-01-12T02:04:26.911906Z","iopub.status.idle":"2022-01-12T02:05:09.334764Z","shell.execute_reply.started":"2022-01-12T02:04:26.91188Z","shell.execute_reply":"2022-01-12T02:05:09.33404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 病気のラベルとpathの読み込み","metadata":{"id":"SuAHc2hu6-Nu"}},{"cell_type":"code","source":"def format_path(st):\n    return GCS_DS_PATH + '/images/' + st + '.jpg'\n\ntest_paths = test.image_id.apply(format_path).values\ntrain_paths = train.image_id.apply(format_path).values\n\ntrain_labels = np.float32(train.loc[:, 'healthy':'scab'].values)\ntrain_paths, valid_paths, train_labels, valid_labels =\\\ntrain_test_split(train_paths, train_labels, test_size=0.15, random_state=2020)","metadata":{"id":"9BALmDtRpnMU","execution":{"iopub.status.busy":"2022-01-12T02:05:09.336015Z","iopub.execute_input":"2022-01-12T02:05:09.336658Z","iopub.status.idle":"2022-01-12T02:05:09.351841Z","shell.execute_reply.started":"2022-01-12T02:05:09.336621Z","shell.execute_reply":"2022-01-12T02:05:09.350987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_image(filename, label=None, image_size=(512, 512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","metadata":{"id":"T84Nnc1jpnMW","execution":{"iopub.status.busy":"2022-01-12T02:05:09.353143Z","iopub.execute_input":"2022-01-12T02:05:09.353545Z","iopub.status.idle":"2022-01-12T02:05:09.362155Z","shell.execute_reply.started":"2022-01-12T02:05:09.353516Z","shell.execute_reply":"2022-01-12T02:05:09.361493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### データセットのオブジェクトを作成","metadata":{"id":"tonEhhQ77Knh"}},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","metadata":{"id":"5rkIRCnupnMZ","execution":{"iopub.status.busy":"2022-01-12T02:05:09.363456Z","iopub.execute_input":"2022-01-12T02:05:09.363737Z","iopub.status.idle":"2022-01-12T02:05:09.610645Z","shell.execute_reply.started":"2022-01-12T02:05:09.363707Z","shell.execute_reply":"2022-01-12T02:05:09.609616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 追加機能を設定","metadata":{"id":"xmirtR2L7TDC"}},{"cell_type":"code","source":"def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n               lr_min=0.00001, lr_rampup_epochs=5, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","metadata":{"id":"uiiCB9SdpnMc","execution":{"iopub.status.busy":"2022-01-12T02:05:09.611791Z","iopub.execute_input":"2022-01-12T02:05:09.612006Z","iopub.status.idle":"2022-01-12T02:05:09.618427Z","shell.execute_reply.started":"2022-01-12T02:05:09.611981Z","shell.execute_reply":"2022-01-12T02:05:09.617586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ハイパーパラメータとコールバックの設定","metadata":{}},{"cell_type":"code","source":"lrfn = build_lrfn()\nSTEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T02:05:09.62Z","iopub.execute_input":"2022-01-12T02:05:09.620472Z","iopub.status.idle":"2022-01-12T02:05:09.631845Z","shell.execute_reply.started":"2022-01-12T02:05:09.620433Z","shell.execute_reply":"2022-01-12T02:05:09.63103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"正直この辺まではTPUのコピペ。この先のdensenetとefficientnetの2つを使って精度の違いを検証","metadata":{}},{"cell_type":"markdown","source":"## DenseNet <a id=\"3.2\"></a>","metadata":{"id":"PRtfn6bJ7qI1"}},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([DenseNet121(input_shape=(512, 512, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"id":"7fllHhN9pnMe","outputId":"c030160f-118a-43db-f431-0fd6b4379292","execution":{"iopub.status.busy":"2022-01-12T02:05:09.633476Z","iopub.execute_input":"2022-01-12T02:05:09.634142Z","iopub.status.idle":"2022-01-12T02:05:31.317178Z","shell.execute_reply.started":"2022-01-12T02:05:09.634098Z","shell.execute_reply":"2022-01-12T02:05:31.3161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DenseNet 基本ブロック","metadata":{}},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[13].output), dpi=70).create(prog='dot', format='svg'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-12T02:05:31.318596Z","iopub.execute_input":"2022-01-12T02:05:31.318921Z","iopub.status.idle":"2022-01-12T02:05:32.336325Z","shell.execute_reply.started":"2022-01-12T02:05:31.31888Z","shell.execute_reply":"2022-01-12T02:05:32.335372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DenseNetの基本ブロックの構造を図解。","metadata":{}},{"cell_type":"markdown","source":"### 使うモデルの図解\n\n参考にしたカーネルで使われていたDenseNetからpoolingへと続くモデルを真似して構築してみる","metadata":{}},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-12T02:05:32.337829Z","iopub.execute_input":"2022-01-12T02:05:32.338106Z","iopub.status.idle":"2022-01-12T02:05:32.462551Z","shell.execute_reply.started":"2022-01-12T02:05:32.338073Z","shell.execute_reply":"2022-01-12T02:05:32.461545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 訓練データの読み込み","metadata":{"id":"V4yafObR7wIo"}},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"id":"V56dwx6opnMh","outputId":"4fc94f9f-ac22-4386-a9d2-6e3192493b33","execution":{"iopub.status.busy":"2022-01-12T02:05:32.464086Z","iopub.execute_input":"2022-01-12T02:05:32.464381Z","iopub.status.idle":"2022-01-12T02:20:00.453059Z","shell.execute_reply.started":"2022-01-12T02:05:32.464348Z","shell.execute_reply":"2022-01-12T02:20:00.452042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 学習結果を取得","metadata":{"id":"H32GrB0L7ulh"}},{"cell_type":"code","source":"## DenseNetのエポックごとのLossと正解率を可視化\ndef display_training_curves(training, validation, yaxis):\n    if yaxis == \"loss\":\n        ylabel = \"Loss\"\n        title = \"エポックごとのloss\"\n    else:\n        ylabel = \"Accuracy\"\n        title = \"エポックごとの正解率\"\n        \n    fig = go.Figure()\n        \n    fig.add_trace(\n        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=training, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"))\n    \n    fig.add_trace(\n        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=validation, marker=dict(color=\"darkorange\"),\n               name=\"Val\"))\n    \n    fig.update_layout(title_text=title, yaxis_title=ylabel, xaxis_title=\"Epochs\", template=\"plotly_white\")\n    fig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-12T02:20:00.454368Z","iopub.execute_input":"2022-01-12T02:20:00.454622Z","iopub.status.idle":"2022-01-12T02:20:00.46392Z","shell.execute_reply.started":"2022-01-12T02:20:00.454593Z","shell.execute_reply":"2022-01-12T02:20:00.462946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### グラフ","metadata":{"id":"U2Rmet4p-ot3"}},{"cell_type":"code","source":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","metadata":{"_kg_hide-input":true,"id":"dKUl8NckpnMn","outputId":"eb37495e-52fc-4e0f-dc0b-e82e708dcf94","execution":{"iopub.status.busy":"2022-01-12T02:20:00.465654Z","iopub.execute_input":"2022-01-12T02:20:00.46597Z","iopub.status.idle":"2022-01-12T02:20:00.516763Z","shell.execute_reply.started":"2022-01-12T02:20:00.465931Z","shell.execute_reply":"2022-01-12T02:20:00.515818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"11エポックあたりから正解率は90%を超えている。最初の方はtrainとvalidationの正解率にバラツキがあるが一定以上学習が進むとどちらも高い正解率を示す。","metadata":{}},{"cell_type":"markdown","source":"### DenseNetの分類結果をいくつか表示","metadata":{}},{"cell_type":"code","source":"def process(img):\n    return cv2.resize(img/255.0, (512, 512)).reshape(-1, 512, 512, 3)\ndef predict(img):\n    return model.layers[2](model.layers[1](model.layers[0](process(img)))).numpy()[0]\n\nfig = make_subplots(rows=4, cols=2)\npreds = predict(train_images[2])\n\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Scab\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Multiple diseases\"\n\ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Healthy\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[2], (205, 136))), row=1, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=1, col=2)\nfig.update_layout(height=1200, width=800, title_text=\"DenseNet Predictions\", showlegend=False)\n\npreds = predict(train_images[0])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Multiple diseases\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[0], (205, 136))), row=2, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=2, col=2)\n\npreds = predict(train_images[3])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Rust\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[3], (205, 136))), row=3, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=3, col=2)\n\npreds = predict(train_images[1])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Scab\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[1], (205, 136))), row=4, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=4, col=2)\n\nfig.update_layout(template=\"plotly_white\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-12T02:20:00.52364Z","iopub.execute_input":"2022-01-12T02:20:00.523884Z","iopub.status.idle":"2022-01-12T02:20:19.921722Z","shell.execute_reply.started":"2022-01-12T02:20:00.523858Z","shell.execute_reply":"2022-01-12T02:20:19.920765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"画像と何の病気に分類されたかが可視化できた。良い精度で学習できているのが分かる。","metadata":{}},{"cell_type":"markdown","source":"## EfficientNet <a id=\"3.3\"></a>\n\nEfficientNetはCNNベースのImageNetモデル。今回はこれを用いてモデルを構築する","metadata":{"id":"isy53hSJ72O5"}},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([efn.EfficientNetB7(input_shape=(512, 512, 3),\n                                                    weights='imagenet',\n                                                    include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n    \n    \n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"id":"x8ELPOLJpnMp","outputId":"03e23ca7-1a98-4a3d-d61b-3edcb1ae0ced","execution":{"iopub.status.busy":"2022-01-12T02:20:19.922851Z","iopub.execute_input":"2022-01-12T02:20:19.923539Z","iopub.status.idle":"2022-01-12T02:21:06.031326Z","shell.execute_reply.started":"2022-01-12T02:20:19.923506Z","shell.execute_reply":"2022-01-12T02:21:06.030457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EfficientNetの基本的な構造","metadata":{}},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[11].output), dpi=70).create(prog='dot', format='svg'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-12T02:21:06.032544Z","iopub.execute_input":"2022-01-12T02:21:06.032766Z","iopub.status.idle":"2022-01-12T02:21:06.256255Z","shell.execute_reply.started":"2022-01-12T02:21:06.03274Z","shell.execute_reply":"2022-01-12T02:21:06.255538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EfficientNetの基本的な構造。","metadata":{}},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-12T02:21:06.257828Z","iopub.execute_input":"2022-01-12T02:21:06.258063Z","iopub.status.idle":"2022-01-12T02:21:06.411442Z","shell.execute_reply.started":"2022-01-12T02:21:06.258034Z","shell.execute_reply":"2022-01-12T02:21:06.410453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"参考にしたカーネルと同じ流れでモデルを設計してみる。","metadata":{}},{"cell_type":"markdown","source":"### 訓練データの読み込み","metadata":{"id":"DKydIBmq76tf"}},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"id":"MHuypFwdpnMr","outputId":"3fde00ef-c8ac-46b4-bd0f-81a1294bf66f","execution":{"iopub.status.busy":"2022-01-12T02:21:06.413007Z","iopub.execute_input":"2022-01-12T02:21:06.413482Z","iopub.status.idle":"2022-01-12T02:36:38.045451Z","shell.execute_reply.started":"2022-01-12T02:21:06.413445Z","shell.execute_reply":"2022-01-12T02:36:38.044472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 学習結果の可視化","metadata":{"id":"hus0ZMYP8Ehy"}},{"cell_type":"code","source":"## densenetと同じく学習精度の推移を可視化\ndisplay_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","metadata":{"_kg_hide-input":true,"id":"94JJ7UNDpnMu","outputId":"f4331b89-9fbc-47aa-884a-0b5d26df1479","execution":{"iopub.status.busy":"2022-01-12T02:36:38.046784Z","iopub.execute_input":"2022-01-12T02:36:38.047088Z","iopub.status.idle":"2022-01-12T02:36:38.08716Z","shell.execute_reply.started":"2022-01-12T02:36:38.04705Z","shell.execute_reply":"2022-01-12T02:36:38.086287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"trainもvalidもおおよそ学習が出来ている。特にvalidationの方がdensenetに比べて比較的早いエポックで90%近くの精度まで上がっている。\n\nefficientnetの方が学習スピードは早いのか。","metadata":{}},{"cell_type":"code","source":"## densenetと同じ関数を使用\ndef process(img):\n    return cv2.resize(img/255.0, (512, 512)).reshape(-1, 512, 512, 3)\ndef predict(img):\n    return model.layers[2](model.layers[1](model.layers[0](process(img)))).numpy()[0]\n\nfig = make_subplots(rows=4, cols=2)\npreds = predict(train_images[2])\n\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Scab\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Multiple diseases\"\n\ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Healthy\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[2], (205, 136))), row=1, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=1, col=2)\nfig.update_layout(height=1200, width=800, title_text=\"EfficientNet Predictions\", showlegend=False)\n\npreds = predict(train_images[0])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Multiple diseases\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[0], (205, 136))), row=2, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=2, col=2)\n\npreds = predict(train_images[3])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Rust\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[3], (205, 136))), row=3, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=3, col=2)\n\npreds = predict(train_images[1])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Scab\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[1], (205, 136))), row=4, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=4, col=2)\nfig.update_layout(template=\"plotly_white\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-12T02:36:38.08897Z","iopub.execute_input":"2022-01-12T02:36:38.089277Z","iopub.status.idle":"2022-01-12T02:37:03.379653Z","shell.execute_reply.started":"2022-01-12T02:36:38.089237Z","shell.execute_reply":"2022-01-12T02:37:03.378879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# まとめと考察 <a id=\"4\"></a>\n\n## 1. EDAについて\n深層学習のため特徴量はこちらで用意していないが、多くのカーネルでrgbのチャネル値の分布をチェックしていたため、画像の色彩分布は学習の際に使われたのではないかと考える。\n\nまた、multi diseaseが画像数が他に比べて少なく精度への影響を懸念していたが、どちらのモデルでも問題なく学習が進んでいたため画像の傘増しが効いたのではないかと考えている。\n\n## 2. 学習前の下準備について\nエッジ検出をCanny関数を用いることで行う方法について学習した。仕組みが完全に理解できたわけではないが、光の勾配を算出してそこが大きい箇所が物体が変わった位置と捉えて、ほぼ正確な葉の位置を検出するというおおよその仕組みは理解できた。\n\n## 3. 学習精度について\n今回はdensenetとefficientnetの2つを用いて学習を行い、densenetよりもefficientnetの方がより少ないエポックで良い精度が出ていることが分かった。画像解析は事前に何を行うかではなく、どのようなモデルを適切な状況において使えるかが学習精度の鍵になってきそうなので、この辺のモデルの特徴はぜひ押さえておきたい。","metadata":{}},{"cell_type":"markdown","source":"## 今後の展望\n今回学習の精度は良かったのだが、DenceNet・EfficientNet共にかなり計算時間がかかってしまった。学習したMobileNetを使うことで多少精度が下がったとしてもどれくらい計算コストが削減できるのかを調査してみたいと考えている。\n\nまた、今回の開発では汎化性能についてまではあまり言及できていないため、実際のkaggle等ではdropout等も用いながら汎化性能についても検証していきたい。","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}