{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"'''# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys, math\nimport zipfile\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd\nimport numpy as np\nfrom kaggle_datasets import KaggleDatasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path(\"plant-pathology-train-tfrecords\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(GCS_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_SIZE = [1365,2048]\nAUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n#GCS_PATTERN = '../input/plant-pathology-2020-fgvc7/images/Train_*.jpg'\nGCS_OUTPUT = './'  # prefix for output file names\nSHARDS = 16\n\ndef _bytestring_feature(list_of_bytestrings):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n\ndef _int_feature(list_of_ints): # int64\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n\ndef _float_feature(list_of_floats): # float32\n  return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_tfrecord(example):\n    TARGET_SIZE = [1365,2048]\n    print(example)\n    features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n        # additional (not very useful) fields to demonstrate TFRecord writing/reading of different types of data\n        \"label\": tf.io.FixedLenFeature([4], tf.int64),  # 4 integers\n        \"size\": tf.io.FixedLenFeature([2], tf.int64),  # two integers\n    }\n    # decode the TFRecord\n    example = tf.io.parse_single_example(example, features)\n    \n    # FixedLenFeature fields are now ready to use: exmple['size']\n    # VarLenFeature fields require additional sparse_to_dense decoding\n    print(example[\"image\"].get_shape())\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n        \n    label  = example['label']\n    height = example['size'][0]\n    width  = example['size'][1]\n    \n    image = tf.reshape(image,[height, width , 3])\n    \n    image = tf.image.resize(image,TARGET_SIZE, method=tf.image.ResizeMethod.LANCZOS3)\n    return image, label, height, width\n    \n# read from TFRecords. For optimal performance, read from multiple\n# TFRecord files at once and set the option experimental_deterministic = False\n# to allow order-altering optimizations.\n\noption_no_order = tf.data.Options()\noption_no_order.experimental_deterministic = False\n\nfilenames = tf.io.gfile.glob(GCS_PATH+\"/*.tfrec\")\nprint(filenames)\ndataset4 = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\ndataset4 = dataset4.with_options(option_no_order)\ndataset4 = dataset4.map(read_tfrecord, num_parallel_calls=AUTO)\ndataset4 = dataset4.shuffle(300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for x in dataset4:\n    print(x)\n    break"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_dataset = dataset4.map(lambda image, label, height, width: (image, label))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for x in final_dataset:\n    print(x)\n    break"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_augment(image, labels):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    print(\"a\",image.get_shape())\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_saturation(image, 0, 2)\n    print(\"b\",image.get_shape())\n    #image = \n    image = tf.math.divide(image,[255])\n    print(\"c\",image.get_shape())\n    return image, labels   \n\nfinal_dataset = final_dataset.map(data_augment, num_parallel_calls=AUTO)\nfinal_dataset = final_dataset.repeat()\nfinal_dataset = final_dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for x in final_dataset:\n    print(x)\n    break"},{"metadata":{},"cell_type":"markdown","source":"def to_float32(image, label,x,y):\n    print(image.get_shape())\n    return tf.cast(image, tf.float32), tf.cast(label,tf.float32)"},{"metadata":{},"cell_type":"markdown","source":"training_dataset = dataset4.map(to_float32)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"markdown","source":"base_dir = '../input/plant-pathology-2020-fgvc7/images/'\ntrain_csv = pd.read_csv('../input/plant-pathology-2020-fgvc7/train.csv')\ntest_csv = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')\n\n#train_csv = train_csv.astype(str)\ntrain_csv = train_csv.sample(frac=1).reset_index(drop=True)\nprint(len(train_csv.index))\n\ntrain_csv[\"image_id\"] = train_csv[\"image_id\"]+'.jpg'\ntest_csv[\"image_id\"] = test_csv[\"image_id\"]+'.jpg'\nsplit_index = int(len(train_csv.index)*0.2)\nval_csv = train_csv[-1*split_index:]\nprint(len(train_csv.index),len(train_csv.index) - split_index)\ntrain_csv = train_csv[:-1* split_index]\nprint(len(train_csv.index),len(val_csv.index),split_index)\ntrain_csv.head(10)\n#train_dir = os.path.join(base_dir, 'train')\n#validation_dir = os.path.join(base_dir, 'validation')\n\n# Directory with our training cat pictures\n#train_cats_dir = os.path.join(train_dir, 'cats')\n\n# Directory with our training dog pictures\n#train_dogs_dir = os.path.join(train_dir, 'dogs')\n\n# Directory with our validation cat pictures\n#validation_cats_dir = os.path.join(validation_dir, 'cats')\n\n# Directory with our validation dog pictures\n#validation_dogs_dir = os.path.join(validation_dir, 'dogs')"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''train_csv = train_csv.astype(str)\nval_csv = val_csv.astype(str)\nprint(train_csv.iloc[5,2],type(train_csv.iloc[5,2]))'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''mask = train_csv[\"healthy\"] == '1'\ntrain_csv.loc[mask,\"healthy\"] = 1'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''print(train_csv.iloc[2,1],type(train_csv.iloc[2,1]))'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"train_csv[\"multiple_diseases\"] = train_csv[\"multiple_diseases\"].astype(int)\ntrain_csv[\"rust\"] = train_csv[\"rust\"].astype(int)\ntrain_csv[\"scab\"] = train_csv[\"scab\"].astype(int)\ntrain_csv[\"healthy\"] = train_csv[\"healthy\"].astype(int)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"val_csv[\"multiple_diseases\"] = val_csv[\"multiple_diseases\"].astype(int)\nval_csv[\"rust\"] = val_csv[\"rust\"].astype(int)\nval_csv[\"scab\"] = val_csv[\"scab\"].astype(int)\nval_csv[\"healthy\"] = val_csv[\"healthy\"].astype(int)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def createAndCompileModel():\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(512, (9,9), strides = (2,2), activation='relu', input_shape=(1365,2048, 3), name = \"conv2D_1\"),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Conv2D(246, (7,7), activation='relu',name = \"conv2D_2\"),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Conv2D(128, (7,7), activation='relu',name = \"conv2D_3\"),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Conv2D(128, (5,5), activation='relu', name = \"conv2D_4\"),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Conv2D(90, (3,3), activation='relu', name = \"conv2D_6\"),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Conv2D(64, (3,3), activation='relu', name = \"conv2D_7\"),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Conv2D(32, (3,3), activation='relu', name = \"conv2D_8\"),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(4, activation='sigmoid')\n    ])\n\n    lr_schedule = ExponentialDecay(\n        1e-4,\n        decay_steps=12000,\n        decay_rate=0.96,\n        staircase=True)\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer=RMSprop(learning_rate=lr_schedule, momentum = 1e-4),\n                  metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model = createAndCompileModel() # define your model normally"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tpu_strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# All images will be rescaled by 1./255\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n      rotation_range=30,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\ntest_datagen = ImageDataGenerator(rescale=1./255)\nval_datagen = ImageDataGenerator(rescale=1./255)\n'''\n# Flow training images in batches of 20 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,  # This is the source directory for training images\n        target_size=(150, 150),  # All images will be resized to 150x150\n        batch_size=20,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')\n\n# Flow validation images in batches of 20 using test_datagen generator\nvalidation_generator = test_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(150, 150),\n        batch_size=20,\n        class_mode='binary')\n'''\nbatchSize = 4\ntrain_generator = train_datagen.flow_from_dataframe(\n        train_csv,\n        directory = base_dir,\n        x_col = 'image_id',\n        y_col = ['healthy','multiple_diseases','rust','scab'],\n        target_size=(750, 750),\n        batch_size=batchSize,\n        class_mode='raw'\n        )\nvalidation_generator = val_datagen.flow_from_dataframe(\n        val_csv,\n        directory = base_dir,\n        x_col = 'image_id',\n        y_col = ['healthy','multiple_diseases','rust','scab'],\n        target_size=(750, 750),\n        batch_size=batchSize,\n        class_mode='raw'\n        )\ntest_generator = test_datagen.flow_from_dataframe(\n        test_csv,\n        directory = base_dir,\n        x_col = 'image_id',\n        y_col = [],\n        target_size=(750, 750),\n        batch_size=batchSize,\n        class_mode=None,\n        shuffle = False\n        )"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    model = tf.keras.models.load_model(\"../input/plant-pathology-2020-tpu/my_modelv5.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nEpoch 45/45 #1000x1000 input\n364/364 [==============================] - 695s 2s/step - loss: 0.1818 - accuracy: 0.8809 - val_loss: 0.2045 - val_accuracy: 0.8819\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_27 (Conv2D)           (None, 994, 994, 246)     36408     \n_________________________________________________________________\nmax_pooling2d_26 (MaxPooling (None, 497, 497, 246)     0         \n_________________________________________________________________\nconv2d_28 (Conv2D)           (None, 493, 493, 246)     1513146   \n_________________________________________________________________\nmax_pooling2d_27 (MaxPooling (None, 246, 246, 246)     0         \n_________________________________________________________________\nconv2d_29 (Conv2D)           (None, 242, 242, 128)     787328    \n_________________________________________________________________\nmax_pooling2d_28 (MaxPooling (None, 121, 121, 128)     0         \n_________________________________________________________________\nconv2d_30 (Conv2D)           (None, 119, 119, 128)     147584    \n_________________________________________________________________\nmax_pooling2d_29 (MaxPooling (None, 59, 59, 128)       0         \n_________________________________________________________________\nconv2d_31 (Conv2D)           (None, 57, 57, 90)        103770    \n_________________________________________________________________\nmax_pooling2d_30 (MaxPooling (None, 28, 28, 90)        0         \n_________________________________________________________________\nconv2d_32 (Conv2D)           (None, 26, 26, 90)        72990     \n_________________________________________________________________\nmax_pooling2d_31 (MaxPooling (None, 13, 13, 90)        0         \n_________________________________________________________________\nconv2d_33 (Conv2D)           (None, 11, 11, 64)        51904     \n_________________________________________________________________\nmax_pooling2d_32 (MaxPooling (None, 5, 5, 64)          0         \n_________________________________________________________________\nconv2d_34 (Conv2D)           (None, 3, 3, 32)          18464     \n_________________________________________________________________\nmax_pooling2d_33 (MaxPooling (None, 1, 1, 32)          0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 32)                0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 128)               4224      \n_________________________________________________________________\ndense_7 (Dense)              (None, 4)                 516       \n=================================================================\nTotal params: 2,736,334\nTrainable params: 2,736,334\nNon-trainable params: 0\n_________________________________________________________________\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nEpoch 30/30\n364/364 [==============================] - 534s 1s/step - loss: 0.2676 - accuracy: 0.8259 - val_loss: 0.3444 - val_accuracy: 0.7967\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 994, 994, 246)     36408     \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 497, 497, 246)     0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 493, 493, 128)     787328    \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 246, 246, 128)     0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 244, 244, 128)     147584    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 122, 122, 128)     0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 120, 120, 90)      103770    \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 60, 60, 90)        0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 58, 58, 64)        51904     \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 29, 29, 64)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 53824)             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               6889600   \n_________________________________________________________________\ndense_3 (Dense)              (None, 4)                 516       \n=================================================================\nTotal params: 8,017,110\nTrainable params: 8,017,110\nNon-trainable params: 0\n_________________________________________________________________\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 998, 998, 246)     6888      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 499, 499, 246)     0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 497, 497, 128)     283520    \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 248, 248, 128)     0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 246, 246, 128)     147584    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 123, 123, 128)     0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 121, 121, 64)      73792     \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 60, 60, 64)        0         \n_________________________________________________________________\ndropout (Dropout)            (None, 60, 60, 64)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 230400)            0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               117965312 \n_________________________________________________________________\ndense_1 (Dense)              (None, 256)               131328    \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_4 (Dense)              (None, 4)                 516       \n=================================================================\nTotal params: 118,658,348\nTrainable params: 118,658,348\nNon-trainable params: 0\n_________________________________________________________________\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trainX = np.load(\"../input/notebookd1c779b8a4/trainDataXNumpy.npy\")\n#trainY = np.load(\"../input/notebookd1c779b8a4/trainDataYNumpy.npy\")\n#valX = np.load(\"../input/notebookd1c779b8a4/valDataXNumpy.npy\")\n#valY = np.load(\"../input/notebookd1c779b8a4/valDataYNumpy.npy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"for x in final_dataset:\n    print(x)\n    break"},{"metadata":{"trusted":true},"cell_type":"code","source":"4 * tpu_strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batchSize = 32\nfinal_dataset2 = final_dataset.batch(batchSize)\nfinal_dataset2 = final_dataset2.prefetch(AUTO)\nTRAIN_STEPS = 1821 // batchSize\nprint(\"TRAINING IMAGES: \", 1821, \", STEPS PER EPOCH: \", TRAIN_STEPS)\n\nhistory = model.fit(\n      final_dataset2,\n      batch_size = batchSize,\n      shuffle=False,\n      steps_per_epoch=TRAIN_STEPS,\n      epochs=40)\n#Epoch 100/100\n#8/8 [==============================] - 58s 7s/step - loss: 0.1785 - accuracy: 0.8737 - val_loss: 0.4360 - val_accuracy: 0.6486\n#Epoch 100/100\n#8/8 [==============================] - 59s 7s/step - loss: 0.1752 - accuracy: 0.8579 - val_loss: 0.3450 - val_accuracy: 0.7143\n# val_accuracy: 0.75 maybe\n#Epoch 99/100\n#29/29 [==============================] - 75s 3s/step - loss: 0.2268 - accuracy: 0.9360 - val_loss: 0.5634 - val_accuracy: 0.7543\n#Epoch 5/50\n#91/91 [==============================] - 80s 874ms/step - loss: 0.4055 - accuracy: 0.6225 - val_loss: 0.4009 - val_accuracy: 0.6108\n#Epoch 100/100 #500x500 input\n#91/91 [==============================] - 154s 2s/step - loss: 0.1259 - accuracy: 0.9181 - val_loss: 0.1227 - val_accuracy: 0.9460\n#Epoch 55/55 #1000x1000 input\n#364/364 [==============================] - 470s 1s/step - loss: 0.2441 - accuracy: 0.8293 - val_loss: 0.2559 - val_accuracy: 0.8819\n#Epoch 30/30 #1000x1000 input\n#364/364 [==============================] - 534s 1s/step - loss: 0.2676 - accuracy: 0.8259 - val_loss: 0.3444 - val_accuracy: 0.7967\n#Epoch 45/45 #1000x1000 input\n#364/364 [==============================] - 695s 2s/step - loss: 0.1818 - accuracy: 0.8809 - val_loss: 0.2045 - val_accuracy: 0.8819","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('./my_modelv6.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\n#val_acc = history.history['val_accuracy']\nloss = history.history['loss']\n#val_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training accuracy')\n#plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training accuracy')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training Loss')\n#plt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''test_csv = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')\ntest_csv[\"image_id\"] = test_csv[\"image_id\"] + '.jpg'\ntest_csv.head()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''test_generator = test_datagen.flow_from_dataframe(\n        test_csv,\n        directory = base_dir,\n        x_col = 'image_id',\n        y_col = ['healthy','multiple_diseases','rust','scab'],\n        target_size=(150, 150),\n        batch_size=200,\n        class_mode='raw'\n        )\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"p = model.predict(test_generator)\nprint(p)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"test_csv = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')\np_df = pd.DataFrame(p,columns=['healthy','multiple_diseases','rust','scab'])\nresult = pd.concat([test_csv, p_df], axis=1)\n#print(result[\"image_id\"][:-4])\n#result[\"image_id\"] = result[\"image_id\"][:-4]"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"result.to_csv(\"./submission.csv\",index = False)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"result.head()"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''p[:5]'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = tf.keras.models.load_model('../input/notebook45bc751087/my_modelv4.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.evaluate(validation_generator)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}