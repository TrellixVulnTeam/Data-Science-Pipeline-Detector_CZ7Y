{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Plant Pathology - The Power of Deep Learning & Computer Vision - Research","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"<center><img src=\"https://cals.ncsu.edu/entomology-and-plant-pathology/wp-content/uploads/sites/8/2019/03/scouting-for-disease-950x535.jpg\" width=\"500px\"></center>","metadata":{}},{"cell_type":"markdown","source":"<font size=3>This is a Plant Pathology Research, I will like to get started on the power of Deep Learning and Computer Vision to diagnose plant diseases by studying plant leaf images. Some categories include \"healthy\", \"rust\" and \"multiple pathologies\".\n\n<font size=3>Learning how to solve this problem, will help understand and get introduce on this field that can be transferred to other research to help having a healthier and brighter future not only for the planet, but for our humanity.\n\n<font size=3>I am looking forward to visualize the data using Seaborn and some Plotly alongside explain as I learn some image processing techniques provided by OpenCV.\n\n<font size=3>Finally, I will show how different pre-trained Tensor Flow models and apply convolution formulas to help find the source of the problem and shed some light for this and oncoming researches.","metadata":{}},{"cell_type":"markdown","source":"### Why using TensorFlow? \n<font size=3> As a way to introduce what it can do and why we are going to use it and to learn how it can help further this research ahead. \n\n<font size=3> Here a video on Why TensorFlow:","metadata":{}},{"cell_type":"code","source":"from IPython.display import HTML\nHTML('<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/yjprpOoH5c8?controls=0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></center>')","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:45:52.376859Z","iopub.execute_input":"2021-05-28T03:45:52.377553Z","iopub.status.idle":"2021-05-28T03:45:52.386132Z","shell.execute_reply.started":"2021-05-28T03:45:52.377489Z","shell.execute_reply":"2021-05-28T03:45:52.38501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Table of Contents","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents <a id=\"0\"></a>\n\n* [<font size=4>Quick Exploratory Analysis</font>](#1)\n    * [1.1. Some techology to use - EfficientNet](#1.1)\n    * [1.2. Setting the ground work](#1.2)\n    * [1.3. Sample image visualization](#1.3)\n    * [1.4. Color Channel distributions and mean values](#1.4)\n    * [1.5. Visualize more sample leaves](#1.5)\n    * [1.6. Target - Is it Healthy?](#1.6)\n    \n    \n* [<font size=4>Image Processing</font>](#2)\n    * [2.1. OpenCV to show images](#2.1)\n    * [2.2. Convolution on RGB images](#2.1)\n    \n\n* [<font size=4>Models</font>](#3)\n* [<font size=4>Conclusions</font>](#4)\n\n* [<font size=4>Thank you note</font>](#5)\n\n* [<font size=4>References</font>](#6)","metadata":{}},{"cell_type":"markdown","source":"# Quick Exploratory Analysis <a id=\"1\"></a> - <a id=\"1\"></a> [<font size=4>^</font>](#0)","metadata":{}},{"cell_type":"markdown","source":"<font size=3> Some of the new technologies that we will use are called EfficientNet","metadata":{}},{"cell_type":"markdown","source":"## Waht is EfficientNet and why use it? <a id=\"1.1\"></a>\n\n<font size=3> EfficientNet is another popular (more recent) Convolutional Neural Network or CNN-based ImageNet model which achieved the State of the Art on several image-based tasks in 2019. EfficientNet performs model scaling in an innovative way to achieve excellent accuracy with significantly fewer parameters. It achieves the same if not greater accuracy than ResNet and DenseNet with a mcuh shallower architecture. Now let us train EfficientNet on leaf images and evaluate its performance.","metadata":{}},{"cell_type":"code","source":"!pip install -q efficientnet\nprint('efficientnet installed')","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:15:53.977953Z","iopub.execute_input":"2021-05-28T04:15:53.978456Z","iopub.status.idle":"2021-05-28T04:16:01.216234Z","shell.execute_reply.started":"2021-05-28T04:15:53.978415Z","shell.execute_reply":"2021-05-28T04:16:01.214995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting the ground work","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport re\n\nimport cv2\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport tensorflow as tf\nfrom IPython.display import SVG\nimport efficientnet.tfkeras as efn\nfrom keras.utils import plot_model\nimport tensorflow.keras.layers as L\nfrom keras.utils import model_to_dot\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.applications import DenseNet121\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\ntqdm.pandas()\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nnp.random.seed(0)\ntf.random.set_seed(0)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint('Ground work set: (pandas, ploty, numpy, seaborn, sklearn, matplotlib, pyplot, cv2, math, tensor, scipy, IPython, keras and Efficientnet imported)')","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:16:01.218665Z","iopub.execute_input":"2021-05-28T04:16:01.219081Z","iopub.status.idle":"2021-05-28T04:16:01.238104Z","shell.execute_reply.started":"2021-05-28T04:16:01.219044Z","shell.execute_reply":"2021-05-28T04:16:01.236531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train and Test data ","metadata":{}},{"cell_type":"code","source":"img_path = \"../input/plant-pathology-2020-fgvc7/images/\"\ntest_path = \"../input/plant-pathology-2020-fgvc7/test.csv\"\ntrain_path = \"../input/plant-pathology-2020-fgvc7/train.csv\"\nsub_path = \"../input/plant-pathology-2020-fgvc7/sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:17:44.02597Z","iopub.execute_input":"2021-05-28T04:17:44.026571Z","iopub.status.idle":"2021-05-28T04:17:44.031038Z","shell.execute_reply.started":"2021-05-28T04:17:44.026535Z","shell.execute_reply":"2021-05-28T04:17:44.030171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os import listdir\nrl = listdir(img_path)\nimg = cv2.imread(img_path+rl[0])\n\n\nb,g,r = cv2.split(img)\nprint(b,g,r)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:16:26.644885Z","iopub.execute_input":"2021-05-28T04:16:26.645304Z","iopub.status.idle":"2021-05-28T04:16:26.710322Z","shell.execute_reply.started":"2021-05-28T04:16:26.645269Z","shell.execute_reply":"2021-05-28T04:16:26.709426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\nsample = 50\nprint(epochs,'epochs and a sample of',sample, 'selected')","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:45:59.837172Z","iopub.execute_input":"2021-05-28T03:45:59.837609Z","iopub.status.idle":"2021-05-28T03:45:59.844999Z","shell.execute_reply.started":"2021-05-28T03:45:59.83757Z","shell.execute_reply":"2021-05-28T03:45:59.843684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Epoch\n<font size=3> What Is an Epoch? The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset. One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters.","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(sub_path)\ntest_data = pd.read_csv(test_path)\ntrain_data = pd.read_csv(train_path)\nprint('sub, test_data and train_data dataframes created')","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:45:59.846829Z","iopub.execute_input":"2021-05-28T03:45:59.847332Z","iopub.status.idle":"2021-05-28T03:45:59.885115Z","shell.execute_reply.started":"2021-05-28T03:45:59.847287Z","shell.execute_reply":"2021-05-28T03:45:59.883851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train data head')\ntrain_data.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:45:59.886716Z","iopub.execute_input":"2021-05-28T03:45:59.887096Z","iopub.status.idle":"2021-05-28T03:45:59.904134Z","shell.execute_reply.started":"2021-05-28T03:45:59.887058Z","shell.execute_reply":"2021-05-28T03:45:59.903004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test data head')\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:45:59.908184Z","iopub.execute_input":"2021-05-28T03:45:59.908603Z","iopub.status.idle":"2021-05-28T03:45:59.9222Z","shell.execute_reply.started":"2021-05-28T03:45:59.908563Z","shell.execute_reply":"2021-05-28T03:45:59.920655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3> Ok now let's see one of those loaded images on the train set. ","metadata":{}},{"cell_type":"code","source":"def image_loading(image_id):\n    file_path = image_id + \".jpg\"\n    image = cv2.imread(img_path + file_path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\ntrain_images = train_data[\"image_id\"][:sample].progress_apply(image_loading)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:45:59.925032Z","iopub.execute_input":"2021-05-28T03:45:59.925534Z","iopub.status.idle":"2021-05-28T03:46:02.088203Z","shell.execute_reply.started":"2021-05-28T03:45:59.925463Z","shell.execute_reply":"2021-05-28T03:46:02.086816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3> images_loading function created and applied to train_images","metadata":{}},{"cell_type":"markdown","source":"## **Sample image visualization** <a id=\"1.2\"></a>\n","metadata":{}},{"cell_type":"markdown","source":"<font size=3> Out of the 50 sample here we are selecting the last image, which look in very bad rusting and scab stage. It seems it may have multiple diseases. \n\n<font size=3> As we we progress on our research we can spot which category is this leave on. </font>","metadata":{}},{"cell_type":"code","source":"fig = px.imshow(cv2.resize(train_images[49], (205, 136)))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:16:34.530004Z","iopub.execute_input":"2021-05-28T04:16:34.530553Z","iopub.status.idle":"2021-05-28T04:16:34.595059Z","shell.execute_reply.started":"2021-05-28T04:16:34.530517Z","shell.execute_reply":"2021-05-28T04:16:34.594102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3> As you hover over the image you can realize you get the pixel coordinates and a array with the three colors density for red, green and blue or RGB. We are not going to get into much deep explaining the colors, but that is the cuprit for the machine learning models that we will use to spot the images that apper healty from the unhealthy ones.","metadata":{}},{"cell_type":"markdown","source":"\n## Color Channel distributions and mean values <a id =\"1.4\"></a>","metadata":{}},{"cell_type":"code","source":"red_values = [np.mean(train_images[idx][0:, 0:, 0]) for idx in range(len(train_images))]\ngreen_values = [np.mean(train_images[idx][0:, 0:, 1]) for idx in range(len(train_images))]\nblue_values = [np.mean(train_images[idx][0:, 0:, 2]) for idx in range(len(train_images))]\nvalues = [np.mean(train_images[idx]) for idx in range(len(train_images))]","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:02.162856Z","iopub.execute_input":"2021-05-28T03:46:02.163182Z","iopub.status.idle":"2021-05-28T03:46:03.057668Z","shell.execute_reply.started":"2021-05-28T03:46:02.163151Z","shell.execute_reply":"2021-05-28T03:46:03.056467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread(img_path+rl[0])\n\nb,g,r = cv2.split(img)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:03.058794Z","iopub.execute_input":"2021-05-28T03:46:03.059087Z","iopub.status.idle":"2021-05-28T03:46:03.106364Z","shell.execute_reply.started":"2021-05-28T03:46:03.05906Z","shell.execute_reply":"2021-05-28T03:46:03.1053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(img)\nplt.imshow(r)\nplt.imshow(g)\nplt.imshow(b)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:03.107645Z","iopub.execute_input":"2021-05-28T03:46:03.107942Z","iopub.status.idle":"2021-05-28T03:46:04.482445Z","shell.execute_reply.started":"2021-05-28T03:46:03.107913Z","shell.execute_reply":"2021-05-28T03:46:04.481105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\n\nfor idx, values in enumerate([red_values, green_values, blue_values]):\n    if idx == 0:\n        color = \"Red\"\n    if idx == 1:\n        color = \"Green\"\n    if idx == 2:\n        color = \"Blue\"\n    fig.add_trace(go.Box(x=[color]*len(values), y=values, name=color, marker=dict(color=color.lower())))\n    \nfig.update_layout(yaxis_title=\"Mean values\", xaxis_title=\"Color channels\",\n                  title=\"Mean values over Color channels\", template=\"plotly_white\")","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:28:40.285206Z","iopub.execute_input":"2021-05-28T04:28:40.286822Z","iopub.status.idle":"2021-05-28T04:28:40.354553Z","shell.execute_reply.started":"2021-05-28T04:28:40.28677Z","shell.execute_reply":"2021-05-28T04:28:40.353498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Color definitions to get the mean values\n<font size=3> Red_values, Green_values, Blue_values are each assigned the numpy mean to the index that correspond to the color. \n\n                               [red, green, blue]\n<font size=3> First value in the color array correspone to red, so we skip to index 0, then 1 for green and 2 for blue. \n\ne.g.:\n\n       red_values = np.mean(train_images[idx][:, :, 0]) for idx in range(len(train_images))\n\n       green_values = np.mean(train_images[idx][:, :, 1]) for idx in range(len(train_images))\n","metadata":{}},{"cell_type":"markdown","source":"## Visualize more sample leaves <a id=\"1.5\"></a>\n\n<font size=3> Now, let us visualize more sample leaves beloning to different categories in the training set.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n\nimport plotly.express as px\nimport numpy as np\n#from skimage import data, filters, img_as_float\nimg = np.array([r,g,b]) #data.camera()\nsigmas = [1, 2, 4]\n#img_sequence = [filters.gaussian(img, sigma=sigma) for sigma in sigmas]\nfig = px.imshow(img, facet_col=0, binary_string=False,\n                labels={'facet_col':'sigma'})\n# Set facet titles\nfor i, sigma in enumerate(sigmas):\n    fig.layout.annotations[i]['text'] = 'sigma = %d' %sigma\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.551991Z","iopub.status.idle":"2021-05-28T03:46:04.552742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nimport numpy as np\n\nimg_rgb = np.array([r,g,b], dtype=np.uint8)\nfig = px.imshow(img_rgb, facet_col=0)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.554746Z","iopub.status.idle":"2021-05-28T03:46:04.555437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\nimg_rgb = [[r], [g, [b]]\nfig = go.Figure(go.Image(z=img_rgb))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.557613Z","iopub.status.idle":"2021-05-28T03:46:04.558316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_leaves(cond=[0, 0, 0, 0], cond_cols=[\"healthy\"], is_cond=True):\n    if not is_cond:\n        cols, rows = 3, min([3, len(train_images)//3])\n        fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(30, rows*20/3))\n        for col in range(cols):\n            for row in range(rows):\n                ax[row, col].imshow(train_images.loc[train_images.index[-row*3-col-1]])\n        return None\n        \n    cond_0 = \"healthy == {}\".format(cond[0])\n    cond_1 = \"scab == {}\".format(cond[1])\n    cond_2 = \"rust == {}\".format(cond[2])\n    cond_3 = \"multiple_diseases == {}\".format(cond[3])\n    \n    cond_list = []\n    for col in cond_cols:\n        if col == \"healthy\":\n            cond_list.append(cond_0)\n        if col == \"scab\":\n            cond_list.append(cond_1)\n        if col == \"rust\":\n            cond_list.append(cond_2)\n        if col == \"multiple_diseases\":\n            cond_list.append(cond_3)\n    \n    data = train_data.loc[0:25]\n    for cond in cond_list:\n        data = data.query(cond)\n        \n    images = train_images.loc[list(data.index)]\n    cols, rows = 3, min([3, len(images)//3])\n    \n    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(30, rows*20/3))\n    for col in range(cols):\n        for row in range(rows):\n            ax[row, col].imshow(images.loc[images.index[row*3+col]])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:53:01.569628Z","iopub.execute_input":"2021-05-28T04:53:01.570237Z","iopub.status.idle":"2021-05-28T04:53:01.588698Z","shell.execute_reply.started":"2021-05-28T04:53:01.570185Z","shell.execute_reply":"2021-05-28T04:53:01.586833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Healthy Leaves?","metadata":{}},{"cell_type":"code","source":"visualize_leaves(cond=[1, 0, 0, 0], cond_cols=[\"healthy\"])","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:53:16.472627Z","iopub.execute_input":"2021-05-28T04:53:16.473159Z","iopub.status.idle":"2021-05-28T04:53:19.659145Z","shell.execute_reply.started":"2021-05-28T04:53:16.473113Z","shell.execute_reply":"2021-05-28T04:53:19.65667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3> We can see that the healthy leaves are completely green, do not have any brown/yellow spots or scars. Healthy leaves do not have scab or rust.","metadata":{}},{"cell_type":"markdown","source":"## Rusted?","metadata":{}},{"cell_type":"markdown","source":"<font size=3> Yes rusted, rust have several brownish-yellow spots across the leaf.","metadata":{}},{"cell_type":"code","source":"visualize_leaves(cond=[0, 0, 1, 0], cond_cols=[\"rust\"])","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.564993Z","iopub.status.idle":"2021-05-28T03:46:04.565694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3> Rust is defined as \"a disease, especially of cereals and other grasses, characterized by rust-colored pustules of spores on the affected leaf blades and sheaths and caused by any of several rust fungi\". The yellow spots are a sign of infection by a special type of fungi called \"rust fungi\". Rust can also be treated with several chemical and non-chemical methods once diagnosed.","metadata":{}},{"cell_type":"markdown","source":"## Pie Chart","metadata":{}},{"cell_type":"code","source":"fig = go.Figure([go.Pie(labels=train_data.columns[1:],\n           values=train_data.iloc[:, 1:].sum().values)])\nfig.update_layout(title_text=\"Pie chart of all categories \", template=\"simple_white\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.567102Z","iopub.status.idle":"2021-05-28T03:46:04.567803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3> Here we visualize the labels and target data. In all the above chart, green represents the \"desired\" or \"healthy\" condition, whearas the others represent the \"undesired\" or \"unhealthy\" conditions. Likewise orange for the sab, dark blue for runs and brown for multimple diseases","metadata":{}},{"cell_type":"markdown","source":"## Scab ?","metadata":{}},{"cell_type":"markdown","source":"<font size=3> Scab have large brown marks and stains across the leaf. ","metadata":{}},{"cell_type":"code","source":"visualize_leaves(cond=[0, 1, 0, 0], cond_cols=[\"scab\"])","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.569509Z","iopub.status.idle":"2021-05-28T03:46:04.570192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3> Scab is defined as \"any of various plant diseases caused by fungi or bacteria and resulting in crustlike spots on fruit, leaves, or roots. The spots caused by such a disease\". The brown marks across the leaf are a sign of these bacterial/fungal infections. \n\nOnce diagnosed, scab can be treated using chemical or non-chemical methods.","metadata":{}},{"cell_type":"markdown","source":"## The target. Is it Healthy? <a id=\"1.6\"><a/>","metadata":{}},{"cell_type":"markdown","source":"<font size=3> It is impossible for a healthy leaf (healthy == 1) to have scab, rust, or multiple diseases. Also, every unhealthy leaf has one of either scab, rust, or multiple diseases.","metadata":{}},{"cell_type":"markdown","source":"<font size=3> In the pie chart above, we saw that most leaves in the training set are unhealthy by 71.7%. Only 5% of plants have multiple diseases, and \"rust\" and \"scab\" occupy approximately 30% of the pie each. \n\nSo, let's dive in to allow convulotional neural networks to help us find and classify healthy and unlhealty leaves. But before that lets see the healthy and unhealhty distribution. ","metadata":{}},{"cell_type":"markdown","source":"## Healthy and Not Healthy Distribution Chart","metadata":{}},{"cell_type":"code","source":"train_data[\"Healthy\"] = train_data[\"healthy\"].apply(bool).apply(str)\nfig = px.histogram(train_data, x=\"Healthy\", title=\"Healthy & Uhnealthy Distribution\", color=\"Healthy\",\\\n            color_discrete_map={\n                \"True\": px.colors.qualitative.Plotly[0],\n                \"False\": px.colors.qualitative.Plotly[1]})\nfig.update_layout(template=\"simple_white\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig.data[1].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[1].marker.line.width = 0.5\nfig","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.571652Z","iopub.status.idle":"2021-05-28T03:46:04.572334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3> We can see the majority or our plants here are unhealthy (healthy == 0). This means that 72% plants in our study are unhealthy and 28% are healthy plants.","metadata":{}},{"cell_type":"markdown","source":"## Image processing - Convolution - <a id=\"2\"></a> [<font size=4>^</font>](#0)","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def edge_and_cut(img):\n    emb_img = img.copy()\n    edges = cv2.Canny(img, 100, 200)\n    edge_coors = []\n    for i in range(edges.shape[0]):\n        for j in range(edges.shape[1]):\n            if edges[i][j] != 0:\n                edge_coors.append((i, j))\n    \n    row_min = edge_coors[np.argsort([coor[0] for coor in edge_coors])[0]][0]\n    row_max = edge_coors[np.argsort([coor[0] for coor in edge_coors])[-1]][0]\n    col_min = edge_coors[np.argsort([coor[1] for coor in edge_coors])[0]][1]\n    col_max = edge_coors[np.argsort([coor[1] for coor in edge_coors])[-1]][1]\n    new_img = img[row_min:row_max, col_min:col_max]\n    \n    emb_img[row_min-10:row_min+10, col_min:col_max] = [255, 0, 0]\n    emb_img[row_max-10:row_max+10, col_min:col_max] = [255, 0, 0]\n    emb_img[row_min:row_max, col_min-10:col_min+10] = [255, 0, 0]\n    emb_img[row_min:row_max, col_max-10:col_max+10] = [255, 0, 0]\n    \n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n    ax[0].imshow(img, cmap='gray')\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[2].imshow(emb_img, cmap='gray')\n    ax[2].set_title('Bounding Box', fontsize=24)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.5739Z","iopub.status.idle":"2021-05-28T03:46:04.574586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is Convolution? <a id=\"2.2\"> </a>\n<font size=3> In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function ( ) that expresses how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it.","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://i.ytimg.com/vi/KTB_OFoAQcc/maxresdefault.jpg\" width=\"500px\"></center>\n\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import HTML\nHTML('<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KTB_OFoAQcc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></center>')","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.576074Z","iopub.status.idle":"2021-05-28T03:46:04.57678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now Let's convolve some leaves","metadata":{}},{"cell_type":"code","source":"def conv(img):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n    kernel = np.ones((7, 7), np.float32)/25\n    conv = cv2.filter2D(img, -1, kernel)\n    ax[0].imshow(img)\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(conv)\n    ax[1].set_title('Convolved Image', fontsize=24)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.578318Z","iopub.status.idle":"2021-05-28T03:46:04.579032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv(train_images[3])\nconv(train_images[4])\nconv(train_images[5])","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.581201Z","iopub.status.idle":"2021-05-28T03:46:04.582081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convolution is a simple algorithm which involves a kernel (a 2D matrix) which moves over the entire image, calculating dot products with each window along the way. The GIF below demonstrates convolution in action.","metadata":{}},{"cell_type":"markdown","source":"## Models <a id=\"3\"></a> - <a id=\"2\"></a> [<font size=4>^</font>](#0)","metadata":{}},{"cell_type":"markdown","source":"### EfficientNet \nEfficientNet is another popular (more recent) CNN-based ImageNet model which achieved the SOTA on several image-based tasks in 2019. EfficientNet performs model scaling in an innovative way to achieve excellent accuracy with significantly fewer parameters. It achieves the same if not greater accuracy than ResNet and DenseNet with a mcuh shallower architecture. Now let us train EfficientNet on leaf images and evaluate its performance.","metadata":{}},{"cell_type":"markdown","source":"# Tensor Set-UP","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync # Batch_Size is for memoery and goes in ranges power of two. eg. 16,32, 64,128..\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:41:15.693503Z","iopub.execute_input":"2021-05-28T04:41:15.694004Z","iopub.status.idle":"2021-05-28T04:41:21.687156Z","shell.execute_reply.started":"2021-05-28T04:41:15.693966Z","shell.execute_reply":"2021-05-28T04:41:21.685641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Assigning labesl and defining path","metadata":{}},{"cell_type":"code","source":"def format_path(st):\n    return GCS_DS_PATH + '/images/' + st + '.jpg'\n\ntest_paths = test_data.image_id.apply(format_path).values\ntrain_paths = train_data.image_id.apply(format_path).values\n\ntrain_labels = np.float32(train_data.loc[:, 'healthy':'scab'].values)\ntrain_paths, valid_paths, train_labels, valid_labels =\\\ntrain_test_split(train_paths, train_labels, test_size=0.15, random_state=2020)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:41:26.655502Z","iopub.execute_input":"2021-05-28T04:41:26.655996Z","iopub.status.idle":"2021-05-28T04:41:26.675212Z","shell.execute_reply.started":"2021-05-28T04:41:26.655938Z","shell.execute_reply":"2021-05-28T04:41:26.67379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Strategy Scope Explanation**","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([efn.EfficientNetB7(input_shape=(512, 512, 3),\n                                                    weights='imagenet',\n                                                    include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n    \n    \n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:41:31.067156Z","iopub.execute_input":"2021-05-28T04:41:31.067533Z","iopub.status.idle":"2021-05-28T04:42:24.6286Z","shell.execute_reply.started":"2021-05-28T04:41:31.0675Z","shell.execute_reply":"2021-05-28T04:42:24.626629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: each Keras Application expects a specific kind of input preprocessing. For EfficientNet, input preprocessing is included as part of the model (as a Rescaling layer), and thus tf.keras.applications.efficientnet.preprocess_input is actually a pass-through function.","metadata":{}},{"cell_type":"markdown","source":"EfficientNet models expect their inputs to be float tensors of pixels with values in the [0-255] range.\n\n### Arguments\n\n#### include_top:\nWhether to include the fully-connected layer at the top of the network. Defaults to True.\n#### weights: \nOne of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. Defaults to 'imagenet'.\n#### input_tensor: \nOptional Keras tensor (i.e. output of layers.Input()) to use as image input for the model.\ninput_shape: Optional shape tuple, only to be specified if include_top is False. It should have exactly 3 inputs channels.\n#### pooling: \nOptional pooling mode for feature extraction when include_top is False. Defaults to None. - None means that the output of the model will be the 4D tensor output of the last convolutional layer. - avg means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D tensor. - max means that global max pooling will be applied.\n#### classes: \nOptional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Defaults to 1000 (number of ImageNet classes).\nclassifier_activation: A str or callable. The activation function to use on the \"top\" layer. Ignored unless include_top=True. Set classifier_activation=None to return the logits of the \"top\" layer. Defaults to 'softmax'. When loading pretrained weights, classifier_activation can only be None or \"softmax\".\n","metadata":{}},{"cell_type":"code","source":"def decode_image(filename, label=None, image_size=(512, 512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:38:38.458907Z","iopub.execute_input":"2021-05-28T04:38:38.459373Z","iopub.status.idle":"2021-05-28T04:38:38.468107Z","shell.execute_reply.started":"2021-05-28T04:38:38.459336Z","shell.execute_reply":"2021-05-28T04:38:38.467088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Set Objects","metadata":{}},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:38:45.976119Z","iopub.execute_input":"2021-05-28T04:38:45.976735Z","iopub.status.idle":"2021-05-28T04:38:46.273281Z","shell.execute_reply.started":"2021-05-28T04:38:45.976677Z","shell.execute_reply":"2021-05-28T04:38:46.272019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utility functions","metadata":{}},{"cell_type":"code","source":"def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n               lr_min=0.00001, lr_rampup_epochs=5, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:38:51.319917Z","iopub.execute_input":"2021-05-28T04:38:51.320378Z","iopub.status.idle":"2021-05-28T04:38:51.328194Z","shell.execute_reply.started":"2021-05-28T04:38:51.320339Z","shell.execute_reply":"2021-05-28T04:38:51.326958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters and Callbacks\n","metadata":{}},{"cell_type":"code","source":"lrfn = build_lrfn()\nSTEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:38:55.416336Z","iopub.execute_input":"2021-05-28T04:38:55.417107Z","iopub.status.idle":"2021-05-28T04:38:55.421953Z","shell.execute_reply.started":"2021-05-28T04:38:55.417064Z","shell.execute_reply":"2021-05-28T04:38:55.420892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training the model","metadata":{}},{"cell_type":"code","source":"history = model.fit(train_data,\n                    epochs=epochs,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T04:38:59.754325Z","iopub.execute_input":"2021-05-28T04:38:59.754956Z","iopub.status.idle":"2021-05-28T04:38:59.898191Z","shell.execute_reply.started":"2021-05-28T04:38:59.754917Z","shell.execute_reply":"2021-05-28T04:38:59.895251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Efficientnet predictions\nNow, I will visualize some sample predictions made by the EfficientNet model. The red bars represent the model's prediction (maximum probability), the green represent the ground truth (label), and the rest of the bars are blue. When the model predicts correctly, the prediction bar is green.","metadata":{}},{"cell_type":"code","source":"def process(img):\n    return cv2.resize(img/255.0, (512, 512)).reshape(-1, 512, 512, 3)\ndef predict(img):\n    return model.layers[2](model.layers[1](model.layers[0](process(img)))).numpy()[0]\n\nfig = make_subplots(rows=4, cols=2)\npreds = predict(train_images[2])\n\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Scab\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Multiple diseases\"\n\ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Healthy\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[2], (205, 136))), row=1, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=1, col=2)\nfig.update_layout(height=1200, width=800, title_text=\"EfficientNet Predictions\", showlegend=False)\n\npreds = predict(train_images[0])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Multiple diseases\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[0], (205, 136))), row=2, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=2, col=2)\n\npreds = predict(train_images[3])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Rust\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[3], (205, 136))), row=3, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=3, col=2)\n\npreds = predict(train_images[1])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Scab\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[1], (205, 136))), row=4, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=4, col=2)\nfig.update_layout(template=\"plotly_white\")","metadata":{"execution":{"iopub.status.busy":"2021-05-28T03:46:04.593049Z","iopub.status.idle":"2021-05-28T03:46:04.594019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3>Models to be used on our R&D to name a few. We plan to use:\n    \n1.DenseNet\n    \n2.EfficientNet\n    \n3.CNNs and Others.","metadata":{}},{"cell_type":"markdown","source":"## Conclusions <a id=\"4\"></a> - <a id=\"2\"></a> [<font size=4>^</font>](#0)","metadata":{}},{"cell_type":"markdown","source":"<font size=3> This is still a quick research in progress. Stay tune for coming up versions. </font>","metadata":{}},{"cell_type":"markdown","source":"# Thank you note <a id=\"5\"></a> - <a id=\"2\"></a> [<font size=4>^</font>](#0)\n\n<font color=\"green\" size=3>Thank you for reading my research and development. Please comment and upvote if you like it to move my learning developent forward. It motivates me to continue my journey to learn more about Data Science for the good of our society :)</font>","metadata":{}},{"cell_type":"markdown","source":"## References <a id=\"6\"></a> - <a id=\"2\"></a> [<font size=4>^</font>](#0)","metadata":{}},{"cell_type":"markdown","source":"Rectified Linear Activation Unit (RELU) https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/\n\nConvolution - Wikipedia https://en.wikipedia.org/wiki/Convolution#:~:text=In%20mathematics%20\n\nKernel Image processing - https://en.wikipedia.org/wiki/Kernel_(image_processing)","metadata":{}},{"cell_type":"markdown","source":"[<font size=4>Back to the table of contents</font>](#0)","metadata":{}}]}