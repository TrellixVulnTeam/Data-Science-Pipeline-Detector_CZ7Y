{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Motivation\n\nIf you look at results of a big machine learning competition, you will most likely find that the top results are achieved by an ensemble of models rather than a single model. For instance, the top-scoring single model architecture at ILSVRC2015 is on place 13. Places 1–12 are taken by various ensembles.\n\nI haven’t seen a tutorial or documentation on how to use multiple neural networks in an ensemble, so I decided to make a practical guide on this topic.\n\n![](https://miro.medium.com/max/1400/1*fy-6esoTWsTutld4fdSyCQ.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport os\nimport seaborn as sns\nimport cv2\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten,  Dropout, BatchNormalization, LeakyReLU,Input\nfrom keras.optimizers import SGD, Adam\nfrom keras.utils import np_utils\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom kaggle_datasets import KaggleDatasets\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TPU Configuration\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \ndef seed_everything(seed=0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 2048\nseed_everything(seed)\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\nAUTO = tf.data.experimental.AUTOTUNE\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configuration\nEPOCHS = 40\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMG_SIZE = 700\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data and perform test train split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef format_path(st):\n    return GCS_DS_PATH + '/images/' + st + '.jpg'\n\ntrain = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/train.csv')\ntest = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/test.csv')\nsub = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv')\n\ntrain_paths = train.image_id.apply(format_path).values\ntest_paths = test.image_id.apply(format_path).values\ntrain_labels = train.loc[:, 'healthy':].values\nSPLIT_VALIDATION =True\nif SPLIT_VALIDATION:\n    train_paths, valid_paths, train_labels, valid_labels =train_test_split(train_paths, train_labels, test_size=0.25, random_state=seed)\n\ndef decode_image(filename, label=None, IMG_SIZE=(IMG_SIZE, IMG_SIZE)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, IMG_SIZE)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\ntf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\ntrain_dataset_1 = (\ntf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(64)\n    .prefetch(AUTO)\n)\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nLR_START = 0.0001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.0001\nLR_RAMPUP_EPOCHS = 4\nLR_SUSTAIN_EPOCHS = 6\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EfficientNet\n\nEfficientNet, which not only focuses on improving the accuracy, but also the efficiency of models.\n\n### What does scaling mean in the context of CNNs?\n\nThere are three scaling dimensions of a CNN: depth, width, and resolution. Depth simply means how deep the networks is which is equivalent to the number of layers in it. Width simply means how wide the network is. One measure of width, for example, is the number of channels in a Conv layer whereas Resolution is simply the image resolution that is being passed to a CNN. The figure below(from the paper itself) will give you a clear idea of what scaling means across different dimensions. We will discuss these in detail as well.\n\n![](https://miro.medium.com/max/1400/1*xQCVt1tFWe7XNWVEmC6hGQ.png)\n\nEfficientNet Architecture\n\nScaling doesn’t change the layer operations, hence it is better to first have a good baseline network and then scale it along different dimensions using the proposed compound scaling. The authors obtained their base network by doing a Neural Architecture Search (NAS) that optimizes for both accuracy and FLOPS. The architecture is similar to M-NASNet as it has been found using the similar search space. The network layers/blocks are as shown below:\n![](https://miro.medium.com/max/1400/1*OpvSpqMP61IO_9cp4mAXnA.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nfrom keras.models import Model\nfrom tensorflow import keras\n!pip install -q efficientnet\nimport efficientnet.tfkeras as efn\n\n\n\nwith strategy.scope():    \n    efficient_net = efn.EfficientNetB7(\n                    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n                    weights='imagenet',\n                    include_top=False\n                    )\n    x = efficient_net.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(4, activation='softmax')(x)\n    model_effnet =  keras.Model(inputs = efficient_net.input,outputs=x)\n    model_effnet.compile(loss=\"categorical_crossentropy\", optimizer= 'adam', metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n\nhistory = model_effnet.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    callbacks=[lr_callback],\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None,\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x = range(0,40), y = history.history['loss'])\nsns.lineplot(x = range(0,40),y = history.history['val_loss'])\nplt.legend(['loss train', 'loss validation'])\nplt.title('Loss evolution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x = range(0,40), y = history.history['accuracy'])\nsns.lineplot(x = range(0,40),y = history.history['val_accuracy'])\nplt.legend(['accuracy train', 'accuracy validation'])\nplt.title('accuracy evolution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhistory.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict= model_effnet.predict(test_dataset)\n\nprediction = np.ndarray(shape = (test.shape[0],4), dtype = np.float32)\n\nif False:\n    for row in range(test.shape[0]):\n        for col in range(4):\n            if predict[row][col] == max(predict[row]):\n                prediction[row][col] = 1\n            else:\n                prediction[row][col] = 0\n\nprediction = pd.DataFrame(prediction)\nprediction.columns = ['healthy', 'multiple_diseases', 'rust', 'scab']\ndf = pd.concat([test.image_id, prediction], axis = 1)\n\n\ndf.to_csv('effi_submission.csv', index = False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Xception\n\nXception is an extension of the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions.\n\n\nThe original depthwise separable convolution is the depthwise convolution followed by a pointwise convolution.\n\n    Depthwise convolution is the channel-wise n×n spatial convolution. Suppose in the figure above, we have 5 channels, then we will have 5 n×n spatial convolution.\n    Pointwise convolution actually is the 1×1 convolution to change the dimension.\n\nCompared with conventional convolution, we do not need to perform convolution across all channels. That means the number of connections are fewer and the model is lighter.\n\n![](https://miro.medium.com/max/1400/1*VvBTMkVRus6bWOqrK1SlLQ.png)\n\n\nThe modified depthwise separable convolution is the pointwise convolution followed by a depthwise convolution. This modification is motivated by the inception module in Inception-v3 that 1×1 convolution is done first before any n×n spatial convolutions. Thus, it is a bit different from the original one. (n=3 here since 3×3 spatial convolutions are used in Inception-v3.)\n\nTwo minor differences:\n\n    The order of operations: As mentioned, the original depthwise separable convolutions as usually implemented (e.g. in TensorFlow) perform first channel-wise spatial convolution and then perform 1×1 convolution whereas the modified depthwise separable convolution perform 1×1 convolution first then channel-wise spatial convolution. This is claimed to be unimportant because when it is used in stacked setting, there are only small differences appeared at the beginning and at the end of all the chained inception modules.\n    \n    The Presence/Absence of Non-Linearity: In the original Inception Module, there is non-linearity after first operation. In Xception, the modified depthwise separable convolution, there is NO intermediate ReLU non-linearity.\n    \n    \n![](https://miro.medium.com/max/1400/1*hOcAEj9QzqgBXcwUzmEvSg.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50V2,ResNet101V2,ResNet152V2,DenseNet201,Xception\nfrom keras.models import Model\nfrom tensorflow import keras\nwith strategy.scope():    \n    Dense_net = Xception(\n                    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n                    weights='imagenet',\n                    include_top=False\n                    )\n    x = Dense_net.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(4, activation='softmax')(x)\n    model_xception =  keras.Model(inputs = Dense_net.input,outputs=x)\n    model_xception.compile(loss=\"categorical_crossentropy\", optimizer= 'adam', metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n\nhistory = model_xception.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    callbacks=[lr_callback],\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict= model_xception.predict(test_dataset)\n\nprediction = np.ndarray(shape = (test.shape[0],4), dtype = np.float32)\nif False:\n    for row in range(test.shape[0]):\n        for col in range(4):\n            if predict[row][col] == max(predict[row]):\n                prediction[row][col] = 1\n            else:\n                prediction[row][col] = 0\n\nprediction = pd.DataFrame(prediction)\nprediction.columns = ['healthy', 'multiple_diseases', 'rust', 'scab']\ndf_dense = pd.concat([test.image_id, prediction], axis = 1)\n\n\ndf_dense.to_csv('dense_submission.csv', index = False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ResNet152\nResNet introduces skip connection (or shortcut connection) to fit the input from the previous layer to the next layer without any modification of the input. Skip connection enables to have deeper network and finally ResNet becomes the Winner of ILSVRC 2015 in image classification, detection, and localization, as well as Winner of MS COCO 2015 detection, and segmentation. This is a 2016 CVPR paper with more than 19000 citations.\n\nSkip / Shortcut Connection in Residual Network (ResNet)\n\nTo solve the problem of vanishing/exploding gradients, a skip / shortcut connection is added to add the input x to the output after few weight layers as below:\n![](https://miro.medium.com/max/894/1*rbhjv7ZdAgXM2MlBUL5Mmw.png)\n\nHence, the output H(x)= F(x) + x. The weight layers actually is to learn a kind of residual mapping: F(x)=H(x)-x.\n\nEven if there is vanishing gradient for the weight layers, we always still have the identity x to transfer back to earlier layers.\nResnet Architecture:\n![](https://miro.medium.com/max/2000/1*6hF97Upuqg_LdsqWY6n_wg.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50V2,ResNet101V2,ResNet152V2,DenseNet201\nfrom keras.models import Model\nfrom tensorflow import keras\nwith strategy.scope():    \n    Res_net = ResNet152V2(\n                    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n                    weights='imagenet',\n                    include_top=False\n                    )\n    x = Res_net.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(4, activation='softmax')(x)\n    model_resnet =  keras.Model(inputs = Res_net.input,outputs=x)\n    model_resnet.compile(loss=\"categorical_crossentropy\", optimizer= 'adam', metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n\nhistory = model_resnet.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    callbacks=[lr_callback],\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict= model_resnet.predict(test_dataset)\n\nprediction = np.ndarray(shape = (test.shape[0],4), dtype = np.float32)\nif False:\n    for row in range(test.shape[0]):\n        for col in range(4):\n            if predict[row][col] == max(predict[row]):\n                prediction[row][col] = 1\n            else:\n                prediction[row][col] = 0\n\nprediction = pd.DataFrame(prediction)\nprediction.columns = ['healthy', 'multiple_diseases', 'rust', 'scab']\ndf_res = pd.concat([test.image_id, prediction], axis = 1)\n\n\ndf_res.to_csv('res_submission.csv', index = False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Process","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def voting(a,b,c):\n    if a==b:\n        return a\n    if b==c:\n        return c\n    if a==c:\n        return a\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_id = df['image_id']\nhealthy = []\nmultiple_diseases = []\nrust = []\nscab = []\nfor i in range(len(df['healthy'])):\n    healthy.append(voting(df['healthy'][i],df_dense['healthy'][i],df_res['healthy'][i]))\n    multiple_diseases.append(voting(df['multiple_diseases'][i],df_dense['multiple_diseases'][i],df_res['multiple_diseases'][i]))\n    rust.append(voting(df['rust'][i],df_dense['rust'][i],df_res['rust'][i]))\n    scab.append(voting(df['scab'][i],df_dense['scab'][i],df_res['scab'][i]))\n    \nfinalsubmission = pd.DataFrame(columns = ['image_id','healthy', 'multiple_diseases', 'rust', 'scab'])\n\nfinalsubmission['image_id'] = image_id\nfinalsubmission['healthy'] = healthy\nfinalsubmission['multiple_diseases'] = multiple_diseases\nfinalsubmission['rust'] = rust\nfinalsubmission['scab'] = scab\nfinalsubmission.to_csv('submission.csv', index = False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VKO Ensemble Process","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cv =\n\nX_train_cv, X_validation_cv, y_train_cv, y_validation_cv = train_test_split\n\nX_test = \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel_stacked = XGBClassifier(n_jobs=-1)\n\nparams = {'max_depth' : [5,10,20],\n         'booster' : 'gblinear', 'gbtree', 'dart',\n         }\n\ncv = GridSearchCV(estimator = model_stacked,\n                 param_grid = params,\n                 n_jobs=-1,\n                 scoring = 'accuracy',\n                 cv=5, #stratified\n                 )\n\ncv.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}