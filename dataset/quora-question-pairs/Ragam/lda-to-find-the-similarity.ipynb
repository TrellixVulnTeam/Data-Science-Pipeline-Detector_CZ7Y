{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"08a048c4-d7d4-0228-8c99-d59c13061555"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"054a80f2-9810-685e-7a27-161b8cf07833"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndf = pd.read_csv(\"../input/train.csv\",nrows=200)\n#df = pd.read_csv(\"../input/train.csv\",nrows = 20000)\ndf = df.dropna()\n\n#from subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"831f88cc-6290-2bbd-3d17-9f55c7efaf4e"},"source":"Document term matrix"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b81fb526-5f39-bac7-b3e8-d0ddc5ddedd7"},"outputs":[],"source":"import re\nimport gensim\nfrom gensim import corpora\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nwords = re.compile(r\"\\w+\",re.I)\nstopword = stopwords.words('english')\n#stemmer = PorterStemmer()\n\ndef tokenize(df):\n    q1 = []\n    q2 = []\n    for q in df.question1.tolist():\n        q1.append([(i.lower()) for i in words.findall(q) if i not in stopword])\n    for q in df.question2.tolist():\n        q2.append([(i.lower()) for i in words.findall(q) if i not in stopword])\n    df[\"q1_tokens\"] = q1\n    df[\"q2_tokens\"] = q2\n    return df\n\ndef train_dictionary(df):\n    q_tokens = df.q1_tokens.tolist() + df.q2_tokens.tolist()\n    dictionary = corpora.Dictionary(q_tokens)\n    return dictionary\n    \ndf = tokenize(df)\ndictionary = train_dictionary(df)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"96cceff5-f0ac-1069-c485-6fc281fd0185"},"outputs":[],"source":"def get_vectors(df, dictionary):\n    \n    question1_vec = [dictionary.doc2bow(text) for text in df.q1_tokens.tolist()]\n    question2_vec = [dictionary.doc2bow(text) for text in df.q2_tokens.tolist()]\n    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n    return question1_csc.transpose(),question2_csc.transpose()\nq1_csc, q2_csc = get_vectors(df, dictionary)\n\nprint (q1_csc.shape)\nprint (q2_csc.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c902d1c6-3899-69ec-ca08-66d1d73a036b"},"outputs":[],"source":"from sklearn.metrics.pairwise import cosine_similarity as cs\nfrom sklearn.metrics.pairwise import manhattan_distances as md\nfrom sklearn.metrics.pairwise import euclidean_distances as ed\nfrom sklearn.metrics import jaccard_similarity_score as jsc\nfrom sklearn.neighbors import DistanceMetric\n\nminkowski_dis = DistanceMetric.get_metric('minkowski')\n\ndef get_similarity_values(q1_csc, q2_csc):\n    cosine_sim = []\n    manhattan_dis = []\n    eucledian_dis = []\n    jaccard_dis = []\n    minkowsk_dis = []\n    \n    for i,j in zip(q1_csc, q2_csc):\n        sim = cs(i,j)\n        cosine_sim.append(sim[0][0])\n        sim = md(i,j)\n        manhattan_dis.append(sim[0][0])\n        sim = ed(i,j)\n        eucledian_dis.append(sim[0][0])\n        x = i.toarray()\n        y = j.toarray()\n        try:\n            sim = jsc(x,y)\n            jaccard_dis.append(sim)\n        except:\n            jaccard_dis.append(0)\n            \n        sim = minkowski_dis.pairwise(x,y)\n        minkowsk_dis.append(sim[0][0])\n    \n    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n\ncosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(q1_csc, q2_csc)\n\ndf[\"cosine\"] = cosine_sim\ndf[\"manhattan\"] = manhattan_dis\ndf[\"eucledian\"] = jaccard_dis\ndf[\"minkowsk\"] = minkowsk_dis\ndf[\"jaccard\"] = jaccard_dis\n\nprint(df.head())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8ce24f5-166a-82ac-7170-d9b25a888ba4"},"outputs":[],"source":"from sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nreg = linear_model.LogisticRegression()\nsvc = SVC()\ndt = tree.DecisionTreeClassifier()\ndf_train, df_test = train_test_split(df, test_size = 0.3)\nytrain = df_train[\"is_duplicate\"]\nytest = df_test[\"is_duplicate\"]\n#print(df_train.head())\nxtrain = df_train.ix[:,'cosine':]\nxtest = df_test.ix[:,'cosine':]\n#print(xtrain.head())\nlr_model=reg.fit(np.array(xtrain), np.array(ytrain))\nsvc_model = svc.fit(np.array(xtrain),np.array(ytrain))\ndt_model = dt.fit(np.array(xtrain),np.array(ytrain))\n\nYpred_lr=lr_model.predict(np.array(xtest))\nYpred_svc = svc_model.predict(np.array(xtest))\nYpred_dt = dt_model.predict(np.array(xtest))\n\naccuracy_lr=accuracy_score(ytest, Ypred_lr)\naccuracy_svc = accuracy_score(ytest,Ypred_svc)\naccuracy_dt = accuracy_score(ytest,Ypred_dt)\n\nprint(\"Accuracy of logistic model\",accuracy_lr)\nprint(\"Accuracy of SVM model\", accuracy_svc)\nprint(\"Accuracy of decision tree classifier model\", accuracy_dt)\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}