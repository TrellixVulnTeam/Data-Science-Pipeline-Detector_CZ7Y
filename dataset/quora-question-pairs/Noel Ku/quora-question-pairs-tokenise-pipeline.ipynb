{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, collections, random, itertools\n\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-14T07:34:24.210448Z","iopub.execute_input":"2021-07-14T07:34:24.211577Z","iopub.status.idle":"2021-07-14T07:34:24.243762Z","shell.execute_reply.started":"2021-07-14T07:34:24.211465Z","shell.execute_reply":"2021-07-14T07:34:24.242461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data\ndf = pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv.zip\")\ndf[\"question1\"] = df[\"question1\"].astype(str)  # resolve nan\ndf[\"question2\"] = df[\"question2\"].astype(str)\ndf[\"qid1\"] -= 1  #  index\ndf[\"qid2\"] -= 1","metadata":{"execution":{"iopub.status.busy":"2021-07-14T07:34:24.258249Z","iopub.execute_input":"2021-07-14T07:34:24.259023Z","iopub.status.idle":"2021-07-14T07:34:26.027068Z","shell.execute_reply.started":"2021-07-14T07:34:24.258973Z","shell.execute_reply":"2021-07-14T07:34:26.026035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all questions are identified with its qid\nqid_to_question = {}\nfor qid1, qid2, question1, question2 in zip(df[\"qid1\"], df[\"qid2\"], df[\"question1\"], df[\"question2\"]):\n    qid_to_question[qid1] = question1\n    qid_to_question[qid2] = question2\nquestions_by_idx = [qid_to_question[qid] for qid in range(max(qid_to_question) + 1)]\nassert len(questions_by_idx) == len(qid_to_question)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T07:34:26.028593Z","iopub.execute_input":"2021-07-14T07:34:26.029063Z","iopub.status.idle":"2021-07-14T07:34:26.686799Z","shell.execute_reply.started":"2021-07-14T07:34:26.029016Z","shell.execute_reply":"2021-07-14T07:34:26.685641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Obtain tokenised and spell checked questions as token list","metadata":{}},{"cell_type":"markdown","source":"## spaCy Tokeniser","metadata":{}},{"cell_type":"code","source":"!pip install -U spacy==2.3.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom spacy.tokenizer import Tokenizer # https://spacy.io/api/tokenizer\n\n!python3 -m spacy download en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\nprint(\"Loaded en_core_web_sm\")\n# from spacy.lang.en import English\n# nlp = English()\n\ntokenizer = Tokenizer(nlp.vocab)\ntokenizer.add_special_case(\"[math]\", [{\"ORTH\": \"[math]\"}]) # see qid=7: '[math]23^{24}[/math]' becomes one token\n# add more special cases here if found\n\ndef tokenise(text, lower=False, split_last_punc=True):\n    \"\"\"\n    returns a list of tokens given a question text\n    note: each punctuation is also considered a token\n    note: \"\\n\" is a token\n    note: \"'s\" is a token\n    note: '(Koh-i-Noor)' is a token\n    \n    see tokenizer instantiation code for special cases or to add\n    \n    lowercase text only after spell check\n    \"\"\"\n    if lower: text = text.lower()\n    tokens = tokenizer(text)\n    token_list = [token.text for token in tokens]\n\n    # further split tokens that end with certain punct e.g. \"me?\" => \"me\", \"?\"\n    if split_last_punc: \n        split_lists = [[token[:-1], token[-1]] if (token[-1] in [\"!\",\"?\",\",\",\":\"]) else [token] for token in token_list]\n        token_list = [token for sublist in split_lists for token in sublist]\n    return token_list","metadata":{"execution":{"iopub.status.busy":"2021-07-14T07:35:13.686108Z","iopub.execute_input":"2021-07-14T07:35:13.686501Z","iopub.status.idle":"2021-07-14T07:35:31.639479Z","shell.execute_reply.started":"2021-07-14T07:35:13.686468Z","shell.execute_reply":"2021-07-14T07:35:31.63831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SymSpell Spell Checker","metadata":{}},{"cell_type":"code","source":"!pip install symspellpy\nfrom symspellpy.symspellpy import SymSpell, Verbosity  # https://github.com/mammothb/symspellpy\nimport pkg_resources\n\n# instantiate spellchecker\nsym = SymSpell(max_dictionary_edit_distance=2, prefix_length=7, count_threshold=1)\n# https://symspellpy.readthedocs.io/en/latest/api/symspellpy.html\ndictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\nsym.load_dictionary(dictionary_path, 0, 1) # might take a short while","metadata":{"execution":{"iopub.status.busy":"2021-07-14T07:35:57.176781Z","iopub.execute_input":"2021-07-14T07:35:57.177265Z","iopub.status.idle":"2021-07-14T07:36:09.37167Z","shell.execute_reply.started":"2021-07-14T07:35:57.17723Z","shell.execute_reply":"2021-07-14T07:36:09.370525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spell checker types\nChoice of which spell checker to use - single or compound - depends on pipeline. Both have tradeoffs. I prefer `spellcheck_single` after including additional rules to make it more robust.","metadata":{}},{"cell_type":"code","source":"def spellcheck_single(word):\n    # returns top correct spelling or the same word if no correction found within max_edit_distance\n    \n    # handle non ascii case\n    if not word.isascii(): return word # do not spellcheck non ascii words e.g. シ\n    \n    # obtain list of suggestions\n    suggestions = sym.lookup(word, Verbosity.CLOSEST, max_edit_distance=2,\n        include_unknown=True, # a mispelled word with no found corrections is returned as is\n        ignore_token=r\"[:,.!?\\\\-]\" # use if want to avoid correcting certain phrases\n        )\n    # get the term from the suggestItem object\n    suggested_words = [suggestion._term for suggestion in suggestions]\n    \n    # check if the input word is legit and return if so else return corrected word\n    word_lower = word.lower()\n    if word_lower in suggested_words: return word_lower # do not correct if input is a legit word\n    else: return suggested_words[0] # top suggestion\n\ndef spellcheck_compound(sent):\n    # spellchecks a sentence\n    suggestions = sym.lookup_compound(sent, max_edit_distance=2)\n    return suggestions[0]._term # returns the top suggestion","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check behaviour of spell checker\nassert spellcheck_single(\"What\") == \"what\", \"Common word should be preserved\"\nassert spellcheck_single(\"DNS\") == \"DNS\", f\"Abbreviations should be preserved\" \nassert spellcheck_single(\"シ\") == \"シ\", f\"Non ascii is preserved\" \nassert spellcheck_single(\"?![].,\") == \"?![].,\", f\"Punctuation preserved\" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spell checker & tokeniser pipelines\n1. Spellcheck compound then tokenise\n2. Tokenise then spellcheck each word\n\nSee last section for comparison of pipelines","metadata":{}},{"cell_type":"code","source":"# Pipelines\n# always lower only after spell check is done\ndef spellcheck_then_tokenise(sent):\n    checked_sent = spellcheck_compound(sent)\n    tokens = tokenise(checked_sent, lower=True) # lower after spell check\n    return tokens\n\ndef tokenise_then_spellcheck(sent):\n    # 8 times faster than spellcheck_then_tokenise\n    tokens = tokenise(sent)\n    checked_tokens = [spellcheck_single(token).lower() for token in tokens] # lower after spell check\n    return checked_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert dataset\n1. `qid_to_processed_token_list`\n2. `token_to_qid`\n2. `qid_to_vec` using spacy built-in model","metadata":{}},{"cell_type":"code","source":"# Process the full set with variable tokenise_pipeline_func\nimport time\ndef pe(start,end,num_iter=1):\n    print(f\"Duration: {end-start:.5f}s Time/Iter: {(end-start)/num_iter:.5f}\")\n\nimport pickle\n\ndef convert_dataset(tokenise_pipeline_func, dir=\"/kaggle/working\"):\n    ## qid_to_token\n    qid_to_processed_token_list = {}\n    start = time.time()\n    for qid, question in tqdm(enumerate(questions_by_idx)):\n        qid_to_processed_token_list[qid] = tokenise_pipeline_func(question)\n    end = time.time()\n    pe(start,end, num_iter = len(questions_by_idx))\n\n    fn = f\"{dir}/qid_to_processed_token_list_{tokenise_pipeline_func.__name__}.pkl\"\n    with open(fn, \"wb\") as f:\n        pickle.dump(qid_to_processed_token_list, f)\n\n    ## token_to_qid\n    token_to_qid = {}\n    start = time.time()\n    for qid, token_list in tqdm(qid_to_processed_token_list.items()):\n        for token in token_list:\n            if token in token_to_qid.keys(): token_to_qid[token].append(qid) # append to existing list given existing token\n            else: token_to_qid[token] = [qid] # start new list given new token\n    end = time.time()\n    pe(start,end, num_iter = len(questions_by_idx))\n\n    fn = f\"{dir}/token_to_qid_{tokenise_pipeline_func.__name__}.pkl\"\n    with open(fn, \"wb\") as f:\n        pickle.dump(token_to_qid, f)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Convert using the 3 pipelines\nconvert_dataset(tokenise_then_spellcheck, dir=\".\") # 33 mins\nconvert_dataset(spellcheck_then_tokenise, dir=\".\") # 58 mins\nconvert_dataset(tokenise, dir=\".\") # 25 sec","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load pkls\nfn = \"/kaggle/working/qid_to_processed_token_list_tokenise_then_spellcheck.pkl\"\nwith open(fn, \"rb\") as f:\n    qid_to_token = pickle.load(f)\n\nfn = \"/kaggle/working/token_to_qid_tokenise_then_spellcheck.pkl\"\nwith open(fn, \"rb\") as f:\n    token_to_qid = pickle.load(f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example\nqid_to_token[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_vec(token_or_list):\n    # converts a token string or a list of tokens into a word or doc vec respectively\n    if type(token_or_list) == list:\n        # token list needs to be joined into a sentence first\n        token_or_list = ' '.join(token_or_list)\n    return nlp(token_or_list).vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qid_to_vec = {}\nfor qid, token_list in tqdm(qid_to_token.items()):\n    qid_to_vec[qid] = to_vec(token_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save\nimport pickle\nwith open(\"/kaggle/working/qid_to_vec.pkl\", \"wb\") as f:\n    pickle.dump(qid_to_vec, f)\n\n# how to load\n# with open(\"/kaggle/working/qid_to_vec.pkl\", \"rb\") as f:\n#     qid_to_vec = pickle.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Different spacy model\n!python3 -m spacy download en_core_web_lg\nnlp2 = spacy.load(\"en_core_web_lg\")\nprint(\"Loaded en_core_web_lg\")\ndef to_vec2(token_or_list):\n    # converts a token string or a list of tokens into a word or doc vec respectively\n    if type(token_or_list) == list:\n        # token list needs to be joined into a sentence first\n        token_or_list = ' '.join(token_or_list)\n    return nlp2(token_or_list).vector\n\nqid_to_vec = {}\nfor qid, token_list in tqdm(qid_to_token.items()):\n    qid_to_vec[qid] = to_vec2(token_list)\n\nwith open(\"/kaggle/working/qid_to_vec_trf.pkl\", \"wb\") as f:\n    pickle.dump(qid_to_vec, f)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T07:36:09.625267Z","iopub.execute_input":"2021-07-14T07:36:09.625664Z","iopub.status.idle":"2021-07-14T07:37:11.048253Z","shell.execute_reply.started":"2021-07-14T07:36:09.625633Z","shell.execute_reply":"2021-07-14T07:37:11.04589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compare performance of pipelines","metadata":{}},{"cell_type":"code","source":"# example_qids = [7, 10, 16, 84, 94]\n\n# just_tokenise = []\n# start=time.time()\n# for qid in tqdm(example_qids):\n#     text = qid_to_question[qid]\n#     tokens = tokenise(text, lower=True)\n#     just_tokenise.append(tokens)\n# end=time.time()\n# pe(start,end,num_iter=len(example_qids))\n    \n# sc_then_t = []\n# start=time.time()\n# for qid in tqdm(example_qids):\n#     text = qid_to_question[qid]\n#     sc_then_t.append(spellcheck_then_tokenise(text))\n# end=time.time()\n# pe(start,end,num_iter=len(example_qids))\n# dur1 = end-start\n\n# t_then_sc = []\n# start=time.time()\n# for qid in tqdm(example_qids):\n#     text = qid_to_question[qid]\n#     t_then_sc.append(tokenise_then_spellcheck(text))\n# end=time.time()\n# pe(start,end,num_iter=len(example_qids))\n# dur2 = end-start\n\n# print(f\"\\nspellcheck_compound then tokenise takes {dur1/dur2:.3f}x longer.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Compare token by token\n# from itertools import zip_longest\n\n# w=30\n# for qid, (l0, l1, l2) in enumerate(zip(just_tokenise,sc_then_t,t_then_sc)):\n#     print(\"\\n\")\n#     print(\"qid: \",qid)\n#     print('{}{}{}'.format(\"T\".ljust(w),\"SC->T\".ljust(w),\"T->SC\"))\n#     for t0, t1, t2 in zip_longest(l0, l1,l2, fillvalue = \" \"):\n#         print('{}{}{}'.format(t0.ljust(w),t1.ljust(w),t2))\n#     if qid == 10: break","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}