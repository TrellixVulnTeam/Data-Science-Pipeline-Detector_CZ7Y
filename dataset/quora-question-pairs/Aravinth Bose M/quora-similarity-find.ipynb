{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The data contains a pair of paragraphs. These text paragraphs are randomly sampled from a raw dataset. Each pair of the sentence may or may not be semantically similar. The dataset considered for this project does not contain any labels. Given below is the solution to the unsupervised machine learning problem","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport re\nimport scipy\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import FastText","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-06T12:35:48.095379Z","iopub.execute_input":"2022-01-06T12:35:48.096487Z","iopub.status.idle":"2022-01-06T12:35:50.025278Z","shell.execute_reply.started":"2022-01-06T12:35:48.096417Z","shell.execute_reply":"2022-01-06T12:35:50.023508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data =  pd.read_csv(\"../input/retail-questions/Questions.csv\", sep='\\t')\nprint(data.head())\nsentences = data[\"Questions\"]\nprint(sentences)\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-01-06T12:35:50.028704Z","iopub.execute_input":"2022-01-06T12:35:50.029584Z","iopub.status.idle":"2022-01-06T12:35:50.074954Z","shell.execute_reply.started":"2022-01-06T12:35:50.029509Z","shell.execute_reply":"2022-01-06T12:35:50.07373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing of Data\n*Removing Special Char, stopwords&lemmitizing*","metadata":{}},{"cell_type":"code","source":"lemmatizer=WordNetLemmatizer()\nstop_words=stopwords.words(\"english\")\n\ndef remove(sent_r):\n    \n    #conver all letters to lower case\n    sent_r=sent_r.lower()\n    \n    #remove special character \n    no_spec=re.sub(\"[^A-Za-z0-9 \" \"]\",\"\",sent_r)\n    \n    temp=' '\n    #lemmitizing and removing stopwords\n    temp=temp.join([lemmatizer.lemmatize(i) for i in no_spec.split(\" \")])\n    l1=temp.split(\" \")\n \n    #removing extra spaces\n    while(\"\" in l1):\n        l1.remove(\"\")\n    return l1\n\n#a=remove(\"Ra^*vic&h&&[andar] () ram &((*&^))scores [Goo##d] runs\")\n#print(a)\n#output ['ravichandar', 'ram', 'score', 'good', 'run']","metadata":{"execution":{"iopub.status.busy":"2022-01-06T12:35:50.077828Z","iopub.execute_input":"2022-01-06T12:35:50.078243Z","iopub.status.idle":"2022-01-06T12:35:50.094075Z","shell.execute_reply.started":"2022-01-06T12:35:50.078177Z","shell.execute_reply":"2022-01-06T12:35:50.092746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FastText\nFastText is an extension to Word2Vec proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). For instance, the tri-grams for the word apple is app, ppl, and ple (ignoring the starting and ending of boundaries of words). The word embedding vector for apple will be the sum of all these n-grams. After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words.\n","metadata":{}},{"cell_type":"code","source":"model = FastText('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-06T12:35:50.09672Z","iopub.execute_input":"2022-01-06T12:35:50.097285Z","iopub.status.idle":"2022-01-06T12:35:58.886988Z","shell.execute_reply.started":"2022-01-06T12:35:50.097207Z","shell.execute_reply":"2022-01-06T12:35:58.885952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vector calculation","metadata":{}},{"cell_type":"code","source":"def vect(s):\n    return np.mean([model[word] for word in s],axis=0)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-06T12:35:58.889249Z","iopub.execute_input":"2022-01-06T12:35:58.889599Z","iopub.status.idle":"2022-01-06T12:35:58.895142Z","shell.execute_reply.started":"2022-01-06T12:35:58.889546Z","shell.execute_reply":"2022-01-06T12:35:58.894302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating vector for each sentence in dataframe and saving it in a new column","metadata":{}},{"cell_type":"code","source":"vect_frame=[]\nfor i in sentences:\n    #cleaning and preprocessing\n    list_clean=remove(i)\n    \n    #coverts sent to vec\n    vector1=vect(list_clean)\n    vect_frame.append(vector1)\n    \n#store vectors in data frame\ndata[\"vectors\"]=vect_frame","metadata":{"execution":{"iopub.status.busy":"2022-01-06T12:35:58.896533Z","iopub.execute_input":"2022-01-06T12:35:58.897012Z","iopub.status.idle":"2022-01-06T12:36:01.154002Z","shell.execute_reply.started":"2022-01-06T12:35:58.896909Z","shell.execute_reply":"2022-01-06T12:36:01.151318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cosine Similarity\nCosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0° is 1, and it is less than 1 for any angle in the interval (0, π] radians.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Calculating cosine similarity for all Vectors in a dataframe and saving it in new column","metadata":{}},{"cell_type":"code","source":"def CompareBoth(vector_single_sentence):\n    result=[]\n    for i in range(len(data[\"vectors\"])):\n        cosine = scipy.spatial.distance.cosine(vector_single_sentence, data[\"vectors\"][i])\n        result.append((1-cosine)*100)\n    \n    #save all results to the dataframe\n    data[\"results\"]=result","metadata":{"execution":{"iopub.status.busy":"2022-01-06T12:36:01.155726Z","iopub.execute_input":"2022-01-06T12:36:01.156167Z","iopub.status.idle":"2022-01-06T12:36:01.162944Z","shell.execute_reply.started":"2022-01-06T12:36:01.156104Z","shell.execute_reply":"2022-01-06T12:36:01.161735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting a sentence from user and displaying Top 5 similar sentences","metadata":{}},{"cell_type":"code","source":"a=input(\"Enter a Text: \") #gets a sentence from user\nprint(\"\\n\")\nlist_clean=remove(a) #cleaning and preprocessing\nprint(list_clean)\nvector_single_sentence=vect(list_clean) # covert to vec\ntop=CompareBoth(vector_single_sentence)\n\n#Printing Top 5 similar sentences\naa=data['results'].nlargest(n=5)\noffset=0\n\nfor i in aa.keys():\n    if data[\"results\"][i]>=offset:\n        print(data[\"Questions\"][i],' ',data[\"results\"][i])","metadata":{"execution":{"iopub.status.busy":"2022-01-06T18:42:34.665494Z","iopub.execute_input":"2022-01-06T18:42:34.666168Z"},"trusted":true},"execution_count":null,"outputs":[]}]}