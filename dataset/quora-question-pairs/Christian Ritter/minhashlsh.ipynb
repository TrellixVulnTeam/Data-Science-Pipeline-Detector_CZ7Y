{"cells":[{"metadata":{},"cell_type":"markdown","source":"# MinHash & LSH Application\n\n* Based on https://medium.com/@bassimfaizal/finding-duplicate-questions-using-datasketch-2ae1f3d8bc5c"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport nltk\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Data Extraction"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"qa_pairs = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Meet and Greet data"},{"metadata":{"trusted":true},"cell_type":"code","source":"qa_pairs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qa_pairs.sample(10, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qa_pairs.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qa_pairs[~qa_pairs['question1'].apply(lambda question: type(question) == str)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qa_pairs = qa_pairs.dropna(subset=['question1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Univariant Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"qa_pairs['is_duplicate'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(qa_pairs['is_duplicate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_len_q1 = qa_pairs['question1'].apply(lambda question: len(set(question)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,4))\nsns.distplot(token_len_q1, ax=ax)\nax.set_xlabel('token lenght per question')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(token_len_q1)\nplt.xlabel('token lenght per question')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: MinHast and LSH "},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install datasketch[scipy]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datasketch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qa_pairs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sents_pairs = pd.concat([qa_pairs[qa_pairs['is_duplicate'] == 0].sample(100, random_state=42), \n                   qa_pairs[qa_pairs['is_duplicate'] == 1].sample(100, random_state=42)]).reset_index(drop=True).sample(frac=1.)\nsents_pairs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sents = pd.concat([sents_pairs['question1'], sents_pairs['question2']])\nsents.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set Representation\n\n'''\nset_dict maps question id (eg 'm23') to set representation of questionnorm_dict maps question id (eg 'm23') to actual question.\n#May use this dictionary to #evaluate results of LSH output.We loop through each question, convert them into shingles, \nand if the shingle isnâ€™t a stop word,\nwe add them to a hashset which will be the value for the set_dict dictionary.\n'''\n\n\nset_dict={} \n\nnorm_dict={} \ncount=1\nfor question in tqdm([x for x in sents]):\n   temp_list = []\n   for shingle in question.split(' '):\n       if shingle not in stop_words:\n           temp_list.append(shingle.lower())\n   set_dict[\"m{0}\".format(count)] = set(temp_list)\n   norm_dict[\"m{0}\".format(count)] = question\n   count +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_dict['m1']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MinHashing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create minHash signatures\n\n'''\nnum_perm is the number of permutations we want for the MinHash algorithm (discussed before). \nThe higher the permutations the longer the runtime.Min_dict maps question id (eg 'm23') to min hash signatures.\nWe loop through all the set representations of questions and calculate the signatures and store them in the min_dict dictionary.\n'''\n\n\nnum_perm = 256\nmin_dict = {}\ncount2 = 1\nfor val in tqdm(set_dict.values()):\n   m = datasketch.MinHash(num_perm=num_perm)\n   for shingle in val:\n       m.update(shingle.encode('utf8'))\n   min_dict[\"m{}\".format(count2)] = m\n   count2+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MinHash data structure:"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_dict['m1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elem_test = next(iter(set_dict['m1']))\nelem_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are two minhash structures the same despite being initialized as different objects?"},{"metadata":{"trusted":true},"cell_type":"code","source":"m1 = datasketch.MinHash(num_perm=num_perm)\nm1.update(elem_test.encode('utf8'))\nm2 = datasketch.MinHash(num_perm=num_perm)\nm2.update(elem_test.encode('utf8'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m1 == m2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m1.jaccard(m2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes."},{"metadata":{},"cell_type":"markdown","source":"Get hash values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_digest = m1.digest()\nfirst_digest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_digest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_text = iter(set_dict['m1'])\nnext(iter_text)\nelem_test2 = next(iter_text)\nelem_test2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m1.update(elem_test2.encode('utf8'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"internal hash values have updated based on new shingle added."},{"metadata":{"trusted":true},"cell_type":"code","source":"(m1.digest() == first_digest).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(set_dict['m1']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSH"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create LSH index\n\n'''\nWe set the Jaccard similarity threshold as a parameter in MinHashLSH. \nWe loop through the signatures or keys in the min_dict dictionary and store them as bands (as described in the theory section of the article). \nDatasketch stores these in a dictionary format, where the key is a question and the values are all the questions deemed similar based on the threshold. \nBut we need them in candidate pairs as they are much easier to evaluate, so we use a function called create_cand_pairs \nwhich simply changes the format of the dictionary to be a list of lists with each sub-list being a candidate pair.\n'''\n\nlsh = datasketch.MinHashLSH(threshold=0.4, num_perm=num_perm)\nfor key in tqdm(min_dict.keys()):\n   lsh.insert(key,min_dict[key]) # insert minhash data structure","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lsh.query: Giving the MinHash of the query set, retrieve the keys (m1, m2 etc.) that references sets with approximate! Jaccard similarities greater than the threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"big_list = []\nfor query in min_dict.keys():\n   big_list.append(lsh.query(min_dict[query]))\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"big_list[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"big_list[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_dict[big_list[5][0]], norm_dict[big_list[5][1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}