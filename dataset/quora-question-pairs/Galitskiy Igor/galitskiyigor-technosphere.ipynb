{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53dd969a-93cb-268d-133b-06b15be551ec"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6325114f-96d6-2ceb-31a0-2ba9787669aa"},"outputs":[],"source":"\nimport os\nimport gc\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\n\nfrom sklearn import model_selection\nfrom sklearn import linear_model\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f05b349-e47a-a399-eaf8-0d111f883242"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6e5e731-ffa8-5dff-68f2-f689765f548c"},"outputs":[],"source":"df_test = pd.read_csv('../input/test.csv')\ndf_test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2ba0ea1-7a0a-c101-5a57-78924e2c573f"},"outputs":[],"source":"# крутая штука)) самые популярные слова в тестовой треин\n\ntrain_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n\nfrom wordcloud import WordCloud\ncloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')"},{"cell_type":"markdown","metadata":{"_cell_guid":"0f7a9862-14c0-b523-5652-80b8cc0af2e8"},"source":"Семантический анализ"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee9615c7-5a29-367d-8f7e-39e73bb70088"},"outputs":[],"source":"# Количество различных штук в тексте\nqmarks = np.mean(train_qs.apply(lambda x: '?' in x))\nmath = np.mean(train_qs.apply(lambda x: '[math]' in x))\nfullstop = np.mean(train_qs.apply(lambda x: '.' in x))\ncapital_first = np.mean(train_qs.apply(lambda x: x[0].isupper()))\ncapitals = np.mean(train_qs.apply(lambda x: max([y.isupper() for y in x])))\nnumbers = np.mean(train_qs.apply(lambda x: max([y.isdigit() for y in x])))\n\nprint('Вопросов с вопросительным знаком: {:.2f}%'.format(qmarks * 100))\nprint('Вопросов про математику: {:.2f}%'.format(math * 100))\nprint('Вопросов с точками: {:.2f}%'.format(fullstop * 100))\nprint('Вопросов в которых первая буква заглавная: {:.2f}%'.format(capital_first * 100))\nprint('Вопросов с заглавными буквами: {:.2f}%'.format(capitals * 100))\nprint('Вопросов с цифирками: {:.2f}%'.format(numbers * 100))"},{"cell_type":"markdown","metadata":{"_cell_guid":"34b1dc29-a79c-95cb-3b7c-614036cbb606"},"source":"Очень интересно, возможно в пригодится в дальнейшем)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"354569a5-0fde-3be9-6341-63fd1cc11580"},"outputs":[],"source":"# смотрим количество общих слов в вопросах и как от этого зависит их похожесть\n\nfrom nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n         # вопросы которые сгенерировал компутер\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R\n\nplt.figure(figsize=(15, 5))\ntrain_word_match = df_train.apply(word_match_share, axis=1, raw=True)\nplt.hist(train_word_match[df_train['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(train_word_match[df_train['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)"},{"cell_type":"markdown","metadata":{"_cell_guid":"66d0bb05-872e-653a-1234-af1bb37eef91"},"source":"Очень интересная зависимость, по ней можно увидеть что если общих слов почти нет - то предложения точно не похожи, а вот если даже много общих слов - то не факт что похожи"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d86a1ff0-5560-3276-4d88-e59511526994"},"outputs":[],"source":"df_train = df_train.dropna(how=\"any\").reset_index(drop=True)\n\nfeatureExtractionStartTime = time.time()\nmaxNumFeatures = 300\n\n# bag of letter sequences (chars)\nBagOfWordsExtractor = CountVectorizer(max_df=0.999, min_df=1000, max_features=maxNumFeatures, \n                                      analyzer='char', ngram_range=(1,2), \n                                      binary=True, lowercase=True)\n# bag of words\n#BagOfWordsExtractor = CountVectorizer(max_df=0.999, min_df=10, max_features=maxNumFeatures, \n#                                      analyzer='word', ngram_range=(1,6), stop_words='english', \n#                                      binary=True, lowercase=True)\n\nBagOfWordsExtractor.fit(pd.concat((df_train.ix[:,'question1'],df_train.ix[:,'question2'])).unique())\n\ntrainQuestion1_BOW_rep = BagOfWordsExtractor.transform(df_train.ix[:,'question1'])\ntrainQuestion2_BOW_rep = BagOfWordsExtractor.transform(df_train.ix[:,'question2'])\nlables = np.array(df_train.ix[:,'is_duplicate'])\n\nfeatureExtractionDurationInMinutes = (time.time()-featureExtractionStartTime)/60.0\nprint(\"feature extraction took %.2f minutes\" % (featureExtractionDurationInMinutes))"},{"cell_type":"markdown","metadata":{"_cell_guid":"09a4d072-14cf-39ba-ff85-e4b9c9b27592"},"source":"Проверил векторизацию по символам и словам, по символам намного лучше)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aabcd947-964d-08ef-16d3-a775a777d01c"},"outputs":[],"source":"# кросс валидация\ncrossValidationStartTime = time.time()\n\nnumCVSplits = 8\nnumSplitsToBreakAfter = 2\n\nX = -(trainQuestion1_BOW_rep != trainQuestion2_BOW_rep).astype(int)\n#X = -(trainQuestion1_BOW_rep != trainQuestion2_BOW_rep).astype(int) + \\\n#      trainQuestion1_BOW_rep.multiply(trainQuestion2_BOW_rep)\ny = lables\n\nlogisticRegressor = linear_model.LogisticRegression(C=0.1, solver='sag')\n\nlogRegAccuracy = []\nlogRegLogLoss = []\nlogRegAUC = []\n\nprint('---------------------------------------------')\nstratifiedCV = model_selection.StratifiedKFold(n_splits=numCVSplits, random_state=2)\nfor k, (trainInds, validInds) in enumerate(stratifiedCV.split(X, y)):\n    foldTrainingStartTime = time.time()\n\n    X_train_cv = X[trainInds,:]\n    X_valid_cv = X[validInds,:]\n\n    y_train_cv = y[trainInds]\n    y_valid_cv = y[validInds]\n\n    logisticRegressor.fit(X_train_cv, y_train_cv)\n\n    y_train_hat =  logisticRegressor.predict_proba(X_train_cv)[:,1]\n    y_valid_hat =  logisticRegressor.predict_proba(X_valid_cv)[:,1]\n\n    logRegAccuracy.append(accuracy_score(y_valid_cv, y_valid_hat > 0.5))\n    logRegLogLoss.append(log_loss(y_valid_cv, y_valid_hat))\n    logRegAUC.append(roc_auc_score(y_valid_cv, y_valid_hat))\n    \n    foldTrainingDurationInMinutes = (time.time()-foldTrainingStartTime)/60.0\n    print('fold %d took %.2f minutes: accuracy = %.3f, log loss = %.4f, AUC = %.3f' % (k+1,\n             foldTrainingDurationInMinutes, logRegAccuracy[-1],logRegLogLoss[-1],logRegAUC[-1]))\n\n    if (k+1) >= numSplitsToBreakAfter:\n        break\n\n\ncrossValidationDurationInMinutes = (time.time()-crossValidationStartTime)/60.0\n\nprint('---------------------------------------------')\nprint('cross validation took %.2f minutes' % (crossValidationDurationInMinutes))\nprint('mean CV: accuracy = %.3f, log loss = %.4f, AUC = %.3f' % (np.array(logRegAccuracy).mean(),\n                                                                 np.array(logRegLogLoss).mean(),\n                                                                 np.array(logRegAUC).mean()))\nprint('---------------------------------------------')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c613d3e5-68e3-cfd0-35a1-7f2a3a2f3a26"},"outputs":[],"source":"# тренеруемся на полном наборе данных\n\ntrainingStartTime = time.time()\n\nlogisticRegressor = linear_model.LogisticRegression(C=0.1, solver='sag', \n                                                    class_weight={1: 0.46, 0: 1.32})\n\n# Стоит заметить: class_weight принимает значения {0.46, 1.32}.\n# Оказывается, что распределение меток на трейне и тесте разное, то есть распределение меток на тесте просто скошено.\n    \n\nlogisticRegressor.fit(X, y)\n\ntrainingDurationInMinutes = (time.time()-trainingStartTime)/60.0\nprint('full training took %.2f minutes' % (trainingDurationInMinutes))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd099799-075c-c2e8-1e18-e9f10b595e9d"},"outputs":[],"source":"# работает с полной тестовой информацией\n\ntestPredictionStartTime = time.time()\n\n\ndf_test.ix[df_test['question1'].isnull(),['question1','question2']] = 'random empty question'\ndf_test.ix[df_test['question2'].isnull(),['question1','question2']] = 'random empty question'\n\ntestQuestion1_BOW_rep = BagOfWordsExtractor.transform(df_test.ix[:,'question1'])\ntestQuestion2_BOW_rep = BagOfWordsExtractor.transform(df_test.ix[:,'question2'])\n\nX_test = -(testQuestion1_BOW_rep != testQuestion2_BOW_rep).astype(int)\n\n#  для избежания ошибки, связанной с памятью\nseperators= [750000,1500000]\ntestPredictions1 = logisticRegressor.predict_proba(X_test[:seperators[0],:])[:,1]\ntestPredictions2 = logisticRegressor.predict_proba(X_test[seperators[0]:seperators[1],:])[:,1]\ntestPredictions3 = logisticRegressor.predict_proba(X_test[seperators[1]:,:])[:,1]\ntestPredictions = np.hstack((testPredictions1,testPredictions2,testPredictions3))\n\n\ntestPredictionDurationInMinutes = (time.time()-testPredictionStartTime)/60.0\nprint('predicting on test took %.2f minutes' % (testPredictionDurationInMinutes))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9785b726-1933-84fd-ba70-61074208ea56"},"outputs":[],"source":"# создаем submission\n\nsubmissionName = 'GalitskiyIgor[Technosphere]'\n\nsubmission = pd.DataFrame()\nsubmission['test_id'] = df_test['test_id']\nsubmission['is_duplicate'] = testPredictions\nsubmission.to_csv(submissionName + '.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f46eee81-745e-bb20-9974-c08dc285bee1"},"source":"В итоге от того какие мы зададим параметры векторизации:\n \n\n - max_features\n - min_df\n - ngram_range\n\nбудет зависеть наш score , чем больше - тем лучше.\nС параметрами которые стоят сейчас выдает 0.38773"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d619a4e-eba2-f69e-c9bb-515bd728f722"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}