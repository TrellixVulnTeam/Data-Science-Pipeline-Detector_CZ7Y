{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"df7341d1-8dee-0a86-6859-4183d3dae6d1"},"source":"Learning feature extraction from the below kernel\nhttps://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e48549e0-d2e3-c439-ce4d-c58fb72ee908"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()\n\n\t\n\ndf_train = pd.read_csv('../input/train.csv')\n\nprint('Total number of question pairs for training: {}'.format(len(df_train)))\nprint('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\nqids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\nprint('Total number of questions in the training data: {}'.format(len(\n    np.unique(qids))))\nprint('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef0f2c5e-75e2-422a-b953-ce89af9486ba"},"outputs":[],"source":"plt.figure(figsize=(12, 5))\nplt.hist(qids.value_counts(), bins=50)\nplt.yscale('log', nonposy='clip')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\nprint()\n\ndf_test = pd.read_csv('../input/test.csv')\ndf_test.head()\n\nprint('Total number of question pairs for testing: {}'.format(len(df_test)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e5f4f36-2e71-961d-c217-045284fd080b"},"outputs":[],"source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\ntest_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n\ndist_train = train_qs.apply(len)\ndist_test = test_qs.apply(len)\nplt.figure(figsize=(15, 10))\nplt.hist(dist_train, bins=200, range=[0, 200], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=200, range=[0, 200], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of character count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of characters', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67f337cb-d17f-380a-eeab-5e2eeef1f31f"},"outputs":[],"source":"dist_train = train_qs.apply(lambda x: len(x.split(' ')))\ndist_test = test_qs.apply(lambda x: len(x.split(' ')))\n\nplt.figure(figsize=(15, 10))\nplt.hist(dist_train, bins=50, range=[0, 50], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=50, range=[0, 50], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of word count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6479355e-9f62-ceaf-d4a9-a8723c61086d"},"outputs":[],"source":"from wordcloud import WordCloud\ncloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c5a6908-2105-18c9-263e-df03db13b8bb"},"outputs":[],"source":"#Semantic Analysis\n\nqmarks = np.mean(train_qs.apply(lambda x: '?' in x))\nmath = np.mean(train_qs.apply(lambda x: '[math]' in x))\nfullstop = np.mean(train_qs.apply(lambda x: '.' in x))\ncapital_first = np.mean(train_qs.apply(lambda x: x[0].isupper()))\ncapitals = np.mean(train_qs.apply(lambda x: max([y.isupper() for y in x])))\nnumbers = np.mean(train_qs.apply(lambda x: max([y.isdigit() for y in x])))\n\nprint('Questions with question marks: {:.2f}%'.format(qmarks * 100))\nprint('Questions with [math] tags: {:.2f}%'.format(math * 100))\nprint('Questions with full stops: {:.2f}%'.format(fullstop * 100))\nprint('Questions with capitalised first letters: {:.2f}%'.format(capital_first * 100))\nprint('Questions with capital letters: {:.2f}%'.format(capitals * 100))\nprint('Questions with numbers: {:.2f}%'.format(numbers * 100))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8709eaa-45f1-5a69-c99d-c0dffc1688d4"},"outputs":[],"source":"from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R\n\ntrain_word_match = df_train.apply(word_match_share, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58fca6c8-697c-a298-b188-20dc709d4177"},"outputs":[],"source":"plt.figure(figsize=(15, 5))\nplt.hist(train_word_match[df_train['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(train_word_match[df_train['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fb585cce-e939-e9ae-60df-ae3d59dee968"},"outputs":[],"source":"from collections import Counter\n\n# If a word appears only once, we ignore it completely (likely a typo)\n# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\n\neps = 5000 \nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}\n\nprint('Most common words and weights: \\n')\nprint(sorted(weights.items(), key=lambda x: x[1] if x[1] > 0 else 9999)[:10])\nprint('\\nLeast common words and weights: ')\n(sorted(weights.items(), key=lambda x: x[1], reverse=True)[:10])\n\n\nfrom collections import Counter\n\n# If a word appears only once, we ignore it completely (likely a typo)\n# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\n\neps = 5000 \nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}\n\nprint('Most common words and weights: \\n')\nprint(sorted(weights.items(), key=lambda x: x[1] if x[1] > 0 else 9999)[:10])\nprint('\\nLeast common words and weights: ')\n(sorted(weights.items(), key=lambda x: x[1], reverse=True)[:10])\n\n\ndef tfidf_word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    \n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    \n    R = np.sum(shared_weights) / np.sum(total_weights)\n    return R\n\nplt.figure(figsize=(15, 5))\ntfidf_train_word_match = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\nplt.hist(tfidf_train_word_match[df_train['is_duplicate'] == 0].fillna(0), bins=20, normed=True, label='Not Duplicate')\nplt.hist(tfidf_train_word_match[df_train['is_duplicate'] == 1].fillna(0), bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over tfidf_word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07f67736-9591-120c-8105-e575623b644b"},"outputs":[],"source":"from sklearn.metrics import roc_auc_score\nprint('Original AUC:', roc_auc_score(df_train['is_duplicate'], train_word_match))\nprint('   TFIDF AUC:', roc_auc_score(df_train['is_duplicate'], tfidf_train_word_match.fillna(0)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a00d0549-56af-9146-1709-74454a79455b"},"outputs":[],"source":"x_train = pd.DataFrame()\nx_test = pd.DataFrame()\nx_train['word_match'] = train_word_match\nx_train['tfidf_word_match'] = tfidf_train_word_match\nx_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)\nx_test['tfidf_word_match'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)\n\ny_train = df_train['is_duplicate'].values\n\n\npos_train = x_train[y_train == 1]\nneg_train = x_train[y_train == 0]\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4de8682f-7a67-6fa1-a4ff-6227c351f72d"},"outputs":[],"source":"p = 0.165\nscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\nwhile scale > 1:\n    neg_train = pd.concat([neg_train, neg_train])\n    scale -=1\nneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\nprint(len(pos_train) / (len(pos_train) + len(neg_train)))\n\nx_train = pd.concat([pos_train, neg_train])\ny_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\ndel pos_train, neg_train"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7d7aba9-f4dc-0183-47c3-1080a686f7c7"},"outputs":[],"source":"#Train test split\nfrom sklearn.cross_validation import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c297cab-9073-ee0b-dd4e-f204f94e6817"},"outputs":[],"source":"#Xgboost model\nimport xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n\nd_test = xgb.DMatrix(x_test)\np_test = bst.predict(d_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ca1617a-025f-4312-57b1-faf0be16b497"},"outputs":[],"source":"sub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = p_test\nsub.to_csv('simple_xgb.csv', index=False)\nprint(\"done\")"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}