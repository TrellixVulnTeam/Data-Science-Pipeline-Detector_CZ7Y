{"cells":[{"metadata":{"_cell_guid":"a93da02c-4974-9116-74ce-3c8c45c3d581","trusted":true},"cell_type":"code","source":"\n#importing the libraries\n\nimport numpy as np \nimport pandas as pd \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the libraries\n\nimport warnings\nimport os\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading the train dataset and printing the tail of the dataset\n\ntrain_data = pd.read_csv(\"../input/major-dataset/train.csv\")\nprint (train_data.shape)\ntrain_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading the test dataset and printing the head of the dataset\n\ntest_data = pd.read_csv('../input/major-dataset/test.csv')\nprint (test_data.shape)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c2bc3874-d7cd-faa8-037f-dccdb4fc8e7c","trusted":true},"cell_type":"code","source":"#dropping the unused columns from the train dataset and test dataset\n\ntrain_data = train_data.drop(['id', 'qid1', 'qid2'], 1)\ntest_data = test_data.drop(['test_id'], 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8234a5a5-822a-6f9e-f4a4-72464f5c2b33","trusted":true},"cell_type":"code","source":"#filling the empty or null values with a space \n\ntrain_data = train_data.fillna('')\ntest_data = test_data.fillna('')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f3e4d73e-ee05-ded2-2c86-962164638e58","trusted":true},"cell_type":"code","source":"#importing the packages required for the cleaning of the data\n\n#pickle used for serialize and deserialize\nimport pickle\n\n#nltk stands for natural language tool kit\nimport nltk\n\n#re is used for substituting the string with regular expressions\nimport re\n\n#stopwords are removed from the text data\nfrom nltk.corpus import stopwords\n\n#word_tokenize used to make the words in a sentence as tokens\nfrom nltk import word_tokenize\n\n#puntuations are used for checking for the puntuations in the strings\nfrom string import punctuation\n\n#snowball stemmer is used for getting a meaning full root word\nfrom nltk.stem import SnowballStemmer\n\nstop_words = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"20befdf0-f412-1df5-071c-4bb0c688a50c","trusted":true},"cell_type":"code","source":"#writing a function which does all the cleaning of data and uses all the packages defined above and data preprocessing \n\nimport re\n\ndef text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n    # Clean the text, with the option to remove stop_words and to stem words.\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n    text = re.sub(r\"what's\", \"\", text)\n    text = re.sub(r\"What's\", \"\", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"I'm\", \"I am\", text)\n    text = re.sub(r\" m \", \" am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"60k\", \" 60000 \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e-mail\", \"email\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = re.sub(r\"quikly\", \"quickly\", text)\n    text = re.sub(r\" usa \", \" America \", text)\n    text = re.sub(r\" USA \", \" America \", text)\n    text = re.sub(r\" u s \", \" America \", text)\n    text = re.sub(r\" uk \", \" England \", text)\n    text = re.sub(r\" UK \", \" England \", text)\n    text = re.sub(r\"india\", \"India\", text)\n    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n    text = re.sub(r\"china\", \"China\", text)\n    text = re.sub(r\"chinese\", \"Chinese\", text) \n    text = re.sub(r\"imrovement\", \"improvement\", text)\n    text = re.sub(r\"intially\", \"initially\", text)\n    text = re.sub(r\"quora\", \"Quora\", text)\n    text = re.sub(r\" dms \", \"direct messages \", text)  \n    text = re.sub(r\"demonitization\", \"demonetization\", text) \n    text = re.sub(r\"actived\", \"active\", text)\n    text = re.sub(r\"kms\", \" kilometers \", text)\n    text = re.sub(r\"KMs\", \" kilometers \", text)\n    text = re.sub(r\" cs \", \" computer science \", text) \n    text = re.sub(r\" upvotes \", \" up votes \", text)\n    text = re.sub(r\" iPhone \", \" phone \", text)\n    text = re.sub(r\"\\0rs \", \" rs \", text) \n    text = re.sub(r\"calender\", \"calendar\", text)\n    text = re.sub(r\"ios\", \"operating system\", text)\n    text = re.sub(r\"gps\", \"GPS\", text)\n    text = re.sub(r\"gst\", \"GST\", text)\n    text = re.sub(r\"programing\", \"programming\", text)\n    text = re.sub(r\"bestfriend\", \"best friend\", text)\n    text = re.sub(r\"dna\", \"DNA\", text)\n    text = re.sub(r\"III\", \"3\", text) \n    text = re.sub(r\"the US\", \"America\", text)\n    text = re.sub(r\"Astrology\", \"astrology\", text)\n    text = re.sub(r\"Method\", \"method\", text)\n    text = re.sub(r\"Find\", \"find\", text) \n    text = re.sub(r\"banglore\", \"Banglore\", text)\n    text = re.sub(r\" J K \", \" JK \", text)\n    \n    # Remove punctuation from text\n    text = ''.join([c for c in text if c not in punctuation])\n    \n    # Optionally, remove stop words\n    if remove_stop_words:\n        text = text.split()\n        text = [w for w in text if not w in stop_words]\n        text = \" \".join(text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a2dc8cde-e352-0af0-0852-82e08ce20aac","trusted":true},"cell_type":"code","source":"#modifying the questions i.e, sentences in our train data into list of words\n\ntrain_data['question1_modified'] = train_data.apply(lambda x: text_to_wordlist(x['question1']), axis = 1)\ntrain_data['question2_modified'] = train_data.apply(lambda x: text_to_wordlist(x['question2']), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c2a3fbb9-af84-ee0b-be83-d0637de08f4f","trusted":true},"cell_type":"code","source":"#modifying the questions i.e, sentences in our test data into list of words\n\ntest_data['question1_modified'] = test_data.apply(lambda x: text_to_wordlist(x['question1']), axis = 1)\ntest_data['question2_modified'] = test_data.apply(lambda x: text_to_wordlist(x['question2']), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dumping the serialized data into particular file defined \n\nimport pickle\n\npickle.dump(train_data['question1_modified'], open('pickle_train_question1_modified', 'wb'))\npickle.dump(train_data['question2_modified'], open('pickle_train_question2_modified', 'wb'))\n\npickle.dump(test_data['question1_modified'], open('pickle_test_question1_modified', 'wb'))\npickle.dump(test_data['question2_modified'], open('pickle_test_question2_modified', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#horizontal align of the texts\n\nfrom keras.preprocessing.text import Tokenizer\n\ntrain_text = np.hstack([train_data.question1_modified, train_data.question2_modified])\ntokenizer = Tokenizer()\n\n#converting anything into text \ntokenizer.fit_on_texts(train_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting the texts generated in previous module into sequences and storing them in dataset\n\ntrain_data['tokenizer_1'] = tokenizer.texts_to_sequences(train_data.question1_modified)\ntrain_data['tokenizer_2'] = tokenizer.texts_to_sequences(train_data.question2_modified)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#similar thing is done with the test data also . horizontal alignment , converting to text\ntest_text = np.hstack([test_data.question1_modified, test_data.question2_modified])\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting to the sequences\n\ntest_data['tokenizer_1'] = tokenizer.texts_to_sequences(test_data.question1_modified)\ntest_data['tokenizer_2'] = tokenizer.texts_to_sequences(test_data.question2_modified)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combining the two sequenced data ( question1 and questions2 ) of trained data\n\ntrain_data['tokenizer'] = train_data['tokenizer_1'] + train_data['tokenizer_2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting to the sequences\n\ntest_data['tokenizer'] = test_data['tokenizer_1'] + test_data['tokenizer_2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the combined sequenced train data\n\nprint (train_data['tokenizer_1'][0])\nprint (train_data['tokenizer_2'][0])\nprint (train_data['tokenizer'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the combined sequenced test data\n\nprint (test_data['tokenizer_1'][0])\nprint (test_data['tokenizer_2'][0])\nprint (test_data['tokenizer'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lamda function to identify max length used for padding \n\nmax_length = 500\nmax_token = np.max([np.max(train_data.tokenizer.max()),np.max(test_data.tokenizer.max())])\nprint (max_length, max_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#storing the columns which is used to feeding the model\n\ny_train = train_data[['is_duplicate']]\nX_train = train_data[['tokenizer']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#same thing with the test data\n\nX_test = test_data[['tokenizer']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the sequence from keras\n\nfrom keras.preprocessing import sequence\n\n#padding every question in training data using max_length ( normalization )\nX_train = sequence.pad_sequences(X_train.tokenizer, maxlen = max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#padding every question in testing data using max_length ( normalization )\n\nX_test = sequence.pad_sequences(X_test.tokenizer, maxlen = max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the different layers to define a model\n\nfrom keras.models import Sequential, Model\nmax_token=500\nfrom keras.layers import Input, Embedding, Dense, Dropout, LSTM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#sequential means linear stack of layers\nmodel_1 = Sequential()\n\n# converting positive values to fixed dense vectors\nmodel_1.add(Embedding(max_token, 32))\n\n#dropping certain amount of nodes(neurons) to overcome overfitting\nmodel_1.add(Dropout(0.3))\n\n#adding the LSTM layer\nmodel_1.add(LSTM(32))\n\n#dropping certain amount of nodes(neurons) to overcome overfitting\nmodel_1.add(Dropout(0.3))\n\n#implements the output function based on sigmoid activation\nmodel_1.add(Dense(1, activation = 'sigmoid'))\n\n#compiling the model defined \n#binary_crossentopy - finding the loss between the true output and predicted output\n#rmsprop - maintain moving average of square of gradients\n\nmodel_1.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the model summary which is defined\n\nmodel_1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the train data to the model and going through the epochs and finding the accuracy\n\nmodel_1.fit(X_train, y_train, epochs = 5, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}