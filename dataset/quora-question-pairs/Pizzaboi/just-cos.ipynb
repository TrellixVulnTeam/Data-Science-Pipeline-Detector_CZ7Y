{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95acb98a-3f03-7217-3920-dec3c054e839"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer   # TfidfVectorizer\nfrom sklearn.metrics.pairwise import paired_cosine_distances\nfrom sklearn.decomposition import TruncatedSVD\nfrom scipy.sparse import hstack, vstack\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import normalize\nfrom nltk.corpus import stopwords\nfrom sklearn.linear_model import LogisticRegression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09ac6a6c-2c83-f4c2-92cb-66a018fdffad"},"outputs":[],"source":"from sklearn.linear_model import SGDClassifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4168ea6a-5930-b27e-b0da-3ae3eb13eb45"},"outputs":[],"source":"# Исходная обучающая выборка\ndata = pd.read_csv('../input/train.csv')\n# Исходна тестовая выборка\ndata_test_b = pd.read_csv('../input/test.csv')\nstop = set(stopwords.words('english'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f05a0330-37d8-33b7-e970-ede6201abe27"},"outputs":[],"source":"data.question1=data.question1.fillna('').astype(str)\ndata.question2=data.question2.fillna('').astype(str)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"502a4620-010c-47a3-18f6-254d47cbb84d"},"outputs":[],"source":"def texts_vectorization(vectorizer, texts_1, texts_2):\n    vectorizer.fit(pd.concat([texts_1.fillna(''), texts_2.fillna('')]))\n    vect_1 = vectorizer.transform(texts_1.fillna(''))\n    vect_2 = vectorizer.transform(texts_2.fillna(''))\n    return vect_1, vect_2\ndef stopwords_go_away(Question):\n    global stop\n    if type(Question)==str:\n        for mark in [',', '.', '!', ':','&', '?', '-', '[', ']', '{', '}', '(', ')', '/', '^', '\"',\"'\"]:\n            Question=Question.replace(mark,' ')\n        words=[i for i in Question.lower().split() if i not in stop]\n        return ' '.join(words)\ndata['question1stop']= data['question1'].apply(stopwords_go_away)\ndata['question2stop']= data['question2'].apply(stopwords_go_away)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"baf95a32-5dc5-4abe-06c5-39e91e655038"},"outputs":[],"source":"def doublewords_go_away(Question):\n    if type(Question)==str:\n        words=list(set(Question.lower().split()))\n        return ' '.join(words)\ndata['question1wd']= data['question1'].apply(doublewords_go_away)\ndata['question2wd']= data['question2'].apply(doublewords_go_away)\ndata['question1min']= data['question1wd'].apply(doublewords_go_away)\ndata['question2min']= data['question2wd'].apply(doublewords_go_away)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d06016e4-a46d-fb42-e634-c371ef9a2639"},"outputs":[],"source":"vect = CountVectorizer(lowercase=True, analyzer='word')\nbag_product, bag_search = texts_vectorization(vect, data.question1, data.question2)\ndata['cosine_similarity_queries'] = paired_cosine_distances(bag_product, bag_search)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f71bc1b-d99e-c598-ec63-49e30f8f80c5"},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\ndef score_on_features(data, features):\n    for clf in [LogisticRegression()]:\n        X = data[features]\n        res = cross_val_score(clf, X, y=data.is_duplicate, scoring='neg_log_loss', cv=10)\n        print (res.mean(), res.std())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0ad05199-11f8-b700-88eb-7249c538d610"},"outputs":[],"source":"features_1 = ['cosine_similarity_queries']\nscore_on_features(data, features_1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"949df277-b023-f4ae-931f-e86e40f063f4"},"outputs":[],"source":"tsvd = TruncatedSVD(n_components=10)\nvert_queiries = vstack([bag_product, bag_search])\nvert_svd = tsvd.fit_transform(vert_queiries)\nproduct_vert_svd, search_vert_svd = vert_svd[:bag_product.shape[0], :], vert_svd[bag_product.shape[0]:, :]\nfeatures_2 = []\nfor num_comp in range(product_vert_svd.shape[1]):\n    f_name = 'comp_svd_diff_{}'.format(num_comp)\n    features_2.append(f_name)\n    data[f_name] = np.abs(product_vert_svd[:, num_comp] - search_vert_svd[:, num_comp])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29ccbac6-2f89-022e-4bac-e37b91e7c208"},"outputs":[],"source":"tsvd = TruncatedSVD(n_components=10)\nside_queries = hstack([bag_product, bag_search])\nsvd_features = tsvd.fit_transform(side_queries)\nfeatures_3 = []\nfor num_comp in range(svd_features.shape[1]):\n    f_name = 'svd_paired_feat_{}'.format(num_comp)\n    features_3.append(f_name)\n    data[f_name] = svd_features[:, num_comp]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a3f2e09-a02f-38f8-8b85-c90fd5d9eddb"},"outputs":[],"source":"data['len1'] = data['question1'].fillna(0).str.len().astype(np.float32)\ndata['len2'] = data['question2'].fillna(0).str.len().astype(np.float32)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7017747-1672-67db-5b05-61591b79e767"},"outputs":[],"source":"data['abs_diff_len1_len2'] = np.abs(data['len1'] - data['len2'])\nfeatures_4=['abs_diff_len1_len2']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40d0535b-54d3-9095-a1a1-789766b4025b"},"outputs":[],"source":"data.abs_diff_len1_len2=data.abs_diff_len1_len2.fillna(100000)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"658f8049-2efe-7d6d-580d-1bbe308a722a"},"outputs":[],"source":"score_on_features(data, features_1+features_2+features_3+features_4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c37b5d0-3688-8e4b-6786-0f513d5aec49"},"outputs":[],"source":"tsvd = TruncatedSVD(n_components=15)\nbag_product.shape, bag_search.shape\nvert_queiries = vstack([bag_product, bag_search])\nvert_queiries.shape\nvert_svd = tsvd.fit_transform(vert_queiries)\nproduct_vert_svd, search_vert_svd = vert_svd[:bag_product.shape[0], :], vert_svd[bag_product.shape[0]:, :]\nproduct_vert_svd.shape, product_vert_svd.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5489941-eb4a-f095-fabb-44fe9641ed56"},"outputs":[],"source":"features_2 = []\nfor num_comp in range(product_vert_svd.shape[1]):\n    f_name = 'comp_svd_diff_{}'.format(num_comp)\n    features_2.append(f_name)\n    data[f_name] = np.abs(product_vert_svd[:, num_comp] - search_vert_svd[:, num_comp])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2efd966c-80b7-18f5-87cc-99046c4e887b"},"outputs":[],"source":"data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f95ec3d9-28c7-4f6d-7e79-d8bdc4b6f482"},"outputs":[],"source":"tsvd = TruncatedSVD(n_components=10)\nside_queries = hstack([bag_product, bag_search])\nsvd_features = tsvd.fit_transform(side_queries)\nfeatures_3 = []\nfor num_comp in range(svd_features.shape[1]):\n    f_name = 'svd_paired_feat_{}'.format(num_comp)\n    features_3.append(f_name)\n    data[f_name] = svd_features[:, num_comp]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d76a7fd-6d18-e3b8-c7d2-af057d91f4d4"},"outputs":[],"source":"score_on_features(data, features_1+features_2+features_3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f59fbac4-c6bd-57f2-ee35-68dfafb860be"},"outputs":[],"source":"data['len1'] = data['question1'].str.len().astype(np.float32)\ndata['len2'] = data['question2'].str.len().astype(np.float32)\ndata['abs_diff_len1_len2'] = (np.abs(data['len1'] - data['len2']))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0c014c6-8ebc-a5eb-3563-7cb2505d2575"},"outputs":[],"source":"data['len1'] = data['question1'].str.len().astype(np.float32)\ndata['len2'] = data['question2'].str.len().astype(np.float32)\ndata['abs_diff_len1_len2'] = (np.abs(data['len1'] - data['len2']))\ndata['len1min'] = data['question1min'].str.len().astype(np.float32)\ndata['len2min'] = data['question2min'].str.len().astype(np.float32)\ndata['abs_diff_len1_len2min'] = (np.abs(data['len1min'] - data['len2min']))\nfeatures_4=['abs_diff_len1_len2','abs_diff_len1_len2min' ]\nscore_on_features(data, features_1+features_2+features_3+features_4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4c6def4-86b6-074b-b8b7-14b3e6a377ec"},"outputs":[],"source":"from sklearn.metrics import jaccard_similarity_score\ndef jaccard(X, Y):\n    res=np.zeros(X.shape[0])\n    for i in range(X.shape[0]):\n        X1=set(X[i].split(' '))\n        Y1=set(Y[i].split(' '))\n        if len((X1 | Y1) - (X1 &Y1))==0:\n            res[i]=1.0\n        else:\n            res[i]=1.0*len(X1 &Y1)/len((X1 | Y1)) #- (X1 &Y1)\n    return res\ndata['jaccard']=jaccard(data.question1.values, data.question2.values)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bceb30ac-b8e9-7291-bd06-e729b12ef12a"},"outputs":[],"source":"features_5=['jaccard' ]\nscore_on_features(data, features_1+features_2+features_3+features_4+features_5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fb716e7f-4443-e2a3-9fa1-67f4c87dfe0a"},"outputs":[],"source":"data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f659a00-0c5b-f399-512e-0f262e888b57"},"outputs":[],"source":"X=data.drop('')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b92d7b0-64c2-0c00-99c2-e20eecf14b6f"},"outputs":[],"source":"model=LogisticRegression().fit(, data['is_duplicate'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"190d4ce2-c477-30db-2f81-13f4b0a83058"},"outputs":[],"source":"data=data_test_b\ndata"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1505776a-4816-33bf-bd58-c13e89127e6c"},"outputs":[],"source":"def texts_vectorization(vectorizer, texts_1, texts_2):\n    vectorizer.fit(pd.concat([texts_1.fillna(''), texts_2.fillna('')]))\n    vect_1 = vectorizer.transform(texts_1.fillna(''))\n    vect_2 = vectorizer.transform(texts_2.fillna(''))\n    return vect_1, vect_2\ndef stopwords_go_away(Question):\n    global stop\n    if type(Question)==str:\n        for mark in [',', '.', '!', ':','&', '?', '-', '[', ']', '{', '}', '(', ')', '/', '^', '\"',\"'\"]:\n            Question=Question.replace(mark,' ')\n        words=[i for i in Question.lower().split() if i not in stop]\n        return ' '.join(words)\ndata.question1= data['question1'].apply(stopwords_go_away)\ndata.question2= data['question2'].apply(stopwords_go_away)\ndef doublewords_go_away(Question):\n    if type(Question)==str:\n        words=list(set(Question.lower().split()))\n        return ' '.join(words)\ndata.question1= data['question1'].apply(doublewords_go_away)\ndata.question2= data['question2'].apply(doublewords_go_away)\nvect = CountVectorizer(lowercase=True, analyzer='word')\nbag_product, bag_search = texts_vectorization(vect, data.question1, data.question2)\ndata['is_duplicate'] = paired_cosine_distances(bag_product, bag_search)\ndata['len1'] = data['question1'].str.len().astype(np.float32)\ndata['len2'] = data['question2'].str.len().astype(np.float32)\ndata['abs_diff_len1_len2'] = (np.abs(data['len1'] - data['len2']))\ndata['len1min'] = data['question1min'].str.len().astype(np.float32)\ndata['len2min'] = data['question2min'].str.len().astype(np.float32)\ndata['abs_diff_len1_len2min'] = (np.abs(data['len1min'] - data['len2min']))\nfeatures_4=['abs_diff_len1_len2','abs_diff_len1_len2min' ]\ndata['jaccard']=jaccard(data.question1.values, data.question2.values)\nfeatures_5=['jaccard' ]\ntsvd = TruncatedSVD(n_components=10)\nvert_queiries = vstack([bag_product, bag_search])\nvert_svd = tsvd.fit_transform(vert_queiries)\nproduct_vert_svd, search_vert_svd = vert_svd[:bag_product.shape[0], :], vert_svd[bag_product.shape[0]:, :]\nfeatures_2 = []\nfor num_comp in range(product_vert_svd.shape[1]):\n    f_name = 'comp_svd_diff_{}'.format(num_comp)\n    features_2.append(f_name)\n    data[f_name] = np.abs(product_vert_svd[:, num_comp] - search_vert_svd[:, num_comp])\ntsvd = TruncatedSVD(n_components=10)\nside_queries = hstack([bag_product, bag_search])\nsvd_features = tsvd.fit_transform(side_queries)\nfeatures_3 = []\nfor num_comp in range(svd_features.shape[1]):\n    f_name = 'svd_paired_feat_{}'.format(num_comp)\n    features_3.append(f_name)\n    data[f_name] = svd_features[:, num_comp]\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c6a9539-7866-87f0-afc2-9c86ff180b1b"},"outputs":[],"source":"data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa5ea6ba-8f9f-a3bb-99c2-5f6c2caffc49"},"outputs":[],"source":"data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad38dd93-0e10-45e1-6b9c-48edeca222f9"},"outputs":[],"source":"data['is_duplicate']=0.2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f287820-d9a2-f0e7-16f0-3e2c04be6657"},"outputs":[],"source":"X=data.fillna(0).drop([''])\nclf= LogisticRegression()\ndata.is_duplicate=model.predict()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"082c35e8-2e71-128f-a2c0-9b1c3a93f843"},"outputs":[],"source":"data.index=data.test_id\ndata.drop(['question1', 'question2', 'test_id'], axis=1).to_csv('submition1')"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}