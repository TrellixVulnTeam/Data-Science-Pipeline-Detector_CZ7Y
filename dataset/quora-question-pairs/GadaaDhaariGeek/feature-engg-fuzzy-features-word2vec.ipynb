{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\ndata = pd.read_csv(zipfile.ZipFile(\"/kaggle/input/quora-question-pairs/train.csv.zip\").open(\"train.csv\"))#.head(2000)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do the initial preprocessing first, which you can find [here](https://www.kaggle.com/gadaadhaarigeek/simple-eda-to-start-with)"},{"metadata":{},"cell_type":"markdown","source":"<h2>1. Few numerical features using feature engineering</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.astype({\"question1\": str, \"question2\":str})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Intial level preprocessing\n\n# Fill null values\ndata.fillna(\" \")\n\n# freq of each questions in q1 and q2, length, number of words\n# number of common_words, unique words total, common_share\n# Sum/difference of freqs of each question\ndata['freq_qid1'] = data.groupby('qid1')['qid1'].transform('count') \ndata['freq_qid2'] = data.groupby('qid2')['qid2'].transform('count')\ndata['q1len'] = data['question1'].str.len() \ndata['q2len'] = data['question2'].str.len()\ndata['q1_n_words'] = data['question1'].apply(lambda row: len(row.split(\" \")))\ndata['q2_n_words'] = data['question2'].apply(lambda row: len(row.split(\" \")))\n\ndef normalized_word_Common(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)\ndata['word_common'] = data.apply(normalized_word_Common, axis=1)\n\ndef normalized_word_Total(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * (len(w1) + len(w2))\ndata['word_total'] = data.apply(normalized_word_Total, axis=1)\n\ndef normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\ndata['word_share'] = data.apply(normalized_word_share, axis=1)\n\ndata['freq_q1+q2'] = data['freq_qid1'] + data['freq_qid2']\ndata['freq_q1-q2'] = abs(data['freq_qid1'] - data['freq_qid2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Engineered dataframe \ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To see the words with special chars\n# Take a peek at the data\n# x = data.head(100)\n# def get_special_words(row):\n#     list_ques = row[\"question1\"].strip().lower().split(\" \")\n#     for word in list_ques:\n#         if word.isalpha() == False:\n#             pass\n#             print(word)\n\n# x.apply(get_special_words, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> 2. Text Cleaning </h2>\n\n- Preprocessing:\n\n    - Removing html tags \n    - Removing Punctuations\n    - Performing stemming\n    - Removing Stopwords\n    - Expanding contractions etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nSAFE_DIV = .0001\nSTOP_WORDS = stopwords.words(\"english\")\n\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\").replace(\"won't\", \"will not\") \\\n    .replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\").replace(\"n't\", \"not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n    .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\").replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n    .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \").replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    porter = PorterStemmer()\n    pattern = re.compile(\"\\W\")\n    \n    if type(x) == type(\"\"):\n        x = re.sub(pattern, ' ', x)\n        \n    if type(x) == type(\"\"):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n        \n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>3. NLP and fuzzy feature extraction</h2>"},{"metadata":{},"cell_type":"markdown","source":"Definition:\n- __Token__: You get a token by splitting sentence a space\n- __Stop_Word__ : stop words as per NLTK.\n- __Word__ : A token that is not a stop_word\n\n\nFeatures:\n- __cwc_min__ :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 <br>cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n<br>\n<br>\n- __cwc_max__ :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 <br>cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n<br>\n<br>\n- __csc_min__ :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2 <br> csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n<br>\n<br>\n- __csc_max__ :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2<br>csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n<br>\n<br>\n- __ctc_min__ :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2<br>ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n<br>\n<br>\n\n- __ctc_max__ :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2<br>ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n<br>\n<br>\n        \n- __last_word_eq__ :  Check if First word of both questions is equal or not<br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n<br>\n<br>\n\n- __first_word_eq__ :  Check if First word of both questions is equal or not<br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n<br>\n<br>\n        \n- __abs_len_diff__ :  Abs. length difference<br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n<br>\n<br>\n\n- __mean_len__ :  Average Token Length of both Questions<br>mean_len = (len(q1_tokens) + len(q2_tokens))/2\n<br>\n<br>\n\n\n- __fuzz_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n<br>\n\n- __fuzz_partial_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n<br>\n\n\n- __token_sort_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n<br>\n\n\n- __token_set_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n<br>\n\n\n\n\n\n- __longest_substr_ratio__ :  Ratio of length longest common substring to min lenghth of token count of Q1 and Q2<br>longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_token_features(q1, q2):\n    token_features = [0.0] * 10\n    \n    # Converting the sentence into tokens\n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    \n    # get the non-stop words in questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    # Get the stopwords in questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords count\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common tokens from question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    # common word count - min\n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    \n    # common word count - max\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    \n    # common stop count - min\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    \n    # common stop count - max\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    \n    # common token count - min\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # common token count - max\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)    \n    \n    # whether last words of both the questions are same or not ?\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # first word of both the questions are same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    # absolute differen b/w the number of tokens in both the questions\n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    # average number of tokens in both the questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n    \n    return token_features\n\ndef get_longest_common_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    print(\"Preprocess questions...\")\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n    \n    print(\"Token features...\")\n    \n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n    \n    #Computing Fuzzy Features and Merging with Dataset\n    \n    # http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n    # https://stackoverflow.com/questions/31806695/when-to-use-which-fuzz-function-to-compare-2-strings\n    # https://github.com/seatgeek/fuzzywuzzy    \n    \n    print(\"Creating fuzzy features... \")\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_common_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)    \n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nimport distance\nimport timeit\nstart = timeit.default_timer()\ndata = extract_features(data)\nstop = timeit.default_timer()\nprint(\"Time elapsed: \", stop-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_duplicate = data[data[\"is_duplicate\"] == 1]\ndata_nonduplicate = data[data[\"is_duplicate\"] == 0]\n\n# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\np = np.dstack([data_duplicate[\"question1\"], data_duplicate[\"question2\"]]).flatten()\nn = np.dstack([data_nonduplicate[\"question1\"], data_nonduplicate[\"question2\"]]).flatten()\n\nprint (\"Number of data points in class 1 (duplicate pairs) :\",len(p)//2)\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n)//2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>3.1. Making WordCloud</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n# reading the text files and removing the Stop Words:\n\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\n\nstopwords.remove(\"no\")\n#stopwords.remove(\"good\")\n#stopwords.remove(\"love\")\nstopwords.remove(\"like\")\n#stopwords.remove(\"best\")\n#stopwords.remove(\"!\")\ntextp_w = \" \".join(p)\ntextn_w = \" \".join(n)\nprint (\"Total number of words in duplicate pair questions :\", len(textp_w.split()))\nprint (\"Total number of words in non duplicate pair questions :\", len(textn_w.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloiud for duplicate pair question's text\nimport matplotlib.pyplot as plt\nwc = WordCloud(background_color=\"white\", height=400, width=800, max_words=len(textp_w), stopwords=stopwords)\nwc.generate(textp_w)\nprint (\"Word Cloud for Duplicate Question pairs\")\nplt.figure(figsize=(12, 8))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wordcloud for non duplicate pairs of question text\nwc = WordCloud(background_color=\"white\", height=400, width=800, max_words=len(textn_w),stopwords=stopwords)\n# generate word cloud\nwc.generate(textn_w)\nprint (\"Word Cloud for non-Duplicate Question pairs:\")\nplt.figure(figsize=(12, 8))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pair plot of features ['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio']\nimport seaborn as sns\nn = data.shape[0]\nsns.pairplot(data[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], \n             hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the token_sort_ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = data[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(data[data['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(data[data['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = data[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(data[data['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(data[data['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_partial_ratio', data = data[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(data[data['is_duplicate'] == 1.0]['fuzz_partial_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(data[data['is_duplicate'] == 0.0]['fuzz_partial_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the three fuzzy features seem to have similar distribution amongst two classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VISUALIZATION\n# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nX = MinMaxScaler().fit_transform(data[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = data['is_duplicate'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\n# TSNE 2d\ntsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\n# TSNE 3d\ntsne3d = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ntrace_data = [trace1]\nlayout=dict(height=800, width=1200, title='3d embedding with engineered features')\nfig=dict(data=trace_data, layout=layout)\npy.iplot(fig, filename='3DBubble')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<h2>4. Avg. Word2Vec </h2>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<h3>4.1. Tf-Idf weighted avg. W2V</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = list(data[\"question1\"]) + list(data[\"question2\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(lowercase=False)\ntfidf.fit_transform(questions)\n\nword2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy \nfrom tqdm import tqdm\n\nnlp = spacy.load(\"en_core_web_sm\")\nvecs1 = []\n\n# Set up in the spacy model\nvect_dim = len(nlp(data[\"question1\"][0])[0].vector)\n\nfor qu1 in tqdm(list(data[\"question1\"])):\n    mean_vec1 = np.zeros([1, vect_dim])\n    if len(qu1) != 0:\n        doc1 = nlp(qu1)\n        mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n        for word1 in doc1:\n            vec1 = word1.vector\n            try: \n                idf = word2tfidf[str(word)]\n            except:\n                idf = 0\n            mean_vec1 += vec1 * idf\n    mean_vec1 = mean_vec1.mean(axis=0)\n    vecs1.append(mean_vec1)        \n\ndata[\"q1_feats_tfidf_avg_w2v\"] = list(vecs1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vecs2 = []\n\n# Set up in the spacy model\nvect_dim = len(nlp(data[\"question2\"][0])[0].vector)\n\nfor qu2 in tqdm(list(data['question2'])):\n    mean_vec2 = np.zeros([1, vect_dim])\n    if len(qu2) != 0:\n        doc2 = nlp(qu2) \n        mean_vec2 = np.zeros([len(doc1), len(doc2[0].vector)])        \n        for word2 in doc2:\n            # word2vec\n            vec2 = word2.vector\n            # fetch idf score\n            try:\n                idf = word2tfidf[str(word2)]\n            except:\n                idf = 0\n            # computing idf weighted avg w2v\n            mean_vec2 += vec2 * idf\n    mean_vec2 = mean_vec2.mean(axis=0)\n    vecs2.append(mean_vec2)\n\ndata['q2_feats_tfidf_avg_w2v'] = list(vecs2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can make BoW based average Word2Vec also","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> 5. All the features </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels1 = ['freq_qid1', 'freq_qid2', 'q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_common', 'word_total', \n          'word_share', 'freq_q1+q2', 'freq_q1-q2']\nwithout_preprocess_fe_df = data[labels1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels2 = ['cwc_min', 'cwc_max', 'csc_min', 'csc_max', 'ctc_min', 'ctc_max', 'last_word_eq', 'first_word_eq', \n           'abs_len_diff', 'mean_len', 'token_set_ratio', 'token_sort_ratio', 'fuzz_ratio', 'fuzz_partial_ratio', \n           'longest_substr_ratio']\nnlp_fuzzy_fe_df = data[labels2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels3 = ['q1_feats_tfidf_avg_w2v', 'q2_feats_tfidf_avg_w2v']\navg_w2v_fe_df = data[labels3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3_q1 = pd.DataFrame(avg_w2v_fe_df[\"q1_feats_tfidf_avg_w2v\"].values.tolist(), index= avg_w2v_fe_df.index)\ndf3_q2 = pd.DataFrame(avg_w2v_fe_df[\"q2_feats_tfidf_avg_w2v\"].values.tolist(), index= avg_w2v_fe_df.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"without_preprocess_fe_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_fuzzy_fe_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Questions 1 tfidf weighted word2vec\ndf3_q1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Questions 2 tfidf weighted word2vec\ndf3_q2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of features in without preprocess dataframe :\", without_preprocess_fe_df.shape[1])\nprint(\"Number of features in nlp dataframe :\", nlp_fuzzy_fe_df.shape[1])\nprint(\"Number of features in question1 w2v  dataframe :\", df3_q1.shape[1])\nprint(\"Number of features in question2 w2v  dataframe :\", df3_q1.shape[1])\nprint(\"Number of features in final dataframe  :\", without_preprocess_fe_df.shape[1]+nlp_fuzzy_fe_df.shape[1]+df3_q1.shape[1]+df3_q2.shape[1])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}