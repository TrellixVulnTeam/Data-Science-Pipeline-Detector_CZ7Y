{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport nltk\nimport re\nimport os\nimport collections\nimport operator\n\nimport sklearn as sk\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords \n\nfrom sklearn.decomposition import PCA\npd.options.display.max_columns = 10\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../\"))\nstop_words=set(stopwords.words('english'))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"trainMaster=pd.read_csv('../input/quora-question-pairs/train.csv')\nprint(trainMaster.head())\nprint(trainMaster.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Run only when you want to train you own Embeddings\ntrainMaster['question1']=trainMaster['question1'].apply(str)\ntrainMaster['question2']=trainMaster['question2'].apply(str)\n\ntextQ1=' '.join(trainMaster['question1'].tolist())\ntextQ2=' '.join(trainMaster['question2'].tolist())\ntextMaster=textQ1+textQ2"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"textMaster=textMaster.lower()\ntextMaster=re.sub('[^A-Za-z\\s]','',textMaster)\ntextMaster=re.sub('\\s+',' ',textMaster)\ntextMaster=textMaster.strip()\ntokens=word_tokenize(textMaster)\n\nlemmatizer=WordNetLemmatizer()\nstemmer=PorterStemmer()\n\nstemList=[]\nlemmList=[]\n\nfor word in tokens:\n    stemList.append(stemmer.stem(word))\nfor word in stemList:\n    lemmList.append(lemmatizer.lemmatize(word))\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"lemmList=[x for x in lemmList if x not in stop_words]\nwordCounter = collections.Counter(lemmList)\nwordCounter = sorted(wordCounter.items(), key=operator.itemgetter(1),reverse=True)\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"wordList=list(map(lambda x:x[0],wordCounter))[:1000]\nindex=range(len(wordList))\n\nword2idx={}\nidx2word={}\nfor wd,val in zip(wordList,index):\n    word2idx[wd]=val\n    idx2word[val]=wd"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"cleanDF=pd.DataFrame()\n\ntempQ1=[]\ntempQ2=[]\nstemListQ1=[]\nlemmListQ1=[]\nstemListQ2=[]\nlemmListQ2=[]\nq1List=[]\nq2List=[]\n\nfor q1,q2 in zip(trainMaster['question1'].tolist(),trainMaster['question2'].tolist()):\n    q1=q1.lower()\n    q1=re.sub('[^A-Za-z\\s]','',q1)\n    q1=re.sub('\\s+',' ',q1)\n    q1=q1.strip()\n    tempQ1=word_tokenize(q1)\n\n    stemListQ1=[]\n    lemmListQ1=[]\n    stemListQ2=[]\n    lemmListQ2=[]\n    for word in tempQ1:\n        stemListQ1.append(stemmer.stem(word))\n    for word in stemListQ1:\n        lemmListQ1.append(lemmatizer.lemmatize(word))\n    lemmListQ1=[x for x in lemmListQ1 if x not in stop_words]\n    q1List.append(lemmListQ1)\n    \n    q2=q2.lower()\n    q2=re.sub('[^A-Za-z\\s]','',q2)\n    q2=re.sub('\\s+',' ',q2)\n    q2=q2.strip()\n    tempQ2=word_tokenize(q2)\n\n    for word in tempQ2:\n        stemListQ2.append(stemmer.stem(word))\n    for word in stemListQ2:\n        lemmListQ2.append(lemmatizer.lemmatize(word))\n    lemmListQ2=[x for x in lemmListQ2 if x not in stop_words]\n    q2List.append(lemmListQ2)\n    "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"q2Clean=''\nq1Clean=''\nq2L=[]\nq1L=[]\n\nfor tempL in q1List:\n    q1Clean=' '.join(tempL)\n    q1L.append(q1Clean)\n\nfor tempL in q2List:\n    q2Clean=' '.join(tempL)\n    q2L.append(q2Clean)\n\ncleanDF['q1_clean']=q1L\ncleanDF['q2_clean']=q2L\ncleanDF['label']=trainMaster['is_duplicate'].tolist()\ncleanDF.to_csv('cleanQQ.csv',index=True)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"try:\n    os.mkdir('../output')\nexcept:\n    pass\ncleanDF.to_csv('../output/cleanQQ.csv',index=True)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"X=[]\nY=[]\ntemp1=np.zeros(1000)\ntemp2=np.zeros(1000)\n\nfor sentList in q1List+q2List:\n    if len(sentList)>1:\n        for idx in range(len(sentList)-1):\n            temp1=np.zeros(1000)\n            temp2=np.zeros(1000)\n            try:\n                temp1[word2idx[sentList[idx]]]=1.0\n                temp2[word2idx[sentList[idx+1]]]=1.0\n            except:\n                continue\n            X.append(temp1)\n            Y.append(temp2)\n            X.append(temp2)\n            Y.append(temp1)"},{"metadata":{},"cell_type":"markdown","source":"## Method 1 Let's do Embedding + Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"filename='../input/embedding/glove.6B.50d.txt'\ndef loadGloVe(filename):\n    vocab=[]\n    embed=[]\n    file=open(filename,'r',encoding='utf8')\n    for lin in file.readlines():\n        try:\n            row=lin.strip().split(' ')\n            vocab.append(row[0])\n            embed.append(row[1:])\n        except:\n            pass\n    file.close()\n    return vocab,embed\nvocab,embed=loadGloVe(filename)\nembeddnig_dim=len(embed[0])\nembedding=np.asarray(embed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Make word2index and index2word\nword2idx={}\nidx2word={}\nfor val,idx in zip(vocab,range(len(vocab))):\n    word2idx[val]=idx\n    idx2word[idx]=val\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=10)\nembedding=pca.fit_transform(embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def digitalizeSent(string,pdSize):\n    tempIdx=2\n    pdIdx=2\n    masterSent=[]\n    string=str(string).lower()\n    loTxt=string.split()\n    counter=0\n    for wd in loTxt:\n        if wd in vocab and counter<pdSize:\n            tempIdx=word2idx[wd]\n            masterSent.append(tempIdx)\n            counter+=1\n        elif counter>=pdSize:\n            break\n        \n    \n    for fill in range(counter,pdSize):\n        masterSent.append(pdIdx)\n    return np.array(masterSent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def makeBatch(batchSize,batchNumber,paddingSize=70):\n    questionEmbeddingBatch1=[]\n    questionEmbeddingBatch2=[]\n    labelBatch=[]\n    for sent in trainMaster['question1'].tolist()[batchNumber*batchSize:batchNumber*batchSize+batchSize]:\n        questionEmbeddingBatch1.append(digitalizeSent(sent,paddingSize))\n\n    for sent in trainMaster['question2'].tolist()[batchNumber*batchSize:batchNumber*batchSize+batchSize]:\n        questionEmbeddingBatch2.append(digitalizeSent(sent,paddingSize))\n        \n    return np.array(questionEmbeddingBatch1),np.array(questionEmbeddingBatch2),np.array(trainMaster['is_duplicate'].tolist()[batchNumber*batchSize:batchNumber*batchSize+batchSize])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\nsess=tf.Session()\ndef snn(address1, address2, dropout_keep_prob,num_features, input_length):\n    def siamese_nn(input_vector, num_hidden):\n        cell_unit = tf.contrib.rnn.BasicLSTMCell # OR tf.nn.rnn_cell.BasicLSTMCell\n        lstm_forward_cell = cell_unit(num_hidden, forget_bias=1.0)\n        lstm_forward_cell = tf.contrib.rnn.DropoutWrapper(lstm_forward_cell, output_keep_prob=dropout_keep_prob)\n        lstm_backward_cell = cell_unit(num_hidden, forget_bias=1.0)\n        lstm_backward_cell = tf.contrib.rnn.DropoutWrapper(lstm_backward_cell, output_keep_prob=dropout_keep_prob)\n    \n        # Split title into a character sequence to accommodate the TF requirment\n        input_embed_split = tf.split(axis=1, num_or_size_splits=input_length, value=input_vector)\n        input_embed_split = [tf.squeeze(x, axis=[1]) for x in input_embed_split]\n\n        try:\n            outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_forward_cell,lstm_backward_cell,\n                                                                    input_embed_split,dtype=tf.float32)\n        except Exception:\n            outputs = tf.contrib.rnn.static_bidirectional_rnn(lstm_forward_cell,lstm_backward_cell,\n                                                              input_embed_split,dtype=tf.float32)\n        temporal_mean = tf.add_n(outputs) / input_length\n        output_size = 10\n        A = tf.get_variable(name=\"A\", shape=[2*num_hidden, output_size],dtype=tf.float32,\n                            initializer=tf.random_normal_initializer(stddev=0.1))\n        b = tf.get_variable(name=\"b\", shape=[output_size], dtype=tf.float32,\n                            initializer=tf.random_normal_initializer(stddev=0.1))\n        \n        final_output = tf.matmul(temporal_mean, A) + b\n        final_output = tf.nn.dropout(final_output, dropout_keep_prob)\n        return(final_output)\n        \n    output1 = siamese_nn(address1, num_features)\n\n    with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n        output2 = siamese_nn(address2, num_features)\n\n    output1 = tf.nn.l2_normalize(output1, 1)\n    output2 = tf.nn.l2_normalize(output2, 1)\n    dot_prod = tf.reduce_sum(tf.multiply(output1, output2), 1)\n    \n    return dot_prod\n\n\ndef get_predictions(scores):\n    predictions = tf.sign(scores, name=\"predictions\")\n    return predictions\n\n\ndef loss(scores, y_target, margin):\n    pos_loss_term = 0.25 * tf.square(tf.subtract(1., scores))\n    pos_mult = tf.add(tf.multiply(0.5, tf.cast(y_target, tf.float32)), 0.5)\n    pos_mult = tf.cast(y_target, tf.float32)\n\n    positive_loss = tf.multiply(pos_mult, pos_loss_term)\n    neg_mult = tf.add(tf.multiply(-0.5, tf.cast(y_target, tf.float32)), 0.5)\n    neg_mult = tf.subtract(1., tf.cast(y_target, tf.float32))\n    \n    negative_loss = neg_mult*tf.square(scores)\n    loss = tf.add(positive_loss, negative_loss)\n    target_zero = tf.equal(tf.cast(y_target, tf.float32), 0.)\n    less_than_margin = tf.less(scores, margin)\n    both_logical = tf.logical_and(target_zero, less_than_margin)\n    both_logical = tf.cast(both_logical, tf.float32)\n    multiplicative_factor = tf.cast(1. - both_logical, tf.float32)\n    total_loss = tf.multiply(loss, multiplicative_factor)\n    avg_loss = tf.reduce_mean(total_loss)\n    return avg_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"address1_ph = tf.placeholder(tf.int32, [None, 70], name=\"q1_ph\")\naddress2_ph = tf.placeholder(tf.int32, [None, 70], name=\"q2_ph\")\ny_target_ph = tf.placeholder(tf.int32, [None], name=\"y_target_ph\")\ndropout_keep_prob_ph = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"address1_embed = tf.nn.embedding_lookup(embedding, address1_ph)\naddress1_embed=tf.cast(address1_embed,tf.float32)\naddress2_embed = tf.nn.embedding_lookup(embedding, address2_ph)\naddress2_embed=tf.cast(address2_embed,tf.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features=128\ntext_snn = snn(address1_embed, address2_embed,dropout_keep_prob_ph, num_features, 70)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_loss = loss(text_snn, y_target_ph, 0.5)\noptimizer = tf.train.AdamOptimizer(0.01)\ntrain_op = optimizer.minimize(batch_loss)\ninit = tf.global_variables_initializer()\nsess.run(init)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss_vec = []\ntrain_acc_vec = []\ndropout_keep_prob=0.75\nfor b in range(2000): ## Modify this on your local machine\n    address1,address2,target_similarity=makeBatch(100,b,paddingSize=70)    \n    train_feed_dict = {address1_ph: address1,address2_ph: address2,\n                       y_target_ph: target_similarity, dropout_keep_prob_ph: dropout_keep_prob}\n    _, train_loss = sess.run([train_op, batch_loss],feed_dict=train_feed_dict)\n    train_loss_vec.append(train_loss)\n    if b%10==0:\n        print('Training Metrics, Batch {0}: Loss={1:.3f}.'.format(b, train_loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}