{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c3fa73f-23c5-b5ff-ac79-e44f5ed44dfa"},"outputs":[],"source":"# -*- coding: utf-8 -*-\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport datetime\nimport operator\nfrom sklearn.cross_validation import train_test_split\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom pylab import plot, show, subplot, specgram, imshow, savefig\n\nRS = 12357\nROUNDS = 315\n\nprint(\"Started\")\nnp.random.seed(RS)\ninput_folder = '../input/'\n\ndef train_xgb(X, y, params):\n    print(\"Will train XGB for {} rounds, RandomSeed: {}\".format(ROUNDS, RS))\n    x, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n\n    xg_train = xgb.DMatrix(x, label=y_train)\n    xg_val = xgb.DMatrix(X_val, label=y_val)\n\n    watchlist = [(xg_train, 'train'), (xg_val, 'eval')]\n    return xgb.train(params, xg_train, ROUNDS, watchlist)\n\n\ndef predict_xgb(clr, X_test):\n    return clr.predict(xgb.DMatrix(X_test))\n\n\ndef create_feature_map(features):\n    outfile = open('xgb.fmap', 'w')\n    i = 0\n    for feat in features:\n        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n        i = i + 1\n    outfile.close()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d0edc2f-e0ea-f8a2-46bb-210495f1ca70"},"outputs":[],"source":"params = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.11\nparams['max_depth'] = 5\nparams['silent'] = 1\nparams['seed'] = RS\n\ndf_train = pd.read_csv(input_folder + 'train.csv')\ndf_test = pd.read_csv(input_folder + 'test.csv')\nprint(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c2d7895-3c7e-8d9b-f829-00d3a7d2bf44"},"outputs":[],"source":"print(\"Features processing\")\ndef get_weight(count, eps=10000, min_count=2):\n    return 0 if count < min_count else 1 / (count + eps)\n\ntrain_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}\n\nstops = set(stopwords.words(\"english\"))\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6287bd09-5b11-f28c-a914-01e1a19c0f14"},"outputs":[],"source":"def word_shares(row):\n    q1 = set(str(row['question1']).lower().split())\n    q1words = q1.difference(stops)\n    if len(q1words) == 0:\n        return '0:0:0:0:0'\n    q2 = set(str(row['question2']).lower().split())\n    q2words = q2.difference(stops)\n    if len(q2words) == 0:\n        return '0:0:0:0:0'\n\n    q1stops = q1.intersection(stops)\n    q2stops = q2.intersection(stops)\n\n    shared_words = q1words.intersection(q2words)\n    shared_weights = [weights.get(w, 0) for w in shared_words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n\n    R1 = np.sum(shared_weights) / np.sum(total_weights)  # tfidf share\n    R2 = len(shared_words) / (len(q1words) + len(q2words))  # count share\n    R31 = len(q1stops) / len(q1words)  # stops in q1\n    R32 = len(q2stops) / len(q2words)  # stops in q2\n    return '{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d041efbb-72ab-c027-2c6f-205d724ad7d9"},"outputs":[],"source":"df = pd.concat([df_train, df_test])\ndf['word_shares'] = df.apply(word_shares, axis=1, raw=True)\n\nx = pd.DataFrame()\n\nx['word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\nx['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\nx['shared_count'] = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n\nx['stops1_ratio'] = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\nx['stops2_ratio'] = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\nx['diff_stops_r'] = x['stops1_ratio'] - x['stops2_ratio']\n\nx['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\nx['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\nx['diff_len'] = x['len_q1'] - x['len_q2']\n\nx['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\nx['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\nx['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n\nx['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\nx['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\nx['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n\nx['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\nx['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\nx['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n\nx['exactly_same'] = (df['question1'] == df['question2']).astype(int)\nx['duplicated'] = df.duplicated(['question1', 'question2']).astype(int)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0517b69-a8c7-8319-00af-9d7dbf1752f8"},"outputs":[],"source":"# ... MY FEATURES HERE ... Waiting For More Test"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6099d202-e9f8-3be8-5dff-88ca581f1525"},"outputs":[],"source":"feature_names = list(x.columns.values)\ncreate_feature_map(feature_names)\nprint(\"Features: {}\".format(feature_names))\n\nx_train = x[:df_train.shape[0]]\nx_test = x[df_train.shape[0]:]\ny_train = df_train['is_duplicate'].values\ndel x, df_train"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd47256d-f7c5-3d0f-d49a-4e354b569ada"},"outputs":[],"source":"if 1:  # Now we oversample the negative class - on your own risk of overfitting!\n    pos_train = x_train[y_train == 1]\n    neg_train = x_train[y_train == 0]\n\n    print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n    p = 0.175\n    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n    while scale > 1:\n        neg_train = pd.concat([neg_train, neg_train])\n        scale -= 1\n    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n    print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n\n    x_train = pd.concat([pos_train, neg_train])\n    y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n    del pos_train, neg_train"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c3d211d-8fd0-fd30-d850-9d8bff8cb00b"},"outputs":[],"source":"print(\"Training data: X_train: {}, Y_train: {}, X_test: {}\".format(x_train.shape, len(y_train), x_test.shape))\nROUNDS = 500\nclr = train_xgb(x_train, y_train, params)\npreds = predict_xgb(clr, x_test)\nprint(preds.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56205b7a-0429-8c21-56d0-c3f48efd2e36"},"outputs":[],"source":"print(\"Translate for output type...\")\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = preds\nsub.to_csv(\"xgb_seed{}_n{}.csv\".format(RS, ROUNDS), index=False)\n\nprint(\"Features importances...\")\nimportance = clr.get_fscore(fmap='xgb.fmap')\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\nft = pd.DataFrame(importance, columns=['feature', 'fscore'])\n\nft.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\nplt.gcf().savefig('features_importance.png')\nprint('Clear.')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb64eab8-21dc-b816-2c38-0434a9947daf"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}