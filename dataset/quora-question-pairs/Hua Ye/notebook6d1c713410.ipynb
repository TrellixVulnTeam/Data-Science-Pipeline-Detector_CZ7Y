{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ba74d1d-e5fb-c911-addb-decc57571b5d"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5193f5d-8e10-a738-77c9-06f2056785d2"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"50192ab8-7cd1-6ee1-ca49-5a7eb1f43f97"},"outputs":[],"source":"df_train = pd.read_csv( \"../input/train.csv\")\ndf_test = pd.read_csv( \"../input/test.csv\")\ntrain_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()\n                    +df_test['question1'].tolist() + df_test['question2'].tolist()\n                    ).astype(str)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ce390b6-8d96-7875-95e8-39f9b0c6ef95"},"outputs":[],"source":"stops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9bf78225-ec7a-cd5a-c217-eae6f8964aa0"},"outputs":[],"source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n# If a word appears only once, we ignore it completely (likely a typo)\n# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\ndef get_weight(count, eps=10000, min_count=2):\n        return 1 / (count + eps)\n\neps = 5000 \nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8547227-3f5b-5a7e-936e-8d185f00a7dd"},"outputs":[],"source":"def tfidf_word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    \n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    total_weights1 = [1 if x == 0 else x for x in total_weights]\n            \n    R = np.sum(shared_weights) / np.sum(total_weights1)\n    return R"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"484f29dd-d11c-7d02-28f8-2ac415a14641"},"outputs":[],"source":"x_train = pd.DataFrame()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40a38a4e-a33f-cb0d-79be-fbdff674e642"},"outputs":[],"source":"x_test = pd.DataFrame()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ebd545b1-506b-9962-1696-675e6bd7589d"},"outputs":[],"source":"tfidf_train_word_match = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\n#train_word_match = df_train.apply(word_match_share, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12288531-649b-32c9-6181-9aa5ffe33aad"},"outputs":[],"source":"#tfidf_train_word_match1 = df_test.apply(tfidf_word_match_share, axis=1, raw=True)\n#x_test['tfidf_word_match'] = tfidf_train_word_match1\nx_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)\nx_test['tfidf_word_match'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02c80674-fb53-27ea-4ff0-d19eaa574b5a"},"outputs":[],"source":"train_word_match = df_train.apply(word_match_share, axis=1, raw=True)\n#x_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)\ntrain_word_match = df_train.apply(word_match_share, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0ab26bc-9b15-539f-3fe0-d355483ac6b3"},"outputs":[],"source":"df_train = df_train.fillna(\"\")\ndf_test =  df_test.fillna(\"\")\nx_train['q1len'] = df_train['question1'].str.len()\nx_train['q2len'] = df_train['question2'].str.len()\nx_train['q1_n_words'] = df_train['question1'].apply(lambda row: len(row.split(\" \")))\nx_train['q2_n_words'] = df_train['question2'].apply(lambda row: len(row.split(\" \")))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"13b4bbbb-03b6-af32-a1dc-7078b7c1969f"},"outputs":[],"source":"x_test['q1len'] = df_test['question1'].str.len()\nx_test['q2len'] = df_test['question2'].str.len()\nx_test['q1_n_words'] = df_test['question1'].apply(lambda row: len(row.split(\" \")))\nx_test['q2_n_words'] = df_test['question2'].apply(lambda row: len(row.split(\" \")))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"956eec3e-350d-4801-a96f-101ba47fec5f"},"outputs":[],"source":"def normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1225b651-1104-965d-ce2e-cca485746406"},"outputs":[],"source":"x_train['word_share'] = df_train.apply(normalized_word_share, axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abd8698b-65f8-d6a4-f9e5-ecc08de53487"},"outputs":[],"source":"x_test['word_share'] = df_test.apply(normalized_word_share, axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8bf3b6e7-5c96-ea65-dabd-8db5c8f80684"},"outputs":[],"source":"# First we create our training and testing data\nx_train['word_match'] = train_word_match\nx_train['tfidf_word_match'] = tfidf_train_word_match\ny_train = df_train['is_duplicate'].values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"513c0120-dab1-79f5-db59-62c7ab87853b"},"outputs":[],"source":"# Finally, we split some of the data off for validation\nfrom sklearn.cross_validation import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, \n                                                      test_size=0.2, random_state=42)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2a325ef-d4b6-c6fc-b08b-5e64cfc9c53e"},"outputs":[],"source":"import xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 40\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 180, watchlist, early_stopping_rounds=30, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e399a771-e971-ebe8-10ff-6410e27c65d5"},"outputs":[],"source":"#x_test.columns = ['word_match', 'q1len','q1_n_words','word_share','tfidf_word_match']\nx_test1 = x_test[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share',\n       'word_match', 'tfidf_word_match']]\n#x_test"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc19e2a2-e6de-502b-4fc9-5f749df1242b"},"outputs":[],"source":"d_test = xgb.DMatrix(x_test1)\np_test = bst.predict(d_test)\n\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = p_test"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8ca9390-4870-40b4-b2c1-0ae43b82d9c8"},"outputs":[],"source":"sub.to_csv('multi_feature_xgb.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}