{"cells":[{"metadata":{},"cell_type":"markdown","source":"We have already done basic level of EDA in part -1\n\nLink -->https://www.kaggle.com/abheeshthmishra/eda-of-quora-question-pairs-part-1"},{"metadata":{},"cell_type":"markdown","source":"Here we will do advance feature extraction"},{"metadata":{"id":"9r763jIis6G4"},"cell_type":"markdown","source":"# EDA: Advanced Feature Extraction.\n"},{"metadata":{"id":"omBYR9Pms6G6","outputId":"b1b032b7-9927-484c-b1eb-2471b0c445e4","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\n\nimport re\nfrom nltk.corpus import stopwords\n#import distance\nfrom nltk.stem import PorterStemmer\n#from bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\n# This package is used for finding longest common subsequence between two strings\n# you can write your own dp code for this\n#import distance\nfrom nltk.stem import PorterStemmer\n#from bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nfrom sklearn.manifold import TSNE\n# Import the Required lib packages for WORD-Cloud generation\n# https://stackoverflow.com/questions/45625434/how-to-install-wordcloud-in-python3-6\nfrom wordcloud import WordCloud, STOPWORDS\nfrom os import path\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can download this file by ruuning EDA part-1 Notebook"},{"metadata":{"id":"FSq37fjBs6HE","trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/12468179/unicodedecodeerror-utf8-codec-cant-decode-byte-0x9c\nif os.path.isfile('../input/quora-assignment/df_fe_without_preprocessing_train.csv'):\n    df = pd.read_csv(\"../input/quora-assignment/df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n    df = df.fillna('')\n    df.head()\nelse:\n    print(\"get df_fe_without_preprocessing_train.csv from drive or run the previous notebook\")","execution_count":null,"outputs":[]},{"metadata":{"id":"iL-KVVkYs6HM","outputId":"45a91ebe-26c8-47ed-f2f3-b9eef725af8a","trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"id":"TWQAzMyWs6HU"},"cell_type":"markdown","source":"<h2>  Preprocessing of Text </h2>"},{"metadata":{"id":"P2_6a1Ils6HV"},"cell_type":"markdown","source":"- Preprocessing:\n    - Removing html tags \n    - Removing Punctuations\n    - Performing stemming\n    - Removing Stopwords\n    - Expanding contractions etc."},{"metadata":{"id":"jgE3QuLws6HW","trusted":true},"cell_type":"code","source":"# To get the results in 4 decemal points\nSAFE_DIV = 0.0001 \n\nSTOP_WORDS = stopwords.words(\"english\")\n\n\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    \n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n               \n    \n    return x\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"LZ4FNZZns6Hc"},"cell_type":"markdown","source":"- Function to Compute and get the features : With 2 parameters of Question 1 and Question 2"},{"metadata":{"id":"FkzcDTs5s6He"},"cell_type":"markdown","source":"<h2>  Advanced Feature Extraction (NLP and Fuzzy Features) </h2>"},{"metadata":{"id":"GSJiSSUEs6He"},"cell_type":"markdown","source":"Definition:\n- __Token__: You get a token by splitting sentence a space\n- __Stop_Word__ : stop words as per NLTK.\n- __Word__ : A token that is not a stop_word\n\n\nFeatures:\n- __cwc_min__ :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 <br>cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n<br>\n<br>\n- __cwc_max__ :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 <br>cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n<br>\n<br>\n- __csc_min__ :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2 <br> csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n<br>\n<br>\n- __csc_max__ :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2<br>csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n<br>\n<br>\n- __ctc_min__ :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2<br>ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n<br>\n<br>\n\n- __ctc_max__ :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2<br>ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n<br>\n<br>\n        \n- __last_word_eq__ :  Check if First word of both questions is equal or not<br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n<br>\n<br>\n\n- __first_word_eq__ :  Check if First word of both questions is equal or not<br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n<br>\n<br>\n        \n- __abs_len_diff__ :  Abs. length difference<br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n<br>\n<br>\n\n- __mean_len__ :  Average Token Length of both Questions<br>mean_len = (len(q1_tokens) + len(q2_tokens))/2\n<br>\n<br>\n\n\n- __fuzz_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n<br>\n\n- __fuzz_partial_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n<br>\n\n\n- __token_sort_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n<br>\n\n\n- __token_set_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n<br>\n\n\n\n\n\n- __longest_substr_ratio__ :  Ratio of length longest common substring to min lenghth of token count of Q1 and Q2<br>longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))\n"},{"metadata":{"id":"h6eap_CRs6Hg","trusted":true},"cell_type":"code","source":"def get_token_features(q1, q2):\n    token_features = [0.0]*10\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n    return token_features\n\n# get the Longest Common sub string\n\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n\n    print(\"token features...\")\n    \n    # Merging Features with dataset\n    \n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n   \n    #Computing Fuzzy Features and Merging with Dataset\n    \n    # do read this blog: http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n    # https://stackoverflow.com/questions/31806695/when-to-use-which-fuzz-function-to-compare-2-strings\n    # https://github.com/seatgeek/fuzzywuzzy\n    print(\"fuzzy features..\")\n\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"id":"drNGg9e-s6Hj","outputId":"3731d0e8-09bc-4add-d1bd-d3a1ca90e3cc","trusted":true},"cell_type":"code","source":"if os.path.isfile('../input/quora-assignment/nlp_features_train.csv'):\n    df = pd.read_csv(\"../input/quora-assignment/nlp_features_train.csv\",encoding='latin-1')\n    df.fillna('')\nelse:\n    print(\"Extracting features for train:\")\n    df = pd.read_csv(\"train.csv\")\n    df = extract_features(df)\n    df.to_csv(\"nlp_features_train.csv\", index=False)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"id":"d923MLB-s6Hr"},"cell_type":"markdown","source":"<h3>3.5.1 Analysis of extracted features </h3>"},{"metadata":{"id":"5vmUIhbhs6Ht"},"cell_type":"markdown","source":"<h4> 3.5.1.1 Plotting Word clouds</h4>"},{"metadata":{"id":"zqriH33Js6Hv"},"cell_type":"markdown","source":"- Creating Word Cloud of Duplicates and Non-Duplicates Question pairs\n- We can observe the most frequent occuring words"},{"metadata":{"id":"M52XmYnNs6Hx","outputId":"39ab593d-d3f0-4c94-8f23-520f8f6da646","trusted":true},"cell_type":"code","source":"df_duplicate = df[df['is_duplicate'] == 1]\ndfp_nonduplicate = df[df['is_duplicate'] == 0]\n\n# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\np = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nn = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\n\nprint (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n\n#Saving the np array into a text file\nnp.savetxt('train_p.txt', p, delimiter=' ', fmt='%s')\nnp.savetxt('train_n.txt', n, delimiter=' ', fmt='%s')","execution_count":null,"outputs":[]},{"metadata":{"id":"9UyKrK_Gs6H6","outputId":"675d9b57-1582-4a67-f611-0b86ae5c5329","trusted":true},"cell_type":"code","source":"# reading the text files and removing the Stop Words:\nd = path.dirname('.')\n\ntextp_w = open(path.join(d, 'train_p.txt')).read()\ntextn_w = open(path.join(d, 'train_n.txt')).read()\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\n\nstopwords.remove(\"no\")\n#stopwords.remove(\"good\")\n#stopwords.remove(\"love\")\nstopwords.remove(\"like\")\n#stopwords.remove(\"best\")\n#stopwords.remove(\"!\")\nprint (\"Total number of words in duplicate pair questions :\",len(textp_w))\nprint (\"Total number of words in non duplicate pair questions :\",len(textn_w))","execution_count":null,"outputs":[]},{"metadata":{"id":"cGK1TbjKs6IB"},"cell_type":"markdown","source":"__ Word Clouds generated from  duplicate pair question's text __"},{"metadata":{"id":"OsV93iiqs6ID","outputId":"9e833bc8-bb52-43db-ca83-0c5e163ed92e","trusted":true},"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords=stopwords)\nwc.generate(textp_w)\nprint (\"Word Cloud for Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"uOOsV5sRs6IJ"},"cell_type":"markdown","source":"__ Word Clouds generated from non duplicate pair question's text __"},{"metadata":{"id":"hww88GIYs6IK","outputId":"ee9ed1d1-6eb5-422a-e80d-6a2c6e035c6b","trusted":true},"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(textn_w),stopwords=stopwords)\n# generate word cloud\nwc.generate(textn_w)\nprint (\"Word Cloud for non-Duplicate Question pairs:\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"q-q3DpiPs6IP"},"cell_type":"markdown","source":"<h4> 3.5.1.2 Pair plot of features ['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'] </h4>"},{"metadata":{"id":"rOuxSDKts6IQ","outputId":"f749f226-2d02-4738-dd7d-e19ca2228217","trusted":true},"cell_type":"code","source":"n = df.shape[0]\nsns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"C8dilaTQs6IW","outputId":"9c2edc06-5fb7-4696-cb34-a3eda2fa79f3","trusted":true},"cell_type":"code","source":"# Distribution of the token_sort_ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"0oxjfCSqs6Ib","outputId":"aa8e4535-e42c-4cd4-d87a-df72859e7df3","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"MIyYxG4Ts6If"},"cell_type":"markdown","source":"<h3>  Visualization </h3>"},{"metadata":{"id":"0QhZBdx1s6Ig","trusted":true},"cell_type":"code","source":"# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndfp_subsampled = df[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = dfp_subsampled['is_duplicate'].values","execution_count":null,"outputs":[]},{"metadata":{"id":"7Agv3hmXs6Ij","outputId":"85e1bea1-0623-4f68-e8a2-1bd83bdcc62b","trusted":true},"cell_type":"code","source":"tsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"id":"qhTHOhWns6Io","outputId":"1e664fdf-7041-4f98-cce2-5ac69618a890","trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"nxFBfJNos6Iv","outputId":"e8f0aa8b-99bf-48c4-c122-e6165af19172","trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne3d = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"id":"relWYEX5s6Iz","outputId":"b471be9c-252a-4bf2-ccb8-0e28ed2fa241","trusted":true},"cell_type":"code","source":"trace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}