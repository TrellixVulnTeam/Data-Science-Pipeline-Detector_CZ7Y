{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Quora Question Pairs\n\n- [Quora Question Pairs challengeQuora Question Pairs challenge](https://www.kaggle.com/c/quora-question-pairs).\n- 쿼라(Quora)는 사용자 커뮤니티이며, 질문자(Seeker)는 궁금한 사항을 올리고, 답변자(Writer)는 해당 사항에 답변하는 형식의 질의 응답형 웹사이트입니다.쿼라(Quora)는 사용자 커뮤니티이며, 질문자(Seeker)는 궁금한 사항을 올리고, 답변자(Writer)는 해당 사항에 답변하는 형식의 질의 응답형 웹사이트입니다.","metadata":{}},{"cell_type":"markdown","source":"## 0. Library 설치","metadata":{}},{"cell_type":"code","source":"# kaggle에 있는 데이터를 불러옵니다.\n# os library는 운영체제(OS)에서 제공되는 여러 기능을 파이썬에서 수행할 수 있게 해줍니다.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2021-10-25T12:17:56.066534Z","iopub.execute_input":"2021-10-25T12:17:56.0668Z","iopub.status.idle":"2021-10-25T12:17:56.119115Z","shell.execute_reply.started":"2021-10-25T12:17:56.066773Z","shell.execute_reply":"2021-10-25T12:17:56.118402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Default 라이브러리\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\nimport math\n\n# visualization 라이브러리\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n# system 라이브러리\nimport sys\nimport csv\nimport gc # garbage collector\nimport sqlite3\nimport os\nfrom os import path\n\n\n# 정규표현식 관련 \nimport re\n\n# 시간 데이터 처리를 위한 라이브러리\nimport time\n\n# Progress bar 관련 라이브러리\nfrom tqdm import tqdm\n\n# NLP 관련 라이브러리\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n# Web Crawling 관련\nfrom bs4 import BeautifulSoup\n\n# 두 문자열 간의 유사성을 측정하는데 사용되는 \nfrom fuzzywuzzy import fuzz\n\n# scikit-learn 라이브러리\nfrom sklearn import model_selection\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import confusion_matrix, normalized_mutual_info_score\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, RandomizedSearchCV \nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import normalize\nfrom sklearn.svm import SVC\n\n\n# 말구름(wordcloud) 그려주는 라이브러리\nfrom wordcloud import WordCloud, STOPWORDS\n\n# PIL: Python Image Library, \n# 설치시 pil이 아닌 image를 사용해야 함. pip install image\nfrom PIL import Image\n\n%matplotlib inline\n\n# Extract Word2Vec vectors\n# https://github.com/explosion/spaCy/issues/1721\n# http://landinghub.visualstudio.com/visual-cpp-build-tools\n\n# 파이썬의 자연어 처리를 라이브러리\nimport spacy\nfrom scipy.sparse import hstack\n\nimport xgboost as xgb\nfrom joblib import dump, load\nfrom collections import Counter, defaultdict\nfrom sqlalchemy import create_engine # Database connection\nfrom mlxtend.classifier import StackingClassifier","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:48:16.379071Z","iopub.execute_input":"2021-10-25T16:48:16.379512Z","iopub.status.idle":"2021-10-25T16:48:16.406115Z","shell.execute_reply.started":"2021-10-25T16:48:16.379449Z","shell.execute_reply":"2021-10-25T16:48:16.405452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %load \"../input/d/elemento/quora-question-pairs/distance/distance/_lcsubstrings.py\"\nfrom array import array\n\ndef lcsubstrings(seq1, seq2, positions=False):    \n\tprint(\"== START lcsubstrings ==\")\n\t'''\n\tpositions=False인 경우\n\t문자열 `seq1`과 `seq2`에서 가장 길게 겹치는 문자열을 찾으면, 문자열 자체를 반환함.\n\t\t>>> lcsubstrings(\"sedentar\", \"dentist\", positions=True)\n\t\t>>> (4, [(2, 0)])\n\n\tpositions=True인 경우\n\t문자열 `seq1`과 `seq2`에서 가장 길게 겹치는 문자열을 찾으면, 그 길이와 위치를 반환함.\n\t\t>>> lcsubstrings(\"sedentar\", \"dentist\")\n\t\t>>> {'dent'}\n\t'''\n\tL1, L2 = len(seq1), len(seq2)\n\tms = []\n\tmlen = last = 0\n\tif L1 < L2:\n\t\tseq1, seq2 = seq2, seq1\n\t\tL1, L2 = L2, L1\n\t\n\tcolumn = array('L', range(L2))\n\t\n\tfor i in range(L1):\n\t\tfor j in range(L2):\n\t\t\told = column[j]\n\t\t\tif seq1[i] == seq2[j]:\n\t\t\t\tif i == 0 or j == 0:\n\t\t\t\t\tcolumn[j] = 1\n\t\t\t\telse:\n\t\t\t\t\tcolumn[j] = last + 1\n\t\t\t\tif column[j] > mlen:\n\t\t\t\t\tmlen = column[j]\n\t\t\t\t\tms = [(i, j)]\n\t\t\t\telif column[j] == mlen:\n\t\t\t\t\tms.append((i, j))\n\t\t\telse:\n\t\t\t\tcolumn[j] = 0\n\t\t\tlast = old\n\t\n\tif positions:\n\t\treturn (mlen, tuple((i - mlen + 1, j - mlen + 1) for i, j in ms if ms))\n\treturn set(seq1[i - mlen + 1:i + 1] for i, _ in ms if ms)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:17:59.181831Z","iopub.execute_input":"2021-10-25T12:17:59.182078Z","iopub.status.idle":"2021-10-25T12:17:59.194497Z","shell.execute_reply.started":"2021-10-25T12:17:59.182051Z","shell.execute_reply":"2021-10-25T12:17:59.193778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. 문제점\n## 1.1 프로젝트 설명  \n- 문제는 유사한 질문들이 많이 올라오기 때문에 질문자(Seeker)는 좋은 답변을 찾기가 힘들고, 답변자(writer)는 사실상 동일한 답변에 대해 다시 답변을 해야 하기 때문에 부담이 됩니다. 문제는 유사한 질문들이 많이 올라오기 때문에 질문자(Seeker)는 좋은 답변을 찾기가 힘들고, 답변자(writer)는 사실상 동일한 답변에 대해 다시 답변을 해야 하기 때문에 부담이 됩니다. \n- 현재 Quora는 Random Forest 모델을 이용하여 중복되는 질문이 동일한 질문인지 식별하고 있습니다. 우리는 더 나은 사용자 경험을 제공하기 위해 질문 쌍들이 중복되는지 해결하여야 합니다.현재 Quora는 Random Forest 모델을 이용하여 중복되는 질문이 동일한 질문인지 식별하고 있습니다. 우리는 더 나은 사용자 경험을 제공하기 위해 질문 쌍들이 중복되는지 해결하여야 합니다.- 문제는 유사한 질문들이 많이 올라오기 때문에 질문자(Seeker)는 좋은 답변을 찾기가 힘들고, 답변자(writer)는 사실상 동일한 답변에 대해 다시 답변을 해야 하기 때문에 부담이 됩니다. 문제는 유사한 질문들이 많이 올라오기 때문에 질문자(Seeker)는 좋은 답변을 찾기가 힘들고, 답변자(writer)는 사실상 동일한 답변에 대해 다시 답변을 해야 하기 때문에 부담이 됩니다.\n- 정답(ground truth)와 예측값(predicted valuue)간의 log loss 값으로 평가를 진행합니다.정답(ground truth)와 예측값(predicted valuue)간의 log loss 값으로 평가를 진행합니다.### 1. 프로젝트 설명  \n- 문제는 유사한 질문들이 많이 올라오기 때문에 질문자(Seeker)는 좋은 답변을 찾기가 힘들고, 답변자(writer)는 사실상 동일한 답변에 대해 다시 답변을 해야 하기 때문에 부담이 됩니다. 문제는 유사한 질문들이 많이 올라오기 때문에 질문자(Seeker)는 좋은 답변을 찾기가 힘들고, 답변자(writer)는 사실상 동일한 답변에 대해 다시 답변을 해야 하기 때문에 부담이 됩니다. \n- 현재 Quora는 Random Forest 모델을 이용하여 중복되는 질문이 동일한 질문인지 식별하고 있습니다. 우리는 더 나은 사용자 경험을 제공하기 위해 질문 쌍들이 중복되는지 해결하여야 합니다.현재 Quora는 Random Forest 모델을 이용하여 중복되는 질문이 동일한 질문인지 식별하고 있습니다. 우리는 더 나은 사용자 경험을 제공하기 위해 질문 쌍들이 중복되는지 해결하여야 합니다.- 문제는 유사한 질문들이 많이 올라오기 때문에 질문자(Seeker)는 좋은 답변을 찾기가 힘들고, 답변자(writer)는 사실상 동일한 답변에 대해 다시 답변을 해야 하기 때문에 부담이 됩니다. 문제는 유사한 질문들이 많이 올라오기 때문에 질문자(Seeker)는 좋은 답변을 찾기가 힘들고, 답변자(writer)는 사실상 동일한 답변에 대해 다시 답변을 해야 하기 때문에 부담이 됩니다.\n  \n## 1.2 평가 방법\n- 정답(ground truth)와 예측값(predicted valuue)간의 log loss 값으로 평가를 진행합니다.정답(ground truth)와 예측값(predicted valuue)간의 log loss 값으로 평가를 진행합니다.\n\n## 1.3 관련 링크\n- Source : https://www.kaggle.com/c/quora-question-pairs\n- Discussions : https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb/comments\n- Kaggle Winning Solution and other approaches: https://www.dropbox.com/sh/93968nfnrzh8bp5/AACZdtsApc1QSTQc7X0H3QZ5a?dl=0\n- Blog 1 : https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning\n- Blog 2 : https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30","metadata":{"execution":{"iopub.status.busy":"2021-09-25T06:07:34.153362Z","iopub.execute_input":"2021-09-25T06:07:34.153675Z","iopub.status.idle":"2021-09-25T06:07:34.158535Z","shell.execute_reply.started":"2021-09-25T06:07:34.153643Z","shell.execute_reply":"2021-09-25T06:07:34.157633Z"}}},{"cell_type":"markdown","source":"# 2. Machine Learning 관련 내용\n## 2.1 파일 설명\n<pre>\n- training을 위한 파일\n- 파일 크기: 60MB \n- 열(column) 정보 : id, qid1, qid2, question1, question2, is_duplicate\n- 행의 수: 404,290\n</pre>\n## 2.2 train.csv의 열(col) 정보\n<pre>\n- question1 및 question2 : 질문을 포함하는 문자열 1, 2\n- qid1 및 qid2 : question1 및 question2의 구분자\n- id : 질문 쌍 구분자\n- is_duplicate : 중복 여부에 대한 labeling\n</pre>","metadata":{}},{"cell_type":"markdown","source":"# 3. EDA(Exploratory Data Analysis)\n## 3.1 data 가져오기","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")\nprint(df.info())\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:17:59.196122Z","iopub.execute_input":"2021-10-25T12:17:59.196829Z","iopub.status.idle":"2021-10-25T12:18:01.895455Z","shell.execute_reply.started":"2021-10-25T12:17:59.19678Z","shell.execute_reply":"2021-10-25T12:18:01.894575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.1 데이터 분포\n- 유사, 비유사 쌍의 수와 비율","metadata":{}},{"cell_type":"code","source":"total_cnt = len(df)\nsimilar_cnt = df.groupby(\"is_duplicate\")['id'].count()[0]\nnot_similar_cnt = df.groupby(\"is_duplicate\")['id'].count()[1]\n\nprint(\"전체: %d \\n비유사(is_duplicate=0): %d \\n유사(is_duplicate=1): %d\" % (total_cnt, similar_cnt, not_similar_cnt))\nprint('유사 쌍의 비율 (is_duplicate = 1): %2.2f%%' % (similar_cnt/total_cnt*100))\nprint('비유사 쌍의 비율 (is_duplicate = 0): %2.2f%%' % (not_similar_cnt/total_cnt*100))","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:18:01.896799Z","iopub.execute_input":"2021-10-25T12:18:01.897014Z","iopub.status.idle":"2021-10-25T12:18:01.921739Z","shell.execute_reply.started":"2021-10-25T12:18:01.896985Z","shell.execute_reply":"2021-10-25T12:18:01.920804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2 질문의 중복 확인","metadata":{}},{"cell_type":"code","source":"# 질문의 중복을 확인하기 위해 qid는 1번이상 사용되므로, 이를 이용하여 질문의 중복정도를 체크한다.\nqids = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n\n# 전체 질문의 수 \nunique_qs = len(np.unique(qids))\n\n# 두번이상 나온 질문\nqs_morethan_onetime = np.sum(qids.value_counts() > 1)\nqs_morethan_onetime_rate = qs_morethan_onetime/unique_qs*100\n\nprint('전체 질문의 수: %d' % unique_qs)\nprint('두 번 이상 반복된 질문의 수: %d, 비율: %2.2f%%' % (qs_morethan_onetime, qs_morethan_onetime_rate))\nprint('가장 많이 반복되는 질문의 반복 횟수: %d' % max(qids.value_counts())) \n\nq_vals = qids.value_counts()\nq_vals = q_vals.values","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:18:01.922927Z","iopub.execute_input":"2021-10-25T12:18:01.923167Z","iopub.status.idle":"2021-10-25T12:18:02.805803Z","shell.execute_reply.started":"2021-10-25T12:18:01.923139Z","shell.execute_reply":"2021-10-25T12:18:02.805114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.3 중복 확인","metadata":{}},{"cell_type":"code","source":"pair_duplicates = df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\nprint (\"Number of duplicate questions\", (pair_duplicates).shape[0] - df.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:18:02.807106Z","iopub.execute_input":"2021-10-25T12:18:02.80762Z","iopub.status.idle":"2021-10-25T12:18:03.085455Z","shell.execute_reply.started":"2021-10-25T12:18:02.807569Z","shell.execute_reply":"2021-10-25T12:18:03.084616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.4 각 질문의 발생 수","metadata":{}},{"cell_type":"code","source":"print ('가장 많이 반복된 질문의 빈도: %d' % max(qids.value_counts())) \n\nplt.figure(figsize=(20, 10))\nplt.hist(qids.value_counts(), bins=160)\nplt.yscale('log', nonpositive='clip')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel(\"Question occurences counts\")\nplt.ylabel('Questions counts')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:18:03.086895Z","iopub.execute_input":"2021-10-25T12:18:03.087638Z","iopub.status.idle":"2021-10-25T12:18:04.589185Z","shell.execute_reply.started":"2021-10-25T12:18:03.08759Z","shell.execute_reply":"2021-10-25T12:18:04.588586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.5 NULL값 확인","metadata":{}},{"cell_type":"code","source":"# null겂을 가진 행이 있는지 확인\nnan_rows = df[df.isnull().any(1)]\nprint (nan_rows)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:18:04.590259Z","iopub.execute_input":"2021-10-25T12:18:04.590594Z","iopub.status.idle":"2021-10-25T12:18:04.702595Z","shell.execute_reply.started":"2021-10-25T12:18:04.590566Z","shell.execute_reply":"2021-10-25T12:18:04.701893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# question2에서 2개의 행에 Nan이 있으므로 na를 채우고, 확인\n# Filling the null values with ' '\ndf = df.fillna('')\nnan_rows = df[df.isnull().any(1)]\nprint (nan_rows)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:18:04.705556Z","iopub.execute_input":"2021-10-25T12:18:04.705803Z","iopub.status.idle":"2021-10-25T12:18:04.971792Z","shell.execute_reply.started":"2021-10-25T12:18:04.705774Z","shell.execute_reply":"2021-10-25T12:18:04.970952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 기본적인 특성 추출\n - ____freq_qid1____ = qid1의 빈도\n - ____freq_qid2____ = qid2의 빈도\n - ____q1len____ = q1의 길이\n - ____q2len____ = q2의 길이\n - ____q1_n_words____ = question 1의 단어 수\n - ____q2_n_words____ = question 2의 단어수\n - ____word_Common____ = question1과 question2에서 동일하게 나타나는 단어 수\n - ____word_Total____ = Question1의 전체 단어수와 Question2의 전체 단어 수의 합\n - ____word_share____ = word_common / word_Total\n - ____freq_q1+freq_q2____ = qid1과 qid2의 빈도 합계\n - ____freq_q1-freq_q2____ = qid1과 qid2의 빈도의 차이의 절대값","metadata":{}},{"cell_type":"code","source":"if os.path.isfile('../input/d/elemento/quora-question-pairs/df_fe_without_preprocessing_train.csv'):\n    print(\"os.path.isfile: TRUE\")\n    df = pd.read_csv(\"../input/d/elemento/quora-question-pairs/df_fe_without_preprocessing_train.csv\", encoding='latin-1')\nelse:\n    print(\"os.path.isfile:  FALSE\")\n    df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \n    df['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\n    df['q1len'] = df['question1'].str.len() \n    df['q2len'] = df['question2'].str.len()\n    df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n    df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n\n    def normalized_word_Common(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * len(w1 & w2)\n    df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n\n    def normalized_word_Total(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * (len(w1) + len(w2))\n    df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n\n    def normalized_word_share(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n    df['word_share'] = df.apply(normalized_word_share, axis=1)\n\n    df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n    df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n\n    df.to_csv(\"df_fe_without_preprocessing_train.csv\", index=False)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:19:29.729311Z","iopub.execute_input":"2021-10-25T13:19:29.729625Z","iopub.status.idle":"2021-10-25T13:19:31.402411Z","shell.execute_reply.started":"2021-10-25T13:19:29.729594Z","shell.execute_reply":"2021-10-25T13:19:31.401564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['question1'][0])\nprint(df['question2'][0])","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:20:06.440553Z","iopub.execute_input":"2021-10-25T13:20:06.440861Z","iopub.status.idle":"2021-10-25T13:20:06.447033Z","shell.execute_reply.started":"2021-10-25T13:20:06.440831Z","shell.execute_reply":"2021-10-25T13:20:06.446062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.1 추출된 특성의 분석","metadata":{}},{"cell_type":"code","source":"print (\"question1에서 최소 단어 수 : \" , min(df['q1_n_words']))\nprint (\"question2에서 최소 단어 수 : \" , min(df['q2_n_words']))\nprint (\"question1에서 최소 단어 수를 갖는 질문의 수 :\", df[df['q1_n_words']== 1].shape[0])\nprint (\"question2에서 최소 단어 수를 갖는 질문의 수 :\", df[df['q2_n_words']== 1].shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:18:07.422987Z","iopub.execute_input":"2021-10-25T12:18:07.423185Z","iopub.status.idle":"2021-10-25T12:18:07.553491Z","shell.execute_reply.started":"2021-10-25T12:18:07.423161Z","shell.execute_reply":"2021-10-25T12:18:07.552851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[0:].head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:18:07.554601Z","iopub.execute_input":"2021-10-25T12:18:07.554836Z","iopub.status.idle":"2021-10-25T12:18:07.575399Z","shell.execute_reply.started":"2021-10-25T12:18:07.554809Z","shell.execute_reply":"2021-10-25T12:18:07.574407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.1.1 word_share(유사도) 특성의 분석\n- is_duplicate가 0이면, word_sahre가 0.1-0.2에 몰려있고, is_duplicate가 1인 경우에는 0.2-0.3 구간에 밀집되어 있는 것을 알 수 있습니다.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_share', data = df[0:])\n\n\nplt.subplot(1,2,2)\nsns.histplot(df[df['is_duplicate'] == 1.0]['word_share'][0:] , label = \"1\", color = '#f49093')\nsns.histplot(df[df['is_duplicate'] == 0.0]['word_share'][0:] , label = \"0\" , color = '#9999ff')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:27:54.45058Z","iopub.execute_input":"2021-10-25T12:27:54.45087Z","iopub.status.idle":"2021-10-25T12:27:56.645286Z","shell.execute_reply.started":"2021-10-25T12:27:54.450839Z","shell.execute_reply":"2021-10-25T12:27:56.644441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.1.2 word_Common 특성\n- is_duplicate가 0이면, 0-4에 몰려있고, is_duplicate가 1인 경우에는 4-8 구간에 밀집되어 있는 것을 알 수 있음.\n- 특성상 word_share와 비슷한 추세를 나타내고 있음.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_Common', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.histplot(df[df['is_duplicate'] == 1.0]['word_Common'][0:], kde=\"True\", label = \"1\", color = '#f49093')\nsns.histplot(df[df['is_duplicate'] == 0.0]['word_Common'][0:], kde=\"True\", label = \"0\", color = '#9999ff')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:27:38.621445Z","iopub.execute_input":"2021-10-25T12:27:38.621758Z","iopub.status.idle":"2021-10-25T12:27:43.567985Z","shell.execute_reply.started":"2021-10-25T12:27:38.621725Z","shell.execute_reply":"2021-10-25T12:27:43.567069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 전처리\n    - html 태그 제거\n    - 구두점(Punctuations) 제거\n    - 형태소 분석\n    - 불용어(Stopwords) 제거\n    - 확장 및 수축등","metadata":{}},{"cell_type":"code","source":"# english 불용어(stopwords) 리스트 \nfrom nltk.corpus import stopwords\n# To get the results in 4 decimal points\nSAFE_DIV = 0.0001 \nSTOP_WORDS = stopwords.words(\"english\")\nprint(STOP_WORDS, len(STOP_WORDS))\n# 179개의 불용어(STOP_WORD)가 있음을 알 수 있다.","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:35:38.13898Z","iopub.execute_input":"2021-10-25T12:35:38.139497Z","iopub.status.idle":"2021-10-25T12:35:38.145155Z","shell.execute_reply.started":"2021-10-25T12:35:38.139449Z","shell.execute_reply":"2021-10-25T12:35:38.1443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 전각문자, 기호문자, 줄임 문자 등을 명확한 단어로 대체\ndef preprocess(x):\n    print(\"== START preprocess x: %s\" % x)\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n       .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n       .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n       .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n       .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n       .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n       .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n            \n    print(\"== END preprocess x: %s\" % x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-10-25T12:38:26.265046Z","iopub.execute_input":"2021-10-25T12:38:26.265803Z","iopub.status.idle":"2021-10-25T12:38:26.276892Z","shell.execute_reply.started":"2021-10-25T12:38:26.265761Z","shell.execute_reply":"2021-10-25T12:38:26.275733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5 고급 특성 추출 (NLP와 Fuzzy 특성)\n\n정의:\n- __Token__: 공백 문자로 문장을 분할하여 Token을 얻을 수 있습니다.\n- __Stop_Word__ : NLTK에 따른 stopword.\n- __Word__ : stopword가 아닌 token\n\n\nFeatures:\n- __cwc_min__ : Q1과 Q2의 최단 길이 단어 수에 대한 common_word_count의 비율 <br>cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n<br>\n- __cwc_max__ : Q1과 Q2의 최장 길이 단어 수에 대한 common_word_count의 비율 <br>cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n<br>\n- __csc_min__ : Q1과 Q2의 최단 길이 stop ward 수에 대한 common_stop_count의 비율 <br> csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n<br>\n- __csc_max__ : Q1과 Q2의 최장 길이 stop ward 수에 대한 common_stop_count의 비율 <br>csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n<br>\n- __ctc_min__ : Q1과 Q2의 최단 길이 token 수에 대한  common_token_count의 비율 <br>ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n<br>\n- __ctc_max__ : Q1과 Q2의 최장 길이 token 수에 대한  common_token_count의 비율 <br>ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n<br>\n- __last_word_eq__ : 두 질문의 마지막 단어가 같은지 확인 <br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n<br>\n- __first_word_eq__ : 두 질문의 첫번째 단어가 같은지 확인 <br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n<br>\n- __abs_len_diff__ : 차이의 절대값 <br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n<br>\n- __mean_len__ :  두 질문의 평균 Token 길이 <br>mean_len = (len(q1_tokens) + len(q2_tokens))/2\n<br>\n- __fuzz_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage<br>\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __fuzz_partial_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage<br>\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __token_sort_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage<br>\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __token_set_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage<br>\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __longest_substr_ratio__ : Q1 및 Q2의 토큰 개수의 최소 길이에 대한 가장 긴 공통 부분 문자열 길이의 비율 <br>longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))","metadata":{}},{"cell_type":"code","source":"def get_token_features(q1, q2):\n    print(\"== START get_token_features q1: %s || q2: %s\" % (q1, q2))\n    token_features = [0.0]*10\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    \n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    # Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    # Average Token Length of both Questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n    return token_features\n\n# Get the Longest Common sub string\ndef get_longest_substr_ratio(a, b):\n    print(\"== START get_longest_substr_ratio a: %s, b: %s\" % (a, b))\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    print(\"== START extract_features df ==\")\n    print(df)\n    # Pre-processing each question\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n\n    print(\"token features...\")\n    \n    # Merging Features with dataset\n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n   \n    # Computing Fuzzy Features and Merging with Dataset\n    # Do read this blog: http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n    # https://stackoverflow.com/questions/31806695/when-to-use-which-fuzz-function-to-compare-2-strings\n    # https://github.com/seatgeek/fuzzywuzzy\n    print(\"fuzzy features..\")\n\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:17:47.418421Z","iopub.execute_input":"2021-10-25T13:17:47.41876Z","iopub.status.idle":"2021-10-25T13:17:47.448784Z","shell.execute_reply.started":"2021-10-25T13:17:47.418725Z","shell.execute_reply":"2021-10-25T13:17:47.447868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.isfile('../input/d/elemento/quora-question-pairs/nlp_features_train.csv'):\n    print(\"os.path.isfile: True \")\n    df = pd.read_csv(\"../input/d/elemento/quora-question-pairs/nlp_features_train.csv\", encoding='latin-1')\n    df.fillna('')\nelse:\n    print(\"os.path.isfile: False\")\n    print(\"Extracting features for train:\")\n    df = pd.read_csv(\"train.csv\")\n    df = extract_features(df)\n    df.to_csv(\"nlp_features_train.csv\", index=False)\n\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:35:38.784192Z","iopub.execute_input":"2021-10-25T13:35:38.78497Z","iopub.status.idle":"2021-10-25T13:35:41.504173Z","shell.execute_reply.started":"2021-10-25T13:35:38.784878Z","shell.execute_reply":"2021-10-25T13:35:41.503574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.1 추출된 특성의 분석\n#### 3.5.1.1 Word clouds 도식화하기\n- 중복 및 비중복 질문 쌍의 단어 구름 만들기\n- 중복 질문 쌍과 비중복 질문 쌍에서 가장 자주 등장하는 단어를 관찰할 수 있다.\n- 아래의 단어 구름을 기반으로, 자주 발생하는 중복 질문 쌍, 중복되지 않은 질문 쌍 및 모든 질문 쌍에서 자주 나타나는 특정 단어가 있음을 알 수 있다.\n- 이러한 관찰은 단어의 빈도를 계산하는 것이 우리에게 중요한 기능을 제공할 수 있다.","metadata":{}},{"cell_type":"code","source":"df_duplicate = df[df['is_duplicate'] == 1]\ndfp_nonduplicate = df[df['is_duplicate'] == 0]\n\n# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\np = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nn = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\n\nprint (\"== 클래스 1에 나타난 데이터 포인터의 수 (duplicate pairs) :\",len(p))\nprint (\"== 클래스 0에 나타난 데이터 포인터의 수 (non duplicate pairs) :\",len(n))\n\n# Saving the np array into a text file\nnp.savetxt('train_p.txt', p, delimiter=' ', fmt='%s')\nnp.savetxt('train_n.txt', n, delimiter=' ', fmt='%s')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:27:44.276638Z","iopub.execute_input":"2021-10-25T13:27:44.277374Z","iopub.status.idle":"2021-10-25T13:27:46.176598Z","shell.execute_reply.started":"2021-10-25T13:27:44.277326Z","shell.execute_reply":"2021-10-25T13:27:46.175689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 텍스트 파일을 읽어들이고, stop word 제거\nd = path.dirname('.')\n\ntextp_w = open(path.join(d, 'train_p.txt')).read()\ntextn_w = open(path.join(d, 'train_n.txt')).read()\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\n\nstopwords.remove(\"no\")\n# stopwords.remove(\"good\")\n# stopwords.remove(\"love\")\nstopwords.remove(\"like\")\n# stopwords.remove(\"best\")\n# stopwords.remove(\"!\")\nprint (\"Total number of words in duplicate pair questions :\",len(textp_w))\nprint (\"Total number of words in non duplicate pair questions :\",len(textn_w))","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:28:10.618331Z","iopub.execute_input":"2021-10-25T13:28:10.618648Z","iopub.status.idle":"2021-10-25T13:28:10.817952Z","shell.execute_reply.started":"2021-10-25T13:28:10.618615Z","shell.execute_reply":"2021-10-25T13:28:10.817284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Clouds 생성\nplt.figure(figsize = (12, 10))\n\nwc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords=stopwords)\nwc.generate(textp_w)\nprint (\"중복 질문 쌍에 대한 Word Cloud\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:30:50.171696Z","iopub.execute_input":"2021-10-25T13:30:50.172015Z","iopub.status.idle":"2021-10-25T13:31:00.295412Z","shell.execute_reply.started":"2021-10-25T13:30:50.171981Z","shell.execute_reply":"2021-10-25T13:31:00.294488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Clouds generated from non duplicate pair question's text\nplt.figure(figsize = (12, 10))\nwc = WordCloud(background_color=\"white\", max_words=len(textn_w),stopwords=stopwords)\nwc.generate(textn_w)\nprint (\"중복되지 않는 질문 쌍에 대한 Word Cloud\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:31:16.805607Z","iopub.execute_input":"2021-10-25T13:31:16.805902Z","iopub.status.idle":"2021-10-25T13:31:40.30187Z","shell.execute_reply.started":"2021-10-25T13:31:16.805872Z","shell.execute_reply":"2021-10-25T13:31:40.300788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.5.1.2 ['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'] 특성 도식화하기","metadata":{}},{"cell_type":"code","source":"n = df.shape[0]\nsns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:35:44.640467Z","iopub.execute_input":"2021-10-25T13:35:44.641177Z","iopub.status.idle":"2021-10-25T13:40:27.637885Z","shell.execute_reply.started":"2021-10-25T13:35:44.641143Z","shell.execute_reply":"2021-10-25T13:40:27.636664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# token_sort_ratio의 분포\nplt.figure(figsize=(16, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:41:24.23829Z","iopub.execute_input":"2021-10-25T13:41:24.238905Z","iopub.status.idle":"2021-10-25T13:41:28.336757Z","shell.execute_reply.started":"2021-10-25T13:41:24.238865Z","shell.execute_reply":"2021-10-25T13:41:28.335594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fuzz_ratio의 분포\nplt.figure(figsize=(16, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T13:50:20.836223Z","iopub.execute_input":"2021-10-25T13:50:20.836776Z","iopub.status.idle":"2021-10-25T13:50:24.842613Z","shell.execute_reply.started":"2021-10-25T13:50:20.836733Z","shell.execute_reply":"2021-10-25T13:50:24.841857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.2 시각화하기","metadata":{}},{"cell_type":"code","source":"%%time\n# 15가지 기능에 대한 차원 축소를 위해 TSNE 사용\n# 3차원으로 정리\nfrom sklearn.preprocessing import MinMaxScaler\n\ndfp_subsampled = df[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[[\n    'cwc_min', 'cwc_max', 'csc_min', 'csc_max', 'ctc_min', 'ctc_max', 'last_word_eq', \n    'first_word_eq', 'abs_len_diff', 'mean_len', 'token_set_ratio', 'token_sort_ratio',  \n    'fuzz_ratio', 'fuzz_partial_ratio', 'longest_substr_ratio']\n])\ny = dfp_subsampled['is_duplicate'].values","metadata":{"execution":{"iopub.status.busy":"2021-10-25T14:13:07.300974Z","iopub.execute_input":"2021-10-25T14:13:07.301265Z","iopub.status.idle":"2021-10-25T14:13:07.31833Z","shell.execute_reply.started":"2021-10-25T14:13:07.301236Z","shell.execute_reply":"2021-10-25T14:13:07.317501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-25T14:17:23.483688Z","iopub.execute_input":"2021-10-25T14:17:23.484044Z","iopub.status.idle":"2021-10-25T14:17:49.05181Z","shell.execute_reply.started":"2021-10-25T14:17:23.484014Z","shell.execute_reply":"2021-10-25T14:17:49.051028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\nsns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,\n   palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T14:23:12.697855Z","iopub.execute_input":"2021-10-25T14:23:12.698167Z","iopub.status.idle":"2021-10-25T14:23:13.245962Z","shell.execute_reply.started":"2021-10-25T14:23:12.698134Z","shell.execute_reply":"2021-10-25T14:23:13.245277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.manifold import TSNE\ntsne3d = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-25T14:23:16.931532Z","iopub.execute_input":"2021-10-25T14:23:16.932391Z","iopub.status.idle":"2021-10-25T14:24:38.10357Z","shell.execute_reply.started":"2021-10-25T14:23:16.93235Z","shell.execute_reply":"2021-10-25T14:24:38.102888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T14:24:38.300742Z","iopub.execute_input":"2021-10-25T14:24:38.301214Z","iopub.status.idle":"2021-10-25T14:24:38.507392Z","shell.execute_reply.started":"2021-10-25T14:24:38.301179Z","shell.execute_reply":"2021-10-25T14:24:38.506602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.6 tfidf 가중 word-vector로 텍스트 데이터 특징화하기\n- 앞서 분석에서 단어에 따라'클래스 0' 또는 클래스 1에서 더 자주 발생한다는 것을 알 수 있었음.\n- 그래서 이를 어떻게 활용하기 위해 **GLoVE**를 사용할 수 있다.\n- GLoVE는 단어 간의 **semantic relationships**를 유지하면서 단어를 벡터로 변환하는 데 도움이 된다는 점에서 W2V와 매우 유사합니다. 유일한 차이점은 작동 방식에 있습니다.","metadata":{}},{"cell_type":"code","source":"# 디코딩 문제 방지하기\ndf = pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T14:29:01.335388Z","iopub.execute_input":"2021-10-25T14:29:01.336143Z","iopub.status.idle":"2021-10-25T14:29:03.13753Z","shell.execute_reply.started":"2021-10-25T14:29:01.33609Z","shell.execute_reply":"2021-10-25T14:29:03.136557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 질문을 unicode로 인코딩하기\n# https://stackoverflow.com/a/6812069\ndf['question1'] = df['question1'].apply(lambda x: str(x))\ndf['question2'] = df['question2'].apply(lambda x: str(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T14:29:04.609967Z","iopub.execute_input":"2021-10-25T14:29:04.610272Z","iopub.status.idle":"2021-10-25T14:29:04.962491Z","shell.execute_reply.started":"2021-10-25T14:29:04.610245Z","shell.execute_reply":"2021-10-25T14:29:04.961536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 텍스트 병합\nquestions = list(df['question1']) + list(df['question2'])\nprint(type(questions))\n\ntfidf = TfidfVectorizer(lowercase=False, )\ntfidf.fit_transform(questions)\n\n# dict key:word and value:tf-idf score\nword2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","metadata":{"execution":{"iopub.status.busy":"2021-10-25T14:30:16.588956Z","iopub.execute_input":"2021-10-25T14:30:16.589654Z","iopub.status.idle":"2021-10-25T14:30:32.099974Z","shell.execute_reply.started":"2021-10-25T14:30:16.589613Z","shell.execute_reply":"2021-10-25T14:30:32.098803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- TF-IDF 점수를 찾고, 이 점수에 따라 각 질문을 word2vec 벡터의 가중 평균으로 변환합니다.\n- 여기서 \"Spacy\"와 함께 사전 훈련된 GLOVE 모델을 사용합니다. https://spacy.io/usage/vectors-similarity\n- 위키피디아에서 학습한 모델로 성능이 강력함.","metadata":{}},{"cell_type":"code","source":"%%time\n\nprint(\"== spacy.load ==\")\n# en_vectors_web_lg에는 100만 개 이상의 고유 벡터가 포함되어 있습니다.\nnlp = spacy.load('en_core_web_sm')\nvecs1 = []\n\nprint(\"== for qu1 in tqdm(list(df['question1'])): ==\")\n# https://github.com/noamraph/tqdm\n# tqdm is used to print the progress bar\nfor qu1 in tqdm(list(df['question1'])):\n    doc1 = nlp(qu1) \n    # 384 is the number of dimensions of vectors \n    mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])    \n    for word1 in doc1:\n        # Word2Vec\n        vec1 = word1.vector\n        # Fetch df score\n        try: idf = word2tfidf[str(word1)]\n        except: idf = 0\n        # Compute final vec\n        mean_vec1 += vec1 * idf\n    mean_vec1 = mean_vec1.mean(axis=0)\n    vecs1.append(mean_vec1)\ndf['q1_feats_m'] = list(vecs1)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T14:37:07.941685Z","iopub.execute_input":"2021-10-25T14:37:07.941976Z","iopub.status.idle":"2021-10-25T15:39:20.728661Z","shell.execute_reply.started":"2021-10-25T14:37:07.941949Z","shell.execute_reply":"2021-10-25T15:39:20.727829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nvecs2 = []\nfor qu2 in tqdm(list(df['question2'])):\n    doc2 = nlp(qu2) \n    mean_vec2 = np.zeros([len(doc1), len(doc2[0].vector)])\n    for word2 in doc2:\n        # Word2Vec\n        vec2 = word2.vector\n        # Fetch df score\n        try: idf = word2tfidf[str(word2)]\n        except: idf = 0\n        # Compute final vec\n        mean_vec2 += vec2 * idf\n    mean_vec2 = mean_vec2.mean(axis=0)\n    vecs2.append(mean_vec2)\ndf['q2_feats_m'] = list(vecs2)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:41:42.642578Z","iopub.execute_input":"2021-10-25T15:41:42.642918Z","iopub.status.idle":"2021-10-25T16:42:10.647626Z","shell.execute_reply.started":"2021-10-25T15:41:42.642885Z","shell.execute_reply":"2021-10-25T16:42:10.646139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepro_features_train.csv(간단한 전처리 기능)\nif os.path.isfile('../input/d/elemento/quora-question-pairs/nlp_features_train.csv'):\n    print(\"os.path.isfile True - nlp_features_train.csv\")\n    dfnlp = pd.read_csv(\"../input/d/elemento/quora-question-pairs/nlp_features_train.csv\",encoding='latin-1')\nelse:\n    print(\"os.path.isfile False - nlp_features_train.csv\")\n    print(\"Download nlp_features_train.csv from drive or run previous notebook\")\n\n# nlp_features_train.csv(NLP 기능)    \nif os.path.isfile('../input/d/elemento/quora-question-pairs/df_fe_without_preprocessing_train.csv'):\n    print(\"os.path.isfile True - df_fe_without_preprocessing_train.csv\")\n    dfppro = pd.read_csv(\"../input/d/elemento/quora-question-pairs/df_fe_without_preprocessing_train.csv\",encoding='latin-1')\nelse:\n    print(\"os.path.isfile False - df_fe_without_preprocessing_train.csv\")\n    print(\"Download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:42:31.243392Z","iopub.execute_input":"2021-10-25T16:42:31.24433Z","iopub.status.idle":"2021-10-25T16:42:36.05992Z","shell.execute_reply.started":"2021-10-25T16:42:31.244279Z","shell.execute_reply":"2021-10-25T16:42:36.058962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 최종 특징 벡터 또는 x_i 4개의 부분으로 구성됩니다. 첫 번째 부분은 원본 데이터 세트에서 제공한 기능과 **기본 기능 추출 섹션(3.3)**에서 엔지니어링한 기능으로 구성됩니다.\n- 2부는 **Advanced Feature Extraction Section(3.5)**에서 엔지니어링한 15개의 기능으로 구성됩니다.","metadata":{}},{"cell_type":"code","source":"df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\ndf2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\ndf3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\ndf3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\ndf3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:42:41.218411Z","iopub.execute_input":"2021-10-25T16:42:41.218751Z","iopub.status.idle":"2021-10-25T16:43:36.500651Z","shell.execute_reply.started":"2021-10-25T16:42:41.21872Z","shell.execute_reply":"2021-10-25T16:43:36.499592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"nlp 특성의 데이터 프레임\")\nprint(\"nlp 데이터 프레임에 존재하는  특성의 수 :\", df1.shape[1])\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:43:36.50247Z","iopub.execute_input":"2021-10-25T16:43:36.502705Z","iopub.status.idle":"2021-10-25T16:43:36.529302Z","shell.execute_reply.started":"2021-10-25T16:43:36.502679Z","shell.execute_reply":"2021-10-25T16:43:36.528608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"전처리 전의 데이터\")\nprint(\"전처리 전의 데이터 프레임에 존재하는 특성의 수 :\", df2.shape[1])\ndf2.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:43:36.530293Z","iopub.execute_input":"2021-10-25T16:43:36.530859Z","iopub.status.idle":"2021-10-25T16:43:36.551373Z","shell.execute_reply.started":"2021-10-25T16:43:36.530826Z","shell.execute_reply":"2021-10-25T16:43:36.550555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Questions 1의 tfidf-weighted word2vec\")\nprint(\"question1 w2v 데이터 프레임에 존재하는 특성의 수 :\", df3_q1.shape[1])\ndf3_q1.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:43:36.553003Z","iopub.execute_input":"2021-10-25T16:43:36.553241Z","iopub.status.idle":"2021-10-25T16:43:36.58271Z","shell.execute_reply.started":"2021-10-25T16:43:36.553214Z","shell.execute_reply":"2021-10-25T16:43:36.581677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Questions 2의 tfidf-weighted word2vec\")\nprint(\"question2 w2v 데이터 프레임에 존재하는 특성의 수 :\", df3_q2.shape[1])\ndf3_q2.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:43:36.584112Z","iopub.execute_input":"2021-10-25T16:43:36.584415Z","iopub.status.idle":"2021-10-25T16:43:36.614272Z","shell.execute_reply.started":"2021-10-25T16:43:36.584376Z","shell.execute_reply":"2021-10-25T16:43:36.613419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"최종 데이터 프레임에 존재하는 특성의 수 :\", df1.shape[1]+df2.shape[1]+df3_q1.shape[1]+df3_q2.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:43:36.615351Z","iopub.execute_input":"2021-10-25T16:43:36.615662Z","iopub.status.idle":"2021-10-25T16:43:36.621418Z","shell.execute_reply.started":"2021-10-25T16:43:36.615621Z","shell.execute_reply":"2021-10-25T16:43:36.620471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 최종 특성을 csv파일에 저장하기\nif not os.path.isfile('../input/d/elemento/quora-question-pairs/final_features_sm.csv'):\n    print(\"final_features_sm is not file\")\n    df3_q1['id'] = df1['id']\n    df3_q2['id'] = df1['id']\n    df1  = df1.merge(df2, on='id',how='left')\n    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n    result  = df1.merge(df2, on='id',how='left')\n    result.to_csv('final_features_sm.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:43:51.529031Z","iopub.execute_input":"2021-10-25T16:43:51.529333Z","iopub.status.idle":"2021-10-25T16:43:51.536794Z","shell.execute_reply.started":"2021-10-25T16:43:51.529299Z","shell.execute_reply":"2021-10-25T16:43:51.536117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. 머신 러닝 모델\n## 4.1 데이터 읽어와서 sql 테이블에 저장하기","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/d/elemento/quora-question-pairs/final_features_sm.csv\", nrows = 100000)\nprint(data.shape)\n\ny_true = data['is_duplicate']\ndata.drop(['Unnamed: 0', 'id', 'is_duplicate'], axis=1, inplace=True)\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:52:07.550447Z","iopub.execute_input":"2021-10-25T16:52:07.550755Z","iopub.status.idle":"2021-10-25T16:52:13.814135Z","shell.execute_reply.started":"2021-10-25T16:52:07.550721Z","shell.execute_reply":"2021-10-25T16:52:13.813239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:57:48.222283Z","iopub.execute_input":"2021-10-25T16:57:48.222707Z","iopub.status.idle":"2021-10-25T16:57:48.260341Z","shell.execute_reply.started":"2021-10-25T16:57:48.22267Z","shell.execute_reply":"2021-10-25T16:57:48.259496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Random train test split(7:3)","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data, y_true, stratify = y_true, test_size = 0.3)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:58:01.294152Z","iopub.execute_input":"2021-10-25T16:58:01.294948Z","iopub.status.idle":"2021-10-25T16:58:01.602188Z","shell.execute_reply.started":"2021-10-25T16:58:01.2949Z","shell.execute_reply":"2021-10-25T16:58:01.601336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train data의 수:\",X_train.shape)\nprint(\"test data의 수:\",X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:58:19.290851Z","iopub.execute_input":"2021-10-25T16:58:19.291639Z","iopub.status.idle":"2021-10-25T16:58:19.297903Z","shell.execute_reply.started":"2021-10-25T16:58:19.2916Z","shell.execute_reply":"2021-10-25T16:58:19.297236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"---------- train data내의 출력 변수의 분산 ----------\")\ntrain_distr = Counter(y_train)\ntrain_len = len(y_train)\nprint(\"Class 0: %2.2f, Class 1: %2.2f\" % (int(train_distr[0])/train_len, int(train_distr[1])/train_len))\n\nprint(\"---------- test data 내의 출력 변수의 분산 ----------\")\ntest_distr = Counter(y_test)\ntest_len = len(y_test)\nprint(\"Class 0: %2.2f, Class 1: %2.2f\" % (int(test_distr[0])/test_len, int(test_distr[1])/test_len))","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:01:32.597531Z","iopub.execute_input":"2021-10-25T17:01:32.598017Z","iopub.status.idle":"2021-10-25T17:01:32.625287Z","shell.execute_reply.started":"2021-10-25T17:01:32.597967Z","shell.execute_reply":"2021-10-25T17:01:32.624612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_confusion_matrix는 y_i, y_i_hat이 주어지면 confusion matrix을 표시합니다.\ndef plot_confusion_matrix(test_y, predict_y):\n    print(\"== START plot_confusion_matrix ==\")\n    \"\"\"    \n    이 함수는 Seaborn 히트맵을 사용하여 3개의 다른 행렬을 플로팅합니다. \n    Confusion Matrix, Precision Matrix 및 Recall Matrix를 구성합니다.\n    confusion matrix는 평소와 같으며, precision matrix에서 각 열의 값 합계는 1입니다.\n    마찬가지로 Recall Matrix에서 각 행의 값의 합은 1입니다.\n    \"\"\"\n    \n    C = confusion_matrix(test_y, predict_y)\n    # 열 내부의 요소의 합으로 confusion matrix의 각 요소들을 나눕니다.\n    A = (((C.T)/(C.sum(axis=1))).T)\n    # 행 내부의 요소의 합으로 confusion matrix의 각 요소들을 나눕니다.    \n    B = (C/C.sum(axis=0))\n\n    plt.figure(figsize=(12,10))\n    \n    labels = [0,1]\n    cmap=sns.light_palette(\"blue\")\n    \n    # heatmap 형식으로 C 표현하기\n    plt.subplot(2, 2, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    # heatmap 형식으로 B 표현하기\n    plt.subplot(2, 2, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    # heatmap 형식으로 A 표현하기\n    plt.subplot(2, 2, 3)\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:10:12.592148Z","iopub.execute_input":"2021-10-25T17:10:12.592513Z","iopub.status.idle":"2021-10-25T17:10:12.605503Z","shell.execute_reply.started":"2021-10-25T17:10:12.592459Z","shell.execute_reply":"2021-10-25T17:10:12.604608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4 random model 생성하기 (최악의 경우의 log-loss 찾기)","metadata":{}},{"cell_type":"code","source":"# We need to generate 9 numbers and the sum of numbers should be 1\n# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n# ref: https://stackoverflow.com/a/18662466/4084039\n# We create an output array that has exactly same size as the CV data\npredicted_y = np.zeros((test_len, 2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs / sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\", log_loss(y_test, predicted_y, eps=1e-15))\n\npredicted_y = np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:11:31.977988Z","iopub.execute_input":"2021-10-25T17:11:31.97832Z","iopub.status.idle":"2021-10-25T17:11:33.132041Z","shell.execute_reply.started":"2021-10-25T17:11:31.978288Z","shell.execute_reply":"2021-10-25T17:11:33.13122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.5 Logistic Regression with hyperparameter tuning# # Hyper-parameters for SGD classifier","metadata":{}},{"cell_type":"code","source":"\n# alpha = [10 ** x for x in range(-5, 2)] \n\n# # Read more about SGDClassifier() at\n# # http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n\n# log_error_array=[]\n# for i in alpha:\n#     clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n#     clf.fit(X_train, y_train)\n#     sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n#     sig_clf.fit(X_train, y_train)\n#     predict_y = sig_clf.predict_proba(X_test)\n#     log_error_array.append(log_loss(y_test, predict_y, labels = clf.classes_))\n#     print('Alpha:', i, \"Log loss:\", log_loss(y_test, predict_y, labels = clf.classes_))\n\n# fig, ax = plt.subplots()\n# ax.plot(alpha, log_error_array,c='g')\n# for i, txt in enumerate(np.round(log_error_array, 3)):\n#     ax.annotate((alpha[i], np.round(txt,3)), (alpha[i], log_error_array[i]))\n# plt.grid()\n# plt.title(\"Cross Validation Error for each alpha\")\n# plt.xlabel(\"Alpha(s)\")\n# plt.ylabel(\"Error measure\")\n# plt.show()\n\n# best_alpha = np.argmin(log_error_array)\n# clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n# clf.fit(X_train, y_train)\n# sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n# sig_clf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the Model\n# dump(sig_clf, 'logistic_regression.joblib')\n\n# Loading the Model\nsig_clf = load('../input/d/elemento/quora-question-pairs/logistic_regression.joblib')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:11:53.386119Z","iopub.execute_input":"2021-10-25T17:11:53.386395Z","iopub.status.idle":"2021-10-25T17:11:53.406695Z","shell.execute_reply.started":"2021-10-25T17:11:53.386367Z","shell.execute_reply":"2021-10-25T17:11:53.406002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_y = sig_clf.predict_proba(X_train)\nprint(\"Best Alpha: 0.01, Train log loss:\",log_loss(y_train, predict_y, labels=sig_clf.classes_))\npredict_y = sig_clf.predict_proba(X_test)\nprint(\"Best Alpha: 0.01, Test log loss:\",log_loss(y_test, predict_y, labels=sig_clf.classes_))\npredicted_y = np.argmax(predict_y, axis=1)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:11:56.205198Z","iopub.execute_input":"2021-10-25T17:11:56.206024Z","iopub.status.idle":"2021-10-25T17:11:57.292155Z","shell.execute_reply.started":"2021-10-25T17:11:56.205984Z","shell.execute_reply":"2021-10-25T17:11:57.291566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.6 Linear SVM with hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"# # Hyper-parameters for SGD classifier\n# alpha = [10 ** x for x in range(-5, 2)]\n\n# # Read more about SGDClassifier() at\n# # http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n\n# log_error_array=[]\n# for i in alpha:\n#     clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n#     clf.fit(X_train, y_train)\n#     sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n#     sig_clf.fit(X_train, y_train)\n#     predict_y = sig_clf.predict_proba(X_test)\n#     log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_))\n#     print('Alpha:', i, \"Log loss:\", log_loss(y_test, predict_y, labels = clf.classes_, eps=1e-15))\n\n# fig, ax = plt.subplots()\n# ax.plot(alpha, log_error_array,c='g')\n# for i, txt in enumerate(np.round(log_error_array,3)):\n#     ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n# plt.grid()\n# plt.title(\"Cross Validation Error for each alpha\")\n# plt.xlabel(\"Alpha(s)\")\n# plt.ylabel(\"Error measure\")\n# plt.show()\n\n# best_alpha = np.argmin(log_error_array)\n# clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n# clf.fit(X_train, y_train)\n# sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n# sig_clf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the Model\n# dump(sig_clf, 'linear_svm.joblib')\n\n# Loading the Model\nsig_clf = load('../input/d/elemento/quora-question-pairs/linear_svm.joblib')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:12:07.453746Z","iopub.execute_input":"2021-10-25T17:12:07.454205Z","iopub.status.idle":"2021-10-25T17:12:07.473016Z","shell.execute_reply.started":"2021-10-25T17:12:07.454174Z","shell.execute_reply":"2021-10-25T17:12:07.47178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_y = sig_clf.predict_proba(X_train)\nprint(\"Best Alpha: 0.01, Train log loss:\", log_loss(y_train, predict_y, labels=sig_clf.classes_))\npredict_y = sig_clf.predict_proba(X_test)\nprint(\"Best Alpha: 0.01, Test log loss:\", log_loss(y_test, predict_y, labels=sig_clf.classes_))\npredicted_y = np.argmax(predict_y, axis=1)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:12:08.313006Z","iopub.execute_input":"2021-10-25T17:12:08.313646Z","iopub.status.idle":"2021-10-25T17:12:09.397361Z","shell.execute_reply.started":"2021-10-25T17:12:08.313606Z","shell.execute_reply":"2021-10-25T17:12:09.39652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 218차원 데이터 세트를 가지고 있으며, 매우 높지도 낮지도 않은 것으로 볼 수 있다.\n- 이 경우 선형 모델과 GBDT와 같은 더 복잡한 모델을 모두 시도하여 선형 모델이 과소 적합하지 않은지 확인할 수 있다.","metadata":{}},{"cell_type":"markdown","source":"## 4.7 XGBoost","metadata":{}},{"cell_type":"code","source":"# params = {}\n# params['objective'] = 'binary:logistic'\n# params['eval_metric'] = 'logloss'\n# params['eta'] = 0.02\n# params['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label = y_train)\nd_test = xgb.DMatrix(X_test, label = y_test)\n\n# watchlist = [(d_train, 'train'), (d_test, 'valid')]\n# bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:04.300807Z","iopub.execute_input":"2021-10-25T17:13:04.301383Z","iopub.status.idle":"2021-10-25T17:13:04.789263Z","shell.execute_reply.started":"2021-10-25T17:13:04.301349Z","shell.execute_reply":"2021-10-25T17:13:04.787432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the Model\n# dump(bst, 'xg_boost.joblib')\n\n# Loading the Model\nbst = load('../input/d/elemento/quora-question-pairs/xg_boost.joblib')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:05.01137Z","iopub.execute_input":"2021-10-25T17:13:05.011687Z","iopub.status.idle":"2021-10-25T17:13:05.276326Z","shell.execute_reply.started":"2021-10-25T17:13:05.011655Z","shell.execute_reply":"2021-10-25T17:13:05.275527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgdmat = xgb.DMatrix(X_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"Test log loss:\", log_loss(y_test, predict_y, labels = [0, 1]))\npredicted_y = np.array(predict_y > 0.5, dtype=int)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:05.840096Z","iopub.execute_input":"2021-10-25T17:13:05.84072Z","iopub.status.idle":"2021-10-25T17:13:07.605351Z","shell.execute_reply.started":"2021-10-25T17:13:05.840667Z","shell.execute_reply":"2021-10-25T17:13:07.60455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Modeling 계속하기\n- 로그 손실을 줄이기 위해 TF-IDF W2V로 벡터라이저와 함께 RandomsearchCV를 사용하여 XgBoost 모델의 하이퍼파라미터 튜닝을 수행합니다.\n- TD_IDF 가중치 word2Vec 대신 간단한 TF-IDF 벡터를 사용하여 모델(로지스틱 회귀, Linear-SVM)을 시도합니다.\n## 5.1 TF-IDF W2V로 XGBoost Hyper-parameter 조정하기","metadata":{}},{"cell_type":"code","source":"# params = {\n#     'n_estimators': [50, 100, 150],\n#     'max_depth': [15, 20, None], \n#     'objective': ['binary:logistic'],\n#     'eval_metric': ['logloss'],\n# }\n\n# xgbr = xgb.XGBClassifier(verbosity = 1)\n# clf = RandomizedSearchCV(xgbr, params)\n# clf.fit(X_train, y_train)\n# print(clf.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The best hyper-parameters are as follows:\n# n_estimators = 150, max_depth = 15, eval_metric = logloss\n\n# Training XGBClassifier with the best hyper-parameters\n# xgbr = xgb.XGBClassifier(verbosity = 1, n_estimators = 150, max_depth = 15, \n#     objective = 'binary:logistic', eval_metric = 'logloss')\n# xgbr.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the Model\n# dump(xgbr, 'xgb_classifier_hpt.joblib')\n\n# Loading the Model\nxgbr = load('../input/d/elemento/quora-question-pairs/xgb_classifier_hpt.joblib')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:15:00.117995Z","iopub.execute_input":"2021-10-25T17:15:00.118719Z","iopub.status.idle":"2021-10-25T17:15:00.995312Z","shell.execute_reply.started":"2021-10-25T17:15:00.118671Z","shell.execute_reply":"2021-10-25T17:15:00.994213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_y = xgbr.predict_proba(X_train)\nprint(\"n_estimators: , max_depth: , Train log loss:\", log_loss(y_train, predict_y, labels=[0,1]))\npredict_y = xgbr.predict_proba(X_test)\nprint(\"n_estimators: , max_depth: , Test log loss:\", log_loss(y_test, predict_y, labels=[0,1]))\npredicted_y = np.argmax(predict_y, axis=1)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:15:01.761069Z","iopub.execute_input":"2021-10-25T17:15:01.761409Z","iopub.status.idle":"2021-10-25T17:15:03.177194Z","shell.execute_reply.started":"2021-10-25T17:15:01.761378Z","shell.execute_reply":"2021-10-25T17:15:03.176581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 TF-IDF로 LR, Linear-SVM 시도하기","metadata":{}},{"cell_type":"code","source":"# # Using TF-IDF\n# df = pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")\n# df.dropna(axis = 0, subset = ['question1', 'question2'], inplace = True)\n# print(df.shape)\n# df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Merge the Questions\n# questions = list(df['question1']) + list(df['question2'])\n\n# tfidf = TfidfVectorizer(min_df = 5, max_features = 500)\n# tfidf.fit(questions)\n\n# q1 = tfidf.transform(df['question1'])\n# q2 = tfidf.transform(df['question2'])\n# print(q1.shape, q2.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dfnlp = pd.read_csv(\"../input/d/elemento/quora-question-pairs/nlp_features_train.csv\",encoding='latin-1')\n# dfppro = pd.read_csv(\"../input/d/elemento/quora-question-pairs/df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n# df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\n# df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n# df3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n\n# # Creating a DataFrame for the TF-IDF feature vectors\n# df3_q1 = pd.DataFrame.sparse.from_spmatrix(q1, index = df3.index)\n# df3_q2 = pd.DataFrame.sparse.from_spmatrix(q2, index = df3.index)\n# print(df3_q1.shape, df3_q2.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Making the Dataset with TF-IDF feature vectors\n# df3_q1['id'] = df1['id']\n# df3_q2['id'] = df1['id']\n# df1  = df1.merge(df2, on='id', how='left')\n# df2  = df3_q1.merge(df3_q2, on='id', how='left')\n# result  = df1.merge(df2, on='id', how='left')\n# result.to_csv('final_features_tfidf.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the TF-IDF based Dataset\ndata = pd.read_csv(\"../input/d/elemento/quora-question-pairs/final_features_tfidf.csv\", nrows = 100000)\nprint(data.shape)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:15:37.64844Z","iopub.execute_input":"2021-10-25T17:15:37.648801Z","iopub.status.idle":"2021-10-25T17:15:53.514805Z","shell.execute_reply.started":"2021-10-25T17:15:37.648763Z","shell.execute_reply":"2021-10-25T17:15:53.513638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = data['is_duplicate']\ndata.drop(['Unnamed: 0', 'id','is_duplicate'], axis=1, inplace=True)\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:15:53.516795Z","iopub.execute_input":"2021-10-25T17:15:53.517052Z","iopub.status.idle":"2021-10-25T17:15:53.763168Z","shell.execute_reply.started":"2021-10-25T17:15:53.517023Z","shell.execute_reply":"2021-10-25T17:15:53.762144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data, y_true, stratify = y_true, test_size = 0.3)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:15:59.499196Z","iopub.execute_input":"2021-10-25T17:15:59.499544Z","iopub.status.idle":"2021-10-25T17:16:00.405885Z","shell.execute_reply.started":"2021-10-25T17:15:59.499511Z","shell.execute_reply":"2021-10-25T17:16:00.404672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2.1 Logistic Regression","metadata":{}},{"cell_type":"code","source":"# # Using the hyper-parameters obtained from the previous hyper-parameter tuning\n# clf = SGDClassifier(alpha=0.01, penalty='l2', loss='log', random_state=42)\n# clf.fit(X_train, y_train)\n# sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n# sig_clf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the Model\n# dump(sig_clf, 'logistic_regression_tfidf.joblib')\n\n# Loading the Model\nsig_clf = load('../input/d/elemento/quora-question-pairs/logistic_regression_tfidf.joblib')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:16:04.179438Z","iopub.execute_input":"2021-10-25T17:16:04.179771Z","iopub.status.idle":"2021-10-25T17:16:04.204008Z","shell.execute_reply.started":"2021-10-25T17:16:04.179738Z","shell.execute_reply":"2021-10-25T17:16:04.203108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the Model\n# dump(sig_clf, 'logistic_regression_tfidf.joblib')\n\n# Loading the Model\nsig_clf = load('../input/d/elemento/quora-question-pairs/logistic_regression_tfidf.joblib')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:16:05.733399Z","iopub.execute_input":"2021-10-25T17:16:05.734311Z","iopub.status.idle":"2021-10-25T17:16:05.746274Z","shell.execute_reply.started":"2021-10-25T17:16:05.734256Z","shell.execute_reply":"2021-10-25T17:16:05.7456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_y = sig_clf.predict_proba(X_train)\nprint(\"Best Alpha: 0.01, Train log loss:\", log_loss(y_train, predict_y, labels=sig_clf.classes_))\npredict_y = sig_clf.predict_proba(X_test)\nprint(\"Best Alpha: 0.01, Test log loss:\", log_loss(y_test, predict_y, labels=sig_clf.classes_))\npredicted_y = np.argmax(predict_y, axis=1)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:16:07.56011Z","iopub.execute_input":"2021-10-25T17:16:07.560924Z","iopub.status.idle":"2021-10-25T17:16:09.344664Z","shell.execute_reply.started":"2021-10-25T17:16:07.560887Z","shell.execute_reply":"2021-10-25T17:16:09.344037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2.2 Linear SVM","metadata":{}},{"cell_type":"code","source":"# # Using the hyper-parameters obtained from the previous hyper-parameter tuning\n# clf = SGDClassifier(alpha=0.01, penalty='l2', loss='hinge', random_state=42)\n# clf.fit(X_train, y_train)\n# sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n# sig_clf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the Model\n# dump(sig_clf, 'linear_svm_tfidf.joblib')\n\n# Loading the Model\nsig_clf = load('../input/d/elemento/quora-question-pairs/linear_svm_tfidf.joblib')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:16:13.156686Z","iopub.execute_input":"2021-10-25T17:16:13.157325Z","iopub.status.idle":"2021-10-25T17:16:13.17453Z","shell.execute_reply.started":"2021-10-25T17:16:13.157279Z","shell.execute_reply":"2021-10-25T17:16:13.173834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_y = sig_clf.predict_proba(X_train)\nprint(\"Best Alpha: 0.01, Train log loss:\", log_loss(y_train, predict_y, labels=sig_clf.classes_))\npredict_y = sig_clf.predict_proba(X_test)\nprint(\"Best Alpha: 0.01, Test log loss:\", log_loss(y_test, predict_y, labels=sig_clf.classes_))\npredicted_y = np.argmax(predict_y, axis=1)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:16:14.030262Z","iopub.execute_input":"2021-10-25T17:16:14.031278Z","iopub.status.idle":"2021-10-25T17:16:15.786945Z","shell.execute_reply.started":"2021-10-25T17:16:14.031176Z","shell.execute_reply":"2021-10-25T17:16:15.786076Z"},"trusted":true},"execution_count":null,"outputs":[]}]}