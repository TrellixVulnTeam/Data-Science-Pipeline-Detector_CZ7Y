{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.palettes.color_palette()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/quora-question-pairs/train.csv')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of question pairs for trainnig :{}'.format(len(df_train)))\nprint('duplicate pairs :{}%'.format(round(df_train['is_duplicate'].mean() * 100 ,2)))\nqids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\nprint('Total number of Questions in the Training data: {}'.format(len(np.unique(qids))))\nprint('number of question that appear multiple times :{}'.format(np.sum(qids.value_counts() > 1)))\n\n# plot \n\nplt.figure(figsize = (12,5))\nplt.hist(qids.value_counts(),bins = 50)\nplt.yscale('log' , nonposy = 'clip')\nplt.title('log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Submission\n\nfrom sklearn.metrics import log_loss\n\np = df_train['is_duplicate'].mean() # Our predicted probability\nprint('predicted score:',log_loss(df_train['is_duplicate'], np.zeros_like(df_train['is_duplicate']) + p))\n\ndf_test = pd.read_csv('../input/quora-question-pairs/test.csv')\nsub = pd.DataFrame({'test_id' : df_test['test_id'],'is_duplicate':p})\nsub.to_csv('naive_submission.csv' , index = False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/quora-question-pairs/test.csv')\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total  number of question pairs ofr testing :{}'.format(len(df_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\ntest_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n\ndist_train = train_qs.apply(len) # Lenght character in rows\ndist_test = test_qs.apply(len)\nplt.figure(figsize=(15,10))\nplt.hist(dist_train , bins = 200 , range = [0,200],color = pal[2],normed = True , label = 'train')\nplt.hist(dist_test , bins = 200 , range = [0,200], color = pal[1],normed = True , alpha = 0.5 , label = 'test')\nplt.title('Normaised histogram of character count in questions ' , fontsize = 15)\nplt.legend()\nplt.xlabel('Number of Characters' , fontsize = 15)\nplt.ylabel('Probability ', fontsize = 15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train{:.2f} max-test {:.2f}'.format(dist_train.mean(),\n                                                                                                                 dist_train.std(),\n                                                                                                                 dist_test.mean(),\n                                                                                                                 dist_test.std(),\n                                                                                                                 dist_train.max(),\n                                                                                                                 dist_test.max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dist_train = train_qs.apply(lambda x: len(x.split(' ')))\ndist_test = test_qs.apply(lambda x: len(x.split(' ')))\n\nplt.figure(figsize=(15, 10))\nplt.hist(dist_train, bins=50, range=[0, 50], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=50, range=[0, 50], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of word count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word cloud\n\nfrom wordcloud import WordCloud\n\ncloud = WordCloud(width = 1440 , height = 1080).generate(\" \".join(train_qs.astype(str)))\nplt.figure(figsize=(20 , 15))\nplt.imshow(cloud)\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# qmarks = np.mean(train_qs.apply(lambda x:'?' in x))\n# math = np.mean(train_qs.apply(lambda x:'[math]' in x))\n# fullstop = np.mean(train_qs.apply(lambda x: '.' in x))\n# capital_first = np.mean(train_qs.apply(lambda x: x[0].isupper()))\n# capitals = np.mean(train_qs.apply(lambda x:max([y.isupper()]for y in x)))\n# numbers = np.mean(train_qs.apply(lambda x: max([y.isdigit()]for y in x)))\n\n# print('question with question marks : {:.2f}%'.format(qmarks * 100))\n# print('question with [math] tages :{:.2f}%',format( math * 100))\n# print('question with full stops: {:.2f}%'.format(fullstop * 100))\n# print('questions with capitalsed frist letters :{:.2f}%'.format(capital_frist * 100))\n# print('questions with capital letters {:.2f}% '.format(capitals * 100))\n# print('question with numbers : {:.2f}%'.format(numbers * 100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R\n\nplt.figure(figsize=(15, 5))\ntrain_word_match = df_train.apply(word_match_share, axis=1, raw=True)\nplt.hist(train_word_match[df_train['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(train_word_match[df_train['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\n# If a word appears only once , we ignore it completely (like a typo)\n# epsilon defines a smoothing constant  , wich makes the effect of extermely rare word smaller\n\ndef get_weight(count , eps = 10000 , min_count = 2):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\neps = 5000\nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word:get_weight(count) for word , count in counts.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Most common words and weights:\\n')\nprint(sorted(weights.items(),key = lambda x:x[1] if x[1] > 0 else 9999)[:10])\nprint('\\nleast common words and weights: ')\n(sorted(weights.items(), key = lambda x:x[1], reverse = True)[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tfidf_word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    \n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    \n    R = np.sum(shared_weights) / np.sum(total_weights)\n    return R","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\ntfidf_train_word_match = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\nplt.hist(tfidf_train_word_match[df_train['is_duplicate'] == 0].fillna(0), bins=20, normed=True, label='Not Duplicate')\nplt.hist(tfidf_train_word_match[df_train['is_duplicate'] == 1].fillna(0), bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over tfidf_word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint('Orginal Auc:',roc_auc_score(df_train['is_duplicate'],train_word_match))\nprint('TFIDF Auc:',roc_auc_score(df_train['is_duplicate'],tfidf_train_word_match.fillna(0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we careat our training and test data\n\nx_train = pd.DataFrame()\nx_test = pd.DataFrame()\nx_train['word_match'] = train_word_match\nx_train['tfidf_word_match'] = tfidf_train_word_match\nx_test['word_match'] = df_test.apply(word_match_share , axis = 1 , raw = True)\nx_test['tfidf_word_match'] = df_test.apply(tfidf_word_match_share , axis = 1 ,raw= True)\ny_train = df_train['is_duplicate'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_train = x_train[y_train == 1]\nneg_train = x_train[y_train == 0]\n\n# Now we oversample the negative class\n# There is likely a much more elegant way to do this..\np = 0.165\nscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\nwhile scale > 1:\n    neg_train = pd.concat([neg_train , neg_train])\n    scale -=1\n    neg_train = pd.concat([neg_train ,neg_train[:int(scale *len(neg_train))]])\n    print(len(pos_train) / (len(pos_train) + len(neg_train)))\n    \n    x_train = pd.concat([pos_train , neg_train])\n    y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n    del pos_train , neg_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finally , we split some of the data off for validation\nfrom sklearn.model_selection import train_test_split\n\nx_train , x_valid , y_train , y_valid = train_test_split(x_train, y_train , test_size = 0.2 , random_state = 4242)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_test = xgb.DMatrix(x_test)\np_test = bst.predict(d_test)\n\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = p_test\nsub.to_csv('simple_xgb.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}