{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-21T14:06:31.534107Z","iopub.execute_input":"2021-09-21T14:06:31.534426Z","iopub.status.idle":"2021-09-21T14:06:31.547317Z","shell.execute_reply.started":"2021-09-21T14:06:31.534385Z","shell.execute_reply":"2021-09-21T14:06:31.546261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !unzip /kaggle/input/quora-question-pairs/train.csv.zip","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:06:31.54973Z","iopub.execute_input":"2021-09-21T14:06:31.550115Z","iopub.status.idle":"2021-09-21T14:06:31.555359Z","shell.execute_reply.started":"2021-09-21T14:06:31.550079Z","shell.execute_reply":"2021-09-21T14:06:31.554531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nfrom tqdm import tqdm\nimport time\nimport random\nimport datetime\n\nfrom sklearn import model_selection\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom transformers import BertModel","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:06:31.556982Z","iopub.execute_input":"2021-09-21T14:06:31.557348Z","iopub.status.idle":"2021-09-21T14:06:31.564483Z","shell.execute_reply.started":"2021-09-21T14:06:31.557314Z","shell.execute_reply":"2021-09-21T14:06:31.563624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\n\n\nclass AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:06:31.566085Z","iopub.execute_input":"2021-09-21T14:06:31.566435Z","iopub.status.idle":"2021-09-21T14:06:31.582771Z","shell.execute_reply.started":"2021-09-21T14:06:31.566402Z","shell.execute_reply":"2021-09-21T14:06:31.581828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Settings:\n#     PROJ_NAME = 'Text-Similarity-Using-BERT'\n#     root_path = os.getcwd().split(PROJ_NAME)[0] + PROJ_NAME + \"\\\\\"\n#     APPLICATION_PATH = root_path + \"backend\\\\services\\\\text_similarity\\\\application\\\\\"\n    # setting up logs path\n#     LOGS_DIRECTORY = root_path + \"backend\\\\services\\\\text_similarity\\\\logs\\\\logs.txt\"\n\n    checkpoint = \"bert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    # training data directory\n    TRAIN_DATA = \"/kaggle/working/train.csv\"\n\n    # test data directory\n    TEST_DATA = \"/kaggle/input/quora-question-pairs/test.cs\"\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # labels\n    possible_labels = {'not_duplicate': 0, 'duplicate': 1}\n    # number of labels\n    num_labels = 1\n    # dropout\n    dropout = 0.3\n    input_dim = 768\n\n    # max length for embeddings\n    max_len = 256\n\n    # bert no decay layers\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    seed_value = 42\n    test_size = 0.2\n\n    # weights path\n    WEIGHTS_PATH = \"text_similarity_model.bin\"\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 10\n    RANDOM_STATE = 42\n    TRAIN_NUM_WORKERS = 4\n    VAL_NUM_WORKERS = 2\n    patience = 4\n    mode = \"max\"\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:06:31.584993Z","iopub.execute_input":"2021-09-21T14:06:31.585327Z","iopub.status.idle":"2021-09-21T14:06:40.040951Z","shell.execute_reply.started":"2021-09-21T14:06:31.585294Z","shell.execute_reply":"2021-09-21T14:06:40.040225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTDataset:\n    def __init__(self, sentence_1, sentence_2, targets):\n        self.settings = Settings\n        self.sentence_1 = sentence_1\n        self.sentence_2 = sentence_2\n        self.targets = targets\n        assert len(self.sentence_1) == len(self.sentence_2) == len(self.targets)\n\n    def __len__(self):\n        return len(self.sentence_1)\n\n    def __getitem__(self, item):\n        s1 = self.sentence_1[item]\n        s2 = self.sentence_2[item]\n        target = self.targets[item]\n\n        inputs = self.settings.tokenizer.encode_plus(\n            s1, s2,\n            add_special_tokens=True,\n            max_length=self.settings.max_len,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return {\n            'input_ids': torch.tensor(ids),\n            'attention_mask': torch.tensor(mask),\n            'token_type_ids': torch.tensor(token_type_ids),\n            'targets': torch.tensor(target)\n        }\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:06:40.042223Z","iopub.execute_input":"2021-09-21T14:06:40.042481Z","iopub.status.idle":"2021-09-21T14:06:40.054884Z","shell.execute_reply.started":"2021-09-21T14:06:40.042448Z","shell.execute_reply":"2021-09-21T14:06:40.054028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTClassifier(nn.Module):\n    def __init__(self, freeze_params=False):\n        super(BERTClassifier, self).__init__()\n        self.settings = Settings\n        self.bert = BertModel.from_pretrained(self.settings.checkpoint, return_dict=False)\n\n        if not freeze_params:\n            # freeze all the parameters\n            for param in self.bert.parameters():\n                param.requires_grad = False\n\n        self.bert_drop = nn.Dropout(self.settings.dropout)\n        self.out = nn.Linear(self.settings.input_dim, self.settings.num_labels)\n\n    def forward(self, ids, mask, token_type_ids):\n        o1, o2 = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        bo = self.bert_drop(o2)\n        output = self.out(bo)\n\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:06:40.057736Z","iopub.execute_input":"2021-09-21T14:06:40.058228Z","iopub.status.idle":"2021-09-21T14:06:40.068951Z","shell.execute_reply.started":"2021-09-21T14:06:40.058193Z","shell.execute_reply":"2021-09-21T14:06:40.068009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Engine:\n    def __init__(self):\n        pass\n\n    def loss_fn(self, outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n    def set_seed(self, seed_value=42):\n        random.seed(seed_value)\n        np.random.seed(seed_value)\n        torch.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n\n    def accuracy_threshold(self, y_pred, y_true, thresh: float = 0.5, sigmoid: bool = True):\n        if sigmoid:\n            y_pred = y_pred.sigmoid()\n        return ((y_pred > thresh) == y_true.byte()).float().mean().item()\n\n    def train_fn(self, data_loader, model, optimizer, device, scheduler):\n        print(\"Starting training...\\n\")\n        # Reset the total loss for this epoch.\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n        t0_epoch, t0_batch = time.time(), time.time()\n        model.train()\n        for step, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n            batch_counts += 1\n            b_input_ids = data['input_ids']\n            b_attn_mask = data['attention_mask']\n            b_labels = data['targets']\n            b_token_type_ids = data['token_type_ids']\n\n            # moving tensors to device\n            b_input_ids = b_input_ids.to(device)\n            b_attn_mask = b_attn_mask.to(device)\n            b_labels = b_labels.to(device)\n            b_token_type_ids = b_token_type_ids.to(device)\n\n            # optimizer.zero_grad()\n\n            # Always clear any previously calculated gradients before performing a\n            # backward pass. PyTorch doesn't do this automatically because\n            # accumulating the gradients is \"convenient while training RNNs\".\n            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n            model.zero_grad()\n\n            logits = model(\n                ids=b_input_ids,\n                mask=b_attn_mask,\n                token_type_ids=b_token_type_ids\n            )\n\n            loss = self.loss_fn(logits, b_labels.float())\n            batch_loss += loss.item()\n            # Accumulate the training loss over all of the batches so that we can\n            # calculate the average loss at the end. `loss` is a Tensor containing a\n            # single value; the `.item()` function just returns the Python value\n            # from the tensor.\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate the gradients.\n            loss.backward()\n            # Clip the norm of the gradients to 1.0.\n            # This is to help prevent the \"exploding gradients\" problem.\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            # Update parameters and take a step using the computed gradient.\n            # The optimizer dictates the \"update rule\"--how the parameters are\n            # modified based on their gradients, the learning rate, etc\n            optimizer.step()\n            # Update the learning rate\n            scheduler.step()\n\n            if step % 2500 == 0 and not step == 0:\n                # Calculate elapsed time in minutes.\n                elapsed = self.format_time(time.time() - t0_epoch)\n                # Report progress.\n                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(data_loader), elapsed))\n\n        # Calculate the average loss over all of the batches.\n        avg_train_loss = total_loss / len(data_loader)\n        # Measure how long this epoch took.\n        training_time = self.format_time(time.time() - t0_epoch)\n\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n\n    def eval_fn(self, data_loader, model, device):\n        print(\"Starting evaluation...\\n\")\n        t0 = time.time()\n        model.eval()\n        val_accuracy = []\n        val_loss = []\n        with torch.no_grad():\n            for step, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n                b_input_ids = data['input_ids']\n                b_attn_mask = data['attention_mask']\n                b_labels = data['targets']\n                b_token_type_ids = data['token_type_ids']\n\n                # moving tensors to device\n                b_input_ids = b_input_ids.to(device)\n                b_attn_mask = b_attn_mask.to(device)\n                b_labels = b_labels.to(device)\n                b_token_type_ids = b_token_type_ids.to(device)\n\n                logits = model(\n                    ids=b_input_ids,\n                    mask=b_attn_mask,\n                    token_type_ids=b_token_type_ids\n                )\n                loss = self.loss_fn(logits, b_labels.float())\n                val_loss.append(loss.item())\n                accuracy = self.accuracy_threshold(logits.view(-1, 1), b_labels.view(-1, 1))\n                val_accuracy.append(accuracy)\n\n        val_loss = np.mean(val_loss)\n        val_accuracy = np.mean(val_accuracy)\n        validation_time = self.format_time(time.time() - t0)\n\n        print(\"  Average Validation Loss: {0:.2f}\".format(val_loss))\n        print(\"  Average Validation Accuracy: {0:.2f}\".format(val_accuracy))\n        print(\"  Validation took: {:}\".format(validation_time))\n\n        return val_loss, val_accuracy\n\n    def format_time(self, elapsed):\n        \"\"\"\n        Takes a time in seconds and returns a string hh:mm:ss\n        \"\"\"\n        # Round to the nearest second.\n        elapsed_rounded = int(round(elapsed))\n\n        # Format as hh:mm:ss\n        return str(datetime.timedelta(seconds=elapsed_rounded))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:06:40.072519Z","iopub.execute_input":"2021-09-21T14:06:40.072884Z","iopub.status.idle":"2021-09-21T14:06:40.096434Z","shell.execute_reply.started":"2021-09-21T14:06:40.072856Z","shell.execute_reply":"2021-09-21T14:06:40.095515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Train:\n    def __init__(self):\n        # initialize required class\n        self.settings = Settings\n        self.engine = Engine()\n        self.early_stopping = EarlyStopping(patience=self.settings.patience,\n                                                  mode=self.settings.mode)\n\n        # initialize required variables\n        self.bert_text_model = None\n        self.optimizer = None\n        self.scheduler = None\n        self.train_data_loader = None\n        self.val_data_loader = None\n        self.total_steps = None\n        self.param_optimizer = None\n        self.optimizer_parameters = None\n\n    def __initialize(self):\n        # Instantiate Bert Classifier\n        self.bert_text_model = BERTClassifier()\n        self.bert_text_model.to(self.settings.DEVICE)\n        self.__optimizer_params()\n\n        # Create the optimizer\n        self.optimizer = AdamW(self.optimizer_parameters,\n                               lr=5e-5,  # Default learning rate\n                               eps=1e-8  # Default epsilon value\n                               )\n\n        # Set up the learning rate scheduler\n        self.scheduler = get_linear_schedule_with_warmup(self.optimizer,\n                                                         num_warmup_steps=0,  # Default value\n                                                         num_training_steps=self.total_steps)\n\n    def __optimizer_params(self):\n        self.param_optimizer = list(self.bert_text_model.named_parameters())\n        self.optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in self.param_optimizer if not any(nd in n for nd in self.settings.no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in self.param_optimizer if any(nd in n for nd in self.settings.no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n    def __create_data_loaders(self, sentence1, sentence2, targets, batch_size, num_workers):\n        dataset = BERTDataset(sentence_1=sentence1,\n                              sentence_2=sentence2,\n                              targets=targets)\n        data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n\n        return data_loader\n\n    def __load_data(self, csv_data_path):\n        df = pd.read_csv(csv_data_path).dropna().reset_index(drop=True)\n        df_train, df_valid = model_selection.train_test_split(\n            df,\n            random_state=self.settings.seed_value,\n            test_size=self.settings.test_size,\n            stratify=df.is_duplicate.values\n\n        )\n\n        df_train = df_train.reset_index(drop=True)\n        df_valid = df_valid.reset_index(drop=True)\n\n        # creating Data Loaders\n        # train data loader\n        self.train_data_loader = self.__create_data_loaders(sentence1=df_train.question1.values,\n                                                            sentence2=df_train.question2.values,\n                                                            targets=df_train.is_duplicate.values,\n                                                            num_workers=self.settings.TRAIN_NUM_WORKERS,\n                                                            batch_size=self.settings.TRAIN_BATCH_SIZE)\n\n        # validation data loader\n        self.val_data_loader = self.__create_data_loaders(sentence1=df_valid.question1.values,\n                                                          sentence2=df_valid.question2.values,\n                                                          targets=df_valid.is_duplicate.values,\n                                                          num_workers=self.settings.VAL_NUM_WORKERS,\n                                                          batch_size=self.settings.VALID_BATCH_SIZE)\n\n        self.total_steps = int(len(df_train) / self.settings.TRAIN_BATCH_SIZE * self.settings.EPOCHS)\n\n    def __train(self):\n        for epochs in range(self.settings.EPOCHS):\n            self.engine.train_fn(data_loader=self.train_data_loader,\n                                 model=self.bert_text_model,\n                                 optimizer=self.optimizer,\n                                 device=self.settings.DEVICE,\n                                 scheduler=self.scheduler)\n\n            val_loss, val_accuracy = self.engine.eval_fn(data_loader=self.val_data_loader,\n                                                         model=self.bert_text_model,\n                                                         device=self.settings.DEVICE)\n\n            print(f\"Validation accuracy = {val_accuracy}\")\n\n            self.early_stopping(epoch_score=val_accuracy,\n                                model=self.bert_text_model,\n                                model_path=self.settings.WEIGHTS_PATH)\n\n            if self.early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n\n    def run(self):\n        try:\n            print(\"Loading and Preparing the Dataset-----!! \")\n            self.__load_data(csv_data_path=self.settings.TRAIN_DATA)\n            print(\"Dataset Successfully Loaded and Prepared-----!! \")\n            print()\n            print(\"Loading and Initializing the Bert Model -----!! \")\n            self.__initialize()\n            print(\"Model Successfully Loaded and Initialized-----!! \")\n            print()\n            print(\"------------------Starting Training-----------!!\")\n            self.engine.set_seed()\n            self.__train()\n            print(\"Training complete-----!!!\")\n\n        except BaseException as ex:\n            print(\"Following Exception Occurred---!! \", str(ex))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:06:40.097947Z","iopub.execute_input":"2021-09-21T14:06:40.098314Z","iopub.status.idle":"2021-09-21T14:06:40.122408Z","shell.execute_reply.started":"2021-09-21T14:06:40.098215Z","shell.execute_reply":"2021-09-21T14:06:40.121773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t1 = Train()\nt1.run()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:06:40.123706Z","iopub.execute_input":"2021-09-21T14:06:40.12397Z","iopub.status.idle":"2021-09-21T16:51:42.190127Z","shell.execute_reply.started":"2021-09-21T14:06:40.123929Z","shell.execute_reply":"2021-09-21T16:51:42.188134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### inference","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}