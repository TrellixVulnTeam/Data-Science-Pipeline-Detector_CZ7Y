{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"13d3633c-304b-73b6-42ed-095405821ce4"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom IPython.display import  display\nfrom collections import defaultdict\nfrom itertools import combinations\npd.set_option('display.max_colwidth',-1)\nimport textblob as tb\nimport nltk as nl\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble\nfrom sklearn import metrics"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9536a7c-d257-f015-b53b-da52114e6018"},"outputs":[],"source":"# let's define union-find function\ndef indices_dict(lis):\n    d = defaultdict(list)\n    for i,(a,b) in enumerate(lis):\n        d[a].append(i)\n        d[b].append(i)\n    return d\n\ndef disjoint_indices(lis):\n    d = indices_dict(lis)\n    sets = []\n    while len(d):\n        que = set(d.popitem()[1])\n        ind = set()\n        while len(que):\n            ind |= que \n            que = set([y for i in que \n                         for x in lis[i] \n                         for y in d.pop(x, [])]) - ind\n        sets += [ind]\n    return sets\n\ndef disjoint_sets(lis):\n    return [set([x for i in s for x in lis[i]]) for s in disjoint_indices(lis)]\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cba2337c-bd72-a19b-a04c-0b8d39aafb68"},"outputs":[],"source":"train_df=pd.read_csv('../input/train.csv')\n# only duplicated questions\nddf=train_df[train_df.is_duplicate==1]\nprint('Duplicates questions shape:',ddf.shape)\n# get all duplicated questions\nclean_ddf1=ddf[['qid1','question1']].drop_duplicates()\nclean_ddf1.columns=['qid','question']\nclean_ddf2=ddf[['qid2','question2']].drop_duplicates()\nclean_ddf2.columns=['qid','question']\nall_dqdf=clean_ddf1.append(clean_ddf2,ignore_index=True)\nprint(all_dqdf.shape)\n# groupby qid1, and then we get all the combinations of id in each group\ndqids12=ddf[['qid1','qid2']]\ndf12list=dqids12.groupby('qid1', as_index=False)['qid2'].agg({'dlist':(lambda x: list(x))})\nprint(len(df12list))\nd12list=df12list.values\nd12list=[[i]+j for i,j in d12list]\n# get all the combinations of id, like (id1,id2)...\nd12ids=set()\nfor ids in d12list:\n    ids_len=len(ids)\n    for i in range(ids_len):\n        for j in range(i+1,ids_len):\n            d12ids.add((ids[i],ids[j]))\nprint(len(d12ids))\n# the same operation of qid2\ndqids21=ddf[['qid2','qid1']]\ndisplay(dqids21.head(2))\ndf21list=dqids21.groupby('qid2', as_index=False)['qid1'].agg({'dlist':(lambda x: list(x))})\nprint(len(df21list))\nids2=df21list.qid2.values\nd21list=df21list.values\nd21list=[[i]+j for i,j in d21list]\nd21ids=set()\nfor ids in d21list:\n    ids_len=len(ids)\n    for i in range(ids_len):\n        for j in range(i+1,ids_len):\n            d21ids.add((ids[i],ids[j]))\nlen(d21ids)\n# merge two set\ndids=list(d12ids | d21ids)\nlen(dids)\n# split data into groups, so that each question in each group are duplicated\ndid_u=disjoint_sets(dids)\nnew_dids=[]\nfor u in did_u:\n    new_dids.extend(list(combinations(u,2)))\nlen(new_dids)\nnew_ddf=pd.DataFrame(new_dids,columns=['qid1','qid2'])\nprint('New duplicated shape:',new_ddf.shape)\ndisplay(new_ddf.head(2))\n# merge with all_dqdf to get question1 description\nnew_ddf=new_ddf.merge(all_dqdf,left_on='qid1',right_on='qid',how='left')\nnew_ddf.drop('qid',inplace=True,axis=1)\nnew_ddf.columns=['qid1','qid2','question1']\nnew_ddf.drop_duplicates(inplace=True)\nprint(new_ddf.shape)\nnew_ddf.head(2)\n# the same operation with qid2\nnew_ddf=new_ddf.merge(all_dqdf,left_on='qid2',right_on='qid',how='left')\nnew_ddf.drop('qid',inplace=True,axis=1)\nnew_ddf.columns=['qid1','qid2','question1','question2']\nnew_ddf.drop_duplicates(inplace=True)\nprint(new_ddf.shape)\nnew_ddf.head(2)\n# is_duplicate flag\nnew_ddf['is_duplicate']=1\nnew_ddf.head(2)\n# let random select 10 rows to check the result\nnew_ddf.sample(10)\n# the orininal duplicated pairs count:\nprint(len(all_dqdf))\n# after we generate more data, then the duplicated pairs count:\nprint(len(new_ddf))\ntrainDF = new_ddf.append(train_df[train_df.is_duplicate==0].drop('id', 1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3433b11-3bee-4a22-9085-cccc8caee2f7"},"outputs":[],"source":"#len(trainDF['question2'])\nlen(trainDF['question1'].dropna())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"426d4f11-5a6d-9fe5-69f8-3f2f2a8dfb82"},"outputs":[],"source":"#building a vocabulary\nvect = CountVectorizer()\nquestions = trainDF['question1'].append(trainDF['question2'])\nq = vect.fit_transform(questions.values.astype('U'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e052b9a-05a0-d54e-7244-b8d4cd7f23e0"},"outputs":[],"source":"#bulding training data\nq1 = vect.transform(trainDF['question1'].values.astype('U'))\nq2 = vect.transform(trainDF['question2'].values.astype('U'))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dde6fa3f-ba4c-54c0-4556-feb760b27ff3"},"outputs":[],"source":"#Training a decision tree on distance of q1-q2 matrices\nX_train = q1-q2\nY_train = trainDF['is_duplicate'].values\nx_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\ndtree = tree.DecisionTreeClassifier(max_depth = 10)\ndtree = dtree.fit(x_train, y_train)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cfd8f94-9a7b-c76a-7635-a28f994a820c"},"outputs":[],"source":"dtree.score(x_test,y_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"745957db-e2e1-c483-6273-51045c2e91d8"},"outputs":[],"source":"from nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer \nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n\nvect = CountVectorizer(ngram_range = (1,2), token_pattern=r'\\b\\w+\\b', \\\n                       analyzer = 'word', encoding='ascii',strip_accents = 'ascii',\\\n                       tokenizer=LemmaTokenizer()) \n\nc = vect.fit_transform(['Bi-grams are cool?', 'more hi.', 'kake metros'])\nc.toarray()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ef12061-a544-c04c-cc89-d23f3fe70083"},"outputs":[],"source":"\nlen(['bi-grams',\n 'are',\n 'cool',\n '?',\n 'more',\n 'hi',\n 'kake',\n 'metro',\n 'bi-grams are',\n 'are cool',\n 'cool ?',\n '? more',\n 'more hi',\n 'hi kake',\n 'kake metro'])"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}