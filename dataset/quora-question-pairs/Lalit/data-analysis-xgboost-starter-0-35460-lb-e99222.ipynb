{"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"}},"cells":[{"metadata":{"_uuid":"fa825f8c3abdc0e57d70f4f96b8bb62128737d17","_cell_guid":"9c355c2c-044d-2b87-e1cf-a6732fb9e802"},"execution_count":null,"source":"# Identifying Duplicate Questions\n","outputs":[],"cell_type":"markdown"},{"metadata":{"_cell_guid":"d9f5b5bf-b8a8-a88a-a16c-7666be39bd7e","trusted":false,"_uuid":"09ed4b347cfeaa1df219884f1b4551c5ba020319","_execution_state":"idle"},"execution_count":null,"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()\n\n#print('# File sizes')\n#for f in os.listdir('../input'):\n    #if 'zip' not in f:\n        #print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"bdb489f7f8d51bd7b153ca99fdd7e14e8866b32f","_cell_guid":"e9b03ade-149a-7d6a-d954-2db4ebb7806f"},"execution_count":null,"source":"## Training set","outputs":[],"cell_type":"markdown"},{"metadata":{"_cell_guid":"0ac4cab6-214b-957b-bb2a-01f7e8d5ed2b","trusted":false,"_uuid":"b4362ceb7cff02583f13a5d00cee58c8b1fd85eb","_execution_state":"idle"},"execution_count":null,"source":"df_train = pd.read_csv('../input/train.csv')\ndf_train.head()","outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"68e0a285-995b-ecb2-fde7-3486c982912b","trusted":false,"_uuid":"4d316c6cfd2a6b8f940d86f2455e372701b52c71","_execution_state":"idle"},"execution_count":null,"source":"print('Total number of question pairs for training: {}'.format(len(df_train)))\n## Always best to get these values in percentage\nprint('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\n#qids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\nqids = pd.Series(df_train['qid1'] + df_train['qid2'])\nprint(type(qids))\nprint('Total number of questions in the training data: {}'.format(len(np.unique(qids))))\nprint('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))\ndf_train.head()\n#plt.figure(figsize=(12, 5))\n#plt.hist(qids.value_counts(), bins=50)\n#plt.yscale('log', nonposy='clip')\n#plt.title('Log-Histogram of question appearance counts')\n#plt.xlabel('Number of occurences of question')\n#plt.ylabel('Number of questions')\n#print()","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"dec9c34563c0cba23b3f3997dc7d653c87c433c5","_cell_guid":"3bdf7cc4-4625-b5e6-996c-ecb063981bf2"},"execution_count":null,"source":"## Test Set","outputs":[],"cell_type":"markdown"},{"metadata":{"_cell_guid":"ff61e6b2-08b2-1734-db03-2db3ff5ee268","trusted":false,"_uuid":"a52d7f4245c1a477852073b8851ea3dad90e30f3","_execution_state":"idle"},"execution_count":null,"source":"df_test = pd.read_csv('../input/test.csv')\ndf_test.head()","outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"1477d9f9-7cbf-09f5-21af-6b1da642ee69","trusted":false,"_uuid":"ea633009bed9ad5f2a4a83a15b93c50b6c176d27","_execution_state":"idle"},"execution_count":null,"source":"print('Total number of question pairs for testing: {}'.format(len(df_test)))","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"4ec71b2a1c46e003a600671d3bb7caaf0a445445","_cell_guid":"39f2c240-caa5-675b-3e6e-f9195808b392"},"execution_count":null,"source":"## Text analysis","outputs":[],"cell_type":"markdown"},{"metadata":{"trusted":false,"_uuid":"fa9880192e3f25311b0f0739b02c69e8463c7b2f","_cell_guid":"ba41ab36-b287-4c5d-4ec5-a2c28034baa7"},"execution_count":null,"source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\ntest_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n\ndist_train = train_qs.apply(len)\ndist_test = test_qs.apply(len)\nplt.figure(figsize=(12, 6))\nplt.hist(dist_train, bins=200, range=[0, 200], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=200, range=[0, 200], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of character count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of characters', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"ce414db6bff5be086ab48d9186f0b0aa25559c97","_cell_guid":"0ed063c0-9d5c-bc09-9c48-22ff739859bd"},"execution_count":null,"source":"Let's do the same for word count. I'll be using a naive method for splitting words (splitting on spaces instead of using a serious tokenizer), although this should still give us a good idea of the distribution.","outputs":[],"cell_type":"markdown"},{"metadata":{"trusted":false,"_uuid":"52347e14006f1832d075e21a1077de27c40de879","_cell_guid":"2a86bc3f-1359-ef11-4411-ec5b54c6dca5"},"execution_count":null,"source":"dist_train = train_qs.apply(lambda x: len(x.split(' ')))\ndist_test = test_qs.apply(lambda x: len(x.split(' ')))\n\nplt.figure(figsize=(13, 6))\nplt.hist(dist_train, bins=50, range=[0, 50], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=50, range=[0, 50], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of word count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"27bb20843c147bb9a5166778a5976b4e6daf54a4","_cell_guid":"f7eb07e3-fe48-c4ca-359e-71e5c7a1a4b3"},"execution_count":null,"source":"We see a similar distribution for word count, with most questions being about 10 words long. It looks to me like the distribution of the training set seems more \"pointy\", while on the test set it is wider. Nevertheless, they are quite similar.","outputs":[],"cell_type":"markdown"},{"metadata":{"_uuid":"f879cdeb21694704434a37b6e9aece83a82780c8","_cell_guid":"e486c424-b529-e366-b455-77ee0df3aa84"},"execution_count":null,"source":"## Semantic Analysis\n\nNext, I will take a look at usage of different punctuation in questions - this may form a basis for some interesting features later on.","outputs":[],"cell_type":"markdown"},{"metadata":{"trusted":false,"_uuid":"74835bae53f1e8b059c3021ef6e4e807556e20f7","_cell_guid":"371a1e30-e5cd-7a24-97d7-b505b5aefb78"},"execution_count":null,"source":"qmarks = np.mean(train_qs.apply(lambda x: '?' in x))\nmath = np.mean(train_qs.apply(lambda x: '[math]' in x))\nfullstop = np.mean(train_qs.apply(lambda x: '.' in x))\ncapital_first = np.mean(train_qs.apply(lambda x: x[0].isupper()))\ncapitals = np.mean(train_qs.apply(lambda x: max([y.isupper() for y in x])))\nnumbers = np.mean(train_qs.apply(lambda x: max([y.isdigit() for y in x])))\n\nprint('Questions with question marks: {:.2f}%'.format(qmarks * 100))\nprint('Questions with [math] tags: {:.2f}%'.format(math * 100))\nprint('Questions with full stops: {:.2f}%'.format(fullstop * 100))\nprint('Questions with capitalised first letters: {:.2f}%'.format(capital_first * 100))\nprint('Questions with capital letters: {:.2f}%'.format(capitals * 100))\nprint('Questions with numbers: {:.2f}%'.format(numbers * 100))","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"0d5b9746a3a37a09409bfe87dc1ec23a623cf5c6","_cell_guid":"656b602a-e321-6ee4-682a-e76959bccc19"},"execution_count":null,"source":"# Initial Feature Analysis\n\nBefore we create a model, we should take a look at how powerful some features are. I will start off with the word share feature from the benchmark model.","outputs":[],"cell_type":"markdown"},{"metadata":{"trusted":false,"_uuid":"dca04409e58de08919ae2d406fd5f41f6ee591a1","_cell_guid":"ef13480a-2522-2d1f-cf8a-56670e0856ab"},"execution_count":null,"source":"from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    #q1words\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R\n\n\ntrain_word_match = df_train.apply(word_match_share, axis=1, raw=True)\n#plt.figure(figsize=(15, 5))\n#plt.hist(train_word_match[df_train['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\n#plt.hist(train_word_match[df_train['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\n#plt.legend()\n#plt.title('Label distribution over word_match_share', fontsize=15)\n#plt.xlabel('word_match_share', fontsize=15)","outputs":[],"cell_type":"code"},{"metadata":{"trusted":false,"_uuid":"4ef82e13e3e92387f8fb108a2dca05c42b85eea1","_cell_guid":"aad0b9b3-cae8-79b5-cee7-2a024cd8a2ad"},"execution_count":null,"source":"from sklearn.metrics import roc_auc_score\nprint('Original AUC:', roc_auc_score(df_train['is_duplicate'], train_word_match))","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"76de530e36c6649dfbac886078361de42b32dfc4","_cell_guid":"24ce20d7-2c37-9833-dfdb-b4aac6b37e96"},"execution_count":null,"source":"## TF-IDF","outputs":[],"cell_type":"markdown"},{"metadata":{"trusted":false,"_uuid":"5b800e5e7943a4ae6690df2d2cdfc58c06c89e9f","_cell_guid":"99d8e8e1-e2fd-25c6-137d-4492b850e1b5"},"execution_count":null,"source":"from collections import Counter\n\n# If a word appears only once, we ignore it completely (likely a typo)\n# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\n\neps = 5000 \nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}","outputs":[],"cell_type":"code"},{"metadata":{"trusted":false,"_uuid":"6351c2c69c1129c2c9e8ab232ebf75014c98d7a0","_cell_guid":"7dd8f860-da10-e807-812d-0098950f27da"},"execution_count":null,"source":"count","outputs":[],"cell_type":"code"},{"metadata":{"trusted":false,"_uuid":"fb4bb99a81c4bc370e035aded36d4e06ebde8501","_cell_guid":"d791c6b1-9860-7c3b-2b96-0767e607d6dc"},"execution_count":null,"source":"print('Most common words and weights: \\n')\nprint(sorted(weights.items(), key=lambda x: x[1] if x[1] > 0 else 9999)[:10])\nprint('\\nLeast common words and weights: ')\n(sorted(weights.items(), key=lambda x: x[1], reverse=True)[:10])","outputs":[],"cell_type":"code"},{"metadata":{"trusted":false,"_uuid":"b8874b4ae10e64a4d4faa565d7a9bbc3edff8092","_cell_guid":"2c8f2e86-f96b-5d36-e2f1-5f4c4c903e26"},"execution_count":null,"source":"def tfidf_word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    \n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    \n    R = np.sum(shared_weights) / np.sum(total_weights)\n    return R","outputs":[],"cell_type":"code"},{"metadata":{"trusted":false,"_uuid":"348a0f2fd51d071ef1d757a9c0202f9b552bd75d","_cell_guid":"157fd891-0b63-22df-4129-e4267cb55ff0"},"execution_count":null,"source":"plt.figure(figsize=(15, 5))\ntfidf_train_word_match = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\nplt.hist(tfidf_train_word_match[df_train['is_duplicate'] == 0].fillna(0), bins=20, normed=True, label='Not Duplicate')\nplt.hist(tfidf_train_word_match[df_train['is_duplicate'] == 1].fillna(0), bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over tfidf_word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)","outputs":[],"cell_type":"code"},{"metadata":{"trusted":false,"_uuid":"2dc3fb93415029ed097290f30fbbdcf16c1c7ec6","_cell_guid":"f1814b89-b895-f442-0ba2-b4e2fc6b6288"},"execution_count":null,"source":"from sklearn.metrics import roc_auc_score\nprint('Original AUC:', roc_auc_score(df_train['is_duplicate'], train_word_match))\nprint('   TFIDF AUC:', roc_auc_score(df_train['is_duplicate'], tfidf_train_word_match.fillna(0)))","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"9c2729e46f98699f17609632345e827a44a35669","_cell_guid":"28b46d9f-82fe-c37e-010c-a3e115687d41"},"execution_count":null,"source":"## Rebalancing the Data\nHowever, before I do this, I would like to rebalance the data that XGBoost receives, since we have 37% positive class in our training data, and only 17% in the test data. By re-balancing the data so our training set has 17% positives, we can ensure that XGBoost outputs probabilities that will better match the data on the leaderboard, and should get a better score (since LogLoss looks at the probabilities themselves and not just the order of the predictions like AUC)","outputs":[],"cell_type":"markdown"},{"metadata":{"trusted":false,"_uuid":"626f88b36f891354c26f307d23aa90b7b64fea25","_cell_guid":"0589da36-d092-c951-f3cd-68df3fd91c7a"},"execution_count":null,"source":"# First we create our training and testing data\nx_train = pd.DataFrame()\nx_test = pd.DataFrame()\nx_train['word_match'] = train_word_match\n#x_train['tfidf_word_match'] = tfidf_train_word_match\nx_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)\n#x_test['tfidf_word_match'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)\n\ny_train = df_train['is_duplicate'].values","outputs":[],"cell_type":"code"},{"metadata":{"trusted":false,"_uuid":"f84c14bf8e0011afbae706f681a265a5199bcbb9","_cell_guid":"5ccae7db-46e1-cf74-fdd8-4d4758bf460f"},"execution_count":null,"source":"pos_train = x_train[y_train == 1]\nneg_train = x_train[y_train == 0]\n\n# Now we oversample the negative class\n# There is likely a much more elegant way to do this...\np = 0.165\nscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\nwhile scale > 1:\n    neg_train = pd.concat([neg_train, neg_train])\n    scale -=1\nneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\nprint(len(pos_train) / (len(pos_train) + len(neg_train)))\n\nx_train = pd.concat([pos_train, neg_train])\ny_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\ndel pos_train, neg_train","outputs":[],"cell_type":"code"},{"metadata":{"trusted":false,"_uuid":"acc70cc5c92d1065f54dd778244b77958e2bc713","_cell_guid":"20684b12-fd83-bad6-b00b-77c51dd14c32"},"execution_count":null,"source":"# Finally, we split some of the data off for validation\nfrom sklearn.cross_validation import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"3ace0ddebce7ea444f1b237635f9d83844a0f298","_cell_guid":"9df47145-da6a-ad51-7bfb-1b588c4bab06"},"execution_count":null,"source":"## XGBoost\n\nNow we can finally run XGBoost on our data, in order to see the score on the leaderboard!","outputs":[],"cell_type":"markdown"},{"metadata":{"trusted":false,"_uuid":"148df9b0036d2befb6e140d3c5d9b2c309f5b7ee","_cell_guid":"a5f9f7fc-5ec4-fc9f-7fc0-608f5f25e7c6"},"execution_count":null,"source":"import xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)","outputs":[],"cell_type":"code"},{"metadata":{"trusted":false,"_uuid":"3b1afdac813a196d39b9a61b8d4076d671d54bfb","_cell_guid":"11616998-a057-45e0-ac3c-d129e5cfec36"},"execution_count":null,"source":"d_test = xgb.DMatrix(x_test)\np_test = bst.predict(d_test)\n\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = p_test\nsub.to_csv('simple_xgb.csv', index=False)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"005a52a1d922db4ece034e0ecf4555fbbe467f03","_cell_guid":"4b74fd4a-5e89-27e1-5ddd-f170bc370697"},"execution_count":null,"source":"**0.35460** on the leaderboard - a good first score!","outputs":[],"cell_type":"markdown"}],"nbformat":4,"nbformat_minor":0}