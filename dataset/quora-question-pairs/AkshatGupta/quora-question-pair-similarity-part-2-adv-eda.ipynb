{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Quora Question Pair Similarity: Advanced EDA, Data Preprocessing","metadata":{}},{"cell_type":"code","source":"!pip install distance\n!pip install fuzzywuzzy\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport os\nimport gc\n\nimport re\nimport distance\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nfrom sklearn.manifold import TSNE\nfrom wordcloud import WordCloud, STOPWORDS\nfrom os import path\nfrom PIL import Image\nimport shutil\n# import nltk\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2022-04-25T10:53:05.965968Z","iopub.execute_input":"2022-04-25T10:53:05.966282Z","iopub.status.idle":"2022-04-25T10:53:26.550335Z","shell.execute_reply.started":"2022-04-25T10:53:05.966248Z","shell.execute_reply":"2022-04-25T10:53:26.549399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.unpack_archive('../input/quora-question-pairs/train.csv.zip', '.')\nshutil.unpack_archive('../input/quora-question-pairs/test.csv.zip', '.')","metadata":{"execution":{"iopub.status.busy":"2022-04-25T10:13:20.723858Z","iopub.execute_input":"2022-04-25T10:13:20.724567Z","iopub.status.idle":"2022-04-25T10:13:28.946936Z","shell.execute_reply.started":"2022-04-25T10:13:20.724518Z","shell.execute_reply":"2022-04-25T10:13:28.946131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://stackoverflow.com/questions/12468179/unicodedecodeerror-utf8-codec-cant-decode-byte-0x9c\nif os.path.isfile('../input/df-fe-without-preprocessing-train/df_fe_without_preprocessing_train.csv'):\n    df = pd.read_csv(\"../input/df-fe-without-preprocessing-train/df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n    df = df.fillna('')\n    df.head()\nelse:\n    print(\"get df_fe_without_preprocessing_train.csv from drive or run the previous notebook\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T10:53:26.552118Z","iopub.execute_input":"2022-04-25T10:53:26.552381Z","iopub.status.idle":"2022-04-25T10:53:28.1753Z","shell.execute_reply.started":"2022-04-25T10:53:26.552325Z","shell.execute_reply":"2022-04-25T10:53:28.174566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T10:53:28.176237Z","iopub.execute_input":"2022-04-25T10:53:28.176787Z","iopub.status.idle":"2022-04-25T10:53:28.194584Z","shell.execute_reply.started":"2022-04-25T10:53:28.176746Z","shell.execute_reply":"2022-04-25T10:53:28.193768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing of Text** <br>\n* Removing HTML Tags\n* Removing Punctuations\n* Performing Stemming\n* Removing Stopwords\n* Expanding Contractions etc. ","metadata":{}},{"cell_type":"code","source":"# To get the results in 4 decemal points\nSAFE_DIV = 0.0001 \n\nSTOP_WORDS = stopwords.words(\"english\")\n\n\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    \n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n               \n    \n    return x\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-25T10:53:33.771493Z","iopub.execute_input":"2022-04-25T10:53:33.771791Z","iopub.status.idle":"2022-04-25T10:53:33.783283Z","shell.execute_reply.started":"2022-04-25T10:53:33.771761Z","shell.execute_reply":"2022-04-25T10:53:33.782549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Function to Compute and get the features : with 2 parameters of Question 1 and Question 2","metadata":{}},{"cell_type":"markdown","source":"**Advanced Feature Extraction(NLP and Fuzzy Features)**","metadata":{}},{"cell_type":"markdown","source":"Definition:\n- __Token__: You get a token by splitting sentence a space\n- __Stop_Word__ : stop words as per NLTK.\n- __Word__ : A token that is not a stop_word\n\n\nFeatures:\n- __cwc_min__ :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 <br>cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n<br>\n- __cwc_max__ :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 <br>cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n<br>\n- __csc_min__ :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2 <br> csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n<br>\n- __csc_max__ :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2<br>csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n<br>\n- __ctc_min__ :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2<br>ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n<br>\n- __ctc_max__ :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2<br>ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n<br>\n- __last_word_eq__ :  Check if First word of both questions is equal or not<br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n<br>\n- __first_word_eq__ :  Check if First word of both questions is equal or not<br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n<br>\n- __abs_len_diff__ :  Abs. length difference<br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n<br>\n- __mean_len__ :  Average Token Length of both Questions<br>mean_len = (len(q1_tokens) + len(q2_tokens))/2\n<br>\n- __fuzz_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __fuzz_partial_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __token_sort_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __token_set_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __longest_substr_ratio__ :  Ratio of length longest common substring to min lenghth of token count of Q1 and Q2<br>longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))\n","metadata":{}},{"cell_type":"code","source":"def get_token_features(q1, q2):\n    token_features = [0.0]*10\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n    return token_features\n\n# get the Longest Common sub string\n\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n\n    print(\"token features...\")\n    \n    # Merging Features with dataset\n    \n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n   \n    #Computing Fuzzy Features and Merging with Dataset\n    \n    # do read this blog: http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n    # https://stackoverflow.com/questions/31806695/when-to-use-which-fuzz-function-to-compare-2-strings\n    # https://github.com/seatgeek/fuzzywuzzy\n    print(\"fuzzy features..\")\n\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-25T10:54:06.357227Z","iopub.execute_input":"2022-04-25T10:54:06.358073Z","iopub.status.idle":"2022-04-25T10:54:06.386066Z","shell.execute_reply.started":"2022-04-25T10:54:06.358019Z","shell.execute_reply":"2022-04-25T10:54:06.385089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.isfile('nlp_features_train.csv'):\n    df = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n    df.fillna('')\nelse:\n    print(\"Extracting features for train:\")\n    df = pd.read_csv(\"train.csv\")\n    df = extract_features(df)\n    df.to_csv(\"nlp_features_train.csv\", index=False)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T10:54:18.571036Z","iopub.execute_input":"2022-04-25T10:54:18.571774Z","iopub.status.idle":"2022-04-25T11:11:57.983123Z","shell.execute_reply.started":"2022-04-25T10:54:18.571725Z","shell.execute_reply":"2022-04-25T11:11:57.982149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of extracted features** <br>\n\n* Plotting Word Clouds\n- Creating Word Cloud of Duplicates and Non-Duplicates Question pairs\n- We can observe the most frequent occuring words","metadata":{}},{"cell_type":"code","source":"df_duplicate = df[df['is_duplicate'] == 1]\ndfp_nonduplicate = df[df['is_duplicate'] ==0 ]\n\n#Converting 2d array array of q1 and q2 and flatten the array: like {{1,2}, {3,4}}\n\np = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nn = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\n\nprint(\"Number of datapoints in question 1 (duplicate pairs) : \", len(p))\nprint(\"Number of datapoints in question 0 (non duplicate pairs) : \", len(n))\n\n#Saving the np array into a text file\nnp.savetxt('train_p.txt', p, delimiter= ' ', fmt='%s')\nnp.savetxt('train_n.txt', p, delimiter= ' ', fmt='%s')","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:12:14.685472Z","iopub.execute_input":"2022-04-25T11:12:14.68575Z","iopub.status.idle":"2022-04-25T11:12:16.024622Z","shell.execute_reply.started":"2022-04-25T11:12:14.685721Z","shell.execute_reply":"2022-04-25T11:12:16.023777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reading the text files and removing the stopwords;\n\nd = path.dirname('')\n\ntextp_w = open(path.join(d, 'train_p.txt')).read()\ntextn_w = open(path.join(d, 'train_p.txt')).read()\n\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.add(\"not\")\n\nstopwords.remove(\"not\")\nstopwords.remove(\"no\")\nstopwords.remove(\"like\")\nprint(\"Total number of words in duplicate pair questions :\", len(textp_w))\nprint(\"Total number of words in non duplicate pair questions :\", len(textn_w))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:12:46.519975Z","iopub.execute_input":"2022-04-25T11:12:46.520437Z","iopub.status.idle":"2022-04-25T11:12:46.633178Z","shell.execute_reply.started":"2022-04-25T11:12:46.520393Z","shell.execute_reply":"2022-04-25T11:12:46.632322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Word Clouds generated from duplicate question's text**","metadata":{}},{"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords = stopwords)\nwc.generate(textp_w)\nprint(\"Word Cloud for Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:12:51.033568Z","iopub.execute_input":"2022-04-25T11:12:51.033851Z","iopub.status.idle":"2022-04-25T11:13:00.787276Z","shell.execute_reply.started":"2022-04-25T11:12:51.033818Z","shell.execute_reply":"2022-04-25T11:13:00.786654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Word Clouds generated from non duplicate pair question's text**","metadata":{}},{"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords = stopwords)\nwc.generate(textn_w)\nprint(\"Word Cloud for Non Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:13:00.788647Z","iopub.execute_input":"2022-04-25T11:13:00.789048Z","iopub.status.idle":"2022-04-25T11:13:10.127811Z","shell.execute_reply.started":"2022-04-25T11:13:00.789014Z","shell.execute_reply":"2022-04-25T11:13:10.126767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Pair plot of features ['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'] </h4>","metadata":{}},{"cell_type":"code","source":"n = df.shape[0]\nsns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:13:10.129841Z","iopub.execute_input":"2022-04-25T11:13:10.130624Z","iopub.status.idle":"2022-04-25T11:17:44.96337Z","shell.execute_reply.started":"2022-04-25T11:13:10.130558Z","shell.execute_reply":"2022-04-25T11:17:44.96241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Distribution of the token sort ratio**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:17:44.964946Z","iopub.execute_input":"2022-04-25T11:17:44.965246Z","iopub.status.idle":"2022-04-25T11:17:48.056636Z","shell.execute_reply.started":"2022-04-25T11:17:44.965215Z","shell.execute_reply":"2022-04-25T11:17:48.055704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:17:48.058157Z","iopub.execute_input":"2022-04-25T11:17:48.05859Z","iopub.status.idle":"2022-04-25T11:17:51.034262Z","shell.execute_reply.started":"2022-04-25T11:17:48.058536Z","shell.execute_reply":"2022-04-25T11:17:51.033414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualization using T-SNE with Dimensionality Reduction for 15 Features**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndfp_subsampled = df[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = dfp_subsampled['is_duplicate'].values","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:17:51.036696Z","iopub.execute_input":"2022-04-25T11:17:51.037003Z","iopub.status.idle":"2022-04-25T11:17:51.04903Z","shell.execute_reply.started":"2022-04-25T11:17:51.036961Z","shell.execute_reply":"2022-04-25T11:17:51.048401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:17:51.050425Z","iopub.execute_input":"2022-04-25T11:17:51.050726Z","iopub.status.idle":"2022-04-25T11:18:15.655433Z","shell.execute_reply.started":"2022-04-25T11:17:51.050683Z","shell.execute_reply":"2022-04-25T11:18:15.654756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:18:15.658992Z","iopub.execute_input":"2022-04-25T11:18:15.659573Z","iopub.status.idle":"2022-04-25T11:18:16.174548Z","shell.execute_reply.started":"2022-04-25T11:18:15.659535Z","shell.execute_reply":"2022-04-25T11:18:16.173499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne3d = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:18:16.175938Z","iopub.execute_input":"2022-04-25T11:18:16.177952Z","iopub.status.idle":"2022-04-25T11:19:14.250105Z","shell.execute_reply.started":"2022-04-25T11:18:16.177908Z","shell.execute_reply":"2022-04-25T11:19:14.249418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:19:14.251568Z","iopub.execute_input":"2022-04-25T11:19:14.252033Z","iopub.status.idle":"2022-04-25T11:19:15.206515Z","shell.execute_reply.started":"2022-04-25T11:19:14.25199Z","shell.execute_reply":"2022-04-25T11:19:15.205417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**End of Notebook** <br>\nModelling will be continued in Part 3: To see Quora Question Pair Similarity, Part 3, Machine Learning Modelling. Click Here","metadata":{}}]}