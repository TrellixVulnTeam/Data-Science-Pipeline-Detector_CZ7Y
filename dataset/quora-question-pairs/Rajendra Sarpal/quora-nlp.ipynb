{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport keras.layers as layers\nfrom keras.models import Model\nfrom keras import backend as K\nnp.random.seed(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/quora-question-pairs/train.csv.zip')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total number of question pairs for training: {}'.format(len(df)))\n\nprint('Duplicate pairs: {}%'.format(round(df['is_duplicate'].mean()*100, 2)))\nqids = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n\n\nprint('Total number of questions in the training data: {}'.format(len(\n    np.unique(qids))))\n\n\nprint('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))\n\n\nplt.figure(figsize=(12, 5))\nplt.hist(qids.value_counts(), bins=50)\nplt.yscale('log', nonposy='clip')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnull = df.isnull().sum()\nprint(null)\n\ndf = df.dropna(subset = ['question1', 'question2'])\n\npd.value_counts(df['is_duplicate']).plot.bar()\nplt.xlabel('Values')\nplt.ylabel('Count')\nplt.title('Class Distribution')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\n\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\nembed = hub.load(module_url)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def UniversalEmbedding(x):\n    return embed(tf.reshape(tf.cast(x, tf.string), [-1]))\n\nDROPOUT = 0.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nq1 = layers.Input(shape=(1,), dtype=tf.string)\nembedding_q1 = layers.Lambda(UniversalEmbedding, output_shape=(512,))(q1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nq2 = layers.Input(shape=(1,), dtype=tf.string)\nembedding_q2 = layers.Lambda(UniversalEmbedding, output_shape=(512,))(q2)\n\n\nmerged = layers.concatenate([embedding_q1, embedding_q2])\nmerged = layers.Dense(200, activation='relu')(merged)\nmerged = layers.Dropout(DROPOUT)(merged)\n\n\nmerged = layers.BatchNormalization()(merged)\nmerged = layers.Dense(200, activation='relu')(merged)\nmerged = layers.Dropout(DROPOUT)(merged)\n\nmerged = layers.BatchNormalization()(merged)\nmerged = layers.Dense(200, activation='relu')(merged)\nmerged = layers.Dropout(DROPOUT)(merged)\n\nmerged = layers.BatchNormalization()(merged)\nmerged = layers.Dense(200, activation='relu')(merged)\nmerged = layers.Dropout(DROPOUT)(merged)\n\n\nmerged = layers.BatchNormalization()(merged)\npred = layers.Dense(2, activation='sigmoid')(merged)\nmodel = Model(inputs=[q1,q2], outputs=pred)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q1= df.question1.tolist()\nq2= df.question2.tolist()\nlabels= df.is_duplicate.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX1 = df['question1']\nX2 = df['question2']\ny = df['is_duplicate']\n\nX1_train, X1_test,X2_train, X2_test, y_train, y_test = train_test_split(X1, X2, y, test_size=0.2, random_state=42)\n\ntrain_q1 = X1_train.tolist()\ntrain_q1 = np.array(train_q1, dtype=object)[:, np.newaxis]\ntrain_q2 = X2_train.tolist()\ntrain_q2 = np.array(train_q2, dtype=object)[:, np.newaxis]\n\ntrain_labels = np.asarray(pd.get_dummies(y_train), dtype = np.int8)\n\ntest_q1 = X1_test.tolist()\ntest_q1 = np.array(test_q1, dtype=object)[:, np.newaxis]\ntest_q2 = X2_test.tolist()\ntest_q2 = np.array(test_q2, dtype=object)[:, np.newaxis]\n\ntest_labels = np.asarray(pd.get_dummies(y_test), dtype = np.int8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\n# Creating the tensorflow session to train the model and save checkpoint after every epoch.\ntf.compat.v1.keras.backend.set_session(tf.compat.v1.Session());\n\n\n\nfilepath=\"./model.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=True, mode='auto', period=1)\ncallbacks_list = [checkpoint]\n\nhistory = model.fit([train_q1, train_q2], \n        train_labels,\n        validation_data=([test_q1, test_q2], test_labels),\n        epochs=10,\n        batch_size=512, callbacks=callbacks_list)\nmodel.save('./final_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\nq1 = \"What is one coin?\t\"\nq2 =  \"What's this coin?\"\nq1 = np.array([[q1],[q1]])\nq2 = np.array([[q2],[q2]])\n\n\n# Using the same tensorflow session for embedding the test string\n\ntf.compat.v1.keras.backend.set_session(tf.compat.v1.Session());\nmodel.load_weights('./model.hdf5')  \n# Predicting the similarity between the two input questions \npredicts = model.predict([q1, q2], verbose=0)\npredict_logits = predicts.argmax(axis=1)\nprint(\"----FINAL RESULT----\")\nif(predict_logits[0] == 1):\n    print(\"****Questions are Similar****\")\nelse:\n    print(\"****Questions are not Similar****\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}