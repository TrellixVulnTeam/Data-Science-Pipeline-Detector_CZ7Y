{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align:center;font-size:30px;\" >Quora Question Pairs : Sentence Transformers and BERT for Semantic Similarity</h1>","metadata":{}},{"cell_type":"markdown","source":"This work demonstrates how to find textual similarity between a pair of documents using Sentence Transformers and pre-trained BERT model. In this work, I have used the \"Quora Question Pairs\" dataset, the details about which can be found from [here](http://www.kaggle.com/c/quora-question-pairs) . ","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"# For printing all the outputs of a cell in the same output window\n\n# from IPython.core.interactiveshell import InteractiveShell  \n# InteractiveShell.ast_node_interactivity = \"all\"         #for enabling\n# InteractiveShell.ast_node_interactivity = \"last_expr\"   #for disabling\n\n\n# Basic Libraries\n\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport re\nimport string\nimport random\nimport math\nimport time\nimport os\nfrom os import listdir\nimport itertools\nimport collections\nfrom collections import Counter, defaultdict\nfrom tqdm import tqdm\nfrom sklearn import utils\n\n# Visualization\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n# Vector Similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# evaluation metrics\nfrom sklearn import metrics\n\n\n# Deep learning\n\n# import tensorflow\n# from tensorflow import keras\n# from tensorflow.keras import backend as KB\n# # from tensorflow.keras import models, layers, preprocessing as keras_processing\n\n\n# Bert language model\n\n!pip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\n!pip install transformers\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# model serialization\nimport pickle","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## Load Dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/quora-question-pairs/train.csv.zip')\nprint(\"Train Dataframe:\")\ntrain_df.head(3)\nprint(f'Train dataframe contains {train_df.shape[0]} samples.')\nprint('Number of features in train data : ', train_df.shape[1])\nprint('Train Features : ', train_df.columns.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset contains below data fields:**\n\n- id:  a simple rowID\n- qid(1, 2):  unique IDs of each question in the pair\n- question(1, 2):  actual text contents of the questions.\n- is_duplicate:  the label we are trying to Predict, i.e. whether the two questions are duplicates of each other.","metadata":{}},{"cell_type":"markdown","source":"## Dataset Analysis","metadata":{}},{"cell_type":"markdown","source":"**Dataset Complete Information at a glance Using Pandas Profiling:**\n\nPandas profiling is a python package which helps us understand our data. It is a simple and fast way to perform exploratory data analysis of a Pandas Dataframe. The Pandas Profiling function extends the pandas DataFrame with df.profile_report() for quick data analysis. It displays a lot of information with a single line of code and that too in an interactive HTML report","metadata":{}},{"cell_type":"code","source":"train_df.profile_report()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check the basic stats of the data:**\n\nThe pandas df.describe() and df.info() functions gives us a basic overview of the entire dataset.","metadata":{}},{"cell_type":"code","source":"# Null values and Data types\nprint('Train Set:\\n')\nprint(train_df.info())\nprint('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# basic stats\nprint('Train set basic stats:')\ntrain_df.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can observe missing values are present in the data, let's handle these values.","metadata":{}},{"cell_type":"markdown","source":"### Handling the Missing Values","metadata":{}},{"cell_type":"code","source":"print('Train data Null values :')\ntrain_df[train_df.isnull().any(1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are 3 null values for question1 and question2 texts which are present across only 3 samples, so we will fill these null values with empty strings.","metadata":{}},{"cell_type":"code","source":"train_df = train_df.fillna(value=\"\")\ntrain_df[train_df.isnull().any(1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check the distribution of output labels:**","metadata":{}},{"cell_type":"code","source":"train_df.is_duplicate.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\ntrain_df.is_duplicate.value_counts().plot(kind='bar', color=['r','g'])\n\nD = mpatches.Patch(color='r', label='Duplicate')\nND = mpatches.Patch(color='g', label='Non-Duplicate')\n\nplt.legend(handles=[D,ND], loc='best')\n\nplt.xlabel('Type of Labels')\nplt.ylabel('Count of Data per Label Category')\nplt.title('Distribution of labels')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check the distribution sentence lengths for Question 1 and Question 2 :**","metadata":{}},{"cell_type":"code","source":"q1_lengths = [len(q1)for q1 in train_df.question1]\nprint(\"Mean sentence length for Question1:\", np.mean(q1_lengths))\n\nplt.figure(figsize=(8,6))\nplt.hist(q1_lengths,bins=50,density=True,color='b')\n# sns.distplot(q1_lengths,bins=50,kde=True,color='b')\nplt.xlabel('Question1 lengths')\nplt.ylabel('Count of Question1 lengths')\nplt.title('Distribution of Question1 sentence lengths')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q2_lengths = [len(q2)for q2 in train_df.question2]\nprint(\"Mean sentence length for Question2:\", np.mean(q2_lengths))\n\nplt.figure(figsize=(8,6))\nplt.hist(q2_lengths,bins=50,density=True,color='r')\n#sns.distplot(q2_lengths,bins=50,kde=True,color='r')\nplt.xlabel('Question2 lengths')\nplt.ylabel('Count of Question2 lengths')\nplt.title('Distribution of Question2 sentence lengths')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing Sentence Transformers","metadata":{}},{"cell_type":"markdown","source":"Generate sentences lists for question1 and question2.","metadata":{}},{"cell_type":"code","source":"sentences_question1 = list(sent for sent in train_df['question1'].values)\nsentences_question2 = list(sent for sent in train_df['question2'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this approach I have defined a model for **Sentence Transformation** using '[bert-base-nli-mean-tokens](http://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens)' repository. The sentence-transformers repository allows to train and use Transformer models for generating sentence and text embeddings. The model is described in the paper [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](http://arxiv.org/abs/1908.10084)","metadata":{}},{"cell_type":"code","source":"st_model = SentenceTransformer('bert-base-nli-mean-tokens')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define a method for generating sentence embeddings for each sentence using Sentnce Transformers model.","metadata":{}},{"cell_type":"code","source":"def generate_sent_embeddings(data):\n    return st_model.encode(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generating sentence embeddings using pretrained sentence transformers is a very time exhaustive process and the time complexity increases with the increase in the data size. So here I have generated the sentence embeddings at first and then reusing the generated embeddings using pickle serialisation.\nSo let's use sentence embeddings for question1 and question2 text generated using pre-trained sentence transformers.","metadata":{}},{"cell_type":"code","source":"if os.path.isfile('../input/quora-question-pairs-sentence-transformers/question1_sent_embeddings.pkl'):\n    #retrieve the question1_sent_embeddings list for usage.\n    with open('../input/quora-question-pairs-sentence-transformers/question1_sent_embeddings.pkl', 'rb') as f: \n        question1_sent_embeddings = pickle.load(f)\nelse:\n    question1_sent_embeddings = generate_sent_embeddings(sentences_question1)\n    #save the question1_sent_embeddings list for later usage.\n    with open('question1_sent_embeddings.pkl', 'wb') as f: \n        pickle.dump(question1_sent_embeddings, f)\n\nprint(\"shape of question1 sentence embeddings:\", question1_sent_embeddings.shape)\ntrain_df['question1_sent_embeddings'] = pd.DataFrame({'question1_sent_embeddings' : list(question1_sent_embeddings)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.isfile('../input/quora-question-pairs-sentence-transformers/question2_sent_embeddings.pkl'):\n    #retrieve the question2_sent_embeddings list for usage.\n    with open('../input/quora-question-pairs-sentence-transformers/question2_sent_embeddings.pkl', 'rb') as f: \n        question2_sent_embeddings = pickle.load(f)\nelse:\n    question2_sent_embeddings = generate_sent_embeddings(sentences_question2)\n    #save the question2_sent_embeddings list for later usage.\n    with open('question2_sent_embeddings.pkl', 'wb') as f: \n        pickle.dump(question2_sent_embeddings, f)\n\nprint(\"shape of question2 sentence embeddings:\", question2_sent_embeddings.shape)\ntrain_df['question2_sent_embeddings'] = pd.DataFrame({'question2_sent_embeddings' : list(question2_sent_embeddings)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's generate textual similarity values for question1 and question2 sentence embeddings using cosine similarity","metadata":{}},{"cell_type":"code","source":"questions_similarity = []\nfor index, row in train_df.iterrows():\n    questions_similarity.append(cosine_similarity([row['question1_sent_embeddings']],[row['question2_sent_embeddings']]))\n\n#convert the question similarity array into 1d array \nquestions_similarity = np.stack(questions_similarity,axis=0)\n# questions_similarity = questions_similarity.tolist()\nques_sim = np.array(questions_similarity).ravel()\n\n# store the question similarity scores in our dataframe\ntrain_df['questions_similarity'] = pd.DataFrame({'questions_similarity' : ques_sim})\ntrain_df['questions_similarity']","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We know that the cosine similarity values ranges between 0 and 1, so we can convert these sentence similarity values into predicted labels (0 and 1) by setting an appropriate threshold for similarity.","metadata":{}},{"cell_type":"code","source":"def similarity_to_predictions(cos_sim, threshold):\n    \"\"\"\n    This function converts the predicted similarities to predicted labels based on the threshold value\n    \"\"\"\n    if (cos_sim >= threshold):\n        return 1\n    else:\n        return 0\n    \ntrain_df['predicted_result'] = train_df['questions_similarity'].apply(similarity_to_predictions, threshold=0.87)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's check the our dataset after processing.","metadata":{}},{"cell_type":"code","source":"train_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At last, we can compare the predictions ('predicted_result') with actual results ('is_duplicate') to check accuracy of the Quora question pairs similarity.","metadata":{}},{"cell_type":"code","source":"metrics.accuracy_score(train_df['is_duplicate'], train_df['predicted_result'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Concluding sentence transformers approach :**\n\nIn this work, I have implemented the Sentence Transformers model based on pre-trained \"BERT\" embeddings. The accuracy of predictions can be adjusted using an appropriate threshold value for sentence similarity. This implementation focuses on simplistic usage of the pre-trained \"BERT\" embeddings and cosine-similarity for evaluating the textual similarity between pair of sentences.","metadata":{}}]}