{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Quora Question Pairs: EDA & Visualization\n- Hola amigos, this notebook covers my code for the EDA (Exploratory Data Analysis) & Visualization of  **Quora Question Pairs** challenge, which can be found [here](https://www.kaggle.com/c/quora-question-pairs).\n- This is the **first notebook**. Check out my [second notebook](https://www.kaggle.com/elemento/quora-questionpairs-modeling), that covers the **Modeling** aspect of this competition.","metadata":{}},{"cell_type":"markdown","source":"# Installing & Importing Packages","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2021-10-03T09:30:15.664742Z","iopub.execute_input":"2021-10-03T09:30:15.665405Z","iopub.status.idle":"2021-10-03T09:30:15.742175Z","shell.execute_reply.started":"2021-10-03T09:30:15.665285Z","shell.execute_reply":"2021-10-03T09:30:15.741179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\nimport sys\n\nimport re\nimport time\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nfrom sklearn.manifold import TSNE\nfrom wordcloud import WordCloud, STOPWORDS\nfrom os import path\nfrom PIL import Image\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n%matplotlib inline\n\n# Extract Word2Vec vectors\n# https://github.com/explosion/spaCy/issues/1721\n# http://landinghub.visualstudio.com/visual-cpp-build-tools\nimport spacy","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:18.76501Z","iopub.execute_input":"2021-10-03T09:30:18.765339Z","iopub.status.idle":"2021-10-03T09:30:21.645315Z","shell.execute_reply.started":"2021-10-03T09:30:18.765306Z","shell.execute_reply":"2021-10-03T09:30:21.644399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %load \"../input/d/elemento/quora-question-pairs/distance/distance/_lcsubstrings.py\"\nfrom array import array\n\ndef lcsubstrings(seq1, seq2, positions=False):\n\t\"\"\"Find the longest common substring(s) in the sequences `seq1` and `seq2`.\n\t\n\tIf positions evaluates to `True` only their positions will be returned,\n\ttogether with their length, in a tuple:\n\t\n\t\t(length, [(start pos in seq1, start pos in seq2)..])\n\t\n\tOtherwise, the substrings themselves will be returned, in a set.\n\t\n\tExample:\n\t\n\t\t>>> lcsubstrings(\"sedentar\", \"dentist\")\n\t\t{'dent'}\n\t\t>>> lcsubstrings(\"sedentar\", \"dentist\", positions=True)\n\t\t(4, [(2, 0)])\n\t\"\"\"\n\tL1, L2 = len(seq1), len(seq2)\n\tms = []\n\tmlen = last = 0\n\tif L1 < L2:\n\t\tseq1, seq2 = seq2, seq1\n\t\tL1, L2 = L2, L1\n\t\n\tcolumn = array('L', range(L2))\n\t\n\tfor i in range(L1):\n\t\tfor j in range(L2):\n\t\t\told = column[j]\n\t\t\tif seq1[i] == seq2[j]:\n\t\t\t\tif i == 0 or j == 0:\n\t\t\t\t\tcolumn[j] = 1\n\t\t\t\telse:\n\t\t\t\t\tcolumn[j] = last + 1\n\t\t\t\tif column[j] > mlen:\n\t\t\t\t\tmlen = column[j]\n\t\t\t\t\tms = [(i, j)]\n\t\t\t\telif column[j] == mlen:\n\t\t\t\t\tms.append((i, j))\n\t\t\telse:\n\t\t\t\tcolumn[j] = 0\n\t\t\tlast = old\n\t\n\tif positions:\n\t\treturn (mlen, tuple((i - mlen + 1, j - mlen + 1) for i, j in ms if ms))\n\treturn set(seq1[i - mlen + 1:i + 1] for i, _ in ms if ms)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:31.814398Z","iopub.execute_input":"2021-10-03T09:30:31.814683Z","iopub.status.idle":"2021-10-03T09:30:31.826804Z","shell.execute_reply.started":"2021-10-03T09:30:31.814655Z","shell.execute_reply":"2021-10-03T09:30:31.825674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Business Problem \n## 1.1 Description \n<p>Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.</p>\n<p>\nOver 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n</p>\n<b>Credits</b>: Kaggle \n\n#### Problem Statement\n- Identify which questions asked on Quora are duplicates of questions that have already been asked. \n- This could be useful to instantly provide answers to questions that have already been answered. \n- We are tasked with predicting whether a pair of questions are duplicates or not. \n\n## 1.2 Sources/Useful Links\n- Source : https://www.kaggle.com/c/quora-question-pairs\n- Discussions : https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb/comments\n- Kaggle Winning Solution and other approaches: https://www.dropbox.com/sh/93968nfnrzh8bp5/AACZdtsApc1QSTQc7X0H3QZ5a?dl=0\n- Blog 1 : https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning\n- Blog 2 : https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30\n\n## 1.3 Real world/Business Objectives and Constraints\n1. The cost of a mis-classification can be very high. For instance, if 2 questions have a slightly different context, and if we classify them as same questions, then we will simply show all the answers of Q1 for Q2 as well.\n2. But any user who will checkout the answers of Q2 will simply be disappointed, and will lose his/her trust in Quora's ecosystem as a platform of getting amazing answers.\n3. You would want a probability of a pair of questions to be duplicates so that you can choose any threshold of choice.\n4. If our model simply outputs 0 or 1 for a pair of questions, we won't be able to set custom thresholds like in the above case.\n5. No strict latency concerns.\n6. Interpretability is partially important. This is because we would like to have a sense as to why our model is classifying 2 questions as similar/dissimilar. But the user won't ask this question, and hence, it's only partially important.","metadata":{"execution":{"iopub.status.busy":"2021-09-25T06:07:34.153362Z","iopub.execute_input":"2021-09-25T06:07:34.153675Z","iopub.status.idle":"2021-09-25T06:07:34.158535Z","shell.execute_reply.started":"2021-09-25T06:07:34.153643Z","shell.execute_reply":"2021-09-25T06:07:34.157633Z"}}},{"cell_type":"markdown","source":"# 2. Machine Learning Problem\n## 2.1 Data\n### 2.1.1 Data Overview\n- Data will be in a file Train.csv\n- Train.csv contains 5 columns : qid1, qid2, question1, question2, is_duplicate\n- Size of Train.csv - 60MB \n- Number of rows in Train.csv = 404,290\n\n### 2.1.2 Example Data point\n<pre>\n\"id\",\"qid1\",\"qid2\",\"question1\",\"question2\",\"is_duplicate\"\n\"0\",\"1\",\"2\",\"What is the step by step guide to invest in share market in india?\",\"What is the step by step guide to invest in share market?\",\"0\"\n\"1\",\"3\",\"4\",\"What is the story of Kohinoor (Koh-i-Noor) Diamond?\",\"What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\",\"0\"\n\"7\",\"15\",\"16\",\"How can I be a good geologist?\",\"What should I do to be a great geologist?\",\"1\"\n\"11\",\"23\",\"24\",\"How do I read and find my YouTube comments?\",\"How can I see all my Youtube comments?\",\"1\"\n</pre>\n\n## 2.2 Mapping the real world problem to an ML problem\n### 2.2.1 Type of Machine Leaning Problem ###\nIt is a binary classification problem, for a given pair of questions we need to predict if they are duplicate or not.\n\n### 2.2.2 Performance Metric \nSource: https://www.kaggle.com/c/quora-question-pairs#evaluation\nMetric(s): \n* log-loss : https://www.kaggle.com/wiki/LogarithmicLoss\n* Binary Confusion Matrix\n\n## 2.3 Train and Test Construction\n- We build train and test by randomly splitting in the ratio of 70:30 or 80:20 whatever we choose, as we have sufficient points to work with.\n- However, if we would have been a Machine Learning Engineer at Quora, then we would have the **timestamp** of the questions, and it would be much better to perform a **time-based splitting**.\n- This is because Quora consists questions from many **highly dynamic topics** such as politics, government policies, etc. In simple words, the questions which are asked after our model is deployed may differ from the previous existing questions to a great extent, and hence, a time-based splitting is a better choice than random splitting for this problem.\n- But since we don't have the timestamp(s) of the questions, hence, we will simply perform a **random splitting** in this case.","metadata":{}},{"cell_type":"markdown","source":"# 3. Exploratory Data Analysis\n## 3.1 Reading data and basic stats","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")\nprint(\"Number of data points:\",df.shape[0])\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:35.232117Z","iopub.execute_input":"2021-10-03T09:30:35.232626Z","iopub.status.idle":"2021-10-03T09:30:37.873239Z","shell.execute_reply.started":"2021-10-03T09:30:35.232581Z","shell.execute_reply":"2021-10-03T09:30:37.872343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:38.190053Z","iopub.execute_input":"2021-10-03T09:30:38.190363Z","iopub.status.idle":"2021-10-03T09:30:38.316161Z","shell.execute_reply.started":"2021-10-03T09:30:38.190333Z","shell.execute_reply":"2021-10-03T09:30:38.315332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are given a minimal number of data fields here, consisting of:\n- id:  Looks like a simple rowID\n- qid{1, 2}:  The unique ID of each question in the pair\n- question{1, 2}:  The actual textual contents of the questions.\n- is_duplicate:  The label that we are trying to predict - whether the two questions are duplicates of each other.","metadata":{}},{"cell_type":"markdown","source":"### 3.2.1 Distribution of data points among output classes\n- Number of duplicate(smilar) and non-duplicate(non similar) questions","metadata":{}},{"cell_type":"code","source":"df.groupby(\"is_duplicate\")['id'].count().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:39.993606Z","iopub.execute_input":"2021-10-03T09:30:39.993877Z","iopub.status.idle":"2021-10-03T09:30:40.243281Z","shell.execute_reply.started":"2021-10-03T09:30:39.993851Z","shell.execute_reply":"2021-10-03T09:30:40.242098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('~> Total number of question pairs for training:\\n   {}'.format(len(df)))\nprint('~> Question pairs are not Similar (is_duplicate = 0):\\n   {}%'.format(100 - round(df['is_duplicate'].mean()*100, 2)))\nprint('~> Question pairs are Similar (is_duplicate = 1):\\n   {}%'.format(round(df['is_duplicate'].mean()*100, 2)))","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:40.300548Z","iopub.execute_input":"2021-10-03T09:30:40.300847Z","iopub.status.idle":"2021-10-03T09:30:40.309705Z","shell.execute_reply.started":"2021-10-03T09:30:40.300819Z","shell.execute_reply":"2021-10-03T09:30:40.308323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2 Number of unique questions","metadata":{}},{"cell_type":"code","source":"qids = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\nunique_qs = len(np.unique(qids))\nqs_morethan_onetime = np.sum(qids.value_counts() > 1)\n\nprint ('Total number of  Unique Questions are: {}'.format(unique_qs))\nprint ('Number of unique questions that appear more than one time: {} ({}%)'.format(qs_morethan_onetime,qs_morethan_onetime/unique_qs*100))\nprint ('Max number of times a single question is repeated: {}'.format(max(qids.value_counts()))) \n\nq_vals = qids.value_counts()\nq_vals = q_vals.values","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:41.184042Z","iopub.execute_input":"2021-10-03T09:30:41.184349Z","iopub.status.idle":"2021-10-03T09:30:42.039316Z","shell.execute_reply.started":"2021-10-03T09:30:41.18432Z","shell.execute_reply":"2021-10-03T09:30:42.038678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = [\"unique_questions\" , \"Repeated Questions\"]\ny =  [unique_qs , qs_morethan_onetime]\n\nplt.figure(figsize=(10, 6))\nplt.title (\"Plot representing unique and repeated questions\")\nsns.barplot(x = x, y = y)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:42.040571Z","iopub.execute_input":"2021-10-03T09:30:42.040918Z","iopub.status.idle":"2021-10-03T09:30:42.226171Z","shell.execute_reply.started":"2021-10-03T09:30:42.040891Z","shell.execute_reply":"2021-10-03T09:30:42.22524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.3 Checking for Duplicates","metadata":{}},{"cell_type":"code","source":"# Checking whether there are any repeated pair of questions\npair_duplicates = df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\nprint (\"Number of duplicate questions\", (pair_duplicates).shape[0] - df.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:42.375827Z","iopub.execute_input":"2021-10-03T09:30:42.376164Z","iopub.status.idle":"2021-10-03T09:30:42.63725Z","shell.execute_reply.started":"2021-10-03T09:30:42.376115Z","shell.execute_reply":"2021-10-03T09:30:42.63633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.4 Number of occurrences of each question","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.hist(qids.value_counts(), bins=160)\nplt.yscale('log', nonpositive='clip')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\n\nprint ('Maximum number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) ","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:42.966045Z","iopub.execute_input":"2021-10-03T09:30:42.966368Z","iopub.status.idle":"2021-10-03T09:30:44.570213Z","shell.execute_reply.started":"2021-10-03T09:30:42.96633Z","shell.execute_reply":"2021-10-03T09:30:44.569308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.5 Checking for NULL values","metadata":{}},{"cell_type":"code","source":"# Checking whether there are any rows with null values\nnan_rows = df[df.isnull().any(1)]\nprint (nan_rows)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:44.571948Z","iopub.execute_input":"2021-10-03T09:30:44.57223Z","iopub.status.idle":"2021-10-03T09:30:44.681443Z","shell.execute_reply.started":"2021-10-03T09:30:44.572201Z","shell.execute_reply":"2021-10-03T09:30:44.680482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are two rows with null values in question2\n# Filling the null values with ' '\ndf = df.fillna('')\nnan_rows = df[df.isnull().any(1)]\nprint (nan_rows)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:44.682372Z","iopub.execute_input":"2021-10-03T09:30:44.682613Z","iopub.status.idle":"2021-10-03T09:30:44.941374Z","shell.execute_reply.started":"2021-10-03T09:30:44.682587Z","shell.execute_reply":"2021-10-03T09:30:44.940399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Basic Feature Extraction (before cleaning)\nLet us now construct a few features like:\n - ____freq_qid1____ = Frequency of qid1's\n - ____freq_qid2____ = Frequency of qid2's \n - ____q1len____ = Length of q1\n - ____q2len____ = Length of q2\n - ____q1_n_words____ = Number of words in Question 1\n - ____q2_n_words____ = Number of words in Question 2\n - ____word_Common____ = (Number of common unique words in Question 1 and Question 2)\n - ____word_Total____ =(Total num of words in Question 1 + Total num of words in Question 2)\n - ____word_share____ = (word_common)/(word_Total)\n - ____freq_q1+freq_q2____ = sum total of frequency of qid1 and qid2 \n - ____freq_q1-freq_q2____ = absolute difference of frequency of qid1 and qid2 ","metadata":{}},{"cell_type":"code","source":"if os.path.isfile('../input/d/elemento/quora-question-pairs/df_fe_without_preprocessing_train.csv'):\n    df = pd.read_csv(\"../input/d/elemento/quora-question-pairs/df_fe_without_preprocessing_train.csv\",encoding='latin-1')\nelse:\n    df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \n    df['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\n    df['q1len'] = df['question1'].str.len() \n    df['q2len'] = df['question2'].str.len()\n    df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n    df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n\n    def normalized_word_Common(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * len(w1 & w2)\n    df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n\n    def normalized_word_Total(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * (len(w1) + len(w2))\n    df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n\n    def normalized_word_share(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n    df['word_share'] = df.apply(normalized_word_share, axis=1)\n\n    df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n    df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n\n    df.to_csv(\"df_fe_without_preprocessing_train.csv\", index=False)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:44.942949Z","iopub.execute_input":"2021-10-03T09:30:44.943201Z","iopub.status.idle":"2021-10-03T09:30:47.742555Z","shell.execute_reply.started":"2021-10-03T09:30:44.94317Z","shell.execute_reply":"2021-10-03T09:30:47.741732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.1 Analysis of some of the extracted features\n- Here are some questions have only one single words.","metadata":{}},{"cell_type":"code","source":"print (\"Minimum length of the questions in question1 : \" , min(df['q1_n_words']))\nprint (\"Minimum length of the questions in question2 : \" , min(df['q2_n_words']))\nprint (\"Number of Questions with minimum length [question1] :\", df[df['q1_n_words']== 1].shape[0])\nprint (\"Number of Questions with minimum length [question2] :\", df[df['q2_n_words']== 1].shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:47.744229Z","iopub.execute_input":"2021-10-03T09:30:47.744463Z","iopub.status.idle":"2021-10-03T09:30:47.872431Z","shell.execute_reply.started":"2021-10-03T09:30:47.744437Z","shell.execute_reply":"2021-10-03T09:30:47.871322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.1.1 Feature: word_share\n- The distributions for normalized word_share have some overlap on the far right-hand side, i.e., there are quite a lot of questions with high word similarity\n- The average word share and Common no. of words of qid1 and qid2 is more when they are duplicate(Similar).\n- In our univariate data analysis, we learnt that if for a particular feature, the distributions are completely over-lapping, then it's the worst case.\n- And similarly, if the distributions are completely separate, then it's the best case. In our case, we have a got the middle of the both worlds.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_share', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.histplot(df[df['is_duplicate'] == 1.0]['word_share'][0:] , kde = True, label = \"1\", color = '#f49093')\nsns.histplot(df[df['is_duplicate'] == 0.0]['word_share'][0:] , kde = True, label = \"0\" , color = '#9999ff')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:47.8737Z","iopub.execute_input":"2021-10-03T09:30:47.874004Z","iopub.status.idle":"2021-10-03T09:30:52.155952Z","shell.execute_reply.started":"2021-10-03T09:30:47.873965Z","shell.execute_reply":"2021-10-03T09:30:52.15526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.1.2 Feature: word_Common\n- The distributions of the word_Common feature in similar and non-similar questions are highly overlapping.\n- Now, if we compare the situation of `word_share` and `word_Common` with each other, we will find that the distributions of word_Common are slightly more overlapping than those of word_share, and hence, word_share is slightly a better feature than word_Common.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_Common', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.histplot(df[df['is_duplicate'] == 1.0]['word_Common'][0:] , kde = True, label = \"1\", color = '#f49093')\nsns.histplot(df[df['is_duplicate'] == 0.0]['word_Common'][0:] , kde = True, label = \"0\" , color = '#9999ff')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:52.157698Z","iopub.execute_input":"2021-10-03T09:30:52.158278Z","iopub.status.idle":"2021-10-03T09:30:57.080834Z","shell.execute_reply.started":"2021-10-03T09:30:52.158244Z","shell.execute_reply":"2021-10-03T09:30:57.079857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Preprocessing of Text\n- Preprocessing\n    - Removing html tags \n    - Removing Punctuations\n    - Performing stemming\n    - Removing Stopwords\n    - Expanding contractions etc","metadata":{}},{"cell_type":"code","source":"# To get the results in 4 decimal points\nSAFE_DIV = 0.0001 \nSTOP_WORDS = stopwords.words(\"english\")\n\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n       .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n       .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n       .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n       .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n       .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n       .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n               \n    return x","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:57.082447Z","iopub.execute_input":"2021-10-03T09:30:57.082875Z","iopub.status.idle":"2021-10-03T09:30:57.101462Z","shell.execute_reply.started":"2021-10-03T09:30:57.082828Z","shell.execute_reply":"2021-10-03T09:30:57.10059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Function to Compute and get the features : With 2 parameters of Question 1 and Question 2**\n## 3.5 Advanced Feature Extraction (NLP and Fuzzy Features)\n\nDefinition:\n- __Token__: You get a token by splitting sentence with the space character\n- __Stop_Word__ : stop words as per NLTK.\n- __Word__ : A token that is not a stop_word\n\n\nFeatures:\n- __cwc_min__ :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 <br>cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n<br>\n- __cwc_max__ :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 <br>cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n<br>\n- __csc_min__ :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2 <br> csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n<br>\n- __csc_max__ :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2<br>csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n<br>\n- __ctc_min__ :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2<br>ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n<br>\n- __ctc_max__ :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2<br>ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n<br>\n- __last_word_eq__ :  Check if the last word of both questions is equal or not<br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n<br>\n- __first_word_eq__ :  Check if the first word of both questions is equal or not<br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n<br>\n- __abs_len_diff__ :  Abs. length difference<br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n<br>\n- __mean_len__ :  Average Token Length of both Questions<br>mean_len = (len(q1_tokens) + len(q2_tokens))/2\n<br>\n- __fuzz_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage<br>\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __fuzz_partial_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage<br>\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __token_sort_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage<br>\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __token_set_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage<br>\nhttp://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n<br>\n- __longest_substr_ratio__ :  Ratio of length longest common substring to min lenghth of token count of Q1 and Q2<br>longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))","metadata":{}},{"cell_type":"code","source":"def get_token_features(q1, q2):\n    token_features = [0.0]*10\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    \n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    # Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    # Average Token Length of both Questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n    return token_features\n\n# Get the Longest Common sub string\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    # Pre-processing each question\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n\n    print(\"token features...\")\n    \n    # Merging Features with dataset\n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n   \n    # Computing Fuzzy Features and Merging with Dataset\n    # Do read this blog: http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n    # https://stackoverflow.com/questions/31806695/when-to-use-which-fuzz-function-to-compare-2-strings\n    # https://github.com/seatgeek/fuzzywuzzy\n    print(\"fuzzy features..\")\n\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:57.103294Z","iopub.execute_input":"2021-10-03T09:30:57.103535Z","iopub.status.idle":"2021-10-03T09:30:57.130846Z","shell.execute_reply.started":"2021-10-03T09:30:57.103508Z","shell.execute_reply":"2021-10-03T09:30:57.129959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.isfile('../input/d/elemento/quora-question-pairs/nlp_features_train.csv'):\n    df = pd.read_csv(\"../input/d/elemento/quora-question-pairs/nlp_features_train.csv\", encoding='latin-1')\n    df.fillna('')\nelse:\n    print(\"Extracting features for train:\")\n    df = pd.read_csv(\"train.csv\")\n    df = extract_features(df)\n    df.to_csv(\"nlp_features_train.csv\", index=False)\n\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:30:57.132087Z","iopub.execute_input":"2021-10-03T09:30:57.132564Z","iopub.status.idle":"2021-10-03T09:31:01.042554Z","shell.execute_reply.started":"2021-10-03T09:30:57.132535Z","shell.execute_reply":"2021-10-03T09:31:01.041736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.1 Analysis of extracted features\n#### 3.5.1.1 Plotting Word clouds\n- Creating Word Cloud of Duplicate and Non-Duplicate Question pairs\n- We can observe the most frequently occuring words in Duplicate Question pairs and Non-Duplicate Question pairs.\n- Based on the below word-clouds, we can conclude that there are certain words that occur more frequently in duplicate pair of questions than non-duplicate pair of questions, and vice-versa.\n- Also, there are certain words that occur with almost the same frequency, in both duplicate as well as non-duplicate pair of questions.\n- These observations suggest that counting the frequency of words could give us an important feature, and we will look at some BoW and TF-IDF based vectorization in the further sections.","metadata":{}},{"cell_type":"code","source":"df_duplicate = df[df['is_duplicate'] == 1]\ndfp_nonduplicate = df[df['is_duplicate'] == 0]\n\n# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\np = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nn = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\n\nprint (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n\n# Saving the np array into a text file\nnp.savetxt('train_p.txt', p, delimiter=' ', fmt='%s')\nnp.savetxt('train_n.txt', n, delimiter=' ', fmt='%s')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:31:01.043551Z","iopub.execute_input":"2021-10-03T09:31:01.043752Z","iopub.status.idle":"2021-10-03T09:31:02.905155Z","shell.execute_reply.started":"2021-10-03T09:31:01.043729Z","shell.execute_reply":"2021-10-03T09:31:02.904335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the text files and removing the Stop Words:\nd = path.dirname('.')\n\ntextp_w = open(path.join(d, 'train_p.txt')).read()\ntextn_w = open(path.join(d, 'train_n.txt')).read()\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\n\nstopwords.remove(\"no\")\n# stopwords.remove(\"good\")\n# stopwords.remove(\"love\")\nstopwords.remove(\"like\")\n# stopwords.remove(\"best\")\n# stopwords.remove(\"!\")\nprint (\"Total number of words in duplicate pair questions :\",len(textp_w))\nprint (\"Total number of words in non duplicate pair questions :\",len(textn_w))","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:31:02.907723Z","iopub.execute_input":"2021-10-03T09:31:02.907985Z","iopub.status.idle":"2021-10-03T09:31:02.979325Z","shell.execute_reply.started":"2021-10-03T09:31:02.907955Z","shell.execute_reply":"2021-10-03T09:31:02.978388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Clouds generated from  duplicate pair question's text\nplt.figure(figsize = (12, 10))\nwc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords=stopwords)\nwc.generate(textp_w)\nprint (\"Word Cloud for Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:31:02.980692Z","iopub.execute_input":"2021-10-03T09:31:02.981242Z","iopub.status.idle":"2021-10-03T09:31:12.558Z","shell.execute_reply.started":"2021-10-03T09:31:02.981199Z","shell.execute_reply":"2021-10-03T09:31:12.557128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Clouds generated from non duplicate pair question's text\nplt.figure(figsize = (12, 10))\nwc = WordCloud(background_color=\"white\", max_words=len(textn_w),stopwords=stopwords)\nwc.generate(textn_w)\nprint (\"Word Cloud for non-Duplicate Question pairs:\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:31:12.559797Z","iopub.execute_input":"2021-10-03T09:31:12.560113Z","iopub.status.idle":"2021-10-03T09:31:35.429012Z","shell.execute_reply.started":"2021-10-03T09:31:12.560072Z","shell.execute_reply":"2021-10-03T09:31:35.428029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.5.1.2 Pair plot of features ['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'] ","metadata":{}},{"cell_type":"code","source":"n = df.shape[0]\nsns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:31:35.430731Z","iopub.execute_input":"2021-10-03T09:31:35.431058Z","iopub.status.idle":"2021-10-03T09:36:19.551429Z","shell.execute_reply.started":"2021-10-03T09:31:35.431017Z","shell.execute_reply":"2021-10-03T09:36:19.550504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of the token_sort_ratio\nplt.figure(figsize=(16, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:36:19.553399Z","iopub.execute_input":"2021-10-03T09:36:19.554524Z","iopub.status.idle":"2021-10-03T09:36:23.533392Z","shell.execute_reply.started":"2021-10-03T09:36:19.554424Z","shell.execute_reply":"2021-10-03T09:36:23.532369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of the fuzz_ratio\nplt.figure(figsize=(16, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:36:23.534959Z","iopub.execute_input":"2021-10-03T09:36:23.535337Z","iopub.status.idle":"2021-10-03T09:36:27.698715Z","shell.execute_reply.started":"2021-10-03T09:36:23.535295Z","shell.execute_reply":"2021-10-03T09:36:27.698093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.2 Visualization","metadata":{}},{"cell_type":"code","source":"# Using TSNE for Dimentionality reduction for 15 Features \n# (Generated after cleaning the data) to 3 dimensions\nfrom sklearn.preprocessing import MinMaxScaler\n\ndfp_subsampled = df[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[[\n    'cwc_min', 'cwc_max', 'csc_min', 'csc_max', 'ctc_min', 'ctc_max', 'last_word_eq', \n    'first_word_eq', 'abs_len_diff', 'mean_len', 'token_set_ratio', 'token_sort_ratio',  \n    'fuzz_ratio', 'fuzz_partial_ratio', 'longest_substr_ratio']\n])\ny = dfp_subsampled['is_duplicate'].values","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:36:27.699722Z","iopub.execute_input":"2021-10-03T09:36:27.700664Z","iopub.status.idle":"2021-10-03T09:36:27.713918Z","shell.execute_reply.started":"2021-10-03T09:36:27.700614Z","shell.execute_reply":"2021-10-03T09:36:27.712904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-03T09:36:27.715382Z","iopub.execute_input":"2021-10-03T09:36:27.715761Z","iopub.status.idle":"2021-10-03T09:36:52.507809Z","shell.execute_reply.started":"2021-10-03T09:36:27.715711Z","shell.execute_reply":"2021-10-03T09:36:52.507094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# Draw the plot in appropriate place in the grid\nsns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,\n   palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:36:52.509229Z","iopub.execute_input":"2021-10-03T09:36:52.511623Z","iopub.status.idle":"2021-10-03T09:36:53.068539Z","shell.execute_reply.started":"2021-10-03T09:36:52.511579Z","shell.execute_reply":"2021-10-03T09:36:53.067854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne3d = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-03T09:36:53.071953Z","iopub.execute_input":"2021-10-03T09:36:53.074335Z","iopub.status.idle":"2021-10-03T09:38:12.794385Z","shell.execute_reply.started":"2021-10-03T09:36:53.074284Z","shell.execute_reply":"2021-10-03T09:38:12.793642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:38:12.79803Z","iopub.execute_input":"2021-10-03T09:38:12.799749Z","iopub.status.idle":"2021-10-03T09:38:13.852238Z","shell.execute_reply.started":"2021-10-03T09:38:12.799709Z","shell.execute_reply":"2021-10-03T09:38:13.851454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.6 Featurizing text data with tfidf weighted word-vectors\n- From our previous analysis, we realized that some words occur more often in `class 1` pairs, while some words occur more often in `class 0` pairs.\n- So, the question is can we use this realization somehow to construct text-based features? We know both TF-IDF and Word2Vec techniques. However, instead of using W2V, we will be usibg **GLoVE** for this problem.\n- GLoVE is very similar to W2V, in that it helps us to convert words into vectors while maintaining the **semantic relationships** among words. The only difference is in the way they work.","metadata":{}},{"cell_type":"code","source":"# Avoid decoding problems\ndf = pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")\n\n# Encode questions to unicode\n# https://stackoverflow.com/a/6812069\n# ----------------- python 2 ---------------------\n# df['question1'] = df['question1'].apply(lambda x: unicode(str(x),\"utf-8\"))\n# df['question2'] = df['question2'].apply(lambda x: unicode(str(x),\"utf-8\"))\n# ----------------- python 3 ---------------------\ndf['question1'] = df['question1'].apply(lambda x: str(x))\ndf['question2'] = df['question2'].apply(lambda x: str(x))","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:42:47.760281Z","iopub.execute_input":"2021-10-03T09:42:47.76064Z","iopub.status.idle":"2021-10-03T09:42:49.747841Z","shell.execute_reply.started":"2021-10-03T09:42:47.760611Z","shell.execute_reply":"2021-10-03T09:42:49.747202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:42:55.330853Z","iopub.execute_input":"2021-10-03T09:42:55.331682Z","iopub.status.idle":"2021-10-03T09:42:55.343749Z","shell.execute_reply.started":"2021-10-03T09:42:55.33163Z","shell.execute_reply":"2021-10-03T09:42:55.342807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge texts\nquestions = list(df['question1']) + list(df['question2'])\n\ntfidf = TfidfVectorizer(lowercase=False, )\ntfidf.fit_transform(questions)\n\n# dict key:word and value:tf-idf score\nword2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:43:12.604917Z","iopub.execute_input":"2021-10-03T09:43:12.60527Z","iopub.status.idle":"2021-10-03T09:43:27.384944Z","shell.execute_reply.started":"2021-10-03T09:43:12.605211Z","shell.execute_reply":"2021-10-03T09:43:27.383854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- After we find TF-IDF scores, we convert each question to a weighted average of word2vec vectors by these scores.\n- Here we use a pre-trained GLOVE model which comes free with \"Spacy\".  https://spacy.io/usage/vectors-similarity\n- It is trained on Wikipedia and therefore, it is stronger in terms of word semantics. ","metadata":{}},{"cell_type":"code","source":"# en_vectors_web_lg, which includes over 1 million unique vectors.\nnlp = spacy.load('en_core_web_sm')\nvecs1 = []\n\n# https://github.com/noamraph/tqdm\n# tqdm is used to print the progress bar\nfor qu1 in tqdm(list(df['question1'])):\n    doc1 = nlp(qu1) \n    # 384 is the number of dimensions of vectors \n    mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n    for word1 in doc1:\n        # Word2Vec\n        vec1 = word1.vector\n        # Fetch df score\n        try: idf = word2tfidf[str(word1)]\n        except: idf = 0\n        # Compute final vec\n        mean_vec1 += vec1 * idf\n    mean_vec1 = mean_vec1.mean(axis=0)\n    vecs1.append(mean_vec1)\ndf['q1_feats_m'] = list(vecs1)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T09:51:49.942039Z","iopub.execute_input":"2021-10-03T09:51:49.942709Z","iopub.status.idle":"2021-10-03T10:51:47.739958Z","shell.execute_reply.started":"2021-10-03T09:51:49.942656Z","shell.execute_reply":"2021-10-03T10:51:47.738212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vecs2 = []\nfor qu2 in tqdm(list(df['question2'])):\n    doc2 = nlp(qu2) \n    mean_vec2 = np.zeros([len(doc1), len(doc2[0].vector)])\n    for word2 in doc2:\n        # Word2Vec\n        vec2 = word2.vector\n        # Fetch df score\n        try: idf = word2tfidf[str(word2)]\n        except: idf = 0\n        # Compute final vec\n        mean_vec2 += vec2 * idf\n    mean_vec2 = mean_vec2.mean(axis=0)\n    vecs2.append(mean_vec2)\ndf['q2_feats_m'] = list(vecs2)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:05:26.634179Z","iopub.execute_input":"2021-10-03T11:05:26.634468Z","iopub.status.idle":"2021-10-03T12:05:29.654835Z","shell.execute_reply.started":"2021-10-03T11:05:26.63444Z","shell.execute_reply":"2021-10-03T12:05:29.652976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepro_features_train.csv (Simple Preprocessing Feartures)\n# nlp_features_train.csv (NLP Features)\nif os.path.isfile('../input/d/elemento/quora-question-pairs/nlp_features_train.csv'):\n    dfnlp = pd.read_csv(\"../input/d/elemento/quora-question-pairs/nlp_features_train.csv\",encoding='latin-1')\nelse:\n    print(\"Download nlp_features_train.csv from drive or run previous notebook\")\n\nif os.path.isfile('../input/d/elemento/quora-question-pairs/df_fe_without_preprocessing_train.csv'):\n    dfppro = pd.read_csv(\"../input/d/elemento/quora-question-pairs/df_fe_without_preprocessing_train.csv\",encoding='latin-1')\nelse:\n    print(\"Download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")","metadata":{"execution":{"iopub.status.busy":"2021-10-03T12:05:29.659381Z","iopub.execute_input":"2021-10-03T12:05:29.659701Z","iopub.status.idle":"2021-10-03T12:05:35.061198Z","shell.execute_reply.started":"2021-10-03T12:05:29.65964Z","shell.execute_reply":"2021-10-03T12:05:35.060261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- So, the final feature vector(s) or x_i(s) consists of 4 parts. The 1st part consists of the features provided to us in the original dataset, and the ones we engineered in the **Basic Feature Extraction Section (3.3)**.\n- The 2nd part consists of the 15 features that we engineered in the **Advanced Feature Extraction Section (3.5)**.\n- The 3rd part consists of the **TF-IDF Weighted GLoVE representation of the Question 1(s)**, and the 4th  part consists of the **TF-IDF Weighted GLoVE representation of the Question 2(s)**.","metadata":{}},{"cell_type":"code","source":"df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\ndf2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\ndf3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\ndf3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\ndf3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T12:05:35.062533Z","iopub.execute_input":"2021-10-03T12:05:35.062743Z","iopub.status.idle":"2021-10-03T12:06:28.910388Z","shell.execute_reply.started":"2021-10-03T12:05:35.062719Z","shell.execute_reply":"2021-10-03T12:06:28.909405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataframe of nlp features\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T12:06:28.912634Z","iopub.execute_input":"2021-10-03T12:06:28.912869Z","iopub.status.idle":"2021-10-03T12:06:28.942277Z","shell.execute_reply.started":"2021-10-03T12:06:28.912843Z","shell.execute_reply":"2021-10-03T12:06:28.941385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data before pre-processing \ndf2.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T12:06:28.943607Z","iopub.execute_input":"2021-10-03T12:06:28.943842Z","iopub.status.idle":"2021-10-03T12:06:28.958937Z","shell.execute_reply.started":"2021-10-03T12:06:28.943817Z","shell.execute_reply":"2021-10-03T12:06:28.958006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Questions 1 tfidf-weighted word2vec\ndf3_q1.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T12:06:28.960515Z","iopub.execute_input":"2021-10-03T12:06:28.960761Z","iopub.status.idle":"2021-10-03T12:06:28.997763Z","shell.execute_reply.started":"2021-10-03T12:06:28.960735Z","shell.execute_reply":"2021-10-03T12:06:28.996965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Questions 2 tfidf-weighted word2vec\ndf3_q2.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T12:06:28.999315Z","iopub.execute_input":"2021-10-03T12:06:28.99963Z","iopub.status.idle":"2021-10-03T12:06:29.02866Z","shell.execute_reply.started":"2021-10-03T12:06:28.999591Z","shell.execute_reply":"2021-10-03T12:06:29.027742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of features in nlp dataframe :\", df1.shape[1])\nprint(\"Number of features in preprocessed dataframe :\", df2.shape[1])\nprint(\"Number of features in question1 w2v  dataframe :\", df3_q1.shape[1])\nprint(\"Number of features in question2 w2v  dataframe :\", df3_q2.shape[1])\nprint(\"Number of features in final dataframe  :\", df1.shape[1]+df2.shape[1]+df3_q1.shape[1]+df3_q2.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-10-03T12:06:29.030066Z","iopub.execute_input":"2021-10-03T12:06:29.030303Z","iopub.status.idle":"2021-10-03T12:06:29.039399Z","shell.execute_reply.started":"2021-10-03T12:06:29.030277Z","shell.execute_reply":"2021-10-03T12:06:29.03824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Storing the final features to CSV file\nif not os.path.isfile('../input/d/elemento/quora-question-pairs/final_features_sm.csv'):\n    df3_q1['id'] = df1['id']\n    df3_q2['id'] = df1['id']\n    df1  = df1.merge(df2, on='id',how='left')\n    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n    result  = df1.merge(df2, on='id',how='left')\n    result.to_csv('final_features_sm.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T12:08:27.057776Z","iopub.execute_input":"2021-10-03T12:08:27.058866Z","iopub.status.idle":"2021-10-03T12:11:25.291857Z","shell.execute_reply.started":"2021-10-03T12:08:27.058822Z","shell.execute_reply":"2021-10-03T12:11:25.290788Z"},"trusted":true},"execution_count":null,"outputs":[]}]}