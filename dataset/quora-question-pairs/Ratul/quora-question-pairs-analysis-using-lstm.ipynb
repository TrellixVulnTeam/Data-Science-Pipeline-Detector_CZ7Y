{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The goal of this competition is to predict which of the provided pairs of questions contain two questions with the same meaning. The ground truth is the set of labels that have been supplied by human experts. The ground truth labels are inherently subjective, as the true meaning of sentences can never be known with certainty. Human labeling is also a 'noisy' process, and reasonable people will disagree. As a result, the ground truth labels on this dataset should be taken to be 'informed' but not 100% accurate, and may include incorrect labeling. We believe the labels, on the whole, to represent a reasonable consensus, but this may often not be true on a case by case basis for individual items in the dataset.**\n\nPlease note: as an anti-cheating measure, Kaggle has supplemented the test set with computer-generated question pairs. Those rows do not come from Quora, and are not counted in the scoring. All of the questions in the training set are genuine examples from Quora.\n\nData fields\nid - the id of a training set question pair\nqid1, qid2 - unique ids of each question (only available in train.csv)\nquestion1, question2 - the full text of each question\nis_duplicate - the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise."},{"metadata":{},"cell_type":"markdown","source":"**DATASET LINK - https://www.kaggle.com/c/quora-question-pairs/data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have 404290 observations and 6 features in the dataset.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv('/kaggle/input/quora-question-pairs/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['is_duplicate']==1].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping null values\ntrain=train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_list1=list(train['question1'])\ntrain_list2=list(train['question2'])\ntrain_list=train_list1+train_list2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size=20000\ntokenizer=Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(train_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence1=tokenizer.texts_to_sequences(train_list1)\nsequence2=tokenizer.texts_to_sequences(train_list2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#padding the sequences to a constant size\nmax_length=100\nsequence1=pad_sequences(sequence1,maxlen=max_length,padding='post')\nsequence2=pad_sequences(sequence2,maxlen=max_length,padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['seq1']=list(sequence1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['seq2']=list(sequence2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=np.asarray(train['is_duplicate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#functional API\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense,Embedding,LSTM,concatenate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Embedding the LSTM Layer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Input\n\ntext_input1=Input(shape=(None,),dtype='int32')\nembedding1=Embedding(vocab_size,64)(text_input1)\nencoded_text1=LSTM(32)(embedding1)\n\ntext_input2=Input(shape=(None,),dtype='int32')\nembedding2=Embedding(vocab_size,64)(text_input2)\nencoded_text2=LSTM(32)(embedding2)\n\nconcatenated=concatenate([encoded_text1,encoded_text2],axis=-1)\n\noutput=Dense(64,activation='relu')(concatenated)\noutput=Dense(1,activation='sigmoid')(output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compiling the model with Adam optimizer and loss as categorical crossentropy and evaluating the results using accuracy.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Model([text_input1,text_input2],output)\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fitting the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit([sequence1,sequence2],labels,epochs = 10,batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**At the end of 10th epoch we are getting an accuracy of 74% and 0.52 loss.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}