{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nfrom transformers import BertModel, BertTokenizer\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 4144959\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(torch.cuda.get_device_name())\n# device = torch.device('cpu')\ndf_test = pd.read_csv('../input/quora-question-pairs/test.csv')\nMODEL_NAME = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\nMAX_LEN = 335","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test set shape', df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimilarityMeasurer(nn.Module):\n    def __init__(self):\n        super(SimilarityMeasurer, self).__init__()\n        self.bert = BertModel.from_pretrained(MODEL_NAME)\n        self.out = nn.Sequential(\n            nn.Linear(self.bert.config.hidden_size, 128),\n            nn.Linear(128, 2),\n        )\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        bert_outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        # return bert_outputs\n        cls_hidden_state = bert_outputs[0].transpose(0, 1)[0]\n        ret = self.out(cls_hidden_state)\n        return ret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model = SimilarityMeasurer()\nModel.load_state_dict(torch.load('../input/quora-question-pairs-model-state/model_state.pkl'))\nModel = Model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('./submission.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow([\"test_id\", \"is_duplicate\"])\n    for i in range(len(df_test)):\n        ''''''\n        text1 = df_test.iloc[i]['question1']\n        text2 = df_test.iloc[i]['question2']\n        encoded = tokenizer.encode_plus(\n            text=text1,\n            text_pair=text2,\n            padding='max_length',\n            truncation=True,\n            max_length=MAX_LEN,\n            return_tensors='pt'\n        )\n        with torch.no_grad():\n            outputs = Model(\n                input_ids=encoded['input_ids'].to(device),\n                attention_mask=encoded['attention_mask'].to(device),\n                token_type_ids=encoded['token_type_ids'].to(device)\n            )\n        outputs = F.softmax(outputs, dim=1)\n        ans = outputs.max(dim=1)[0].item()\n        writer.writerow([i, ans])\n        if i % 1000 == 0:\n            print(f\"Progress: {i}/{len(df_test)}\")\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}