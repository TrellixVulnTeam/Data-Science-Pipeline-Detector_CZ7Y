{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70db50f1-4b05-66f8-4a10-ef10f7296761"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom subprocess import check_output\nfrom collections import Counter\nfrom sklearn.cross_validation import train_test_split\nimport xgboost as xgb\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02a0dc36-ade6-4c52-159d-9b56d0e7f201"},"outputs":[],"source":"train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"287559d8-dde9-0b9c-406b-0e819d21a16a"},"outputs":[],"source":"train_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a5ed49b-f59e-13f7-1459-2eca47100ba4"},"outputs":[],"source":"test_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d588c466-86e2-4e1d-fccd-2e648e9d9f2e"},"outputs":[],"source":"is_dup = train_data.is_duplicate.value_counts()\nis_dup"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eee2c31d-b1eb-7334-6eb7-a411abd61c1b"},"outputs":[],"source":"stops = set(stopwords.words(\"english\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d89472e-2dd7-fd7d-013d-ed58d5f26330"},"outputs":[],"source":"def word_match_share(row):\n    #вычмсл\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0a9bf50-f9bf-860e-045d-6309571ba543"},"outputs":[],"source":"train_qs = pd.Series(train_data['question1'].tolist() + train_data['question2'].tolist()).astype(str)\ntest_qs = pd.Series(test_data['question1'].tolist() + test_data['question2'].tolist()).astype(str)\nfrom collections import Counter\ndef get_weight(count, eps=10000, min_count=10):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}#сопоставляем слову котрое встретилось больше min_count опред вес"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec5ec779-c7dd-e37e-d85a-8037ef22a312"},"outputs":[],"source":"def tfidf_word_match_share(row):\n    #вычисляем отношения веса слов не попавшим в стопслова и находяшихся одновреммено в обоих вопросах к обшему весу\n\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n      \n        return 0\n    \n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    \n    R = np.sum(shared_weights) / np.sum(total_weights)\n    return R"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6a228df-4a53-6a99-944b-95873a5a15d3"},"outputs":[],"source":"train_word_match = train_data.apply(word_match_share, axis=1, raw=True)\ntfidf_train_word_match = train_data.apply(tfidf_word_match_share, axis=1, raw=True)\nx_train = pd.DataFrame()\nx_test = pd.DataFrame()\nx_train['word_match'] = train_data.apply(word_match_share, axis=1, raw=True)\nx_train['tfidf_word_match'] = train_data.apply(tfidf_word_match_share, axis=1, raw=True)\nx_test['word_match'] = test_data.apply(word_match_share, axis=1, raw=True)\nx_test['tfidf_word_match'] = test_data.apply(tfidf_word_match_share, axis=1, raw=True)\n\ny_train = train_data['is_duplicate'].values #применяем функции"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9b02ec5-74b3-cc47-6c62-fe686ba0f9d0"},"outputs":[],"source":"from sklearn.cross_validation import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6bf03280-696a-f065-16ba-b3cce1395c85"},"outputs":[],"source":"import xgboost as xgb\n\n\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10) #вычисляем бинарную регрессию"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"856ecdc3-fe36-6a8f-80ca-56a9015025e4"},"outputs":[],"source":"d_test = xgb.DMatrix(x_test)\np_test = bst.predict(d_test)\n\nsub = pd.DataFrame()\nsub['test_id'] = test_data['test_id']\nsub['is_duplicate'] = p_test\nsub.to_csv('simple_xgb.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}