{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding,Activation,Flatten,merge,TimeDistributed,CuDNNGRU,Bidirectional\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers import concatenate,subtract,add,maximum,multiply,Layer,Lambda\nfrom keras.backend import backend as K\n\nfrom tqdm import tqdm\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"## read data from path\nMAX_SEQUENCE_LENGTH = 40\nMAX_NB_WORDS = 4000000\nnrows = 10000000\nEMBEDDING_DIM = 300\n\n# nrows = 10000\n\npath = '../input/quora-question-pairs/'\ntrain = pd.read_csv(path+\"train.csv\",nrows=nrows).astype(str)\ntest = pd.read_csv(path+\"test.csv\",nrows=nrows).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"## generate keras inputs\n## tokenizer and padding\ncorpus = []\n\nfeats = ['question1','question2']\nfor f in feats:\n    train[f] = train[f].astype(str)\n    test[f] = test[f].astype(str)\n    corpus+=train[f].values.tolist()\n    \n    \ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(corpus)\nX_q1 = tokenizer.texts_to_sequences(train['question1'])\nX_q2 = tokenizer.texts_to_sequences(train['question2'])\n\nX_test_q1 = tokenizer.texts_to_sequences(test['question1'])\nX_test_q2 = tokenizer.texts_to_sequences(test['question2'])\n\n\nX_q1 = pad_sequences(X_q1, maxlen=MAX_SEQUENCE_LENGTH)\nX_q2 = pad_sequences(X_q2, maxlen=MAX_SEQUENCE_LENGTH)\nX_test_q1 = pad_sequences(X_test_q1, maxlen=MAX_SEQUENCE_LENGTH)\nX_test_q2 = pad_sequences(X_test_q2, maxlen=MAX_SEQUENCE_LENGTH)\n\ny = train['is_duplicate'].values\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens' % len(word_index))\nnb_words = min(MAX_NB_WORDS, len(word_index))+1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## question \n    how to use pretrained embedding like glove,word2vec\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\nglove_path = '../input/glove840b300dtxt/glove.840B.300d.txt'\nembedding_matrix,unknown_words = build_matrix(word_index,glove_path)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"## eval metric is logloss\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"## validation set\nfrom sklearn.model_selection import train_test_split\nX_train_q1,X_val_q1,X_train_q2,X_val_q2,y_train,y_val = train_test_split(X_q1,X_q2,y,train_size=0.8,random_state=1024)\nprint(X_train_q1.shape,X_val_q1.shape)\nX_train = [X_train_q1,X_train_q2]\nX_val = [X_val_q1,X_val_q2]\nX_test = [X_test_q1,X_test_q2]\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"def get_model_siamese(nb_words=50000,MAX_SEQUENCE_LENGTH=200,EMBEDDING_DIM=200,act='relu',embedding_matrix=None):\n    ########################################\n    ## define the model structure\n    ########################################\n    \n    input_q1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    input_q2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\n    if embedding_matrix is not None:\n        weights=[embedding_matrix],\n\n        embedding_layer = Embedding(nb_words,\n                EMBEDDING_DIM,\n                input_length=MAX_SEQUENCE_LENGTH,\n                weights=[embedding_matrix],\n                trainable=False)\n    else:\n        embedding_layer = Embedding(nb_words,\n                EMBEDDING_DIM,\n                input_length=MAX_SEQUENCE_LENGTH,\n                trainable=True)\n\n    embedded_sequences_q1 = embedding_layer(input_q1)\n    embedded_sequences_q2 = embedding_layer(input_q2)\n\n    \n    bilstm_layer = Bidirectional(CuDNNGRU(64,return_sequences=False))\n\n    x1 = bilstm_layer(embedded_sequences_q1)\n    x2 = bilstm_layer(embedded_sequences_q2)\n\n    diff = subtract([x1,x2])\n    summation = add([x1,x2])\n    \n    x = concatenate([x1,x2,diff,summation],1)\n\n    x = Dense(64,activation=act)(x)\n    \n    preds = Dense(1,activation='sigmoid')(x)\n\n\n    model = Model(inputs=[input_q1,input_q2],outputs=preds)\n    model.compile(loss='binary_crossentropy',\n            optimizer='adam',\n            metrics=['accuracy'])\n    print(model.summary())\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"## train model\n\nearly_stopping =EarlyStopping(monitor='val_loss', patience=5)\nbst_model_path = 'best_model.h5'\nmodel_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n\nmodel = get_model_siamese(nb_words=nb_words,MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH,EMBEDDING_DIM=EMBEDDING_DIM,act='relu',embedding_matrix=embedding_matrix)\nmodel.fit(X_train,y_train,epochs=250,validation_data=(X_val, y_val),batch_size=128,shuffle=True,verbose=1,callbacks=[early_stopping, model_checkpoint])\n\nmodel.load_weights(bst_model_path)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## make submission \ny_pred_test = model.predict(X_test)\n\nsubmission = pd.DataFrame()\nsubmission['test_id'] = test['test_id']\nsubmission['is_duplicate'] = y_pred_test\nsubmission.to_csv('submission_siamese.csv',index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}