{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nimport distance\nfrom wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:42.866519Z","iopub.execute_input":"2022-04-01T12:43:42.867701Z","iopub.status.idle":"2022-04-01T12:43:42.876419Z","shell.execute_reply.started":"2022-04-01T12:43:42.86764Z","shell.execute_reply":"2022-04-01T12:43:42.875439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/quora-train/train.csv')\ndata.head(4)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:42.946875Z","iopub.execute_input":"2022-04-01T12:43:42.947289Z","iopub.status.idle":"2022-04-01T12:43:44.593371Z","shell.execute_reply.started":"2022-04-01T12:43:42.947259Z","shell.execute_reply":"2022-04-01T12:43:44.592511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:44.595297Z","iopub.execute_input":"2022-04-01T12:43:44.595964Z","iopub.status.idle":"2022-04-01T12:43:44.601142Z","shell.execute_reply.started":"2022-04-01T12:43:44.595915Z","shell.execute_reply":"2022-04-01T12:43:44.600259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting 50,000 random rows from a total of 4 Lakh rows:\n\ndata_ = data.sample(n = 50000)\ndata_.head(4)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:44.602669Z","iopub.execute_input":"2022-04-01T12:43:44.602936Z","iopub.status.idle":"2022-04-01T12:43:44.655924Z","shell.execute_reply.started":"2022-04-01T12:43:44.602905Z","shell.execute_reply":"2022-04-01T12:43:44.654999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Storing two questions in seperate variable:\n\nq1 = data_.iloc[:,3].values\nq2 = data_.iloc[:,4].values\ny = data_.iloc[:,-1].values # class label","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:44.657746Z","iopub.execute_input":"2022-04-01T12:43:44.658554Z","iopub.status.idle":"2022-04-01T12:43:44.664448Z","shell.execute_reply.started":"2022-04-01T12:43:44.658517Z","shell.execute_reply":"2022-04-01T12:43:44.66337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = data_['is_duplicate'].value_counts()\nlabels","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:44.665901Z","iopub.execute_input":"2022-04-01T12:43:44.666306Z","iopub.status.idle":"2022-04-01T12:43:44.685146Z","shell.execute_reply.started":"2022-04-01T12:43:44.66626Z","shell.execute_reply":"2022-04-01T12:43:44.684339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of class label: 0 = Not Duplicate, 1 = Duplicate\n","metadata":{}},{"cell_type":"code","source":"x_axis = [str(0),str(1)]\ny_axis = [labels[0],labels[1]]\n\nplt.bar(x_axis,y_axis,width = 0.4)\nplt.xlabel('Duplicate or not')\nplt.ylabel('Number of questions')\nplt.title('Frequency of Duplicate questions')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:44.686594Z","iopub.execute_input":"2022-04-01T12:43:44.686996Z","iopub.status.idle":"2022-04-01T12:43:44.90197Z","shell.execute_reply.started":"2022-04-01T12:43:44.686954Z","shell.execute_reply":"2022-04-01T12:43:44.901071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Percentage of similar questions:\n\nsimilar = ((labels[1])/(labels[0] + labels[1]))*100\nnot_similar = ((labels[0])/(labels[0] + labels[1]))*100\nprint('Percentage of Similar question pairs in dataset is: {}%'.format(similar))\nprint('Percentage of Not Similar questions in dataset is: {}%'.format(not_similar))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:44.903317Z","iopub.execute_input":"2022-04-01T12:43:44.903601Z","iopub.status.idle":"2022-04-01T12:43:44.910295Z","shell.execute_reply.started":"2022-04-01T12:43:44.903559Z","shell.execute_reply":"2022-04-01T12:43:44.909239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking null values:","metadata":{}},{"cell_type":"code","source":"null_value = data_[data_.isnull().any(1)]\nprint(null_value)\nprint('*'*75)\ndata_ = data_.fillna('')\nnan_rows = data_[data_.isnull().any(1)]\nprint (nan_rows)\n\n# .any(1) this returns the instance where the dataframe has null values","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:44.911528Z","iopub.execute_input":"2022-04-01T12:43:44.911764Z","iopub.status.idle":"2022-04-01T12:43:45.008092Z","shell.execute_reply.started":"2022-04-01T12:43:44.911736Z","shell.execute_reply":"2022-04-01T12:43:45.00668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Genreal idea about data:","metadata":{}},{"cell_type":"code","source":"ques = pd.Series(data_['qid1'].tolist() + data_['qid2'].tolist())\nunique_ques = len(np.unique(ques))\nrepeat = np.sum(ques.value_counts() > 1)\nmax_ques = max(ques.value_counts())\nvalues = ques.value_counts()\n\nidx = ques.value_counts().index.tolist()[0] # To get the qid value for question that repeats maximum number of times\n\nmaxi = data_.loc[(data_['qid1'] == idx)]\nmaxi2 = maxi[maxi.columns[[3]]]\n\nprint('Question ID and their number of occurences')\nprint(values[:5])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:45.009837Z","iopub.execute_input":"2022-04-01T12:43:45.010156Z","iopub.status.idle":"2022-04-01T12:43:45.12976Z","shell.execute_reply.started":"2022-04-01T12:43:45.010113Z","shell.execute_reply":"2022-04-01T12:43:45.128736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total number of unique questions are: ',unique_ques)\nprint('*'*75)\nprint('{}% of the unique questions repeat themselves'.format(round((repeat/unique_ques)*100,2)))\nprint('*'*75)\nprint('The following question is repeated {} times'.format(max_ques))\n(maxi2)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:45.132387Z","iopub.execute_input":"2022-04-01T12:43:45.132629Z","iopub.status.idle":"2022-04-01T12:43:45.146664Z","shell.execute_reply.started":"2022-04-01T12:43:45.1326Z","shell.execute_reply":"2022-04-01T12:43:45.145782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning:","metadata":{}},{"cell_type":"code","source":"# not removing stop-words yet:\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nlemma = WordNetLemmatizer()\n\nimport re\ndef clean(text):\n    '''\n    This function gets rid of all punctuation marks, special characters\n    and expands the contracted words and returns words in its lemma form\n    \n    '''\n    sent = text.lower().strip() # Done for whole sentence\n    sent = re.sub('[^a-zA-Z]',' ',text) # Done for whole sentence\n    sent = sent.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"doesn't\", \"does not\")\n    sent = sent.split()\n    final = [lemma.lemmatize(word) for word in sent]\n    final_sent =  ' '.join(final)\n    return final_sent\n\n# The strip() method removes any leading (spaces at the beginning) and trailing (spaces at the end) characters","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:45.148592Z","iopub.execute_input":"2022-04-01T12:43:45.148936Z","iopub.status.idle":"2022-04-01T12:43:46.014752Z","shell.execute_reply.started":"2022-04-01T12:43:45.148891Z","shell.execute_reply":"2022-04-01T12:43:46.013873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q1_clean = [] # This stores all the question1 text which has been cleaned \nfor z in range(len(q1)):\n    res = clean(str(q1[z]))\n    q1_clean.append(res)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:46.018571Z","iopub.execute_input":"2022-04-01T12:43:46.018855Z","iopub.status.idle":"2022-04-01T12:43:51.685522Z","shell.execute_reply.started":"2022-04-01T12:43:46.018803Z","shell.execute_reply":"2022-04-01T12:43:51.684568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q2_clean = [] # This stores all the question2 text which has been cleaned \nfor v in range(len(q2)):\n    res2 = clean(str(q2[v]))\n    q2_clean.append(res2)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:51.686735Z","iopub.execute_input":"2022-04-01T12:43:51.686996Z","iopub.status.idle":"2022-04-01T12:43:55.255254Z","shell.execute_reply.started":"2022-04-01T12:43:51.686966Z","shell.execute_reply":"2022-04-01T12:43:55.254361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wordcloud:","metadata":{}},{"cell_type":"code","source":"duplicate = data_[data_['is_duplicate'] == 1] # Storing data having class label as 1 \nnot_duplicate = data_[data_['is_duplicate'] == 0] # Storing data having class label as 0 \n\nsimilar = np.dstack([duplicate[\"question1\"],duplicate[\"question2\"]]).flatten()\nno_similar = np.dstack([not_duplicate[\"question1\"], not_duplicate[\"question2\"]]).flatten()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:55.256435Z","iopub.execute_input":"2022-04-01T12:43:55.256906Z","iopub.status.idle":"2022-04-01T12:43:55.279606Z","shell.execute_reply.started":"2022-04-01T12:43:55.256874Z","shell.execute_reply":"2022-04-01T12:43:55.278673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# An example of using np.dstack and flatten() operation:\ngfg1 = np.array([1, 2, 3])\ngfg2 = np.array([4, 5, 6])\n\nprint(np.dstack((gfg1, gfg2)))\nprint(np.dstack((gfg1, gfg2)).flatten())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:55.28109Z","iopub.execute_input":"2022-04-01T12:43:55.281623Z","iopub.status.idle":"2022-04-01T12:43:55.288645Z","shell.execute_reply.started":"2022-04-01T12:43:55.281578Z","shell.execute_reply":"2022-04-01T12:43:55.287868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print((similar)[:4])","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:55.289693Z","iopub.execute_input":"2022-04-01T12:43:55.28992Z","iopub.status.idle":"2022-04-01T12:43:55.303184Z","shell.execute_reply.started":"2022-04-01T12:43:55.289894Z","shell.execute_reply":"2022-04-01T12:43:55.30244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(no_similar[:4])","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:55.304626Z","iopub.execute_input":"2022-04-01T12:43:55.304973Z","iopub.status.idle":"2022-04-01T12:43:55.311392Z","shell.execute_reply.started":"2022-04-01T12:43:55.304931Z","shell.execute_reply":"2022-04-01T12:43:55.31036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Cloud for Duplicate question:\n\nstop_words = set(stopwords.words(\"english\"))\n\nduplicate_words = ''\nfor j in (similar): # This loop extracts words from sentences given to it as input\n    c = (str(j).split())\n    for v in c:\n        b = v.lower()\n        duplicate_words += \"\".join(b)+\" \"\n            \nwc_q = WordCloud(width = 800, height = 800,background_color ='white',stopwords = stop_words,min_font_size = 10)\nwc_q.generate(duplicate_words)\nprint('Word Cloud for Duplicate question')\nplt.imshow(wc_q, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:55.31513Z","iopub.execute_input":"2022-04-01T12:43:55.315351Z","iopub.status.idle":"2022-04-01T12:43:59.180724Z","shell.execute_reply.started":"2022-04-01T12:43:55.315325Z","shell.execute_reply":"2022-04-01T12:43:59.179833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Cloud for Non-Duplicate question:\n\nnot_duplicate_words = ''\nfor p in (no_similar):\n    d = (str(p).split())\n    for n in d:\n        s = n.lower()\n        not_duplicate_words += \"\".join(s)+\" \"\n            \nwc_q_ = WordCloud(width = 800, height = 800,background_color ='white',stopwords = stop_words,min_font_size = 10)\nwc_q_.generate(not_duplicate_words)\nprint('Word Cloud for Non Duplicate question')\nplt.imshow(wc_q_, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:43:59.182161Z","iopub.execute_input":"2022-04-01T12:43:59.183153Z","iopub.status.idle":"2022-04-01T12:44:05.017186Z","shell.execute_reply.started":"2022-04-01T12:43:59.183115Z","shell.execute_reply":"2022-04-01T12:44:05.016261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Feature Extraction:","metadata":{}},{"cell_type":"code","source":"# Length of sentence\nq1_len = [len(i.split()) for i in q1_clean]\nq2_len = [len(j.split()) for j in q2_clean]\n\n# Common words between two sentences\ndef comm(k):\n    w1 = set(map(lambda word: word.lower().strip(), q1_clean[k].split(\" \"))) \n    w2 = set(map(lambda word: word.lower().strip(), q2_clean[k].split(\" \")))    \n    return len(w1&w2)\ncommon = [comm(r) for r in range(len(q1_clean))] # storing the number of common words\n\n# Word_share: (common_word_count/total number of words)\n\ntotal = [a+b for a,b in zip(q1_len,q2_len)]\nword_share = [round((c/d),3) for c,d in zip(common,total)] \n\n# cwc_min & cwc_max: (Ratio of common_word_count to min & max length of word count of Q1 and Q2)\n\nmin_len = [min(u,t) for u,t in zip(q1_len,q2_len)] # this returns length of either Q1 or Q2, whichever has minimum one.\nmax_len = [max(u_,t_) for u_,t_ in zip(q1_len,q2_len)] # this returns length of either Q1 or Q2, whichever has maximum one.\n\ncwc_min = [round((e/f),3) if f!=0 else 0 for e,f in zip(common,min_len)] #if min_len =0, appending zero to avoid zero divison error\ncwc_max = [round((e/f),3) for e,f in zip(common,max_len)]\n\n# first word equal or not:\n\nfirst_word = []\nfor g in range(len(q1_clean)):\n    if len(q1_clean[g].split()) != 0 and len(q2_clean[g].split()) != 0: # handling the condition when length of question = 0\n        s3 = q1_clean[g].split()[0]\n        s4 = q2_clean[g].split()[0]\n    \n        if s3 == s4:\n            first_word.append(1) # if first word of Q1 & Q2 is same, append 1\n        else:\n            first_word.append(0) # if first word of Q1 & Q2 is NOT same, append 0\n    else:\n        first_word.append(0)\n        \n\n# last word equal or not:\nlast_word = []\nfor g in range(len(q1_clean)):\n    if len(q1_clean[g].split()) != 0 and len(q2_clean[g].split()) != 0:\n        s3 = q1_clean[g].split()[-1]\n        s4 = q2_clean[g].split()[-1]\n        if s3 == s4: \n            last_word.append(1) # if last word of Q1 & Q2 is same, append 1. Else 0\n        else:\n            last_word.append(0)\n    else:\n        last_word.append(0)\n        \nlen_diff = [abs(t1-t2) for t1,t2 in zip(q1_len,q2_len)] #this returns absolute difference between number of words in Q1 & Q2\navg_len = [(t1+t2)/2 for t1,t2 in zip(q1_len,q2_len)] #this returns average number of words in Q1 & Q2","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:44:05.018459Z","iopub.execute_input":"2022-04-01T12:44:05.018685Z","iopub.status.idle":"2022-04-01T12:44:06.150925Z","shell.execute_reply.started":"2022-04-01T12:44:05.018656Z","shell.execute_reply":"2022-04-01T12:44:06.150147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding these new extracted features to the dataframe\n\ndata_['q1_length'] = q1_len\ndata_['q2_length'] = q2_len\ndata_['common words'] = common\ndata_['word share'] = word_share\ndata_['cwc_min'] = cwc_min\ndata_['cwc_max'] = cwc_max\ndata_['first word equal'] = first_word\ndata_['last word equal'] = last_word\ndata_['difference in no.of words'] = len_diff\ndata_['avg length of words'] = avg_len","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:44:06.152107Z","iopub.execute_input":"2022-04-01T12:44:06.152453Z","iopub.status.idle":"2022-04-01T12:44:06.316748Z","shell.execute_reply.started":"2022-04-01T12:44:06.152424Z","shell.execute_reply":"2022-04-01T12:44:06.315878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(data_.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:44:06.31964Z","iopub.execute_input":"2022-04-01T12:44:06.320166Z","iopub.status.idle":"2022-04-01T12:44:06.340535Z","shell.execute_reply.started":"2022-04-01T12:44:06.320121Z","shell.execute_reply":"2022-04-01T12:44:06.339594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization for \"Word Share\":\n\nplt.figure(figsize=(8, 6))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word share', data = data_[0:])\n\nplt.subplot(1,2,2)\nsns.kdeplot(data_[data_['is_duplicate'] == 1.0]['word share'][0:] , label = \"1\", color = 'red')\nsns.kdeplot(data_[data_['is_duplicate'] == 0.0]['word share'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:44:06.341953Z","iopub.execute_input":"2022-04-01T12:44:06.342202Z","iopub.status.idle":"2022-04-01T12:44:07.139466Z","shell.execute_reply.started":"2022-04-01T12:44:06.342171Z","shell.execute_reply":"2022-04-01T12:44:07.138559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-- There is significant overlap when word_share is used as feature to classify question pairs as duplicate or not.So, this feature doesn't help much.","metadata":{}},{"cell_type":"code","source":"# Data visualization for \"Common words\":\n\nplt.figure(figsize=(8, 6))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'common words', data = data_[0:])\n\nplt.subplot(1,2,2)\nsns.kdeplot(data_[data_['is_duplicate'] == 1.0]['common words'][0:] , label = \"1\", color = 'red')\nsns.kdeplot(data_[data_['is_duplicate'] == 0.0]['common words'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:44:07.140663Z","iopub.execute_input":"2022-04-01T12:44:07.140913Z","iopub.status.idle":"2022-04-01T12:44:07.992304Z","shell.execute_reply.started":"2022-04-01T12:44:07.140883Z","shell.execute_reply":"2022-04-01T12:44:07.991696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-- Highly overlapping distibution is observed. Hence, this feature too doesn't do much good for classifying labels 0 & 1","metadata":{}},{"cell_type":"markdown","source":"# Advanced Feature Extraction: (Fuzzwuzzy)","metadata":{}},{"cell_type":"code","source":"# To get longest substring ratio\n\ndef lsubstring_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b)) # This returns the length of longest common substring\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1) # This gives us the LCS ratio\n\n# partial ratio:\npartial = [fuzz.partial_ratio(k.split(),l.split()) for k,l in zip(q1_clean,q2_clean)]\n\n# token sort ratio:\ntoken_sort = [fuzz.token_sort_ratio(k.split(),l.split()) for k,l in zip(q1_clean,q2_clean)]\n\n# token set ratio:\ntoken_set = [fuzz.token_set_ratio(k.split(),l.split()) for k,l in zip(q1_clean,q2_clean)]\n\n# WRatio:\nwratio = [fuzz.WRatio(k.split(),l.split()) for k,l in zip(q1_clean,q2_clean)]\n\n# LCSubstring ratio\nlcs = [lsubstring_ratio(k,l) for k,l in zip(q1_clean,q2_clean)]\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:44:07.993389Z","iopub.execute_input":"2022-04-01T12:44:07.993958Z","iopub.status.idle":"2022-04-01T12:45:46.180618Z","shell.execute_reply.started":"2022-04-01T12:44:07.993924Z","shell.execute_reply":"2022-04-01T12:45:46.179554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding these new advanced features to the dataframe:\n\ndata_[\"token_set_ratio\"] = token_set\ndata_[\"token_sort_ratio\"] = token_sort\ndata_[\"fuzz_WRatio\"] = wratio\ndata_[\"fuzz_partial_ratio\"] = partial\ndata_[\"longest_substr_ratio\"]  = lcs\n\ndata_.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:45:46.182152Z","iopub.execute_input":"2022-04-01T12:45:46.182449Z","iopub.status.idle":"2022-04-01T12:45:46.310269Z","shell.execute_reply.started":"2022-04-01T12:45:46.182414Z","shell.execute_reply":"2022-04-01T12:45:46.309391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization of Advanced Features:","metadata":{}},{"cell_type":"code","source":"# Pair plot of some of the advanced features:\n\nn = len(data_)\nsns.pairplot(data_[['token_set_ratio', 'token_sort_ratio', 'fuzz_partial_ratio', 'fuzz_WRatio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['token_set_ratio', 'token_sort_ratio', 'fuzz_partial_ratio', 'fuzz_WRatio'])\nplt.show()\n\n# vars:list of variable names\n# Variables within data to use, otherwise use every column with a numeric datatype.","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:45:46.311555Z","iopub.execute_input":"2022-04-01T12:45:46.311798Z","iopub.status.idle":"2022-04-01T12:46:25.354765Z","shell.execute_reply.started":"2022-04-01T12:45:46.311768Z","shell.execute_reply":"2022-04-01T12:46:25.353826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-- From above combination of features we see that 'token sort/set ratio' & 'fuzz_partial_ratio' do fairly good.","metadata":{}},{"cell_type":"code","source":"# Distribution of the fuzz_partial_ratio:\n\nplt.figure(figsize=(8, 6))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_partial_ratio', data = data_[0:])\n\nplt.subplot(1,2,2)\nsns.kdeplot(data_[data_['is_duplicate'] == 1.0]['fuzz_partial_ratio'][0:] , label = \"1\", color = 'red',)\nsns.kdeplot(data_[data_['is_duplicate'] == 0.0]['fuzz_partial_ratio'][0:] , label = \"0\" , color = 'blue')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:25.359448Z","iopub.execute_input":"2022-04-01T12:46:25.360022Z","iopub.status.idle":"2022-04-01T12:46:26.244605Z","shell.execute_reply.started":"2022-04-01T12:46:25.359975Z","shell.execute_reply":"2022-04-01T12:46:26.243695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-- Fuzz partial ratio as a feature has major overlap for values between 45 - 100.\n","metadata":{}},{"cell_type":"code","source":"# Distribution of the WRatio:\n\nplt.figure(figsize=(8, 6))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_WRatio', data = data_[0:])\n\nplt.subplot(1,2,2)\nsns.kdeplot(data_[data_['is_duplicate'] == 1.0]['fuzz_WRatio'][0:] , label = \"1\", color = 'red',)\nsns.kdeplot(data_[data_['is_duplicate'] == 0.0]['fuzz_WRatio'][0:] , label = \"0\" , color = 'blue')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:26.245791Z","iopub.execute_input":"2022-04-01T12:46:26.246399Z","iopub.status.idle":"2022-04-01T12:46:27.08379Z","shell.execute_reply.started":"2022-04-01T12:46:26.246363Z","shell.execute_reply":"2022-04-01T12:46:27.082603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-- For WRatio <45, class label 0 is easily distinguishable","metadata":{}},{"cell_type":"code","source":"# Distribution of the Token Set ratio:\n\nplt.figure(figsize=(8, 6))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'token_set_ratio', data = data_[0:])\n\nplt.subplot(1,2,2)\nsns.kdeplot(data_[data_['is_duplicate'] == 1.0]['token_set_ratio'][0:] , label = \"1\", color = 'red',)\nsns.kdeplot(data_[data_['is_duplicate'] == 0.0]['token_set_ratio'][0:] , label = \"0\" , color = 'blue')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:27.086029Z","iopub.execute_input":"2022-04-01T12:46:27.086483Z","iopub.status.idle":"2022-04-01T12:46:27.90961Z","shell.execute_reply.started":"2022-04-01T12:46:27.086428Z","shell.execute_reply":"2022-04-01T12:46:27.908948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-- Token set ratio as a feature works fairly well as seen from kdeplot.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing:","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tqdm import tqdm\nfrom gensim.models import Word2Vec","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:27.910857Z","iopub.execute_input":"2022-04-01T12:46:27.91165Z","iopub.status.idle":"2022-04-01T12:46:28.240781Z","shell.execute_reply.started":"2022-04-01T12:46:27.911613Z","shell.execute_reply":"2022-04-01T12:46:28.239872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing stop words:\nall_stopwords = stopwords.words('english')\n\n# Removing the following words from list containing stopwords\nall_stopwords.remove('not')\nall_stopwords.remove('but')\nall_stopwords.remove('because')\nall_stopwords.remove('against')\nall_stopwords.remove('between')\nall_stopwords.remove('up')\nall_stopwords.remove('down')\nall_stopwords.remove('in')\nall_stopwords.remove('out')\nall_stopwords.remove('once')\nall_stopwords.remove('before')\nall_stopwords.remove('after')\nall_stopwords.remove('few')\nall_stopwords.remove('more')\nall_stopwords.remove('most')\nall_stopwords.remove('no')\nall_stopwords.remove('nor')\nall_stopwords.remove('same')\nall_stopwords.remove('some')\n\ndef remove_stopwords(texts):\n    '''\n    This function removes stopwords from the sentences\n    \n    '''\n    sentence = texts.lower().strip()\n    sentence = sentence.split()\n    final1 = [word1 for word1 in sentence if not word1 in set(all_stopwords)]\n    final1_ = ' '.join(final1)\n    return final1_\n\nq1_clean1 = [] # this contains question1 without any stopwords.\nfor z in range(len(q1_clean)):\n    res = remove_stopwords(str(q1_clean[z]))\n    q1_clean1.append(res)\n\nq2_clean2 = [] # this contains question2 without any stopwords.\nfor z in range(len(q1_clean)):\n    res2 = remove_stopwords(str(q2_clean[z]))\n    q2_clean2.append(res2)  ","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:28.242065Z","iopub.execute_input":"2022-04-01T12:46:28.24295Z","iopub.status.idle":"2022-04-01T12:46:31.787301Z","shell.execute_reply.started":"2022-04-01T12:46:28.242905Z","shell.execute_reply":"2022-04-01T12:46:31.785983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(q1_clean1[:3])\nprint('*'*50)\nprint(q2_clean2[:3])","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:31.789512Z","iopub.execute_input":"2022-04-01T12:46:31.789914Z","iopub.status.idle":"2022-04-01T12:46:31.796488Z","shell.execute_reply.started":"2022-04-01T12:46:31.789873Z","shell.execute_reply":"2022-04-01T12:46:31.795481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF-IDF Vector:\n\ntotal_questions = (q1_clean1) + (q2_clean2)\ntfidf = TfidfVectorizer(max_features = 500, min_df=50)\ntfidf.fit(total_questions)\nq1_vector = tfidf.transform(q1_clean1).toarray()\nq2_vector = tfidf.transform(q2_clean2).toarray()\n\n# Creating a dictionary with word as a key, and the idf as a value\n# This is done so that we can get TF-IDF values.\n\ntfidf_values = dict(zip(tfidf.get_feature_names(), list(tfidf.idf_)))\nprint('Shape of tf-idf vector is: ',q1_vector.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:31.798261Z","iopub.execute_input":"2022-04-01T12:46:31.798528Z","iopub.status.idle":"2022-04-01T12:46:34.968051Z","shell.execute_reply.started":"2022-04-01T12:46:31.798495Z","shell.execute_reply":"2022-04-01T12:46:34.966239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = tfidf.get_feature_names()\nprint(features[:25])","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:34.970299Z","iopub.execute_input":"2022-04-01T12:46:34.970703Z","iopub.status.idle":"2022-04-01T12:46:34.981574Z","shell.execute_reply.started":"2022-04-01T12:46:34.970653Z","shell.execute_reply":"2022-04-01T12:46:34.979768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating my own W2V model on the Quora question pair corpus:","metadata":{}},{"cell_type":"code","source":"# Creating w2v model on question 1:\nq1_list = []\nfor new in (q1_clean1):\n    q1_list.append(new.split())\n    \nw2v_q1=Word2Vec(q1_list,min_count=5,vector_size=200, workers=2)\nw2v_words_q1 = list(w2v_q1.wv.key_to_index)\nprint(\"sample words \", w2v_words_q1[0:50])","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:34.983671Z","iopub.execute_input":"2022-04-01T12:46:34.984447Z","iopub.status.idle":"2022-04-01T12:46:38.141175Z","shell.execute_reply.started":"2022-04-01T12:46:34.984393Z","shell.execute_reply":"2022-04-01T12:46:38.140507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating w2v model on question 2:\nq2_list = []\nfor new2 in (q2_clean2):\n    q2_list.append(new2.split())\n    \nw2v_q2 = Word2Vec(q2_list,min_count=5,vector_size=200, workers=2)\nw2v_words_q2 = list(w2v_q2.wv.key_to_index)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:38.142102Z","iopub.execute_input":"2022-04-01T12:46:38.142436Z","iopub.status.idle":"2022-04-01T12:46:40.819966Z","shell.execute_reply.started":"2022-04-01T12:46:38.142403Z","shell.execute_reply":"2022-04-01T12:46:40.819222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting TF-IDF*W2V values for question 1 :\n\ntfidf_w2v_q1 = [] # the tfidf-w2v for each question1 is stored in this list\n\nfor sent1 in tqdm(q1_list): # for each question1\n    sent_vec1 = np.zeros(200) \n    tfidf_sum1 =0\n    for word1 in sent1: # for each word in question1\n        if word1 in w2v_words_q1 and word1 in features:\n            vec1 = w2v_q1.wv[word1] # w2v vector for the word\n            tf_idf_q1 = tfidf_values[word1]*(sent1.count(word1)/len(sent1)) # idf * tf = tfidf\n            sent_vec1 += (vec1 * tf_idf_q1) # w2v * tfidf\n            tfidf_sum1 += tf_idf_q1 # summation of tfidf\n    if tfidf_sum1 != 0: # handling boundary condition\n        sent_vec1 = sent_vec1/tfidf_sum1\n    tfidf_w2v_q1.append(sent_vec1)\n\ntfidf_w2v_q1_list = list(tfidf_w2v_q1)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:40.821267Z","iopub.execute_input":"2022-04-01T12:46:40.821703Z","iopub.status.idle":"2022-04-01T12:46:53.175142Z","shell.execute_reply.started":"2022-04-01T12:46:40.821655Z","shell.execute_reply":"2022-04-01T12:46:53.174118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting TF-IDF*W2V values for question 2 :\n\ntfidf_w2v_q2 = [] # the tfidf-w2v for each question 2 is stored in this list\n\nfor sent2 in tqdm(q2_list): # for each question2\n    sent_vec2 = np.zeros(200) \n    tfidf_sum2 =0\n    for word2 in sent2: # for each word in question2\n        if word2 in w2v_words_q2 and word2 in features:\n            vec2 = w2v_q2.wv[word2] \n            tf_idf_q2 = tfidf_values[word2]*(sent2.count(word2)/len(sent2)) \n            sent_vec2 += (vec2 * tf_idf_q2) \n            tfidf_sum2 += tf_idf_q2 \n    if tfidf_sum2 != 0:\n        sent_vec2 = sent_vec2/tfidf_sum2\n    tfidf_w2v_q2.append(sent_vec2)\n\ntfidf_w2v_q2_list = list(tfidf_w2v_q2)\n   ","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:46:53.176602Z","iopub.execute_input":"2022-04-01T12:46:53.177005Z","iopub.status.idle":"2022-04-01T12:47:05.70245Z","shell.execute_reply.started":"2022-04-01T12:46:53.176959Z","shell.execute_reply":"2022-04-01T12:47:05.701707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The dimensions of TF_IDF_W2V is:',len(tfidf_w2v_q2[0]))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:47:05.703386Z","iopub.execute_input":"2022-04-01T12:47:05.703607Z","iopub.status.idle":"2022-04-01T12:47:05.71078Z","shell.execute_reply.started":"2022-04-01T12:47:05.703581Z","shell.execute_reply":"2022-04-01T12:47:05.709665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the features which won't be required in Modeling\n\nnew_data = data_.drop(['qid1','qid2','question1','question2'], axis = 1)\nprint(new_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:47:05.712086Z","iopub.execute_input":"2022-04-01T12:47:05.712367Z","iopub.status.idle":"2022-04-01T12:47:05.729581Z","shell.execute_reply.started":"2022-04-01T12:47:05.712335Z","shell.execute_reply":"2022-04-01T12:47:05.728934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n1 = pd.DataFrame(tfidf_w2v_q1_list,index = new_data.index) # Storing the q1 vector here\nn2 = pd.DataFrame(tfidf_w2v_q2_list,index = new_data.index) # Storing the q2 vector here\nn1['id']=new_data['id']\nn2['id']=new_data['id']","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:47:05.730725Z","iopub.execute_input":"2022-04-01T12:47:05.731485Z","iopub.status.idle":"2022-04-01T12:47:19.393297Z","shell.execute_reply.started":"2022-04-01T12:47:05.73141Z","shell.execute_reply":"2022-04-01T12:47:19.391737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merging all the dataframes:\n\nn3 = n1.merge(n2,on = 'id',how ='left')\nfinal_data = new_data.merge(n3,on = 'id',how ='left')\nprint('Final dimensions of the data:',final_data.shape)\nfinal_data","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:47:19.395915Z","iopub.execute_input":"2022-04-01T12:47:19.396293Z","iopub.status.idle":"2022-04-01T12:47:19.957145Z","shell.execute_reply.started":"2022-04-01T12:47:19.396257Z","shell.execute_reply":"2022-04-01T12:47:19.956302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling:","metadata":{}},{"cell_type":"code","source":"import math\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:47:19.959139Z","iopub.execute_input":"2022-04-01T12:47:19.960017Z","iopub.status.idle":"2022-04-01T12:47:20.184346Z","shell.execute_reply.started":"2022-04-01T12:47:19.959953Z","shell.execute_reply":"2022-04-01T12:47:20.182573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = final_data.drop(['is_duplicate','id'],axis = 1)\ny = final_data['is_duplicate']","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:47:20.186085Z","iopub.execute_input":"2022-04-01T12:47:20.186427Z","iopub.status.idle":"2022-04-01T12:47:20.3726Z","shell.execute_reply.started":"2022-04-01T12:47:20.186389Z","shell.execute_reply":"2022-04-01T12:47:20.37137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data into Train,CV and Test Sets:\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,stratify = y,random_state = 0)\nx_train1,x_cv,y_train1,y_cv = train_test_split(x_train,y_train,test_size = 0.2,stratify = y_train,random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:47:20.374363Z","iopub.execute_input":"2022-04-01T12:47:20.375585Z","iopub.status.idle":"2022-04-01T12:47:20.723721Z","shell.execute_reply.started":"2022-04-01T12:47:20.375529Z","shell.execute_reply":"2022-04-01T12:47:20.722794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train data dimensions:',x_train1.shape)\nprint('Cross validation data dimensions:',x_cv.shape)\nprint('Test data dimensions:',x_test.shape)\nprint('Total train data dimensions:',x_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:47:20.724847Z","iopub.execute_input":"2022-04-01T12:47:20.725078Z","iopub.status.idle":"2022-04-01T12:47:20.731229Z","shell.execute_reply.started":"2022-04-01T12:47:20.725051Z","shell.execute_reply":"2022-04-01T12:47:20.730158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Model: (To check the log-loss to beat)","metadata":{}},{"cell_type":"code","source":"# Since we have binary classification problem, this model randomly generates either 1 or 0 as predicted class label.\n\npredicted_y = np.zeros((len(y_test),2))\nfor i in range(len(y_test)):\n    random_prob = np.random.rand(1,2)\n    predicted_y[i] = ((random_prob/sum(sum(random_prob)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:47:20.732699Z","iopub.execute_input":"2022-04-01T12:47:20.733075Z","iopub.status.idle":"2022-04-01T12:47:20.870421Z","shell.execute_reply.started":"2022-04-01T12:47:20.733033Z","shell.execute_reply":"2022-04-01T12:47:20.868935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression with Hyperparameter Tuning:","metadata":{}},{"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparameter for SGD classifier.\n\nlog_error_array=[] # this list contains the log-loss obtained with different values of alpha\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(x_train1, y_train1)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(x_train1, y_train1)\n    predict_y = sig_clf.predict_proba(x_cv) # getting the predictions in form of probabilities to use log_loss as a metric\n    \n    log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\n\nbest_alpha = np.argmin(log_error_array) # selecting the alpha with minimum log_loss\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(x_train, y_train) # now fitting on entire train data\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(x_train, y_train)\n\npredict_y = sig_clf.predict_proba(x_train)\nprint('*'*100)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(x_test)\nprint('*'*100)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:47:20.872288Z","iopub.execute_input":"2022-04-01T12:47:20.872738Z","iopub.status.idle":"2022-04-01T12:52:35.381357Z","shell.execute_reply.started":"2022-04-01T12:47:20.872687Z","shell.execute_reply":"2022-04-01T12:52:35.380278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest:","metadata":{}},{"cell_type":"code","source":"classifier = RandomForestClassifier(n_estimators = 15, criterion = 'entropy', random_state = 0, max_depth = 5)\nclassifier.fit(x_train, y_train)\n\nsig_clf = CalibratedClassifierCV(classifier, method=\"sigmoid\")\nsig_clf.fit(x_train, y_train)\n\npredict_y_rf_train = sig_clf.predict_proba(x_train)\nprint('*'*100)\n\nprint(\"The train log loss is:\",log_loss(y_train, predict_y_rf_train, labels=classifier.classes_, eps=1e-15))\npredict_y_rf_test = sig_clf.predict_proba(x_test)\nprint('*'*100)\nprint( \"The test log loss is:\",log_loss(y_test, predict_y_rf_test, labels=classifier.classes_, eps=1e-15))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:52:35.383101Z","iopub.execute_input":"2022-04-01T12:52:35.383645Z","iopub.status.idle":"2022-04-01T12:53:04.926519Z","shell.execute_reply.started":"2022-04-01T12:52:35.383598Z","shell.execute_reply":"2022-04-01T12:53:04.9256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost:","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.01\nparams['max_depth'] = 5\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_test = xgb.DMatrix(x_test, label=y_test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 300, watchlist, early_stopping_rounds = 20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(x_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n# https://xgboost.readthedocs.io/en/stable/python/python_intro.html","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:53:04.928003Z","iopub.execute_input":"2022-04-01T12:53:04.928314Z","iopub.status.idle":"2022-04-01T12:59:07.291923Z","shell.execute_reply.started":"2022-04-01T12:53:04.928271Z","shell.execute_reply":"2022-04-01T12:59:07.290894Z"},"trusted":true},"execution_count":null,"outputs":[]}]}