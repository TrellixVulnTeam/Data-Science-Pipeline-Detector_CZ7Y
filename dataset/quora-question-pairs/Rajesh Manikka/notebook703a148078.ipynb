{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","version":"3.6.3","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py"}},"cells":[{"metadata":{"_cell_guid":"d6673536-6ee8-4f11-92ce-1c8e12202926","_uuid":"44de0de7fe83e40bf36f5d67681a789485f84ba7"},"cell_type":"code","execution_count":null,"source":"import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import Word2Vec\nfrom gensim import corpora\nfrom gensim import models\n\nfrom gensim.matutils import jaccard, cossim\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cross_validation import train_test_split\nimport xgboost as xgb\nfrom sklearn.cross_validation import train_test_split\nimport os.path\n\n\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\nstop = stopwords.words('english')\n\ndef preprocess(data):\n    return data.lower().split()\n\ntrain = pd.read_csv(\"../input/train.csv\", keep_default_na='')\ntest = pd.read_csv(\"../input/test.csv\", keep_default_na='')\n\ncombine = [train, test]\n\nprint(\"Preprocessing questions...\")    \nfor dataset in combine:\n    dataset['processed_q1'] = dataset['question1'].apply(preprocess)\n    dataset['processed_q2'] = dataset['question2'].apply(preprocess)\n\n\nall_questions = train['processed_q1'] + train['processed_q1'] + test['processed_q1'] + test['processed_q2']\nall_questions = all_questions[all_questions.notnull()]\n\ndict_file = 'quora.dict'\ncorpus_file = 'corpus.mm'\nword_vec_file = 'word2vec.model'\n\n\nif os.path.isfile(dict_file):\n    print(\"Loading dictionary file\")\n    dictionary = corpora.Dictionary.load(dict_file)\nelse:    \n    print(\"Generating dictionary file\")\n    dictionary = corpora.Dictionary(all_questions)\n    dictionary.save(dict_file)\n\nif os.path.isfile(corpus_file):\n    print(\"Loading corpus file\")\n    corpus = corpora.MmCorpus(corpus_file)\nelse:    \n    print(\"Generating corpus file\")\n    corpus = [dictionary.doc2bow(question) for question in all_questions]\n    corpora.MmCorpus.serialize(corpus_file, corpus) \n\nif os.path.isfile(word_vec_file):\n    print(\"Loading word 2 vect file\")\n    w2v_model = Word2Vec.load(word_vec_file)\nelse:\n    print(\"Generating word 2 vect file\")\n    w2v_model = Word2Vec(all_questions, min_count=10)\n    w2v_model.save(word_vec_file)\n    \ndef to_bow(doc):\n    return dictionary.doc2bow(doc)\n\ndef filter_in_vacob(doc):\n    return list(filter(lambda x: x in w2v_model, doc))\n\nprint(\"Computing similarity metrics\")\nfor dataset in combine:\n    dataset['jaccard'] = dataset.apply(lambda d: jaccard(to_bow(d['processed_q1']), to_bow(d['processed_q2'])), axis=1)\n    dataset['cosine'] = dataset.apply(lambda d: cossim(to_bow(d['processed_q1']), to_bow(d['processed_q2'])), axis=1)\n    dataset['wv_doc_1'] = dataset['processed_q1'].apply(lambda d:filter_in_vacob(d))\n    dataset['wv_doc_2'] = dataset['processed_q2'].apply(lambda d:filter_in_vacob(d))\n    dataset['wv_sim'] = dataset.apply(lambda d: w2v_model.wv.n_similarity(d['wv_doc_1'], d['wv_doc_2']) if d['wv_doc_1'] and d['wv_doc_2'] else -1, axis=1)\n\n    \nx_train = pd.DataFrame()\nx_test = pd.DataFrame()\n\nx_train['jaccard'] = train['jaccard']\nx_train['cosine'] = train['cosine']\nx_train['wv_sim'] = train['wv_sim']\n\nx_test['jaccard'] = test['jaccard']\nx_test['cosine'] = test['cosine']\nx_test['wv_sim'] = test['wv_sim']\n\ny_train = train['is_duplicate']\n\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n    ","outputs":[]},{"metadata":{"collapsed":true},"cell_type":"code","execution_count":null,"source":"params = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, verbose_eval=10)\nd_test = xgb.DMatrix(x_test)\np_test = bst.predict(d_test)\n\nsub = pd.DataFrame()\nsub['test_id'] = test['test_id']\nsub['is_duplicate'] = p_test\nsub.to_csv('simple_xgb.csv', index=False)","outputs":[]},{"metadata":{},"cell_type":"code","execution_count":null,"source":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict_proba(x_test)\nprint(random_forest.score(x_train, y_train))\n\ny_pred_df = pd.DataFrame({'0':y_pred[:,0],'1':y_pred[:,1]})\nsub = pd.DataFrame()\nsub['test_id'] = test['test_id']\nsub['is_duplicate'] = y_pred_df['1']\nsub.to_csv('random_forest_probability.csv', index=False)","outputs":[]}],"nbformat":4,"nbformat_minor":1}