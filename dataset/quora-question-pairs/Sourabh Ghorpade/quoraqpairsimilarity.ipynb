{"cells":[{"metadata":{"id":"hkI4OpiH0sLX","outputId":"7a790b84-22c5-4f09-b66c-2fa617894433","trusted":true},"cell_type":"code","source":"!pip install fuzzywuzzy\n!pip install distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nimport distance\ndata=pd.read_csv('../input/quoraqpairtrain/train.csv')\nfrom nltk.corpus import stopwords\nimport nltk\nimport ssl\nimport re\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"id":"ORBHmwBUJeEc","trusted":true},"cell_type":"code","source":"print(\"Top 5 Datapoints : \\n\",data.head())\nprint()\nprint(\"Shape of Data : \",data.shape)\nprint()\nprint(\"Shape of Data where questions are same : \",data[data['is_duplicate']==1].shape)\nprint()\nprint(\"Shape of Data where questions are different : \",data[data['is_duplicate']==0].shape)\nprint()\nprint(\"Distribution of the two classes :\\n\",data['is_duplicate'].value_counts())\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"su1SNIz9qq1t","trusted":true},"cell_type":"code","source":"def preprocess(x):\n        x=str(x).lower()\n        x=x.replace(',000,000','m').replace(',000','k').replace(\"'\",\"`\").replace(\"won't\",'will not').replace(\"can't\",\"can not\").replace('cannot','can not').replace(\"n't\",'not').replace(\"what's\",\"what is\").replace(\"it's\",\"it is\").replace(\"'ve\",\"have\").replace(\"i'm\",\"i am\").replace(\"'re\",\"are\").replace(\"he's\",\"he is\").replace(\"she's\",\"she is \").replace(\"'s\",\"own\").replace(\"%\",\" precent\").replace(\"₹\",\" rupeee\").replace(\"$\",\"dollar\").replace(\"€\",\"euro\").replace(\"'ll\",\"will\")\n        porter = PorterStemmer()\n        pattern = re.compile('\\W')\n    \n        if type(x) == type(''):\n            x = re.sub(pattern, ' ', x)\n        if type(x) == type(''):\n          x = porter.stem(x)\n          example1 = BeautifulSoup(x)\n          x = example1.get_text()      \n        return str(x)","execution_count":null,"outputs":[]},{"metadata":{"id":"iGee4WdzJnWH","trusted":true},"cell_type":"code","source":"data.isnull().sum() #Now we have no null entries in our dataset as you can seee we had null entries in qustions only and we cannot afford to fill these entries with any other technique to handle nul values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words=stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def token_features(q1,q2):\n  feature=[0.0]*10\n  \n  q1_tokens=q1.split()\n  q2_tokens=q2.split()\n\n  if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return feature\n  q1_words=set([word for word in q1_tokens if word not in stop_words])\n  q2_words=set([word for word in q2_tokens if word not in stop_words]) \n\n  q1_stop_words=set([word for word in q1_tokens if word in stop_words])\n  q2_stop_words=set([word for word in q2_tokens if word in stop_words]) \n\n  common_word_count=len(q1_words.intersection(q2_words))\n  common_token_count=len(set(q1_tokens).intersection(set(q2_tokens)))\n  common_stop_count = len(q1_stop_words.intersection(q2_stop_words))\n\n  feature[0]=common_word_count/(min(len(q1_words),len(q2_words))+0.0001)\n  feature[1]=common_word_count/(max(len(q1_words),len(q2_words))+0.0001)\n  feature[2]=common_stop_count/(min(len(q1_stop_words),len(q2_stop_words))+0.0001)\n  feature[3]=common_stop_count/(max(len(q1_stop_words),len(q2_stop_words))+0.0001)\n  feature[4]=common_token_count/(min(len(q1_tokens),len(q2_tokens))+0.0001)\n  feature[5]=common_token_count/(max(len(q1_tokens),len(q2_tokens))+0.0001)\n  feature[6] = int(q1_tokens[-1] == q2_tokens[-1])\n  feature[7] = int(q1_tokens[0] == q2_tokens[0])  \n  feature[8] = abs(len(q1_tokens) - len(q2_tokens))\n    #Average Token Length of both Questions\n  feature[9] = (len(q1_tokens) + len(q2_tokens))/2\n  return feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Jaccard_dist(x,y):\n    return (len(set(str(x)).intersection(set(str(y))))/len(set(str(x)).union(set(str(y)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Featurization\nfrom tqdm import tqdm\ndef featurize(data):\n    data[\"question1\"] = data[\"question1\"].apply(preprocess)\n    data[\"question2\"] = data[\"question2\"].apply(preprocess)\n    data=data[data['question1'].notnull()]\n    data=data[data['question2'].notnull()]\n    data.reset_index(inplace=True,drop=True)\n    for j,i in enumerate(data['question1']):\n        if len(i)<1:\n            data.drop([j],inplace=True)\n    data.reset_index(inplace=True,drop=True)    \n    for j,i in enumerate(data['question2']):\n        if len(i)<1:\n            data.drop([j],inplace=True)\n    data.reset_index(inplace=True,drop=True)\n    \n    vc=data['question1'].value_counts()\n    vc2=data['question2'].value_counts()\n    data['qid1_freq']=pd.Series()\n    data['qid2_freq']=pd.Series()\n    data['len_of_q1']=pd.Series()\n    data['len_of_q2']=pd.Series()\n    data['words_in_q1']=pd.Series()\n    data['words_in_q2']=pd.Series()\n    data['common_words_count']=pd.Series()\n    data['common_words_of_total']=pd.Series()\n    data['total_words_count']=pd.Series()\n    data['sum_of_freq']=pd.Series()\n    data['diff_of_freq']=pd.Series()\n    for i in tqdm(range(0,len(data))):\n        data['qid1_freq'][i]=vc[data['question1'][i]]\n        data['qid2_freq'][i]=vc2[data['question2'][i]]\n        data['len_of_q1'][i]=len(data['question1'][i])\n        data['len_of_q2'][i]=len(data['question2'][i])\n        data['words_in_q1'][i]=len(data['question1'][i].split())\n        data['words_in_q2'][i]=len(data['question2'][i].split())\n        data['common_words_count'][i]=len(set(data['question1'][i]).intersection(set(data['question2'][i])))\n        data['total_words_count'][i]=len(data['question1'][i].split()+data['question2'][i].split())\n        data['common_words_of_total'][i]=len(set(data['question1'][i]).intersection(set(data['question2'][i])))/len(data['question1'][i].split()+data['question2'][i].split())\n        data['sum_of_freq'][i]=vc[data['question1'][i]]+vc2[data['question2'][i]]\n        data['diff_of_freq'][i]=abs(vc[data['question1'][i]]-vc2[data['question2'][i]])\n    features=data.apply(lambda x:token_features(x['question1'],x['question2']),axis=1)\n    data[\"cwc_min\"]= list(map(lambda x: x[0],features))\n    data[\"cwc_max\"]= list(map(lambda x: x[1], features))\n    data[\"csc_min\"]= list(map(lambda x: x[2], features))\n    data[\"csc_max\"] = list(map(lambda x: x[3], features))\n    data[\"ctc_min\"]= list(map(lambda x: x[4], features))\n    data[\"ctc_max\"]= list(map(lambda x: x[5], features))\n    data[\"last_word_eq\"]= list(map(lambda x: x[6], features))\n    data[\"first_word_eq\"]= list(map(lambda x: x[7], features))\n    data[\"abs_len_diff\"]= list(map(lambda x: x[8], features))\n    data[\"mean_len\"]=list(map(lambda x: x[9], features))\n    #Fuzzy_Features\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    data[\"token_sort_ratio\"]      = data.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    data[\"fuzz_ratio\"]            = data.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    data[\"fuzz_partial_ratio\"]    = data.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    data[\"longest_substr_ratio\"]  = data.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    data[\"Jaccard_dist\"]  = data.apply(lambda x: Jaccard_dist(x[\"question1\"], x[\"question2\"]), axis=1)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data=featurize(data)","execution_count":null,"outputs":[]},{"metadata":{"id":"9FOi4mE8gvEL","scrolled":false,"trusted":true},"cell_type":"code","source":"import spacy\nfrom scipy.sparse import hstack\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nX=data.drop('is_duplicate',axis=1)\ny=data['is_duplicate']\nfrom sklearn.model_selection import train_test_split #To avoid data leakage problem we did the splitting at the start\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=41,stratify=y)\n\nX_train.reset_index(drop=True,inplace=True)\nX_test.reset_index(drop=True,inplace=True)\n\nquestions=list(X_train['question1']+X_train['question2'])\ntfidf_vectorizer=TfidfVectorizer()\ntfidf_vectorizer.fit_transform(questions)\nword_and_its_tfidf=dict(zip(tfidf_vectorizer.get_feature_names(),tfidf_vectorizer.idf_))\n\ntfidf_vectorizer.transform(list(X_test['question1']+X_test['question2']))\nword_and_its_tfidf_test=dict(zip(tfidf_vectorizer.get_feature_names(),tfidf_vectorizer.idf_))\n\ndef featurize_with_glove(tfidf_matrix,data):\n    vecs1=[]\n    for qu1 in tqdm(list(data['question1'])):\n        doc1=nlp(qu1)\n        mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n        for word1 in doc1:\n            vec1=word1.vector\n            try:\n                idf = tfidf_matrix[str(word1)]\n            except:\n                idf = 0\n            mean_vec1 += vec1 * idf\n        mean_vec1 = mean_vec1.mean(axis=0)\n        vecs1.append(mean_vec1)\n    #data=data.join(pd.DataFrame(np.array(vecs1).T.tolist()),lsuffix='_')\n    vecs2=[]\n    for qu2 in tqdm(list(data['question2'])):\n        doc2=nlp(qu2)\n        mean_vec2 = np.zeros([len(doc2), len(doc2[0].vector)])\n        for word2 in doc2:\n            vec2=word2.vector\n            try:\n                idf = tfidf_matrix[str(word2)]\n            except:\n                idf = 0\n            mean_vec2 += vec2 * idf\n        mean_vec2 = mean_vec2.mean(axis=0)\n        vecs2.append(mean_vec2)\n    #data=data.join(pd.DataFrame(np.array(vecs2).T.tolist()),rsuffix='_')\n    return data,vecs1,vecs2\nX_train,train_vec1,train_vec2=featurize_with_glove(word_and_its_tfidf,X_train)\nX_test,test_vec1,test_vec2=featurize_with_glove(word_and_its_tfidf_test,X_test)\nX_tr=X_train.iloc[:,5:]\nX_tst=X_test.iloc[:,5:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npickle.dump(X_tr,open('X_tr.pickle','wb'))\npickle.dump(X_tst,open('X_tst.pickle','wb'))\npickle.dump(y_train,open('y_train.pickle','wb'))\npickle.dump(y_test,open('y_test.pickle','wb'))\npickle.dump(tfidf_vectorizer,open('tfidf_vectorizer.pickle','wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A=pd.DataFrame(train_vec1, columns=['q2_0',\n'q2_1',\n'q2_2',\n'q2_3',\n'q2_4',\n'q2_5',\n'q2_6',\n'q2_7',\n'q2_8',\n'q2_9',\n'q2_10',\n'q2_11',\n'q2_12',\n'q2_13',\n'q2_14',\n'q2_15',\n'q2_16',\n'q2_17',\n'q2_18',\n'q2_19',\n'q2_20',\n'q2_21',\n'q2_22',\n'q2_23',\n'q2_24',\n'q2_25',\n'q2_26',\n'q2_27',\n'q2_28',\n'q2_29',\n'q2_30',\n'q2_31',\n'q2_32',\n'q2_33',\n'q2_34',\n'q2_35',\n'q2_36',\n'q2_37',\n'q2_38',\n'q2_39',\n'q2_40',\n'q2_41',\n'q2_42',\n'q2_43',\n'q2_44',\n'q2_45',\n'q2_46',\n'q2_47',\n'q2_48',\n'q2_49',\n'q2_50',\n'q2_51',\n'q2_52',\n'q2_53',\n'q2_54',\n'q2_55',\n'q2_56',\n'q2_57',\n'q2_58',\n'q2_59',\n'q2_60',\n'q2_61',\n'q2_62',\n'q2_63',\n'q2_64',\n'q2_65',\n'q2_66',\n'q2_67',\n'q2_68',\n'q2_69',\n'q2_70',\n'q2_71',\n'q2_72',\n'q2_73',\n'q2_74',\n'q2_75',\n'q2_76',\n'q2_77',\n'q2_78',\n'q2_79',\n'q2_80',\n'q2_81',\n'q2_82',\n'q2_83',\n'q2_84',\n'q2_85',\n'q2_86',\n'q2_87',\n'q2_88',\n'q2_89',\n'q2_90',\n'q2_91',\n'q2_92',\n'q2_93',\n'q2_94',\n'q2_95'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"B=pd.DataFrame(train_vec2, columns=['q1_0',\n'q1_1',\n'q1_2',\n'q1_3',\n'q1_4',\n'q1_5',\n'q1_6',\n'q1_7',\n'q1_8',\n'q1_9',\n'q1_10',\n'q1_11',\n'q1_12',\n'q1_13',\n'q1_14',\n'q1_15',\n'q1_16',\n'q1_17',\n'q1_18',\n'q1_19',\n'q1_20',\n'q1_21',\n'q1_22',\n'q1_23',\n'q1_24',\n'q1_25',\n'q1_26',\n'q1_27',\n'q1_28',\n'q1_29',\n'q1_30',\n'q1_31',\n'q1_32',\n'q1_33',\n'q1_34',\n'q1_35',\n'q1_36',\n'q1_37',\n'q1_38',\n'q1_39',\n'q1_40',\n'q1_41',\n'q1_42',\n'q1_43',\n'q1_44',\n'q1_45',\n'q1_46',\n'q1_47',\n'q1_48',\n'q1_49',\n'q1_50',\n'q1_51',\n'q1_52',\n'q1_53',\n'q1_54',\n'q1_55',\n'q1_56',\n'q1_57',\n'q1_58',\n'q1_59',\n'q1_60',\n'q1_61',\n'q1_62',\n'q1_63',\n'q1_64',\n'q1_65',\n'q1_66',\n'q1_67',\n'q1_68',\n'q1_69',\n'q1_70',\n'q1_71',\n'q1_72',\n'q1_73',\n'q1_74',\n'q1_75',\n'q1_76',\n'q1_77',\n'q1_78',\n'q1_79',\n'q1_80',\n'q1_81',\n'q1_82',\n'q1_83',\n'q1_84',\n'q1_85',\n'q1_86',\n'q1_87',\n'q1_88',\n'q1_89',\n'q1_90',\n'q1_91',\n'q1_92',\n'q1_93',\n'q1_94',\n'q1_95'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr=np.hstack([X_tr,B,A])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A=pd.DataFrame(test_vec1, columns=['q2_0',\n'q2_1',\n'q2_2',\n'q2_3',\n'q2_4',\n'q2_5',\n'q2_6',\n'q2_7',\n'q2_8',\n'q2_9',\n'q2_10',\n'q2_11',\n'q2_12',\n'q2_13',\n'q2_14',\n'q2_15',\n'q2_16',\n'q2_17',\n'q2_18',\n'q2_19',\n'q2_20',\n'q2_21',\n'q2_22',\n'q2_23',\n'q2_24',\n'q2_25',\n'q2_26',\n'q2_27',\n'q2_28',\n'q2_29',\n'q2_30',\n'q2_31',\n'q2_32',\n'q2_33',\n'q2_34',\n'q2_35',\n'q2_36',\n'q2_37',\n'q2_38',\n'q2_39',\n'q2_40',\n'q2_41',\n'q2_42',\n'q2_43',\n'q2_44',\n'q2_45',\n'q2_46',\n'q2_47',\n'q2_48',\n'q2_49',\n'q2_50',\n'q2_51',\n'q2_52',\n'q2_53',\n'q2_54',\n'q2_55',\n'q2_56',\n'q2_57',\n'q2_58',\n'q2_59',\n'q2_60',\n'q2_61',\n'q2_62',\n'q2_63',\n'q2_64',\n'q2_65',\n'q2_66',\n'q2_67',\n'q2_68',\n'q2_69',\n'q2_70',\n'q2_71',\n'q2_72',\n'q2_73',\n'q2_74',\n'q2_75',\n'q2_76',\n'q2_77',\n'q2_78',\n'q2_79',\n'q2_80',\n'q2_81',\n'q2_82',\n'q2_83',\n'q2_84',\n'q2_85',\n'q2_86',\n'q2_87',\n'q2_88',\n'q2_89',\n'q2_90',\n'q2_91',\n'q2_92',\n'q2_93',\n'q2_94',\n'q2_95'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"B=pd.DataFrame(test_vec2, columns=['q1_0',\n'q1_1',\n'q1_2',\n'q1_3',\n'q1_4',\n'q1_5',\n'q1_6',\n'q1_7',\n'q1_8',\n'q1_9',\n'q1_10',\n'q1_11',\n'q1_12',\n'q1_13',\n'q1_14',\n'q1_15',\n'q1_16',\n'q1_17',\n'q1_18',\n'q1_19',\n'q1_20',\n'q1_21',\n'q1_22',\n'q1_23',\n'q1_24',\n'q1_25',\n'q1_26',\n'q1_27',\n'q1_28',\n'q1_29',\n'q1_30',\n'q1_31',\n'q1_32',\n'q1_33',\n'q1_34',\n'q1_35',\n'q1_36',\n'q1_37',\n'q1_38',\n'q1_39',\n'q1_40',\n'q1_41',\n'q1_42',\n'q1_43',\n'q1_44',\n'q1_45',\n'q1_46',\n'q1_47',\n'q1_48',\n'q1_49',\n'q1_50',\n'q1_51',\n'q1_52',\n'q1_53',\n'q1_54',\n'q1_55',\n'q1_56',\n'q1_57',\n'q1_58',\n'q1_59',\n'q1_60',\n'q1_61',\n'q1_62',\n'q1_63',\n'q1_64',\n'q1_65',\n'q1_66',\n'q1_67',\n'q1_68',\n'q1_69',\n'q1_70',\n'q1_71',\n'q1_72',\n'q1_73',\n'q1_74',\n'q1_75',\n'q1_76',\n'q1_77',\n'q1_78',\n'q1_79',\n'q1_80',\n'q1_81',\n'q1_82',\n'q1_83',\n'q1_84',\n'q1_85',\n'q1_86',\n'q1_87',\n'q1_88',\n'q1_89',\n'q1_90',\n'q1_91',\n'q1_92',\n'q1_93',\n'q1_94',\n'q1_95'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tst=np.hstack((X_tst,B,A))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_tr,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.score(X_tst,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open('X_tr.pickle','wb') as f:\n    pickle.dump(X_tr,f)\nwith open('X_tst.pickle','wb') as f:\n    pickle.dump(X_tst,f)\nwith open('y_train.pickle','wb') as f:\n    pickle.dump(y_train,f)\nwith open('y_test.pickle','wb') as f:\n    pickle.dump(y_test,f)\nwith open('tfidf_vectorizer.pickle','wb') as f:\n    pickle.dump(tfidf_vectorizer,f)\nwith open('glove_nlp.pickle','wb') as f:\n    pickle.dump(nlp,f)\nwith open('logistc_regressor.pickle','wb') as f:\n    pickle.dump(lr,f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open('X_tr.pickle','rb') as f:\n    X_tr=pickle.load(f)\nwith open('X_tst.pickle','rb') as f:\n    X_tst=pickle.load(f)\nwith open('y_train.pickle','rb') as f:\n    y_train=pickle.load(f)\nwith open('y_test.pickle','rb') as f:\n    y_test=pickle.load(f)\nwith open('tfidf_vectorizer.pickle','rb') as f:\n    tfidf_vectorizer=pickle.load(f)\nwith open('glove_nlp.pickle','rb') as f:\n    nlp=pickle.load(f)\nwith open('logistc_regressor.pickle','rb') as f:\n    lr=pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=lr.predict(X_tst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test.values,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test.values,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRfc=RandomForestClassifier(n_estimators=500,n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Rfc.fit(X_tr,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test,Rfc.predict(X_tst))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nprint(\"RandomForest log loss\",log_loss(y_test,Rfc.predict_proba(X_tst)))\nprint(\"RandomForest Train log loss\",log_loss(y_train,Rfc.predict_proba(X_tr)))\nprint()\nprint(\"Logistic Regression log loss\",log_loss(y_test,lr.predict_proba(X_tst)))\nprint()\nprint(\"CatBoostClassifier Train log loss\",log_loss(y_train,cat.predict_proba(X_tr)))\nprint(\"CatBoostClassifier log loss\",log_loss(y_test,cat.predict_proba(X_tst)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Rfc.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test.values,Rfc.predict(X_tst)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\ncat=CatBoostClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat.fit(X_tr,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test.values,cat.predict(X_tst))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test.values,cat.predict(X_tst)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat.score(X_tst,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}