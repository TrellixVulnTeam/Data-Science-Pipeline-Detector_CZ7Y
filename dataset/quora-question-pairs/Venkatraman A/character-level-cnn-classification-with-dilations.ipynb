{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, Input\nfrom keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\nfrom keras.preprocessing import text as keras_text, sequence as keras_seq\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport pickle\nmaxlen = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not train:\n    with open('../input/cnn-classification-with-dilations-outputs/tokenizer.pickle', 'rb') as handle:\n        tokenizer = pickle.load(handle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(conv_layers = 2, \n                dilation_rates = [0, 2, 4, 8, 16], \n                embed_size = 256):\n    inp1 = Input(shape=(None, ))\n    inp2 = Input(shape=(None, ))\n    x1 = Embedding(input_dim = len(tokenizer.word_counts)+1, \n                  output_dim = embed_size)(inp1)\n    x2 = Embedding(input_dim = len(tokenizer.word_counts)+1, \n                  output_dim = embed_size)(inp2)\n    prefilt_x1 = Dropout(0.25)(x1)\n    prefilt_x2 = Dropout(0.25)(x2)\n    out_conv = []\n    # dilation rate lets us use ngrams and skip grams to process\n    count = 0\n    for prefilt_x in [prefilt_x1, prefilt_x2]:\n        count += 1\n        for dilation_rate in dilation_rates:\n            x = prefilt_x\n            for i in range(2):\n                if dilation_rate>0:\n                    x = Conv1D(16*2**(i), \n                               kernel_size = 3, \n                               dilation_rate = dilation_rate,\n                              activation = 'relu',\n                              name = 'ngram_{}_cnn_{}'.format(dilation_rate, str(count)+str(i))\n                              )(x)\n                else:\n                    x = Conv1D(16*2**(i), \n                               kernel_size = 1,\n                              activation = 'relu',\n                              name = 'word_fcl_{}'.format(str(count)+str(i)))(x)\n            out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n    x = concatenate(out_conv, axis = -1)    \n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=[inp1, inp2], outputs=x)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\nmodel = build_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(\"../input/quora-question-pairs/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if train:\n    train_data = pd.read_csv(\"../input/train.csv\")\n\n    from collections import Counter\n    Counter(train_data.is_duplicate)\n\n    #train data shape\n    train_data.shape\n\n    question1_ids = train_data[[\"qid1\", \"question1\"]]\n    question2_ids = train_data[[\"qid2\", \"question2\"]]\n    question2_ids.rename(columns = {'qid2':'qid1', 'question2':'question1'}, inplace = True)\n\n    question1_ids = question1_ids.drop_duplicates(\"qid1\", keep=\"last\")\n    question2_ids = question2_ids.drop_duplicates(\"qid1\", keep=\"last\")\n    question_ids = pd.concat([question1_ids, question2_ids])\n\n    import gc\n    del question1_ids, question2_ids\n    gc.collect()\n\n    question_ids = question_ids.drop_duplicates(\"qid1\", keep=\"last\")\n    question_ids[\"len\"] = question_ids.question1.map(str).apply(len)\n\n    question_ids.sort_values(by=[\"len\"]).reset_index(drop=True)\n\n    all_test_question = pd.concat([test_data.question1, test_data.question2])\n\n    all_test_question = all_test_question.drop_duplicates(keep=False)\n\n    all_sentences = list(map(str, pd.concat([all_test_question, question_ids.question1]).tolist()))\n\n    tokenizer = keras_text.Tokenizer(char_level = True)\n    tokenizer.fit_on_texts(all_sentences)\n\n    import pickle\n    with open('tokenizer.pickle', 'wb') as handle:\n        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    # train data\n    list_tokenized_q1 = tokenizer.texts_to_sequences(train_data.question1.map(str))\n    list_tokenized_q2 = tokenizer.texts_to_sequences(train_data.question2.map(str))\n    X_t_q1 = keras_seq.pad_sequences(list_tokenized_q1, maxlen=maxlen)\n    X_t_q2 = keras_seq.pad_sequences(list_tokenized_q2, maxlen=maxlen)\n\n    y = train_data.is_duplicate\n\n    from sklearn.model_selection import train_test_split\n    print('Distribution of Total Positive Labels (important for validation)')\n    print(pd.value_counts(y))\n    x_indicies = np.array(range(len(X_t_q1)))\n    X_train_indicies, X_test_indicies, y_train, y_test = train_test_split(x_indicies, y, \n                                                            test_size = 0.2, \n                                                            stratify = y,\n                                                           random_state = 2017)\n\n    batch_size = 256 # large enough that some other labels come in\n    epochs = 100\n\n    file_path=\"best_weights.h5\"\n    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=100)\n\n    callbacks_list = [checkpoint, early] #early\n    model.fit([X_t_q1[X_train_indicies], X_t_q2[X_train_indicies]], y_train, \n              validation_data=([X_t_q1[X_test_indicies], X_t_q2[X_test_indicies]], y_test),\n              batch_size=batch_size, \n              epochs=epochs, \n              shuffle = True,\n              callbacks=callbacks_list)\n\n    import gc\n    del train_data, X_t_q1, X_t_q2, list_tokenized_q1, list_tokenized_q2, all_sentences\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test data\nlist_tokenized_test_q1 = tokenizer.texts_to_sequences(test_data.question1.map(str))\nlist_tokenized_test_q2 = tokenizer.texts_to_sequences(test_data.question2.map(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path=\"../input/cnn-classification-with-dilations-outputs/best_weights.h5\"\nmodel.load_weights(file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end = 100000\nprediction_batchs = []\nfor start in range(0, len(list_tokenized_test_q1), end):\n    X_te_q1 = keras_seq.pad_sequences(list_tokenized_test_q1[start: start+end], maxlen=maxlen)\n    X_te_q2 = keras_seq.pad_sequences(list_tokenized_test_q2[start: start+end], maxlen=maxlen)\n    predict_batch = model.predict([X_te_q1, X_te_q2])\n    prediction_batchs.append(predict_batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.concatenate(np.array(prediction_batchs)).reshape(len(list_tokenized_test_q1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/quora-question-pairs/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[\"is_duplicate\"] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}