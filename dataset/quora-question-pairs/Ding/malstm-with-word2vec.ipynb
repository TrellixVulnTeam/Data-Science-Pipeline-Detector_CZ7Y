{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reference\nSiamese Manhattan LSTM methods and preprocessing techniques based on: https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport string\nimport re\nimport numpy as np\nfrom collections import Counter, defaultdict\nfrom pathlib import Path\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom nltk.corpus import stopwords\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import KeyedVectors\nfrom nltk.corpus import stopwords\nimport nltk\n# nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = Path('../input')\ntrain_df = pd.read_csv(PATH/\"quora-question-pairs/train.csv.zip\")\ntest_df = pd.read_csv(PATH/\"quora-question-pairs/test.csv\")\nword2vec = KeyedVectors.load_word2vec_format(\"../input/googleword2vec/GoogleNews-vectors-negative300.bin\", binary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stops = set(stopwords.words('english'))\n\ndef text_to_word_list(text):\n    ''' Pre process and convert texts to a list of words '''\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    text = text.split()\n\n    return text\n\n# Prepare embedding\nvocab2index = {\"<PAD>\":0, \"UNK\":1}\nwords = [\"<PAD>\", \"UNK\"]  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_cols = ['question1', 'question2']\n\n# Iterate over the questions only of both training and test datasets\nfor dataset in [train_df, test_df]:\n    for index, row in tqdm(dataset.iterrows()):\n\n        # Iterate through the text of both questions of the row\n        for question in questions_cols:\n\n            q2n = []  # q2n -> question numbers representation\n            for word in text_to_word_list(row[question]):\n                # Check for unwanted words\n                if word in stops and word not in word2vec.vocab:\n                    continue\n\n                if word not in vocab2index:\n                    vocab2index[word] = len(words)\n                    q2n.append(len(words))\n                    words.append(word)\n                else:\n                    q2n.append(vocab2index[word])\n\n            # Replace questions as word to question as number representation\n            dataset.at[index, question] = q2n\n          ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Create Embedding Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 300\nembeddings_matrix = 1 * np.random.randn(len(vocab2index) + 1, embedding_dim)  # This will be the embedding matrix\nembeddings_matrix[0] = 0  # So that the padding will be ignored\n\n# Build the embedding matrix\nfor word, index in vocab2index.items():\n    if word in word2vec.vocab:\n        embeddings_matrix[index] = word2vec.word_vec(word)\ndel word2vec\n\nV = len(embeddings_matrix)\nprint(V)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, valid = train_test_split(train_df, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s, vocab2index=vocab2index, N=50, padding_start=True):\n    '''helper function to add paddings to sentence'''\n    enc = np.zeros(N, dtype=np.int32)\n    enc1 = np.array(s)\n    l = min(N, len(enc1))\n    if not padding_start:\n        enc[:l] = enc1[:l]\n    else:\n        enc[N-l:] = enc1[:l]\n    return enc, l","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuoraDataset(Dataset):\n    def __init__(self, df, is_train=True):\n        self.is_train = is_train\n        self.X1 = [encode_sentence(train) for train in df.question1]\n        self.X2 = [encode_sentence(train) for train in df.question2]\n        if self.is_train:\n            self.y = df.is_duplicate.values\n    \n    def __len__(self):\n        return len(self.X1)\n    \n    def __getitem__(self, idx):\n        if self.is_train:\n            x1 = self.X1[idx]\n            x2 = self.X2[idx]\n            return x1, x2, self.y[idx]\n        else:\n            x1 = self.X1[idx]\n            x2 = self.X2[idx]\n            return x1, x2\n\ntrain_ds = QuoraDataset(train, is_train=True)\nvalid_ds = QuoraDataset(valid, is_train=True)\ntest_ds = QuoraDataset(test_df, is_train=False)\n\nbatch_size = 3000\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\ntest_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTM Model and Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def exponent_neg_manhattan_distance(x1, x2):\n    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n    return torch.exp((-torch.sum(torch.abs(x1 - x2), dim=1)))\n    # the distance function here gives data in range [0, 1]\n    # use binary_cross_entropy as loss fucntion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, pre_weights):\n        super(LSTMModel,self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        if pre_weights is not None:\n            self.embeddings.weight.data.copy_(torch.from_numpy(pre_weights))\n            self.embeddings.weight.requires_grad = False ## freeze embeddings\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, x1, x2):\n        x1 = self.embeddings(x1)\n        x2 = self.embeddings(x2)\n        x1 = self.dropout(x1)\n        x2 = self.dropout(x2)\n        x1_lstm, (h1, ct) = self.lstm(x1)\n        x2_lstm, (h2, ct) = self.lstm(x2)\n        return exponent_neg_manhattan_distance(h1[-1], h2[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epocs(model, optimizer, train_dl, valid_dl, epochs=10):\n    for i in tqdm(range(epochs)):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x1, x2, y in train_dl:\n            x1 = x1[0].long().cuda()\n            x2 = x2[0].long().cuda()\n            y_pred = model(x1, x2).cpu()\n            optimizer.zero_grad()\n            loss = F.binary_cross_entropy(y_pred, y.float())\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc = val_metrics(model, valid_dl)\n        if i % 5 == 1:\n            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n\n\ndef val_metrics(model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for x1, x2, y in valid_dl:\n        x1 = x1[0].long().cuda()\n        x2 = x2[0].long().cuda()\n        y_hat = model(x1, x2).cpu()\n        loss = F.binary_cross_entropy(y_hat, y.float())\n        y_pred = y_hat > 0.5\n        correct += (y_pred.float() == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss/total, correct/total\n\n\ndef update_optimizer(optimizer, lr):\n    for i, param_group in enumerate(optimizer.param_groups):\n        param_group[\"lr\"] = lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LSTMModel(V, 300, 50, pre_weights=embeddings_matrix).cuda()\nparameters = filter(lambda p: p.requires_grad, model.parameters())\noptimizer = torch.optim.Adam(parameters, lr=0.01)\ntrain_epocs(model, optimizer, train_dl, valid_dl, epochs=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = []\nfor i, (x1, x2) in tqdm(enumerate(test_dl)):\n    x1 = x1[0].long().cuda()\n    x2 = x2[0].long().cuda()\n    pred = model(x1, x2).cpu().detach().numpy()\n    prediction.extend(pred)\n\nsubmission = pd.read_csv(\"../input/quora-question-pairs/sample_submission.csv.zip\")\nsubmission['is_duplicate'] = np.array(prediction)\nsubmission.to_csv('submission_lstm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}