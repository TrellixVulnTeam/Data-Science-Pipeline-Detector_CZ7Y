{"cells":[{"metadata":{},"cell_type":"markdown","source":"**This notebook is an assignment from the course: ['Natural Language Processing with Sequence Models'](https://www.coursera.org/learn/sequence-models-in-nlp)**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install trax\nimport trax\nfrom trax import layers as tl\nfrom trax.supervised import training\nfrom trax.fastmath import numpy as fastnp\nimport random as rnd\n\nrnd.seed(11)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!unzip '../input/quora-question-pairs/train.csv.zip'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('./train.csv',low_memory=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cut_df = int(len(df)*0.95)\n\ndf = df.sample(frac=1)\n\ntrain,test = df[:cut_df],df[cut_df:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train),len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.is_duplicate==1]\nlen(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1_train_words = np.array(train['question1'])\nQ2_train_words = np.array(train['question2'])\n\nQ1_test_words = np.array(test['question1'])\nQ2_test_words = np.array(test['question2'])\ny_test  = np.array(test['is_duplicate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_dir='gs://trax-ml/vocabs/'\nvocab_file='en_32k.subword'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sentence = 'It is nice to learn new things today!'\nQ1_train = list(map(list,list(trax.data.tokenize(iter(Q1_train_words),vocab_file=vocab_file))))\nQ2_train = list(map(list,list(trax.data.tokenize(iter(Q2_train_words),vocab_file=vocab_file))))\n\nQ1_test = list(map(list,list(trax.data.tokenize(iter(Q1_test_words),vocab_file=vocab_file))))\nQ2_test = list(map(list,list(trax.data.tokenize(iter(Q2_test_words),vocab_file=vocab_file))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cut_off = int(len(Q1_train)*.8)\ntrain_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\nval_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trax.data.text_encoder.PAD_ID","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_generator(Q1, Q2, batch_size, pad=0, shuffle=True):\n\n    input1 = []\n    input2 = []\n    idx = 0\n    len_q = len(Q1)\n    question_indexes = [*range(len_q)]\n    \n    if shuffle:\n        rnd.shuffle(question_indexes)\n    \n    while True:\n        if idx >= len_q:\n            idx = 0\n            if shuffle:\n                rnd.shuffle(question_indexes)\n#         print(len_q,idx)\n        q1 = Q1[question_indexes[idx]]\n        q2 = Q2[question_indexes[idx]]\n        \n        idx += 1\n        \n        input1.append(q1)\n        input2.append(q2)\n        \n        if len(input1) == batch_size:\n            max_len = len(max(max(input1,key=len),max(input2,key=len),key=len))\n            max_len =  2**int(np.ceil(np.log2(max_len)))\n#             print(max_len)\n            b1 = []\n            b2 = []\n            for q1, q2 in zip(input1, input2):  \n#                 print(q1.shape,q2.shape)\n                q1 = q1+[pad]*(max_len-len(q1))                \n                q2 = q2+[pad]*(max_len-len(q2))\n                \n                b1.append(q1)                \n                b2.append(q2)\n            yield np.array(b1), np.array(b2)\n\n            input1, input2 = [], [] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 2\nres1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))\nprint(\"First questions  : \",'\\n', res1, '\\n')\nprint(\"Second questions : \",'\\n', res2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Siamese(vocab_size=33000, d_model=128, mode='train'):\n\n    def normalize(x):  # normalizes the vectors to have L2 norm 1\n        return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n    \n    q_processor = tl.Serial(  # Processor will run on Q1 and Q2.\n        tl.Embedding(vocab_size,d_model), # Embedding layer\n        tl.LSTM(d_model), # LSTM layer\n        tl.Mean(axis=1), # Mean over columns\n        tl.Fn('Normalize', lambda x: normalize(x))  # Apply normalize function\n    )  # Returns one vector of shape [batch_size, d_model].\n        \n    # Run on Q1 and Q2 in parallel.\n    model = tl.Parallel(q_processor, q_processor)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Siamese();model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def TripletLossFn(v1, v2, margin=0.25):\n    \"\"\"Custom Loss function.\n\n    Args:\n        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q1.\n        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q2.\n        margin (float, optional): Desired margin. Defaults to 0.25.\n\n    Returns:\n        jax.interpreters.xla.DeviceArray: Triplet Loss.\n    \"\"\"\n    \n    scores = fastnp.dot(v1,v2.T) \n\n    batch_size = len(scores)\n    # use fastnp to grab all postive `diagonal` entries in `scores`\n    positive = fastnp.diagonal(scores)  # the positive ones (duplicates)\n\n    # multiply `fastnp.eye(batch_size)` with 2.0 and subtract it out of `scores`\n    negative_without_positive = scores-fastnp.eye(batch_size)*2\n\n    # take the row by row `max` of `negative_without_positive`. \n    closest_negative = negative_without_positive.max(axis=[1])\n\n    # subtract `fastnp.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`\n    negative_zero_on_duplicate = (1-fastnp.eye(batch_size))*scores\n    \n    # use `fastnp.sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)` \n    mean_negative = fastnp.sum(negative_zero_on_duplicate,axis=1)/(batch_size-1)\n    # compute `fastnp.maximum` among 0.0 and `A`\n    # A = subtract `positive` from `margin` and add `closest_negative` \n    triplet_loss1 = fastnp.maximum(margin-positive+closest_negative,0.0)\n    \n    # compute `fastnp.maximum` among 0.0 and `B`\n    # B = subtract `positive` from `margin` and add `mean_negative`\n    triplet_loss2 = fastnp.maximum(margin-positive+mean_negative,0)\n\n    # add the two losses together and take the `fastnp.mean` of it\n    triplet_loss = fastnp.mean(triplet_loss1+triplet_loss2)\n#     print(triplet_loss)\n    \n\n    \n    return triplet_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial\ndef TripletLoss(margin=0.25):\n    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n    return tl.Fn('TripletLoss', triplet_loss_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\ntrain_generator = data_generator(train_Q1, train_Q2, batch_size)\nval_generator = data_generator(val_Q1, val_Q2, batch_size)\n# print('train_Q1.shape ', train_Q1.shape)\n# print('val_Q1.shape   ', val_Q1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir='model/'):\n    \"\"\"Training the Siamese Model\n\n    Args:\n        Siamese (function): Function that returns the Siamese model.\n        TripletLoss (function): Function that defines the TripletLoss loss function.\n        lr_schedule (function): Trax multifactor schedule function.\n        train_generator (generator, optional): Training generator. Defaults to train_generator.\n        val_generator (generator, optional): Validation generator. Defaults to val_generator.\n        output_dir (str, optional): Path to save model to. Defaults to 'model/'.\n\n    Returns:\n        trax.supervised.training.Loop: Training loop for the model.\n    \"\"\"\n    output_dir = os.path.expanduser(output_dir)\n\n    train_task = training.TrainTask(\n        labeled_data=train_generator,       \n        loss_layer=TripletLoss(),         \n        optimizer=trax.optimizers.Adam(0.01),\n        lr_schedule=lr_schedule, \n    )\n\n    eval_task = training.EvalTask(\n        labeled_data=val_generator,      \n        metrics=[TripletLoss()],          \n    )\n    \n    training_loop = training.Loop(Siamese(),\n                                  train_task,\n                                  eval_tasks=eval_task,\n                                  output_dir=output_dir)\n\n    return training_loop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_steps = 1000\ntraining_loop = train_model(Siamese, TripletLoss, lr_schedule)\ntraining_loop.run(n_steps = train_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classify(test_Q1, test_Q2, y, threshold, model, data_generator=data_generator, batch_size=64):\n    \"\"\"Function to test the accuracy of the model.\n\n    Args:\n        test_Q1 (numpy.ndarray): Array of Q1 questions.\n        test_Q2 (numpy.ndarray): Array of Q2 questions.\n        y (numpy.ndarray): Array of actual target.\n        threshold (float): Desired threshold.\n        model (trax.layers.combinators.Parallel): The Siamese model.\n        vocab (collections.defaultdict): The vocabulary used.\n        data_generator (function): Data generator function. Defaults to data_generator.\n        batch_size (int, optional): Size of the batches. Defaults to 64.\n\n    Returns:\n        float: Accuracy of the model.\n    \"\"\"\n    accuracy = 0\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    for i in range(0, len(test_Q1), batch_size):\n        # Call the data generator (built in Ex 01) with shuffle=False using next()\n        # use batch size chuncks of questions as Q1 & Q2 arguments of the data generator. e.g x[i:i + batch_size]\n        # Hint: use `vocab['<PAD>']` for the `pad` argument of the data generator\n        q1, q2 = next(data_generator(test_Q1[i:i+batch_size],test_Q2[i:i+batch_size],\n                                     batch_size,shuffle=False))\n        # use batch size chuncks of actual output targets (same syntax as example above)\n        \n        y_test = y[i:i+batch_size]\n#         print(y_test.shape)\n        # Call the model\n        v1, v2 = model([q1,q2])\n#         print(v1,v2)\n#         print(i)\n        for j in range(batch_size):\n            # take dot product to compute cos similarity of each pair of entries, v1[j], v2[j]\n            # don't forget to transpose the second argument\n            d = fastnp.dot(v1[j],v2[j].T)\n            # is d greater than the threshold?\n            res = d>threshold\n#             print(j,res)\n            # increment accurancy if y_test is equal `res`\n            accuracy += float(y_test[j]==res)\n    # compute accuracy using accuracy and total length of test questions\n    accuracy = accuracy/len(test_Q1)\n    ### END CODE HERE ###\n    \n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1_test = Q1_test[:512*39]\nQ2_test = Q2_test[:512*39]\ny_test = y_test[:512*39].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Siamese()\nmodel.init_from_file('./model/model.pkl.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = classify(Q1_test,Q2_test, y_test, 0.7, model, batch_size = 512)\nprint(\"Accuracy\", accuracy)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}