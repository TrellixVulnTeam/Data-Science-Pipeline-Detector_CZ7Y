{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a7e4fe69-5bec-f07e-0958-81d3b701e9ae"},"source":"Get data in, clean up, simple distance measure. Note: Neural Nets would likely be a good solution."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94c90fd8-1f29-c734-71b8-7a33d87ea51f"},"outputs":[],"source":"\nimport numpy as np\nimport pandas as pd \nimport warnings\nimport re\nimport matplotlib.pyplot as plt\nimport random\n\nfrom Levenshtein import *\nfrom nltk.corpus import *\nfrom nltk import ChartParser\nfrom subprocess import check_output\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nwarnings.filterwarnings('ignore')\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a769f07-aa90-2329-b171-ce4f545a39ce"},"source":"Read input"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d021cbc8-69ce-c06f-d2d5-08758cedb41b"},"outputs":[],"source":"test = pd.read_csv('../input/test.csv', encoding='ISO-8859-1').fillna(\"\")\ntrain = pd.read_csv('../input/train.csv', encoding ='ISO-8859-1').fillna(\"\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"15592c91-1321-02d6-ffd3-9e7900fa2d55"},"source":"For testing, cut down size of test and train."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9402ec73-82f7-9242-85b2-3085f605cac8"},"outputs":[],"source":"\nunique_ids1 = train['id'].unique()\nrand_ids1 = [unique_ids1[i] for i in sorted(random.sample(range(len(unique_ids1)), 1000)) ]\ntrain = train[train.id.isin(rand_ids1)]\n\ntrain.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a3aea4e-2027-befa-7300-9fb4e6835de7"},"outputs":[],"source":"unique_ids2 = test['test_id'].unique()\nrand_ids2 = [unique_ids2[i] for i in sorted(random.sample(range(len(unique_ids2)), 1000)) ]\ntest = test[test.test_id.isin(rand_ids2)]\n\ntest.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"228209b2-5c9c-5e24-98c0-b8ad1f146be1"},"source":"Get stopwords, to remove."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fba178b-7902-4d9f-35e5-10e89a491f08"},"outputs":[],"source":"\nstopwords = stopwords.words('english')\nprint(stopwords)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4f212af6-96b2-c109-f672-b7bca25ead3c"},"source":"Check test and train now."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2e873b4-e52d-5666-9da0-70d068e99a12"},"outputs":[],"source":"test.info()\n\ntrain.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1acb9373-a69c-1bb6-7108-2f3294e77404"},"outputs":[],"source":"test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3efd78dc-691b-b8d2-0236-446b45d3f3e9"},"outputs":[],"source":"train.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"cd092eb7-1e27-2a79-3f0f-12200342246d"},"source":"Plot for duplicates."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80ff991d-f902-d1f6-f05f-8843dc9f89de"},"outputs":[],"source":"train.groupby(\"is_duplicate\")['id'].count().plot.bar()"},{"cell_type":"markdown","metadata":{"_cell_guid":"80bda22f-5d2c-6acf-d606-21e18fea1910"},"source":"Remove all special characters, split by spaces, and remove stopword entries in list for train and test."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f042fa95-f1f1-e0ee-f617-666c6d2a80eb"},"outputs":[],"source":"train = train.dropna()              \ntrain['question1'] = train['question1'].apply(lambda x: x.rstrip('?'))\ntrain['question2'] = train['question2'].apply(lambda x: x.rstrip('?'))\ntrain['question1'] = train['question1'].str.lower().str.split(' ')\ntrain['question2'] = train['question2'].str.lower().str.split(' ')\ntrain['question1'] = train['question1'].apply(lambda x: [item for item in x if item not in stopwords])\ntrain['question2'] = train['question2'].apply(lambda x: [item for item in x if item not in stopwords])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d06667e-87da-55fd-770b-4142742c5ef0"},"outputs":[],"source":"#test = test.dropna()     \ntest['question1'] = test['question1'].apply(lambda x: x.rstrip('?'))\ntest['question2'] = test['question2'].apply(lambda x: x.rstrip('?'))\ntest['question1'] = test['question1'].str.lower().str.split(' ')\ntest['question2'] = test['question2'].str.lower().str.split(' ')\ntest['question1'] = test['question1'].apply(lambda x: [item for item in x if item not in stopwords])\ntest['question2'] = test['question2'].apply(lambda x: [item for item in x if item not in stopwords])"},{"cell_type":"markdown","metadata":{"_cell_guid":"600d4286-1d6e-2672-839c-ad44b874dc79"},"source":"Get features, len, common word count, percentage of common words, seqratio similarity."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b2d19cc-f191-c709-ee79-3220c614d399"},"outputs":[],"source":"train['Common'] = train.apply(lambda row: len(list(set(row['question1']).intersection(row['question2']))), axis=1)\ntest['Common'] = test.apply(lambda row: len(list(set(row['question1']).intersection(row['question2']))), axis=1)\n\ntrain['Average'] = train.apply(lambda row: 0.5*(len(row['question1'])+len(row['question2'])), axis=1)\ntest['Average'] = test.apply(lambda row: 0.5*(len(row['question1'])+len(row['question2'])), axis=1)\n\ntrain['Percentage'] = train.apply(lambda row: row['Common'] * 100.0 / (row['Average'] + 1), axis=1)\ntest['Percentage'] = test.apply(lambda row: 1 if row['Average'] == 0 else row['Common']/(row['Average']), axis=1)\n\ntrain['seqratio'] = train.apply(lambda row: seqratio(row['question1'], row['question2']) * 100.0, axis=1)\ntest['seqratio'] = test.apply(lambda row: seqratio(row['question1'], row['question2']) * 100.0, axis=1)\n\ntrain['avg2'] = train.apply(lambda row: (row['seqratio'] + row['Percentage']) / 200.0, axis=1)\ntest['avg2'] = test.apply(lambda row: (row['seqratio'] + row['Percentage']) / 200.0, axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1aee1ae-0b4d-5b4d-33c9-c894506d1afe"},"outputs":[],"source":"train.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1e57ac7c-db25-ea1a-d887-5cea8dad77ea"},"source":"Check correlations."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1785826c-df63-a0d9-1def-69bdb6341ee8"},"outputs":[],"source":"train.corr()['is_duplicate']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed2acbe7-9df1-0dbd-04cf-bbbb4cb0b70d"},"outputs":[],"source":"\n\n# Check these for later\n#dfTrain = pd.DataFrame({'test_id' : range(0,2345796)})\n#dfTest = pd.DataFrame({'test_id' : range(0,2345796)})\n#dfTrain[['Common', 'Average', 'Percentage', 'seqratio', 'avg2']] = scaler.fit_transform(train[['Common', 'Average', 'Percentage', 'seqratio', 'avg2']])\n#dfTest[['Common', 'Average', 'Percentage', 'seqratio', 'avg2']] = scaler.fit_transform(test[['Common', 'Average', 'Percentage', 'seqratio', 'avg2']])\n\n#df = pd.DataFrame({'test_id' : range(0,2345796)})\n#df.fillna(0, inplace = True)\n#df['is_duplicate'] = pd.Series(test['avg2'])\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler().fit(train[['Common', 'Average', 'Percentage', 'seqratio', 'avg2']])\n\nX = scaler.transform(train[['Common', 'Average', 'Percentage', 'seqratio', 'avg2']])\ny = train['is_duplicate']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=13)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\nclf = LogisticRegression()\ngrid = {\n    'C': [1e-6, 1e-3, 1e0],\n    'penalty': ['l1', 'l2']\n}\ncv = GridSearchCV(clf, grid, scoring='neg_log_loss', n_jobs=-1, verbose=1)\ncv.fit(X_train, y_train)\n\nretrained = cv.best_estimator_.fit(X, y)\n\nscaler = MinMaxScaler().fit(test[['Common', 'Average', 'Percentage', 'seqratio', 'avg2']])\n\nX_submission = scaler.transform(test[['Common', 'Average', 'Percentage', 'seqratio', 'avg2']])\ny_submission = retrained.predict_proba(X_submission)[:,1]\n\ndftest = pd.read_csv(\"../input/test.csv\").fillna(\"\")\nsubmission = pd.DataFrame({'test_id': dftest['test_id'], 'is_duplicate': y_submission})\nsubmission.head()\n\nprint(submission.shape)\nsubmission.to_csv('submit.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36aa7177-a8fd-bcde-54ea-3f893ded1680"},"outputs":[],"source":"submission.head(10)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}