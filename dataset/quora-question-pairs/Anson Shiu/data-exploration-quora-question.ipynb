{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, ngrams\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import dataset\nq_quora = pd.read_csv(\"../input/first-quora-dataset/q_quora.csv\")\nq_quora = q_quora[['id','qid1','qid2','question1','question2','is_duplicate']]\nq_quora.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data columns\n* **id:** id for a training set question pair\n* **qid1,qid2:** unique ids of each question\n* **question1, question2:** the full text of the question\n* **is_duplicate:** target label: 1: two questions have the same meaning, 0: t**wo questions do not have the same meaning"},{"metadata":{},"cell_type":"markdown","source":"## Data statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of rows and columns\nq_quora.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label distribution\nq_quora.is_duplicate.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data types\nq_quora.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"q_quora_clean = q_quora[(q_quora['is_duplicate'] == \"0\") | (q_quora['is_duplicate'] == \"1\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label distribution\n* In total, there is around 63% of non-duplicate questions and 37% o duplicate questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_distri = q_quora_clean.is_duplicate.value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(label_distri.index, label_distri.values, alpha=0.8)\nplt.title(\"The distribution of Label\")\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Is Duplicate', fontsize=12)\nplt.show()\n\nlabel_distri / label_distri.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explorations"},{"metadata":{},"cell_type":"markdown","source":"### 1) word counts distribution\n* Most questions have around 6 - 13 words\n* Right Skewed data: max: 237 words, min: 1-2 words\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all_questions = pd.DataFrame(pd.concat([q_quora_clean['question1'], q_quora_clean['question2']]))\ndf_all_questions.columns = ['questions']\ndf_all_questions = df_all_questions.reset_index(drop=True)\n# word count\ndf_all_questions['word_counts'] = df_all_questions['questions'].apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count_distri = df_all_questions['word_counts'].value_counts()\n\nplt.figure(figsize=(32,18))\nsns.barplot(word_count_distri.index, word_count_distri.values, alpha=0.8)\nplt.title('Distribution of word counts')\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('word counts in the question', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2) character counts distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all_questions['character_counts'] = df_all_questions['questions'].apply(lambda x: len(str(x)))\ncharac_counts_dist = df_all_questions['character_counts'].value_counts()\n\nplt.figure(figsize=(40,10))\nsns.barplot(charac_counts_dist.index, charac_counts_dist.values, alpha=0.8)\nplt.title('Distribution of character counts')\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('character counts in the question', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 3) Common words from question 1 and question 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# common english stop words\neng_stopwords = set(stopwords.words('english'))\n\ndef get_unigrams(question):\n    return [word for word in word_tokenize(question.lower()) if word not in eng_stopwords]\n\ndef get_common_unigrams(row):\n    return len(set(row['unigram_ques1']).intersection(set(row['unigram_ques2'])))\n\ndef get_common_unigram_ratio(row):\n    return row[\"unigrams_common_count\"] / max(len(set('unigrams_ques1').union(set('unigrams_ques2'))),1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Unigram"},{"metadata":{"trusted":true},"cell_type":"code","source":"q_quora_clean['unigram_ques1'] = q_quora_clean['question1'].apply(lambda x: get_unigrams(str(x)))\nq_quora_clean['unigram_ques2'] = q_quora_clean['question2'].apply(lambda x: get_unigrams(str(x)))\nq_quora_clean['unigrams_common_count'] = q_quora_clean.apply(lambda row: get_common_unigrams(row),axis=1)\nq_quora_clean['unigrams_common_ratio'] = q_quora_clean.apply(lambda row: get_common_unigram_ratio(row), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unigrams_count = q_quora_clean['unigrams_common_count'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(unigrams_count.index, unigrams_count.values, alpha=0.8)\nplt.title('Distribution of Unigrams Count')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Common unigrams count', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are only a few question pairs with no common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.violinplot(x=\"is_duplicate\", y=\"unigrams_common_count\", data=q_quora_clean, palette=\"muted\")\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common unigrams count', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.violinplot(x=\"is_duplicate\", y=\"unigrams_common_ratio\", data=q_quora_clean)\nplt.ylim(0,1)\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common unigrams ratio', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4) Normalized word share count"},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip(), str(row['question1']).split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), str(row['question2']).split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n\nq_quora_clean['q1len'] = q_quora_clean['question1'].str.len()\nq_quora_clean['q2len'] = q_quora_clean['question2'].str.len()\n\nq_quora_clean['q1_n_words'] = q_quora_clean['question1'].apply(lambda row: len(str(row).split(\" \")))\nq_quora_clean['q2_n_words'] = q_quora_clean['question2'].apply(lambda row: len(str(row).split(\" \")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip(), str(row['question1']).split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), str(row['question2']).split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n\nq_quora_clean['word_share'] = q_quora_clean.apply(normalized_word_share, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_share', data = q_quora_clean[0:50000])\nplt.subplot(1,2,2)\nsns.distplot(q_quora_clean[q_quora_clean['is_duplicate'] == '1']['word_share'][0:10000], color = 'green')\nsns.distplot(q_quora_clean[q_quora_clean['is_duplicate'] == '0']['word_share'][0:10000], color = 'orange')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graphs above, the distributions for normalized word share have some overlap on the far right hand side. It means there are quite a lot of questions with high word similarity but belong to both duplicates and non-duplicates."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}