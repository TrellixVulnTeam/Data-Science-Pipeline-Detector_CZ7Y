{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Problem Statement\nMultiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question.","metadata":{}},{"cell_type":"markdown","source":"### Dataset Description\n1. id - the id of a training set question pair\n2. qid1, qid2 - unique ids of each question (only available in train.csv)\n3. question1, question2 - the full text of each question\n4. is_duplicate - the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise.","metadata":{}},{"cell_type":"code","source":"import zipfile\n\nz= zipfile.ZipFile('../input/quora-question-pairs/train.csv.zip')\nz.extractall()\nz= zipfile.ZipFile('../input/quora-question-pairs/test.csv.zip')\nz.extractall()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:38.738118Z","iopub.execute_input":"2022-06-19T15:50:38.73844Z","iopub.status.idle":"2022-06-19T15:50:46.262386Z","shell.execute_reply.started":"2022-06-19T15:50:38.738404Z","shell.execute_reply":"2022-06-19T15:50:46.261766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom fuzzywuzzy import fuzz\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings as wg\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import plot_confusion_matrix, log_loss\n\nfrom nltk.stem import WordNetLemmatizer\nimport spacy\nfrom tqdm import tqdm\n\nimport vaex\nfrom vaex.ml.sklearn import IncrementalPredictor\nfrom collections import Counter, defaultdict\nwg.filterwarnings(\"ignore\")\n\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:27:13.754125Z","iopub.execute_input":"2022-06-19T16:27:13.755215Z","iopub.status.idle":"2022-06-19T16:27:27.254417Z","shell.execute_reply.started":"2022-06-19T16:27:13.755157Z","shell.execute_reply":"2022-06-19T16:27:27.2535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"./train.csv\")\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:50.336122Z","iopub.execute_input":"2022-06-19T15:50:50.33641Z","iopub.status.idle":"2022-06-19T15:50:51.689993Z","shell.execute_reply.started":"2022-06-19T15:50:50.33638Z","shell.execute_reply":"2022-06-19T15:50:51.68909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df[~df['Unnamed: 8'].isna()]","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:53.496319Z","iopub.execute_input":"2022-06-19T15:50:53.496592Z","iopub.status.idle":"2022-06-19T15:50:53.500165Z","shell.execute_reply.started":"2022-06-19T15:50:53.496562Z","shell.execute_reply":"2022-06-19T15:50:53.499523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df.drop([\"Unnamed: 6\", \"Unnamed: 7\", \"Unnamed: 8\", \"Unnamed: 9\", \"Unnamed: 10\", \"Unnamed: 11\", \n#          \"Unnamed: 12\"], axis = 'columns')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:54.404895Z","iopub.execute_input":"2022-06-19T15:50:54.405399Z","iopub.status.idle":"2022-06-19T15:50:54.408468Z","shell.execute_reply.started":"2022-06-19T15:50:54.405336Z","shell.execute_reply":"2022-06-19T15:50:54.407842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:55.028607Z","iopub.execute_input":"2022-06-19T15:50:55.028955Z","iopub.status.idle":"2022-06-19T15:50:55.040282Z","shell.execute_reply.started":"2022-06-19T15:50:55.028919Z","shell.execute_reply":"2022-06-19T15:50:55.039444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:55.522686Z","iopub.execute_input":"2022-06-19T15:50:55.523401Z","iopub.status.idle":"2022-06-19T15:50:55.529447Z","shell.execute_reply.started":"2022-06-19T15:50:55.523348Z","shell.execute_reply":"2022-06-19T15:50:55.528782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:55.797301Z","iopub.execute_input":"2022-06-19T15:50:55.797822Z","iopub.status.idle":"2022-06-19T15:50:56.250742Z","shell.execute_reply.started":"2022-06-19T15:50:55.797787Z","shell.execute_reply":"2022-06-19T15:50:56.249925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:56.252243Z","iopub.execute_input":"2022-06-19T15:50:56.252897Z","iopub.status.idle":"2022-06-19T15:50:56.372899Z","shell.execute_reply.started":"2022-06-19T15:50:56.252856Z","shell.execute_reply":"2022-06-19T15:50:56.371982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:56.397942Z","iopub.execute_input":"2022-06-19T15:50:56.398227Z","iopub.status.idle":"2022-06-19T15:50:56.518456Z","shell.execute_reply.started":"2022-06-19T15:50:56.398193Z","shell.execute_reply":"2022-06-19T15:50:56.517424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"is_duplicate\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:56.70228Z","iopub.execute_input":"2022-06-19T15:50:56.702559Z","iopub.status.idle":"2022-06-19T15:50:56.711906Z","shell.execute_reply.started":"2022-06-19T15:50:56.702529Z","shell.execute_reply":"2022-06-19T15:50:56.711069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[75, \"question1\"], df.loc[75, \"question2\"], df.loc[75, \"is_duplicate\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:56.941445Z","iopub.execute_input":"2022-06-19T15:50:56.942333Z","iopub.status.idle":"2022-06-19T15:50:56.963184Z","shell.execute_reply.started":"2022-06-19T15:50:56.942276Z","shell.execute_reply":"2022-06-19T15:50:56.962622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df[\"is_duplicate\"].value_counts().plot(kind = \"bar\")","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:57.44701Z","iopub.execute_input":"2022-06-19T15:50:57.447635Z","iopub.status.idle":"2022-06-19T15:50:57.561145Z","shell.execute_reply.started":"2022-06-19T15:50:57.447594Z","shell.execute_reply":"2022-06-19T15:50:57.560264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:57.790726Z","iopub.execute_input":"2022-06-19T15:50:57.791352Z","iopub.status.idle":"2022-06-19T15:50:57.79946Z","shell.execute_reply.started":"2022-06-19T15:50:57.791306Z","shell.execute_reply":"2022-06-19T15:50:57.798487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:58.120292Z","iopub.execute_input":"2022-06-19T15:50:58.121113Z","iopub.status.idle":"2022-06-19T15:50:58.517984Z","shell.execute_reply.started":"2022-06-19T15:50:58.121067Z","shell.execute_reply":"2022-06-19T15:50:58.517053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text1 = \" \".join(review for review in df.question1.astype(str))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:58.519807Z","iopub.execute_input":"2022-06-19T15:50:58.520393Z","iopub.status.idle":"2022-06-19T15:50:58.761564Z","shell.execute_reply.started":"2022-06-19T15:50:58.520349Z","shell.execute_reply":"2022-06-19T15:50:58.760721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"There are {} words in the combination of all cells in column 'question1'.\".format(len(text1)))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:58.807454Z","iopub.execute_input":"2022-06-19T15:50:58.808249Z","iopub.status.idle":"2022-06-19T15:50:58.813208Z","shell.execute_reply.started":"2022-06-19T15:50:58.808208Z","shell.execute_reply":"2022-06-19T15:50:58.812403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(STOPWORDS)\nwordcloud1 = WordCloud(stopwords=stop_words, background_color=\"black\", width=800, height=400).generate(text1)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:50:59.322887Z","iopub.execute_input":"2022-06-19T15:50:59.323739Z","iopub.status.idle":"2022-06-19T15:51:16.25322Z","shell.execute_reply.started":"2022-06-19T15:50:59.323668Z","shell.execute_reply":"2022-06-19T15:51:16.252288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Common words in question1 column**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(40,20))\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud1, interpolation='bilinear')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:51:16.254638Z","iopub.execute_input":"2022-06-19T15:51:16.25488Z","iopub.status.idle":"2022-06-19T15:51:17.372421Z","shell.execute_reply.started":"2022-06-19T15:51:16.254845Z","shell.execute_reply":"2022-06-19T15:51:17.371405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text2 = \" \".join(review for review in df.question2.astype(str))\nprint (\"There are {} words in the combination of all cells in column 'question1'.\".format(len(text2)))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:51:17.374012Z","iopub.execute_input":"2022-06-19T15:51:17.374335Z","iopub.status.idle":"2022-06-19T15:51:17.578003Z","shell.execute_reply.started":"2022-06-19T15:51:17.374297Z","shell.execute_reply":"2022-06-19T15:51:17.577146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud2 = WordCloud(stopwords=stop_words, background_color=\"black\", width=800, height=400).generate(text2)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:51:17.579959Z","iopub.execute_input":"2022-06-19T15:51:17.580207Z","iopub.status.idle":"2022-06-19T15:51:34.258042Z","shell.execute_reply.started":"2022-06-19T15:51:17.580165Z","shell.execute_reply":"2022-06-19T15:51:34.257238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://)**Common words in question2 column**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(40,20))\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:00.042894Z","iopub.execute_input":"2022-06-19T15:52:00.043215Z","iopub.status.idle":"2022-06-19T15:52:01.180246Z","shell.execute_reply.started":"2022-06-19T15:52:00.043181Z","shell.execute_reply":"2022-06-19T15:52:01.179435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:01.181688Z","iopub.execute_input":"2022-06-19T15:52:01.182068Z","iopub.status.idle":"2022-06-19T15:52:01.193109Z","shell.execute_reply.started":"2022-06-19T15:52:01.182035Z","shell.execute_reply":"2022-06-19T15:52:01.192249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"question1\"] = df[\"question1\"].replace(\" ?\",\"?\")\ndf[\"question2\"] = df[\"question2\"].replace(\" ?\",\"?\")","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:10.99674Z","iopub.execute_input":"2022-06-19T15:52:10.997043Z","iopub.status.idle":"2022-06-19T15:52:11.10847Z","shell.execute_reply.started":"2022-06-19T15:52:10.997006Z","shell.execute_reply":"2022-06-19T15:52:11.107559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_string(string):\n    return str(string)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:11.497822Z","iopub.execute_input":"2022-06-19T15:52:11.498111Z","iopub.status.idle":"2022-06-19T15:52:11.502516Z","shell.execute_reply.started":"2022-06-19T15:52:11.498078Z","shell.execute_reply":"2022-06-19T15:52:11.501889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"question1\"] = df[\"question1\"].apply(to_string)\ndf[\"question2\"] = df[\"question2\"].apply(to_string)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:12.105036Z","iopub.execute_input":"2022-06-19T15:52:12.105647Z","iopub.status.idle":"2022-06-19T15:52:12.49534Z","shell.execute_reply.started":"2022-06-19T15:52:12.105611Z","shell.execute_reply":"2022-06-19T15:52:12.494436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def string_length(string):\n    return len(string)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:13.043023Z","iopub.execute_input":"2022-06-19T15:52:13.043319Z","iopub.status.idle":"2022-06-19T15:52:13.047597Z","shell.execute_reply.started":"2022-06-19T15:52:13.043287Z","shell.execute_reply":"2022-06-19T15:52:13.046756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of characters in each string\ndf[\"char_count1\"] = df[\"question1\"].apply(string_length)\ndf[\"char_count2\"] = df[\"question2\"].apply(string_length)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:13.997955Z","iopub.execute_input":"2022-06-19T15:52:13.998936Z","iopub.status.idle":"2022-06-19T15:52:14.525678Z","shell.execute_reply.started":"2022-06-19T15:52:13.998884Z","shell.execute_reply":"2022-06-19T15:52:14.525028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def num_of_words(string):\n    return len(string.split())","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:14.526855Z","iopub.execute_input":"2022-06-19T15:52:14.52757Z","iopub.status.idle":"2022-06-19T15:52:14.531075Z","shell.execute_reply.started":"2022-06-19T15:52:14.527537Z","shell.execute_reply":"2022-06-19T15:52:14.530341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of words in each question\ndf[\"word_count1\"] = df[\"question1\"].apply(num_of_words)\ndf[\"word_count2\"] = df[\"question2\"].apply(num_of_words)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:15.205615Z","iopub.execute_input":"2022-06-19T15:52:15.205933Z","iopub.status.idle":"2022-06-19T15:52:16.326557Z","shell.execute_reply.started":"2022-06-19T15:52:15.205899Z","shell.execute_reply":"2022-06-19T15:52:16.325923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def common_words(string1, string2):\n    s1 = set(string1.split())\n    s2 = set(string2.split())\n    return len(s1.intersection(s2))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:16.327923Z","iopub.execute_input":"2022-06-19T15:52:16.328173Z","iopub.status.idle":"2022-06-19T15:52:16.332549Z","shell.execute_reply.started":"2022-06-19T15:52:16.328139Z","shell.execute_reply":"2022-06-19T15:52:16.331993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of common words in question1 and question2\ndf[\"words_common\"] = df.apply(lambda x: common_words(x.question1, x.question2), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:16.352221Z","iopub.execute_input":"2022-06-19T15:52:16.352839Z","iopub.status.idle":"2022-06-19T15:52:30.496476Z","shell.execute_reply.started":"2022-06-19T15:52:16.352801Z","shell.execute_reply":"2022-06-19T15:52:30.495626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total number of words in question1 and question2\ndf[\"words_total\"] = df[\"word_count1\"] + df[\"word_count2\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:30.498029Z","iopub.execute_input":"2022-06-19T15:52:30.498274Z","iopub.status.idle":"2022-06-19T15:52:30.50497Z","shell.execute_reply.started":"2022-06-19T15:52:30.498243Z","shell.execute_reply":"2022-06-19T15:52:30.503981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shared words ratio:- common_words/total_words\ndf[\"shared_words_ratio\"] = df[\"words_common\"] / df[\"words_total\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:30.506563Z","iopub.execute_input":"2022-06-19T15:52:30.507058Z","iopub.status.idle":"2022-06-19T15:52:30.51986Z","shell.execute_reply.started":"2022-06-19T15:52:30.507012Z","shell.execute_reply":"2022-06-19T15:52:30.518959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def first_word_eq(string1, string2):\n    q1 = string1.split()\n    q2 = string2.split()\n    \n    if q1[0] == q2[0]:\n        return 1\n    return 0\n\ndef last_word_eq(string1, string2):\n    q1 = string1.split()\n    q2 = string2.split()\n    \n    if q1[-1] == q2[-1]:\n        return 1\n    return 0","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:38.99129Z","iopub.execute_input":"2022-06-19T15:52:38.991972Z","iopub.status.idle":"2022-06-19T15:52:38.997628Z","shell.execute_reply.started":"2022-06-19T15:52:38.991924Z","shell.execute_reply":"2022-06-19T15:52:38.996872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1 if first word is same else 0\ndf[\"first_word_eq\"] = df.apply(lambda x: first_word_eq(x.question1, x.question2), axis=1)\n# 1 if last word is same else 0\ndf[\"last_word_eq\"] = df.apply(lambda x: last_word_eq(x.question1, x.question2), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:52:42.858817Z","iopub.execute_input":"2022-06-19T15:52:42.859412Z","iopub.status.idle":"2022-06-19T15:53:07.404391Z","shell.execute_reply.started":"2022-06-19T15:52:42.859366Z","shell.execute_reply":"2022-06-19T15:53:07.403444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Caclulating string similarity between question1 and question2 using fuzzywuzzy**<br>\n - ratio\n - partial_ratio\n - token_sort_ratio\n - token_set_ratio\n \nNot a single ratio is perfect that's why all of them are used one by one<br>\n[FuzzyWuzzy: Fuzzy String Matching in Python](https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/)","metadata":{}},{"cell_type":"code","source":"df[\"question1\"][1], df[\"question2\"][1]","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:53:11.123955Z","iopub.execute_input":"2022-06-19T15:53:11.124269Z","iopub.status.idle":"2022-06-19T15:53:11.129798Z","shell.execute_reply.started":"2022-06-19T15:53:11.124225Z","shell.execute_reply":"2022-06-19T15:53:11.129226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Taking simple ratio\", fuzz.ratio(df[\"question1\"][1], df[\"question2\"][1]))\nprint(\"Taking partial ratio\", fuzz.partial_ratio(df[\"question1\"][1], df[\"question2\"][1]))\nprint(\"Taking token sort ratio\", fuzz.token_sort_ratio(df[\"question1\"][1], df[\"question2\"][1]))\nprint(\"Taking token set ratio\", fuzz.token_set_ratio(df[\"question1\"][1], df[\"question2\"][1]))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:53:11.678761Z","iopub.execute_input":"2022-06-19T15:53:11.67903Z","iopub.status.idle":"2022-06-19T15:53:11.688849Z","shell.execute_reply.started":"2022-06-19T15:53:11.679003Z","shell.execute_reply":"2022-06-19T15:53:11.687884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fuzz_ratio(string1, string2):\n    return fuzz.ratio(string1, string2)\n\ndef fuzz_partial_ratio(string1, string2):\n    return fuzz.partial_ratio(string1, string2)\n\ndef fuzz_token_sort_ratio(string1, string2):\n    return fuzz.token_sort_ratio(string1, string2)\n\ndef fuzz_token_set_ratio(string1, string2):\n    return fuzz.token_set_ratio(string1, string2)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:53:12.410763Z","iopub.execute_input":"2022-06-19T15:53:12.412808Z","iopub.status.idle":"2022-06-19T15:53:12.426809Z","shell.execute_reply.started":"2022-06-19T15:53:12.412329Z","shell.execute_reply":"2022-06-19T15:53:12.423686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"fuzz_ratio\"] = df.apply(lambda x: fuzz_ratio(x.question1, x.question2), axis=1)\ndf[\"fuzz_partial_ratio\"] = df.apply(lambda x: fuzz_partial_ratio(x.question1, x.question2), axis=1)\ndf[\"fuzz_token_sort_ratio\"] = df.apply(lambda x: fuzz_token_sort_ratio(x.question1, x.question2), axis=1)\ndf[\"fuzz_token_set_ratio\"] = df.apply(lambda x: fuzz_token_set_ratio(x.question1, x.question2), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:53:13.29969Z","iopub.execute_input":"2022-06-19T15:53:13.300217Z","iopub.status.idle":"2022-06-19T15:55:31.300331Z","shell.execute_reply.started":"2022-06-19T15:53:13.300167Z","shell.execute_reply":"2022-06-19T15:55:31.299285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average number of words in the two questions\ndf[\"avg_words\"] = (df[\"word_count1\"] + df[\"word_count2\"])/2\n\n\n# difference in the number of words in the two strings\ndf[\"word_diff\"] = np.abs(df[\"word_count1\"] - df[\"word_count2\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:55:31.302289Z","iopub.execute_input":"2022-06-19T15:55:31.30262Z","iopub.status.idle":"2022-06-19T15:55:31.31503Z","shell.execute_reply.started":"2022-06-19T15:55:31.302574Z","shell.execute_reply":"2022-06-19T15:55:31.314057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:55:31.316373Z","iopub.execute_input":"2022-06-19T15:55:31.316619Z","iopub.status.idle":"2022-06-19T15:55:31.338897Z","shell.execute_reply.started":"2022-06-19T15:55:31.316591Z","shell.execute_reply":"2022-06-19T15:55:31.337889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking how much the new features are able to distinguish the duplicate and non duplicate questions**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df)\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'] , label = \"0\" , color = 'blue' )\n\nplt.suptitle(\"fuzz_ratio distribution for duplicate and non duplicate\", fontsize = 16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:55:31.341215Z","iopub.execute_input":"2022-06-19T15:55:31.341484Z","iopub.status.idle":"2022-06-19T15:55:34.477651Z","shell.execute_reply.started":"2022-06-19T15:55:31.341448Z","shell.execute_reply":"2022-06-19T15:55:34.47676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'shared_words_ratio', data = df)\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['shared_words_ratio'] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['shared_words_ratio'] , label = \"0\" , color = 'blue' )\n\nplt.suptitle(\"shared_words_ratio distribution for duplicate and non duplicate\", fontsize = 16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:55:34.479184Z","iopub.execute_input":"2022-06-19T15:55:34.479399Z","iopub.status.idle":"2022-06-19T15:55:37.471529Z","shell.execute_reply.started":"2022-06-19T15:55:34.479372Z","shell.execute_reply":"2022-06-19T15:55:37.470812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_diff', data = df)\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_diff'] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_diff'] , label = \"0\" , color = 'blue' )\n\nplt.suptitle(\"word_diff distribution for duplicate and non duplicate\", fontsize = 16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:55:37.472876Z","iopub.execute_input":"2022-06-19T15:55:37.473293Z","iopub.status.idle":"2022-06-19T15:55:40.393488Z","shell.execute_reply.started":"2022-06-19T15:55:37.473245Z","shell.execute_reply":"2022-06-19T15:55:40.392664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Preprocessing Techniques\n\n* Expand Contractions\n* Lower Case\n* Remove Punctuations\n* Remove Stopwords\n* Stemming and Lemmatization\n* Remove White spaces","metadata":{}},{"cell_type":"code","source":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how does\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\" u \": \" you \",\n\" ur \": \" your \",\n\" n \": \" and \"}","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:55:40.394862Z","iopub.execute_input":"2022-06-19T15:55:40.395082Z","iopub.status.idle":"2022-06-19T15:55:40.408732Z","shell.execute_reply.started":"2022-06-19T15:55:40.395054Z","shell.execute_reply":"2022-06-19T15:55:40.407842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n# stop_words = set(stopwords.words('english'))\nclass text_preprocessing:\n    def __init__(self):\n        pass\n        \n    def cont_to_exp(self,x): # Contraction to expansion\n        if type(x) is str:\n            x = x.replace(\"\\w*\\\\\", \"\")\n            for key in contractions:\n                value = contractions[key]\n                x = x.replace(key, value)\n            return x\n        else:\n            return x\n    \n    def to_lower(self, text):\n        return text.lower()\n    \n    def special_char_removal(self, x):\n        return re.sub(r\"[^\\w+ ]+\", \"\", x)\n    \n    def remove_stopwords(self,text):\n        return \" \".join([word for word in str(text).split() if word not in stop_words])\n    \n    def lemmatize_words(self, text):\n        return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    \n    def digit_removal(self, text): #Some questions are like maths equaltions\n        return re.sub('\\w+\\d+\\w*','',text)\n    \n    def removing_spaces(self, text):\n        return text.strip()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:55:40.410252Z","iopub.execute_input":"2022-06-19T15:55:40.410731Z","iopub.status.idle":"2022-06-19T15:55:40.42408Z","shell.execute_reply.started":"2022-06-19T15:55:40.410676Z","shell.execute_reply":"2022-06-19T15:55:40.423021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pro = text_preprocessing()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:55:40.426168Z","iopub.execute_input":"2022-06-19T15:55:40.426406Z","iopub.status.idle":"2022-06-19T15:55:40.437058Z","shell.execute_reply.started":"2022-06-19T15:55:40.426379Z","shell.execute_reply":"2022-06-19T15:55:40.43643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"question1\"] = df[\"question1\"].apply(lambda x: pro.cont_to_exp(x))\ndf[\"question1\"] = df[\"question1\"].apply(lambda x: pro.to_lower(x))\ndf[\"question1\"] = df[\"question1\"].apply(lambda x: pro.special_char_removal(x))\ndf[\"question1\"] = df[\"question1\"].apply(lambda x: pro.remove_stopwords(x))\ndf[\"question1\"] = df[\"question1\"].apply(lambda x: pro.lemmatize_words(x))\ndf[\"question1\"] = df[\"question1\"].apply(lambda x: pro.digit_removal(x))\ndf[\"question1\"] = df[\"question1\"].apply(lambda x: pro.removing_spaces(x))\n\n\ndf[\"question2\"] = df[\"question2\"].apply(lambda x: pro.cont_to_exp(x))\ndf[\"question2\"] = df[\"question2\"].apply(lambda x: pro.to_lower(x))\ndf[\"question2\"] = df[\"question2\"].apply(lambda x: pro.special_char_removal(x))\ndf[\"question2\"] = df[\"question2\"].apply(lambda x: pro.remove_stopwords(x))\ndf[\"question2\"] = df[\"question2\"].apply(lambda x: pro.lemmatize_words(x))\ndf[\"question2\"] = df[\"question2\"].apply(lambda x: pro.digit_removal(x))\ndf[\"question2\"] = df[\"question2\"].apply(lambda x: pro.removing_spaces(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:55:40.439857Z","iopub.execute_input":"2022-06-19T15:55:40.440514Z","iopub.status.idle":"2022-06-19T15:56:41.674254Z","shell.execute_reply.started":"2022-06-19T15:55:40.440481Z","shell.execute_reply":"2022-06-19T15:56:41.673549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:56:41.675362Z","iopub.execute_input":"2022-06-19T15:56:41.676046Z","iopub.status.idle":"2022-06-19T15:56:41.697696Z","shell.execute_reply.started":"2022-06-19T15:56:41.676012Z","shell.execute_reply":"2022-06-19T15:56:41.697105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Word embeddings","metadata":{}},{"cell_type":"markdown","source":"## 1. Word2Vec","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:56:41.698965Z","iopub.execute_input":"2022-06-19T15:56:41.699358Z","iopub.status.idle":"2022-06-19T15:56:45.581194Z","shell.execute_reply.started":"2022-06-19T15:56:41.699329Z","shell.execute_reply":"2022-06-19T15:56:45.580539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_vec(string):\n    doc = nlp(string)\n    vec = doc.vector\n    return vec","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:56:45.582569Z","iopub.execute_input":"2022-06-19T15:56:45.583015Z","iopub.status.idle":"2022-06-19T15:56:45.587157Z","shell.execute_reply.started":"2022-06-19T15:56:45.582984Z","shell.execute_reply":"2022-06-19T15:56:45.586208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Uncomment the below code to get the final dataframe having word2vec embeddings and derived features**","metadata":{}},{"cell_type":"code","source":"# %%time\n# df[\"q1_vector\"] = df[\"question1\"].apply(lambda x: get_vec(x))\n# df[\"q2_vector\"] = df[\"question2\"].apply(lambda x: get_vec(x))\n\n# df_q1 = pd.DataFrame(df.q1_vector.values.tolist(), index= df.index)\n# df_q2 = pd.DataFrame(df.q2_vector.values.tolist(), index= df.index)\n\n# df_features = df.drop(['id', 'question1', 'question2',\n#                       'q1_vector', 'q2_vector'], axis = 'columns')\n\n# print(\"Number of independent features generated after feature engineering:\", df_features.shape[1] - 2)\n# print(\"Features generated through word2vec of question_1:\", df_q1.shape[1])\n# print(\"Features generated through word2vec of question_2:\", df_q2.shape[1])\n\n# final_df = pd.concat([df_features, df_q1, df_q2], axis = 'columns')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:56:45.588495Z","iopub.execute_input":"2022-06-19T15:56:45.588738Z","iopub.status.idle":"2022-06-19T15:56:45.598186Z","shell.execute_reply.started":"2022-06-19T15:56:45.588695Z","shell.execute_reply":"2022-06-19T15:56:45.59752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\n\ndef create_download_link(filename, title = \"Download CSV file\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:56:45.599248Z","iopub.execute_input":"2022-06-19T15:56:45.599615Z","iopub.status.idle":"2022-06-19T15:56:45.609685Z","shell.execute_reply.started":"2022-06-19T15:56:45.599584Z","shell.execute_reply":"2022-06-19T15:56:45.608969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a link to download the dataframe which was saved with .to_csv method\n\n# create_download_link(filename='word2vec.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:56:45.610858Z","iopub.execute_input":"2022-06-19T15:56:45.611247Z","iopub.status.idle":"2022-06-19T15:56:45.624291Z","shell.execute_reply.started":"2022-06-19T15:56:45.611215Z","shell.execute_reply":"2022-06-19T15:56:45.623643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Sent2Vec transformer","metadata":{}},{"cell_type":"code","source":"pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:56:45.625547Z","iopub.execute_input":"2022-06-19T15:56:45.626042Z","iopub.status.idle":"2022-06-19T15:57:00.569289Z","shell.execute_reply.started":"2022-06-19T15:56:45.626008Z","shell.execute_reply":"2022-06-19T15:57:00.568246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Click here to read more about the below transformer](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L3-v2)\n\n<br>\nReason to use this particular transformer:<br>\n* It occupies lesser memory.<br>\n* It was having far better speed than any other transformer.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\n# Model Size:- 61MB, Model Speed:- 19000, Avg Performance:- 50.74","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:57:00.571082Z","iopub.execute_input":"2022-06-19T15:57:00.571336Z","iopub.status.idle":"2022-06-19T15:57:14.779195Z","shell.execute_reply.started":"2022-06-19T15:57:00.571303Z","shell.execute_reply":"2022-06-19T15:57:14.778165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sent_vec(string):\n    embedding = model.encode(string,show_progress_bar = False)\n    return embedding","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:57:14.780509Z","iopub.execute_input":"2022-06-19T15:57:14.780777Z","iopub.status.idle":"2022-06-19T15:57:14.785517Z","shell.execute_reply.started":"2022-06-19T15:57:14.780744Z","shell.execute_reply":"2022-06-19T15:57:14.78441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Uncomment the below code to get the final dataframe having sent2vec embeddings and derived features**","metadata":{}},{"cell_type":"code","source":"# %%timeit\n# df[\"q1_vector\"] = df[\"question1\"].apply(lambda x: get_sent_vec(x))\n# df[\"q2_vector\"] = df[\"question2\"].apply(lambda x: get_sent_vec(x))\n\n# df_q1 = pd.DataFrame(df.q1_vector.values.tolist(), index= df.index)\n# df_q2 = pd.DataFrame(df.q2_vector.values.tolist(), index= df.index)\n\n# df_features = df.drop(['id', 'question1', 'question2',\n#                       'q1_vector', 'q2_vector'], axis = 'columns')\n\n# final_df = pd.concat([df_features, df_q1, df_q2], axis = 'columns')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:57:14.78658Z","iopub.execute_input":"2022-06-19T15:57:14.78681Z","iopub.status.idle":"2022-06-19T15:57:14.797623Z","shell.execute_reply.started":"2022-06-19T15:57:14.786781Z","shell.execute_reply":"2022-06-19T15:57:14.796764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a link to download the dataframe which was saved with .to_csv method\n\n# create_download_link(filename='sent2vec.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:57:14.798965Z","iopub.execute_input":"2022-06-19T15:57:14.799357Z","iopub.status.idle":"2022-06-19T15:57:14.807662Z","shell.execute_reply.started":"2022-06-19T15:57:14.799327Z","shell.execute_reply":"2022-06-19T15:57:14.807103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_df = pd.read_csv(\"../input/embeddings/embeddings_data/word2vec.csv\")\nsent2v_df = pd.read_csv(\"../input/embeddings/embeddings_data/sent2vec.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:27:29.696432Z","iopub.execute_input":"2022-06-19T16:27:29.696925Z","iopub.status.idle":"2022-06-19T16:30:56.754785Z","shell.execute_reply.started":"2022-06-19T16:27:29.696883Z","shell.execute_reply":"2022-06-19T16:30:56.753861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_df.shape, sent2v_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:31:03.859812Z","iopub.execute_input":"2022-06-19T16:31:03.860091Z","iopub.status.idle":"2022-06-19T16:31:03.869193Z","shell.execute_reply.started":"2022-06-19T16:31:03.860064Z","shell.execute_reply":"2022-06-19T16:31:03.86831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Since, the data is very huge, the preprocessing on CSV will take a lot of time, so here I am converting my data into hdf5 which stores the data into hierarchical order. Also, I am using vaex as it works efficiently on large datasets**","metadata":{}},{"cell_type":"code","source":"vaexdf_w2v = vaex.from_pandas(w2v_df)\nvaexdf_sent2v = vaex.from_pandas(sent2v_df)\n\nvaexdf_w2v.export_hdf5('word2vec.hdf5')\nvaexdf_sent2v.export_hdf5('sent2vec.hdf5')\n\nvaexdf_w2v = vaex.open(\"./word2vec.hdf5\")\nvaexdf_sent2v = vaex.open(\"./sent2vec.hdf5\")","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:31:07.527896Z","iopub.execute_input":"2022-06-19T16:31:07.528208Z","iopub.status.idle":"2022-06-19T16:31:37.21506Z","shell.execute_reply.started":"2022-06-19T16:31:07.528174Z","shell.execute_reply":"2022-06-19T16:31:37.213838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Taking random 100,000 records for fast model training**","metadata":{}},{"cell_type":"code","source":"vaexdf_w2v = vaexdf_w2v.shuffle(random_state = 0)\nvaexdf_sent2v = vaexdf_sent2v.shuffle(random_state = 1)\n\nvaexdf_w2v = vaexdf_w2v[:100000]\nvaexdf_sent2v = vaexdf_sent2v[:100000]","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:31:40.97541Z","iopub.execute_input":"2022-06-19T16:31:40.975763Z","iopub.status.idle":"2022-06-19T16:31:41.519043Z","shell.execute_reply.started":"2022-06-19T16:31:40.975726Z","shell.execute_reply":"2022-06-19T16:31:41.518197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"w2v_train , w2v_test = vaexdf_w2v.ml.train_test_split(test_size = 0.2)\nsent2v_train, sent2v_test = vaexdf_sent2v.ml.train_test_split(test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:31:43.324883Z","iopub.execute_input":"2022-06-19T16:31:43.325192Z","iopub.status.idle":"2022-06-19T16:31:43.51571Z","shell.execute_reply.started":"2022-06-19T16:31:43.325158Z","shell.execute_reply":"2022-06-19T16:31:43.51477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_train.shape, w2v_test.shape, sent2v_train.shape, sent2v_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:31:49.049483Z","iopub.execute_input":"2022-06-19T16:31:49.050365Z","iopub.status.idle":"2022-06-19T16:31:49.060006Z","shell.execute_reply.started":"2022-06-19T16:31:49.050302Z","shell.execute_reply":"2022-06-19T16:31:49.05931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking class distribution in train and test data**","metadata":{}},{"cell_type":"code","source":"print(\"-\"*10, \"Distribution of output variable in WORD2VEC train data\", \"-\"*10)\ntrain_distr = Counter(w2v_train['is_duplicate'].to_pandas_series())\ntrain_len = len(w2v_train['is_duplicate'].to_pandas_series())\nprint(\"\\t Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\n\nprint(\"\\n\")\n\nprint(\"-\"*10, \"Distribution of output variable in WORD2VEC test data\", \"-\"*10)\ntest_distr = Counter(w2v_test['is_duplicate'].to_pandas_series())\ntest_len = len(w2v_test['is_duplicate'].to_pandas_series())\nprint(\"\\t Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:31:53.640284Z","iopub.execute_input":"2022-06-19T16:31:53.640573Z","iopub.status.idle":"2022-06-19T16:31:53.675106Z","shell.execute_reply.started":"2022-06-19T16:31:53.640545Z","shell.execute_reply":"2022-06-19T16:31:53.674208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-\"*10, \"Distribution of output variable in SENT2VEC train data\", \"-\"*10)\ntrain_distr = Counter(sent2v_train['is_duplicate'].to_pandas_series())\ntrain_len = len(sent2v_train['is_duplicate'].to_pandas_series())\nprint(\"\\t Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\n\nprint(\"\\n\")\n\nprint(\"-\"*10, \"Distribution of output variable in SENT2VEC test data\", \"-\"*10)\ntest_distr = Counter(sent2v_test['is_duplicate'].to_pandas_series())\ntest_len = len(sent2v_test['is_duplicate'].to_pandas_series())\nprint(\"\\t Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:31:56.302598Z","iopub.execute_input":"2022-06-19T16:31:56.303294Z","iopub.status.idle":"2022-06-19T16:31:56.338309Z","shell.execute_reply.started":"2022-06-19T16:31:56.303249Z","shell.execute_reply":"2022-06-19T16:31:56.337431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling","metadata":{}},{"cell_type":"code","source":"w2v_features = list(w2v_train.column_names)\nw2v_features.remove('is_duplicate')\n\nsent2v_features = list(sent2v_train.column_names)\nsent2v_features.remove('is_duplicate')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:32:00.712629Z","iopub.execute_input":"2022-06-19T16:32:00.712994Z","iopub.status.idle":"2022-06-19T16:32:00.718874Z","shell.execute_reply.started":"2022-06-19T16:32:00.712955Z","shell.execute_reply":"2022-06-19T16:32:00.717803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(w2v_features), len(sent2v_features)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:32:01.079828Z","iopub.execute_input":"2022-06-19T16:32:01.080968Z","iopub.status.idle":"2022-06-19T16:32:01.086499Z","shell.execute_reply.started":"2022-06-19T16:32:01.080924Z","shell.execute_reply":"2022-06-19T16:32:01.085897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Scaling of both the dataframes\n\nscaler_w2v = vaex.ml.MinMaxScaler(features = w2v_features, prefix = \"scaled_\")\nscaler_sent2v = vaex.ml.MinMaxScaler(features = sent2v_features, prefix = \"scaled_\")\n\n# Fitting the scaler to the train set\nscaler_w2v.fit(w2v_train)\nscaler_sent2v.fit(sent2v_train)\n\n# Getting the transformed training and test set\nw2v_train_trans = scaler_w2v.transform(w2v_train)\nw2v_test_trans = scaler_w2v.transform(w2v_test)\n\nsent2v_train_trans = scaler_sent2v.transform(sent2v_train)\nsent2v_test_trans = scaler_sent2v.transform(sent2v_test)\n\n# Removing the initial non scaled features from the scaled dataframe\nw2v_train_trans.drop(w2v_features, inplace = True)\nw2v_test_trans.drop(w2v_features, inplace = True)\n\nsent2v_train_trans.drop(sent2v_features, inplace = True)\nsent2v_test_trans.drop(sent2v_features, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:32:04.346359Z","iopub.execute_input":"2022-06-19T16:32:04.346666Z","iopub.status.idle":"2022-06-19T16:35:31.752613Z","shell.execute_reply.started":"2022-06-19T16:32:04.346636Z","shell.execute_reply":"2022-06-19T16:35:31.751659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression and Linear SVM With Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"def sgd_tuned_model(X_train, y_train, X_test, y_test, loss):\n    alpha = [10 ** x for x in range(-5, 2)] # This is basically the learning rate, the constant in regularization\n\n    log_error_array=[]\n    for i in alpha:\n        clf = SGDClassifier(alpha=i, penalty='l2', loss=loss, random_state=0)\n        clf.fit(X_train, y_train)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(X_train, y_train)\n        predict_y = sig_clf.predict_proba(X_test)\n        log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n        print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n    fig, ax = plt.subplots()\n    ax.plot(alpha, log_error_array,c='g')\n    for i, txt in enumerate(np.round(log_error_array,3)):\n        ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n    plt.grid()\n    plt.title(\"Cross Validation Error for each alpha\")\n    plt.xlabel(\"Alpha i's\")\n    plt.ylabel(\"Error measure\")\n    plt.show()\n\n\n    best_alpha = np.argmin(log_error_array)\n    clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss=loss, random_state=0)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n\n    predict_y = sig_clf.predict_proba(X_train)\n    print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n    predict_y = sig_clf.predict_proba(X_test)\n    print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    predicted_y =np.argmax(predict_y,axis=1)\n    print(\"Total number of data points :\", len(predicted_y))\n    plot_confusion_matrix(sig_clf,X_test, y_test, normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:49:45.529294Z","iopub.execute_input":"2022-06-19T16:49:45.529658Z","iopub.status.idle":"2022-06-19T16:49:45.548639Z","shell.execute_reply.started":"2022-06-19T16:49:45.529623Z","shell.execute_reply":"2022-06-19T16:49:45.547309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_w2v = w2v_train_trans.drop('is_duplicate').to_pandas_df()\ny_train_w2v = w2v_train_trans['is_duplicate'].to_pandas_series()\nX_test_w2v = w2v_test_trans.drop('is_duplicate').to_pandas_df()\ny_test_w2v = w2v_test_trans['is_duplicate'].to_pandas_series()\n\nX_train_sent2v = sent2v_train_trans.drop('is_duplicate').to_pandas_df()\ny_train_sent2v = sent2v_train_trans['is_duplicate'].to_pandas_series()\nX_test_sent2v = sent2v_test_trans.drop('is_duplicate').to_pandas_df()\ny_test_sent2v = sent2v_test_trans['is_duplicate'].to_pandas_series()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:36:20.269378Z","iopub.execute_input":"2022-06-19T16:36:20.269742Z","iopub.status.idle":"2022-06-19T16:37:54.308395Z","shell.execute_reply.started":"2022-06-19T16:36:20.269686Z","shell.execute_reply":"2022-06-19T16:37:54.307241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_w2v.shape,X_train_sent2v.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:38:27.282146Z","iopub.execute_input":"2022-06-19T16:38:27.28304Z","iopub.status.idle":"2022-06-19T16:38:27.29044Z","shell.execute_reply.started":"2022-06-19T16:38:27.282998Z","shell.execute_reply":"2022-06-19T16:38:27.289258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementing LR with tuning on Word2Vec embeddings data\nsgd_tuned_model(X_train_w2v, y_train_w2v, X_test_w2v, y_test_w2v, 'log')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T16:49:49.071991Z","iopub.execute_input":"2022-06-19T16:49:49.072363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementing LR with tuning on Sent2Vec embeddings data\nsgd_tuned_model(X_train_sent2v, y_train_sent2v, X_test_sent2v, y_test_sent2v, 'log')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:48:01.475248Z","iopub.status.idle":"2022-06-19T15:48:01.475591Z","shell.execute_reply.started":"2022-06-19T15:48:01.47542Z","shell.execute_reply":"2022-06-19T15:48:01.475441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementing Linear SVM with tuning on Word2Vec embeddings data\nsgd_tuned_model(X_train_w2v, y_train_w2v, X_test_w2v, y_test_w2v, 'hinge')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:48:01.477016Z","iopub.status.idle":"2022-06-19T15:48:01.477343Z","shell.execute_reply.started":"2022-06-19T15:48:01.477163Z","shell.execute_reply":"2022-06-19T15:48:01.477187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementing Linear SVM with tuning on Sent2Vec embeddings data\nsgd_tuned_model(X_train_sent2v, y_train_sent2v, X_test_sent2v, y_test_sent2v, 'hinge')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:48:01.478216Z","iopub.status.idle":"2022-06-19T15:48:01.478528Z","shell.execute_reply.started":"2022-06-19T15:48:01.478354Z","shell.execute_reply":"2022-06-19T15:48:01.478378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Xgboost with hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"w2v_train.shape, w2v_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:48:01.47944Z","iopub.status.idle":"2022-06-19T15:48:01.479805Z","shell.execute_reply.started":"2022-06-19T15:48:01.479589Z","shell.execute_reply":"2022-06-19T15:48:01.479613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def xgb_classifier(params, X_train, X_test, y_train, y_test):\n#     params = {}\n#     params['objective'] = 'binary:logistic'\n#     params['eval_metric'] = 'logloss'\n#     params['eta'] = 0.02\n#     params['max_depth'] = 4\n\n    d_train = xgb.DMatrix(X_train, label=y_train)\n    d_test = xgb.DMatrix(X_test, label=y_test)\n\n    watchlist = [(d_train, 'train'), (d_test, 'valid')]\n\n    bst = xgb.train(params, d_train, 400, watchlist,verbose_eval= False,early_stopping_rounds=20)\n\n#     xgdmat = xgb.DMatrix(X_train,y_train)\n    predict_y = bst.predict(d_test)\n    print(\"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:48:01.481127Z","iopub.status.idle":"2022-06-19T15:48:01.48145Z","shell.execute_reply.started":"2022-06-19T15:48:01.481281Z","shell.execute_reply":"2022-06-19T15:48:01.481301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# params for hyperparameter tuning\n# w2v_params={\n#     \"learning_rate\"    : [0.0001, 0.001, 0.01, 0.10] ,\n#     \"max_depth\"        : [5, 10, 20, 100, 200],\n#     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7],\n#     \"n_estimators\" : [50, 100, 150, 200]\n# }\n\n# sent2v_params={\n#     \"learning_rate\"    : [0.0001, 0.001, 0.01, 0.10],\n#     \"max_depth\"        : [4, 5, 10, 15, 20],\n#     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7],\n#     \"n_estimators\" : [50, 100, 150, 200]\n# }","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:48:01.482535Z","iopub.status.idle":"2022-06-19T15:48:01.482903Z","shell.execute_reply.started":"2022-06-19T15:48:01.482724Z","shell.execute_reply":"2022-06-19T15:48:01.482747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Implementing XGB classifier with tuning on Word2Vec embeddings data\n# X_train = sent2v_train_trans.drop('is_duplicate').to_pandas_df()\n# y_train = sent2v_train_trans['is_duplicate'].to_pandas_series()\n# X_test = sent2v_test_trans.drop('is_duplicate').to_pandas_df()\n# y_test = sent2v_test_trans['is_duplicate'].to_pandas_series()\n\n# clf = xgb.XGBClassifier(objective = 'binary:logistic', eval_metric = 'logloss')\n\n# random_search = RandomizedSearchCV(clf,\n#                                    param_distributions = sent2v_params,\n#                                    n_iter = 3,\n#                                    n_jobs = -1,\n#                                    cv = 5,\n#                                    verbose = 3)\n# random_search.fit(X_train, y_train)\n\n# random_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:48:01.48441Z","iopub.status.idle":"2022-06-19T15:48:01.485156Z","shell.execute_reply.started":"2022-06-19T15:48:01.484869Z","shell.execute_reply":"2022-06-19T15:48:01.484948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best params using random search cv for Xgboost on W2V embeddings: <br>\n{'n_estimators': 200,\n 'max_depth': 10,\n 'learning_rate': 0.1,\n 'colsample_bytree': 0.3}","metadata":{}},{"cell_type":"code","source":"# Implementing XGBclasifier on Word2Vec data\n\nparams = {'objective' : 'binary:logistic', \n          'eval_metric' : 'logloss',\n          'n_parameters' : 200,\n          'max_depth' : 10, \n          'learning_rate' : 0.1,\n          'colsample_bytree' : 0.3}\n\nX_train = w2v_train.drop('is_duplicate').to_pandas_df()\ny_train = w2v_train['is_duplicate'].to_pandas_series()\nX_test = w2v_test.drop('is_duplicate').to_pandas_df()\ny_test = w2v_test['is_duplicate'].to_pandas_series()\n\nxgb_classifier(params, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:48:01.486325Z","iopub.status.idle":"2022-06-19T15:48:01.487034Z","shell.execute_reply.started":"2022-06-19T15:48:01.486792Z","shell.execute_reply":"2022-06-19T15:48:01.486827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best params using random search cv for xgboost on Sent2Vec embeddings:<br>\n{'n_estimators': 100,\n 'max_depth': 15,\n 'learning_rate': 0.01,\n 'colsample_bytree': 0.5}","metadata":{}},{"cell_type":"code","source":"# Implementing XGBclasifier on Sent2Vec data\nparams = {'objective' : 'binary:logistic', \n          'eval_metric' : 'logloss',\n          'n_parameters' : 100,\n          'max_depth' : 15, \n          'learning_rate' : 0.01,\n          'colsample_bytree' : 0.5}\n\nX_train = sent2v_train.drop('is_duplicate').to_pandas_df()\ny_train = sent2v_train['is_duplicate'].to_pandas_series()\nX_test = sent2v_test.drop('is_duplicate').to_pandas_df()\ny_test = sent2v_test['is_duplicate'].to_pandas_series()\n\nxgb_classifier(params, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T15:48:01.488144Z","iopub.status.idle":"2022-06-19T15:48:01.488456Z","shell.execute_reply.started":"2022-06-19T15:48:01.48829Z","shell.execute_reply":"2022-06-19T15:48:01.488312Z"},"trusted":true},"execution_count":null,"outputs":[]}]}