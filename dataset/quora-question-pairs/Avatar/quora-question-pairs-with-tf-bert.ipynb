{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Please Upvote this notebook if you like the approach ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train[df_train['question1'].apply(lambda x: isinstance(x, str))]\ndf_train = df_train[df_train['question2'].apply(lambda x: isinstance(x, str))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### meta-features\n* number of common tokens(non-stopwords) in both the questions\n* tokens count difference","metadata":{}},{"cell_type":"code","source":"import re, string, six\n\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\n\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n\ndef tokenize(s): \n    return re_tok.sub(r' \\1 ', s).split()\n\ndef clean_text(s):\n    try:\n        return re.sub(r'[^A-Za-z0-9,?\"\\'. ]+', '', s).encode('utf-8').decode('utf-8').lower()\n    except:\n        return \"\"\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    try:\n        for word in tokenize(row['question1']):\n            if word not in stops:\n                q1words[word] = 1\n        for word in tokenize(row['question2']):\n            if word not in stops:\n                q2words[word] = 1\n        if len(q1words) == 0 or len(q2words) == 0:\n            return 0\n        shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n        shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n        return (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    except:\n        return 0\n\ndef word_count_diff(row):\n    try:\n        q1words = len(list(filter(lambda x: x.lower() not in stops, tokenize(row['question1']))))\n        q2words = len(list(filter(lambda x: x.lower() not in stops, tokenize(row['question2']))))\n        return abs(q1words - q2words)\n    except:\n        return 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['wms'] = df_train.apply(word_match_share, axis=1)\ndf_train['wcd'] = df_train.apply(word_count_diff, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.groupby(['is_duplicate']).agg({'wcd': np.mean}).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['question1'] = df_train['question1'].apply(lambda x: clean_text(x))\ndf_train['question2'] = df_train['question2'].apply(lambda x: clean_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\n\nhub_url = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"\nembed = hub.KerasLayer(hub_url, trainable=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def euc_dist(x, y):\n    return np.sqrt(np.dot((x-y), (x-y)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_q1 = df_train['question1'].tolist()\nX_train_q2 = df_train['question2'].tolist()\nX_wms = df_train['wms'].tolist()\nX_wcd = df_train['wcd'].tolist()\ny_train = (1-df_train['is_duplicate']).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train_q1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_q1, X_test_q1, X_train_q2, X_test_q2, X_wms_train, X_wms_test, X_wcd_train, X_wcd_test, y_train, y_test = train_test_split(X_train_q1, X_train_q2, X_wms, X_wcd, y_train, test_size=0.2, random_state=42, stratify=y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_wcd_train[:4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n# tf.config.run_functions_eagerly(False)\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input1 = Input(shape=(), dtype=tf.string)\ninput2 = Input(shape=(), dtype=tf.string)\ninput_wms = Input(shape=(1,), dtype=tf.float16)\ninput_wcd = Input(shape=(1,), dtype=tf.float16)\n\nembed1 = embed(input1)\nembed2 = embed(input2)\n\ndist = Lambda(lambda x: K.sqrt(K.sum(K.square(x[0] - x[1]), axis=-1, keepdims=True)))([embed1,embed2])\n\nconcat = Concatenate(axis=1)([dist, input_wms, input_wcd])\n\nhidden = Dense(9, activation=\"relu\", kernel_regularizer=l2(1e-4))(concat)\n\nout = Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(1e-4))(hidden)\nmodel = Model(inputs=[input1, input2, input_wms, input_wcd], outputs=out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(1e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callbacks defined\n\n# learning rate schedule\ndef step_decay(epoch):\n    initial_lrate = 0.003\n    drop = 0.5\n    epochs_drop = 3\n    lrate = initial_lrate * (drop**((1 + epoch)/epochs_drop))\n    return lrate\n\nlrate_scheduler = LearningRateScheduler(step_decay)\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\nmodel_chkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n\nmodel.fit(x=[np.array(X_train_q1), np.array(X_train_q2), np.array(X_wms_train), np.array(X_wcd_train)],\n          y=np.array(y_train),\n          batch_size=128,\n          epochs=5,\n          validation_data=([np.array(X_test_q1), np.array(X_test_q2), np.array(X_wms_test), np.array(X_wcd_test)], np.array(y_test)),\n          callbacks=[lrate_scheduler, early_stop, model_chkpoint])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(\"../input/quora-question-pairs/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['wms'] = df_test.apply(word_match_share, axis=1)\ndf_test['wcd'] = df_test.apply(word_count_diff, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['question1'] = df_test['question1'].apply(lambda x: clean_text(x))\ndf_test['question2'] = df_test['question2'].apply(lambda x: clean_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_q1 = df_test['question1'].tolist()\nX_test_q2 = df_test['question2'].tolist()\nX_test_wms = df_test['wms'].tolist()\nX_test_wcd = df_test['wcd'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\npreds = []\nbatch_size = 512\nsteps = len(X_test_q1) // batch_size + 1\nfor i in tqdm(range(0, steps)):\n    X_test_q1_batch = np.array(X_test_q1[i*batch_size: i*batch_size+batch_size])\n    X_test_q2_batch = np.array(X_test_q2[i*batch_size: i*batch_size+batch_size])\n    X_test_wms_batch = np.array(X_test_wms[i*batch_size: i*batch_size+batch_size])\n    X_test_wcd_batch = np.array(X_test_wcd[i*batch_size: i*batch_size+batch_size])\n    preds.extend(model.predict([X_test_q1_batch, X_test_q2_batch, X_test_wms_batch, X_test_wcd_batch]))","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [1 - x[0] for x in preds]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['is_duplicate'] = preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop(['question1', 'question2', 'wms', 'wcd'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.set_index('test_id').to_csv(\"submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}