{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport os\nimport re\nimport string\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.metrics import log_loss, make_scorer\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.grid_search import RandomizedSearchCV\n","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"train_file = \"../input/train.csv\"\ntest_file = \"../input/test.csv\"","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"test = pd.read_csv(test_file)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"train = pd.read_csv(train_file)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"train.head(5)","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"train['is_duplicate'].value_counts()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"train[train['is_duplicate']==1].head(5)","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"train.dropna(inplace = True)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"train.shape","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"def tokenize(text):\n    \"\"\"\n    Given a string, return a list of words normalized as follows.\n    Split the string to make words first by using regex compile() function\n    and string.punctuation + '0-9\\\\r\\\\t\\\\n]' to replace all those\n    char with a space character.\n    Split on space to get word list.\n    Ignore words < 3 char long.\n    Lowercase all words\n    Remove English stop words\n    \"\"\"\n    re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n    regex = re.compile(re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n    \n    \n    regex = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])'+re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n    words = regex.sub(r' \\1 ', text).split()\n    words = [w.lower() for w in words]\n    words = [w.strip() for w in words]\n    words = [w for w in words if len(w) > 2]  # ignore a, an, to, at, be, ...\n    words = [w for w in words if w not in stop_words.ENGLISH_STOP_WORDS]\n    return words\n","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"veczr = CountVectorizer(lowercase = True, analyzer= 'word', stop_words='english', min_df = 0.0001, max_features= 3000)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"```min_df  = 0.0001``` - if the word occurs less than in 0.01% of total documents, ignore it","metadata":{}},{"cell_type":"code","source":"voc = veczr.fit(pd.concat([train['question1'],train['question2']])) # Learn a vocabulary dictionary of all tokens in both questions ","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"q1 = voc.transform(train['question1']) # Transform documents (question 1) to document-term matrix \nq2 = voc.transform(train['question2']) # Same for question 2","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"from scipy.sparse import hstack\n\nX = hstack((q1,q2)) # stacking together two matrices\nX.shape\n# Convert from coo to csr format\nX = X.toarray()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"y = train['is_duplicate']\ny.shape","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"x_train,x_val,y_train, y_val= train_test_split(X,train['is_duplicate'], test_size = 0.2, random_state =42)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"Let's just check how it works.\n```.fit``` of CountVectorizer returned us a vocubulary of words which it \"learned\" from question1 and question2.\n\n```.transform``` transformed questions into doc-term matrix, using the vocabulary.\n\nLet's see some words contained in the vocabulary:","metadata":{}},{"cell_type":"code","source":"vocab = veczr.get_feature_names(); vocab[800:900]","outputs":[],"execution_count":null,"metadata":{"scrolled":true}},{"cell_type":"markdown","source":"Now, let's take a look at question 1, say, at a position 90 in the intitial data frame.","metadata":{}},{"cell_type":"code","source":"train.loc[390,'question1'].split()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"1. Okay, let's find the position of a word \"wedding\" in our vocabulary...","metadata":{}},{"cell_type":"code","source":"veczr.vocabulary_['wedding'] # find the position of the word 'wedding'","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"Finanlly, let's see how many times the word \"wedding\" occurs in question 390? (Should be twice)","metadata":{}},{"cell_type":"code","source":"X[390,2931] # Yep!","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"Now, let's train the Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"The competition uses the following evaluation metric:\nlog-loss = (-1/N)sum(yi log(pi) + (1-yi) log(1-pi))","metadata":{}},{"cell_type":"code","source":"mlr = LogisticRegression(C = 0.1, dual = True, n_jobs = -1)\nmlr.fit(x_train,y_train)","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"# make predictions\npred_train = mlr.predict(x_train)\npred_prob_train = mlr.predict_proba(x_train)\npred_val = mlr.predict(x_val)\npred_prob_val = mlr.predict_proba(x_val)\nprint(\"Accuracy of training:\",(pred_train.T == y_train).mean())\nprint(\"log-loss of training is:\", log_loss(y_train,pred_prob_train))\nprint(\"Accuracy of validation:\",(pred_val.T == y_val).mean())\nprint(\"log-loss of validation is:\", log_loss(y_val,pred_prob_val))","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"# use binarized version\nmlr = LogisticRegression(C = 0.1, dual = True, n_jobs = -1)\nmlr.fit(x_train.sign(),y_train)\npred_train = mlr.predict(x_train.sign())\npred_prob_train = mlr.predict_proba(x_train.sign())\npred_val = mlr.predict(x_val.sign())\npred_prob_val = mlr.predict_proba(x_val.sign())\nprint(\"Accuracy of training:\",(pred_train.T == y_train).mean())\nprint(\"log-loss of training is:\", log_loss(y_train,pred_prob_train))\nprint(\"Accuracy of validation:\",(pred_val.T == y_val).mean())\nprint(\"log-loss of validation is:\", log_loss(y_val,pred_prob_val))","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"Now, instead of single words, let's use bigrams and trigrams as tokens","metadata":{}},{"cell_type":"code","source":"veczr1 = CountVectorizer(lowercase = True, ngram_range= (1,3), stop_words='english', min_df = 0.0001, max_features= 3000)\nvoc1 = veczr1.fit(pd.concat([train['question1'],train['question2']])) # Learn a vocabulary dictionary of all tokens in both questions ","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"q1 = voc1.transform(train['question1']) # Transform documents (question 1) to document-term matrix \nq2 = voc1.transform(train['question2']) # Same for question 2","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"X1 = hstack([q1,q2]) # statcking together two matrices\nX1.shape","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"x1_train,x1_val,y1_train, y1_val= train_test_split(X1,train['is_duplicate'], test_size = 0.2, random_state =42)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"# use binarized version\nmlr1 = LogisticRegression(C = 0.1, dual = True, n_jobs = -1)\nmlr1.fit(x1_train.sign(),y1_train)\npred_train = mlr.predict(x1_train.sign())\npred_prob_train = mlr.predict_proba(x1_train.sign())\npred_val = mlr.predict(x1_val.sign())\npred_prob_val = mlr.predict_proba(x1_val.sign())\nprint(\"Accuracy of training:\",(pred_train.T == y1_train).mean())\nprint(\"log-loss of training is:\", log_loss(y1_train,pred_prob_train))\nprint(\"Accuracy of validation:\",(pred_val.T == y1_val).mean())\nprint(\"log-loss of validation is:\", log_loss(y1_val,pred_prob_val))","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"mrf = RandomForestClassifier(n_estimators = 50, min_samples_leaf = 3, max_features = 0.5, n_jobs = -1, max_depth = 20, random_state = 42, oob_score = True)\nmrf.fit(x_train,y_train)","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"def print_score(m):\n    res = [log_loss(y_train,m.predict_proba(x_train)), log_loss(y_val,m.predict_proba(x_val)),\n                m.score(x_train, y_train), m.score(x_val, y_val)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"### RF parameters tuning","metadata":{}},{"cell_type":"markdown","source":"There are the following hyperparameters we can tune:\n\n- **n_estimators** - the number of decision trees in a forest\n- **max_depth** - the depth of a tree\n- **min_samples_leaf** - the minimum sample in the leaf. The node with less than this limit is not allowed to split\n- **max_features** - a fraction (from 0 to 1) of features to be considered in a tree","metadata":{}},{"cell_type":"code","source":"log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"param_grid = {#\"n_estimators\": np.arange(25, 100, 25,dtype=int)}\n              \"max_depth\": np.arange(45, 105, 10)} \n              #\"min_samples_split\": np.arange(1,150,1),\n              #\"min_samples_leaf\": [10,50,100]}\n              #\"max_leaf_nodes\": np.arange(2,60,6),\n              #\"min_weight_fraction_leaf\": np.arange(0.1,0.4, 0.1)}","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"m = RandomForestClassifier(random_state=42, n_estimators = 100, n_jobs = -1, oob_score=True)\nm.fit(x_train, y_train)","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"random_cv = RandomizedSearchCV(m, param_distributions = param_grid, cv = 3, scoring=log_loss_scorer)\nrandom_cv.fit(x_train, y_train)\n\nprint(random_cv.best_score_)\nprint(random_cv.best_params_)\nprint(random_cv.best_estimator_)    ","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid_search = GridSearchCV(m, param_grid=param_grid, scoring = 'accuracy')\ngrid_search.fit(x_train, y_train)\n","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"print(grid_search.cv_results_)\nresult = grid_search.cv_results_","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"grid_search.n_splits_","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"md_score = result['mean_test_score']\nmd = range(1,50,5)","outputs":[],"execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(md,md_score)","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\nplt.show()","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}}}