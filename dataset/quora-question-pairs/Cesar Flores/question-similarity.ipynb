{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"#\n# read sets into dataframes\ndf_train = pd.read_csv('../input/train.csv')\n# for running local\n#df_train = pd.read_csv('data/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n#\n# basic information about the dataset\nprint(\"there are {0} question pairs in training and {1} columns\".format(df_train.shape[0], df_train.shape[1]))\n#print(\"there are {0} question pairs in testing and {1} columns\".format(df_test.shape[0], df_test.shape[1]))\nprint(\"-------- TRAIN DATA TYPES --------\")\nprint(df_train.dtypes)\n#print(\"-------- TEST DATA TYPES --------\")\n#print(df_test.dtypes)\nprint(\"-------- TRAIN MISSING VALUES --------\")\nprint(df_train.isnull().sum())\n#print(\"-------- TEST MISSING VALUES --------\")\n#print(df_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ef86e9452990e825056eb9cfb083bd60b00d16d","trusted":true,"collapsed":true},"cell_type":"code","source":"#\n# there are a small number of missing values, first i will see if it's just error and replace\n# based on the question id\nmissingq1_qid1 = df_train.loc[df_train['question1'].isnull()]['qid1'].tolist()[0]\nmissingq2_qid2 = df_train.loc[df_train['question2'].isnull()]['qid2'].tolist()[0]   # this is the same question so just one item \nprint(\"id for missing q1: {0}, id for missing q2: {1}\".format(missingq1_qid1, missingq2_qid2))\n#\n# retrieve from dataset questions with previous ids (question 1 first)\nif len(df_train.loc[(df_train['qid1'] == missingq1_qid1) & (df_train['id'] != df_train.loc[df_train['question1'].isnull()]['id'].tolist()[0])]) == 0:\n    print(\"no question to replace missing q1\")\nelse:\n    print(\"replace missing q1 with question id: {}\".format(df_train.loc[(df_train['qid1'] == missingq1_qid1) & (df_train['id'] != df_train.loc[df_train['question1'].isnull()]['id'].tolist()[0])]['id']))\n#\n# drop the missing values\ndf_train.dropna(inplace=True)\ndf_train = df_train.reset_index(drop=True)\nprint(\"-------- TRAIN MISSING VALUES --------\")\nprint(df_train.isnull().sum())\nprint(df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6a397129c6f45a8b4b41ac8bca4104b2673210b","trusted":true,"collapsed":true},"cell_type":"code","source":"#\n# as is_duplicate is our target and as a binary column we can compute the mean to get\n# the percentage of duplicate question in the dataset\nprint(\"percentage of duplicate questions in dataset: {0}%\".format(df_train['is_duplicate'].mean()*100))\nprint(\"-------- COUNT, UNIQUE, TOP AND FREQUENCY FOR QUESTIONS 1 --------\")\nprint(df_train.qid1.astype(str).describe())\nprint(\"-------- COUNT, UNIQUE, TOP AND FREQUENCY FOR QUESTIONS 2 --------\")\nprint(df_train.qid2.astype(str).describe())\nprint(\"top question 1 in dataset: \\'{0}\\'\".format(df_train.loc[df_train['qid1'] == 8461, 'question1'].iloc[0]))\nprint(\"top question 2 in dataset: \\'{0}\\'\".format(df_train.loc[df_train['qid2'] == 30782, 'question2'].iloc[0]))\nprint(\"-------- PERCENTAGE OF UNIQUE QUESTIONS --------\")\nprint(\"Q1: {}%\".format(df_train.qid1.astype(str).describe()['unique']/df_train.qid1.astype(str).describe()['count']*100))\nprint(\"Q2: {}%\".format(df_train.qid2.astype(str).describe()['unique']/df_train.qid2.astype(str).describe()['count']*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d2f655eb663c723e206efa73bc57152a4a2e7dc","collapsed":true},"cell_type":"markdown","source":"It seems that by categorizing these questions we can gain useful knowledge at a question level prior detecting if they are duplicate or not. As printed in the previous snippet, the top questions for both 1 and 2 belong to specific categories and very different from each other. One is essentially \"language\" category while the second one is \"social media\""},{"metadata":{"_uuid":"31aef3c56d24acd695e349f794900b657074a985","trusted":true,"collapsed":true},"cell_type":"code","source":"#\n# visualization on unique questions and duplicate questions\n# how many repeated questions q1 are duplicated\n# how many repeated questions q2 are duplicated\n# how many repeated questions q1 and q2 are duplicated\nprint(df_train.loc[df_train.duplicated('qid1')].is_duplicate.describe())\nprint(df_train.loc[df_train.duplicated('qid2')].is_duplicate.describe())\nprint(df_train.loc[df_train.duplicated('qid1') & df_train.duplicated('qid2')].is_duplicate.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5e90251b5ab3f74976054ac11774f0abde61ff3","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"#\n# visualization on unique questions and duplicate questions\n# how many unique questions q1 are duplicated\n# how many unique questions q2 are duplicated\n# how many unique questions q1 and q2 are duplicated\nprint(df_train.loc[df_train.duplicated('qid1') == False].is_duplicate.describe())\nprint(df_train.loc[df_train.duplicated('qid2') == False].is_duplicate.describe())\nprint(df_train.loc[(df_train.duplicated('qid1') == False) & (df_train.duplicated('qid2') == False)].is_duplicate.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bf137c4f7c46ab888c0187d4844d7a12a71ed87","collapsed":true,"trusted":true},"cell_type":"code","source":"#\n# number of words per question (withouth pre-processing)\nimport spacy\nnlp = spacy.load('en')\n#\n# tokenize words in question1 and quetion2 and get the length of tokens \ndf_train['length_question1'] = df_train['question1'].apply(lambda x: len(nlp(x,  disable=['parser', 'tagger', 'ner'])))\ndf_train['length_question2'] = df_train['question2'].apply(lambda x: len(nlp(x,  disable=['parser', 'tagger', 'ner'])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"225d98161f5603dbff6d4e153594492c5d120ccc","trusted":true,"collapsed":true},"cell_type":"code","source":"#\n# Visualizing the distribution of variables in dataset\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats, integrate\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set(color_codes=True)\n\nnp.random.seed(sum(map(ord, \"distributions\")))\n\nfig, ax = plt.subplots(1,2,figsize=(30,15))\n# frequency density on y axis\nsns.distplot(ax=ax[0], bins=50, a=df_train['length_question1'])\nsns.distplot(ax=ax[1], bins=50, a=df_train['length_question2'])\nprint(\"question 1 length mean: {0} and median: {1}\".format(df_train['length_question1'].mean(), df_train['length_question1'].median()))\nprint(\"question 2 length mean: {0} and median: {1}\".format(df_train['length_question2'].mean(), df_train['length_question2'].median()))\nprint(\"maximum length for question 1: {0}, question: \\n{2}\\n\\nmaximum length for question 2: {1}, question: \\n{3}\".format(df_train['length_question1'].max(), df_train['length_question2'].max(), df_train.loc[df_train['length_question1'] == 146].question1.item(), df_train.loc[df_train['length_question2'] == 271].question2.iloc[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b52daa51d18a10b8abdb189c7cfc936082a8941f"},"cell_type":"markdown","source":"Univariate distribution for both questions length skewed right, mean is towards the right and median closer to the pick (both containing the same median). questions length for second set are similar distributed with the question lenght from the first set. duplicated question 2 for the same length, none of the highest values are find duplicates in the training dataset."},{"metadata":{"_uuid":"1f8da48cb795e1e4d3fc9fc2b57f699a1809bf09","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"#\n# scatter plot to see the relation of both questions length in each axis\nsns.jointplot(size=20, ratio=5, x=\"length_question1\", y=\"length_question2\", data=df_train[['length_question1', 'length_question2']]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7b9fd9a4f5331aaed08cb64417add536a47f88a"},"cell_type":"markdown","source":"Without cleaning stopwords and pre-processing text i want to see how length of both questions relate to the target variable (is_duplicate)"},{"metadata":{"_uuid":"cf7ee20244508b13454f00b4df074dbb30b494bf","trusted":true,"collapsed":true},"cell_type":"code","source":"#\n# scatter plot for both variables, color by is_duplicate or not\nsns.lmplot(x=\"length_question1\", y=\"length_question2\", data=df_train[['length_question1', 'length_question2', 'is_duplicate']],\n           fit_reg=False, hue='is_duplicate', legend=True, size=15)\n#\n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0731adc02d18a36648a197df40859ab2f5f0054e"},"cell_type":"markdown","source":"Most duplicate questions appear in the range of the same length and in the other side questions with substantial difference in length can mean different questions. it would be a good excercise to plot this difference."},{"metadata":{"_uuid":"185a73aba9c3bcd54fcee3b912a7e524d27edb65","trusted":true,"collapsed":true},"cell_type":"code","source":"#\n# caluclate the difference in between lengths for questions (1 and 2)\ndf_train['length_difference_q12'] = abs(df_train['length_question1'] - df_train['length_question2'])\n#\n# plot the absolute difference in between the lengths of questions\nplt.figure(figsize=(20,10))\nsns.regplot(data=df_train, x=\"length_difference_q12\", y=\"is_duplicate\", logistic=True, n_boot=500, y_jitter=.03)\n#\n# print correlation matrix\ndf_train[['length_question1', 'length_question2', 'length_difference_q12', 'is_duplicate']].corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bba22060ecd3a0aabdab611b461d841c9b19f67","collapsed":true},"cell_type":"markdown","source":"I can see that duplicate questions tend to appear when the difference of their lengths is less than 40~ but at the same time not duplicate questions tend to happen too with this criteria, in less proportion but not fully a rule. This also can happen becuase most of the questions tend to have similar lengths and in previous plots i can also see that only a few cases have substantial differences on length.\nFrom previous results and final correlation, i conclude that:\n* the pearson correlation between question length 1 and 2 is about 0.462468, which indicates that there is a moderate positive relationship between the variables.\n* the pearson correlation between difference in lengths and is duplicate is about -0.206474, as a negative value, indicates that as the difference increases, the target value decreases, this actually makes sense as lower distance of length can represent similar questions (1 for is_duplicate) while substantial differences on length tend to be different questions (0 for is_duplicate)"},{"metadata":{"_uuid":"8970e9312575599d575ac5f9f2eba2079bdd697b","collapsed":true},"cell_type":"markdown","source":"Now that i have a baseline on the question lengths in both sides as a good indicator of a duplicate question, i can focus on the rest of the features related to natural language processing. For the first task I will find relevant words, ngrams and how they interact in between questions and within the target value."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f9ea64aef97582cbe30ee3b6113c08592b1dd375"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5475b5562724f0ca4c36ae2f8735cbe1fe41220a"},"cell_type":"code","source":"from wordcloud import WordCloud\n#\n# create corpus for both set of questions\nquestion1_corpus = \" \".join(df_train['question1'].tolist())\nquestion2_corpus = \" \".join(df_train['question2'].tolist())\n#\n# wordcloud\ncloud_1 = WordCloud(width=1920, height=1080, background_color=\"white\", mode=\"RGB\").generate(question1_corpus)\ncloud_2 = WordCloud(width=1920, height=1080, background_color=\"white\", mode=\"RGB\").generate(question2_corpus)\n# plot definitions\nfont = {'weight': 'bold', 'size': 28}\nplt.figure(figsize=(20, 15))\nplt.title(\"WordCoud for Question 1\", loc=\"center\", fontdict=font)\nplt.imshow(cloud_1)\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"081fbec0c477744a3da4901462e373493c70a5f0"},"cell_type":"code","source":"# wordcloud for question 2\nplt.figure(figsize=(20, 15))\nplt.title(\"WordCoud for Question 2\", loc=\"center\", fontdict=font)\nplt.imshow(cloud_2)\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e33a1dfc095a9f78576c38d0098a1a5aefca4612"},"cell_type":"markdown","source":"for both set of questions, it seems to have similarities on the amount words are being used, I will plot both into one WordCloud"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8a48b37d18d5f5e41bb4aa464da0fee718995744"},"cell_type":"code","source":"questions_corpus = question1_corpus + question2_corpus\nprint(\"Total naive tokens in corpus {}\".format(len(questions_corpus.split(\" \"))))\ncloud = WordCloud(width=1920, height=1080, background_color=\"white\", mode=\"RGB\").generate(questions_corpus)\n# plot definitions\nfont = {'weight': 'bold', 'size': 28}\nplt.figure(figsize=(20, 15))\nplt.title(\"WordCoud for Question Corpus\", loc=\"center\", fontdict=font)\nplt.imshow(cloud)\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5c897c90e15fde29045ad0ab6f32f32ea0d9514"},"cell_type":"markdown","source":"I would also like to see how these terms interact with each other, I will use NLTK to plot to most frequent ngrams"},{"metadata":{"trusted":true,"_uuid":"e4f47b9900a5b5d1cea7bfe41e58c6b36684b65d","collapsed":true},"cell_type":"code","source":"import re\n# import contractions\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import Counter\n#\n# pre-process text to fix contractions and remove question marks and non alphanumeric characters\n# to not sum them in the grams as corpus was joint into one\n# questions_corpus = contractions.fix(questions_corpus)\nquestions_corpus = re.sub(r\"'\", '',questions_corpus)\nquestions_corpus = re.sub(r'[^A-Za-z0-9]', ' ',questions_corpus)\n# tokenize resulting corpus by words\ntoken = nltk.word_tokenize(questions_corpus)\n# build ngrams for 3, 4 and 5 windows\ntrigrams = ngrams(token,3)\nfourgrams = ngrams(token,4)\nfivegrams = ngrams(token,5)\ntrigram_counter = Counter(trigrams)\nfourgram_counter = Counter(fourgrams)\nfivegram_counter = Counter(fivegrams)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"73fa64979cda00d06805a210ea9092c297473c88"},"cell_type":"code","source":"tgc_t10 = trigram_counter.most_common(10)\nfgc_t10 = fourgram_counter.most_common(10)\nftgc_10 = fivegram_counter.most_common(10)\nprint(\"------ TRIGRAM ------\")\nfor idx in range(0, len(tgc_t10)):\n    print(\"Word Combination {0}, frequency: {1}\".format(tgc_t10[idx][0],tgc_t10[idx][1]))\nprint(\"------ FOURGRAM ------\")\nfor idx in range(0, len(fgc_t10)):\n    print(\"Word Combination {0}, frequency: {1}\".format(fgc_t10[idx][0],fgc_t10[idx][1]))\nprint(\"------ FIVEGRAM ------\")\nfor idx in range(0, len(ftgc_10)):\n    print(\"Word Combination {0}, frequency: {1}\".format(ftgc_10[idx][0],ftgc_10[idx][1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5afe5a560944d0d8115cf49986c895c475108ae9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}