{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import logging\nimport numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nlogging.basicConfig(level=logging.DEBUG, format='%(asctime)s -- %(name)s -- %(levelname)s -- %(message)s')\nlogger = logging.getLogger(__name__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ntrain_data = pd.read_table('../input/quora-question-pairs/train.csv', sep=',')[['question1', 'question2', 'is_duplicate']].dropna().values\ntest_data = pd.read_table('../input/quora-question-pairs/test.csv', sep=',')[['question1', 'question2']].fillna('').values\ntrain_data_context, train_data_label = train_data[:, 0:2], train_data[:, 2]\nprint(train_data_context.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_split_data_context, valid_split_data_context, train_split_data_label, valid_split_data_label = train_test_split(train_data_context, train_data_label, test_size=0.05)\n\ntrain_split_data_context = [line[0] + ' ' + line[1] for line in train_split_data_context]\nvalid_split_data_context = [line[0] + ' ' + line[1] for line in valid_split_data_context]\n\nprint(train_split_data_context[0])\nprint(valid_split_data_context[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(\n    num_words=50000, # 词表去20000，词表的提取根据TF的计算结果排序\n    filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ',\n    lower=True,\n    split=' ',\n    char_level=False,\n    oov_token=None,\n    document_count=0)\n\ntokenizer.fit_on_texts(tqdm(train_split_data_context))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\n\ntrain_split_data_index = np.array(tokenizer.texts_to_sequences(tqdm(train_split_data_context)))\nvalid_split_data_index = np.array(tokenizer.texts_to_sequences(tqdm(valid_split_data_context)))\ntrain_split_data_index = pad_sequences(tqdm(train_split_data_index), maxlen=50, padding='post')\nvalid_split_data_index = pad_sequences(tqdm(valid_split_data_index), maxlen=50, padding='post')\n\nprint(train_split_data_index[0])\nprint(valid_split_data_index[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\ndef get_batch(epoches, batch_size, data, label):\n    data = list(zip(train_split_data_index, label))\n    for epoch in range(epoches):\n        random.shuffle(data)\n        for batch in range(0, len(data), batch_size):\n            if batch + batch_size >= len(data):\n                yield data[batch: len(data)]\n            else:\n                yield data[batch: (batch + batch_size)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nclass MyModel(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dims):\n        super(MyModel, self).__init__()\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dims)\n        self.logiticRegression_one = tf.keras.layers.Dense(1, input_shape=(embedding_dims,), activation=\"sigmoid\")\n        self.logiticRegression_two = tf.keras.layers.Dense(2, activation='softmax')\n\n    def call(self, sentence):\n        embedding = self.embedding(sentence)\n        sentence_embedding = tf.reduce_mean(embedding, axis=1)\n        result = self.logiticRegression_two(sentence_embedding)\n        print(\"for_return: \", result)\n        return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MyModel(vocab_size=50005,\n                embedding_dims=128)\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntest_accuracy = tf.keras.metrics.Mean(name='test_accuracy')\ntrain_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n\n@tf.function\ndef test_step(sentence, label):\n    predictions = model(sentence)\n    labels = tf.concat((tf.expand_dims(1 - label, axis=-1), tf.expand_dims(label, axis=-1)), axis=1)\n    losses = tf.nn.weighted_cross_entropy_with_logits(labels, predictions, pos_weight=0.3632292393, name=None)\n    \n    predict_label = tf.argmax(predictions, axis=1)\n    correct_predict = tf.equal(tf.cast(label, dtype=tf.int32), tf.cast(predict_label, dtype=tf.int32))\n\n    test_loss(losses)\n    test_accuracy(correct_predict)\n\n\n@tf.function\ndef train_step(sentence, label):\n    with tf.GradientTape() as tape:\n        predictions = model(sentence)\n        labels = tf.concat((tf.expand_dims(1 - label, axis=-1), tf.expand_dims(label, axis=-1)), axis=1)\n        losses = tf.nn.weighted_cross_entropy_with_logits(labels, predictions, pos_weight=0.3632292393, name=None)\n        \n        predict_label = tf.argmax(predictions, axis=1)\n        correct_predict = tf.equal(tf.cast(label, dtype=tf.int32), tf.cast(predict_label, dtype=tf.int32))\n\n    gradients = tape.gradient(losses, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    train_loss(losses)\n    train_accuracy(correct_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS, BATCH_SIZE = 10, 320\ntrain_data_index = tf.convert_to_tensor(train_split_data_index, dtype=tf.int32)\ntrain_data_label = tf.convert_to_tensor(train_split_data_label, dtype=tf.float32)\nvalid_data_index = tf.convert_to_tensor(valid_split_data_index, dtype=tf.int32)\nvalid_data_label = tf.convert_to_tensor(valid_split_data_label, dtype=tf.float32)\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_data_index, train_data_label)).repeat(EPOCHS).batch(BATCH_SIZE)\n\nstep = 0\nfor data, label in train_ds:\n    step += 1\n    train_step(data, label)\n    if step % 100 == 0:\n        test_step(valid_data_index, valid_data_label)\n        template = 'Step {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n        print(template.format(step,\n                              train_loss.result().numpy(),\n                              train_accuracy.result().numpy(),\n                              test_loss.result().numpy(),\n                              test_accuracy.result().numpy()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}