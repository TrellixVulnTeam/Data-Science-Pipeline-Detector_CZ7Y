{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c8f9202a-9409-5af3-97ea-dd948b1298e0"},"source":"Hello everyone interested in knowing more about NLP. I am looking for more methods to implement, and even though I don't seem to have a lot of results, I am enjoying what I am doing.\n\nSo, here goes another.\n\nThis implementation is trying to use very basic NLP (not deep learning) algorithms to extract semantic relations between sentences. I might combine all of my previous methods and make a feature set out of it to train with a classifier or train multiple classifiers and use bagging/boosting for accuracy.\n\nAll that is coming next is an implementation of [this paper](https://trac.research.cc.gatech.edu/ccl/export/158/SecondMindProject/SM/SM.WordNet/Paper/WordNetDotNet_Semantic_Similarity.pdf)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84942a99-6e4b-e6a9-7ec7-02dc9c0dfb82"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk import word_tokenize\n\nimport re\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"d2558b4d-169d-637b-73a8-3fd4257671a0"},"source":"The tokenize(), posTag(), and stemmer() are simple functions to perform word_tokenization, part of speech tagging and return stemmed words in two input sentences."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"051663c2-7e8b-09ab-61e0-12e6f7316037"},"outputs":[],"source":"import nltk\nfrom nltk import word_tokenize\nimport pandas as pd\n\n\ndef tokenize(q1, q2):\n    \"\"\"\n        q1 and q2 are sentences/questions. Function returns a list of tokens for both.\n    \"\"\"\n    return word_tokenize(q1), word_tokenize(q2)\n\n\ndef posTag(q1, q2):\n    \"\"\"\n        q1 and q2 are lists. Function returns a list of POS tagged tokens for both.\n    \"\"\"\n    return nltk.pos_tag(q1), nltk.pos_tag(q2)\n\n\ndef stemmer(tag_q1, tag_q2):\n    \"\"\"\n        tag_q = tagged lists. Function returns a stemmed list.\n    \"\"\"\n\n    stem_q1 = []\n    stem_q2 = []\n\n    for token in tag_q1:\n        stem_q1.append(stem(token))\n\n    for token in tag_q2:\n        stem_q2.append(stem(token))\n\n    return stem_q1, stem_q2"},{"cell_type":"markdown","metadata":{"_cell_guid":"b201e133-92f8-a9c7-06b4-e3bdf40c50af"},"source":"The Micheal Lesk Algorithm is famous for word sense disambiguation (WSD).\n\nWSD is the process of finding the best possible sense of a word from all the given senses of the word. The Micheal Lesk algorithm uses the WordNet to gather the gloss of all the senses of the word in the sentence and then calculates the maximum overlap with the senses returning whichever gives the maximum overlap.\n\nThe implementation here is a bit different from both the paper and the Micheal Lesk algorithm. My Adapted Lesk Algorithm finds the sense which is best related to the sentence (excluding the stop words). The paper defines a constant k which they use to limit the context of the sentence being matched. The overlap is calculated using the gloss of all the context words such that the most matching sense of the word will be accepted as the best sense of the word in the context."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b63fdcd-0ee2-aaa9-6b31-9b9b3168c1d3"},"outputs":[],"source":"class Lesk(object):\n\n    def __init__(self, sentence):\n        self.sentence = sentence\n        self.meanings = {}\n        for word in sentence:\n            self.meanings[word] = ''\n\n    def getSenses(self, word):\n        # print word\n        return wn.synsets(word.lower())\n\n    def getGloss(self, senses):\n\n        gloss = {}\n\n        for sense in senses:\n            gloss[sense.name()] = []\n\n        for sense in senses:\n            gloss[sense.name()] += word_tokenize(sense.definition())\n\n        return gloss\n\n    def getAll(self, word):\n        senses = self.getSenses(word)\n\n        if senses == []:\n            return {word.lower(): senses}\n\n        return self.getGloss(senses)\n\n    def Score(self, set1, set2):\n        # Base\n        overlap = 0\n\n        # Step\n        for word in set1:\n            if word in set2:\n                overlap += 1\n\n        return overlap\n\n    def overlapScore(self, word1, word2):\n\n        gloss_set1 = self.getAll(word1)\n        if self.meanings[word2] == '':\n            gloss_set2 = self.getAll(word2)\n        else:\n            # print 'here'\n            gloss_set2 = self.getGloss([wn.synset(self.meanings[word2])])\n\n        # print gloss_set2\n\n        score = {}\n        for i in gloss_set1.keys():\n            score[i] = 0\n            for j in gloss_set2.keys():\n                score[i] += self.Score(gloss_set1[i], gloss_set2[j])\n\n        bestSense = None\n        max_score = 0\n        for i in gloss_set1.keys():\n            if score[i] > max_score:\n                max_score = score[i]\n                bestSense = i\n\n        return bestSense, max_score\n\n    def lesk(self, word, sentence):\n        maxOverlap = 0\n        context = sentence\n        word_sense = []\n        meaning = {}\n\n        senses = self.getSenses(word)\n\n        for sense in senses:\n            meaning[sense.name()] = 0\n\n        for word_context in context:\n            if not word == word_context:\n                score = self.overlapScore(word, word_context)\n                if score[0] == None:\n                    continue\n                meaning[score[0]] += score[1]\n\n        if senses == []:\n            return word, None, None\n\n        self.meanings[word] = max(meaning.keys(), key=lambda x: meaning[x])\n\n        return word, self.meanings[word], wn.synset(self.meanings[word]).definition()"},{"cell_type":"markdown","metadata":{"_cell_guid":"13939991-41ba-47f0-aab0-8bd6079aafe2"},"source":"Now, we need some similarity measures which define how related the two words are (after their best senses have been calculated). I have used two different similarity measures, one is path similarity which calculates the distance in the words in the hyponym taxonymy graph.\n\nWup similarity is the Wu & Palmer's similarity."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd0e808d-6ea8-a818-8cec-50a27c9f4436"},"outputs":[],"source":"import math\nimport numpy as np\nfrom scipy import spatial\nfrom nltk.corpus import wordnet as wn\nfrom nltk.metrics import edit_distance\n\ndef path(set1, set2):\n    return wn.path_similarity(set1, set2)\n\n\ndef wup(set1, set2):\n    return wn.wup_similarity(set1, set2)\n\n\ndef edit(word1, word2):\n    if float(edit_distance(word1, word2)) == 0.0:\n        return 0.0\n    return 1.0 / float(edit_distance(word1, word2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0bd342c-9ce5-9b8e-dbe0-32bfdb37acbf"},"outputs":[],"source":"def computePath(q1, q2):\n\n    R = np.zeros((len(q1), len(q2)))\n\n    for i in range(len(q1)):\n        for j in range(len(q2)):\n            if q1[i][1] == None or q2[j][1] == None:\n                sim = edit(q1[i][0], q2[j][0])\n            else:\n                sim = path(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n\n            if sim == None:\n                sim = edit(q1[i][0], q2[j][0])\n\n            R[i, j] = sim\n\n    # print R\n\n    return R\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"552e2e52-1092-a3f1-08af-ec2a48fb8400"},"outputs":[],"source":"def computeWup(q1, q2):\n\n    R = np.zeros((len(q1), len(q2)))\n\n    for i in range(len(q1)):\n        for j in range(len(q2)):\n            if q1[i][1] == None or q2[j][1] == None:\n                sim = edit(q1[i][0], q2[j][0])\n            else:\n                sim = wup(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n\n            if sim == None:\n                sim = edit(q1[i][0], q2[j][0])\n\n            R[i, j] = sim\n\n    # print R\n\n    return R"},{"cell_type":"markdown","metadata":{"_cell_guid":"eaec7497-6f76-6b1a-8412-4e28c8ed9e82"},"source":"This is the fifth part of the algorithm mentioned below. Combines the similarity measures calculated for the two sentences and produces a single similarity score."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08375cac-fb23-74ce-3cbe-65f1fbf22f3d"},"outputs":[],"source":"def overallSim(q1, q2, R):\n\n    sum_X = 0.0\n    sum_Y = 0.0\n\n    for i in range(len(q1)):\n        max_i = 0.0\n        for j in range(len(q2)):\n            if R[i, j] > max_i:\n                max_i = R[i, j]\n        sum_X += max_i\n\n    for i in range(len(q1)):\n        max_j = 0.0\n        for j in range(len(q2)):\n            if R[i, j] > max_j:\n                max_j = R[i, j]\n        sum_Y += max_j\n        \n    if (float(len(q1)) + float(len(q2))) == 0.0:\n        return 0.0\n        \n    overall = (sum_X + sum_Y) / (2 * (float(len(q1)) + float(len(q2))))\n\n    return overall"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a0d0e0b-7b6f-4079-6379-44ec9ea8215d"},"source":"You can call this mostly the main method, the semantic representations here follow this algorithm:\n\n1.   Tokenization. \n\n2.   Perform part of speech tagging.\n\n3.   Word sense disambiguation using adapted Lesk Algorithm.\n\n4.   Building a semantic similarity relations matrix R[m, n] of each pair of word senses, where R[i, j] is the semantic similarity between the most appropriate sense of word at position i of X and the most appropriate sense of word at position j of Y. Thus, R[i,j] is also the weight of edge connect from i to j. If a word does not exist in the dictionary we use the edit-distance similarity instead and output a lower associated weight.\n\n5.   I have not been able to compile the results from both the similarity measures. the paper mentions two methods of doing it, using match(X, Y) -- the matching measure between the word tokens X and Y -- and using the Dice Coefficient. I still figuring out some of the drawbacks of these methods and which one would be better to use. \n**Any help on this is appreciated**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b7e94963-a06a-c1ca-f0b3-3aca7c263f78"},"outputs":[],"source":"def semanticSimilarity(q1, q2):\n\n    tokens_q1, tokens_q2 = tokenize(q1, q2)\n    # stem_q1, stem_q2 = stemmer(tokens_q1, tokens_q2)\n    tag_q1, tag_q2 = posTag(tokens_q1, tokens_q2)\n\n    sentence = []\n    for i, word in enumerate(tag_q1):\n        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n            sentence.append(word[0])\n\n    sense1 = Lesk(sentence)\n    sentence1Means = []\n    for word in sentence:\n        sentence1Means.append(sense1.lesk(word, sentence))\n\n    sentence = []\n    for i, word in enumerate(tag_q2):\n        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n            sentence.append(word[0])\n\n    sense2 = Lesk(sentence)\n    sentence2Means = []\n    for word in sentence:\n        sentence2Means.append(sense2.lesk(word, sentence))\n    # for i, word in enumerate(sentence1Means):\n    #     print sentence1Means[i][0], sentence2Means[i][0]\n\n    R1 = computePath(sentence1Means, sentence2Means)\n    R2 = computeWup(sentence1Means, sentence2Means)\n\n    R = (R1 + R2) / 2\n\n    # print R\n\n    return overallSim(sentence1Means, sentence2Means, R)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cddae36-de6f-2dca-c92f-7276e78a6eb2"},"outputs":[],"source":"import nltk\nSTOP_WORDS = nltk.corpus.stopwords.words()\ndef clean_sentence(val):\n    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n    regex = re.compile('([^\\s\\w]|_)+')\n    sentence = regex.sub('', val).lower()\n    sentence = sentence.split(\" \")\n\n    for word in list(sentence):\n        if word in STOP_WORDS:\n            sentence.remove(word)\n\n    sentence = \" \".join(sentence)\n    return sentence"},{"cell_type":"markdown","metadata":{"_cell_guid":"d6ea5124-bcd6-dede-5fdd-09899cb73b9c"},"source":"This sequential code runs all the methods above. First cleans the data, then takes two sentences and prints out the sentence path similarity output and the WUP Similarity output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb04aee9-e6e5-3d6e-628a-f07cb16a6f48"},"outputs":[],"source":"from sklearn.metrics import log_loss\n\nX_train = pd.read_csv('../input/train.csv')\nX_train = X_train.dropna(how=\"any\")\n\ny = X_train['is_duplicate']\n\nprint('Exported Cleaned train Data, no need for cleaning')\nfor col in ['question1', 'question2']:\n    X_train[col] = X_train[col].apply(clean_sentence)\n\ny_pred = []\ncount = 0\nprint('Calculating similarity for the training data, please wait.')\nfor row in X_train.itertuples():\n    # print row\n    q1 = str(row[4])\n    q2 = str(row[5])\n\n    sim = semanticSimilarity(q1, q2)\n    count += 1\n    if count % 10000 == 0:\n        print(str(count)+\", \"+str(sim)+\", \"+str(row[6]))\n    y_pred.append(sim)\n    \noutput = pd.DataFrame(list(zip(X_train['id'].tolist(), y_pred)), columns=['id', 'similarity'])\noutput.to_csv('semantic_train.csv', index=False)\n\nprint(\"Log Loss Score:\")\nprint(log_loss(y, np.array(y_pred)))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}