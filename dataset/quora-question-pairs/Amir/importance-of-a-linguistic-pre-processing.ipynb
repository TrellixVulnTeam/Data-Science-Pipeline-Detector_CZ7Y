{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python"},"_is_fork":false,"_change_revision":0},"nbformat":4,"cells":[{"metadata":{"_uuid":"fd22aa8ae7eb38352f215db4bb1ca2921eeb197a","_cell_guid":"128cea0c-f1d9-8159-671c-8b3caf442281"},"cell_type":"markdown","source":"I've seen a couple of nice kernels here, but no one explained the importance of a morphological pre-processing of the data. So I decided to compare two approaches of a morphological normalization: stemming and lemmatization. Both of them reduce the word to the regularized form, but a stemming reduces the word to the word stem, and a lemmatization reduces the word to it's morphological root with the help of dictionary lookup. \n\nI evaluate the efficiency of these approaches by comparison their performance with the naive Bag of Means method: every word is encoded with a word embedding vector, and then the common vector of two messages is computed as a mean vector of these vectors. Some of the researches proved that such approach can be a very strong baseline (Faruqui et al., 2014; Yu et al., 2014; Gershman and Tenenbaum, 2015; Kenter and de Rijke, 2015). Then I use obtained vectors as feature vectors to train the classifiers.\n\nI will also make a comparison with a default approach (no morphological pre-processing). "},{"metadata":{"_uuid":"8b527ff50aa8fd50dfda236c3bf911db422d2414","_cell_guid":"9c6b1327-f6fd-3cfe-4215-7186f630ae22"},"cell_type":"markdown","source":"Okay, let's load NLTK and try to implement these two approaches with a Lancaster Stemmer (one of the most popular stemming algorithms) and a WordNet Lemmatizer (based on WordNetâ€™s built-in morphy function):"},{"metadata":{"_execution_state":"busy","_uuid":"b1e3aae041a51c69cfce7cc0ab51c086d18b05d2","collapsed":true,"_cell_guid":"fccb251e-cbea-44ae-58d0-5b335430d348"},"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem import LancasterStemmer\nstemmer =  LancasterStemmer()\nlemmer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc6bd4280cf58b6cee7e1461f7fd5f557649b37a","_cell_guid":"27992bcc-405e-28f5-664c-f75586bcc275"},"cell_type":"markdown","source":"A visible example of how do they work:"},{"metadata":{"_execution_state":"busy","_uuid":"2e41d9c63f36e8808845eec1a674bba9216fead6","collapsed":true,"_cell_guid":"0357ed55-550f-37e5-8f24-d2de6e23c20b"},"cell_type":"code","source":"print(stemmer.stem('dictionaries'))\nprint(lemmer.lemmatize('dictionaries'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85ef559b1ca1587b747994ecbd99bd610752ed52","_cell_guid":"b2282d1d-1631-74c2-7a7d-9ccb415f011d"},"cell_type":"markdown","source":"So, what approach will be better for the given task? Let's see."},{"metadata":{"_uuid":"3dceb17c44593c7556ec6e08b105ea48230c24b4","_cell_guid":"6192a8ba-af1a-d13c-2f3a-1445b9ad83a1"},"cell_type":"markdown","source":"First of all, we need to load modules for linear algebra and data analysis as well as `gensim` (for training a `Word2Vec`, a classic algorithm for obtaining word embeddings). We also need some stuff from `scikit-learn` to teach and evaluate the classifier and `pyplot` to draw plots. `seaborn` will make the plots more beautiful. "},{"metadata":{"_execution_state":"busy","_uuid":"b8d00e1bea57e6f78c4a6411559cd517e579253f","collapsed":true,"_cell_guid":"a37caf49-44d7-890f-bfb7-f6f229ccc0fd"},"cell_type":"code","source":"from gensim import models\nimport numpy as np\nfrom pandas import DataFrame, Series\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom gensim import models\nimport matplotlib.pyplot as plt\nimport seaborn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b67cbb8c03772688fb4b45d9f9f26f1fdb264d8b","_cell_guid":"c7c7731e-eb20-6bba-1783-3e71ccecfa8c"},"cell_type":"markdown","source":"And a little bit more of the linguistic tools! We will use a tokenization( breaking a stream of text up into meaningful elements called tokens, for instance, words) and a stop-word dictionary for English."},{"metadata":{"_execution_state":"busy","_uuid":"0143017892da0258a9014102df8360024ce53484","collapsed":true,"_cell_guid":"fbc71a92-f9bd-5b88-6bf7-1881babb30f6"},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import wordpunct_tokenize, RegexpTokenizer\nstop = stopwords.words('english')\nalpha_tokenizer = RegexpTokenizer('[A-Za-z]\\w+')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b329e8772bd8fb7ec0eba2014cd33318b538ab9b","_cell_guid":"1b9e6efd-1f92-6d31-7bc7-d4b0933cb563"},"cell_type":"markdown","source":"And check if the .csv-files with the data are okay."},{"metadata":{"_execution_state":"busy","_uuid":"11853bb6c42216e467884b7e59308e851d52cc27","collapsed":true,"_cell_guid":"c962afe9-2b44-0cce-4f38-cca4222f8865"},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64d33c3637f81fbe52dc6781c64ad4f35a3a774a","_cell_guid":"ef7cfc2a-138a-2917-97ee-c74003661062"},"cell_type":"markdown","source":"So let's write some code. First of all, let's train a Word2Vec model. We will use the training set as a training corpus (Previously I used the test set, but it uses much more memory while the model trained on it has the same efficiency; thanks to @Gian12 for the notion). This set contains some NaN values, but we can just drop them since in our task their lack is not meaningful."},{"metadata":{"_execution_state":"idle","_uuid":"7c629996cb7d3e4453d531fdaa276a856a61b5a5","collapsed":true,"_cell_guid":"2f2d8885-e125-665c-6990-38be275aff12"},"cell_type":"code","source":"df_train = DataFrame.from_csv('../input/train.csv').dropna()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df5534d5176b890c97cd670fb614c9f8d7244232","_cell_guid":"0a4ee6ef-6da8-ba7e-9c8b-11ed1a40340d"},"cell_type":"markdown","source":"Let's make a list of sentences by merging the questions."},{"metadata":{"_execution_state":"idle","_uuid":"1d556bea94ae2ad40da6642047fade9a535a40a7","collapsed":true,"_cell_guid":"a33d548b-a949-9f5f-1852-3457270a6223"},"cell_type":"code","source":"texts = np.concatenate([df_train.question1.values, df_train.question2.values])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ac7c4570a848a05557af51d78918445b879cff8","_cell_guid":"9d66ba7a-1a1c-cb6d-37f2-0f600be1f8f3"},"cell_type":"markdown","source":"Okay, now we are up to the key method of preprocessing comparation.  It provides lemmatization or stemming depending on the given flag."},{"metadata":{"_execution_state":"idle","_uuid":"3e7812b34442a2fcc599879b5b47e41e7c5aac80","collapsed":true,"_cell_guid":"adb0159b-230b-7574-579d-6c2d4c28825d"},"cell_type":"code","source":"def process_sent(words, lemmatize=False, stem=False):\n    words = words.lower()\n    tokens = alpha_tokenizer.tokenize(words)\n    for index, word in enumerate(tokens):\n        if lemmatize:\n            tokens[index] = lemmer.lemmatize(word)\n        elif stem:\n            tokens[index] = stemmer.stem(word)\n        else:\n            tokens[index] = word\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee310b91e772b509cd7925c1894d6770f0dfcf99","_cell_guid":"2a48fb9f-579f-8b42-9a10-3ea07ce69e9d"},"cell_type":"markdown","source":"And then we can make two different corpora to train the model: stemmed corpus and lemmatized corpus. We will also make a \"clean\" corpus for sure."},{"metadata":{"_execution_state":"busy","_uuid":"53681746924f1020340c1885fbda825f9446d8a0","collapsed":true,"_cell_guid":"afe337ff-1762-77c4-4942-cdebca08cc0f"},"cell_type":"code","source":"corpus_lemmatized = [process_sent(sent, lemmatize=True, stem=False) for sent in texts]","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"busy","_uuid":"447caa9fdd0e3f62db30833e5b9d324e3b8bb2eb","collapsed":true,"_cell_guid":"79614809-add2-00f9-fdae-b5e383aa8a63"},"cell_type":"code","source":"corpus_stemmed = [process_sent(sent, lemmatize=False, stem=True) for sent in texts]","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"busy","_uuid":"9ebf98903d5843f6a6abfae2fdd9537eef6fa293","collapsed":true,"_cell_guid":"639ab373-a8d4-d1ad-d98c-f5fa14a340ef"},"cell_type":"code","source":"corpus = [process_sent(sent) for sent in texts]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"293fdf8ecbaeced70ffe964fea5d7322847a4231","_cell_guid":"f2e10a20-9307-120a-75ea-34a6516712b0"},"cell_type":"markdown","source":"Now let's train the models. I've pre-defined these hyperparameters since models on them have the best performance. You can also try to play with them yourself."},{"metadata":{"_execution_state":"busy","_uuid":"8f79d4299ad1d8392e34225b58892d740dfd00e9","collapsed":true,"_cell_guid":"f20d2e3b-1ac7-48ea-9ddb-2e9658af06d4"},"cell_type":"code","source":"VECTOR_SIZE = 100","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"busy","_uuid":"53a0bd473d65f309bedf2eb6042d650740ff63bc","collapsed":true,"_cell_guid":"da8ca7ed-8d87-4611-dcc1-06220f3d4cb0"},"cell_type":"code","source":"min_count = 10\nsize = VECTOR_SIZE\nwindow = 10","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"busy","_uuid":"6a218c6def8ab970282dc29fcdd8bde4c6dc916d","collapsed":true,"_cell_guid":"cb40b0f4-cb43-8522-faa4-1ed275702271"},"cell_type":"code","source":"model_lemmatized = models.Word2Vec(corpus_lemmatized, min_count=min_count, \n                                   size=size, window=window)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"busy","_uuid":"153a174d9752bd65e24150a78e9e5fb6f7bf25f5","collapsed":true,"_cell_guid":"7a6db5ac-c607-6e3e-b203-b0e0039fbdb6"},"cell_type":"code","source":"model_stemmed = models.Word2Vec(corpus_stemmed, min_count=min_count, \n                                size=size, window=window)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"busy","_uuid":"7af5ddd409b8f5518230acf14de38c9b849daf44","collapsed":true,"_cell_guid":"04d58da8-d262-9281-4a94-a70421683147"},"cell_type":"code","source":"model = models.Word2Vec(corpus, min_count=min_count, \n                                size=size, window=window)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f12ec9a06a37f6be6d4051d5212ba468c60f251","_cell_guid":"21d3f5b9-ad1d-3998-a5cc-4d40c94aa70c"},"cell_type":"markdown","source":"Let's check the result of one of the models."},{"metadata":{"_execution_state":"busy","_uuid":"a3e9fb317457a70af1551086ca1e220aa5d2e2fd","collapsed":true,"_cell_guid":"919ba6e3-9245-ff2a-8ada-7345a56fedd9"},"cell_type":"code","source":"model_lemmatized.most_similar('playstation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0297679ff8ae7ef69a4283bc4edf00c507683aa5","_cell_guid":"06a633ea-bd5f-ca79-3e65-5df8d7f5fb0a"},"cell_type":"markdown","source":"Great! The most similar words seem to be pretty meaningful. So, we have three trained models, we can encode the text data with the vectors - let's make some experiments! Let's make data sets from the loaded data frame. I take a chunk of the traning data because the run of the script on the full data takes too much time."},{"metadata":{"_execution_state":"busy","_uuid":"a054b842d72d2f65a37231f5b22a03372bb4fcf2","collapsed":true,"_cell_guid":"55a669e8-88f7-9897-cb07-45753869299d"},"cell_type":"code","source":"q1 = df_train.question1.values[200000:]\nq2 = df_train.question2.values[200000:]\nY = np.array(df_train.is_duplicate.values)[200000:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e66b413f93a816f2f82d5d8d4d75595a91447c2","_cell_guid":"7da270f7-c112-1972-6f97-a7ba5fb8e137"},"cell_type":"markdown","source":"A little bit modified `preprocess`. Now it returns only words which model's vocabulary contains."},{"metadata":{"_execution_state":"busy","_uuid":"c8cf915ffc935caf2c508e9a6a6418db7359ea77","collapsed":true,"_cell_guid":"a2ec0699-ccd6-c3ef-0d59-4f33f5f9341d"},"cell_type":"code","source":"def preprocess_check(words, lemmatize=False, stem=False):\n    words = words.lower()\n    tokens = alpha_tokenizer.tokenize(words)\n    model_tokens = []\n    for index, word in enumerate(tokens):\n        if lemmatize:\n            lem_word = lemmer.lemmatize(word)\n            if lem_word in model_lemmatized.wv.vocab:\n                model_tokens.append(lem_word)\n        elif stem:\n            stem_word = stemmer.stem(word)\n            if stem_word in model_stemmed.wv.vocab:\n                model_tokens.append(stem_word)\n        else:\n            if word in model.wv.vocab:\n                model_tokens.append(word)\n    return model_tokens","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc871b0efebee8f86b5da97955d68c5fab7fa812","_cell_guid":"68105b0a-8bad-3214-2a37-db45f997911b"},"cell_type":"markdown","source":"This method will help to obtaining a bag of means by vectorising the messages."},{"metadata":{"_execution_state":"busy","_uuid":"44d7ad8cbef01cd8f3b61863f731b8f2caaaacdd","collapsed":true,"_cell_guid":"8a6434ce-8b23-90ce-bf09-7019914741a2"},"cell_type":"code","source":"old_err_state = np.seterr(all='raise')\n\ndef vectorize(words, words_2, model, num_features, lemmatize=False, stem=False):\n    features = np.zeros((num_features), dtype='float32')\n    words_amount = 0\n    \n    words = preprocess_check(words, lemmatize, stem)\n    words_2 = preprocess_check(words_2, lemmatize, stem)\n    for word in words: \n            words_amount = words_amount + 1\n            features = np.add(features, model[word])\n    for word in words_2: \n            words_amount = words_amount + 1\n            features = np.add(features, model[word])\n    try:\n        features = np.divide(features, words_amount)\n    except FloatingPointError:\n        features = np.zeros(num_features, dtype='float32')\n    return features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bc0948f85d504cc7b9efaa1d3c2ad00e3dd1acd","_cell_guid":"b173ea37-b692-532b-0431-1235b5e07b23"},"cell_type":"markdown","source":"And now we can obtain the features matrices."},{"metadata":{"_execution_state":"busy","_uuid":"7911d119260c9bf4c18442f5eb7f08101d93e3ab","collapsed":true,"_cell_guid":"c819209c-0972-35d6-e205-c71b119db67a"},"cell_type":"code","source":"X_lem = []\nfor index, sentence in enumerate(q1):\n    X_lem.append(vectorize(sentence, q2[index], model_lemmatized, VECTOR_SIZE, True, False))\nX_lem = np.array(X_lem)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"busy","_uuid":"98aad3e1873fd80f5baa85ae76dcc224dff17eea","collapsed":true,"_cell_guid":"be86dc59-5516-9b87-2822-cf6e600a7a54"},"cell_type":"code","source":"X_stem = []\nfor index, sentence in enumerate(q1):\n    X_stem.append(vectorize(sentence, q2[index], model_stemmed, VECTOR_SIZE, False, True))\nX_stem = np.array(X_stem)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"busy","_uuid":"1d0818ecf868ee89a937be1102418dcbb477c443","collapsed":true,"_cell_guid":"475b0393-b426-f16c-7b86-45b9330035a0"},"cell_type":"code","source":"X = []\nfor index, sentence in enumerate(q1):\n    X.append(vectorize(sentence, q2[index], model, VECTOR_SIZE))\nX = np.array(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97dc7fc88c26e4cc892bfa7c3f374d1dbdc569d8","_cell_guid":"86cafd36-28de-3a01-966f-be57dcb495a5"},"cell_type":"markdown","source":"That's almost all! Now we can train the classifier and evaluate it's performance. It's better to use a metric classifier because we are performing operations in the vector space, so I choose a Logistic Regression. But of course you can try a something different and see what can change. \n\nI also use cross-validation to train and to evaluate on the same data set. "},{"metadata":{"_execution_state":"busy","_uuid":"5fa0238134bcf0f6cbcf44aae28ff0928688036b","collapsed":true,"_cell_guid":"176faf5f-9e3e-2dbe-da95-d261f631a311"},"cell_type":"code","source":"results = []\ntitle_font = {'size':'10', 'color':'black', 'weight':'normal',\n                  'verticalalignment':'bottom'} \naxis_font = {'size':'10'}\n\nplt.figure(figsize=(10, 5))\nplt.xlabel('Training examples', **axis_font)\nplt.ylabel('Accuracy',  **axis_font)\nplt.tick_params(labelsize=10)\n\nfor X_set, name, lstyle in [(X_lem, 'Lemmatizaton', 'dotted'),\n            (X_stem, 'Stemming', 'dashed'),\n            (X, 'Default', 'dashdot'),\n            ]:\n    estimator = LogisticRegression(C = 1)\n    cv = ShuffleSplit(n_splits=6, test_size=0.01, random_state=0)\n    train_sizes=np.linspace(0.01, 0.99, 6)\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X_set, Y, cv=cv, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    results.append({'preprocessing' : name, 'score' : train_scores_mean[-1]})\n    plt.plot(train_sizes, train_scores_mean, label=name, linewidth=5, linestyle=lstyle)\n   \n\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"600a9de1187a611a679b74a0e625ce4575176bad","_cell_guid":"9cf12d6b-ae09-4633-6d46-6ed585e705be"},"cell_type":"markdown","source":"So, the lemmatized model outperformed the \"clear\" model! And the stemmed model showed the worst result. Why does it happen?\n\nWell, any morphological pre-processing of the training data for the model reduces the amount of information that model can obtain from the corpus. Some of the information, like the difference in morphological roots of the same words, seems to be not necessary, so it is better to remove it. This removal is a mush-have in synthetic languages (languages with high morpheme-per-word ratio, like Russian), and, as we can see, it is also pretty helpful in our task. \n\nThe same thing about stemming. Stemming further reduces the amount of information, making one stem for the different word forms. Sometimes this is helpful, but sometimes this can bring noise to the model since some stems of the different words can be ambiguous, and the model can't be able to separate \"playstation\" and, say, \"play\".\n\nIn other words, there is no silver bullet, and you should always check various option of pre-processing if you want to reach the best performance. However, lemmatisation nine times out of ten will increase the performance of your model.\n\nHowever, the logarithmic loss of my approach is not very high, but you can use this notebook as a baseline and try to beat it's score yourself! Just download it and uncomment the commented strings (because Kaggle doesn't allow to use so much memory)"},{"metadata":{"_execution_state":"busy","_uuid":"3c50769e52e6cac4e36f2b930bab1eb711b0012f","collapsed":true,"_cell_guid":"eac27795-998a-ba84-6d52-4c6b7dd99840"},"cell_type":"code","source":"clf = LogisticRegression(C = 1)\nclf.fit(X, Y)\n\n#df_test = DataFrame.from_csv('../input/test.csv').fillna('None')\nq1 = df_train.question1.values[:100]\nq2 = df_train.question2.values[:100]\n#q1 = df_test.question1.values\n#q2 = df_test.question2.values\n\nX_test = []\nfor index, sentence in enumerate(q1):\n    X_test.append(vectorize(sentence, q2[index], model, VECTOR_SIZE))\nX_test = np.array(X_test)\n\nresult = clf.predict(X_test)\n\nsub = DataFrame()\nsub['is_duplicate'] = result\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a806dbc634d0c5de5a102ceb3f982ad9ee34ff25","_cell_guid":"36601a1c-f930-c33c-716c-9e97e8d3ccf3"},"cell_type":"markdown","source":"Thanks for reading this notebook. I'm glad if it helped you to learn something new.\n\nI will highly appreciate any critique or feedback. Feel free to write your thoughts at the comments section!"}],"nbformat_minor":1}