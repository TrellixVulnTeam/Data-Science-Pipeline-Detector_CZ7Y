{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import warnings\nfrom random import shuffle\nimport os\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc29314ed243f5669eb2ddd5f282c585735e856d","collapsed":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\nprint (train_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60d3abc8bd1e41fa66612b93f507fac69b997914","collapsed":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/test.csv')\nprint (test_data.shape)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8f8fb75c66bdc6196f48bf64e498fbe9e66ecd7c"},"cell_type":"code","source":"train_data = train_data.drop(['id', 'qid1', 'qid2'], 1)\ntest_data = test_data.drop(['test_id'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ebfe1e927254ae370b35df51982a10674018799c"},"cell_type":"code","source":"train_data = train_data.fillna('')\ntest_data = test_data.fillna('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"81ab04d9731804fa26a746833da791f5e5018d32"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"73890279601bcf0f5e32fd345f0a532bf660252b"},"cell_type":"code","source":"import pickle\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom string import punctuation\nfrom nltk.stem import SnowballStemmer\n\nstop_words = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"75de017d5d6f40fe4e586c3e518f86cc9c70aeab"},"cell_type":"code","source":"import re\n\ndef text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n    # Clean the text, with the option to remove stop_words and to stem words.\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n    text = re.sub(r\"what's\", \"\", text)\n    text = re.sub(r\"What's\", \"\", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"I'm\", \"I am\", text)\n    text = re.sub(r\" m \", \" am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"60k\", \" 60000 \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e-mail\", \"email\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = re.sub(r\"quikly\", \"quickly\", text)\n    text = re.sub(r\" usa \", \" America \", text)\n    text = re.sub(r\" USA \", \" America \", text)\n    text = re.sub(r\" u s \", \" America \", text)\n    text = re.sub(r\" uk \", \" England \", text)\n    text = re.sub(r\" UK \", \" England \", text)\n    text = re.sub(r\"india\", \"India\", text)\n    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n    text = re.sub(r\"china\", \"China\", text)\n    text = re.sub(r\"chinese\", \"Chinese\", text) \n    text = re.sub(r\"imrovement\", \"improvement\", text)\n    text = re.sub(r\"intially\", \"initially\", text)\n    text = re.sub(r\"quora\", \"Quora\", text)\n    text = re.sub(r\" dms \", \"direct messages \", text)  \n    text = re.sub(r\"demonitization\", \"demonetization\", text) \n    text = re.sub(r\"actived\", \"active\", text)\n    text = re.sub(r\"kms\", \" kilometers \", text)\n    text = re.sub(r\"KMs\", \" kilometers \", text)\n    text = re.sub(r\" cs \", \" computer science \", text) \n    text = re.sub(r\" upvotes \", \" up votes \", text)\n    text = re.sub(r\" iPhone \", \" phone \", text)\n    text = re.sub(r\"\\0rs \", \" rs \", text) \n    text = re.sub(r\"calender\", \"calendar\", text)\n    text = re.sub(r\"ios\", \"operating system\", text)\n    text = re.sub(r\"gps\", \"GPS\", text)\n    text = re.sub(r\"gst\", \"GST\", text)\n    text = re.sub(r\"programing\", \"programming\", text)\n    text = re.sub(r\"bestfriend\", \"best friend\", text)\n    text = re.sub(r\"dna\", \"DNA\", text)\n    text = re.sub(r\"III\", \"3\", text) \n    text = re.sub(r\"the US\", \"America\", text)\n    text = re.sub(r\"Astrology\", \"astrology\", text)\n    text = re.sub(r\"Method\", \"method\", text)\n    text = re.sub(r\"Find\", \"find\", text) \n    text = re.sub(r\"banglore\", \"Banglore\", text)\n    text = re.sub(r\" J K \", \" JK \", text)\n    \n    # Remove punctuation from text\n    text = ''.join([c for c in text if c not in punctuation])\n    \n    # Optionally, remove stop words\n    if remove_stop_words:\n        text = text.split()\n        text = [w for w in text if not w in stop_words]\n        text = \" \".join(text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1520ad1dc00bbf86742521eb77047f453b4619fd"},"cell_type":"code","source":"train_data['question1_modified'] = train_data.apply(lambda x: text_to_wordlist(x['question1']), axis = 1)\ntrain_data['question2_modified'] = train_data.apply(lambda x: text_to_wordlist(x['question2']), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9dfb71d6827834aec8e76201a333be74445fef45"},"cell_type":"code","source":"test_data['question1_modified'] = test_data.apply(lambda x: text_to_wordlist(x['question1']), axis = 1)\ntest_data['question2_modified'] = test_data.apply(lambda x: text_to_wordlist(x['question2']), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9cda48f8e68aaee4cd6692dcba4720020e8c901f"},"cell_type":"code","source":"import pickle\n\npickle.dump(train_data['question1_modified'], open('pickle_train_question1_modified', 'wb'))\npickle.dump(train_data['question2_modified'], open('pickle_train_question2_modified', 'wb'))\n\npickle.dump(test_data['question1_modified'], open('pickle_test_question1_modified', 'wb'))\npickle.dump(test_data['question2_modified'], open('pickle_test_question2_modified', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb471769cad5a388eb9a09a723b3b287589e9c83","collapsed":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntrain_text = np.hstack([train_data.question1_modified, train_data.question2_modified])\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0a180edc4f1d7198ba43a096bb89cef84a9606ab"},"cell_type":"code","source":"train_data['tokenizer_1'] = tokenizer.texts_to_sequences(train_data.question1_modified)\ntrain_data['tokenizer_2'] = tokenizer.texts_to_sequences(train_data.question2_modified)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"09578f197d0c3a903ecdb9d2303dd86cab71b10c"},"cell_type":"code","source":"test_text = np.hstack([test_data.question1_modified, test_data.question2_modified])\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7b1e4e8a7a518e0f8fd29a6f123ca02e1fd4c40d"},"cell_type":"code","source":"test_data['tokenizer_1'] = tokenizer.texts_to_sequences(test_data.question1_modified)\ntest_data['tokenizer_2'] = tokenizer.texts_to_sequences(test_data.question2_modified)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7f2c8bc78b2d307488ec10245fa948594d4bfdf5"},"cell_type":"code","source":"train_data['tokenizer'] = train_data['tokenizer_1'] + train_data['tokenizer_2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fa9284978e8ae8f4169a4629a9056b8416bd931","collapsed":true},"cell_type":"code","source":"test_data['tokenizer'] = test_data['tokenizer_1'] + test_data['tokenizer_2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff81e280469eb89f3e299c170831a6a01ec25dd8","collapsed":true},"cell_type":"code","source":"print (train_data['tokenizer_1'][0])\nprint (train_data['tokenizer_2'][0])\nprint (train_data['tokenizer'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e1d32fffbd3e75b504b6f2cb80f110fa9761f3f","collapsed":true},"cell_type":"code","source":"print (test_data['tokenizer_1'][0])\nprint (test_data['tokenizer_2'][0])\nprint (test_data['tokenizer'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8e02db97360cd71e1dfd0752a6d1ffaa311a72b8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a47046589d6fe7fbdb90d5e2bcf6c913bc9f4bb","collapsed":true},"cell_type":"code","source":"max_length = 500\nmax_token = np.max([np.max(train_data.tokenizer.max()),np.max(test_data.tokenizer.max())])\nprint (max_length, max_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7daab1e2121d9a20e717c44db7689608af8994af"},"cell_type":"code","source":"y_train = train_data[['is_duplicate']]\nX_train = train_data[['tokenizer']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"50cca97cb06418e12b7be983da89beeeacbc5f33"},"cell_type":"code","source":"X_test = test_data[['tokenizer']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cafda363179358a3e7cff7a09fad1ed2fa491ccd","collapsed":true},"cell_type":"code","source":"from keras.preprocessing import sequence\n\nX_train = sequence.pad_sequences(X_train.tokenizer, maxlen = max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"419b876ee895b144de6568c6bd7b321a330795a8","collapsed":true},"cell_type":"code","source":"X_test = sequence.pad_sequences(X_test.tokenizer, maxlen = max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3529a32fe7e7aacd048a1e188590d63c83ae98d0"},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Input, Embedding, Dense, Dropout, LSTM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d0977f526b180cc5e9d12ff64be9b442a3fec134"},"cell_type":"code","source":"model_1 = Sequential()\nmodel_1.add(Embedding(max_token, 32))\nmodel_1.add(Dropout(0.3))\n\nmodel_1.add(LSTM(32))\n\nmodel_1.add(Dropout(0.3))\nmodel_1.add(Dense(1, activation = 'sigmoid'))\nmodel_1.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c055352fa65defaabdfed983cea7b61eaff5b15","collapsed":true},"cell_type":"code","source":"model_1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1580bc396eb160450b63b24152d257fe8afda9af","collapsed":true},"cell_type":"code","source":"model_1.fit(X_train, y_train, epochs = 5, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c438089a07ab0da22b6dd7023dc5a774bcc088e4"},"cell_type":"code","source":"prediction = model.predict(X_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}