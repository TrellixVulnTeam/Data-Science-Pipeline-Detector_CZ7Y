{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nfrom bs4 import BeautifulSoup\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T16:09:19.816737Z","iopub.execute_input":"2022-03-12T16:09:19.81731Z","iopub.status.idle":"2022-03-12T16:09:21.02126Z","shell.execute_reply.started":"2022-03-12T16:09:19.817218Z","shell.execute_reply":"2022-03-12T16:09:21.020486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install distance","metadata":{"execution":{"iopub.status.busy":"2022-03-12T16:09:24.400633Z","iopub.execute_input":"2022-03-12T16:09:24.400914Z","iopub.status.idle":"2022-03-12T16:09:36.976644Z","shell.execute_reply.started":"2022-03-12T16:09:24.400886Z","shell.execute_reply":"2022-03-12T16:09:36.975656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/quora-question-pairs/train.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T16:12:42.736246Z","iopub.execute_input":"2022-03-12T16:12:42.736751Z","iopub.status.idle":"2022-03-12T16:12:44.911467Z","shell.execute_reply.started":"2022-03-12T16:12:42.736701Z","shell.execute_reply":"2022-03-12T16:12:44.910533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = df.sample(30000,random_state=2)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T16:12:53.201264Z","iopub.execute_input":"2022-03-12T16:12:53.201544Z","iopub.status.idle":"2022-03-12T16:12:53.230784Z","shell.execute_reply.started":"2022-03-12T16:12:53.201512Z","shell.execute_reply":"2022-03-12T16:12:53.230167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T16:12:57.968586Z","iopub.execute_input":"2022-03-12T16:12:57.968864Z","iopub.status.idle":"2022-03-12T16:12:57.991514Z","shell.execute_reply.started":"2022-03-12T16:12:57.968835Z","shell.execute_reply":"2022-03-12T16:12:57.99096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(q):\n    \n    q = str(q).lower().strip()\n    \n    # Replace certain special characters with their string equivalents\n    q = q.replace('%', ' percent')\n    q = q.replace('$', ' dollar ')\n    q = q.replace('₹', ' rupee ')\n    q = q.replace('€', ' euro ')\n    q = q.replace('@', ' at ')\n    \n    # The pattern '[math]' appears around 900 times in the whole dataset.\n    q = q.replace('[math]', '')\n    \n    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n    q = q.replace(',000,000,000 ', 'b ')\n    q = q.replace(',000,000 ', 'm ')\n    q = q.replace(',000 ', 'k ')\n    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n    \n    # Decontracting words\n    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n    # https://stackoverflow.com/a/19794953\n    contractions = { \n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"can not\",\n    \"can't've\": \"can not have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n    }\n\n    q_decontracted = []\n\n    for word in q.split():\n        if word in contractions:\n            word = contractions[word]\n\n        q_decontracted.append(word)\n\n    q = ' '.join(q_decontracted)\n    q = q.replace(\"'ve\", \" have\")\n    q = q.replace(\"n't\", \" not\")\n    q = q.replace(\"'re\", \" are\")\n    q = q.replace(\"'ll\", \" will\")\n    \n    # Removing HTML tags\n    q = BeautifulSoup(q)\n    q = q.get_text()\n    \n    # Remove punctuations\n    pattern = re.compile('\\W')\n    q = re.sub(pattern, ' ', q).strip()\n\n    \n    return q\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess(\"I've already! wasn't <b>done</b>?\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['question1'] = new_df['question1'].apply(preprocess)\nnew_df['question2'] = new_df['question2'].apply(preprocess)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['q1_len'] = new_df['question1'].str.len() \nnew_df['q2_len'] = new_df['question2'].str.len()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['q1_num_words'] = new_df['question1'].apply(lambda row: len(row.split(\" \")))\nnew_df['q2_num_words'] = new_df['question2'].apply(lambda row: len(row.split(\" \")))\nnew_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def common_words(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return len(w1 & w2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['word_common'] = new_df.apply(common_words, axis=1)\nnew_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def total_words(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return (len(w1) + len(w2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['word_total'] = new_df.apply(total_words, axis=1)\nnew_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['word_share'] = round(new_df['word_common']/new_df['word_total'],2)\nnew_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Advanced Features\nfrom nltk.corpus import stopwords\n\ndef fetch_token_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    SAFE_DIV = 0.0001 \n\n    STOP_WORDS = stopwords.words(\"english\")\n    \n    token_features = [0.0]*8\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    return token_features\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_features = new_df.apply(fetch_token_features, axis=1)\n\nnew_df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\nnew_df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\nnew_df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\nnew_df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\nnew_df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\nnew_df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\nnew_df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\nnew_df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import distance\n\ndef fetch_length_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    length_features = [0.0]*3\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return length_features\n    \n    # Absolute length features\n    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n    \n    strs = list(distance.lcsubstrings(q1, q2))\n    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n    \n    return length_features\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length_features = new_df.apply(fetch_length_features, axis=1)\n\nnew_df['abs_len_diff'] = list(map(lambda x: x[0], length_features))\nnew_df['mean_len'] = list(map(lambda x: x[1], length_features))\nnew_df['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install fuzzywuzzy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fuzzy Features\nfrom fuzzywuzzy import fuzz\n\ndef fetch_fuzzy_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    fuzzy_features = [0.0]*4\n    \n    # fuzz_ratio\n    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n\n    # fuzz_partial_ratio\n    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n\n    # token_sort_ratio\n    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n\n    # token_set_ratio\n    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n\n    return fuzzy_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fuzzy_features = new_df.apply(fetch_fuzzy_features, axis=1)\n\n# Creating new feature columns for fuzzy features\nnew_df['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\nnew_df['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\nnew_df['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\nnew_df['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(new_df.shape)\nnew_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(new_df[['ctc_min', 'cwc_min', 'csc_min', 'is_duplicate']],hue='is_duplicate')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(new_df[['ctc_max', 'cwc_max', 'csc_max', 'is_duplicate']],hue='is_duplicate')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(new_df[['last_word_eq', 'first_word_eq', 'is_duplicate']],hue='is_duplicate')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(new_df[['mean_len', 'abs_len_diff','longest_substr_ratio', 'is_duplicate']],hue='is_duplicate')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(new_df[['fuzz_ratio', 'fuzz_partial_ratio','token_sort_ratio','token_set_ratio', 'is_duplicate']],hue='is_duplicate')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nX = MinMaxScaler().fit_transform(new_df[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = new_df['is_duplicate'].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=x_df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne3d = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install plotly","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\ntrace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ques_df = new_df[['question1','question2']]\nques_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = new_df.drop(columns=['id','qid1','qid2','question1','question2'])\nprint(final_df.shape)\nfinal_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# merge texts\nquestions = list(ques_df['question1']) + list(ques_df['question2'])\n\ncv = CountVectorizer(max_features=3000)\nq1_arr, q2_arr = np.vsplit(cv.fit_transform(questions).toarray(),2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df1 = pd.DataFrame(q1_arr, index= ques_df.index)\ntemp_df2 = pd.DataFrame(q2_arr, index= ques_df.index)\ntemp_df = pd.concat([temp_df1, temp_df2], axis=1)\ntemp_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = pd.concat([final_df, temp_df], axis=1)\nprint(final_df.shape)\nfinal_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(final_df.iloc[:,1:].values,final_df.iloc[:,0].values,test_size=0.2,random_state=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\naccuracy_score(y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install xgboost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\ny_pred1 = xgb.predict(X_test)\naccuracy_score(y_test,y_pred1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for random forest model\nconfusion_matrix(y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for xgboost model\nconfusion_matrix(y_test,y_pred1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_common_words(q1,q2):\n    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n    return len(w1 & w2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_total_words(q1,q2):\n    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n    return (len(w1) + len(w2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_fetch_token_features(q1,q2):\n    \n    SAFE_DIV = 0.0001 \n\n    STOP_WORDS = stopwords.words(\"english\")\n    \n    token_features = [0.0]*8\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    return token_features\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_fetch_length_features(q1,q2):\n    \n    length_features = [0.0]*3\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return length_features\n    \n    # Absolute length features\n    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n    \n    strs = list(distance.lcsubstrings(q1, q2))\n    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n    \n    return length_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_fetch_fuzzy_features(q1,q2):\n    \n    fuzzy_features = [0.0]*4\n    \n    # fuzz_ratio\n    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n\n    # fuzz_partial_ratio\n    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n\n    # token_sort_ratio\n    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n\n    # token_set_ratio\n    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n\n    return fuzzy_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def query_point_creator(q1,q2):\n    \n    input_query = []\n    \n    # preprocess\n    q1 = preprocess(q1)\n    q2 = preprocess(q2)\n    \n    # fetch basic features\n    input_query.append(len(q1))\n    input_query.append(len(q2))\n    \n    input_query.append(len(q1.split(\" \")))\n    input_query.append(len(q2.split(\" \")))\n    \n    input_query.append(test_common_words(q1,q2))\n    input_query.append(test_total_words(q1,q2))\n    input_query.append(round(test_common_words(q1,q2)/test_total_words(q1,q2),2))\n    \n    # fetch token features\n    token_features = test_fetch_token_features(q1,q2)\n    input_query.extend(token_features)\n    \n    # fetch length based features\n    length_features = test_fetch_length_features(q1,q2)\n    input_query.extend(length_features)\n    \n    # fetch fuzzy features\n    fuzzy_features = test_fetch_fuzzy_features(q1,q2)\n    input_query.extend(fuzzy_features)\n    \n    # bow feature for q1\n    q1_bow = cv.transform([q1]).toarray()\n    \n    # bow feature for q2\n    q2_bow = cv.transform([q2]).toarray()\n    \n    \n    \n    return np.hstack((np.array(input_query).reshape(1,22),q1_bow,q2_bow))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q1 = 'Where is the capital of India?'\nq2 = 'What is the current capital of Pakistan?'\nq3 = 'Which city serves as the capital of India?'\nq4 = 'What is the business capital of India?'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf.predict(query_point_creator(q1,q2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\npickle.dump(rf,open('model.pkl','wb'))\npickle.dump(cv,open('cv.pkl','wb'))\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}