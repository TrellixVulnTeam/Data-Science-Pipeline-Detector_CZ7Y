{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"6479f538-bd33-94e6-701a-7f06455a00fa"},"source":"[GitHub repo][1]\n\nA lot of these cells will not work in the Kaggle environment. You need to download multiple files and I am unaware of a way of doing so here. Download the ipynb file, move the first two cells to their own python files and remove them from your notebook.\n\nUsing the instructions from [this page][2]. You can train a model ([my model][3]) for vectorizing words. This file is used below.\n\n\n  [1]: https://github.com/apjansing/Quora-Question-Pairs/\n  [2]: http://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim\n  [3]: https://drive.google.com/open?id=0B5yf6IhYey8cWU1yNlNjeHY5d2s"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45512bca-6cd3-105c-40e2-65719c8da400"},"outputs":[],"source":"# a function to test the similarity of the words\n# Some of the values are hard-coded, but this is a first draft of my code and I will make something better\n## in the near future.\ndef testQuestions(first, second):\n    count = 0.0\n    for f in first:\n        for s in second:\n            try:\n                sim = M.similarity(f, s)\n                if sim > .45:\n                    count += 1.0\n                    #print \"Similarity between\", f, \"and\", s, \"is\", M.similarity(f, s)    \n            except:\n                pass\n    try:\n        if count/len(first) > .8 or count/len(second) > .8:\n            return 1\n        return 0\n    except:\n        return 0"},{"cell_type":"markdown","metadata":{"_cell_guid":"241d7cef-597a-1526-95a1-5e8316622046"},"source":"Here is the code for processing your Wiki data.\n\nRun it with \n**python process_wiki.py enwiki-latest-pages-articles.xml.bz2 wiki.en.text**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d84e96c5-723b-7e3c-18b3-7950afdae110"},"outputs":[],"source":"#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# process_wiki.py\nimport logging\nimport os.path\nimport sys\n \nfrom gensim.corpora import WikiCorpus\n \nif __name__ == '__main__':\n    program = os.path.basename(sys.argv[0])\n    logger = logging.getLogger(program)\n\n    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n    logging.root.setLevel(level=logging.INFO)\n    logger.info(\"running %s\" % ' '.join(sys.argv))\n\n    # check and process input arguments\n    if len(sys.argv) < 3:\n        print globals()['__doc__'] % locals()\n        sys.exit(1)\n    inp, outp = sys.argv[1:3]\n    space = \" \"\n    i = 0\n\n    output = open(outp, 'w')\n    wiki = WikiCorpus(inp, lemmatize=False, dictionary={})\n    for text in wiki.get_texts():\n        output.write(space.join(text) + \"\\n\")\n        i = i + 1\n        if (i % 10000 == 0):\n            logger.info(\"Saved \" + str(i) + \" articles\")\n\n    output.close()\n    logger.info(\"Finished Saved \" + str(i) + \" articles\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"39d936b8-12f3-75db-4ed1-ff931bc1403a"},"source":"Here is the code to creating your model from the processed Wiki data.\n\nRun it with **python train_word2vec_model.py wiki.en.text wiki.en.word2vec.model**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29083105-04e2-6b83-0106-21fea61f1cac"},"outputs":[],"source":"#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# train_word2vec_model.py\nimport logging\nimport os.path\nimport sys\nimport multiprocessing\n \nfrom gensim.corpora import  WikiCorpus\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\n \n \nif __name__ == '__main__':\n    program = os.path.basename(sys.argv[0])\n    logger = logging.getLogger(program)\n\n    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n    logging.root.setLevel(level=logging.INFO)\n    logger.info(\"running %s\" % ' '.join(sys.argv))\n\n    # check and process input arguments\n\n    if len(sys.argv) < 3:\n        print globals()['__doc__'] % locals()\n        sys.exit(1)\n    inp, outp = sys.argv[1:3]\n\n    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count())\n\n    # trim unneeded model memory = use (much) less RAM\n    model.init_sims(replace=True)\n\n    model.save(outp)"},{"cell_type":"markdown","metadata":{"_cell_guid":"763b7bf3-a167-0e95-bd97-8674ab455119"},"source":"Now that we've trained a model on a large corpus of text, we need to think of how we want to interrogate the information provided. My first thought was that stopwords do no provide a lot of information. Not to say that stopwords are useless, but removing them may speed things up."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2ed9b317-b79a-9535-93ba-54a37c7b9f9d"},"outputs":[],"source":"from nltk.corpus import stopwords\nimport nltk\nimport re\nimport pandas as pd\n\n## uncomment if you need to download the nltk gives you problems about not having the \"english\" stopwords corpus\n#nltk.download(\"stopwords\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93879b9d-850a-6e06-857d-964fe094b760"},"outputs":[],"source":"def formatPart(part):\n\tpart = removeSymbols(part)\n\tpart = removeStopWords(part.split(\" \"))\t\n\treturn part.strip()\n\ndef removeSymbols(line):\n\treturn re.sub('[^\\w]', ' ', line)\n\n\ndef removeStopWords(words):\n\tline = \"\"\n\tfor word in words:\n\t\tif word not in stopwords.words('english'):\n\t\t\tline = line + \" \" + word\n\treturn line\n\ndef processPart(part, j):\n\tif j is 3 or j is 4:\n\t\treturn formatPart(part)\n\telse:\n\t\treturn part\n\ndef getHeader(noStop):\n\twith open(\"train.csv\", \"r\") as F:\n\t\tfor f in F:\n\t\t\tnoStop.write(f)\n\t\t\tbreak"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6dd36341-d239-92ab-9bc7-e8c45c669357"},"outputs":[],"source":"trainingSet = pd.read_csv(\"train.csv\", quotechar='\"').as_matrix()\nrows = len(trainingSet[:,0])\n#L = []\nwith open(\"trainNoStopWords.csv\", \"w\") as noStop:\n\tgetHeader(noStop)\n\tfor i in range(rows):\n\t\tl = []\n\t\tfor j in range(len(trainingSet[i,:])):\n\t\t\tl.append(processPart(str(trainingSet[i,j]), j))\n\t\tnoStop.write(\",\".join(l) + \"\\n\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"071a9363-a535-145f-96ee-7728d6590081"},"outputs":[],"source":"# other imports you may need (change your path to the location of your model)\nimport gensim as gm\nimport numpy as np\nimport scipy as sp\n\nM = gm.models.Word2Vec.load(\"/home/alex/Documents/Wiki dump/wiki.en.word2vec.model\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"b3f9e511-b77d-ed3b-4666-4cb12d129aaa"},"source":"Now, using the version of the training data with the stopwords removed, we can test for some similarity between the target classifications and the classifications proved by the cell below."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6120583-b8f4-c137-9070-e9d7d60e526d"},"outputs":[],"source":"with open(\"trainNoStopWords.csv\", \"r\") as F:\n    correct = 0.0\n    total = 0.0\n    header = True\n    for f in F:\n        if header:\n            header = False\n        else:\n            total += 1.0\n            parts = f.split(\",\")\n            if testQuestions(parts[3], parts[4]) == int(parts[5]):\n                correct += 1.0\n                if int(correct) % 10000 is 0:\n                    print(correct/total)\n    print(correct/total)"},{"cell_type":"markdown","metadata":{"_cell_guid":"61aff816-eff4-4dbe-c9cc-763de630d28e"},"source":"Some sample output of the above cell: \n\n    0.626095667418\n    0.628515760033\n    0.626448662532\n    0.626910116762\n    0.627454917364\n    0.627569111048\n    0.628151976884\n    0.627657738235\n    0.627466291116\n    0.627801564481\n    0.627656842886\n    0.628249226468\n    0.627655465431\n    0.627920953722\n    0.627956629129\n    0.62809631857\n    0.628361270768\n\nWe can see that this model isn't all that great, but it is a something to work from."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}