{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\nfrom string import punctuation\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  DISPLAYING THE DATASET USING PANDAS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the dataset using Pandas\ndata = pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv.zip\")\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SUMMARY OF THE DATAFRAME"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CHECKING THE PRESENCE OF NULL VALUES"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the presence of Null values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# REPLACING NULL VALUES"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing Null values\ndata[\"question1\"].fillna( method ='ffill', inplace = True) \ndata[\"question2\"].fillna( method ='ffill', inplace = True) \ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Counting the unique values in \"is_duplicate\" column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Counting the unique values in \"is_duplicate\" column\na = data.pivot_table(index = ['is_duplicate'], aggfunc ='size') \na = a.reset_index()\na.columns= [\"Values\", \"Counts\"]\na","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing unique value counts of the column \"is_duplicate\" using pie chart"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Pie Chart\nfig = plt.figure(figsize =(5, 30)) \nplt.pie(a[\"Counts\"], labels = a[\"Values\"])\nplt.legend(a[\"Counts\"], fontsize=10)\nplt.title(\"Unique value Counts in is_duplicate column\", fontsize=25)\n\n# Displaying Pie Chart \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RESULTS OF THE ABOVE VISUALIZATION "},{"metadata":{"trusted":true},"cell_type":"code","source":"b = a.loc[a['Values'] == 0, 'Counts'].iloc[0]\nc = a.loc[a['Values'] == 1, 'Counts'].iloc[0]\nd = round((b/(b+c))*100,2)\ne = round(100-d,2) \nprint('Total number of question pairs for training : ',len(data))\nprint(\"Number of similar question pairs            : \", b)\nprint(\"Percentage of similar question pairs        : \", d, \"%\")\nprint(\"Number of non-similar question pairs        : \", c)\nprint(\"Percentage of non-similar question pairs    : \", e, \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Concatenating two dataframes \"qid1\" and \"qid2\" and providing results from it"},{"metadata":{"trusted":true},"cell_type":"code","source":"qids = data[\"qid1\"].append(data[\"qid2\"]) \nf = len(np.unique(qids))\ng = np.sum(qids.value_counts() == 1)\nh = round((g/f)*100,2)\ni = np.sum(qids.value_counts() > 1)\nj = round((i/f)*100,2)\nprint('Total number of questions in the training data         : ', f)\nprint('Number of questions that appear single time            : ', g)\nprint('Percentage of questions that appear single time        : ', h, '%')\nprint('Number of questions that appear multiple times         : ', i)\nprint('Percentage of questions that appear multiple times     : ', j, '%')\nprint('Maximum number of times a single question is repeated  : ', max(qids.value_counts()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Counting the unique values in \"qids\" column"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = qids.value_counts() \nk = k.reset_index()\nk.columns= [\"Values\", \"Counts\"]\nk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VISUALIZING QUESTION APPEARANCES USING STACK PLOT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Stack Plot\nplt.stackplot(k[\"Counts\"], k[\"Values\"], color=\"red\") \nplt.xlabel(\"COUNTS\") \nplt.ylabel(\"NUMBER OF QUESTIONS\") \nplt.title(\"QUESTION APPEARANCE COUNTS VISUALIZATION\")\n\n# Displaying Stack Plot\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # DISPLAYING TOP 20 UNIQUE VALUES HAVING HIGHER NUMBER OF COUNTS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying top 20 unique values having higher number of counts\nk = k[0:20]\nk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VISUALIZING TOP 20 UNIQUE VALUES HAVING HIGHER NUMBER OF COUNTS USING LINEAR PLOT WITH CUSTOMIZATIONS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Linear Plot with Customizations\nfig = plt.figure(figsize =(15, 5)) \nplt.plot(k[\"Counts\"], k[\"Values\"], color='orange', linestyle='dashed', linewidth = 3, marker='o', markerfacecolor='blue', markersize=12)\nplt.xlabel(\"COUNTS\")\nplt.ylabel(\"VALUES\") \nplt.title(\"VISUALIZING TOP 20 UNIQUE VALUES HAVING HIGHER NUMBER OF COUNTS\")\n\n# Displaying Linear Plot with Customizations\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CREATING A NEW DATAFRAME"},{"metadata":{"trusted":true},"cell_type":"code","source":"l = {'Values':['Single', 'Multiple'], 'Counts':[g, i]} \nl = pd.DataFrame(l) \nl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VISUALIZING COMPARISON BETWEEN SINGLE AND MULITPLE TIMES REPEATED QUESTIONS USING DONUT PLOT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Donut Plot\ncircle = plt.Circle( (0,0), 0.5, color='white')\nplt.pie(l[\"Counts\"], labels=l[\"Values\"])\np=plt.gcf()\np.gca().add_artist(circle)\nplt.legend(l[\"Counts\"])\nplt.title(\"COMPARISON BETWEEN SINGLE AND MULITPLE TIMES REPEATED QUESTIONS\")\n\n# Displaying Donut Plot\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PREVIEW SOME OF COMPARISONS BETWEEN QUESTION PAIRS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview some of comparisons between question pairs\na = 0 \nfor i in range(a,a+10):\n    print(data.question1[i])\n    print(data.question2[i])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nstop_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# REMOVING PUNCTUATIONS AND STOP WORDS FROM QUESTIONS"},{"metadata":{"trusted":true},"cell_type":"code","source":"def words(text, remove_stop_words=True, stem_words=False):\n    # Remove punctuation from questions\n    text = ''.join([c for c in text if c not in punctuation])\n    \n    # Lowering the words in questions\n    text = text.lower()\n    \n    # Remove stop words from questions\n    if remove_stop_words:\n        text = text.split()\n        text = [w for w in text if not w in stop_words]\n        text = \" \".join(text)\n    \n    # Return a list of words\n    return(text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process(question_list, questions):\n    for question in questions:\n        question_list.append(words(question))\nprocessed_question1 = []\nprocessed_question2 = []\nprocess(processed_question1, data.question1)\nprocess(processed_question2, data.question2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PREVIEW SOME OF COMPARISONS BETWEEN QUESTION PAIRS AFTER PROCESSING"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview some of comparisons between question pairs after processing\na = 0 \nfor i in range(a,a+10):\n    print(processed_question1[i])\n    print(processed_question2[i])\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DEFINING TFIDF VECTORIZER"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define tfidf vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(analyzer = 'word',\n                        stop_words = 'english',\n                        lowercase = True,\n                        max_features = 300,\n                        norm = 'l1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONCATENATING QUESTION COLUMNS"},{"metadata":{"trusted":true},"cell_type":"code","source":"words = pd.concat([data.question1, data.question2], axis = 0)\nwords.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FITTING AND TRANSFORMING QUESTIONS WITH TFIDF VECTORIZER"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf.fit(words)\ndata_q1 = tfidf.transform(data.question1)\ndata_q2 = tfidf.transform(data.question2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ASSIGNING INDEPENDENT AND DEPENDENT VARIABLES"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = abs(data_q1 - data_q2)\ny = data['is_duplicate']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SPLITTING INTO TRAIN AND TEST SET"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRAINING MACHINE LEARNING MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter=1000)\nlr.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PREDICTING THE TEST RESULTS"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr.predict(x_test) \ny_pred ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONFUSION MATRIX"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \ncm = confusion_matrix(y_test, y_pred) \nTP = cm[1][1] \nTN = cm[0][0]\nFP = cm[1][0]\nFN = cm[0][1] \nprint(\"True Positive  : \", TP)\nprint(\"True Negative  : \", TN)\nprint(\"False Positive : \", FP)\nprint(\"False Negative : \", FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy = (TP + TN) / (TP + TN + FP + FN) \nAccuracy","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}