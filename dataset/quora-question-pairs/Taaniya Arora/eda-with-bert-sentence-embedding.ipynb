{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### This notebook explores the duplicate and non-duplicate set of samples in the training set by using similarity scores of their sentence embeddings and also highlights incorrectly labelled cases."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_colwidth', -1)\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install bert-for-tf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! du -sh /kaggle/input/quora-question-pairs/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! unzip /kaggle/input/quora-question-pairs/train.csv.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of train.csv\n\n! du -sh ./*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport bert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"./train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of classes - is duplicate / not-duplicate"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby(\"is_duplicate\").count()['id'].plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df['question1'].isnull()].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df['question2'].isnull()].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing rows with Null questions\n\ntrain_df = train_df[~train_df['question1'].isnull()]\ntrain_df = train_df[~train_df['question2'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring similarity scores between question pairs using Language-Agnostic-Bert-Sentence embedding model\n\n[LABSE](https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html) - is a multilingual model to preduce sentence embeddings based on Bert and combines methods for obtaining sentence embeddings with MLM and Translation Language Model pretrained encoders. \n\nIt is trained on Monolingual data and bilingual translation pairs."},{"metadata":{"trusted":true},"cell_type":"code","source":"LABSE_model_URL = \"https://tfhub.dev/google/LaBSE/1\"\nMAX_SEQ_LENGTH = 64","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LABSE model is available from TFhub and is loaded from there to wrap it as a callable object and to be used as a Keras Layer. Its vocab_file is stored as atf.saved_model.Asset and the do_lower_case flag is stored as a tf.Variable object on the SavedModel."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Define Model containing LABSE as Keras layers\n\ndef getModel(model_url, max_seq_length):\n    # Load the saved LaBSE model as Keras layer. \n    # Set trainable to True to enable weight update for fine-tuning the model for down stream task\n    labse_layer = hub.KerasLayer(handle=model_url, trainable=True, name='labse')\n\n    # Define Inputs\n    input_word_ids = tf.keras.Input(shape=(max_seq_length, ), dtype=tf.int32, name='input_word_ids')\n    input_mask = tf.keras.Input(shape=(max_seq_length, ), dtype=tf.int32, name='input_mask')\n    input_segment_ids = tf.keras.Input(shape=(max_seq_length, ), dtype=tf.int32, name='input_segment_ids')\n\n    # LABSE layer \n    pooled_output, _ = labse_layer([input_word_ids, input_mask ,input_segment_ids])\n\n    # The output is L2 normalized shape [batch_size, 768] representing a complete sentence embedding\n    pooled_output = tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1), name='l2_normalized_pooling')(pooled_output)\n\n    # Define Model\n    return tf.keras.Model(inputs=[input_word_ids, input_mask ,input_segment_ids], outputs=pooled_output), labse_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Input preparation\nlabse_model, labse_layer = getModel(LABSE_model_URL, MAX_SEQ_LENGTH)\n\n# labse_layer = hub.KerasLayer(handle=LABSE_model_URL, trainable=True, name='labse')\n\nvocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()  # Get vocab file path as numpy array \ndo_lower_case = labse_layer.resolved_object.do_lower_case.numpy()   # Get Boolean Variable as numpy array\ntokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n\nprint(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labse_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_input(input_strings, tokenizer, max_seq_length):\n    input_ids_all, input_mask_all, segment_ids_all = [], [], []\n    \n    for input_string in tqdm(input_strings):\n        # Tokenize input\n        input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n        input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n        sequence_length = min(len(input_ids), max_seq_length)\n\n        # Padding or truncation.\n        if len(input_ids) >= max_seq_length:\n          input_ids = input_ids[:max_seq_length]\n        else:\n          input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n\n        input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n\n        input_ids_all.append(input_ids)\n        input_mask_all.append(input_mask)\n        segment_ids_all.append([0] * max_seq_length)\n\n    return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(input_text):\n    input_ids, input_mask, segment_ids = create_input(input_text, tokenizer, MAX_SEQ_LENGTH)\n    return labse_model.predict([input_ids, input_mask, segment_ids])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obtaining embeddings for each question in the pair\n\nWe can run the following code to obtain embeddings. It takes ~30 minutes to run it for 400K records in a single Tesla T4 GPU (16GB).\n\nI have already run it saved the results. Let's go the the next steps by importing the results file.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_df = train_df\n# question1_array = sample_df['question1'].values\n# question2_array = sample_df['question2'].values\n\n# %time question1_embeddings = encode(question1_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %time question2_embeddings = encode(question2_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# question1_embeddings.shape, question2_embeddings.shape     # shape - ((404287, 768), (404287, 768))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Computing consine similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# product = question1_embeddings * question2_embeddings\n# print(product.shape)                                # shape (404287, 768)\n\n# cosine_similarity = product.sum(axis=1)\n# cosine_similarity.shape                             # (404287,) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cos_similarity = prod.sum(axis=1)\n# cos_similarity.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_df['similarity'] = cos_similarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/train-df-with-similarity-score/train_with_similarity_scores.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing the results file which has the original train.csv added with the cosine similarity between the question pair."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = pd.read_csv(\"/kaggle/input/train-df-with-similarity-score/train_with_similarity_scores.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the distribution of similarity score for  duplicate and non-duplicate question pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sample_df[sample_df['is_duplicate'] == 1.0]['similarity'][0:] , label = \"1\", color = 'red')\nsns.distplot(sample_df[sample_df['is_duplicate'] == 0.0]['similarity'][0:] , label = \"0\", color = 'green')\nplt.xlabel(\"Question pair cosine similarity \")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x = 'is_duplicate', y = 'similarity', data = sample_df[0:])\nplt.ylabel(\"Question pair cosine similarity\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the similarity score obtained using sentence embeddings of pretrained LABSE model, the median of distribution of similarity score for both duplicate and non-duplicate questions is higher than 0.5. \n\nLet's look into more detail for each case next.\n\n### Duplicate question pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df[sample_df['is_duplicate'] == 1].describe()['similarity']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Duplicate Question pairs with similarity score < 0.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_low_simlarity_prop = sample_df[(sample_df['is_duplicate'] == 1) & (sample_df['similarity'] < 0.5)]['similarity'].count() / sample_df[sample_df['is_duplicate'] == 1]['similarity'].count()\n\nprint(\" {} percent of duplicate question pairs have similarity score less than 0.5 \".format(duplicate_low_simlarity_prop * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df[(sample_df['is_duplicate'] == 1) & (sample_df['similarity'] < 0.5)].describe()['similarity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Going through top question pairs with lowest similarity scores\n\nsample_df[(sample_df['is_duplicate'] == 1)][['question1', 'question2','similarity']].sort_values(by='similarity').head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Going through top question pairs with highest similarity scores.\n\nsample_df[(sample_df['is_duplicate'] == 1)][['question1', 'question2','similarity']].sort_values(by='similarity', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Question pairs with similarity scores are identical in the training dataset. We'll explore non-identical."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Going through top question pairs with highest similarity scores.\n\nsample_df[(sample_df['is_duplicate'] == 1) & (sample_df['similarity'] < 0.8)][['question1', 'question2','similarity']].sort_values(by='similarity', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Non-duplicate question pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df[sample_df['is_duplicate'] == 0].describe()['similarity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_duplicate_high_simlarity_prop = sample_df[(sample_df['is_duplicate'] == 0) & (sample_df['similarity'] > 0.5)]['similarity'].count() / sample_df[sample_df['is_duplicate'] == 0]['similarity'].count()\n\nprint(\" {} percent of Non-duplicate question pairs have similarity score more than 0.5 \".format(non_duplicate_high_simlarity_prop * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df[(sample_df['is_duplicate'] == 0) & (sample_df['similarity'] < 0.5)].describe()['similarity']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Non-duplicate question pairs with similarity score > 0.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df[(sample_df['is_duplicate'] == 0) & (sample_df['similarity'] > 0.5)].describe()['similarity']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While exploring, I came across the following top instances where the question pairs are duplicate but are labelled as otherwise."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df[(sample_df['is_duplicate'] == 0) & (sample_df['similarity'] > 0.5)].sort_values(by='similarity', ascending=False).head(23)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"True non-duplicate cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df_v2 = sample_df[(sample_df['is_duplicate'] == 0) & (sample_df['similarity'] > 0.5)]\nsample_df_v2[(sample_df_v2['similarity'] <= 0.9 )].sort_values(by='similarity', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### References\n* https://github.com/Taaniya/natural-language-understanding/blob/master/Explore_Language_Agnostic_BERT_Sentence_Embedding.ipynb\n* https://tfhub.dev/google/LaBSE/1"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}