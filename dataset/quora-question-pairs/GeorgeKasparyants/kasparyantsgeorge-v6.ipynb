{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cfdcd288-eddb-00ea-55fe-9c2f74f2111f"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\n\nfrom nltk import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\n\nfrom nltk.stem import SnowballStemmer\ns_stemmer = SnowballStemmer('english')\n\nfrom nltk.util import ngrams\n\nimport Levenshtein \n\nfrom tqdm import tqdm, tqdm_pandas\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"701a3c7b-228b-8b4d-c44d-a85eb9b1272a"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\ndf_test  = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c2c3a2f-ac80-d9ae-a817-cd82d35de483"},"outputs":[],"source":"print(df_train.info())\nprint(df_test.info())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc3d335f-f1a8-34b0-3c4e-4c9d9f62cb55"},"outputs":[],"source":"df_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ff0719f-8740-b0ac-6c7a-17d82e09072b"},"outputs":[],"source":"df_test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59187e7d-d06f-8066-dda2-e8e7d9a01883"},"outputs":[],"source":"df_train.groupby('is_duplicate').id.count().plot.bar()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d048045-54c9-e330-7821-47d41b83a034"},"outputs":[],"source":"#Метрика Жоккара кастомированная к строкам#\ndef JokkarMetric(x, y): \n    x = set(word_tokenize(x))\n    y = set(word_tokenize(y))\n    return (len(x.intersection(y)) +.0) / (len(x.union(y)) + .0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b49eb14d-e3a6-5fdc-47c2-cc464b911c2f"},"source":"Сперва сделаем наивный submition. Выведем вероятность вообще встретить пару одинаковых вопросов в тренировочной выборке. То есть предполагаем, что вопросы из train и test принадлежат одному вероятностному распределению, и их совпадение - случайная величина."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"885433c5-29f9-fc63-68ab-d63289222c74"},"outputs":[],"source":"#Чистка\ndf_train.question1 = df_train.question1.map(lambda x : str(x).lower())\ndf_train.question2 = df_train.question2.map(lambda x : str(x).lower())"},{"cell_type":"markdown","metadata":{"_cell_guid":"95144c69-434f-4b32-2280-35c19976bed5"},"source":"Добавим Жоккара"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d5d2f76-8b79-77af-632a-cef3347769d1"},"outputs":[],"source":"df_train[\"Jokkar\"] = df_train.apply(func=lambda x : JokkarMetric(x.question1, x.question2), axis=1)\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90dfa976-c539-0789-c57b-ee6cf8982494"},"outputs":[],"source":"sns.distplot(df_train[df_train.is_duplicate==0].Jokkar)\nsns.distplot(df_train[df_train.is_duplicate==1].Jokkar)"},{"cell_type":"markdown","metadata":{"_cell_guid":"89b1be22-f63b-a544-44b0-8b7606bba51e"},"source":"Добавим абсолютную разность между длиной слов"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c34af30-9138-0f74-b81c-dfb0d2601746"},"outputs":[],"source":"def L1Metric(x, y): \n    x = Counter(word_tokenize(x))\n    y = Counter(word_tokenize(y))\n    return np.abs(len(x) - len(y)+ .0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35d2a48f-ec37-0d83-42b0-ff29e4a08e62"},"outputs":[],"source":"df_train[\"L1Metric\"] = df_train.apply(func=lambda x : L1Metric(x.question1, x.question2), axis=1)\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"72194f50-07d0-8e61-fb00-b571fce3f16b"},"outputs":[],"source":"sns.distplot(df_train[df_train.is_duplicate==0].L1Metric)\nsns.distplot(df_train[df_train.is_duplicate==1].L1Metric)"},{"cell_type":"markdown","metadata":{"_cell_guid":"db43f209-8ff7-cc80-e7b7-f3f4c5bbbe3e"},"source":"Количество одинаковых слов"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4acadfec-5717-1c71-e89f-df82822c6cfe"},"outputs":[],"source":"def InterMetric(x, y): \n    x = set(word_tokenize(x))\n    y = set(word_tokenize(y))\n    return len(x.intersection(y)) + .0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"97ff31ce-9b6a-4be4-16e5-172b99a783b4"},"outputs":[],"source":"df_train[\"InterMetric\"] = df_train.apply(func=lambda x : InterMetric(x.question1, x.question2), axis=1)\ndf_train.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8030fdbe-6b6d-d186-92ff-b03861314334"},"source":"Посмотрим на распределение"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4399ff2-68c1-73b2-b876-f30a419c9c1e"},"outputs":[],"source":"sns.distplot(df_train[df_train.is_duplicate==0].InterMetric)\nsns.distplot(df_train[df_train.is_duplicate==1].InterMetric)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74aea5c8-ed1c-524c-3554-01c3e00a9e5a"},"outputs":[],"source":"def BigramJokkarMetric(x, y):\n    x = set(ngrams(word_tokenize(x), 2))\n    y = set(ngrams(word_tokenize(y), 2))\n    return (len(x.intersection(y)) +.01) / (len(x.union(y)) + .01)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"276738d5-770f-9f91-e85a-d270cbcce711"},"outputs":[],"source":"df_train[\"BigramJokkar\"] = df_train.apply(func=lambda x : BigramJokkarMetric(x.question1, x.question2), axis=1)\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f03e4fcc-f2ff-f9aa-d790-8614a10bc6f6"},"outputs":[],"source":"sns.distplot(df_train[df_train.is_duplicate==0].BigramJokkar)\nsns.distplot(df_train[df_train.is_duplicate==1].BigramJokkar)"},{"cell_type":"markdown","metadata":{"_cell_guid":"878dc73e-195b-dad1-b0ec-c8574d3d871f"},"source":"# Совпадают ли сами вопросы"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a6a053b-843c-77f8-7f01-384b989b71d4"},"outputs":[],"source":"def QuesMetric(x, y):\n    x = word_tokenize(x)\n    y = word_tokenize(y)\n    return (x[0] == y[0]) + .0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6e1fc31-afff-3d8a-c6be-59ecf98acaac"},"outputs":[],"source":"df_train['QuestEq'] = df_train.apply(func = lambda x : QuesMetric(x.question1, x.question2), axis=1)\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d87d6fd9-04c2-4eac-907b-73429c4b0360"},"outputs":[],"source":"def Quest1Metric(x, y):\n    x = word_tokenize(x)\n    y = word_tokenize(y)\n    return ((len(x) > 1) and (len(y) > 1) and (x[1] == y[1]))+.0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59769755-a4b6-7ce9-ea80-722222674901"},"outputs":[],"source":"df_train['Quest1Eq'] = df_train.apply(func = lambda x : Quest1Metric(x.question1, x.question2), axis=1)\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03f93447-ae28-e9b4-1999-06490e778a35"},"outputs":[],"source":"sns.factorplot(x=\"QuestEq\", y=\"Quest1Eq\", hue=\"is_duplicate\", data=df_train, kind=\"bar\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"3ba622a0-b882-3a56-0112-4d36c603dd0f"},"source":"# Расстояние Левенштейна между вопросами"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4600d8a1-8bb4-b4bc-553c-246c0681e8e6"},"outputs":[],"source":"def LevenMetric(x, y):\n    x = x.replace(' ', '')\n    y = y.replace(' ', '')\n    return Levenshtein.distance(x, y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c62a9a9-bdec-100e-fa32-845fdb0d60fb"},"outputs":[],"source":"df_train[\"LevenMetric\"] = df_train.apply(func=lambda x : LevenMetric(x.question1, x.question2), axis=1)\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"339f92fb-ad7a-bd22-8676-5f571e74fb64"},"outputs":[],"source":"sns.distplot(df_train[df_train.is_duplicate==0].LevenMetric)\nsns.distplot(df_train[df_train.is_duplicate==1].LevenMetric)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c4941245-28e0-665a-9a0c-4070444e558b"},"source":"Спец. символы"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1188642d-8a68-b57f-0e3b-571536d0046a"},"outputs":[],"source":"def Spec1(x, y):\n    return (1 ^ ('?' in x) ^ ('?' in y)) + .0\ndef Spec2(x, y):\n    return (1 ^ ('[math]' in x) ^ ('[math]' in y)) + .0\ndef Spec3(x, y):\n    return (1 ^ ('.' in x) ^ ('.' in y)) + .0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa291e10-38ca-dbf7-490b-73e6b9f523eb"},"outputs":[],"source":"df_train['Spec1'] = df_train.apply(func=lambda x : Spec1(x.question1, x.question2), axis=1)\ndf_train['Spec2'] = df_train.apply(func=lambda x : Spec2(x.question1, x.question2), axis=1)\ndf_train['Spec3'] = df_train.apply(func=lambda x : Spec3(x.question1, x.question2), axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"01197021-c29a-ea0a-52c6-a8277bbeb12c"},"source":"# Добавим использование tf-idf "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e291b2b6-4b73-95fa-9923-51fde02d3534"},"outputs":[],"source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\ntest_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3bf2478d-2e24-4217-9f77-49b2fe967c19"},"outputs":[],"source":"dist_train = train_qs.apply(len)\ndist_test = test_qs.apply(len)\n\npal = sns.color_palette()\nplt.figure(figsize=(15, 10))\nplt.hist(dist_train, bins=200, range=[0, 200], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=200, range=[0, 200], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of character count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of characters', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b319f0ff-8f8c-0b76-c693-2442c34ef864"},"outputs":[],"source":"from collections import Counter\n\n# If a word appears only once, we ignore it completely (likely a typo)\n# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\n\neps = 5000 \nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eecc8ca0-d1da-785a-42fd-5d67b030fab9"},"outputs":[],"source":"from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef tfidf_word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    \n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    \n    R = np.sum(shared_weights) / np.sum(total_weights)\n    return R"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7424af8b-7a07-1c8d-13f3-a463e2d0d9a6"},"outputs":[],"source":"plt.figure(figsize=(15, 5))\ndf_train['tfidf'] = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\nplt.hist(df_train['tfidf'][df_train['is_duplicate'] == 0].fillna(0), bins=20, normed=True, label='Not Duplicate')\nplt.hist(df_train['tfidf'][df_train['is_duplicate'] == 1].fillna(0), bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over tfidf_word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a37b5c27-8f52-e42c-7844-a1fa8d4bb4ca"},"source":"# Еще одна очень хорошая фича, а именно, это в некотором смысле улучшение фичи InterMetric, игнорируются стопслова"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54d9abc4-3fe0-691e-ee69-9c5e7f2f45af"},"outputs":[],"source":"def word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R\n\nplt.figure(figsize=(15, 5))\ndf_train['word_match'] = df_train.apply(word_match_share, axis=1, raw=True)\nplt.hist(df_train['word_match'][df_train['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(df_train['word_match'][df_train['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6264ce44-76f7-e4c9-73a9-15ffaf566a64"},"source":"Проделаем все это безобразие с тестом"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f79329ca-b1be-cd3f-d46d-cdf99dc076d5"},"outputs":[],"source":"%%time\ndf_test.question1 = df_test.question1.map(lambda x : str(x).lower())\ndf_test.question2 = df_test.question2.map(lambda x : str(x).lower())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2800306-e67d-12ed-5529-3501e0797406"},"outputs":[],"source":"%%time\ndf_test[\"Jokkar\"] = df_test.apply(func=lambda x : JokkarMetric(x.question1, x.question2), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4c49a25-29e6-1f79-b2d2-51c0c7014047"},"outputs":[],"source":"%%time\ndf_test[\"L1Metric\"] = df_test.apply(func=lambda x : L1Metric(x.question1, x.question2), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08ee48b2-63ee-6624-41fa-969714a6718c"},"outputs":[],"source":"%%time\ndf_test[\"InterMetric\"] = df_test.apply(func=lambda x : InterMetric(x.question1, x.question2), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39c6218b-360c-a238-3f18-4af10c3e148d"},"outputs":[],"source":"%%time\ndf_test[\"BigramJokkar\"] = df_test.apply(func=lambda x : BigramJokkarMetric(x.question1, x.question2), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"248517bb-ef50-cfec-8be8-9e62a89b0570"},"outputs":[],"source":"%%time\ndf_test['QuestEq'] = df_test.apply(func = lambda x : QuesMetric(x.question1, x.question2), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37d720bd-e02e-66e8-38a5-1ff18b4268dc"},"outputs":[],"source":"%%time\ndf_test['Quest1Eq'] = df_test.apply(func = lambda x : Quest1Metric(x.question1, x.question2), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42b4f85a-8d34-7298-c1e4-49a0fe414954"},"outputs":[],"source":"%%time\ndf_test[\"LevenMetric\"] = df_test.apply(func=lambda x : LevenMetric(x.question1, x.question2), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d50df874-561d-b04e-d54f-41cd5b87d6a3"},"outputs":[],"source":"%%time\ndf_test['Spec1'] = df_test.apply(func=lambda x : Spec1(x.question1, x.question2), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cdc58621-e9c4-9eaf-8f6e-35aaadbc90bd"},"outputs":[],"source":"%%time\ndf_test['Spec2'] = df_test.apply(func=lambda x : Spec2(x.question1, x.question2), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb488af5-5375-e2ef-5eda-c41b79e5228a"},"outputs":[],"source":"%%time\ndf_test['Spec3'] = df_test.apply(func=lambda x : Spec3(x.question1, x.question2), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce44d608-f062-b4a6-a88b-25c203224af9"},"outputs":[],"source":"%%time\ndf_test['tfidf'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"112ec1eb-8da7-70ff-3e7f-74839d2a02e2"},"outputs":[],"source":"%%time\ndf_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dbc299b6-0f09-0f71-24d6-5e539508cd16"},"outputs":[],"source":"df_test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"404284b6-fbd9-979e-f4f9-81ebb86e730f"},"outputs":[],"source":"x_train = df_train.drop(['question1', 'question2', 'qid1', 'qid2', 'is_duplicate', 'id', 'LevenMetric'], axis=1).values\ny_train = df_train.is_duplicate.values\nx_test = df_test.drop(['question1', 'question2', 'test_id', 'LevenMetric'], axis=1).values"},{"cell_type":"markdown","metadata":{"_cell_guid":"9142ee89-5997-dc0b-3903-8a8223fae2da"},"source":"# Сделаем выборку равномерной"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45a5ef94-3c54-9512-dac3-2601ec3a97a3"},"outputs":[],"source":"print(len(df_train[df_train.is_duplicate==0]), len(df_train[df_train.is_duplicate==1]))\ndf_train = pd.concat([df_train[df_train.is_duplicate==1], df_train]).sample(n = len(df_train))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ad74884f-9f03-a775-fe3f-fe32641e71df"},"source":"Натравим на это дело xgboost"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"efd40c56-0502-34ba-fb86-d61a5e96f92a"},"outputs":[],"source":"import xgboost as xgb"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c766272-ff86-c9ac-1013-4bec567b4d67"},"outputs":[],"source":"# Параметры\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2062831-f38c-af8d-c4f2-7656efe5c1bb"},"outputs":[],"source":"d_train = xgb.DMatrix(x_train, label=y_train)\nwatchlist = [(d_train, 'train')]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d94c56c5-0dca-837d-5b84-d0894d79d49f"},"outputs":[],"source":"bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df5982ad-c587-5fcd-a353-3d8fbb14db5c"},"outputs":[],"source":"d_test = xgb.DMatrix(x_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fbcdadfc-673c-b144-0352-e499d2253e5e"},"outputs":[],"source":"y_test = bst.predict(d_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07d7c292-8a7a-b673-37a5-60fcddb0818b"},"outputs":[],"source":"sub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = y_test\nsub.to_csv('sub.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e79433ee-4ddc-bcf4-b3e5-b148ddf2cc21"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}