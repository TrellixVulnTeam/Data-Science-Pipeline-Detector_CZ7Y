{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0d518ac-e53c-27fd-f314-f36422695162"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01923eb1-a0e4-56e7-a7ac-f055365da02c"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2c41719-c2d6-e663-2e06-42f1de89fcf7"},"outputs":[],"source":"#Чистка\ndf_train.question1 = df_train.question1.map(lambda x : str(x).lower())\ndf_train.question2 = df_train.question2.map(lambda x : str(x).lower())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff8da054-3f12-19e6-32de-470b6f5c30a1"},"outputs":[],"source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\ntest_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c998234d-6246-7023-b74b-a19f309f3bfa"},"outputs":[],"source":"train_qs.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"ae076383-7ed6-07eb-db4e-1091095e301f"},"source":"Понятно, что нужно отбирать те признаки, в которых просвечиваются две гауссианы с несовпадающими центрами (или хотя бы одна с шумом)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"779f6543-ef3e-feb0-b3c8-6c2caa4eea02"},"outputs":[],"source":"from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef WordMatch(row):\n    q1 = set(str(row['question1']).split()).difference(stops)\n    q2 = set(str(row['question2']).split()).difference(stops)\n    \n    if len(q1) == 0 or len(q2) == 0:\n        return 0\n    \n    inter1 = q1.difference(q2)\n    inter2 = q2.difference(q1)\n    return (len(inter1) + len(inter2))/(len(q1) + len(q2) + .0)\n\n\ndf_train['WordMatch'] = df_train.apply(WordMatch, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c56b9d49-fd3e-0abf-cc3e-782dccaba350"},"outputs":[],"source":"sns.distplot(df_train[df_train.is_duplicate==0].WordMatch, kde=False)\nsns.distplot(df_train[df_train.is_duplicate==1].WordMatch, kde=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9315ba19-117a-3140-4676-4ae3b633a240"},"outputs":[],"source":"from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef Jakkar(row):\n    q1 = set(str(row['question1']).split()).difference(stops)\n    q2 = set(str(row['question2']).split()).difference(stops)\n    \n    if len(q1) == 0 or len(q2) == 0:\n        return 0\n    \n    inter = q1.intersection(q2)\n    un = q1.union(q2)\n    return 1 - (len(inter))/(len(un) + .0)\n\n\ndf_train['Jakkar'] = df_train.apply(Jakkar, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e402166-9aeb-ce87-0b2a-2184afe00096"},"outputs":[],"source":"sns.distplot(df_train[df_train.is_duplicate==0].Jakkar, kde=False)\nsns.distplot(df_train[df_train.is_duplicate==1].Jakkar, kde=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f0274cf-7a51-3f2f-8f1a-821cf13cd450"},"source":"Jakkar дает более точный срез, где начинаются недубликаты, а WordMatch дает более широкое место для разброса недубликатов"},{"cell_type":"markdown","metadata":{"_cell_guid":"c27c91ac-6fde-3fc4-6ee3-3c43556117af"},"source":"Используем сперва стандартный sklearn - TfIdfVectorizer"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a01fea6-3d8e-183c-0a21-de80c29b43fd"},"outputs":[],"source":"tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n\ntfidf_txt = pd.Series(test_qs.tolist() + train_qs.tolist())\ntfidf.fit_transform(tfidf_txt)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4c72511b-0e83-9629-ed85-75806ac62a4f"},"source":"Просто добавим фичи, как бы представляем фразу как сумму весов входящих в нее слов"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bac10232-82b0-c6da-d03c-c8cfc40b9969"},"outputs":[],"source":"from collections import Counter\n\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0.0\n    else:\n        return 1.0 / (count + eps)\n\n\nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"119530d9-0b47-f917-23f1-ef9ad01e2beb"},"outputs":[],"source":"def WeightMatch(row):\n    q1 = set(str(row['question1']).split()).difference(stops)\n    q2 = set(str(row['question2']).split()).difference(stops)\n    \n    if len(q1) == 0 or len(q2) == 0:\n        return 0\n    \n    shared_weights = [weights.get(w, 0) for w in q1.intersection(q2)]\n    total_weights = [weights.get(w, 0) for w in q1.union(q2)]\n    \n    R = np.sum(shared_weights) / (np.sum(total_weights) + .0)\n    return R\n\ndf_train['WeightMatch'] = df_train.apply(WeightMatch, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d4c899f-dd8e-7b69-0f7a-c86438890180"},"outputs":[],"source":"sns.distplot(df_train[df_train.is_duplicate==0].WeightMatch, kde=False)\nsns.distplot(df_train[df_train.is_duplicate==1].WeightMatch, kde=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29a10e7c-27fe-6293-0e71-9f6e56987f0a"},"outputs":[],"source":"x_test = pd.DataFrame()\n\nx_test['WordMatch'] = df_test.apply(WordMatch, axis=1, raw=True)\nx_test['Jakkar'] = df_test.apply(Jakkar, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88f9ff52-739f-e0d1-5563-65508215fc08"},"outputs":[],"source":"x_test['WeightMatch'] = df_test.apply(WeightMatch, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ae8323cf-6a82-e714-ecb4-fb407cd72d43"},"outputs":[],"source":"x_train = df_train.drop(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], axis=1)\ny_train = df_train['is_duplicate'].values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b8d44da-a86c-92f2-fabf-56c86b69b5fd"},"outputs":[],"source":"pos_train = x_train[y_train == 1]\nneg_train = x_train[y_train == 0]\n\n# Now we oversample the negative class\n# There is likely a much more elegant way to do this...\np = 0.165\nscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\nwhile scale > 1:\n    neg_train = pd.concat([neg_train, neg_train])\n    scale -=1\nneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\nprint(len(pos_train) / (len(pos_train) + len(neg_train)))\n\nx_train = pd.concat([pos_train, neg_train])\ny_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\ndel pos_train, neg_train"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36bb9cf4-e389-839c-cb35-59f637ad5c9c"},"outputs":[],"source":"import xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(x_train, label=y_train)\n\nwatchlist = [(d_train, 'train')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71369237-4803-28aa-494e-a636d2a112e6"},"outputs":[],"source":"d_test = xgb.DMatrix(x_test)\np_test = bst.predict(d_test)\n\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = p_test\nsub.to_csv('simple.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f550016-8192-2ed6-b5aa-33e55733c8bb"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}