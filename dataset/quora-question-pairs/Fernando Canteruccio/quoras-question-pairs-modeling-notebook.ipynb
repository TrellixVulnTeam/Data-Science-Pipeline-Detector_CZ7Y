{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f5cb1ba8-3b56-3561-03dd-8adaef679a4b"},"source":"# Quoras Question Pairs Modeling Notebook\n\nThis notebook try to predict if some pair of Quoras questions are duplicated or not."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95323b2e-f181-854d-934c-a69012fae42d"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom zipfile import ZipFile\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ce2373f-9cdc-4979-201f-6f8a0044d003"},"source":"First, lets get the train dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39e24195-20d5-df3b-49e7-4c7bba691eb7"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43ff4751-f961-7409-b11f-b036fa1ecc5b"},"outputs":[],"source":"texts = df_train[['question1','question2']]\nlabels = df_train['is_duplicate']\n\ndel df_train"},{"cell_type":"markdown","metadata":{"_cell_guid":"641d8cb0-01ba-3609-6397-b625bc8bf13f"},"source":"Now, lets build our model. First we need tokenize the questions to create a word index, then we use it with the Glove model as our embedding layer, that transforms the input vector with our words index to a dense vector that represents our words sequence. We need to load the pre-trained GloVe model in order to use it as our embedding model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3b75d3e-fe99-104a-f05b-f6924f812efe"},"outputs":[],"source":"# Params\nMAX_NB_WORDS = 100000\nMAX_SEQUENCE_LENGTH = 40\nVALIDATION_SPLIT = 0.1\nEMBEDDING_DIM = 128"},{"cell_type":"markdown","metadata":{"_cell_guid":"b522ef03-cec5-b73f-5791-a451ae3cc6d3"},"source":"Prepare the questions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d343f0f-401b-2c57-1211-6a5554aa2c3f"},"outputs":[],"source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\n\ntk = Tokenizer(num_words=MAX_NB_WORDS)\n\ntk.fit_on_texts(list(texts.question1.values.astype(str)) + list(texts.question2.values.astype(str)))\nx1 = tk.texts_to_sequences(texts.question1.values.astype(str))\nx1 = pad_sequences(x1, maxlen=MAX_SEQUENCE_LENGTH)\n\nx2 = tk.texts_to_sequences(texts.question2.values.astype(str))\nx2 = pad_sequences(x2, maxlen=MAX_SEQUENCE_LENGTH)\n\nword_index = tk.word_index\n\n#labels = to_categorical(labels)\n\nprint('Shape of data tensor:', x1.shape, x2.shape)\nprint('Shape of label tensor:', labels.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3daf784c-eea1-aa57-7606-4ea666485452"},"outputs":[],"source":"from keras.layers import Dense, Dropout, Lambda, TimeDistributed, PReLU, Merge, Activation, Embedding\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\n\nencoder_1 = Sequential()\nencoder_1.add(Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            input_length=MAX_SEQUENCE_LENGTH))\n\nencoder_1.add(TimeDistributed(Dense(EMBEDDING_DIM, activation='relu')))\nencoder_1.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(EMBEDDING_DIM,)))\n\nencoder_2 = Sequential()\nencoder_2.add(Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            input_length=MAX_SEQUENCE_LENGTH))\n\nencoder_2.add(TimeDistributed(Dense(EMBEDDING_DIM, activation='relu')))\nencoder_2.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(EMBEDDING_DIM,)))\n\nmodel = Sequential()\nmodel.add(Merge([encoder_1, encoder_2], mode='concat'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(EMBEDDING_DIM))\nmodel.add(PReLU())\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(EMBEDDING_DIM))\nmodel.add(PReLU())\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ncheckpoint = ModelCheckpoint('weights.h5', monitor='val_acc', save_best_only=True, verbose=2)\n\nmodel.fit([x1, x2], y=labels, batch_size=384, nb_epoch=1,\n                 verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint])"},{"cell_type":"markdown","metadata":{"_cell_guid":"dd19ba01-f32c-a626-1d27-0dcb8a1b9e4d"},"source":"As kaggle have time limit for running kernels, this models trains just one epoch and is pretty small. A bigger/depper model with proper training time will perform better."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}