{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\nfrom string import punctuation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-17T18:06:44.232382Z","iopub.execute_input":"2021-11-17T18:06:44.232793Z","iopub.status.idle":"2021-11-17T18:06:45.451976Z","shell.execute_reply.started":"2021-11-17T18:06:44.232708Z","shell.execute_reply":"2021-11-17T18:06:45.451243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Bidirectional\nfrom tensorflow.keras import layers, utils, callbacks, optimizers, regularizers\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:07:19.272217Z","iopub.execute_input":"2021-11-17T18:07:19.272813Z","iopub.status.idle":"2021-11-17T18:07:23.83513Z","shell.execute_reply.started":"2021-11-17T18:07:19.272773Z","shell.execute_reply":"2021-11-17T18:07:23.834378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:07:24.882246Z","iopub.execute_input":"2021-11-17T18:07:24.882744Z","iopub.status.idle":"2021-11-17T18:07:25.04074Z","shell.execute_reply.started":"2021-11-17T18:07:24.882704Z","shell.execute_reply":"2021-11-17T18:07:25.039793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")\ntest = pd.read_csv(\"../input/quora-question-pairs/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:07:41.581896Z","iopub.execute_input":"2021-11-17T18:07:41.582202Z","iopub.status.idle":"2021-11-17T18:07:58.513947Z","shell.execute_reply.started":"2021-11-17T18:07:41.582164Z","shell.execute_reply":"2021-11-17T18:07:58.513248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_dataframe_train(train):\n    stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n              'Is','If','While','This']\n    def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n        text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n        text = re.sub(r\"what's\", \"\", text)\n        text = re.sub(r\"What's\", \"\", text)\n        text = re.sub(r\"\\'s\", \" \", text)\n        text = re.sub(r\"\\'ve\", \" have \", text)\n        text = re.sub(r\"can't\", \"cannot \", text)\n        text = re.sub(r\"n't\", \" not \", text)\n        text = re.sub(r\"I'm\", \"I am\", text)\n        text = re.sub(r\" m \", \" am \", text)\n        text = re.sub(r\"\\'re\", \" are \", text)\n        text = re.sub(r\"\\'d\", \" would \", text)\n        text = re.sub(r\"\\'ll\", \" will \", text)\n        text = re.sub(r\"60k\", \" 60000 \", text)\n        text = re.sub(r\" e g \", \" eg \", text)\n        text = re.sub(r\" b g \", \" bg \", text)\n        text = re.sub(r\"\\0s\", \"0\", text)\n        text = re.sub(r\" 9 11 \", \"911\", text)\n        text = re.sub(r\"e-mail\", \"email\", text)\n        text = re.sub(r\"\\s{2,}\", \" \", text)\n        text = re.sub(r\"quikly\", \"quickly\", text)\n        text = re.sub(r\" usa \", \" America \", text)\n        text = re.sub(r\" USA \", \" America \", text)\n        text = re.sub(r\" u s \", \" America \", text)\n        text = re.sub(r\" uk \", \" England \", text)\n        text = re.sub(r\" UK \", \" England \", text)\n        text = re.sub(r\"india\", \"India\", text)\n        text = re.sub(r\"switzerland\", \"Switzerland\", text)\n        text = re.sub(r\"china\", \"China\", text)\n        text = re.sub(r\"chinese\", \"Chinese\", text) \n        text = re.sub(r\"imrovement\", \"improvement\", text)\n        text = re.sub(r\"intially\", \"initially\", text)\n        text = re.sub(r\"quora\", \"Quora\", text)\n        text = re.sub(r\" dms \", \"direct messages \", text)  \n        text = re.sub(r\"demonitization\", \"demonetization\", text) \n        text = re.sub(r\"actived\", \"active\", text)\n        text = re.sub(r\"kms\", \" kilometers \", text)\n        text = re.sub(r\"KMs\", \" kilometers \", text)\n        text = re.sub(r\" cs \", \" computer science \", text) \n        text = re.sub(r\" upvotes \", \" up votes \", text)\n        text = re.sub(r\" iPhone \", \" phone \", text)\n        text = re.sub(r\"\\0rs \", \" rs \", text) \n        text = re.sub(r\"calender\", \"calendar\", text)\n        text = re.sub(r\"ios\", \"operating system\", text)\n        text = re.sub(r\"gps\", \"GPS\", text)\n        text = re.sub(r\"gst\", \"GST\", text)\n        text = re.sub(r\"programing\", \"programming\", text)\n        text = re.sub(r\"bestfriend\", \"best friend\", text)\n        text = re.sub(r\"dna\", \"DNA\", text)\n        text = re.sub(r\"III\", \"3\", text) \n        text = re.sub(r\"the US\", \"America\", text)\n        text = re.sub(r\"Astrology\", \"astrology\", text)\n        text = re.sub(r\"Method\", \"method\", text)\n        text = re.sub(r\"Find\", \"find\", text) \n        text = re.sub(r\"banglore\", \"Banglore\", text)\n        text = re.sub(r\" J K \", \" JK \", text)\n\n        text = ''.join([c for c in text if c not in punctuation])\n\n        if remove_stop_words:\n            text = text.split()\n            text = [w for w in text if not w in stop_words]\n            text = \" \".join(text)\n\n        if stem_words:\n            text = text.split()\n            stemmer = SnowballStemmer('english')\n            stemmed_words = [stemmer.stem(word) for word in text]\n            text = \" \".join(stemmed_words)\n\n        return(text)\n    \n    def process_questions(question_list, questions, question_list_name, dataframe):\n        for question in questions:\n            question_list.append(text_to_wordlist(str(question)))\n            if len(question_list) % 100000 == 0:\n                progress = len(question_list)/len(dataframe) * 100\n                print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))\n            \n    \n    train_question1 = []\n    process_questions(train_question1, train.question1, 'train_question1', train)\n\n    train_question2 = []\n    process_questions(train_question2, train.question2, 'train_question2', train)\n\n    train[\"question1\"] = train_question1\n    train[\"question2\"] = train_question2\n    \n    return train","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:07:00.601317Z","iopub.execute_input":"2021-11-17T17:07:00.601808Z","iopub.status.idle":"2021-11-17T17:07:00.632153Z","shell.execute_reply.started":"2021-11-17T17:07:00.601771Z","shell.execute_reply":"2021-11-17T17:07:00.631437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_dataframe_test(test):\n    stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n              'Is','If','While','This']\n    def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n        text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n        text = re.sub(r\"what's\", \"\", text)\n        text = re.sub(r\"What's\", \"\", text)\n        text = re.sub(r\"\\'s\", \" \", text)\n        text = re.sub(r\"\\'ve\", \" have \", text)\n        text = re.sub(r\"can't\", \"cannot \", text)\n        text = re.sub(r\"n't\", \" not \", text)\n        text = re.sub(r\"I'm\", \"I am\", text)\n        text = re.sub(r\" m \", \" am \", text)\n        text = re.sub(r\"\\'re\", \" are \", text)\n        text = re.sub(r\"\\'d\", \" would \", text)\n        text = re.sub(r\"\\'ll\", \" will \", text)\n        text = re.sub(r\"60k\", \" 60000 \", text)\n        text = re.sub(r\" e g \", \" eg \", text)\n        text = re.sub(r\" b g \", \" bg \", text)\n        text = re.sub(r\"\\0s\", \"0\", text)\n        text = re.sub(r\" 9 11 \", \"911\", text)\n        text = re.sub(r\"e-mail\", \"email\", text)\n        text = re.sub(r\"\\s{2,}\", \" \", text)\n        text = re.sub(r\"quikly\", \"quickly\", text)\n        text = re.sub(r\" usa \", \" America \", text)\n        text = re.sub(r\" USA \", \" America \", text)\n        text = re.sub(r\" u s \", \" America \", text)\n        text = re.sub(r\" uk \", \" England \", text)\n        text = re.sub(r\" UK \", \" England \", text)\n        text = re.sub(r\"india\", \"India\", text)\n        text = re.sub(r\"switzerland\", \"Switzerland\", text)\n        text = re.sub(r\"china\", \"China\", text)\n        text = re.sub(r\"chinese\", \"Chinese\", text) \n        text = re.sub(r\"imrovement\", \"improvement\", text)\n        text = re.sub(r\"intially\", \"initially\", text)\n        text = re.sub(r\"quora\", \"Quora\", text)\n        text = re.sub(r\" dms \", \"direct messages \", text)  \n        text = re.sub(r\"demonitization\", \"demonetization\", text) \n        text = re.sub(r\"actived\", \"active\", text)\n        text = re.sub(r\"kms\", \" kilometers \", text)\n        text = re.sub(r\"KMs\", \" kilometers \", text)\n        text = re.sub(r\" cs \", \" computer science \", text) \n        text = re.sub(r\" upvotes \", \" up votes \", text)\n        text = re.sub(r\" iPhone \", \" phone \", text)\n        text = re.sub(r\"\\0rs \", \" rs \", text) \n        text = re.sub(r\"calender\", \"calendar\", text)\n        text = re.sub(r\"ios\", \"operating system\", text)\n        text = re.sub(r\"gps\", \"GPS\", text)\n        text = re.sub(r\"gst\", \"GST\", text)\n        text = re.sub(r\"programing\", \"programming\", text)\n        text = re.sub(r\"bestfriend\", \"best friend\", text)\n        text = re.sub(r\"dna\", \"DNA\", text)\n        text = re.sub(r\"III\", \"3\", text) \n        text = re.sub(r\"the US\", \"America\", text)\n        text = re.sub(r\"Astrology\", \"astrology\", text)\n        text = re.sub(r\"Method\", \"method\", text)\n        text = re.sub(r\"Find\", \"find\", text) \n        text = re.sub(r\"banglore\", \"Banglore\", text)\n        text = re.sub(r\" J K \", \" JK \", text)\n\n        text = ''.join([c for c in text if c not in punctuation])\n\n        if remove_stop_words:\n            text = text.split()\n            text = [w for w in text if not w in stop_words]\n            text = \" \".join(text)\n\n        if stem_words:\n            text = text.split()\n            stemmer = SnowballStemmer('english')\n            stemmed_words = [stemmer.stem(word) for word in text]\n            text = \" \".join(stemmed_words)\n\n        return(text)\n    \n    def process_questions(question_list, questions, question_list_name, dataframe):\n        for question in questions:\n            question_list.append(text_to_wordlist(str(question)))\n            if len(question_list) % 100000 == 0:\n                progress = len(question_list)/len(dataframe) * 100\n                print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))\n            \n    \n    test_question1 = []\n    process_questions(test_question1, test.question1, 'test_question1', test)\n\n    test_question2 = []\n    process_questions(test_question2, test.question2, 'test_question2', test)\n\n    test[\"question1\"] = test_question1\n    test[\"question2\"] = test_question2\n    \n    return test","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:07:00.825649Z","iopub.execute_input":"2021-11-17T17:07:00.825955Z","iopub.status.idle":"2021-11-17T17:07:00.854943Z","shell.execute_reply.started":"2021-11-17T17:07:00.825922Z","shell.execute_reply":"2021-11-17T17:07:00.854189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = clean_dataframe_train(train)\ntest = clean_dataframe_test(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:07:01.077776Z","iopub.execute_input":"2021-11-17T17:07:01.078419Z","iopub.status.idle":"2021-11-17T17:18:14.870933Z","shell.execute_reply.started":"2021-11-17T17:07:01.078383Z","shell.execute_reply":"2021-11-17T17:18:14.87014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 100\nEMBEDDING_DIM = 300","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:18:14.872662Z","iopub.execute_input":"2021-11-17T17:18:14.872925Z","iopub.status.idle":"2021-11-17T17:18:14.876509Z","shell.execute_reply.started":"2021-11-17T17:18:14.87289Z","shell.execute_reply":"2021-11-17T17:18:14.875701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = []\n\nfeats = ['question1','question2']\nfor f in feats:\n    train[f] = train[f].astype(str)\n    test[f] = test[f].astype(str)\n    corpus+=train[f].values.tolist()\n    \n    \ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\nX_q1 = tokenizer.texts_to_sequences(train['question1'])\nX_q2 = tokenizer.texts_to_sequences(train['question2'])\n\nX_test_q1 = tokenizer.texts_to_sequences(test['question1'])\nX_test_q2 = tokenizer.texts_to_sequences(test['question2'])\n\n\nX_q1 = pad_sequences(X_q1, maxlen=MAX_SEQUENCE_LENGTH)\nX_q2 = pad_sequences(X_q2, maxlen=MAX_SEQUENCE_LENGTH)\nX_test_q1 = pad_sequences(X_test_q1, maxlen=MAX_SEQUENCE_LENGTH)\nX_test_q2 = pad_sequences(X_test_q2, maxlen=MAX_SEQUENCE_LENGTH)\n\ny = train['is_duplicate'].values\n\nword_index = tokenizer.word_index\nnb_words = len(word_index)+1","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:18:14.877879Z","iopub.execute_input":"2021-11-17T17:18:14.878254Z","iopub.status.idle":"2021-11-17T17:20:59.132548Z","shell.execute_reply.started":"2021-11-17T17:18:14.87822Z","shell.execute_reply":"2021-11-17T17:20:59.131754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\nglove_path = '../input/glove840b300dtxt/glove.840B.300d.txt'\nembedding_matrix,unknown_words = build_matrix(word_index,glove_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:20:59.134598Z","iopub.execute_input":"2021-11-17T17:20:59.134847Z","iopub.status.idle":"2021-11-17T17:25:18.134761Z","shell.execute_reply.started":"2021-11-17T17:20:59.134814Z","shell.execute_reply":"2021-11-17T17:25:18.133999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nX_train_q1,X_val_q1,X_train_q2,X_val_q2,y_train,y_val = train_test_split(X_q1,X_q2,y,train_size=0.8,random_state=1024)\nprint(X_train_q1.shape,X_val_q1.shape)\nX_train = [X_train_q1,X_train_q2]\nX_val = [X_val_q1,X_val_q2]\nX_test = [X_test_q1,X_test_q2]","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:25:18.136257Z","iopub.execute_input":"2021-11-17T17:25:18.136539Z","iopub.status.idle":"2021-11-17T17:25:18.253255Z","shell.execute_reply.started":"2021-11-17T17:25:18.136503Z","shell.execute_reply":"2021-11-17T17:25:18.252523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_q1 = Input(shape = X_train[0].shape[1])\ninput_q2 = Input(shape = X_train[0].shape[1])\n\nembedding_layer = Embedding(nb_words,\n                            EMBEDDING_DIM,\n                            input_length = X_train[0].shape[1],\n                            weights = [embedding_matrix],\n                            trainable=False)\n\n\nembedded_sequences_q1 = embedding_layer(input_q1)\nembedded_sequences_q2 = embedding_layer(input_q2)\n\nbilstm_layer = Bidirectional(LSTM(64, return_sequences=False))\n\nx1 = bilstm_layer(embedded_sequences_q1)\nx2 = bilstm_layer(embedded_sequences_q2)\n\ndistance = layers.Concatenate()([x1, x2])\nx = Dense(64, activation=\"relu\")(distance)\npreds = Dense(1, activation=\"sigmoid\")(x)\n\nmodel = Model(inputs=[input_q1, input_q2], outputs=preds)\nmodel.compile(loss=\"binary_crossentropy\",\n              optimizer=\"adam\",\n              metrics=[\"accuracy\"])\n\n\n\nmodel.summary()\nutils.plot_model(model, show_shapes=True, expand_nested=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:25:18.254585Z","iopub.execute_input":"2021-11-17T17:25:18.254991Z","iopub.status.idle":"2021-11-17T17:25:22.395954Z","shell.execute_reply.started":"2021-11-17T17:25:18.254952Z","shell.execute_reply":"2021-11-17T17:25:22.395124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = callbacks.EarlyStopping(\n    monitor='val_loss', patience=5, verbose=1, restore_best_weights=True\n)\n\nrlp = callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=2, min_lr=1e-10, mode='min', verbose=1\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=32, \n    epochs=100,\n    callbacks=[es, rlp]\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:58:10.47248Z","iopub.execute_input":"2021-11-17T17:58:10.47311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test = model.predict(X_test)\n\nsubmission = pd.DataFrame()\nsubmission['test_id'] = test['test_id']\nsubmission['is_duplicate'] = y_pred_test\nsubmission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:28:56.607482Z","iopub.execute_input":"2021-11-17T17:28:56.608158Z","iopub.status.idle":"2021-11-17T17:42:32.351223Z","shell.execute_reply.started":"2021-11-17T17:28:56.608116Z","shell.execute_reply":"2021-11-17T17:42:32.350466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}