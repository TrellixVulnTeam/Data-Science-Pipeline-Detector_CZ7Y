{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"45bfec16-ca10-8fda-70e7-98646ec67ab1"},"source":"Visualizing Word Vectors with t-SNE\n\nTSNE is pretty useful when it comes to visualizing similarity between objects. It works by taking a group of high-dimensional (100 dimensions via Word2Vec) vocabulary word feature vectors, then compresses them down to 2-dimensional x,y coordinate pairs. The idea is to keep similar words close together on the plane, while maximizing the distance between dissimilar words.\n\nSteps\n1.Clean the data\n2.Build a corpus\n3.Train a Word2Vec Model\n4.Visualize t-SNE representations of the most common words\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02fed8da-9976-9f4f-59e2-4e79406e536e"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport nltk\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2040e5cb-a9f6-5d2f-9a6e-e8bf5c504e57"},"outputs":[],"source":"data = pd.read_csv(\"../input/train.csv\")\ndata.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9d566b7-8e47-071a-15a1-ea1fba0dfb12"},"outputs":[],"source":"##data_test = pd.read_csv(\"../input/test.csv\")\n#data_test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"087724b7-35af-1fc4-9f36-e827cba4a09a"},"outputs":[],"source":"STOP_WORDS = nltk.corpus.stopwords.words()\n\ndef clean_sentence(val):\n    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n    regex = re.compile('([^\\s\\w]|_)+')\n    sentence = regex.sub('', val).lower()\n    sentence = sentence.split(\" \")\n    \n    for word in list(sentence):\n        if word in STOP_WORDS:\n            sentence.remove(word)  \n            \n    sentence = \" \".join(sentence)\n    return sentence\n\ndef clean_dataframe(data):\n    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n    data = data.dropna(how=\"any\")\n    \n    for col in ['question1', 'question2']:\n        data[col] = data[col].apply(clean_sentence)\n    \n    return data\n\ndata = clean_dataframe(data)\ndata.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd9402b4-e8fa-c1fe-0eb8-d73c9cc678b4"},"outputs":[],"source":"##data_test = clean_dataframe(data_test)\n#data_test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05b7ad63-f138-1112-adc6-c1fba8b4ed23"},"outputs":[],"source":"def build_corpus(data):\n    corpus = []\n    \n    for col in ['question1','question2']:\n         for sentence in data[col].iteritems():\n            word_list = sentence[1].split(\" \")\n            corpus.append(word_list)\n     \n    return corpus\n\ncorpus = build_corpus(data)        \ncorpus[0:2]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5394001-ea76-ded1-19c4-6cf005ea3b74"},"outputs":[],"source":"#corpus_test = build_corpus(data_test)\n#corpus_test[0:2]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a711d8c-be31-e984-9222-125f694d3027"},"outputs":[],"source":"def build_corpus_q(data):\n    corpus = []\n    for sentence in data.iteritems():\n            #word_list = sentence[1].split(\" \")\n            #corpus.append(word_list)\n            corpus.append(sentence[1])\n     \n    return corpus\n\ncorpus_q1 = build_corpus_q(data['question1'])\ncorpus_q2 = build_corpus_q(data['question2'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68151665-b61f-5772-7226-6bfb0ef2f2ee"},"outputs":[],"source":"#corpus_test_q1 = build_corpus_q(data_test['question1'])\n#corpus_test_q2 = build_corpus_q(data_test['question2'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44fae1a6-6b09-5d6b-7cb0-d41be47da284"},"outputs":[],"source":"from gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=200, workers=4)\nmodel.wv['trump']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac463ff2-25dd-713c-abda-086d9220a266"},"outputs":[],"source":"##model_test = word2vec.Word2Vec(corpus_test, size=100, window=20, min_count=200, workers=4)\n#model_test.wv['trump']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3302a39b-6830-345f-7c4c-a6feea8e2b87"},"outputs":[],"source":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da582a96-a2ce-b230-a8aa-11088b67c6e9"},"outputs":[],"source":"tsne_plot(model)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f6287e6-2057-da6e-6126-8625cf5dee52"},"outputs":[],"source":"#tsne_plot(model_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1207a1b-d9bf-4e20-c7ed-35dcfb5a259c"},"outputs":[],"source":"# A more selective model\n#model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=1000, workers=4)\n#tsne_plot(model)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f4b9b70-ce72-72c4-2c81-0715b9fa378c"},"outputs":[],"source":"model.most_similar('india')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3780e60a-bef8-98e6-4cf1-8055149c0b0a"},"outputs":[],"source":"def get_tsne_vector(model):\n    \"Creates a TSNE model and use it with Word2Vec to find how similar both questions are\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n    \n    return labels, x, y"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ac5c011-f97a-75a2-7a43-5d46371a1742"},"outputs":[],"source":"words_set, X, Y = get_tsne_vector(model)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41a456d2-8c4b-feb9-ee24-c6bce5ae8877"},"outputs":[],"source":"#words_test, X_test, Y_test = get_tsne_vector(model_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71301f48-7b74-fb82-f015-73aeba1e0c0e"},"outputs":[],"source":"train_qs = pd.Series(data['question1'].tolist() + data['question2'].tolist()).astype(str)\nfrom collections import Counter\n\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\n\neps = 5000 \n\nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2755b0d-6999-f3f2-6572-f4cfe05d7833"},"outputs":[],"source":"#train_qs_test = pd.Series(data_test['question1'].tolist() + data_test['question2'].tolist()).astype(str)\n#from collections import Counter\n\n#def get_weight(count, eps=10000, min_count=2):\n#    if count < min_count:\n#        return 0\n#    else:\n#        return 1 / (count + eps)\n\n#eps = 5000 \n\n#words_test = (\" \".join(train_qs_test)).lower().split()\n#counts_test = Counter(words_test)\n#weights_test = {word: get_weight(count) for word, count in counts_test.items()}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b935fb9-9ee1-c8d8-bcb3-90ffcb72bf96"},"outputs":[],"source":"def tfidf_word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in STOP_WORDS:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in STOP_WORDS:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    \n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    \n    R = np.sum(shared_weights) / np.sum(total_weights)\n    return R"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe915dfa-6003-fcbc-1068-6013ca8df67e"},"outputs":[],"source":"##def tfidf_word_match_share_test(row):\n#    q1words = {}\n#    q2words = {}\n#    for word in str(row['question1']).lower().split():\n#        if word not in STOP_WORDS:\n#            q1words[word] = 1\n#    for word in str(row['question2']).lower().split():\n#        if word not in stoSTOP_WORDSps:\n#           q2words[word] = 1\n#    if len(q1words) == 0 or len(q2words) == 0:\n#        # The computer-generated chaff includes a few questions that are nothing but stopwords\n#        return 0\n    \n#    shared_weights = [weights_test.get(w, 0) for w in q1words.keys() if w in q2words] + [weights_test.get(w, 0) for w in q2words.keys() if w in q1words]\n#    total_weights = [weights_test.get(w, 0) for w in q1words] + [weights_test.get(w, 0) for w in q2words]\n    \n#    R = np.sum(shared_weights) / np.sum(total_weights)\n#    return R"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9cfb3dac-8c1e-6c81-5236-e8a4b07ad67a"},"outputs":[],"source":"tfidf_train_word_match = data.apply(tfidf_word_match_share, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d55fce3-16e3-e396-906b-e0621d9ef907"},"outputs":[],"source":"#tfidf_train_word_match_test = data_test.apply(tfidf_word_match_share_test, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32f7959f-1ac9-80ab-3918-a20187b5e653"},"outputs":[],"source":"def word2vec_and_tfidf(data,words,X,Y,tfidf_train_word_match):\n    val = []\n    for index, row in data.iterrows():\n        val1x, val1y = findVal(row['question1'],words,X,Y,tfidf_train_word_match,index)\n        val2x, val2y = findVal(row['question2'],words,X,Y,tfidf_train_word_match,index)\n        temp = np.square(val1x-val2x) + np.square(val1y-val2y)\n        val.append(temp)\n      \n    return val"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d971942f-fb9c-ca00-0424-f1a3bea61b53"},"outputs":[],"source":"def findVal(ques,words,X,Y,tfidf,i):\n    valx = 0\n    valy = 0\n    for wrd in list(ques.split(\" \")):\n        if wrd in words:\n            index = words.index(wrd)\n            valx = valx + tfidf[i]*X[index]\n            valy = valy + tfidf[i]*Y[index]\n    \n    return valx, valy"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1ba84ce-7d2b-5f2b-1ddc-3b468d36edef"},"outputs":[],"source":"data['val'] = word2vec_and_tfidf(data,words_set,X,Y,tfidf_train_word_match)\ndata.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11eb65e5-f968-f737-05a3-c87c528a0ad6"},"outputs":[],"source":"data[data['is_duplicate']==1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f89d9d2-d252-085b-439c-249740327d66"},"outputs":[],"source":"#data.to_csv(\"data.csv\", sep='|', encoding='utf-8')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38a5413a-843b-7d1f-892d-7b8dd1beadd0"},"outputs":[],"source":"#temp = pd.read_csv('data.csv')\n#temp.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcdc0229-f414-c060-7824-9c33fca3175f"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d4f0fa9-d248-cd6c-97b8-41e39a0e02c0"},"outputs":[],"source":"data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\ndata['len_q2'] = data.question2.apply(lambda x: len(str(x)))\ndata['diff_len'] = data.len_q1 - data.len_q2\ndata['len_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ','')))))\ndata['len_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ','')))))\ndata['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\ndata['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\ndata['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))),axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1b2a92c-9e8e-2a26-f062-697df0ab4bfa"},"outputs":[],"source":"from fuzzywuzzy import fuzz\n\ndata['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']),str(x['question2'])),axis=1)\ndata['fuzz_wratio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']),str(x['question2'])),axis=1)\ndata['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']),str(x['question2'])),axis=1)\ndata['fuzz_partial_token_set_ratio'] = data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']),str(x['question2'])),axis=1)\ndata['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']),str(x['question2'])),axis=1)\ndata['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']),str(x['question2'])),axis=1)\ndata['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']),str(x['question2'])),axis=1)\ndata.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cbfefa0d-b2c6-f0d6-08bd-1ee6711868fb"},"outputs":[],"source":"data.fillna(0, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29d27ca5-889b-7dd3-474f-4ee0a031de3a"},"outputs":[],"source":"train, valid = train_test_split(data, test_size = 0.2)\nx_valid = valid\nx_train = train\ny_valid = valid['is_duplicate']\ny_train = train['is_duplicate']\n#x_train.drop(['id','qid1','qid2','question1','question2','is_duplicate'],axis=1,inplace=True)\n#x_test.drop(['id','qid1','qid2','question1','question2','is_duplicate'],axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d644c2d-2118-e858-9f4c-981acd0d0d28"},"outputs":[],"source":"#x_train.head()\n#x_test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b47eb081-c4e7-03db-368f-23ac585e7e35"},"outputs":[],"source":"x_train.drop(['question1','question2','is_duplicate'],axis=1,inplace=True)\nx_valid.drop(['question1','question2','is_duplicate'],axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6e225f6-856f-a5df-7834-a0df6d178870"},"outputs":[],"source":"model = RandomForestClassifier(100,oob_score=True)\nmodel.fit(x_train,y_train)\nmodel.score(x_valid,y_valid)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ca1880c-d22b-5449-a997-117c2c882ca0"},"outputs":[],"source":"import xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"592f7187-3eaf-02a7-8d21-0b3f70ed7001"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}