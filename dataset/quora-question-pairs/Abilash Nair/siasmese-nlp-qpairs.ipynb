{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-08T11:13:08.27016Z","iopub.execute_input":"2021-10-08T11:13:08.271311Z","iopub.status.idle":"2021-10-08T11:13:08.370963Z","shell.execute_reply.started":"2021-10-08T11:13:08.27119Z","shell.execute_reply":"2021-10-08T11:13:08.370271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data =  pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:13:08.37279Z","iopub.execute_input":"2021-10-08T11:13:08.373059Z","iopub.status.idle":"2021-10-08T11:13:10.715835Z","shell.execute_reply.started":"2021-10-08T11:13:08.373013Z","shell.execute_reply":"2021-10-08T11:13:10.715115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:13:10.717124Z","iopub.execute_input":"2021-10-08T11:13:10.717398Z","iopub.status.idle":"2021-10-08T11:13:10.747757Z","shell.execute_reply.started":"2021-10-08T11:13:10.717364Z","shell.execute_reply":"2021-10-08T11:13:10.746985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test =  pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:13:10.749942Z","iopub.execute_input":"2021-10-08T11:13:10.750222Z","iopub.status.idle":"2021-10-08T11:13:12.246878Z","shell.execute_reply.started":"2021-10-08T11:13:10.750187Z","shell.execute_reply":"2021-10-08T11:13:12.246114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:13:12.248689Z","iopub.execute_input":"2021-10-08T11:13:12.249418Z","iopub.status.idle":"2021-10-08T11:13:12.26632Z","shell.execute_reply.started":"2021-10-08T11:13:12.249378Z","shell.execute_reply":"2021-10-08T11:13:12.265726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:13:12.267346Z","iopub.execute_input":"2021-10-08T11:13:12.267627Z","iopub.status.idle":"2021-10-08T11:13:51.860835Z","shell.execute_reply.started":"2021-10-08T11:13:12.267591Z","shell.execute_reply":"2021-10-08T11:13:51.859814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gensim==3.8.0","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:13:51.862258Z","iopub.execute_input":"2021-10-08T11:13:51.862492Z","iopub.status.idle":"2021-10-08T11:14:02.830379Z","shell.execute_reply.started":"2021-10-08T11:13:51.862461Z","shell.execute_reply":"2021-10-08T11:14:02.829495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nfrom nltk.corpus import stopwords\nfrom gensim.models import KeyedVectors\n\nimport gensim\n\nimport numpy as np\n\nimport itertools\n\n\ndef text_to_word_list(text):\n    # Pre process and convert texts to a list of words\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    text = text.split()\n\n    return text\n\n\ndef make_w2v_embeddings(df, embedding_dim=300, empty_w2v=False):\n    vocabs = {}\n    vocabs_cnt = 0\n\n    vocabs_not_w2v = {}\n    vocabs_not_w2v_cnt = 0\n\n    # Stopwords\n    stops = set(stopwords.words('english'))\n\n    # Load word2vec\n    print(\"Loading word2vec model(it may takes 2-3 mins) ...\")\n    from gensim import models\n    \n    if empty_w2v:\n        word2vec = EmptyWord2Vec\n    else:\n        word2vec = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\", binary=True)\n        #word2vec = models.Word2Vec.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\", binary=True)\n\n        # word2vec = gensim.models.word2vec.Word2Vec.load(\"./data/Quora-Question-Pairs.w2v\").wv\n\n    for index, row in df.iterrows():\n        # Print the number of embedded sentences.\n        if index != 0 and index % 1000 == 0:\n            print(\"{:,} sentences embedded.\".format(index), flush=True)\n\n        # Iterate through the text of both questions of the row\n        for question in ['question1', 'question2']:\n\n            q2n = []  # q2n -> question numbers representation\n            for word in text_to_word_list(row[question]):\n                # Check for unwanted words\n                if word in stops:\n                    continue\n\n                # If a word is missing from word2vec model.\n                if word not in word2vec.vocab:\n                    if word not in vocabs_not_w2v:\n                        vocabs_not_w2v_cnt += 1\n                        vocabs_not_w2v[word] = 1\n\n                # If you have never seen a word, append it to vocab dictionary.\n                if word not in vocabs:\n                    vocabs_cnt += 1\n                    vocabs[word] = vocabs_cnt\n                    q2n.append(vocabs_cnt)\n                else:\n                    q2n.append(vocabs[word])\n\n            # Append question as number representation\n            df.at[index, question + '_n'] = q2n\n\n    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n    embeddings[0] = 0  # So that the padding will be ignored\n\n    # Build the embedding matrix\n    for word, index in vocabs.items():\n        if word in word2vec.vocab:\n            embeddings[index] = word2vec.word_vec(word)\n    del word2vec\n\n    return df, embeddings\n\n\ndef split_and_zero_padding(df, max_seq_length):\n    # Split to dicts\n    X = {'left': df['question1_n'], 'right': df['question2_n']}\n\n    # Zero padding\n    for dataset, side in itertools.product([X], ['left', 'right']):\n        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n\n    return dataset\n\n\n#  --\n\nclass ManDist(Layer):\n    \"\"\"\n    Keras Custom Layer that calculates Manhattan Distance.\n    \"\"\"\n\n    # initialize the layer, No need to include inputs parameter!\n    def __init__(self, **kwargs):\n        self.result = None\n        super(ManDist, self).__init__(**kwargs)\n\n    # input_shape will automatic collect input shapes to build layer\n    def build(self, input_shape):\n        super(ManDist, self).build(input_shape)\n\n    # This is where the layer's logic lives.\n    def call(self, x, **kwargs):\n        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n        return self.result\n\n    # return output shape\n    def compute_output_shape(self, input_shape):\n        return K.int_shape(self.result)\n\n\nclass EmptyWord2Vec:\n    \"\"\"\n    Just for test use.\n    \"\"\"\n    vocab = {}\n    word_vec = {}","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:14:02.834542Z","iopub.execute_input":"2021-10-08T11:14:02.834775Z","iopub.status.idle":"2021-10-08T11:14:10.69467Z","shell.execute_reply.started":"2021-10-08T11:14:02.834746Z","shell.execute_reply":"2021-10-08T11:14:10.69392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time\nimport pandas as pd\n\nimport matplotlib\n\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\nfrom tensorflow.python.keras.models import Model, Sequential\nfrom tensorflow.python.keras.layers import Input, Embedding, LSTM, GRU, Conv1D, Conv2D, GlobalMaxPool1D, Dense, Dropout\n\n#from util import make_w2v_embeddings\n#from util import split_and_zero_padding\n#from util import ManDist\n\n# File paths\nTRAIN_CSV = \"../input/quora-question-pairs/train.csv.zip\"\n\n# Load training set\ntrain_df = pd.read_csv(TRAIN_CSV)\nfor q in ['question1', 'question2']:\n    train_df[q + '_n'] = train_df[q]\n\n# Make word2vec embeddings\nembedding_dim = 300\nmax_seq_length = 20\nuse_w2v = True\n\ntrain_df, embeddings = make_w2v_embeddings(train_df, embedding_dim=embedding_dim, empty_w2v=not use_w2v)\n\n# Split to train validation\nvalidation_size = int(len(train_df) * 0.1)\ntraining_size = len(train_df) - validation_size\n\nX = train_df[['question1_n', 'question2_n']]\nY = train_df['is_duplicate']\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n\nX_train = split_and_zero_padding(X_train, max_seq_length)\nX_validation = split_and_zero_padding(X_validation, max_seq_length)\n\n# Convert labels to their numpy representations\nY_train = Y_train.values\nY_validation = Y_validation.values\n\n# Make sure everything is ok\nassert X_train['left'].shape == X_train['right'].shape\nassert len(X_train['left']) == len(Y_train)\n\n# --\n\n# Model variables\ngpus = 2\nbatch_size = 1024 * gpus\nn_epoch = 2\nn_hidden = 50\n\n# Define the shared model\nx = Sequential()\nx.add(Embedding(len(embeddings), embedding_dim,\n                weights=[embeddings], input_shape=(max_seq_length,), trainable=False))\n# CNN\n# x.add(Conv1D(250, kernel_size=5, activation='relu'))\n# x.add(GlobalMaxPool1D())\n# x.add(Dense(250, activation='relu'))\n# x.add(Dropout(0.3))\n# x.add(Dense(1, activation='sigmoid'))\n# LSTM\nx.add(LSTM(n_hidden))\n\nshared_model = x\n\n# The visible layer\nleft_input = Input(shape=(max_seq_length,), dtype='int32')\nright_input = Input(shape=(max_seq_length,), dtype='int32')\n\n# Pack it all up into a Manhattan Distance model\nmalstm_distance = ManDist()([shared_model(left_input), shared_model(right_input)])\nmodel = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n\n#if gpus >= 2:\n    # `multi_gpu_model()` is a so quite buggy. it breaks the saved model.\n #   model = tf.keras.utils.multi_gpu_model(model, gpus=gpus)\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\nmodel.summary()\nshared_model.summary()\n\n# Start trainings\ntraining_start_time = time()\nmalstm_trained = model.fit([X_train['left'], X_train['right']], Y_train,\n                           batch_size=batch_size, epochs=n_epoch,\n                           validation_data=([X_validation['left'], X_validation['right']], Y_validation))\ntraining_end_time = time()\nprint(\"Training time finished.\\n%d epochs in %12.2f\" % (n_epoch,\n                                                        training_end_time - training_start_time))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:14:10.699001Z","iopub.execute_input":"2021-10-08T11:14:10.700928Z","iopub.status.idle":"2021-10-08T11:19:58.285393Z","shell.execute_reply.started":"2021-10-08T11:14:10.70089Z","shell.execute_reply":"2021-10-08T11:19:58.284301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nmalstm_distance = ManDist()([shared_model(left_input), shared_model(right_input)])\n\n# Added the logits layer followed by Dense to get binary_crossentropy loss\n# becauase our targets are binary not continuous\n\nmalstm_distance_logits = Dense(1)(malstm_distance)\nmodel = Model(inputs=[left_input, right_input], outputs=[malstm_distance_logits])\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('./data/SiameseLSTM.h5')\n\n# Plot accuracy\nplt.subplot(211)\nplt.plot(malstm_trained.history['accuracy'])\nplt.plot(malstm_trained.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\n# Plot loss\nplt.subplot(212)\nplt.plot(malstm_trained.history['loss'])\nplt.plot(malstm_trained.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\nplt.tight_layout(h_pad=1.0)\nplt.savefig('./data/history-graph.png')\n\nprint(str(malstm_trained.history['val_accuracy'][-1])[:6] +\n      \"(max: \" + str(max(malstm_trained.history['val_accuracy']))[:6] + \")\")\nprint(\"Done.\")","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:20:21.21642Z","iopub.execute_input":"2021-10-08T11:20:21.216699Z","iopub.status.idle":"2021-10-08T11:20:21.833998Z","shell.execute_reply.started":"2021-10-08T11:20:21.21667Z","shell.execute_reply":"2021-10-08T11:20:21.83317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nimport tensorflow as tf\n\n#from util import make_w2v_embeddings\n#from util import split_and_zero_padding\n#from util import ManDist\n\n# File paths\nTEST_CSV = '../input/quora-question-pairs/train.csv.zip'\n\n# Load training set\ntest_df = pd.read_csv(TEST_CSV)\nfor q in ['question1', 'question2']:\n    test_df[q + '_n'] = test_df[q]\n\n# Make word2vec embeddings\nembedding_dim = 300\nmax_seq_length = 20\ntest_df, embeddings = make_w2v_embeddings(test_df, embedding_dim=embedding_dim, empty_w2v=False)\n\n# Split to dicts and append zero padding.\nX_test = split_and_zero_padding(test_df, max_seq_length)\n\n# Make sure everything is ok\nassert X_test['left'].shape == X_test['right'].shape\n\n# --\n\nmodel = tf.keras.models.load_model('./data/SiameseLSTM.h5', custom_objects={'ManDist': ManDist})\nmodel.summary()\n\nprediction = model.predict([X_test['left'], X_test['right']])\nprint(prediction)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T11:28:59.659904Z","iopub.execute_input":"2021-10-08T11:28:59.660193Z","iopub.status.idle":"2021-10-08T11:37:33.375847Z","shell.execute_reply.started":"2021-10-08T11:28:59.660164Z","shell.execute_reply":"2021-10-08T11:37:33.374914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}