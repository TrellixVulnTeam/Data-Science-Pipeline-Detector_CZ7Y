{"cells":[{"metadata":{"_uuid":"43a02d0fd33d89c1065243460415190a67e7c8da"},"cell_type":"markdown","source":"### \n![LOGO_ADWAY](http://www.adway-conseil.com/wp-content/themes/adway/images/logo_adway.png?raw=true \"Logo Title Text 1\")\n<h1><span style=\"color:#006600\"><div style=\"text-align:center;\"> Formation Natural Language Processing </div></span></h1>\n\n<h1><span style=\"color:#006600\"><div style=\"text-align:center;\"> Session 2 - Cas pratique  </div></span></h1>\n\n\n<h2><div style=\"text-align:center;\"> Quora Pair Question </div></h2>\n\n\n\n\n\n\n\n\n**Prérequis** :  Avoir bien écouté la première session !\n\n\n"},{"metadata":{"_uuid":"d81fab480cd8443fc1547d905748f5484d6f7534"},"cell_type":"markdown","source":"## Sommaire\n\nSurprise !!!\n\n\n\n## Introduction\n\nL'objectif de cette compétition est d'identifier quelles questions posées sur Quora, un forum accueillant plus de 100 millions de visiteurs par mois, sont des doubles de questions déjà posées. \nUne telle étude peut être utile afin de fournir instantanément des réponses aux questions précédemment résolues. \nCette compétition consiste donc à prédire si une paire de questions sur Quora est identique. Pour ce problème, nous avons environ 400.000 exemples d'entraînement. Chaque ligne se compose de deux phrases et d'une étiquette binaire qui nous indique si les deux questions étaient identiques ou non\n\n\n### Description de la compétition\nLe but de la compétition \"Quora Question Pair\" est de prédire laquelle des paires de questions fournies contient en réalité deux questions ayant le même sens.  \n"},{"metadata":{"_uuid":"a9b4be0323cbd73993a26329ea1d69075603142e"},"cell_type":"markdown","source":"# <span style=\"color:#006600\"> 1. Initialisation de l'environnement </span>\n\n## <span style=\"color:#39ac39\"> 1.1. Téléchargement des librairies </span>"},{"metadata":{"trusted":true,"_uuid":"bfcdbdd7e61007c1f933a15d7c5d2363f43995c1","collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk.data\nimport re\nimport time\n#from pattern.text.en import singularize\n#from inflection import singularize\nfrom nltk.corpus import stopwords\nfrom collections import Counter, defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\n%matplotlib inline\n#import heapq\nimport string \nimport operator\nimport unicodedata\n# import mathtqdm\nimport gensim\nimport itertools\nimport sys\n\nimport time # pour le timer\n#import hunspell\n#from spellchecker import SpellChecker\n# from tqdm import tqdm_notebook, tqdm\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n    \n    \n#tqdm_notebook(tqdm.pandas())\npal = sns.color_palette()\n#pd.options.display.max_colwidth=100\nfrom functools import reduce\n\n#Telecharger NLTK, wordnet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c796d61cf716e0f5cd45e56ec2b08db1900ddf81"},"cell_type":"markdown","source":"## <span style=\"color:#39ac39\"> 1.2. Téléchargement des bases de données de train et test\n\nLa base de données d'apprentissage est composée des données suivantes : \n* **id** - identifiant de la paire de question\n* **qid1, qid2** - respectivement, identifiant unique de la question 1 (resp. question 2 ) \n* **question1, question2** - texte intégrale pour chacune des questions composants la paire\n* **is_duplicate** - il s'agit de la variable (variable a prédire) qui prend la valeur 1 si les deux questions ont le même sens, 0 sinon"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# Téléchargement de la base de train \ndf_train = pd.read_csv('../input/quora-question-pairs/train.csv')\n\n# Téléchargement de la base de test\ndf_test = pd.read_csv('../input/quora-question-pairs/test.csv', \n                      nrows=580000)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac83bc67a1104d46a30745d0bfb5c884e0f655f3"},"cell_type":"markdown","source":"# <span style=\"color:#006600\">2. Description des bases de données </span>\n\n## <span style=\"color:#39ac39\"> 2.1. Snapshot du train"},{"metadata":{"trusted":true,"_uuid":"63012f756d4d7d68f641506080ae12314f26f04d","scrolled":true},"cell_type":"code","source":"print(\"Dimension de la base de train : {} \\n\".format(df_train.shape)) # (404287, 5)\ndf_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c74bdb95761af04a2474879acf59c97ab5c017d8","scrolled":true,"collapsed":true},"cell_type":"code","source":"df_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"678488fae379ce58d0f5a8833df881bbbe8499be"},"cell_type":"markdown","source":"## <span style=\"color:#39ac39\"> 2.2. Snapshot du test"},{"metadata":{"trusted":true,"_uuid":"ca40d7ec3240f79d9d97e0f8e9de7fa53d5c52f1","collapsed":true},"cell_type":"code","source":"print(\"Dimension de la base de test : {} \\n\".format(df_test.shape)) # (2345796, 3)\ndf_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"817566c1f640c80f8eea21e9520c073642c26ce9","collapsed":true},"cell_type":"code","source":"df_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fc2dce283a778f61361490a6fb19bb484799cd2"},"cell_type":"markdown","source":"## <span style=\"color:#39ac39\"> 2.3. Quelques statistiques sur la base de train"},{"metadata":{"trusted":true,"_uuid":"3e2790135ea7b1ed04aecad3ae1abc7f113591cf","scrolled":true,"collapsed":true},"cell_type":"code","source":"print(\"Analyse de la variable cible (is_duplicate) : \\n******************************************** \")\nprint('Total number of question pairs for training: {}'.format(len(df_train)))\nprint('Duplicate pairs: {}% \\n******************************************** '.format(round(df_train['is_duplicate'].mean()*100, 2)))\nprint('An example of duplicate pair : \\nQuestion1 : {} \\nQuestion2 : {} \\n******************************************** '.format(df_train[df_train['is_duplicate']==1][1:2].question1.values,\n                                                                               df_train[df_train['is_duplicate']==1][1:2].question2.values))\ndf_train.groupby(\"is_duplicate\")['id'].count().plot.bar()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc508eccd0956676bca50cef3c60c92b95fb2c80","collapsed":true},"cell_type":"code","source":"# Suppression de la colonne ID et des valeurs manquantes \ndf_train.drop(['id'], axis=1, inplace=True)\ndf_train = df_train.dropna()\n\n# Concatenation des questions en une seule liste\nqids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\n\nprint('Total number of questions in the training data: {}'.format(len(np.unique(qids))))\nprint('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))\n\nplt.figure(figsize=(12, 5))\nplt.hist(qids.value_counts(), bins=50)\nplt.yscale('log', nonposy='clip')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21205e37041f80917b07ee69643de8af2b146817"},"cell_type":"markdown","source":"## <span style=\"color:#39ac39\"> 2.4. Datavisualisation - WordCloud\n\nUn <span style=\"background-color: #ccffcc;\"> wordcloud </span> (ou nuage de mots-clés / nuage de tags) est une répresentation visuelle de mots dont la taille est proportionelle à la fréquence de ce mot dans un texte donné."},{"metadata":{"trusted":true,"_uuid":"0a07fde22d0bdbafbfade3fb3a59d007819b07b2","collapsed":true},"cell_type":"code","source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n\nfrom wordcloud import WordCloud\ncloud = WordCloud(width=1440, \n                  height=1080).generate(\" \".join(train_qs.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"479323b4443b71a47416f26d2d8271ee97aedb36"},"cell_type":"markdown","source":"**Conclusion** : avec cette représentation graphique, on constate qu'il y a beaucoup de mot qui ne sont pas particulièrement utiles pour mettre en valeurs les grands thèmes des questions"},{"metadata":{"_uuid":"0c24272b4923bc9ef2fb4c044c4d2dd49d678c5a"},"cell_type":"markdown","source":"# <span style=\"color:#006600\">3. Prétraitement de la base et construction des features</span>\n\n##  <span style=\"color:#39ac39\"> 3.1. Suppression des phrases trop courtes\nCe premier prétraitement consiste à supprimer de notre base les phrases trop courte (moins de 11 caractères) car il est peu probable d'apprendre beaucoup de ces données. \n\nQuelques exemples sont affichés ci dessous : "},{"metadata":{"trusted":true,"_uuid":"9c85370d525ac7a4599e964bd05e232f89ea5e15","collapsed":true},"cell_type":"code","source":"print(\"Nombre de question 1 contenant moins de 11 caractères : \", len(df_train.index[df_train['question1'].apply(len)<11]))\nprint(\"Nombre de question 2 contenant moins de 11 caractères : \", len(df_train.index[df_train['question2'].apply(len)<11]))\n\nprint(\"\\nAffichage de quelques exemples pour les questions 1 : \\n -------------------------------------------------------- \")\nlistIndex1 = df_train.index[df_train['question1'].apply(len)<11][0:10]\nprint(df_train.loc[listIndex1].question1)\n\nprint(\"\\nAffichage de quelques exemples pour les questions 2 : \\n --------------------------------------------------------\")\nlistIndex2 = df_train.index[df_train['question2'].apply(len)<11][0:10]\nprint(df_train.loc[listIndex2].question2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd47f9d657894094045f16d4e5d553a9bbd8207e"},"cell_type":"markdown","source":"Ces questions sont donc supprimées : "},{"metadata":{"trusted":true,"_uuid":"13a21e283812ca72c885d0dc106a87d8e9bac895","collapsed":true},"cell_type":"code","source":"df_train.drop(df_train.index[df_train['question1'].apply(len)<11],inplace=True)\ndf_train.drop(df_train.index[df_train['question2'].apply(len)<11],inplace=True)\n\nprint(\"Dimension de la base de train : {}\".format(df_train.shape)) # (404287, 5)\nprint(\"Dimension de la base de test : {}\".format(df_test.shape)) # (2345796, 3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e514cf5c8b2e72d3aaa785f1148995ea7f2d828"},"cell_type":"markdown","source":" ## <span style=\"color:#39ac39\"> 3.2. Creation d'une table id/question pour travailler sur le prétaitement de chaque phrase\n\n### Sur la table d'apprentissage\n\n** Démarche ** : on va créer une liste unique pour lancer les traitements sur toutes les observations en même temps"},{"metadata":{"trusted":true,"_uuid":"4f1ea86669fdc856224f8a288e15810631f40b39","collapsed":true},"cell_type":"code","source":"# >>> Sur la base de train \nlistdf = [pd.DataFrame(df_train[['qid1','question1']].values),\n          pd.DataFrame(df_train[['qid2','question2']].values)]\nqdf=pd.concat(listdf)\ndel listdf; gc.collect()\nprint(\"Dimension de notre liste de question : \", qdf.shape)\nqdf.columns = [\"id\",\"question_orig\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"980df9184a4c6b8b358e91cda51c0829f0087632"},"cell_type":"markdown","source":"Verification s'il y a des id qui réapparaissent : "},{"metadata":{"trusted":true,"_uuid":"c75b26ec5a5c1b69ca983e6535e4e1f62e820484","collapsed":true},"cell_type":"code","source":"import collections\na = qdf.id.tolist()\nres= [item for item, count in collections.Counter(a).items() if count > 1]\nprint(\"Liste des id qui sont présents plusieurs fois dans la base : \", list(res[0:10]))\nprint(\"\\nAffichage d'un exemple : \") \nqdf[qdf.id == 3 ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b99c804ae9c93ea5986c9a397a97735a3cb4334","collapsed":true},"cell_type":"code","source":"# >>> Mise en forme de notre dataframe            \nprint(\"Nombre d'observation avant suppression des doublons : \", qdf.shape )\nqdf.drop_duplicates(subset=\"id\",inplace=True)\nprint(\"Nombre d'observation après suppression des doublons : \", qdf.shape )\nqdf.reset_index(inplace=True)\n\n# >>> Affichage d'un exemple\nni = 10000\nprint(\"Affichage de la question {} : \\n {}\".format(ni, list(qdf[qdf['id']==ni].question_orig)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56f6f9ba3debe2ccd41f5030e8b22cdafaacae9f"},"cell_type":"markdown","source":"### Sur la table de test"},{"metadata":{"trusted":true,"_uuid":"0f1ac3e05e4676bf3ab6b19791b65da586889429","collapsed":true},"cell_type":"code","source":"\"\"\"\n# >>> Generation des index pour chacune des questions\na = pd.Series(range(df_test.shape[0]*2))+1\nb = a.values.reshape(df_test.shape[0],2)\nc = pd.DataFrame(b,\n                 columns= [\"qid1\",\"qid2\"],\n                dtype='int64')\n\ndf_test['qid1']=np.nan\ndf_test['qid1'] = c.qid1\n\ndf_test['qid2']=np.nan\ndf_test['qid2'] = c.qid2\n\n# >>> Creation d'une seule et unique liste, qui contient la liste de question1 puis la liste de question2 \n# >>> Sur la base de train \nlistdf = [pd.DataFrame(df_test[['qid1','question1']].values),\n          pd.DataFrame(df_test[['qid2','question2']].values)]\nqdf_test=pd.concat(listdf)\ndel listdf; gc.collect()\nprint(\"Dimension de notre liste de question : \", qdf_test.shape)\n\n# >>> Mise en forme de notre dataframe\nqdf_test.columns = [\"id\",\"question_orig\"]\nqdf_test.drop_duplicates(subset=\"id\",inplace=True)\nqdf_test.reset_index(inplace=True)\n\nqdf_test.question_orig = qdf_test.question_orig.astype(\"str\")\n\n# >>> Affichage d'un exemple\nni = 10000\nprint(\"Affichage de la question {} : \\n {}\".format(ni, list(qdf_test[qdf_test['id']==ni].question_orig)))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3177eec43acd0202814e1215302a9189126ff798","collapsed":true},"cell_type":"code","source":"def Trouve_Exemples(s):\n    print(qdf['question_orig'][qdf[\"question_orig\"].apply(lambda x : s in x)][0:10])\n    \nTrouve_Exemples(\"milk\")\nTrouve_Exemples(\"000\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd607331821a737322dbd8122c6c6cdfa27b3919"},"cell_type":"markdown","source":" ## <span style=\"color:#39ac39\"> 3.3. Creations de features basiques\n \n Communément, des features assez basiques sont créées. Ces features ont pour objectif de mettre en exergue les éléments les plus basiques d'une phrase telle que :\n * La **présence d'une lettre capitale** en début de phrase (méthode _isupper()_ )\n * La **présence d'un caractère** en particulier (ici, on va rechercher le point d'interrogation \"?\" ou le point \".\")\n * La **présence de chiffre/nombre** (méthode _isdigit()_ )\n "},{"metadata":{"trusted":true,"_uuid":"a92a2dbf60e6e6030652d2ee521f94d4c4e1d6c4","collapsed":true},"cell_type":"code","source":"def basic_features(df):\n    qmarks = np.mean(df[\"question_orig\"].apply(lambda x: '?' in x))\n    math = np.mean(df[\"question_orig\"].apply(lambda x: '[math]' in x))\n    fullstop = np.mean(df[\"question_orig\"].apply(lambda x: '.' in x))\n    capital_first = np.mean(df[\"question_orig\"].apply(lambda x: x[0].isupper()))\n    capitals = np.mean(df[\"question_orig\"].apply(lambda x: max([y.isupper() for y in x])))\n    numbers = np.mean(df[\"question_orig\"].apply(lambda x: max([y.isdigit() for y in x])))\n\n    print('Questions with question marks: {:.2f}%'.format(qmarks * 100))\n    print('Questions with [math] tags: {:.2f}%'.format(math * 100))\n    print('Questions with full stops: {:.2f}%'.format(fullstop * 100))\n    print('Questions with capitalised first letters: {:.2f}%'.format(capital_first * 100))\n    print('Questions with capital letters: {:.2f}%'.format(capitals * 100))\n    print('Questions with numbers: {:.2f}%'.format(numbers * 100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b42f8694190433d65421c9f90596ca8bd93f43a","collapsed":true},"cell_type":"code","source":"# Lancement sur la base d'apprentissage\nbasic_features(qdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"98b628c8ecaed01b852d854806765afec50cefe7"},"cell_type":"code","source":"\"\"\"\n# Lancement sur la base de test\nbasic_features(qdf_test)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96b95fabe85c00e538e0164f04bb00131c56167b"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 3.4. Retraitement des phrases\n\n\n**Objectif** : \n1. extraire les nombres et les formules de maths\n2. réduire le vocabulaire en remplaçant les abréviations, en corrigeant les fautes d'orthographe et en lemmatizant\n3. repérer le mot interrogatif\n4. detecter les questions avec plusieurs phrases\n"},{"metadata":{"_uuid":"d1fcb9b70a76c279c352eb01219f23ac028365e9"},"cell_type":"markdown","source":"### <span style=\"color:#53c653\"> 3.4.1. Détection des équations\nOn va extraire tout ce qui est mathématiques (ce qui inclut les nombres). Une équation, c'est aussi simple et beau que ca : \n\n\\begin{align}\n\\nabla \\times \\vec{\\mathbf{B}} -\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{E}}}{\\partial t} & = \\frac{4\\pi}{c}\\vec{\\mathbf{j}} \\\\   \\nabla \\cdot \\vec{\\mathbf{E}} & = 4 \\pi \\rho \\\\\n\\nabla \\times \\vec{\\mathbf{E}}\\, +\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{B}}}{\\partial t} & = \\vec{\\mathbf{0}} \\\\\n\\nabla \\cdot \\vec{\\mathbf{B}} & = 0\n\\end{align}\n\n*Vous aurez deviner tous les équations de Maxwell ... *\n\nAussi belle que puisse être une équation manuscrite, l'écrire sur un éditeur de texte nécessite de nombreux tour de passe passe. \n\nPar exemple, et en utilisant Markdown comme langage  : \n\n\\begin{align} \\nabla \\times \\vec{\\mathbf{B}} \\end{align} s'écrit : \\ nabla \\ times \\ vec{ \\ mathbf { B } }\n\n\nSur Quora, on peut écrire des formules de maths à l'aide de la balise <span style=\"color:blue\">**[/math]**</span> malheureusement tout le monde ne l'utilise pas...\n\nPour cela, nous allons utiliser plusieurs fonctions qui vont permettre d'identifier les formules contenues dans le texte : \n* **corrections_maths(s)** : harmonisation des balises, transcodification des abréviations et split les nombres avec les unités monétaires\n* **Extrait_Math_Bien_Balise(s)** : sortir du texte les formules qui ont été correctement balisés. \n* **Extrait_Formule_Non_Balisee(s)** : Cette fonction détecter une section de phrase qui n'emploie que les termes généralement utilisés pour écrire des fonctions mathématiques (tel que x, y, z, 0, ... 9, +, -, cos...). Elle fait elle-même appel à deux autres fonctions :\n  * **Verifie_Formule** qui fait un peu plus de tests (une formule doit au moins contenir un opérateur mathématique) et \n  * **Nombre(s)** (qui détermine si la formule n'est qu'un nombre) "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eb4c20c500b4abe03235f4e77495bdbf163eba22"},"cell_type":"code","source":"def init_feature(df):\n    df['question']=np.nan\n    df['formule']=np.nan\n    df['nombre']=np.nan\n    df['questions_multiples']=np.nan\n    df['n_questions']=np.nan\n    df['mots_interrogatifs']=np.nan\n    df['sac_de_mots']=np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bdacb36c4713b34df4868115d4f117c01c0af02","collapsed":true},"cell_type":"code","source":"# Initialisation des nouvelles variables sur la base d'apprentissage\ninit_feature(qdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e7f93e3f60b57b206cc085938a3c04a490c2b165"},"cell_type":"code","source":"\"\"\"\n# Initialisation des nouvelles variables sur la base d'apprentissage\ninit_feature(qdf_test)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98add0f634611d726fe1336192fe4be662e35468"},"cell_type":"markdown","source":"#### >>> Definition des fonctions qui nous permettent de réaliser la détection d'équation"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ec93f45218cf770b4ae47bf7bc854ee56abfe950"},"cell_type":"code","source":"def Corrections_Maths(s):\n    # Recuperation des balises maths\n    s=s.replace('{/math]','[/math]')\n    s=s.replace('[\\math]','[/math]')\n    \n    # 60k -> 60000\n    s=re.sub(r'(\\d+)(k)(?=[^a-zA-Z])',r'\\g<1>000',s)\n    s=re.sub(r'(\\d+)(K)(?=[^a-zA-Z])',r'\\g<1>000',s)\n    \n    #Separer la monnaie\n    s=s.replace('$',' $ ')\n    s=s.replace('¥',' ¥')\n    s=s.replace('€',' € ')\n    s=s.replace('£',' £')\n    \n    return s\n\ndef Extrait_Math_Bien_Balise(s):\n    formule = []\n    reste_de_la_phrase = s\n    \n    e1=s.find(\"[math]\")\n    e2=s.find(\"[/math]\")\n    while(e1!=-1 and e2!=-1 and e1<e2):\n        formule.append(reste_de_la_phrase[e1+6:e2])\n        reste_de_la_phrase=reste_de_la_phrase[0][:e1] + reste_de_la_phrase[0][e2+7:]\n        e1=reste_de_la_phrase.find(\"[math]\")\n        e2=reste_de_la_phrase.find(\"[/math]\")\n    return pd.Series([reste_de_la_phrase, formule])\n\ndef Nombre(s):\n    s=s.replace(',','')\n    try:\n        float(s)\n        return float(s)\n    except ValueError:\n        return -0.135792468\n\ndef Verifie_Formule(s):\n    if len(s)==0:\n        return False\n    #print(s)\n    if s[0]!=' ':\n        #print('cond1')\n        return False\n    if re.search(r'[^ ∝∞√%\\.,\\|:!\\-/×=<>\\^\\+\\*\\(\\)\\[\\]\\{\\}…]',s)==None:\n        #print('cond2')\n        return False\n    if re.search(r'[0-9∝∞√%\\.,:!\\|\\-/×=<>\\^\\+\\*\\(\\)\\[\\]\\{\\}…]',s)==None:\n        #print('cond3')\n        return False\n    return True\n\n\n#Pas bon, cf exemples#\ndef Extrait_Formule_Non_Balisee(s):\n    \n    reste_de_la_phrase = s\n    nombre = []\n    formule = []\n    \n    formules = re.findall(r' (?:[0-9xyz\\|\\(\\[{]|cos |cos\\(|sin |sin\\(|tan |tan\\(|csc |csc\\(|exp |exp\\(|log |log\\(|ln||alpha|beta|gamma|delta|theta|pi ])(?:[0-9∝∞√%\\|\\.×,:!\\-/=<> \\^\\+\\*\\(\\)\\[\\]\\{\\}…xyz]|cos |cos\\(|sin |sin\\(|tan |tan\\(|csc |csc\\(|exp |exp\\(|log |log\\(|ln|alpha|beta|gamma|delta|theta|pi)*',s)#(?=[^\\.:,\\(\\[\\{])',s)#(?:[0-9xyz!)\\]}]|alpha|beta|gamma|delta|theta|pi])(?=[^a-zA-Z]|$)',s)#*',s)\n    #print(formules)\n    formules = filter(Verifie_Formule,formules)\n    #print(formules)\n    for x in formules:\n        x=x.strip()\n        l = Nombre(x)\n        if(l!= -0.135792468):\n            nombre.append(l)\n        else:\n            formule.append(x)\n        e1=reste_de_la_phrase.find(x)\n        reste_de_la_phrase=reste_de_la_phrase[:e1]+\" \"+reste_de_la_phrase[e1+len(x):]\n        \n    nombre.sort()\n    \n    return pd.Series([reste_de_la_phrase, nombre, formule])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1ca22902488b4178f94c4d52e389377f438825d"},"cell_type":"markdown","source":"Lancement des fonctions précédemment définies pour identifier les équations dans le texte.\n\n#### >>> Lancement sur la base de train : "},{"metadata":{"trusted":true,"_uuid":"67783ea69011e892102db7f273ddc122ecd81e1f","collapsed":true},"cell_type":"code","source":"# Creation d'une nouvelle variable \"question\" qui subira les différentes transformations\nqdf['question']=qdf['question_orig']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39faedce4203456b6b16c28db8a4735a54628d87","collapsed":true},"cell_type":"code","source":"import time\n\nstart = time.time()\nprint(\"Lancement des traitements mathématiques : \\n\")\n\nprint(\"   >>> Corrections simples...\")\nqdf['question']=qdf['question'].apply(Corrections_Maths)\n\nprint(\"   >>> Extractions simples...\")\nqdf[['question','formule']] = qdf['question'].apply(Extrait_Math_Bien_Balise)\n\nprint(\"   >>> Extractions compliquees...\")\nqdf[['question','nombre','formule']] = qdf['question'].apply(Extrait_Formule_Non_Balisee)\n\nprint(\"\\nFin des traitements mathématiques : \\n\")\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee1cf7be67b66e3639e2be0696cadd3f4c3882ac"},"cell_type":"markdown","source":"#### >>> Lancement sur la base de test : \n\nDans l'idéal, chacune des étapes qui vont suivre doivent etre lancé sur la base de train et la base de test pour pouvoir réaliser les prédictions sur cette dernière "},{"metadata":{"trusted":true,"_uuid":"a4859003d5841ec3493485686af7a5ce2044eea5","collapsed":true},"cell_type":"code","source":"\"\"\"\n\n# Initialisation des nouvelles variables sur la base de test\nstart = time.time()\nprint(\"Lancement des traitements mathématiques : \\n\")\n\nprint(\"   >>> Corrections simples...\")\nqdf_test['question']=qdf_test['question_orig'].apply(Corrections_Maths)\n\nprint(\"   >>> Extractions simples...\")\nqdf_test[['question','formule']] = qdf_test['question'].apply(Extrait_Math_Bien_Balise)\n\nprint(\"   >>> Extractions compliquees...\")\nqdf_test[['question','nombre','formule']] = qdf_test['question'].apply(Extrait_Formule_Non_Balisee)\n\nprint(\"\\nFin des traitements mathématiques : \\n\")\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3b6c8e0233a813dd76185604427b11811b3085c"},"cell_type":"markdown","source":"#### >>> Quelques affichages "},{"metadata":{"trusted":true,"_uuid":"e0c697f5414f9741bc5f20886b695a3436542bdb","scrolled":true,"collapsed":true},"cell_type":"code","source":"print(qdf[['question_orig','formule']][qdf['formule'].apply(len)>0][0:10])\nprint(qdf[['question_orig','nombre']][qdf['nombre'].apply(len)>0][0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af62267d7c2b5eab59c0092f7c308c728059ee92","collapsed":true},"cell_type":"code","source":"print(\"Nombre de variable : \", len(list(qdf.columns)))\nprint(\"Liste des variables : \", list(qdf.columns))\nqdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9638d85f0dfef4ce91cd2095765b41525ec79e01"},"cell_type":"markdown","source":"### <span style=\"color:#53c653\"> 3.4.2. Lower case\n\nLa transformation des lettres majuscules contenues dans notre corpus en lettre minuscule participe à *la normalisation et a la préparation des données textuelle*. \n\nSouvent, c'est une bonne idée : cela permettra par egalement d'harmoniser le mot \"Automobile\" placé en début de phrase (donc comporte une lettre capitale) et le mot \"automobile\", placé indifféremment dans la phrase. Mais il y a toujours quelques petites subtilités comme les noms propres ou les acronymes... \n\nDeux méthodes sont présentées : \n- la méthode naive, qui consiste à tout harmoniser indifféremment de la *\"signification\"* du mot\n- le truecasing qui prend en compte les particularités du langage et laisse en majuscule les noms propres connus par exemple\n\n#### <span style=\"color:darkgreen\">La méthode naive</span>"},{"metadata":{"trusted":true,"_uuid":"8a2afd6d4bf8145975a28a37d1c69f12ca6df5ae","collapsed":true},"cell_type":"code","source":"start = time.time()\nprint(\"Lancement de la transformation 'naive' en lowercase : \\n\")\n\nprint(\"\\n>>> Exemple avant retraitement : \", list(qdf[qdf.index==5629].question))\n\ndef transform_to_lowercase(text,var):\n    lowercase = []\n    for txt in list(text[var]):\n        lowercase.append(txt.lower())\n    return(pd.Series(lowercase))\n\nLowerCaseNaif = transform_to_lowercase(qdf,\"question\")\n\n\n# >>> Selectionnons un exemple en particulier pour pouvoir comparer l'avant et l'après\nqdf.question = pd.DataFrame(LowerCaseNaif)        \nprint(\">>> Exemple après retraitement : \", list(qdf[qdf.index==5629].question))\n\nprint(\"\\nTemps d'execution : {:.2f} secondes\".format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0016fdd9166b3b4f718f87c232b1ab232bd9f1c2","collapsed":true},"cell_type":"code","source":"print(\"Nombre de variable : \", len(list(qdf.columns)))\nprint(\"Liste des variables : \", list(qdf.columns))\nqdf.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3b611d0fee41a6838efc981d735ae9eb498f815"},"cell_type":"markdown","source":" #### <span style=\"color:darkgreen\">Le truecasing</span>\n \nD'un autre côté, beaucoup de noms propres sont dérivés de noms communs et ne se distinguent donc que par cas, y compris les sociétés (General Motors, Associated Press), les organisations gouvernementales (Fed vs. fed) et les noms de personnes (Bush, Black).\n\nPrenons par exemple cette phrase : \"I'm George Bush, I'm American AND I like Cars such as General Motors\"\nDans le cas précédent, toutes les lettres capitales seraient remplacées par des lettres minuscules. Or, cette phrase comporte de nombreux noms propres qui pourraient perdre leurs sens suite à cette modification. Typiquement : \n* General Motors -> general motors\nLes traitements statistiques qui suivent ces prétraitement ne sauront plus faire la différence entre la grande marque General Motors et des moteurs généraux...\n\nAfin de conserver les lettres capitals lorsque celles ci sont requises, une technique existe : le <span style=\"color:red\">**truecasing**</span>\nCette technique s'appuie sur les résultats issues du pos-tagging...\n\nUn petit exemple avec notre phrase : "},{"metadata":{"trusted":true,"_uuid":"97e385fcfa735f5211bcb35f9027c4f30002d87e","collapsed":true},"cell_type":"code","source":"\"\"\"\nQuelques exemples de Truecasing\nComme l'execution de ce programme peut etre long pour notre base de donnée, \non ne le lance que sur une phrase pour comprendre la différence avec la méthode naive\n\"\"\"\n\nimport spacy, re # Import library\nnlp = spacy.load('en_core_web_sm')\n\nmysentence = \"I'm George Bush, I'm AmeRiCAn. I like CARs such aS General  Motors\"\nprint (mysentence)\n\ndoc = nlp(mysentence)\ntagged_sent = [(w.text, w.tag_) for w in doc]\nnb_NNP = sum(char in [\"NNP\", \"NNPS\"] for char in [w.tag_ for w in doc]) \n# On compte le nombre de nom propre présent dans la phrase\nnormalized_sent = [w.lower() if t not in [\"NNP\", \"NNPS\"] else w for (w,t) in tagged_sent]\n# Dans cette ligne de commande, on va mettre en minuscule tous les mots qui ne correspondent pas à des noms propres...\nnormalized_sent[0] = normalized_sent[0].capitalize()\n# Cette ligne de commande permet de conserver la lettre capitale du premier mot de la phrase. Cette étape n'est pas essentielle et peut etre supprimée. Elle est juste présentée pour l'exemple.\nstring = re.sub(\" (?=[\\.,'!?:;])\", \"\", ' '.join(normalized_sent))\nprint (string)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8e569af21c2ede19ed9ccc1daa352625e1c81f2"},"cell_type":"markdown","source":"### <span style=\"color:#53c653\"> 3.4.3. Abreviation et acronymes\n\nIl n'existe pas de méthode automatique pour gérer ce problème. La technique couramment utilisée consiste à créer un dictionnaire (objet Python) mettant en correspondance la contraction et sa forme entière.\n\n**Remarque** : pour définir des chaines de caractères, l'utilisation de la double quote ( \" ) est préférable._"},{"metadata":{"trusted":true,"_uuid":"29741cc34cac7616f599c800021957d0471b9a8e","collapsed":true},"cell_type":"code","source":"CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n                   \"this's\": \"this is\",\n                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" ,\n                   \"date of birth\" : \"DOB\" , \"birth date\" : \"DOB\" ,\"birthdate\" : \"DOB\",\n                  } \n\ndef Corrections_Abreviations_Acronymes(s):\n    # Abreviations\n    s=s.replace('Assn.', 'Association')\n    s=s.replace('Mt.', 'Mount')\n    s=s.replace('Ave.', 'Avenue')\n    s=s.replace('St.', 'Street')\n    s=s.replace('Dept.', 'Department')\n    s=s.replace('No.', 'Number')\n    s=s.replace(' no.', ' Number')\n    s=s.replace('etc.', ' ')\n    s=s.replace(' vs ',' versus ')\n    s=s.replace(' vs. ',' versus ')\n    s=s.replace('btwn','between')\n    s=s.replace('Btwn','Between')\n    s=s.replace(' u ',' you ')\n    \n    #Noms propres, acronymes\n    s=re.sub(r'(the USA)(?=[^a-zA-Z])',\"America\", s)\n    s=re.sub(r'(the U.S.A.)(?=[^a-zA-Z])',\"America\", s)\n    s=re.sub(r'(The U.S.A)(?=[^a-zA-Z])',\"America\", s)\n    s=re.sub(r'(The U.S)(?=[^a-zA-Z])',\"America\", s)\n    s=re.sub(r'(the US)(?=[^a-zA-Z])',\"America\", s)\n    s=re.sub(r'(the U.S)(?=[^a-zA-Z])',\"America\", s)\n    s=re.sub(r'(The U.S)(?=[^a-zA-Z])',\"America\", s)\n    s=re.sub(r'(The U.S)(?=[^a-zA-Z])',\"America\", s)\n    s=s.replace('United States of America', 'America')\n    s=s.replace('United States', 'America')\n    s=s.replace('World Trade Organization', 'WTO')\n    s=re.sub(r'( UK)(?=[^a-zA-Z])',\" United Kingdom\", s)\n    s=re.sub(r'( uk)(?=[^a-zA-Z])',\" United Kingdom\", s)\n    s=s.replace('date of birth', 'DOB')\n    s=s.replace('birth date','DOB')\n    s=s.replace('birthdate','DOB')\n    s=s.replace('M.Sc.','MSc')\n    s=s.replace('B.Sc.','BSc')\n    s=s.replace('B. Tech.','BTech')\n    s=s.replace('Master of Science','MSc')\n    s=s.replace('Bachelor of Science','BSc')\n    s=s.replace('Bachelor of Technology','BTech')\n    s=s.replace('Master of science','MSc')\n    s=s.replace('Bachelor of science','BSc')\n    s=s.replace('Bachelor of technology','BTech')\n    s=s.replace('Ph.D.','PhD')\n    s=s.replace('Ph.D','PhD')\n    s=s.replace('doctorate','PhD')\n    s=s.replace('New York','New-York')\n    s=s.replace(' NY ',' New-York ')\n    s=s.replace('Los Angeles','Los-Angeles')\n    s=s.replace(' LA ',' Los-Angeles ')\n    s=s.replace('San Francisco','San-Francisco')\n    s=s.replace('United Nations','UN')\n    s=s.replace('UPSC','civil service')\n    s=s.replace('IAS','civil service')\n    s=s.replace('upsc','civil service')\n    s=s.replace('ias','civil service')\n    s=s.replace('Orange is the New Black','OITNB')\n    s=s.replace('World War III','WW3')\n    s=s.replace('World War 3','WW3')\n    s=s.replace('WWIII','WW3')\n    s=s.replace('World War II','WW2')\n    s=s.replace('World War 2','WW2')\n    s=s.replace('WWII','WW2')\n    s=s.replace('World War I','WW1')\n    s=s.replace('World War 1','WW1')\n    s=s.replace('WWI','WW1')\n    s=s.replace('Third World War','WW3')\n    s=s.replace('third world war','WW3')\n    s=s.replace('Third world war','WW3')\n    s=s.replace('Second World War','WW2')\n    s=s.replace('second world war','WW2')\n    s=s.replace('Second world war','WW2')\n    s=s.replace('World War I','WW1')\n    s=s.replace('World War 1','WW1')\n    s=s.replace('WWI','WW1')\n    s=s.replace('1st',\"first\")\n    s=s.replace('2nd',\"second\")\n    s=s.replace('3rd',\"third\")\n    s=s.replace('4th',\"fourth\")\n    s=s.replace('5th',\"fifth\")\n    s=s.replace('6th',\"sixth\")\n    s=s.replace('7th',\"seventh\")\n    s=s.replace('8th',\"eighth\")\n    s=s.replace('9th',\"nineth\")\n    #s=s.replace('Wich ', 'Which')\n    s=s.replace('Wat ', 'What')\n    s=s.replace(\"(^|\\W)\\d+($|\\W)\", \" \")\n    return s\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d2d75e65fc0629606a35a9cbe8cb031d806b4f7","collapsed":true},"cell_type":"code","source":"print(\"Quelques affichages avant retraitement des contractions : \\n --------------------------------------------------------\")\nprint(\">>> Exemple avec ' what's ' : \\n\", list(qdf[qdf.index==21].question))\nprint(\">>> Exemple avec ' date of birth ' : \\n\", list(qdf[qdf.index==24227].question))\nprint(\">>> Exemple avec ' vs ' : \\n\", list(qdf[qdf.index==1872].question))\nprint(\">>> Exemple avec ' UPSC ' : \\n\", list(qdf[qdf.index==38].question))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72237ce487a9d0a6b5611a3bd37b972b58e328d9","collapsed":true},"cell_type":"code","source":"def expand_contractions(sentence, contraction_mapping = CONTRACTION_MAP): \n    # mytok = sent_tokenize(sentence)\n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),  \n                                      flags=re.IGNORECASE|re.DOTALL) \n    def expand_match(contraction): \n        match = contraction.group(0) \n        first_char = match[0] \n        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())                        \n        try:\n            expanded_contraction[1:] \n            return(first_char+expanded_contraction[1:] )\n        except TypeError:\n            return(\"\")\n    \n    expanded_sentence = contractions_pattern.sub(expand_match, sentence) \n\n    return expanded_sentence ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7bf24685f858598fa31bb9a66fec3ee4456e495","collapsed":true},"cell_type":"code","source":"start = time.time();\nprint(\"Lancement de l'expansion des contractions : \\n\")\n\nres = [expand_contractions(txt, CONTRACTION_MAP) for txt in qdf[\"question\"]]\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))\n\nqdf[\"question_orig\"] = pd.Series(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"375ce093f5bcd02653a4e58a3ac89b8bf37c154e","collapsed":true},"cell_type":"code","source":"start = time.time();\nprint(\"Corrections_texte\")\nqdf['question']=qdf['question'].apply(Corrections_Abreviations_Acronymes)\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07555a5b36e5c70e160da2d447f67d4d988ce860","collapsed":true},"cell_type":"code","source":"# >>> Verification qu'il n'y a plus les contractions définies dans notre dictionnaire\nprint(\"Quelques affichages après retraitement des contractions : \\n --------------------------------------------------------\")\nprint(\">>> Exemple avec ' what's ' : \\n\", list(qdf[qdf.index==21].question_orig))\nprint(\">>> Exemple avec ' date of birth ' : \\n\", list(qdf[qdf.index==24227].question_orig))\nprint(\">>> Exemple avec ' vs ' : \\n\", list(qdf[qdf.index==1872].question_orig))\nprint(\">>> Exemple avec ' UPSC ' : \\n\", list(qdf[qdf.index==38].question_orig))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b911e72ddc2bbaab4ab28021cba373303b91ded"},"cell_type":"markdown","source":"Quelques controles et affichages : "},{"metadata":{"trusted":true,"_uuid":"91dfc0956a3b1f829f390ccd2bb88f946d43ca1b","collapsed":true},"cell_type":"code","source":"print(\"Nombre de variable : \", len(list(qdf.columns)))\nprint(\"Liste des variables : \", list(qdf.columns))\nqdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02e7af00bd4058d973b0b5559f52d75afc05af14"},"cell_type":"markdown","source":"### <span style=\"color:#53c653\"> 3.4.3. Ponctuation\n\nCette étape est à faire _après_ avoir fait l'expanding contraction car sinon supprimer les apostrophes ou autres ponctuations qui symbolisent la contraction.\n\nOn repère généralement les différentes phrases de la question à l'aide du point suivi d'un espace.  Le problème est que cette convention n'est pas toujours respectée !!! Typiquement, lorsqu'il s'agit de phrase interrogative (la ponctuation finale est le point d'interrogation \"?\") ou exclamatives (cette fois, c'est le point d'exclamation \"!\"). \n\nComment faire dans ces conditions ? \nune méthode simple consiste à décoller le point du mot qui suit. \"Simple!\" me direz-vous? Trop à vrai dire... Et c'est oublier qu'il existe toujours des exceptions ! En effet, il y a des points collés *légitimes*, comme le point dans une URL ou dans une extension *fichier*.pdf. \n\nLa liste des ponctuations est la suivante : "},{"metadata":{"trusted":true,"_uuid":"98bb06d4f5a8c4df15d6bd850f3cfcc9b240d8b6","collapsed":true},"cell_type":"code","source":"import string\nponctuation = set(string.punctuation)\nprint(ponctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8de1fa6bbc82c391d9318a46661e30cd2599eb2f","collapsed":true},"cell_type":"code","source":"def Corrections_Ponctuations(s):\n    s=s.replace('/',' ')\n    s=s.replace('\"','')\n    s=s.replace(\"'\",' ')\n    s=s.replace(\"'\",' ')\n    s=s.replace(\"``\",'')\n    s=s.replace(\"''\",'')\n    s=s.replace('~',' ')\n    s=s.replace('...','. ')\n    s=s.replace('..','. ')\n    return s\n\n#On prepare les extensions\n\nf2={'io', 'in', 'is', 'js', 'it', 'us', 'fm', 'tv', 'ca', 'vn', 'no', 'fr', 'sc', \\\n    'la', 'de', 'ed', 'ex', 'eu', 'e', 'ru', 'be', 'py', 'ai', 'as', 'im', 'nl', 'gv', \\\n    'gq', 'ga', 'ts', 'tk', 'dt', 'em', 'cs', 'cr', 'jp', 'cn', 'cm', 'cd', 'pt', 'pz', \\\n    'la', 'pg', 'pl', 'hr', 'uk', 'aa', 'va', 'an', 'ar', 'at', 'ie', 'nd', 'sh', 'ke', \\\n    'st', 'se', 'sd'}\n\n\nf3={'com', 'net', 'org', 'exe', 'dll', 'app', 'php', 'pdf', 'edu', 'emz', 'eng', 'zip',\\\n    'xml', 'png', 'jpg', 'inc', 'rtf', 'cda', 'dbb', 'mp3', 'wrf', 'm4v', 'cue', 'mov',\\\n    'psd', 'dmg', 'cpp', 'bak', 'vhd', 'dir', 'htm', 'odt', 'lnk', 'tmp', 'dds', 'rpm',\\\n    'ddf', 'obj', 'css', 'vtf', 'jar', 'api', 'ppt', 'mbp', 'swf', 'gif', 'wav', 'jmx',\\\n    'tec', 'qsv', 'nh3', 'flp', 'dta', 'tar', 'rar', 'avi', 'hw6', 'mid', 'csv', 'doc'}\n\n\nf4={'html', 'java', 'jpeg', 'json', 'conf', 'arch', 'fxml', 'yaml', 'vmdk', 'aspx', \\\n    'addr', 'proc', 'ajax', 'flac', 'adhd', 'nrkt'}\n\ndef Decollage_De_Points(s):\n    point = s.find('.') \n    #if point!=-1:\n    #    print(s)\n    while (point>1) and (point!=len(s)-1) : \n        if (s[point+1]!=' ') and (s[point+1]!='.') :\n            debut_mot = s.rfind(' ',0,point) + 1\n            fin_mot = s.find(' ',point+1) - 1\n     #       print(debut_mot, fin_mot)\n            if fin_mot==-2:\n                fin_mot = len(s)-1\n            while s[fin_mot] in ponctuation:\n                fin_mot -= 1\n            mot = s[debut_mot:fin_mot+1]\n     #       print(mot)\n            legitime = False\n            if (point+2==fin_mot) and (mot[-2:] in f2):\n                legitime = True\n            if (point+3==fin_mot) and (mot[-3:] in f3):\n                legitime = True\n            if (point+4==fin_mot) and (mot[-4:] in f4):\n                legitime = True\n            if ('http' in mot) or ('www' in mot):\n                legitime = True\n            if not legitime:\n                s=s[:point+1]+' '+s[point+1:]\n        if point==len(s)-1:\n            point = -1\n        else:\n            point = s.find('.',point+1)\n    return s\n\ndef remove_before_token(sentence, keep_apostrophe = False):\n    \n    sentence = sentence.strip()\n    \n    if keep_apostrophe:\n        PATTERN = re.compile(\"(\\.|\\!|\\?|\\(|\\)|\\-|\\$|\\&|\\*|\\%|\\@|\\~)\")\n        filtered_sentence = re.sub(PATTERN, r' ', sentence)\n    else :\n        PATTERN = r'[^a-zA-Z0-9]'\n        filtered_sentence = re.sub(PATTERN, r' ', sentence)\n    return(filtered_sentence)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb61b8199ecc8c38fb7d2ad8c0fdc2da24b632de","collapsed":true},"cell_type":"code","source":"print(\"Lancement des procedures relatives aux ponctuations : \\n\")\n\nstart = time.time();\n\nprint(\"Quelques affichages avant retraitement des contractions : \\n --------------------------------------------------------\\n\", \n      list(qdf[qdf.index==1].question))\n\nprint(\"\\n   >>> Correction de la ponctuation...\")\nqdf['question_orig']=qdf['question'].apply(Corrections_Ponctuations)\n\nprint(\"   >>> Décollage de points...\")\nqdf['question_orig']=qdf['question'].apply(Decollage_De_Points)\n\nprint(\"   >>> Suppression de la ponctuation inutile...\\n\")\ntext = [remove_before_token(txt, keep_apostrophe = False) for txt in list(qdf.question)]\nqdf[\"question\"] = pd.DataFrame(text)\n\nprint(\"Quelques affichages après retraitement des contractions : \\n --------------------------------------------------------\\n\", \n      list(qdf[qdf.index==1].question))\n            \nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"33a54f19dd0cae6140183304fe89d6502daf54f4"},"cell_type":"markdown","source":"Quelques controles et affichages : "},{"metadata":{"trusted":true,"_uuid":"2057e67527bf216b2b9a9a5388150e90dbf2e0b3","collapsed":true},"cell_type":"code","source":"print(\"Nombre de variable : \", len(list(qdf.columns)))\nprint(\"Liste des variables : \", list(qdf.columns))\nqdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"70b5fe8f5d0c2cf990591ebf39572a451d25b805"},"cell_type":"markdown","source":"### <span style=\"color:#53c653\"> 3.4.4. Decoupage des phrases multiples\n\nCertaines \"questions\" peuvent contenir en réalité plusieurs phrases. \nPar exemple : \n\n"},{"metadata":{"trusted":true,"_uuid":"d0694f994b95255b6189ef878263f73d9bdbd37d","collapsed":true},"cell_type":"code","source":"print(\"Lancement des procedures relatives aux découpage de phrases multiples : \\n\")\n\nstart = time.time();\n\ndetecteur_de_phrases = nltk.data.load('tokenizers/punkt/english.pickle')\nqdf['questions_multiples']=qdf['question'].apply(detecteur_de_phrases.tokenize)\nqdf['n_questions']=qdf['questions_multiples'].apply(len)\nqdf.head()\n\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db22956b3374722c962af8cab7aa7704c279ad96","collapsed":true},"cell_type":"code","source":"print(\"Nombre de variable : \", len(list(qdf.columns)))\nprint(\"Liste des variables : \", list(qdf.columns))\nqdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70fec0c02e02adc927ed6177d4a3cacd23246fd1","collapsed":true},"cell_type":"code","source":"print(qdf['question_orig'][qdf['n_questions']==0])\n\n##Que faire ??\nqdf.head()\nlist(qdf[qdf.index==55096].question)\n\n\"\"\" TO DO - a revoir \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfffbef0642a855a83199c1dd9890595de1f100d"},"cell_type":"markdown","source":"Quelques petits soucis encore ..."},{"metadata":{"trusted":true,"_uuid":"b03f612ed3d9ff905720af9caf342ed190f49086","collapsed":true},"cell_type":"code","source":"#plt.hist(qdf['n_questions'],bins=22)\n#plt.show()\nprint(qdf['n_questions'].value_counts())\nprint(qdf['question'][qdf['n_questions']==8])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e8850bae29cdc853e501b30d42690c610fe9bd1"},"cell_type":"markdown","source":"# <span style=\"color:#006600\">4. Extraction des mots et correcteurs orthographique \n\nLes nombreuses études déjà réalisées, tant en NLP qu’en linguistique pure, s’accordent toutes plus ou moins sur un schéma relativement similaire : \n\n```\n1. Les tokens et la tokenisation\n2. Le correcteur orthographique \n3. Les stopwords\n4. Les pos-tagging\n5. Lemmatisation\n```\n\n##  <span style=\"color:#39ac39\"> 4.1. Les tokens et la tokenisation\nCette première étape consiste a découper le texte en plusieurs **<span style=\"background-color: #ccffcc;\">tokens</span>**.  Les tokens sont les éléments porteurs de sens les plus simples au sein d'une phrase (autrement dit, les mots!). \n\nEtant donné que les espaces entre les mots permettent de les délimiter entre eux, une manière de *tokenizer* est donc de séparer les mots par rapport à ces espaces. \n\n** Un petit exemple ** : \n\n```Le chat dort.```\n\nCette phrase comporte trois mots séparés par des espaces, on peut donc en déduire qu'il y a trois tokens. \n\nSi on considère maintenant : *Aujourd'hui, le chat dort*. Combien y a-t-il de tokens ? et combien y a-t-il de tokens dans y *Combien y a-t-il de tokens a-t-il* ?\nDans le cas de *a-t-il*, *va-t’en* etc, on comprends qu'il y a plusieurs tokens et pourtant... aucun espace ! On aurait envie dans ce cas d'étendre notre règle précédente et de considérer que certains signes de ponctuation peuvent etre utilisé comme séparateur de mot... \nMais cette règle conduirait à considérer le mot *Aujourd'hui* comme deux tokens, ... alors qu'il s'agit bien d'un seul et même token. \n\n\nUn petit exemple pratique : "},{"metadata":{"trusted":true,"_uuid":"afd10f662d84d80f4c199291214588ea8010eb16","collapsed":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize, word_tokenize\n\nEXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n\nprint(sent_tokenize(EXAMPLE_TEXT))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"440708b3ca7f8f22bf1278fe796ab00bbbf5dc7f"},"cell_type":"markdown","source":"**Quelques commentaires **: \n1. La ponctuation est considérer comme un token à part entière\n2. La séparation de *shouldn't* donne les tokens *should* et *n't*. Cela signifie que la procédure utilisée comprend bien ces deux mots concaténés ne sont pas porteur d'un sens unique (avec respectivement un verbe et une négation).\n3. Le mot *pinkish-blue* est traité comme un seul et unique token\n"},{"metadata":{"_uuid":"f43880421b7f06c51ef097fde2c59482a98c087c"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 4.2. Le correcteur orthographique\n\n** _Remarque_** : On ne doit pas confondre le ** <span style=\"background-color:#ccffcc\"> correcteur orthographique </span>** et le ** <span style=\"background-color:#ccffcc\"> correcteur grammatical </span>**. Le correcteur orthographique compare les mots du texte aux mots d'un dictionnaire. Si les mots du texte sont dans les dictionnaires, ils sont acceptés, sinon une ou plusieurs propositions de mots proches sont faites par le correcteur orthographique. Le correcteur grammatical vérifie que les mots du texte, bien qu'ils soient dans les dictionnaires, sont conformes aux règles de grammaire (accords, ordre des mots, etc.) et aux règles de la sémantique (phrase ayant un sens, absence de confusion d'homophones, etc.).\n\nDeux méthodes présentées : \n1. Méthode basée sur un calcul de distance entre le mot et les suggestions (distance de Levenshtein)\n2. Méthode basée sur la probabilité qu'une des suggestions soient la bonne (algorithme de Norvig)\n\n### <span style=\"color:#53c653\"> Methode 1 : En utilisant la distance de Levenshtein\n\nPlusieurs distance dans le module nltk : avec edit_distance qui correspond a la distance de Levenshtein (source : http://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/ ) \n* **Hamming** : présuppose que deux chaines de caractères sont des mêmes longueurs\n* **Levenshtein** : contrairement la distance de Hamming, la distance de Levenshtein permet de comparer des chaines dont la longueur est différente. La distance est calculée en déterminant le nombre de transformation qu'il est nécessaire pour passer d'une chaine de caractère A a la chaine de caractère B. Ces transformations peuvent etre : \n    * Substitution \n    * Insertion \n    * Suppression\n    Par exemple : La distance de Levenshtein entre \"rain\" et \"shine\" est de 3 puisqu'il y a deux substitutions et une insertion :\n    \"rain\" -> \"sain\" -> \"shin\" -> \"shine\". \n* **Damerau-Levenshtein**\n* **Jaro Winkler**\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"626fb5de7cbc47e51f672fe646737c02ca935151"},"cell_type":"code","source":"def hamming_distance(s1, s2):\n    s1 = re.sub('[^A-Za-z0-9]+', '', s1); print(len(s1))\n    s2 = re.sub('[^A-Za-z0-9]+', '', s2); print(len(s2))\n    assert len(s1) == len(s2)\n    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))\n\ndef damerau_levenshtein(s1, s2):\n    d = {}\n    len_s1 = len(s1)\n    len_s2 = len(s2)\n    for i in range(-1, len_s1 + 1):\n        d[(i, -1)] = i + 1\n    for j in range(-1, len_s2 + 1):\n        d[(-1, j)] = j + 1\n\n    for i in range(len_s1):\n        for j in range(len_s2):\n            if s1[i] == s2[j]:\n                cost = 0\n            else:\n                cost = 1\n            d[(i, j)] = min(\n                d[i - 1, j] + 1,  # Deletion\n                d[i, j - 1] + 1,  # Insertion\n                d[i - 1, j - 1] + cost,  # Substitution\n            )\n\n            if i and j and s1[i] == s2[j - 1] and s1[i - 1] == s2[j]:\n                d[i, j] = min(d[i, j], d[i - 2, j - 2] + cost)  # transposition\n    return d[len_s1 - 1, len_s2 - 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e456eb0498175dd7e5c543a42971dcf1df60ab26","collapsed":true},"cell_type":"code","source":"import hunspell\nfrom nltk.metrics import *\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom Levenshtein import jaro_winkler\n\nhobj = hunspell.HunSpell(\"/usr/share/hunspell/en_US.dic\", \"/usr/share/hunspell/en_US.aff\")\n\ndef correct_word(myword):\n    res = [x for x in list(hobj.suggest(myword)) if edit_distance(myword,x) == 1 ] ; print(res)\n    if len(res) == 1:\n        correct = res\n    if len(res) == 0: \n        res = [x for x in list(hobj.suggest(myword)) if edit_distance(myword,x) == 2 ] \n        if len(res) == 1: correct = res\n        elif len(res) == 2: \n            tmp = [jaro_winkler(myword,x) for x in res]; \n            correct = res[tmp.index(max(tmp))] # mettre le max au sens de jaro winkler\n        else : correct = None\n    if len(res) > 1:\n            tmp = [jaro_winkler(myword,x) for x in res]; \n            correct = res[tmp.index(max(tmp))] # mettre le max au sens de jaro winkler\n    print(\"The best candidate for {} is : {}\".format(myword,correct))\n    return(correct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c10676094055ad07e10c463212c75b6b43811f8f","collapsed":true},"cell_type":"code","source":"# Quelques exemples : \ncorrect_word(\"mywords\")\ncorrect_word(\"spooky\")\ncorrect_word(\"mustkaes\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e1f5dc72d8f83b34ddb56bba0e82151a028210d"},"cell_type":"markdown","source":"Avec cette méthode, le dernier mot n'est pas reconnu "},{"metadata":{"_uuid":"4be1a8e95cc455c55b245d8c3c408e0c3971cb06"},"cell_type":"markdown","source":"### <span style=\"color:#53c653\"> Methode 2 : Peter Norvig\nCette méthode tente de choisir  la correction d'orthographe la plus probable pour un mot donné. Il n'y a aucun moyen de savoir avec certitude (par exemple, faut-il corriger les *\"lates\" *en *\"late\"* ou *\"latest\"* ou *\"lattes\"* ou ...?), Ce qui suggère que nous utilisons des probabilités. Nous essayons de trouver la correction, parmi toutes les corrections possibles, qui maximise la probabilité que ce soit la correction voulue, étant donné le mot original.\n\nCette probabilité est déterminée en comptant le nombre de fois que chaque mot apparaît dans un fichier texte d'environ un million de mots, appelé **_big.txt_**. Il s'agit d'une concaténation : \n* d'extraits de livres du domaine public du Projet Gutenberg et \n* de listes de mots les plus fréquents de Wiktionary et du British National Corpus\n\nPour plus d'information : https://norvig.com/spell-correct.html "},{"metadata":{"trusted":true,"_uuid":"38e9f69a54a6fdd0115983e9bfc4c6302174edb8","collapsed":true},"cell_type":"code","source":"import re\nimport nltk\nfrom collections import Counter\n\ndef words(text): return re.findall(r'\\w+', text.lower())\nWORDS = Counter(words(open('../input/spelling/big.txt').read()))\n\ndef P(word, N=sum(WORDS.values())): \n    \"Probability of `word`.\"\n    return WORDS[word] / N\n\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6419dc3e3a8fdc29b84c7a048eb9c934240e19d8","collapsed":true},"cell_type":"code","source":"# Exemple avec des mots au hasard \nprint(correction('mywords'))\nprint(correction('spooky'))\nprint(correction(\"mustkaes\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbf3de7bf06c47163a944866c13d948ce33b9e95"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 4.3. Les stopwords\n\nLes ** <span style=\"background-color:#ccffcc\"> \" stopwords \"</span>** sont en très grande partie composée de mots qui n’ont pas de sess en eux-mêmes mais qui sont utilisés dans la constructions des phrases (ex. : prépositions, pronoms, verbes uxilaires, articles). \n\nUne des difficultés est qu'il n'existe pas de liste universelle de stopwords. Ils sont **caractéristiques d’une langue**. En effet, il est assez peu probable que vous utilisiez dans une phrase francaise *the* ou de *are* , sauf si vous buvez du thé tout en discutant de métrologie (l'are est une unité de mesure de superficie).\n\nLa librairie <span style=\"background-color:#ccffcc\"> NLTK </span> fournit une liste des stopwords les plus communs. Pour accéder a cette liste prédéfinie : \n\n```\nfrom nltk.corpus import stopwords\nstopwords.words('english')\n```\n\n**Quelques exemples** : 'i','me','my','myself','we','our','ours','ourselves','you',\"you're\",\"you've\",\"you'll\",'because','as','until','while','of','at','by','for','with','about','against',\n\nEt en francais ? Ca existe aussi ! \n```\nfrom nltk.corpus import stopwords\nstopwords.words('french')\n```\n**Quelques exemples** :  'au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', etc...\n\nVous pouvez également ajouter votre stopwords, voire même supprimer quelques uns qui sont présents dans la liste de NLTK, en utilisant respectivement les lignes de commandes suivantes : \n\n```\nstop += ['would']\nstop.remove('how')\n```\n\nComme dans notre cas, les mots interrogatifs nous interesses (puisqu'il s'agit de faire la correspondance entre des questions!), nous allons conserver les \"What\", \"Who\" etc..."},{"metadata":{"trusted":true,"_uuid":"650011331ad7d603377ad22efa99704f9867a1b3","collapsed":true},"cell_type":"code","source":"#  Apportons notre touche personnelle sur les stopwords...\nstop = stopwords.words('english')\nstop.remove('what')\nstop.remove('which')\nstop.remove('who')\nstop.remove('whom')\nstop.remove('when')\nstop.remove('where')\nstop.remove('why')\nstop.remove('how')\n# A faire autrement == traitement de la ponctuation et non pas stopwords\n# stop += [',','.','...','(',')','[',']','{','}','!','?',';',':','*','=','<','>','&','$','+',\"&\",'-']\nstop += ['would']\nstopword_question = set(stop)\n\n# Affichages : \nprint(\"Les stopwords conservés sont les suivants : \\n \", stopword_question)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e4328fe1fd8a87c6f1655e7df52c4d97b2a3bec"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 4.4. Le pos-tagging (Part of Speech tagging)\n\nLe <span style=\"background-color:#ccffcc\"> **  pos-tagging ** </span> consiste à étiqueter les mots comme noms, adjectifs, verbes ... etc. Encore plus impressionnant, il *tag* également si le verbe est conjugé, s'il y a des accords... et plus encore.  L'étiquetage est dépendant de la qualité du texte (pas de faute d'orthographe par exemple !!)\n\n**<span style=\"color:darkgreen\">Petits exemple _sans_ faute </span>** : \n![POSTAG_CORRECT](https://github.com/sdaymier/NLP/blob/master/POSTAG_Correct.PNG?raw=true \"Logo Title Text 1\")\n\n**<span style=\"color:darkgreen\">Petits exemple _avec_ des fautes</span>** : \n![POSTAG_INCORRECT](https://github.com/sdaymier/NLP/blob/master/POSTAG_Incorrect.PNG?raw=true \"Logo Title Text 1\")\n\n\nVoici une liste des tags, ce qu'ils signifient, et quelques exemples (liste non exhaustive) :\n \n\n| Tag | Meaning | English Examples |\n| ------------- |:----------------:|:----------------:|\n| ADJ | adjective | new, good, high, special, big, local |\n| ADP | adposition  | on, of, at, with, by, into, under |\n| ADV  | adverb | really, already, still, early, now |\n| CONJ  | conjunction |  and, or, but, if, while, although  |\n|  DET | determiner, article  | the, a, some, most, every, no, which  |\n| NOUN | noun | noun year, home, costs, time, Africa   |\n| NUM |numeral | twenty-four, fourth, 1991, 14:24  |\n| PRT  |particle  |  at, on, out, over per, that, up, with   |\n| PRON | pronoun |   he, their, her, its, my, I, us  |\n| VERB | verb |  is, say, told, given, playing, would  |\n| \".\" | punctuation | \" . , ; !\"  |\n|X | other  | ersatz, esprit, dunno, gr8, univeristy  |"},{"metadata":{"trusted":true,"_uuid":"18bdac80d61216fd01762fc51dff768f0b8259b6","collapsed":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize, word_tokenize\n\nEXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\nwords = nltk.word_tokenize(EXAMPLE_TEXT)\ntagged = nltk.pos_tag(words)\n\nprint(tagged)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e3798a52338c0daa9f0394608264e7f2fedccf1"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 4.5. Lemmatisation\n\nLa  <span style=\"background-color:#ccffcc\">lemmatisation</span> consiste a analyser les termes de manière a identifier sa forme canonique (lemme) qui existe réellement. L’idée est de réduire les différentes formes (pluriel, féminin, conjugaison…) en une seule. La technique fait à la fois référence a un dictionnaire et à l’analyse morphosyntaxique des mots. Elle est spécifique à chaque langue.\n\n**Un petit exemple ** : Le mot «** invisible** » est composé de trois morphèmes : **_in- vis- ible_**, « voir » étant le lemme du mot. A partir de ces éléments, on peut déterminer comment le lemme a été altéré et pourquoi. Dans notre cas : \n* le préfixe «* in *» apporte une connotation négative au verbe voir \n* le suffixe « *ible* » exprime la capacité à faire quelque chose."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d8082a750721489c1ad6c3e776834141a53ca84e"},"cell_type":"code","source":"def Traducteur_Tag_Pour_Lemmatizer(tag):\n    if tag.startswith('V'):\n        return wordnet.VERB\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    if tag.startswith('N'):\n        return wordnet.NOUN\n    if tag.startswith('R'):\n        return wordnet.ADV\n    return wordnet.NOUN\n\ndef lemmatise(phrase_taggee):\n    return [nltk.stem.WordNetLemmatizer().lemmatize((x[0].lower()), Traducteur_Tag_Pour_Lemmatizer(x[1])) \\\n            for x in phrase_taggee if x[0].lower() not in stopword_question]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f02920a93170b50a132a06cb5661e9924235f92"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 4.6. Application de l'extracteur de mot (driver des fonctions précédentes)\n\nLa fonction définie ci dessous est le driver des étapes à suivre , que nous avons vu en détail précédemment.: \n\n\n** WARNING ** : Très long !!!\nExemple : Temps d'execution : 62.61 secondes sur la base de train avec uniquement les tokens"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"84476ffe21e70b346d9a39be465693e1ed8de30f"},"cell_type":"code","source":"# Version complète\ndef extractWords(phrases_multiples):\n    q_multiples_tokenizees  = [nltk.word_tokenize(x) for x in phrases_multiples] # 1. Tokenization\n    q_multiple_correct      = [correction(x) for x in phrases_multiples]         # 2. Correcteur orthographique\n    q_multiples_taggees     = [nltk.pos_tag(x) for x in q_multiples_tokenizees]  # 3. Stopwords et pos tagging\n    q_multiples_lemmatisees = [lemmatise(x) for x in q_multiples_taggees]        # 4. Lemmatization\n\n    return [x for x in q_multiples_lemmatisees if len(x)>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"55c2182380f5c0cc3982596e9f0a9f80266a7f9c"},"cell_type":"code","source":"# Version allégée\ndef extractWords(phrases_multiples):\n    q_multiples_tokenizees  = [nltk.word_tokenize(x) for x in phrases_multiples] # 1. Tokenization\n    # q_multiple_correct      = [correction(x) for x in phrases_multiples]         # 2. Correcteur orthographique\n    # q_multiples_taggees     = [nltk.pos_tag(x) for x in q_multiples_tokenizees]  # 3. Stopwords et pos tagging\n    # q_multiples_lemmatisees = [lemmatise(x) for x in q_multiples_taggees]        # 4. Lemmatization\n\n    return [x for x in q_multiples_tokenizees if len(x)>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edd47f5e6b735429eeab14c613c88b988e494cf7","collapsed":true},"cell_type":"code","source":"start = time.time();\nprint(\"Lancement des procedures relatives aux extractions de mots et correcteurs orthographique : \\n\")\n\nqdf[\"sac_de_mots\"]=qdf[\"questions_multiples\"].apply(extractWords)\n\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"537dd1c8d24f6adf60db644158d33730fb393cb6"},"cell_type":"markdown","source":"Quelques controles et affichages : "},{"metadata":{"trusted":true,"_uuid":"9c8ae9eec4183c8f6e93d7c6d178c43a53273161","collapsed":true},"cell_type":"code","source":"print(\"Nombre de variable : \", len(list(qdf.columns)))\nprint(\"Liste des variables : \", list(qdf.columns))\nqdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bc9f80ab13b81c76475ba04af32c8a322a79ef3c"},"cell_type":"code","source":"# qdf.to_csv(\"QDF_TRAIN_PostTreatment.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6567aed9b2717723bd98179a022a213c5139a50"},"cell_type":"markdown","source":"Quelques affichages : "},{"metadata":{"trusted":true,"_uuid":"96a8204a6ecc77693cb9eb63695e678a60851c93","collapsed":true},"cell_type":"code","source":"qdf[100:110]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"335ba86db35f41738f46548946e299611c335629"},"cell_type":"markdown","source":"# <span style=\"color:#006600\"> 5. Autres features : \n##  <span style=\"color:#39ac39\"> 5.1. Repérer le mot interrogatif"},{"metadata":{"trusted":true,"_uuid":"f7dc088a0a5838fefb9e8898d0b4c812a71eef61","collapsed":true},"cell_type":"code","source":"mots_interrogatifs=['what','which','how','when','where','who','why','whom']\n\nstart = time.time();\nprint(\"Lancement des procedures relatives aux extractions des mots interrogatifs : \\n\")\n\n\ndef Trouve_MI(SacDeMots):\n        \n    for mot in SacDeMots:\n        if mot in mots_interrogatifs:\n            return mot\n    \n    return 'que_dalle'\n\ndef Liste_MI(ListePhrases):\n    return [Trouve_MI(x) for x in ListePhrases]\n    \n    \nqdf['mots_interrogatifs']=qdf['sac_de_mots'].apply(Liste_MI)\n\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))\n\nprint(\"Nombre de variable : \", len(list(qdf.columns)))\nprint(\"Liste des variables : \", list(qdf.columns))\nqdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7e7f8c95158f42f1064542f78c9ed71483f6d64"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 5.2. Creation de feature avec Word2Vec"},{"metadata":{"trusted":true,"_uuid":"fb9004435ceea71cab06b42eb1890f97608b4962"},"cell_type":"code","source":"import gensim\n../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\nmodel = gensim.models.KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)\nindecision = model.similarity('chess','sausage')\n\ndef similarite_mot(mot1, mot2):\n    if mot1==mot2:\n        return 1.0\n    if (mot1 in model.vocab) and (mot2 in model.vocab):\n        return model.similarity(mot1, mot2)#vec_mot1.dot(model[mot2])\n    return indecision\n\ndef similarite_phrase(p1,p2):\n    similarite=1\n    n_mots=0\n\n    p_courte=p1\n    p_longue=p2\n    if len(p2)<len(p1):\n        p_courte=p2\n        p_longue=p1\n    if len(p_courte)==0.1:\n        return indecision   ### 0 ?\n    \n    #print(\"p1 :\",p_courte)\n    #print(\"p2 :\",p_longue)\n    for mot in p_courte:\n        #print(mot)\n        n_mots+=1\n        ressemblances = [similarite_mot(mot, mot2) for mot2 in p_longue]\n        #print(ressemblances)\n        similarite *= max(ressemblances)        \n        \n    return similarite**(1.0/n_mots)    \n\n\ndef similarite_Word2Vec(ligne):\n    sdm1 = qdf['sac_de_mots'][qdf['id']==ligne['qid1']].iloc[0]\n    sdm2 = qdf['sac_de_mots'][qdf['id']==ligne['qid2']].iloc[0]\n    #print(ligne['qid1'],sdm1)\n    #print(ligne['qid2'],sdm2)\n    \n    if len(sdm1)==0:\n        return indecision #### ?\n    if len(sdm2)==0:\n        return indecision #### ?\n    \n    similarites = []\n    \n    for p1 in sdm1:\n        for p2 in sdm2:\n            similarites += [similarite_phrase(p1,p2)]\n    #print(similarites)\n    return max(similarites)\n\ndef similarite_phrase_2(p1,p2):\n    similarite=1\n    n_mots=0\n    \n    \n    p_courte=p1\n    p_longue=p2\n    if len(p2)<len(p1):\n        p_courte=p2\n        p_longue=p1\n    if len(p_courte)==0.1:\n        return indecision   ### 0 ?\n    \n    #print(\"p1 :\",p_courte)\n    #print(\"p2 :\",p_longue)\n    for mot in p_longue:\n        #print(mot)\n        n_mots+=1\n        ressemblances = [similarite_mot(mot, mot2) for mot2 in p_courte]\n        #print(ressemblances)\n        similarite *= max(ressemblances)        \n        \n    return similarite**(1.0/n_mots)    \n\n\ndef similarite_Word2Vec_2(ligne):\n    sdm1 = qdf['sac_de_mots'][qdf['id']==ligne['qid1']].iloc[0]\n    sdm2 = qdf['sac_de_mots'][qdf['id']==ligne['qid2']].iloc[0]\n    #print(ligne['qid1'],sdm1)\n    #print(ligne['qid2'],sdm2)\n    \n    if len(sdm1)==0:\n        return indecision #### ?\n    if len(sdm2)==0:\n        return indecision #### ?\n    \n    similarites = []\n    \n    for p1 in sdm1:\n        for p2 in sdm2:\n            similarites += [similarite_phrase_2(p1,p2)]\n    #print(similarites)\n    return max(similarites)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d1fd502f1a32318db85871fa13f92a46f71d8e2c"},"cell_type":"code","source":"\"\"\"\n# Lancement des fonctions\ndf_train['similarite_w2vec']=df_train.progress_apply(similarite_Word2Vec, axis=1)\ndf_train['similarite_w2vec_2']=df_train.progress_apply(similarite_Word2Vec_2, axis=1)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ce96adf6a5d7bb0d48493c75d2d901a9a9e7912","collapsed":true},"cell_type":"code","source":"df_Word2vec = pd.read_csv('../input/featuresword2vec/features_w2vec.csv')\nprint(\"Dimension des features de Word2Vec : \", df_Word2vec.shape)\nprint(\"Dimension des features de df_train : \", df_train.shape)\n\n# Concatenation des bases : \ndf_train[\"id\"] = df_train.index\ndf_train = df_train.merge(df_Word2vec, left_on='id', right_on='Unnamed: 0', how='outer')\ndf_train.drop([\"id\",'Unnamed: 0'], axis = 1, inplace= True)\n\nprint(\"Dimension des features de df_train : \", df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c9d1b15ce2d7371841a1b6249599094c56440df","collapsed":true},"cell_type":"code","source":"plt.hist(df_train[(df_train['is_duplicate']==1) & ~(df_train['similarite_w2vec'].isnull())]['similarite_w2vec'])\nplt.show()\nplt.hist(df_train[(df_train['is_duplicate']==0) & ~(df_train['similarite_w2vec'].isnull())]['similarite_w2vec'])\nplt.show()\nplt.hist(df_train[(df_train['is_duplicate']==1) & ~(df_train['similarite_w2vec_2'].isnull())]['similarite_w2vec_2'])\nplt.show()\nplt.hist(df_train[(df_train['is_duplicate']==0) & ~(df_train['similarite_w2vec_2'].isnull())]['similarite_w2vec_2'])\nplt.show()\ndf_train['similarite']=df_train['similarite_w2vec']*df_train['similarite_w2vec_2']\ndf_train['similarite'].loc[df_train['similarite'].isnull()]=indecision*indecision\nplt.hist(df_train[(df_train['is_duplicate']==1) ]['similarite'])\nplt.show()\nplt.hist(df_train[(df_train['is_duplicate']==0) ]['similarite'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5a5dea9b01efc92cad5d1fd55aa3e5a6e97be87"},"cell_type":"markdown","source":"Extract pour la formation : "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0e510f1236325771485d736d4990191a46da09a4"},"cell_type":"code","source":"\"\"\"\nnoformule = qdf.id[qdf['formule'].apply(len)==0][0:10].tolist()\nformule = qdf.id[qdf['formule'].apply(len)>0][0:10].tolist() \nnonumber = qdf.id[qdf['nombre'].apply(len)==0][0:10].tolist()\nnumber = qdf.id[qdf['nombre'].apply(len)>0][0:5].tolist()\nnoquestion = qdf.id[qdf['n_questions']==0][0:10].tolist()\nquestion1 = qdf.id[qdf['n_questions']==1][0:10].tolist()\n\nlist_id = noformule + formule + nonumber + number + noquestion + question1\nset_id = set(list_id)\n\ndf_train.iloc()\ndf1 = df_train.loc[df_train['qid1'].isin(set_id)]\ndf2 = df_train.loc[df_train['qid2'].isin(set_id)]\n\ndf = pd.concat([df1,df2])\nprint(df.shape)\n\ndf.to_csv(\"Extract_before_formation.csv\", index=False)\nqdf.to_csv(\"QDF_TRAIN_PostTFIDF.csv\", index=False)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9475cf3dca7a316261776ddbf2960b083303e743"},"cell_type":"markdown","source":"# <span style=\"color:#006600\"> 6. Modelisation\n\n##  <span style=\"color:#39ac39\"> 6.1 Reconstitution de la base d'apprentissage"},{"metadata":{"trusted":true,"_uuid":"92e40c44b3631ae50c900804e26838ab0000771b","collapsed":true},"cell_type":"code","source":"print(\"Récupération des id\")\ndf_qid1 = qdf.loc[qdf['id'].isin(df_train.qid1.tolist())]\ndf_qid2 = qdf.loc[qdf['id'].isin(df_train.qid2.tolist())]\nprint(\"Shape de df_qid1\", df_qid1.shape)\nprint(\"Shape de df_qid2\", df_qid2.shape)\n\ndf_qid1.drop([\"index\"], axis = 1, inplace = True)\ndf_qid2.drop([\"index\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"712c39fd69c2b7a0772cc5de1d0cc1d309ba1f43","collapsed":true},"cell_type":"code","source":"print(\"Renommer les colonnes : \")\ndf_qid1.columns = [\"id_1\",\"question_orig_1\",\"question_1\",\"formule_1\",\"nombre_1\",\"questions_multiples_1\",\n                   'n_questions_1', 'mots_interrogatifs_1', 'sac_de_mots_1' \n                   # ,'tous_les_mots_1', 'tous_les_mots_uniques_1', \n                   # 'tf_idf1_1','tf_idf2_1', 'tf_idf3_1'\n                  ]\n  \ndf_qid2.columns = [\"id_2\",\"question_orig_2\",\"question_2\",\"formule_2\",\"nombre_2\",\"questions_multiples_2\",\n                   'n_questions_2', 'mots_interrogatifs_2', 'sac_de_mots_2',  \n                   # 'tous_les_mots_2', 'tous_les_mots_uniques_2', \n                   # 'tf_idf1_2','tf_idf2_2', 'tf_idf3_2'\n                  ]\nprint(\"Affichage des noms de colonnes : \\n\", list(df_qid1.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c5fd57ab046f451f64eb45375ab6d91991c7629","collapsed":true},"cell_type":"code","source":"# Merge df 1\ndf_qid1.id_1 = df_qid1.id_1.astype(\"int64\") ; \ndf_qid2.id_2 = df_qid2.id_2.astype(\"int64\") ;\n# Suppression des colonnes inutiles \ndf_qid1.drop(['question_orig_1'], axis=1, inplace=True)\ndf_qid2.drop(['question_orig_2'], axis=1, inplace=True)\n# Affichage des caractéristiques des tables\nprint(\"Info sur df_qid1 : \\n\", df_qid1.info())\nprint(\"Info sur df_qid2 : \\n\", df_qid2.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bae6cb50bad4321405f5cdd90c0a723d1ef354dc","collapsed":true},"cell_type":"code","source":"print(\"Constitution de la base de modélisation :\")\ndf_model = df_train; print(df_model.shape)\n\nprint(\"\\n   >>> Ajout des données de qdf1 : \")\ndf_model = df_model.merge(df_qid1, left_on = \"qid1\", right_on = \"id_1\") ; print(df_model.shape)\ndel df_qid1; gc.collect()\n\nprint(\"\\n   >>> Ajout des données de qdf2 : \")\ndf_model = df_model.merge(df_qid2, left_on = \"qid2\", right_on = \"id_2\") ; print(df_model.shape)\ndel df_qid2; gc.collect()\n\nprint(\"\\n   >>> Suppression des variables inutiles: \")\ndf_model.drop([\"id_1\",\"id_2\"], axis = 1, inplace = True)\n\ndf_model.sort_values(['qid2'], ascending=[True], inplace=True)\nprint(df_model.columns)\nprint(\"End\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92c0b26d46b7ab00016d716412f8262cc2e9ce19","collapsed":true},"cell_type":"code","source":"print(\"Suppression des variables inutiles : \")\nlist_column_drop = ['qid1','qid2','question1','question2',\n                    \"questions_multiples_1\",\"questions_multiples_2\", \n                    \"sac_de_mots_1\",\n                    # ,\"tous_les_mots_1\",\"tous_les_mots_uniques_1\",\n                    # \"tf_idf1_1\",\"tf_idf2_1\",\"tf_idf3_1\",\n                    \"sac_de_mots_2\",\n                    # ,\"tous_les_mots_2\",\"tous_les_mots_uniques_2\",\n                    # \"tf_idf1_2\",\"tf_idf2_2\",\"tf_idf3_2\",\n                    \"mots_interrogatifs_1\",\"mots_interrogatifs_2\",\n                    \"formule_1\",\"nombre_1\",\"formule_2\",\"nombre_2\"]\ndf_model.drop(list_column_drop, axis = 1, inplace = True)\n\n# Visualistion du résultat\nprint(df_model.columns.tolist())\ndf_model.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dbc3b7669e16ffa0ea2beec718ce5beecdd08b6d"},"cell_type":"code","source":"def get_weight(count, eps=10000, min_count=2):\n    return 0 if count < min_count else 1 / (count + eps)\n\ndef word_shares(row):\n    q1 = set(str(row['question_1']).lower().split())\n    q1words = q1.difference(stops)\n    if len(q1words) == 0:\n        return '0:0:0:0:0'\n\n    q2 = set(str(row['question_2']).lower().split())\n    q2words = q2.difference(stops)\n    if len(q2words) == 0:\n        return '0:0:0:0:0'\n\n    q1stops = q1.intersection(stops)\n    q2stops = q2.intersection(stops)\n\n    shared_words = q1words.intersection(q2words)\n    shared_weights = [weights.get(w, 0) for w in shared_words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n\n    R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n    R2 = len(shared_words) / (len(q1words) + len(q2words)) #count share\n    R31 = len(q1stops) / len(q1words) #stops in q1\n    R32 = len(q2stops) / len(q2words) #stops in q2\n    return '{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96d0132747caf4f343183b5cdc30b3c6c5fbaf4a"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 6.2. Variables additionnelles"},{"metadata":{"trusted":true,"_uuid":"b286e49a27653a553ca5ebf9855ef5d4779d4fe6","collapsed":true},"cell_type":"code","source":"start = time.time();\nprint(\"Creation de nouvelles features : \\n\")\n\ntrain_qs = pd.Series(df_model['question_1'].tolist() + df_model['question_2'].tolist()).astype(str)\nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}\nstops = set(stopwords.words(\"english\"))\n\nprint(\"   >>> Procedure\")\ndf_model['word_shares'] = df_model.apply(word_shares, axis=1, raw=True)\n\n\nprint(\"   >>> Retraitement de la base : \")\ndf_model['word_match']       = df_model['word_shares'].apply(lambda x: float(x.split(':')[0]))\ndf_model['tfidf_word_match'] = df_model['word_shares'].apply(lambda x: float(x.split(':')[1]))\ndf_model['shared_count']     = df_model['word_shares'].apply(lambda x: float(x.split(':')[2]))\n\ndf_model['stops1_ratio']     = df_model['word_shares'].apply(lambda x: float(x.split(':')[3]))\ndf_model['stops2_ratio']     = df_model['word_shares'].apply(lambda x: float(x.split(':')[4]))\ndf_model['diff_stops_r']     = df_model['stops1_ratio'] - df_model['stops2_ratio']\n\ndf_model['len_q1'] = df_model['question_1'].apply(lambda x: len(str(x)))\ndf_model['len_q2'] = df_model['question_2'].apply(lambda x: len(str(x)))\ndf_model['diff_len'] = df_model['len_q1'] - df_model['len_q2']\n\ndf_model['len_char_q1'] = df_model['question_1'].apply(lambda x: len(str(x).replace(' ', '')))\ndf_model['len_char_q2'] = df_model['question_2'].apply(lambda x: len(str(x).replace(' ', '')))\ndf_model['diff_len_char'] = df_model['len_char_q1'] - df_model['len_char_q2']\n\ndf_model['len_word_q1'] = df_model['question_1'].apply(lambda x: len(str(x).split()))\ndf_model['len_word_q2'] = df_model['question_2'].apply(lambda x: len(str(x).split()))\ndf_model['diff_len_word'] = df_model['len_word_q1'] - df_model['len_word_q2']\n\ndf_model['avg_world_len1'] = df_model['len_char_q1'] / df_model['len_word_q1']\ndf_model['avg_world_len2'] = df_model['len_char_q2'] / df_model['len_word_q2']\ndf_model['diff_avg_word'] = df_model['avg_world_len1'] - df_model['avg_world_len2']\n\ndf_model.drop([\"word_shares\"], axis = 1, inplace = True)\n\n\"\"\"\ndf_model['common_words'] = df_model.apply(lambda x: \n                                    len(set(str(df_model['question_1']).lower().split()).intersection(set(str(df_model['question_2']).lower().split()))),\n                                    axis=1)\n\ndf_model['exactly_same'] = (df_model['question_1'] == df_model['question_2']).astype(int)\ndf_model['duplicated'] = df_model.duplicated(['question_1','question_2']).astype(int)\n\"\"\"\n\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b888474681dab8fcdc9775ad2e9b4f566d2bbdb"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 6.3. TF IDF sur les phrases nettoyées\n\n### <span style=\"color:#53c653\"> 6.3.1. Quelques petits rappel de la dernière fois\n\n\nPour certaines tâches spécifiques impliquant plusieurs textes, (comparaison de textes, ou recherche par-mot clé), il vaudra mieux utiliser le modèle de probabilité TF-IDF qui est plus performant que le modèle Unigram. \n\n![TFIDF](https://github.com/sdaymier/NLP/blob/master/TFIDF.PNG?raw=true \"Logo Title Text 1\")\n\nDans ce modèle, un mot est important s'il apparaît \n* Beaucoup dans le corpus,\n* Mais dans peu de textes.\nCela signifie qu'il est vraiment porteur d'un sens fort, qui permet de discriminer entre les textes. \n\n### <span style=\"color:#53c653\"> 6.3.2. Methode 1 : \"A la mano\"\nComment allons nous calculer ?\n* **TF** : Indique la fréquence du mot dans chaque document du corpus. C'est le rapport entre le nombre de fois que le mot apparaît dans un document et le nombre total de mots dans ce document. Il augmente à mesure que le nombre d'occurrences de ce mot dans le document augmente. Chaque document a son propre tf.\n* **IDF** : utilisé pour calculer le poids des mots rares dans tous les documents du corpus. Les mots qui apparaissent rarement dans le corpus ont un score IDF élevé.\n \n \nAvec notre base de donnés : chaque phrase est un _document_. "},{"metadata":{"_uuid":"1cb87a81c450b03852b5ad72fdc0f5766fff6853"},"cell_type":"markdown","source":"##### Creation du vocabulaire"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8edfdab984620092701cbe4ac97e6e2cbd74c7d8"},"cell_type":"code","source":"#Creation du vocabulaire\nfrom functools import reduce\n\ndef Fusionne_Liste_De_Liste_De_Mots(llm):\n    return [x for lm in llm for x in lm] \n\ndef Unique(l):\n    return list(set(l))\n\nstart = time.time();\nprint(\"Construction du vocabulaire : \\n\")\n\nprint(\"   >>> Recupération de tous les mots présents dans le sac de mot, créé sur toutes les phrases du corpus : \")\nqdf[\"tous_les_mots\"] =  qdf['sac_de_mots'].apply(Fusionne_Liste_De_Liste_De_Mots)\nprint(\"   >>> Creation d'une liste de tous les mots unique : \")\nqdf[\"tous_les_mots_uniques\"] =  qdf['tous_les_mots'].apply(Unique)\nprint(\"   >>> Affichage du résultat : \")\nprint(qdf['tous_les_mots'].head(5))\n\n# Création d'un objet Python permettant de réaliser des boucles efficientes (ce sont des itérables)\n#    >>> Pour les mots (avec doublons)\n#    >>> Pour la liste de mot unique\nvocab = itertools.chain.from_iterable(qdf['tous_les_mots'].values)\nvocab_pour_doc_freq = itertools.chain.from_iterable(qdf['tous_les_mots_uniques'].values)\n\n# Fin de la procédure\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8df82e1ab78127035d24d294de69d1f4702a6fc7"},"cell_type":"markdown","source":"##### Creation de notre dictionnaire"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5b3403570335c87b6eb34bffbc25631043297744"},"cell_type":"code","source":"import math\n\nprint(\"   >>> Comptage de l'occurence des mots dans toutes nos phrases : \")\ndic_frequence = Counter(vocab)\nprint(len(dic_frequence))\n\nprint(\"   >>> Comptage de l'occurence des mots uniques dans toutes nos phrases : \")\ndic_frequence_pour_doc_freq = Counter(vocab_pour_doc_freq)\n\nprint(\"   >>> Comptage de l'occurence des mots dans toutes nos phrases : \")\nfrequence_mots = sorted(dic_frequence.items(), key=operator.itemgetter(1), reverse=True)\nprint(frequence_mots[0:100])\nprint(frequence_mots[5000:5100])\nprint(frequence_mots[60000:60100])\nprint(frequence_mots[-100:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b406658e667df41ebadf5b1f2f90697656ec4b2e"},"cell_type":"markdown","source":"##### Calcul de l'IDF selon différentes méthodes : \n\nDifférentes formules sont testées :\n* dict_idf1 :\n* dict_idf2 : \n* dict_idf3 : "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"edbf1350738691bb686fa2ca24cedeba35df7907"},"cell_type":"code","source":"dict_idf1 = {mot: 1/(10.0+dic_frequence_pour_doc_freq[mot]) for mot in dic_frequence_pour_doc_freq}\ndict_idf2 = {mot: math.log(float(len(qdf))/dic_frequence_pour_doc_freq[mot]) for mot in dic_frequence_pour_doc_freq}\ndict_idf3 = {mot: math.log((float(len(qdf))-dic_frequence_pour_doc_freq[mot]+0.5) \\\n                           /(dic_frequence_pour_doc_freq[mot]+0.5)) for mot in dic_frequence_pour_doc_freq}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"813fe158eb952d4e34afb0a766b65f82e3b30af3"},"cell_type":"markdown","source":"##### Calcul du TF - IDF selon les différentes formules de l'IDF\n\nPour déterminer le TF, on calcle simplement l'occurence des mots dans notre document. \nPar exemple, pour le premier document de notre base d'apprentissage (c'est-à-dire la première phrase/question), on a : \n\n** \"what is the step by step guide to invest in share market in india?\" ** : \n* 'step': 2, \n* 'in': 2, \n* 'what': 1, \n* 'is': 1,\n* 'the': 1, \n* 'by': 1,\n* 'guide': 1, \n* 'to': 1,\n* 'invest': 1,\n* 'share': 1,\n* 'market': 1,\n* 'india': 1\n\nCe résultat est obtenu en utilisant la fonction *Counter()*"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1de3c23cc8b28db448a5fba2f78c92119b7f46bb"},"cell_type":"code","source":"def tf_idf1_sac_de_mots(sac_de_mots):\n    compt = Counter(sac_de_mots)                       # Nombre de mot dans le sac de mot de la phrase \n    sac_de_mots = list(set(sac_de_mots))               # Lister de manière unique les mots présent dans le sac de mot\n    print(\"\\nprint sac_de_mots : \\n\" , sac_de_mots)\n    return {x:(float(compt[x]))*dict_idf1[x] for x in sac_de_mots} \n\ndef tf_idf2_sac_de_mots(sac_de_mots):\n    compt = Counter(sac_de_mots)\n    sac_de_mots = list(set(sac_de_mots))\n    return {x:(float(compt[x]))*dict_idf2[x] for x in sac_de_mots}\n\ndef tf_idf3_sac_de_mots(sac_de_mots):\n    compt = Counter(sac_de_mots)\n    sac_de_mots = list(set(sac_de_mots))\n    return {x:(float(compt[x]))*dict_idf3[x] for x in sac_de_mots}\n\ndef norm1(dic_tf_idf):\n    return sum(list(dic_tf_idf.values()))\n\ndef norm2(dic_tf_idf):\n    return sum([x*x for x in list(dic_tf_idf.values())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"184566ef9c866b0cbacfa5aa6d84bd4f5f00663e","collapsed":true},"cell_type":"code","source":"# Avec la fonction apply on présente les phrases une a une.\n# res=qdf['sac_de_mots'][0:1].apply(lambda x: [tf_idf1_sac_de_mots(y) for y in x]) \n\n# qdf[\"tf_idf2\"]=qdf['sac_de_mots'].apply(lambda x: [tf_idf2_sac_de_mots(y) for y in x])\n# qdf[\"tf_idf3\"]=qdf['sac_de_mots'].apply(lambda x: [tf_idf3_sac_de_mots(y) for y in x])\n\n# print(\"Nombre de variable : \", len(list(qdf.columns)))\n# print(\"Liste des variables : \", list(qdf.columns))\n# qdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"84080df5c0849bd6336f598b616972e3885c113e"},"cell_type":"code","source":"# Avec la fonction apply on présente les phrases une a une.\nqdf[\"tf_idf1\"]=qdf['sac_de_mots'].apply(lambda x: [tf_idf1_sac_de_mots(y) for y in x]) \nqdf[\"tf_idf2\"]=qdf['sac_de_mots'].apply(lambda x: [tf_idf2_sac_de_mots(y) for y in x])\nqdf[\"tf_idf3\"]=qdf['sac_de_mots'].apply(lambda x: [tf_idf3_sac_de_mots(y) for y in x])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43c4a686714b0230ad02b1d39c20461a9f8632aa"},"cell_type":"markdown","source":"##### Quelques affichages"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8a5c2499cc37ca951e527145161455702a2d7ed5"},"cell_type":"code","source":"qdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb70cbdb8bbebfe1382b40383991416631dd65c8"},"cell_type":"markdown","source":"### <span style=\"color:#53c653\"> 6.3.3. Methode 2 : Utilisation du module \"TfidfVectorizer\" de Sklearn\n\nPlus facile a utiliser, moins de risque de se tromper dans les formules mais moins flexibles que précédemment.\n\nDans ce module, l'IDF est calculé comme suit : \n\nidf(d,t) = log(n * / df(d,t)) + 1 avec :\n* n : le nombre total de document\n* df(d,t) : le _document frequency_\n* +1 : paramètre de régularisation au cas où le log serait nul.\n\n**Quelques paramètres **: \n* **strip_accents** : Supprimer les accents pendant l'étape de prétraitement\n* **ngram_range** : Les limites inférieure et supérieure de la plage de valeurs n pour différents n-grammes à extraire. Toutes les valeurs de n telles que min_n <= n <= max_n seront utilisées\n* **min_df** : Lors de la construction du vocabulaire, ignorez les termes qui ont une fréquence de document strictement inférieure au seuil donné\n* **max_df** : Lors de la construction du vocabulaire, ignorez les termes qui ont une fréquence de document strictement supérieure au seuil donné (permet d'éliminer notamment les stopwords spécifiques au corpus)\n* **max_features** :  Permet de considérer uniquement les x premiers features (ordonné par la fréquence d'occurence dans le corpus), si le paramètre n'est pas mis a \"none\"\n* **use_idf ** : Activer la repondération idf (paramètre booléen)\n* **smooth_idf** : permet d'empecher les divisions par 0\n* **sublinear_tf ** : Appliquez une mise à l'échelle sur le tf, c'est-à-dire remplacez tf par 1 + log (tf)."},{"metadata":{"trusted":true,"_uuid":"652283f31e2df7db85ed87f411905e984a56c03d","collapsed":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport scipy  as sp\n\nstart = time.time();\n\nprint(\"Lancement du TF-IDF : \\n\")\n\n# term-frequency-inverse-document-frequency\n\ntfv = TfidfVectorizer(min_df=2,  \n                      max_features=20000, \n                      lowercase=True, \n                      stop_words= 'english',\n                      strip_accents='unicode',    \n                      ngram_range=(1, 1),        \n                      use_idf=1,\n                      smooth_idf=1,\n                      sublinear_tf=1)\n            \n# Fit TFIDF - Learn the idf vector (global term weights)\ntfv.fit(pd.concat([df_model['question_1'],df_model['question_2']]))\n\n# Transform data - Transform a count matrix to a tf or tf-idf representation\ntr1 = tfv.transform(df_model['question_1']) \ntr2 = tfv.transform(df_model['question_2'])\n\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))\n\nprint(\"SHape of tr1\", tr1.shape)\nprint(\"SHape of tr2\", tr2.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"102fac96dd6e65f42b5b4e6eb3fe8c01e5d25171"},"cell_type":"markdown","source":"**Remarque **: il est possible de lancer le TFIDF sur des bigrams, trigram ... Ngram en modifiant le paramètre **ngram_range**. En effet, si on considère :\n* ngram_range = (1, 1) : on réalise le tf-idf sur des unigrams (sac de mot donc ! )\n* ngram_range = (1, 2) : le tf idf est réalisé sur des bigrams \n* etc ...\n\nIl est donc possible de réaliser plusieurs TF-IDF et de concatener les résultats.\n\nExemple de programme python: \n```\n# 1. Realisation du TF IDF sur des unigrams\ntfidf_word = TfidfVectorizer(analyzer='word',\n                             ngram_range=(1, 1)) \nX_word = tfidf_word.fit_transform(X)\n\n# 2. Realisation du TF IDF sur des bigrams\ntfidf_2word = TfidfVectorizer(analyzer='char', \n                              ngram_range=(2,2))\nX_2word = tfidf_2word.fit_transform(X)\n\n# 3. Concatenation des résultats\nX_all_TFIDF = sparse.hstack([X_word, X_2word])\n```"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"17bbcacdba69477c02c97db7d6c4874a7e98f14b"},"cell_type":"markdown","source":"### <span style=\"color:#53c653\"> 6.3.4. Analyses des résultats du tfidf\n\n#### Affichage du vocabulaire et du comptage de mot - calculé par TF IDF de scikit learn"},{"metadata":{"trusted":true,"_uuid":"be1cb86ea292c0c4eda83cc7454a9d9de7c5eae7","collapsed":true},"cell_type":"code","source":"tfv.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f15a6a26687268914c053ac4ce034436f1c0da3f"},"cell_type":"markdown","source":"#### Recuperation des features créées par le TF-IDF de scikit learn \n\n* la fonction \"get_feature_names()\" permet de récupérer la liste de tous les mots du vocabulaire de tf-idf, dans le même ordre que les colonnes de la matrice."},{"metadata":{"trusted":true,"_uuid":"631a3e7d994f75b3e5b6617b2c40afe2e5e74053","collapsed":true},"cell_type":"code","source":"features = tfv.get_feature_names()\nprint(\"Quelques exemples de feature : \\n\", features[10000:10020])\nprint(\"Quelques exemples de feature : \\n\", features[51:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cb43cf2c679ab1efb0c7550bcdec5acfb23486a2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbf2c31ba6988a6fae748ccd5dbfbed1b163c46e"},"cell_type":"markdown","source":"#### Highest score pour une question\n\nNous créons une fonction qui prend une seule ligne de la matrice tf-idf (correspondant à un document particulier - cad dans notre cas a une phrase), et renvoie les _n_ mots les plus performants.\n\nComment procède-t-on ? \n* Dans la fonction **\"top_tfidf_feats(params)\"**: On utilise la fonction _argsort()_ pour  réordonner en ordre décroissant les scores obtenues par le TF IDF puis on sélectionne le premier _top_n_. Nous retournons alors un DataFrame pandas avec les mots eux-mêmes (noms de caractéristiques) et leur score associé.\n* Dans la fonction **\"top_feats_in_doc(params)\"** : le TF IDF de scikit learn produit une **sparse matrix** , qui ne supporte pas toutes les opérations habituelles de matrice ou de tableau. Donc, afin d'appliquer la fonction ci-dessus, nous convertissons cette matrice en un format plus usuel "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1bbb32311ea84a7c01fde4080d0e2fdf02e386ad"},"cell_type":"code","source":"def top_tfidf_feats(row, features, top_n=20):\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats, columns=['features', 'score'])\n    return df\n\ndef top_feats_in_doc(X, features, row_id, top_n=25):\n    row = np.squeeze(X[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fea2de86fc231aacb6ed9c5757d2b5bb6d6dfa55","collapsed":true},"cell_type":"code","source":"irow = 1\nprint(\"Highest score pour la ligne {} :  \\n{}\".format(irow, top_feats_in_doc(tr1, features, irow, 5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92f0746641fba4db44f1ac96233c975dba566cdd","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86b33dd58ac2db0899a5a93adba65a5a89fc6eea"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 6.4. Ajout des variables complémentaires"},{"metadata":{"trusted":true,"_uuid":"16fc64b8b95805e6a1a2b0ec67748b229fe5afe4","collapsed":true},"cell_type":"code","source":"start = time.time();\n\nprint(\"Ajout des variables complémentaires : \")\n# on ajoute toutes les variables numériques que l'on a créée précédemment\ndf_additional = df_model.drop([\"question_1\",\"question_2\",\"is_duplicate\"], axis = 1)\n\ndf_additional = df_additional.fillna(0)\ndf_additional.isnull().sum()\n\ny = df_model.is_duplicate.values\nX = sp.sparse.hstack([tr1,tr2,np.array(df_additional)])\nprint(\"Shape of design matrix : \", X.shape)\nprint(\"Shape du vecteur cible : \", y.shape)\n# Z = sp.sparse.hstack([ts1,ts2])\n\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60c8b1fab8407759a8e4e0c4d125b0a77f339812"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 6.5. Split en table d'apprentissage et de validation\n\nPour construire un modèle, on utilise trois tables : \n* La base de <span style=\"background-color:#ccffcc\"> ** \"train\" ** </span> :\n* La base de <span style=\"background-color:#ccffcc\"> ** \"validation\" ** </span> :\n* La base de <span style=\"background-color:#ccffcc\"> ** \"test\" ** </span> :\n\n\n![TrainValidTest](https://github.com/sdaymier/NLP/blob/master/TrainValidTest.PNG?raw=true \"RandomForest\")\n\n"},{"metadata":{"trusted":true,"_uuid":"0b112667e7d2a5c77b6117f07c637072b790b820","collapsed":true},"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\n\nstart = time.time();\nprint(\"Creation de la base d'apprentissage et de validation : \\n\")\n\nx_train, x_val, y_train, y_val = train_test_split(X, \n                                                  y, \n                                                  test_size=0.4,\n                                                  random_state=42)\nprint(\"-------------------------------------------------------- \")\nprint(\"Dimension du vecteur cible pour l'apprentissage y_train : \", y_train.shape)\nprint(\"Dimension de la design matrix pour l'apprentissage x_train : \", x_train.shape)\nprint(\"\\n-------------------------------------------------------- \")\nprint(\"Dimension du vecteur cible pour la validation y_val : \", y_val.shape)\nprint(\"Dimension de la design matrix pour la validation x_val : \", x_val.shape)\n\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fad971258f9cf6b9c676f9d78c04f570aa0a775b"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 6.6. Random Forest\n\n### <span style=\"color:#53c653\"> 6.6.1. Aspect théorique global\n\n\nLes  <span style=\"background-color:#ccffcc\"> **   forêts aléatoires ** </span> (ou <span style=\"background-color:#ccffcc\"> **   forêts aléatoires ** </span> Random Forest) sont conçues pour améliorer la précision des modèles CART en construisant des plusieurs arbres (c'est-à-dire une forêt!). \nL'inconvénient de l'utilisation de Random Forests est que les modèles deviennent moins faciles à comprendre et moins interprétables, mais ils peuvent améliorer la précision de vos prédictions.\n\n\nLe schéma suivant synthétise l'algorithme : \n![RandomForest](https://github.com/sdaymier/NLP/blob/master/RandomForest.PNG?raw=true \"RandomForest\")\n\n### <span style=\"color:#53c653\"> 6.6.2. En pratique avec le module de sklearn\n\n**Note** : temps d'execution - 164.64 secondes avec : \n* n_estimators = 5, \n* min_samples_split = 100"},{"metadata":{"trusted":true,"_uuid":"9df1f0f52715ec118c48ca8caa10c422ebcea7e3","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n\nstart = time.time();\nprint(\"RANDOM FOREST : \\n\")\n\nprint(\"   >>> Setting : \")\nrf = RandomForestClassifier(n_estimators = 5, \n                            min_samples_split = 100,  \n                            verbose=1)\nprint(\"   >>> Fitting : \")\nrf.fit(x_train, y_train)\n\nprint(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2e8c6bec301c3bbb7a8bcefc31ba2ecc3ad07e7f"},"cell_type":"markdown","source":"##  <span style=\"color:#39ac39\"> 6.7. Evaluation\n### <span style=\"color:#53c653\"> 6.7.1. Quelques metrics !\n\n#### AUC \n"},{"metadata":{"trusted":true,"_uuid":"bb6142ede5b3afd387cc9362cbd310cd4ecc918e","collapsed":true},"cell_type":"code","source":"from sklearn import metrics\n\nprint(\"AUC\")\nprint(\"   >>> AUC sur la base d'apprentissage : \")\nx_train_pred =  rf.predict(x_train)\nacc = roc_auc_score(y_train, x_train_pred)\nprint(\"AUC sur la base de train  : \", acc)\n\nprint(\"\\n   >>> AUC sur la base de validation : \")\nx_val_pred = rf.predict(x_val)\nacc_valid = roc_auc_score(y_val, x_val_pred)\nprint(\"AUC sur la base de validation  : \", acc_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abad21c158e3be9bfbb7ad57b237f215ba0b3bdd"},"cell_type":"markdown","source":"#### Accuracy\n"},{"metadata":{"trusted":true,"_uuid":"ef05d519d35a75ed4abdf70e103a18fe0e9fc917","collapsed":true},"cell_type":"code","source":"print(\"Accuracy sur la base d'apprentissage : \\n\", metrics.accuracy_score(y_train,x_train_pred))\nprint(\"\\nAccuracy sur la base de validation : \\n\", metrics.accuracy_score(y_val,x_val_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ba04f1ef4197eb5d080cbf5f2f9e18c78821f42"},"cell_type":"markdown","source":"#### Matrice de confusion"},{"metadata":{"trusted":true,"_uuid":"d08cde95d33c5b80b58e7427c0f4e3eb21ffd782","collapsed":true},"cell_type":"code","source":"print(\"Matrice de confusion sur la base d'apprentissage : \\n\", metrics.confusion_matrix(y_train,x_train_pred))\nprint(\"\\nMatrice de confusion sur la base de validation : \\n\", metrics.confusion_matrix(y_val,x_val_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19ac66f0ab5d67c7169462a6f4fa12e9eb96a2ae"},"cell_type":"markdown","source":"### <span style=\"color:#53c653\"> 6.7.2. Importance des variables"},{"metadata":{"trusted":true,"_uuid":"d40d661c19519bc47a9a4563e535dc71f0c9eab4","collapsed":true},"cell_type":"code","source":"#This prints the top 10 most important features\nres = pd.DataFrame(sorted(zip(rf.feature_importances_, \n           tfv.get_feature_names()), \n       reverse=True))\nres.columns = [\"Importance\",\"Feature\"]\nres.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb40453ac612333611a767f5c5313a2d9d177a45","collapsed":true},"cell_type":"code","source":"import seaborn as sns\ndata_to_plot = res[0:10]\nplt.figure(figsize = (8,5))\nsns.barplot(x = data_to_plot.Importance, y = data_to_plot.Feature, orient = 'h')\nplt.title('Importance des variables - Random Forest Result', fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2351a1f2063ef1833c15c820df5e75b30d1cbead"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67e400a98aa90dc2e77fbb5021fc3a01b5857d97","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}