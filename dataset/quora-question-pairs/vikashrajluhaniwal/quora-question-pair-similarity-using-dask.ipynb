{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Notebook - Table of Content\n\n1. [**Importing necessary libraries**](#1.-Importing-necessary-libraries)   \n2. [**Loading data using dask**](#2.-Loading-data-using-dask)   \n3. [**Basic Data Analysis**](#3.-Basic-Data-Analysis)  \n    3.1 [**Checking for class imbalance**](#3.1-Checking-for-class-imbalance)  \n    3.2 [**Number of distinct questions**](#3.2-Number-of-distinct-questions)  \n4. [**Data preprocessing**](#4.-Data-preprocessing)  \n    4.1 [**Checking for duplicates**](#4.1-Checking-for-duplicates)  \n    4.2 [**Checking for missing values**](#4.2-Checking-for-missing-values)  \n5. [**Basic Feature Extraction**](#5.-Basic-Feature-Extraction)  \n    5.1 [**Analysis on few extracted features**](#5.1-Analysis-on-few-extracted-features)  \n6. [**Text preprocessing**](#6.-Text-preprocessing)   \n    6.1 [**Analysing extracted features **](#6.1-Analysing-extracted-features )  \n7. [**Featurization through weighted tf-idf based word vectors**](#7.-Featurization-through-weighted-tf-idf-based-word-vectors) \n8. [**Merging all the extacted features**](#8.-Merging-all-the-extacted-features)\n9. [**Machine Learning models**](#9.-Machine-Learning-models)  \n    9.1 [**Fitting Logistic Regression**](#9.1-Fitting-Logistic-Regression-model)  "},{"metadata":{},"cell_type":"markdown","source":"**Additional NOTE**\n\nIf you are interested in learning or exploring more about importance of feature selection in machine learning, then refer to my below blog offering.\n\nhttps://www.analyticsvidhya.com/blog/2020/10/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python/"},{"metadata":{},"cell_type":"markdown","source":"### 1. Importing necessary libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nfrom wordcloud import WordCloud, STOPWORDS\nfrom os import path\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport spacy\n\nimport dask.dataframe as dd\nimport dask.array as da","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Loading data using dask"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv.zip\")\ndf = dd.from_pandas(train_df, npartitions=5)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.visualize()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Basic Data Analysis\n\n#### 3.1 Checking for class imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,7))\nsns.countplot(df.iloc[:,5].compute())\nplt.title(\"Barplot of is_duplicate\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of qustion pairs : \", len(df))\nprint(\"% of question pairs which are similar : \", (len(df[df[\"is_duplicate\"]==1]) / len(df))*100)\nprint(\"% of question pairs which are not similar : \", (len(df[df[\"is_duplicate\"]==0]) / len(df))*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2 Number of distinct questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"distinct_qus = len(set(df['qid1'].compute().tolist() + df['qid2'].compute().tolist()))\nprint(\"Total number of distinct questions : \", distinct_qus)\n\nappended_series = df['qid1'].append(train_df['qid2']).compute()\nqus_freq_more_than_one = sum(appended_series.value_counts()>1)\nprint(\"Repeated questions(Number of questions having frequency more than one time) : \", qus_freq_more_than_one)\nprint(\"Highest repeat frequency : \", max(appended_series.value_counts()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,7))\nsns.barplot([\"Distinct\", \"Repeated\"], [distinct_qus, qus_freq_more_than_one])\nplt.title(\"Barplot indicating distinct and repeated questions\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Data preprocessing\n\n#### 4.1 Checking for duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of duplicate question pairs : \", df[['qid1','qid2']].compute().duplicated().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Frequency of each question"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nsns.distplot(appended_series.value_counts(),bins = 200, kde = False, color = \"blue\")\nplt.yscale('log', nonposy='clip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2 Checking for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().compute().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.fillna('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Basic Feature Extraction\n\n- freq_qid1 = Frequency of qid1's\n- freq_qid2 = Frequency of qid2's\n- q1len = Length of q1\n- q2len = Length of q2\n- q1_n_words = Number of words in Question 1\n- q2_n_words = Number of words in Question 2\n- word_Common = (Number of common unique words in Question 1 and Question 2)\n- word_Total =(Total num of words in Question 1 + Total num of words in Question 2)\n- word_share = (word_common)/(word_Total)\n- freq_q1+freq_q2 = sum total of frequency of qid1 and qid2\n- freq_q1-freq_q2 = absolute difference of frequency of qid1 and qid2"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count').compute()\ndf['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count').compute() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['q1len'] = df['question1'].str.len().compute()\ndf['q2len'] = df['question2'].str.len().compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['q1_n_words'] = df.apply(lambda row: len(row.question1.split(\" \")),axis=1).compute()\ndf['q2_n_words'] = df.apply(lambda row: len(row.question2.split(\" \")),axis=1).compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stripped_common_words(row):\n        set1 = set(map(lambda i: i.lower().strip(), row.question1.split(\" \")))\n        set2 = set(map(lambda i: i.lower().strip(), row.question2.split(\" \")))    \n        return 1.0 * len(set1 & set2)\ndf['word_Common'] = df.apply(stripped_common_words, axis=1).compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stripped_word_total(row):\n        set1 = set(map(lambda i: i.lower().strip(), row.question1.split(\" \")))\n        set2 = set(map(lambda i: i.lower().strip(), row.question2.split(\" \")))    \n        return 1.0 * (len(set1) + len(set2))\ndf['word_Total'] = df.apply(stripped_word_total, axis=1).compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stripped_word_share(row):\n        set1 = set(map(lambda i: i.lower().strip(), row.question1.split(\" \")))\n        set2 = set(map(lambda i: i.lower().strip(), row.question2.split(\" \")))    \n        return 1.0 * len(set1 & set2)/(len(set1) + len(set2))\ndf['word_share'] = df.apply(stripped_word_share, axis=1).compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2'].compute()\ndf['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2']).compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.1 Analysis on few extracted features"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Minimum number of words in question1 : \" , df['q1_n_words'].min().compute())\nprint (\"Minimum number of words in question2 : \" , df['q2_n_words'].min().compute())\nprint (\"Number of Questions with minimum words [question1] :\", len(df[df['q1_n_words']== 1]))\nprint (\"Number of Questions with minimum words [question2] :\", len(df[df['q2_n_words']== 1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.1.a Univariate analysis of feature word_share"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of word_share across both the duplicacy level\")\nsns.violinplot(x = df['is_duplicate'].compute(), y = df['word_share'].compute(),ax=ax1)\nax2.set_title(\"Distribution of word_share across both the duplicacy level\")\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_share'].compute() , label = \"1\", ax=ax2)\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_share'].compute() , label = \"0\" , ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.1.b Univariate analysis of feature word_common"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of word_Common across both the duplicacy level\")\nsns.violinplot(x = df['is_duplicate'].compute(), y = df['word_Common'].compute(),ax=ax1)\nax2.set_title(\"Distribution of word_Common across both the duplicacy level\")\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_Common'].compute() , label = \"1\", color = 'red',ax=ax2)\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_Common'].compute() , label = \"0\" , color = 'blue' ,ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Text preprocessing\n\nIt involves - \n- Removing html tags\n- Removing Punctuations\n- Removing Stopwords\n- Performing stemming\n- Expanding contractions etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = stopwords.words(\"english\")\ndef text_preprocess(txt):\n    txt = str(txt).lower()\n    txt = txt.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n          .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n          .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n          .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n          .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    txt = re.sub(r\"([0-9]+)000000\", r\"\\1m\", txt)\n    txt = re.sub(r\"([0-9]+)000\", r\"\\1k\", txt)\n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    if type(txt) == type(''):\n        txt = re.sub(pattern, ' ', txt)\n    if type(txt) == type(''):\n        txt = porter.stem(txt)\n        example1 = BeautifulSoup(txt)\n        txt = example1.get_text()\n               \n    return txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"safe_div = 0.0001\ndef fetch_token_features(q1, q2):\n    token_features = [0.0]*10\n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    q1_words = set([word for word in q1_tokens if word not in stop_words])\n    q2_words = set([word for word in q2_tokens if word not in stop_words])\n    q1_stops = set([word for word in q1_tokens if word in stop_words])\n    q2_stops = set([word for word in q2_tokens if word in stop_words])\n    common_word_count = len(q1_words & q2_words)\n    common_stop_count = len(q1_stops & q2_stops)\n    common_token_count = len(set(q1_tokens) & set(q2_tokens))\n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + safe_div)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + safe_div)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + safe_div)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + safe_div)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + safe_div)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + safe_div)\n    token_features[6]= int(q1_tokens[-1] == q2_tokens[-1])\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n    return token_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fetch the Longest Common sub string\ndef fetch_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].apply(text_preprocess).compute()\n    df[\"question2\"] = df[\"question2\"].apply(text_preprocess).compute()\n    token_features = df.apply(lambda row: fetch_token_features(row.question1, row.question2), axis=1).compute()\n    cwc_min = dd.from_array(np.array(list(map(lambda i: i[0], token_features)))).compute()\n    cwc_min.name = \"cwc_min\"\n    df.merge(cwc_min.to_frame())\n    cwc_max = dd.from_array(np.array(list(map(lambda i: i[1], token_features)))).compute()\n    cwc_max.name = \"cwc_max\"\n    df.merge(cwc_max.to_frame())\n    csc_min = dd.from_array(np.array(list(map(lambda i: i[2], token_features)))).compute()\n    csc_min.name = \"csc_min\"\n    df.merge(csc_min.to_frame())\n    csc_max = dd.from_array(np.array(list(map(lambda i: i[3], token_features)))).compute()\n    csc_max.name = \"csc_max\"\n    df.merge(csc_max.to_frame())\n    ctc_min = dd.from_array(np.array(list(map(lambda i: i[4], token_features)))).compute()\n    ctc_min.name = \"ctc_min\"\n    df.merge(ctc_min.to_frame())\n    ctc_max = dd.from_array(np.array(list(map(lambda i: i[5], token_features)))).compute()\n    ctc_max.name = \"ctc_max\"\n    df.merge(ctc_max.to_frame())\n    last_word_eq = dd.from_array(np.array(list(map(lambda i: i[6], token_features)))).compute()\n    last_word_eq.name = \"last_word_eq\"\n    df.merge(last_word_eq.to_frame())\n    first_word_eq = dd.from_array(np.array(list(map(lambda i: i[7], token_features)))).compute()\n    first_word_eq.name = \"first_word_eq\"\n    df.merge(first_word_eq.to_frame())\n    abs_len_diff = dd.from_array(np.array(list(map(lambda i: i[8], token_features)))).compute()\n    abs_len_diff.name = \"abs_len_diff\"\n    df.merge(abs_len_diff.to_frame())\n    mean_len = dd.from_array(np.array(list(map(lambda i: i[9], token_features)))).compute()\n    mean_len.name = \"mean_len\"\n    df.merge(mean_len.to_frame())\n    df[\"token_set_ratio\"] = df.apply(lambda row: fuzz.token_set_ratio(row.question1, row.question2), axis=1).compute()\n    df[\"token_sort_ratio\"] = df.apply(lambda row: fuzz.token_sort_ratio(row.question1, row.question2), axis=1).compute()\n    df[\"fuzz_ratio\"] = df.apply(lambda row: fuzz.QRatio(row.question1, row.question2), axis=1).compute()\n    df[\"fuzz_partial_ratio\"] = df.apply(lambda row: fuzz.partial_ratio(row.question1, row.question2), axis=1).compute()\n    df[\"longest_substr_ratio\"]  = df.apply(lambda row: fetch_longest_substr_ratio(row.question1, row.question2), axis=1).compute()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff = dd.from_pandas(train_df, npartitions=5)\ndff.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = extract_features(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1 Analysing extracted features \n\n#### 6.1.a Word cloud formation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_duplicate = df[df['is_duplicate'] == 1].compute()\ndf_nonduplicate = df[df['is_duplicate'] == 0].compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_duplicate = df[df['is_duplicate'] == 1].compute()\ndf_nonduplicate = df[df['is_duplicate'] == 0].compute()\n\nduplicate_flatten = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nnonduplicate_flatten = np.dstack([df_nonduplicate[\"question1\"], df_nonduplicate[\"question2\"]]).flatten()\nprint (\"Number of questions in duplicate pairs set(class 1) : \",duplicate_flatten.shape[0])\nprint (\"Number of questions in non-duplicate pairs set(class 0) : \",nonduplicate_flatten.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/working/\")\nnp.savetxt('train_duplicate.txt', duplicate_flatten, delimiter=' ', fmt='%s')\nnp.savetxt('train_nonduplicate.txt', nonduplicate_flatten, delimiter=' ', fmt='%s')\n#Reading the text files\nduplicate_w = open(path.join(\"/kaggle/working/\", 'train_duplicate.txt')).read()\nnonduplicate_w = open(path.join(\"/kaggle/working/\", 'train_nonduplicate.txt')).read()\nprint (\"Total number of words in duplicate pair set :\",len(duplicate_w))\nprint (\"Total number of words in non duplicate pair set :\",len(nonduplicate_w))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(STOPWORDS)\nstop_words.add(\"said\")\nstop_words.add(\"br\")\nstop_words.add(\" \")\nstop_words.remove(\"not\")\nstop_words.remove(\"no\")\nstop_words.remove(\"like\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(duplicate_w), stopwords=stop_words)\nwc.generate(duplicate_w)\nplt.figure(figsize =(10,8))\nplt.title(\"Word cloud for duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(nonduplicate_w), stopwords=stop_words)\nwc.generate(nonduplicate_w)\nplt.figure(figsize =(10,8))\nplt.title(\"Word cloud for non-duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.1.b Distribution of the token_sort_ratio "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of token_sort_ration across both the duplicacy level\")\nsns.violinplot(x = df['is_duplicate'].compute(), y = df['token_sort_ratio'].compute(), ax=ax1)\nax2.set_title(\"Distribution of token_sort_ration across both the duplicacy level\")\nsns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'].compute() , label = \"1\",ax=ax2)\nsns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'].compute() , label = \"0\" , ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.1.c Distribution of the fuzz_ratio "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of token_sort_ration across both the duplicacy level\")\nsns.violinplot(x = df['is_duplicate'].compute(), y = df['fuzz_ratio'].compute(), ax=ax1)\nax2.set_title(\"Distribution of token_sort_ration across both the duplicacy level\")\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'].compute() , label = \"1\",ax=ax2)\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'].compute() , label = \"0\" , ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Featurization through weighted tf-idf based word vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"ddf = dd.from_pandas(train_df, npartitions=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddf['question1'] = ddf.apply(lambda row: str(row.question1), axis=1).compute()\nddf['question2'] = ddf.apply(lambda row: str(row.question2), axis=1).compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merge_questions = list(ddf['question1'].compute()) + list(ddf['question2'].compute())\ntfidf = TfidfVectorizer(lowercase=False)\ntfidf.fit_transform(merge_questions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_to_idf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v = spacy.load('en_core_web_sm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_vec_q1 = []\nfor qus1 in tqdm(list(ddf['question1'].compute())):\n    doc_q1 = w2v(qus1)\n    mean_vec_q1 = np.zeros([len(doc_q1), len(doc_q1[0].vector)])\n    for word in doc_q1:\n        vec = word.vector\n        try:\n            idf = word_to_idf[str(word)]\n        except:\n            idf = 0\n        mean_vec_q1 += vec * idf\n    mean_vec_q1 = mean_vec_q1.mean(axis=0)\n    w2v_vec_q1.append(mean_vec_q1)\nq1_feats_m = dd.from_array(np.array(list(w2v_vec_q1))).compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_vec_q2 = []\nfor qus2 in tqdm(list(ddf['question2'].compute())):\n    doc_q2 = w2v(qus2)\n    mean_vec_q2 = np.zeros([len(doc_q2), len(doc_q2[0].vector)])\n    for word in doc_q2:\n        vec = word.vector\n        try:\n            idf = word_to_idf[str(word)]\n        except:\n            idf = 0\n        mean_vec_q2 += vec * idf\n    mean_vec_q2 = mean_vec_q2.mean(axis=0)\n    w2v_vec_q2.append(mean_vec_q2)\nq2_feats_m = dd.from_array(np.array(list(w2v_vec_q2))).compute()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. Merging all the extacted features"},{"metadata":{"trusted":true},"cell_type":"code","source":"q1_feats_m[\"id\"] = df[\"id\"]\nq2_feats_m[\"id\"] = df[\"id\"]\ndf_q = q1_feats_m.merge(q2_feats_m,on =\"id\",how = \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop([\"qid1\", \"qid2\", \"question1\",\"question2\"], axis=1).compute()\ndf_final = df.merge(df_q,on =\"id\",how = \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = dd.from_pandas(df_final, npartitions=5)\ndf_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting into train and test set with 70:30 ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"from dask_ml.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_final[\"is_duplicate\"].compute()\ndf_final = df_final.drop(['id', 'is_duplicate'],axis=1).compute()\nX_train,X_test, y_train, y_test = train_test_split(df_final, y, test_size=0.3,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training data size :\",X_train.shape)\nprint(\"Test data size :\",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9. Machine Learning models\n\n#### 9.1 Fitting Logistic Regression model \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from dask_ml.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(random_state = 42)\nclf.fit(X_train.values, y_train.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from dask_ml.metrics import accuracy_score, log_loss\ny_pred = clf.predict_proba(X_test.values)\nprint(\"Log loss of the model : \", log_loss(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test.values)\nprint(\"Accuracy of the model : \", accuracy_score(da.from_array(y_test, chunks = 5),da.from_array(y_pred, chunks = 5)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}