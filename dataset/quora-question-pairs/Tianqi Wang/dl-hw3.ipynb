{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basic Function\nimport numpy as np \nimport pandas as pd \nimport os\nimport spacy\nimport string\nimport re\nimport numpy as np\nfrom spacy.symbols import ORTH\nfrom collections import Counter\nfrom tqdm import tqdm_notebook\n\n# Keras for text preprocessing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/quora-question-pairs/train.csv')#.fillna('something')\ntest = pd.read_csv('../input/quora-question-pairs/test.csv')#.fillna('something')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# note that spacy_tok takes a while run it just once\ndef encode_sentence(path, vocab2index, N=400, padding_start=True):\n    x = spacy_tok(path.read_text())\n    enc = np.zeros(N, dtype=np.int32)\n    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in x])\n    l = min(N, len(enc1))\n    if padding_start:\n        enc[:l] = enc1[:l]\n    else:\n        enc[N-l:] = enc1[:l]\n    return enc, l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\ndef sub_br(x): return re_br.sub(\"\\n\", x)\n\nmy_tok = spacy.load('en')\ndef spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(sub_br(x))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# counts = Counter()\n# for text in tqdm_notebook(train.question1):\n#     counts.update(spacy_tok(text))\n# for text in tqdm_notebook(train.question2):\n#     counts.update(spacy_tok(text))\n# for text in tqdm_notebook(test.question1):\n#     counts.update(spacy_tok(text))\n# for text in tqdm_notebook(teat.question2):\n#     counts.update(spacy_tok(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN=60\nWORD_NUM = 180000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_q1 = train[\"question1\"].fillna(\"something\").values\ntrain_q2 = train[\"question2\"].fillna(\"something\").values\n\ntest_q1 = test[\"question1\"].fillna(\"something\").values\ntest_q2 = test[\"question2\"].fillna(\"something\").values\n\ntokenizer = Tokenizer(num_words=MAX_LEN, filters='!\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(list(train_q1)+list(train_q2)+list(test_q1)+list(test_q2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_q1_seq = tokenizer.texts_to_sequences(train_q1)\n# train_q2_seq = tokenizer.texts_to_sequences(train_q2)\n# test_q1_seq = tokenizer.texts_to_sequences(test_q1)\n# test_q2_seq = tokenizer.texts_to_sequences(test_q2)\n# train_q1_seq = pad_sequences(train_q1_seq, maxlen=MAX_LEN)\n# train_q2_seq = pad_sequences(train_q2_seq, maxlen=MAX_LEN)\n# test_q1_seq = pad_sequences(test_q1_seq, maxlen=MAX_LEN)\n# test_q2_seq = pad_sequences(test_q2_seq, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_glove(word_index):\n    EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n    \n#     all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.005838499,0.48782197\n#     embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(WORD_NUM, len(word_index))\n#     embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, 300))\n    for word, i in tqdm_notebook(word_index.items()):\n        if i >= WORD_NUM: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embeddings = load_glove(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuoraDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=60, train=True):\n        self.train = train\n        self.q1 = df['question1'].fillna('something').values\n        self.q2 = df['question2'].fillna('something').values\n        \n        self.q1 = tokenizer.texts_to_sequences(self.q1)\n        self.q2 = tokenizer.texts_to_sequences(self.q2)\n        \n        self.q1 = pad_sequences(self.q1, maxlen=max_len)\n        self.q2 = pad_sequences(self.q2, maxlen=max_len)\n            \n        if train:\n            self.y = df['is_duplicate'].values\n        \n    def __len__(self):\n        return len(self.q1)\n    \n    def __getitem__(self, idx):\n        if self.train:\n            return self.q1[idx], self.q2[idx], self.y[idx]\n        else:\n            return self.q1[idx], self.q2[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, valid_df = train_test_split(train, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = QuoraDataset(train_df, tokenizer, train=True)\nvalid_ds = QuoraDataset(valid_df, tokenizer, train=True)\ntest_ds = QuoraDataset(test, tokenizer, train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_BS = 128\nTEST_BS = 2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size=TRAIN_BS, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=TEST_BS, shuffle=False)\ntest_dl = DataLoader(test_ds, batch_size=TEST_BS, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, hidden_size, max_features=18000, embed_size=300):\n        super(NeuralNet, self).__init__()\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n\n        self.lstm1 = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n        \n        self.linear = nn.Linear(4*hidden_size, 2*16)\n        self.relu = nn.ELU()\n        self.dropout = nn.Dropout(0.1)\n        \n        self.out = nn.Linear(2*16, 1)\n        \n    def forward(self, q1, q2):\n        q1_embedding = self.embedding(q1)\n        q2_embedding = self.embedding(q2)\n        \n        q1_lstm, (h1, _) = self.lstm1(q1_embedding)\n        q2_lstm, (h2, _) = self.lstm2(q2_embedding)\n        \n#         print(q1_lstm.shape)\n#         print(q2_lstm.shape)\n#         print(h1.shape)\n#         print(h2.shape)\n#         print(torch.mean(q1_lstm,dim=1).shape)\n    \n        \n#         avg_pool = torch.mean(h1, 1)\n#         max_pool, _ = torch.max(h2, 1)\n        \n        q1 = torch.cat((h1[0], h1[1]), 1)\n        q2 = torch.cat((h2[0], h2[1]), 1)\n        \n#         q1 = self.linear(q1)\n#         q2 = self.linear(q2)\n        \n        \n        conc = self.relu(self.linear(torch.cat([q1,q2],dim=1)))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_metrics(model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for q1, q2, y in valid_dl:\n        q1 = q1.long().cuda()\n        q2 = q2.long().cuda()\n        y = y.float().cuda().unsqueeze(1)\n        y_hat = model(q1, q2)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        y_pred = y_hat > 0\n        correct += (y_pred.float() == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss/total, correct/total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epocs(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for q1,q2, y in tqdm_notebook(train_dl):\n            q1 = q1.long().cuda()\n            q2 = q2.long().cuda()\n            y = y.float().cuda()\n            y_pred = model(q1, q2)\n            optimizer.zero_grad()\n            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc = val_metrics(model, valid_dl)\n#         if i % 1 == 1:\n        print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = NeuralNet(glove_embeddings,hidden_size=128).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epocs(model, epochs=10, lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next(iter(test_dl))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = np.zeros(len(test_ds))\nfor i, (q1, q2) in enumerate(test_dl):\n    q1 = q1.long().cuda()\n    q2 = q2.long().cuda()\n    y_pred = model(q1, q2).detach()\n    test_preds[i * TEST_BS:(i+1) * TEST_BS] = sigmoid(y_pred.cpu().numpy())[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/quora-question-pairs/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['is_duplicate'] = test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}