{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d0450932-4f99-46cd-9b87-9fe60fdad81c"},"source":"This is my first kernel for this competition and on Kaggle too. Please provide your feedback."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08b7deb9-87ab-de21-f56d-da28d88e113a"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b8ded710-55f7-6425-22af-3d888aacdcf5"},"source":"If you like this notebook then please upvote (button at the top right)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8af9a3e0-fd4c-7155-70ac-cf418d2df726"},"outputs":[],"source":"df_train = pd.read_csv(\"../input/q_quora.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a79958a5-3169-f050-2163-2c5e55d61eef"},"outputs":[],"source":"print  (df_train.iloc[0,4])\nprint (df_train.head())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d72783b-055e-8975-fe63-b7b84144debf"},"outputs":[],"source":"print(df_train.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee07f019-fad3-60fb-1d27-e1330c528492"},"outputs":[],"source":"import numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\npal = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\n\nis_dup = df_train['is_duplicate'].value_counts()\n\nplt.figure(figsize=(8,4))\n\nsns.barplot(is_dup.index, is_dup.values, alpha=0.8, color=pal[1])\n\nplt.ylabel('Number of Occurrences', fontsize=12)\n\nplt.xlabel('Is Duplicate', fontsize=12)\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1fa70e0d-e3ec-36fa-9c02-10f1a4245be7"},"outputs":[],"source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\nfrom wordcloud import WordCloud\ncloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6207940-0aaf-4d3e-7a68-5eddd60c9448"},"outputs":[],"source":"from nltk.corpus import stopwords\nimport re\n\n\n\ndef question_to_words( raw_question ):\n\n    # 2. Remove non-letters        \n    letters_only = re.sub(\"^a-zA-Z\",' ',raw_question)\n    \n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    #\n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2dfec1f6-b7b5-5d8e-eab7-0a49529709f4"},"outputs":[],"source":"print (df_train.iloc[0,4])\nq1=df_train.iloc[0,4]\nprint (\"type of q1 is \", type(q1))\nprint (question_to_words(q1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c43920fe-d2a8-4894-69d6-f49bf148b742"},"outputs":[],"source":"cleanq1s = []\ncleanq2s=[]\nnumqs=df_train.shape[0]\nprint (numqs)\nfor i in range( 0, numqs):\n    # Call our function for each one, and add the result to the list of\n    # clean reviews\n    cleanq1s.append(question_to_words(list(df_train['question1'])[i] ) )\n    cleanq2s.append(question_to_words(list(df_train['question2'])[i] ) )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd08518c-9edb-ca44-243c-48db03c01311"},"outputs":[],"source":"print (\"Creating the bag of words...\\n\")\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n# bag of words tool.  \nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) \n\n# fit_transform() does two functions: First, it fits the model\n# and learns the vocabulary; second, it transforms our training data\n# into feature vectors. The input to fit_transform should be a list of \n# strings.\ntrain_data_features1 = vectorizer.fit_transform(cleanq1s)\ntrain_data_features2 = vectorizer.fit_transform(cleanq2s)\n\n# Numpy arrays are easy to work with, so convert the result to an \n# array\ntrain_data_features1 = train_data_features1.toarray()\ntrain_data_features2 = train_data_features2.toarray()\nprint (train_data_features1.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d77921d4-0140-1100-3a0b-232c04496828"},"outputs":[],"source":"vocab = vectorizer.get_feature_names()\nprint (vocab[0:100])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1279afc4-33e8-d6f7-0e7e-ac6cd973ab43"},"outputs":[],"source":"print (\"Training the random forest...\")\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize a Random Forest classifier with 100 trees\nforest = RandomForestClassifier(n_estimators = 100) \n\n# Fit the forest to the training set, using the bag of words as \n# features and the sentiment labels as the response variable\n#\n# This may take a few minutes to run\ntrain_data_features1\n#forest = forest.fit( train_data_features1-train_data_features2, df_train[\"is_duplicate\"] )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dca74d37-dc1e-195b-49fc-90454ce7c76f"},"outputs":[],"source":"#preds=forest.predict(train_data_features1-train_data_features2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18b2eb9d-8de2-e18a-7358-9a7394011c2d"},"outputs":[],"source":"from sklearn.metrics import roc_auc_score"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45f5d942-fb0c-efac-6aab-c64e7216f9d9"},"outputs":[],"source":"from sklearn.cross_validation import train_test_split\n\n\ny = df_train['is_duplicate']\nX = train_data_features1-train_data_features2\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\nprint (X_train[0:4])\n\nforest = forest.fit( X_train, y_train )\npreds_train=forest.predict(X_train)\npreds_test=forest.predict(X_test)\nfrom sklearn.metrics import roc_auc_score\nprint('Train AUC:', roc_auc_score(y_train, preds_train))\nprint('Test AUC:', roc_auc_score(y_test, preds_test))\n#print (train_data_features1[0:4])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfca5de8-1451-ad9c-e5fa-26ab867f9816"},"outputs":[],"source":"from sklearn.metrics import log_loss\nprint('log loss train:', log_loss(y_train, preds_train))\nprint('log loss test:', log_loss(y_test, preds_test))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eefdb707-adef-ad5b-5ee6-9807fc04dffd"},"outputs":[],"source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntvec = TfidfVectorizer(min_df=.0025, max_df=.1, stop_words=\"english\", ngram_range=(1,2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c67f0bc6-088f-3e11-3491-d8099236f78c"},"outputs":[],"source":"# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n# bag of words tool.  \nvectorizertd = TfidfVectorizer( stop_words=\"english\",max_features=1000)\n\n# fit_transform() does two functions: First, it fits the model\n# and learns the vocabulary; second, it transforms our training data\n# into feature vectors. The input to fit_transform should be a list of \n# strings.\ntrain_data_features1td = vectorizertd.fit_transform(cleanq1s)\ntrain_data_features2td = vectorizertd.fit_transform(cleanq2s)\n\n# Numpy arrays are easy to work with, so convert the result to an \n# array\ntrain_data_features1td = train_data_features1td.toarray()\ntrain_data_features2td = train_data_features2td.toarray()\nprint (train_data_features1td.shape)\nprint (train_data_features2td.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"447ff91a-d73f-fe2d-221e-9d9f2061ec88"},"outputs":[],"source":"print (\"Training the random forest...\")\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize a Random Forest classifier with 100 trees\nforest = RandomForestClassifier(n_estimators = 100) \n\n# Fit the forest to the training set, using the bag of words as \n# features and the sentiment labels as the response variable\n#\n# This may take a few minutes to run\nnp.nonzero(train_data_features1td)\ntrain_data_features1td[4041,895]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7756eedc-694c-750e-6cc3-3111cfbb5eee"},"outputs":[],"source":"#from sklearn.cross_validation import train_test_split\n\ny = df_train['is_duplicate']\nXtd1 = train_data_features1td-train_data_features2td\n\n\nX_traintd,X_testtd,y_traintd,y_testtd = train_test_split(Xtd1,y,test_size=0.3)\nprint (X_train[0:4])\n\nforest = forest.fit( X_traintd, y_traintd )\npreds_traintd=forest.predict(X_traintd)\npreds_testtd=forest.predict(X_testtd)\n#from sklearn.metrics import roc_auc_score\nprint('Train AUC:', roc_auc_score(y_traintd, preds_traintd))\nprint('Test AUC:', roc_auc_score(y_testtd, preds_testtd))\n#print (train_data_features1[0:4])\nprint('log loss train with td_idf:', log_loss(y_traintd, preds_traintd))\nprint('log loss test with td_idf', log_loss(y_testtd, preds_testtd))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"347aedda-b5f8-d0aa-a0a1-ec296d5d5003"},"outputs":[],"source":"import logging\nfrom sklearn.metrics.pairwise import cosine_similarity as cossim\ncos_sim_feat=cossim(train_data_features1td,train_data_features2td)\nX_traincos,X_testcos,y_traincos,y_testcos = train_test_split(cos_sim_feat,y,test_size=0.3)\n#print (X_train[0:4])\n\nforest = forest.fit( X_traincos, y_traincos )\npreds_traincos=forest.predict(X_traincos)\npreds_testcos=forest.predict(X_testcos)\n#from sklearn.metrics import roc_auc_score\nprint('Train AUC:', roc_auc_score(y_traincos, preds_traincos))\nprint('Test AUC:', roc_auc_score(y_testcos, preds_testcos))\n#print (train_data_features1[0:4])\nprint('log loss train with cossine:', log_loss(y_traincos, preds_traincos))\nprint('log loss test with cossine', log_loss(y_testcos, preds_testcos))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b61b7a79-5abf-cd4a-2192-1a1830a6e60d"},"outputs":[],"source":"from gensim.models import word2vec"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee1ade4e-d3d0-d18d-7e9a-29dae078f173","collapsed":true},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}