{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/quora-question-pairs/train.csv.zip')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In question2 , we have 2 null values\n* In question 1, we have 1 null value"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('is_duplicate')['id'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"percentage of 0's and 1's in is_duplicate label"},{"metadata":{"trusted":true},"cell_type":"code","source":"dup_0=df[df['is_duplicate']==0].shape[0]\ndup_1=df.shape[0]-dup_0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Percentage of 0's or not duplicates in dataset: {0:.2f}\".format(dup_0*100/(dup_0+dup_1)))\nprint(\"Percentage of 1's or duplicates in dataset: {0:.2f}\".format(dup_1*100/(dup_0+dup_1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of unique questions in the dataset\nfrom collections import Counter\nq_list1=df['qid1'].tolist()\nq_list2=df['qid2'].tolist()\nq_list1.extend(q_list2)\nc=Counter(q_list1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of unique questions in dataset: \",len(c.values()))\nprint(\"Maximum number of times a question is repeated :  \",max(c.values()))\nprint(\"Number of times questions are repeated in dataset: \",sum(c.values())-len(c.values()))\nt=0\nfor i in c.values():\n    if i>1:\n        t+=1\nprint(\"Number of unique questions that are repeated: \",t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking whether there are any duplicate pairs\ngroup_df_count=df.groupby(['qid1','qid2']).count().shape[0]\nprint(\"Number of duplicate pairs: \",group_df_count-df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.hist(c.values(),bins=160)\nplt.yscale('log',nonposy='clip')\nplt.title('log-histogram of questions occurrences')\nplt.xlabel('number of occurrences of questions')\nplt.ylabel('total number of questions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The above graph shows that there are a lot of questions which are not repeated"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.fillna(' ',inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.isnull().any(1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No more null values are present"},{"metadata":{},"cell_type":"markdown","source":"Let us now construct a few features like:\n - ____freq_qid1____ = Frequency of qid1's\n - ____freq_qid2____ = Frequency of qid2's \n - ____q1len____ = Length of q1\n - ____q2len____ = Length of q2\n - ____q1_n_words____ = Number of words in Question 1\n - ____q2_n_words____ = Number of words in Question 2\n - ____word_Common____ = (Number of common unique words in Question 1 and Question 2)\n - ____word_Total____ =(Total num of words in Question 1 + Total num of words in Question 2)\n - ____word_share____ = (word_common)/(word_Total)\n - ____freq_q1+freq_q2____ = sum total of frequency of qid1 and qid2 \n - ____freq_q1-freq_q2____ = absolute difference of frequency of qid1 and qid2 "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \ndf['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\ndf['q1len'] = df['question1'].str.len() \ndf['q2len'] = df['question2'].str.len()\ndf['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\ndf['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\ndef normalized_word_Common(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)\ndf['word_Common'] = df.apply(normalized_word_Common, axis=1)\ndef normalized_word_Total(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * (len(w1) + len(w2))\ndf['word_Total'] = df.apply(normalized_word_Total, axis=1)\ndef normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\ndf['word_share'] = df.apply(normalized_word_share, axis=1)\ndf['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\ndf['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check out minimum length of question1 and question2 and  number of questions having that length "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"minimum length of question 1: \",df['q1_n_words'].min())\nprint(\"minimum length of question 2: \",df['q2_n_words'].min())\nt1=df['q1_n_words'].min()\nt2=df['q2_n_words'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"number of question 1 with minimum length: \",df[df['q1_n_words']==t1].shape[0])\nprint(\"number of question 2 with minimum length: \",df[df['q2_n_words']==t2].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"markdown","source":"### Feature: word_share "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nsns.violinplot(x='is_duplicate',y='word_share',data=df)\nplt.title('violin plot')\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate']==1]['word_share'],color='red',label='1-duplicate')\nsns.distplot(df[df['is_duplicate']==0]['word_share'],color='blue',label='0-not duplicate')\nplt.title('PDF for word_share')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that as the word_share increases, there are more chances they are similar"},{"metadata":{},"cell_type":"markdown","source":"### Next feature : word_Common"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nsns.violinplot(x='is_duplicate',y='word_Common',data=df)\nplt.title('Violin plot for word_Common')\nplt.subplot(1,2,2)\nplt.title('PDF for word_Common')\nsns.distplot(df[df['is_duplicate']==1]['word_Common'],color='red',label='1-duplicate')\nsns.distplot(df[df['is_duplicate']==0]['word_Common'],color='blue',label='0-not duplicate')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that graphs are highly overlapping, word_Common may not be that useful as compared to word_share"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nplt.title('Violin plot for freq_q1+q2')\nsns.violinplot(x='is_duplicate',y='freq_q1+q2',data=df)\nplt.subplot(1,2,2)\nplt.title('PDF for freq_q1+q2')\nsns.distplot(df[df['is_duplicate']==1]['freq_q1+q2'],color='red',label='1-duplicate')\nsns.distplot(df[df['is_duplicate']==0]['freq_q1+q2'],color='blue',label='0-not duplicate')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nplt.title('Violin plot for freq_q1-q2')\nsns.violinplot(x='is_duplicate',y='freq_q1-q2',data=df)\nplt.subplot(1,2,2)\nplt.title('PDF for freq_q1-q2')\nsns.distplot(df[df['is_duplicate']==1]['freq_q1-q2'],color='red',label='1-duplicate')\nsns.distplot(df[df['is_duplicate']==0]['freq_q1-q2'],color='blue',label='0-not duplicate')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Both sum as well as difference of frequencies graphs are highly overlapping , therefore they may not be useful "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nplt.title('Violin plot for q1_n_words')\nsns.violinplot(x='is_duplicate',y='q1_n_words',data=df)\nplt.subplot(1,2,2)\nplt.title('PDF for q1_n_words')\nsns.distplot(df[df['is_duplicate']==1]['q1_n_words'],color='red',label='1-duplicate')\nsns.distplot(df[df['is_duplicate']==0]['q1_n_words'],color='blue',label='0-not duplicate')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nsns.violinplot(x='is_duplicate',y='q2_n_words',data=df)\nplt.title('Violin plot for q2_n_words')\nplt.subplot(1,2,2)\nplt.title('PDF for q2_n_words')\nsns.distplot(df[df['is_duplicate']==1]['q2_n_words'],color='red',label='1-duplicate')\nsns.distplot(df[df['is_duplicate']==0]['q2_n_words'],color='blue',label='0-not duplicate')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nplt.title('Violin plot for q1len')\nsns.violinplot(x='is_duplicate',y='q1len',data=df)\nplt.subplot(1,2,2)\nplt.title('PDF for q1len')\nsns.distplot(df[df['is_duplicate']==1]['q1len'],color='red',label='1-duplicate')\nsns.distplot(df[df['is_duplicate']==0]['q1len'],color='blue',label='0-not duplicate')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nplt.title('Violin plot for q2len')\nsns.violinplot(x='is_duplicate',y='q2len',data=df)\nplt.subplot(1,2,2)\nplt.title('PDF for q2len')\nsns.distplot(df[df['is_duplicate']==1]['q2len'],color='red',label='1-duplicate')\nsns.distplot(df[df['is_duplicate']==0]['q2len'],color='blue',label='0-not duplicate')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the graphs except word_share are highly overlapping"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Text Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"- Preprocessing:\n    - Removing html tags   -> (removing angular brackets )\n    - Removing Punctuations   -> (comma , full-stop , exclaimation marks etc)\n    - Performing stemming  -> (reducing a word to its base form)\n    - Removing Stopwords  -> (removing words which don't add meaning)\n    - Expanding contractions etc.-> (replace won't with will not , isn't with is not and so on)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To get the results in 4 decemal points\nSAFE_DIV = 0.0001 \n\nSTOP_WORDS = stopwords.words(\"english\")\n\n\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    \n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n               \n    \n    return x\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Definition:\n- __Token__: You get a token by splitting sentence a space\n- __Stop_Word__ : stop words as per NLTK.\n- __Word__ : A token that is not a stop_word\n\n\nFeatures:\n- __cwc_min__ :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 <br>cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n<br>\n<br>\n- __cwc_max__ :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 <br>cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n<br>\n<br>\n- __csc_min__ :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2 <br> csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n<br>\n<br>\n- __csc_max__ :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2<br>csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n<br>\n<br>\n- __ctc_min__ :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2<br>ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n<br>\n<br>\n\n- __ctc_max__ :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2<br>ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n<br>\n<br>\n        \n- __last_word_eq__ :  Check if First word of both questions is equal or not<br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n<br>\n<br>\n\n- __first_word_eq__ :  Check if First word of both questions is equal or not<br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n<br>\n<br>\n        \n- __abs_len_diff__ :  Abs. length difference<br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n<br>\n<br>\n\n- __mean_len__ :  Average Token Length of both Questions<br>mean_len = (len(q1_tokens) + len(q2_tokens))/2\n<br>\n<br>\n\n\n- __fuzz_ratio__ \n<br>\n<br>\n\n- __fuzz_partial_ratio__ \n<br>\n<br>\n\n\n- __token_sort_ratio__ \n<br>\n<br>\n\n\n- __token_set_ratio__ \n<br>\n<br>\n\n\n\n\n\n- __longest_substr_ratio__ :  Ratio of length longest common substring to min length of token count of Q1 and Q2<br>longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\npip install distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_token_features(q1, q2):\n    token_features = [0.0]*10\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n    return token_features\n\n# get the Longest Common sub string\n\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n\n    print(\"token features...\")\n    \n    # Merging Features with dataset\n    \n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n   \n    #Computing Fuzzy Features and Merging with Dataset\n    \n\n    print(\"fuzzy features..\")\n\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\npip install fuzzywuzzy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fuzzywuzzy import fuzz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=extract_features(df)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(\"nlp_features_train.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing our data after feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = df.shape[0]\nsns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the token_sort_ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nplt.title('Violin plot for token_sort_ratio')\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nplt.title('PDF for token_sort_ratio')\nsns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nplt.title('Violin plot for fuzz_ratio')\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nplt.title('PDF for fuzz ratio')\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see most of our newly generated features are useful in the classification task.\n### Let's now see , if our data is seperable in n-dimensional space or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('nlp_features_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndfp_subsampled = df[0:10000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = dfp_subsampled['is_duplicate'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=df2, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['question1'] = df['question1'].apply(lambda x: str(x))\ndf['question2'] = df['question2'].apply(lambda x: str(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n# merge texts\nquestions = list(df['question1']) + list(df['question2'])\n\ntfidf = TfidfVectorizer(lowercase=False, )\ntfidf.fit_transform(questions)\n\n# dict key:word and value:tf-idf score\nword2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To convert questions (text) into numeric value , we'll avg TF-IDF Glove Vector.\n### GLOVE is similar to W2V , except It is trained on Wikipedia and therefore, it is stronger in terms of word semantics."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df['is_duplicate']\ndf.drop('is_duplicate',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test = train_test_split(df, y, stratify=y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of data points in train data :\",X_train.shape)\nprint(\"Number of data points in test data :\",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n# merge texts\nquestions = list(df['question1']) + list(df['question2'])\n\ntfidf = TfidfVectorizer(lowercase=False, )\ntfidf.fit_transform(questions)\n\n# dict key:word and value:tf-idf score\nword2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# en_vectors_web_lg, which includes over 1 million unique vectors.\nnlp = spacy.load('en_core_web_sm')\n\nvecs1 = []\n# https://github.com/noamraph/tqdm\n# tqdm is used to print the progress bar\nfor qu1 in list(df['question1']):\n    doc1 = nlp(qu1) \n    # 384 is the number of dimensions of vectors \n    mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n    for word1 in doc1:\n        # word2vec\n        vec1 = word1.vector\n        # fetch df score\n        try:\n            idf = word2tfidf[str(word1)]\n        except:\n            idf = 0\n        # compute final vec\n        mean_vec1 += vec1 * idf\n    mean_vec1 = mean_vec1.mean(axis=0)\n    vecs1.append(mean_vec1)\ndf['q1_feats_m'] = list(vecs1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vecs2 = []\nfor qu2 in list(df['question2']):\n    doc2 = nlp(qu2) \n    mean_vec2 = np.zeros([len(doc2), len(doc2[0].vector)])\n    for word2 in doc2:\n        # word2vec\n        vec2 = word2.vector\n        # fetch df score\n        try:\n            idf = word2tfidf[str(word2)]\n        except:\n            #print word\n            idf = 0\n        # compute final vec\n        mean_vec2 += vec2 * idf\n    mean_vec2 = mean_vec2.mean(axis=0)\n    vecs2.append(mean_vec2)\ndf['q2_feats_m'] = list(vecs2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('final_training.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}