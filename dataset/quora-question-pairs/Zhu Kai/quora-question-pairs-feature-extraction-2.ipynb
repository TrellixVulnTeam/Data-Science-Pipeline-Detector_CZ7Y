{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2 特征提取二\n\n## GloVe相似度\n\nGloVe相似度是本节要提取的唯一一个特征。没有在2.1节中完成此项工作，是因为导入训练好的GloVe模型后，内存逼近Kernel的16GB上限，因此不得不开设一个新的Kernel专门提取GloVe相关的特征。\n\n**GloVe是如何实现的？**\n\n- 根据语料库构建共现矩阵$X$，矩阵中的元素$X_{ij}$代表单词$i$和上下文单词$j$在特定大小的上下文窗口内共同出现的次数。GloVe对上下文窗口内不同距离的单词赋予不同的权重，衰减函数为$decay = 1/d$，距离远的两个单词占总计数的权重更小。\n\n- 构建词向量和共现矩阵的近似关系：\n\n<center>$w_{i}^{T}\\overset{\\sim}w_{j}+b_{i}+\\overset{\\sim}b_{j}=log(X_{ij})$</center>\n\n&ensp;&ensp;&ensp;其中，$w_{i}^{T}$和$\\overset{\\sim}w_{j}$是要最终求解的词向量，而$b_{i}$和$\\overset{\\sim}b_{j}$分别是两个词向量的偏置项。\n\n- 构造损失函数：\n\n<center>$J=\\sum_{i,j=1}^{V}f(X_{ij})(w_{i}^{T}\\overset{\\sim}w_{j}+b_{i}+\\overset{\\sim}b_{j}-log(X_{ij}))^{2}$</center>\n\n&ensp;&ensp;&ensp;上面的损失函数可以理解为由平方损失函数加上一个权重函数$f(X_{ij})$得到，$f(X_{ij})$可取为\n\n$$f(x)=\n\\begin{cases}\n(x/x_{max})^{\\alpha}& x<x_{max}\\\\\n1& otherwise\n\\end{cases}$$\n\n&ensp;&ensp;&ensp;其中，$\\alpha$的取值都是0.75，而$x_{max}$取值都是100。这样一来，越经常一起出现的单词权重越大，而当到达一定程度之后又不再增加，并且对于从来没有一起出现过的单词，也就是$X_{ij}=0$时，有$f(X_{if})$也为0。\n\n- GloVe的学习同样基于梯度下降办法\n\n**为什么不适用Word2Vec？**\n\n使用Word2Vec有两种办法，一种是自己对训练集建立语料库，然后使用Gensim的Word2Vec函数实现，缺点是语料库太小，参数设定也缺乏指导，另一种办法是下载Google已经训练好的模型，但奈何相关页面最近两天好像崩溃了？于是本文转向了GloVe。\n\nKaggle的DataSet中刚好有本来需要在GloVe官网上下载的文本文件，导入Kernel后存放在embeddings_index中，它包含了绝大多数单词到他们的词向量的映射。"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"../input/glove840b300dtxt/glove.840B.300d.txt\", encoding=\"utf-8\")\nembeddings_index = {}\nfor line in f:\n    values = line.split()\n    word = \"\".join(values[:-300])   \n    coefs = np.asarray(values[-300:], dtype=\"float32\")\n    embeddings_index[word] = coefs\nf.close()\nprint(\"Found {} word vectors of glove.\".format(len(embeddings_index)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 训练集"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_orig = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/train_orig.csv\")\ntrain_stop = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/train_stop.csv\")\n\ntrain_orig.fillna(\"\", inplace = True)\ntrain_stop.fillna(\"\", inplace = True)\n\ntrain = pd.read_csv(\"../input/quora-question-pairs-feature-extraction-1/train.csv\")\ntrainlabel = pd.read_csv(\"../input/quora-question-pairs-feature-extraction-1/trainlabel.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"定义函数get_word_vector得到每个问题包含的所有词向量。测试集或数据集里可能有一些非常罕见的单词，即便GloVe的训练数据量已经达到了60B级别也未能覆盖。对于这种特殊情况，判断两个问题是否都包含了这个词，如果是则有可能含有较多信息，用varity记录下来，否则忽略。"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_word_vector(row):\n    wordlist1 = word_tokenize(row[\"question1\"])\n    wordlist2 = word_tokenize(row[\"question2\"])\n    \n    rarity = 0  # 用于标记问题对是否含有非常罕见的词的特征\n    \n    embeddings_list1 = []\n    for string in wordlist1:\n        try:\n            embeddings_list1.append(embeddings_index[string])\n        except KeyError:\n            if string in wordlist2:  # 如果两个问题包含这个词，令rarity=1，否则不进行处理\n                rarity = 1\n            else:\n                pass          \n    \n    embeddings_list2 = []\n    for string in wordlist2:\n        try:\n            embeddings_list2.append(embeddings_index[string])\n        except KeyError:\n            if string in wordlist1:\n                rarity = 1\n            else:\n                pass  \n    \n    return pd.Series([embeddings_list1, embeddings_list2, rarity])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"对去除停用词前和去除停用词后两个训练集都计算GloVe的相关特征，首先处理train_orig。"},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_orig = train_orig.apply(get_word_vector, axis = 1)\nvector_orig.columns = [\"question1\", \"question2\", \"rarity\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**通过词向量的平均值来衡量句子的语义，构建1-范数和2-范数以及夹角余弦值三个特征。**定义函数diff_word_vector完成计算。"},{"metadata":{"trusted":true},"cell_type":"code","source":"def diff_word_vector(row):\n    mean1 = np.mean(np.array(row[\"question1\"]), axis = 0)\n    mean2 = np.mean(np.array(row[\"question2\"]), axis = 0)\n    diff = mean1 - mean2\n    L1 = np.sum(np.abs(diff))  # np.linalg.norm不能对空的数组计算1-范数和2-范数，所以只能手动计算\n    L2 = np.sum(diff ** 2) ** 0.5\n    norm1 = np.sum(mean1 ** 2) ** 0.5\n    norm2 = np.sum(mean2 ** 2) ** 0.5\n    cos = np.sum(mean1 * mean2) / (norm1 * norm2)\n    return pd.Series([L1, L2, cos])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_vector_orig = vector_orig.apply(diff_word_vector, axis = 1)\nfeatures_vector_orig = pd.concat([diff_vector_orig, vector_orig[\"rarity\"]], axis = 1)\nfeatures_vector_orig.columns = [\"diff_word_vector_L1_orig\", \"diff_word_vector_L2_orig\", \n                                \"word_vector_cos_orig\", \"varity\"]\ntrain = pd.concat([train, features_vector_orig], axis = 1)\n\ndel vector_orig, diff_vector_orig, features_vector_orig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"以同样的方法对去除停用词后的训练集计算相关特征。"},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_stop = train_stop.apply(get_word_vector, axis = 1)\nvector_stop.columns = [\"question1\", \"question2\", \"rarity\"]\n\ndiff_vector_stop = vector_stop.apply(diff_word_vector, axis = 1)\ndiff_vector_stop.columns = [\"diff_word_vector_L1_stop\", \"diff_word_vector_L2_stop\", \n                            \"word_vector_cos_stop\"]\ntrain = pd.concat([train, diff_vector_stop], axis = 1)\n\ndel vector_stop, diff_vector_stop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv(\"train.csv\", index = False)\ntrainlabel.to_csv(\"trainlabel.csv\", index = False)\n\ndel train, trainlabel, train_orig, train_stop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 测试集"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_orig = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/test_orig.csv\")\ntest_orig.fillna(\"\", inplace = True)\n\nvector_orig = test_orig.apply(get_word_vector, axis = 1)\nvector_orig.columns = [\"question1\", \"question2\", \"rarity\"]\ndel test_orig\n\ndiff_vector_orig = vector_orig.apply(diff_word_vector, axis = 1)\nfeatures_vector_orig = pd.concat([diff_vector_orig, vector_orig[\"rarity\"]], axis = 1)\nfeatures_vector_orig.columns = [\"diff_word_vector_L1_orig\", \"diff_word_vector_L2_orig\", \n                                \"word_vector_cos_orig\", \"varity\"]\ndel vector_orig, diff_vector_orig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stop = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/test_stop.csv\")\ntest_stop.fillna(\"\", inplace = True)\n\nvector_stop = test_stop.apply(get_word_vector, axis = 1)\nvector_stop.columns = [\"question1\", \"question2\", \"rarity\"]\ndel test_stop, embeddings_index\n\ndiff_vector_stop = vector_stop.apply(diff_word_vector, axis = 1)\ndiff_vector_stop.columns = [\"diff_word_vector_L1_stop\", \"diff_word_vector_L2_stop\", \n                            \"word_vector_cos_stop\"]\ndel vector_stop\n\ntest = pd.read_csv(\"../input/quora-question-pairs-feature-extraction-1/test.csv\")\ntest = pd.concat([test, features_vector_orig, diff_vector_stop], axis = 1)\ndel features_vector_orig, diff_vector_stop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.to_csv(\"test.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"第三部分模型训练见[Quora Question Pairs: XGBoost](https://www.kaggle.com/benjaminkz/quora-question-pairs-xgboost)。"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}