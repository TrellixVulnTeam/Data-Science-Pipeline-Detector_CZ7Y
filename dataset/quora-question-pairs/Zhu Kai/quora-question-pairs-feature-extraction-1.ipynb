{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import defaultdict\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.1 特征提取一\n\n特征提取分成两个部分进行，这样做是因为导入训练好的GloVe后内存严重紧张。第一部分提取了9大类，总共30个零散的特征，费时约3h。第二部分提取GloVe相关的特征。\n\n- 字母总数差异/字母总数差异比例\n- 单词总数差异/单词总数差异比例\n- 共享单词个数/共享单词比例\n- Jaccard相似度\n- TF-IDF相似度\n- 问题对的路径数量\n- 问题出现频数\n- 第一个单词的差异\n- 情感相似度"},{"metadata":{},"cell_type":"markdown","source":"第一步导入数据清洗所得到的训练集。"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_orig = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/train_orig.csv\")\ntest_orig = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/test_orig.csv\")\ntrain_stop = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/train_stop.csv\")\ntest_stop = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/test_stop.csv\")\ntrain_stem = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/train_stem.csv\")\ntest_stem = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/test_stem.csv\")\ntrain_lem = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/train_lem.csv\")\ntest_lem = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/test_lem.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"实测训练集和测试集部分样本的question1和question2特征变为只含一个空格的字符串，输出再读取会被视为NaN，因此有必要重新用空字符串填充。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_orig = train_orig.fillna(\"\")\ntest_orig = test_orig.fillna(\"\")\ntrain_stop = train_stop.fillna(\"\")\ntest_stop = test_stop.fillna(\"\")\ntrain_stem = train_stem.fillna(\"\")\ntest_stem = test_stem.fillna(\"\")\ntrain_lem = train_lem.fillna(\"\")\ntest_lem = test_lem.fillna(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"所有生成的特征存放在train和test中。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.DataFrame(index = train_orig.index)\ntest = pd.DataFrame(index = test_orig.index)\ntrainlabel = train_orig[[\"is_duplicate\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 字母总数差异与差异比例"},{"metadata":{"trusted":true},"cell_type":"code","source":"len1 = train_orig[\"question1\"].apply(len)\nlen2 = train_orig[\"question2\"].apply(len)\ntrain[\"diff_char\"] = abs(len1 - len2)\ntrain[\"diff_char_rate\"] = 2 * abs(len1 - len2) / (len1 + len2)\n\nlen3 = test_orig[\"question1\"].apply(len)\nlen4 = test_orig[\"question2\"].apply(len)\ntest[\"diff_char\"] = abs(len3 - len4)\ntest[\"diff_char_rate\"] = 2 * abs(len3 - len4) / (len3 + len4)\n\ndel len1, len2, len3, len4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 单词总数差异与差异比例"},{"metadata":{"trusted":true},"cell_type":"code","source":"def words_count(text):\n    wordlist = word_tokenize(text)\n    count = len(wordlist)\n    return count\n\ncount1 = train_orig[\"question1\"].apply(words_count)\ncount2 = train_orig[\"question2\"].apply(words_count)\ntrain[\"diff_words\"] = abs(count1 - count2)\ntrain[\"diff_words_rate\"] = 2 * abs(count1 - count2) / (count1 + count2)\n\ncount3 = test_orig[\"question1\"].apply(words_count)\ncount4 = test_orig[\"question2\"].apply(words_count)\ntest[\"diff_words\"] = abs(count3 - count4)\ntest[\"diff_words_rate\"] = 2 * abs(count3 - count4) / (count3 + count4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 共享单词个数/比例与Jaccard相似度\n\n由于本文认为这三个特征较为重要，因此本文对四种训练集都进行了计算。"},{"metadata":{"trusted":true},"cell_type":"code","source":"def shared_words_count(text1, text2):\n    wordlist1 = word_tokenize(text1)\n    wordlist2 = word_tokenize(text2)\n    wordset1 = set(wordlist1)\n    wordset2 = set(wordlist2)\n    inter = wordset1 & wordset2\n    union = wordset1 | wordset2\n    count = len(inter)\n    rate = 2 * count / (len(wordset1) + len(wordset2) + 1)  # 为了防止wordset1和wordset2同时为空，也即text1和text2都是只包含一个空格的字符串\n    jaccard = count / (len(union) + 1)\n    return pd.Series([count, rate, jaccard])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"share_train_orig = train_orig[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_train_stop = train_stop[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_train_stem = train_stem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_train_lem = train_lem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_words_train = pd.concat([share_train_orig, share_train_stop, share_train_stem, share_train_lem], axis = 1)\nshare_words_train.columns = [\"share_words_count_orig\", \"share_words_rate_orig\", \"jaccard_orig\", \n                             \"share_words_count_stop\", \"share_words_rate_stop\", \"jaccard_stop\", \n                             \"share_words_count_stem\", \"share_words_rate_stem\", \"jaccard_stem\",\n                             \"share_words_count_lem\", \"share_words_rate_lem\", \"jaccard_lem\"]\ntrain = pd.concat([train, share_words_train], axis = 1)\n\nshare_test_orig = test_orig[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_test_stop = test_stop[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_test_stem = test_stem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_test_lem = test_lem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_words_test = pd.concat([share_test_orig, share_test_stop, share_test_stem, share_test_lem], axis = 1)\nshare_words_test.columns = [\"share_words_count_orig\", \"share_words_rate_orig\", \"jaccard_orig\", \n                            \"share_words_count_stop\", \"share_words_rate_stop\", \"jaccard_stop\", \n                            \"share_words_count_stem\", \"share_words_rate_stem\", \"jaccard_stem\",\n                            \"share_words_count_lem\", \"share_words_rate_lem\", \"jaccard_lem\"]\ntest = pd.concat([test, share_words_test], axis = 1)\n\ndel share_train_orig, share_train_stop, share_train_stem, share_train_lem, share_words_train\ndel share_test_orig, share_test_stop, share_test_stem, share_test_lem, share_words_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF相似度\n\n本文用问题的TF-IDF向量的差的1-范数和2-范数，以及夹角余弦值衡量问题对的TF-IDF相似度。\n\n若仅统计训练集上单词的词频，那么当测试集中出现训练集上从未出现过的单词时，TF-IDF的计算便会出现问题。Scikit-Learn中的tfidf.transform函数会把其频数从0调整至1，换言之，认为该词至少出现了一次，然后正常展开计算。又因为测试集中的问题包含机器生成的干扰项，会使词频的估计产生偏差，因此统计词频时不应把测试集中的问题也包含在内。"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordbag = pd.concat([train_orig[\"question1\"], train_orig[\"question2\"]], axis = 0)\ntfidf = TfidfVectorizer(analyzer = \"word\", stop_words = \"english\", lowercase = True)\ntfidf.fit(wordbag)\n\ndel wordbag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_q1_train = tfidf.transform(train_orig[\"question1\"])\ntfidf_q2_train = tfidf.transform(train_orig[\"question2\"])\n\ndiff = tfidf_q1_train - tfidf_q2_train\ndiff_tfidf_L1_train = np.sum(np.abs(diff), axis = 1)  # 统一用numpy的函数比较好\ndiff_tfidf_L2_train = np.sum(diff.multiply(diff), axis = 1)\ndiff_tfidf_L1_norm_train = 2 * np.array(np.sum(np.abs(diff), axis = 1)) / pd.DataFrame(count1 + count2).values\ndiff_tfidf_L2_norm_train = 2 * np.array(np.sum(diff.multiply(diff), axis = 1)) / pd.DataFrame(count1 + count2).values\n# tfidf_q1_train和tfidf_q2_train，以及diff都是稀疏矩阵\n# 转换成数组再做对应元素的运算将会报错，可以用matrix对象自带的方法multiply实现\ncos_tfidf_train = np.sum(tfidf_q1_train.multiply(tfidf_q2_train), axis = 1)  # 由于词的tfidf表示是经过标准化的，所以内积即为夹角余弦值\n\ntrain[\"diff_tfidf_L1\"] = diff_tfidf_L1_train\ntrain[\"diff_tfidf_L2\"] = diff_tfidf_L2_train\ntrain[\"diff_tfidf_L1_norm\"] = diff_tfidf_L1_norm_train\ntrain[\"diff_tfidf_L2_norm\"] = diff_tfidf_L2_norm_train\ntrain[\"cos_tfidf\"] = cos_tfidf_train\n\ndel tfidf_q1_train, tfidf_q2_train, diff, diff_tfidf_L1_train, diff_tfidf_L2_train\ndel diff_tfidf_L1_norm_train, diff_tfidf_L2_norm_train, cos_tfidf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_q1_test = tfidf.transform(test_orig[\"question1\"])\ntfidf_q2_test = tfidf.transform(test_orig[\"question2\"])\n\ndiff = tfidf_q1_test - tfidf_q2_test\ndiff_tfidf_L1_test = np.sum(np.abs(diff), axis = 1)\ndiff_tfidf_L2_test = np.sum(diff.multiply(diff), axis = 1)\ndiff_tfidf_L1_norm_test = 2 * np.array(np.sum(np.abs(diff), axis = 1)) / pd.DataFrame(count3 + count4).values\ndiff_tfidf_L2_norm_test = 2 * np.array(np.sum(diff.multiply(diff), axis = 1)) / pd.DataFrame(count3 + count4).values\ncos_tfidf_test = np.sum(tfidf_q1_test.multiply(tfidf_q2_test), axis = 1)\n\ntest[\"diff_tfidf_L1\"] = diff_tfidf_L1_test\ntest[\"diff_tfidf_L2\"] = diff_tfidf_L2_test\ntest[\"diff_tfidf_L1_norm\"] = diff_tfidf_L1_norm_test\ntest[\"diff_tfidf_L2_norm\"] = diff_tfidf_L2_norm_test\ntest[\"cos_tfidf\"] = cos_tfidf_test\n\ndel tfidf_q1_test, tfidf_q2_test, diff, diff_tfidf_L1_test, diff_tfidf_L2_test\ndel diff_tfidf_L1_norm_test, diff_tfidf_L2_norm_test, cos_tfidf_test\ndel count1, count2, count3, count4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 问题对的路径数量\n\n利用图的观点分析问题之间的联系。将每一个问题视为一个结点，若两个问题出现在同一个样本中，则为相应结点添加一条边，如是可以得到一张无向图。如果问题对的同时出现不是偶然造成的，那么问题对之间的连通路径越多，则联系越紧密，统计每个问题对的路径数量可以得到一个新的特征。囿于算法复杂度限制，下面的代码仅统计了通过一个结点的路径数量。"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = pd.read_csv(\"../input/quora-question-pairs/train.csv\")\nte = pd.read_csv(\"../input/quora-question-pairs/test.csv\")\n\nques = pd.concat([tr[[\"question1\", \"question2\"]], te[[\"question1\", \"question2\"]]], \n                 axis = 0).reset_index(drop = \"index\")\nq_dict = defaultdict(set)\nfor i in range(ques.shape[0]):\n        q_dict[ques.question1[i]].add(ques.question2[i])\n        q_dict[ques.question2[i]].add(ques.question1[i])\n\ndef q1_q2_intersect(row):\n    return len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']])))\n\ntrain[\"q1_q2_intersect\"] = tr.apply(q1_q2_intersect, axis=1, raw=True)\ntest[\"q1_q2_intersect\"] = te.apply(q1_q2_intersect, axis=1, raw=True)\n\ndel ques","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 问题出现频数\n\n被提问次数越多的问题，则越有可能是重复的问题。"},{"metadata":{"trusted":true},"cell_type":"code","source":"def q1_freq(row):\n    return(len(q_dict[row[\"question1\"]]))\ndef q2_freq(row):\n    return(len(q_dict[row[\"question2\"]]))\n\ntrain[\"q1_freq\"] = tr.apply(q1_freq, axis=1, raw=True)\ntrain[\"q2_freq\"] = tr.apply(q2_freq, axis=1, raw=True)\ntrain[\"q1_q2_freq_average\"] = (train[\"q1_freq\"] + train[\"q2_freq\"]) / 2\n\ntest[\"q1_freq\"] = te.apply(q1_freq, axis=1, raw=True)\ntest[\"q2_freq\"] = te.apply(q2_freq, axis=1, raw=True)\ntest[\"q1_q2_freq_average\"] = (test[\"q1_freq\"] + test[\"q2_freq\"]) / 2\n\ndel tr, te, q_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 第一个单词的差异\n\n问题的第一个单词通常都是疑问副词，如what,when,how,which，他们都有明显不同的含义。"},{"metadata":{"trusted":true},"cell_type":"code","source":"def same_start_word(row):\n    wordlist1 = word_tokenize(row[\"question1\"])\n    wordlist2 = word_tokenize(row[\"question2\"])\n    if wordlist1 and wordlist2:  # 为了防止question1或question2是只包含分隔符的空问题\n        return int(wordlist1[0] == wordlist2[0])\n    else:\n        return 0\n\ntrain[\"same_start_word\"] = train_orig.apply(same_start_word, axis = 1)\ntest[\"same_start_word\"] = test_orig.apply(same_start_word, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 情感相似度\n\n情感也是分析语义的重要一个维度。NLTK中有函数SentimentIntensityAnalyzer可以对字符串的情感做简要的评分，其中neg代表负面感情，neu代表中性，pos代表正面，compound代表情感复杂度。对问题对的情感评分作差，可以得到新的特征。"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_analyzer = SentimentIntensityAnalyzer()\ndef sentiment_analyze(row):\n    sen1 = sentiment_analyzer.polarity_scores(row[\"question1\"])\n    sen2 = sentiment_analyzer.polarity_scores(row[\"question2\"])\n    diff_neg = np.abs(sen1[\"neg\"] - sen2[\"neg\"])\n    diff_neu = np.abs(sen1[\"neu\"] - sen2[\"neu\"])\n    diff_pos = np.abs(sen1[\"pos\"] - sen2[\"pos\"])\n    diff_com = np.abs(sen1[\"compound\"] - sen2[\"compound\"])\n    return pd.Series([diff_neg, diff_neu, diff_pos, diff_com])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sen_train = train_orig.apply(sentiment_analyze, axis = 1)\nsen_train.columns = [\"diff_sen_neg\", \"diff_sen_neu\", \"diff_sen_pos\", \"diff_sen_com\"]\ntrain = pd.concat([train, sen_train], axis = 1)\n\nsen_test = test_orig.apply(sentiment_analyze, axis = 1)\nsen_test.columns = [\"diff_sen_neg\", \"diff_sen_neu\", \"diff_sen_pos\", \"diff_sen_com\"]\ntest = pd.concat([test, sen_test], axis = 1)\n\ndel sen_train, sen_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv(\"train.csv\", index = False)\ntest.to_csv(\"test.csv\", index = False)\ntrainlabel.to_csv(\"trainlabel.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"第二部分的特征提取详见[Quora Question Pairs: Feature Extraction 2](https://www.kaggle.com/benjaminkz/quora-question-pairs-feature-extraction-2)。"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}