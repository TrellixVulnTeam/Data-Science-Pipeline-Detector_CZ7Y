{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"552dc3e7-9937-d5e8-1d43-8375b57ce090"},"outputs":[],"source":"import os\nimport re\nimport csv\nimport codecs\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\n\nfrom gensim.models import KeyedVectors\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport sys"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"343171de-08a3-1bf2-875c-597bf0e57c8e"},"outputs":[],"source":"########################################\n## set directories and parameters\n########################################\nBASE_DIR = '../input/'\nEMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\nTRAIN_DATA_FILE = BASE_DIR + 'train.csv'\nTEST_DATA_FILE = BASE_DIR + 'test.csv'\nMAX_SEQUENCE_LENGTH = 30\nMAX_NB_WORDS = 200000\nEMBEDDING_DIM = 300\nVALIDATION_SPLIT = 0.1\n\nnum_lstm = np.random.randint(175, 275)\nnum_dense = np.random.randint(100, 150)\nrate_drop_lstm = 0.15 + np.random.rand() * 0.25\nrate_drop_dense = 0.15 + np.random.rand() * 0.25\n\nact = 'relu'\nre_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n\nSTAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n        rate_drop_dense)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49311fec-cf40-477e-b589-0907161c0e9c"},"outputs":[],"source":"########################################\n## index word vectors\n########################################\nprint('Indexing word vectors')\n\nword2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n        binary=True)\nprint('Found %s word vectors of word2vec' % len(word2vec.vocab))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0369355b-40a5-455b-05b4-3f7db6cfcc66"},"outputs":[],"source":"\n########################################\n## process texts in datasets\n########################################\nprint('Processing text dataset')\n\n# The function \"text_to_wordlist\" is from\n# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\ndef text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case and split them\n    text = text.lower().split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)\n\ntexts_1 = [] \ntexts_2 = []\nlabels = []\nwith codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    header = next(reader)\n    for values in reader:\n        texts_1.append(text_to_wordlist(values[3]))\n        texts_2.append(text_to_wordlist(values[4]))\n        labels.append(int(values[5]))\nprint('Found %s texts in train.csv' % len(texts_1))\n\ntest_texts_1 = []\ntest_texts_2 = []\ntest_ids = []\nwith codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    header = next(reader)\n    for values in reader:\n        test_texts_1.append(text_to_wordlist(values[1]))\n        test_texts_2.append(text_to_wordlist(values[2]))\n        test_ids.append(values[0])\nprint('Found %s texts in test.csv' % len(test_texts_1))\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n\nsequences_1 = tokenizer.texts_to_sequences(texts_1)\nsequences_2 = tokenizer.texts_to_sequences(texts_2)\ntest_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\ntest_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens' % len(word_index))\n\ndata_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\ndata_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = np.array(labels)\nprint('Shape of data tensor:', data_1.shape)\nprint('Shape of label tensor:', labels.shape)\n\ntest_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\ntest_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\ntest_ids = np.array(test_ids)\n\n########################################\n## prepare embeddings\n########################################\nprint('Preparing embedding matrix')\n\nnb_words = min(MAX_NB_WORDS, len(word_index))+1\n\nembedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if word in word2vec.vocab:\n        embedding_matrix[i] = word2vec.word_vec(word)\nprint('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n\n########################################\n## sample train/validation data\n########################################\n#np.random.seed(1234)\nperm = np.random.permutation(len(data_1))\nidx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\nidx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n\ndata_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\ndata_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\nlabels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n\ndata_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\ndata_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\nlabels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n\nweight_val = np.ones(len(labels_val))\nif re_weight:\n    weight_val *= 0.472001959\n    weight_val[labels_val==0] = 1.309028344\n\n########################################\n## define the model structure\n########################################\nembedding_layer = Embedding(nb_words,\n        EMBEDDING_DIM,\n        weights=[embedding_matrix],\n        input_length=MAX_SEQUENCE_LENGTH,\n        trainable=False)\nlstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n\nsequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences_1 = embedding_layer(sequence_1_input)\nx1 = lstm_layer(embedded_sequences_1)\n\nsequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences_2 = embedding_layer(sequence_2_input)\ny1 = lstm_layer(embedded_sequences_2)\n\nmerged = concatenate([x1, y1])\nmerged = Dropout(rate_drop_dense)(merged)\nmerged = BatchNormalization()(merged)\n\nmerged = Dense(num_dense, activation=act)(merged)\nmerged = Dropout(rate_drop_dense)(merged)\nmerged = BatchNormalization()(merged)\n\npreds = Dense(1, activation='sigmoid')(merged)\n\n########################################\n## add class weight\n########################################\nif re_weight:\n    class_weight = {0: 1.309028344, 1: 0.472001959}\nelse:\n    class_weight = None\n\n########################################\n## train the model\n########################################\nmodel = Model(inputs=[sequence_1_input, sequence_2_input], \\\n        outputs=preds)\nmodel.compile(loss='binary_crossentropy',\n        optimizer='nadam',\n        metrics=['acc'])\n#model.summary()\nprint(STAMP)\n\nearly_stopping =EarlyStopping(monitor='val_loss', patience=3)\nbst_model_path = STAMP + '.h5'\nmodel_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n\nhist = model.fit([data_1_train, data_2_train], labels_train, \\\n        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n        epochs=200, batch_size=2048, shuffle=True, \\\n        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n\nmodel.load_weights(bst_model_path)\nbst_val_score = min(hist.history['val_loss'])\n\n########################################\n## make the submission\n########################################\nprint('Start making the submission before fine-tuning')\n\npreds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\npreds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\npreds /= 2\n\nsubmission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\nsubmission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}