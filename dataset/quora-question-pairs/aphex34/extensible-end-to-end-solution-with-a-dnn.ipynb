{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"187c47ed-f68b-7abe-05d8-1296a816ecca"},"source":"This kernel presents an extensible approach to handling the Quora Question Pairs competition with a deep neural network. Using the final model's predictions should yield ~0.35 on the public leaderboard."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7838992e-246a-8dd9-d7cf-ee5f3f04ab6a"},"outputs":[],"source":"import re\nimport nltk\nimport random\nimport gensim\nimport pickle\nimport logging\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Model\nfrom keras.layers import Embedding\nfrom keras.layers import Lambda\nfrom keras.layers.merge import concatenate\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.layers import Convolution1D, GlobalMaxPooling1D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import ModelCheckpoint\n\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"},{"cell_type":"markdown","metadata":{"_cell_guid":"efc61246-e00c-32b6-f021-fdc6d78b02b6"},"source":"### Define some useful functions we need to represent text"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5518a5ba-5790-d088-d41e-8df28e533c3a"},"outputs":[],"source":"stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n \ndef preprocess(text, min_length=2, swords=set()):\n    \"\"\"\n    Does preprocessing on an input string by lowering it, tokenizing, filtering out stopwords,\n    tokens shorter than min_length and tokens consisting of not English letters.\n    \"\"\"\n    text = str(text).lower()\n    words = map(lambda word: word.lower(), nltk.word_tokenize(text))\n    words = [word for word in words if word not in swords]\n    p = re.compile('[a-zA-Z]+');\n    filtered_tokens = list(filter(lambda token: p.match(token) and len(token)>=min_length, words))\n    return filtered_tokens\n\ndef build_vocab(tokenlists, max_size=20000, emb_model=None):\n    \"\"\"\n    Builds a vocabulary of at most max_size words from the supplied list of lists of tokens.\n    If a word embedding model is provided, adds only the words present in the model vocabulary.\n    \"\"\"\n\n    all_words = list(itertools.chain.from_iterable(tokenlists))\n    counter = Counter(all_words)\n    if emb_model:\n        counter = Counter(x for x in counter if x in emb_model)\n            \n    vocab = counter.most_common(max_size-2)\n\n    voc_words = [k[0] for k in vocab]\n\n    voc = {}\n    voc['NULL'] = 0\n    voc['UNKN'] = 1\n    for i, k in enumerate(voc_words):\n        voc[k] = i+2\n\n    rvoc = {v: k for k, v in voc.items()}\n\n    return voc, rvoc\n\ndef vectorize_tokens(tokens, token_to_id, max_len):\n    \"\"\"\n    Converts a list of tokens to a list of token ids using the supplied dictionary.\n    Pads resulting list with NULL identifiers up to max_len length.\n    \"\"\"\n    ids = []\n    for token in tokens:\n        ids.append(token_to_id.get(token, voc[\"UNKN\"]))\n\n    ids = ids[:max_len]\n    if len(ids) < max_len:\n        ids += (max_len-len(ids))*[token_to_id[\"NULL\"]]\n\n    return ids\n\ndef vectorize(tok_lists, token_to_id, max_len=150):\n    \"\"\"\n    Converts a list of lists of tokens to a numpy array of token identifiers\n    \"\"\"\n    \n    token_matrix = []\n        \n    for tok_list in tok_lists:\n        token_ids = vectorize_tokens(tok_list, token_to_id, max_len)\n        token_matrix.append(token_ids)\n    \n    token_matrix = np.array(token_matrix)\n        \n    return token_matrix\n\ndef get_embeddings(model, rev_voc, dim=300):\n\n    myembeddings = []\n    for key in sorted(rev_voc.keys()):\n        val = rev_voc[key]\n        if val == 'NULL':\n            myembeddings.append(np.zeros((dim,)))\n        elif val == 'UNKN':\n            myembeddings.append(np.random.normal(size=(dim,)))\n        else:\n            try:\n                myembeddings.append(model[val])\n            except KeyError:\n                print(\"OOV: {}\".format(val))\n                myembeddings.append(np.random.normal(size=(dim,)))\n\n    myembeddings = np.array(myembeddings)\n    return myembeddings"},{"cell_type":"markdown","metadata":{"_cell_guid":"2144d5a2-4e52-eb98-7f12-af72b392f07f"},"source":"### Load train/test datasets"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcfc81ee-f223-b263-0acc-bfafe5c722fb"},"outputs":[],"source":"training_data = pd.read_csv(\"/kaggle/input/train.csv\")\ntesting_data = pd.read_csv(\"/kaggle/input/test.csv\")\nlabels = np.array(list(training_data['is_duplicate']))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ec573f73-bf0a-d008-7d32-c11b5fcd4105"},"source":"### Preprocess all texts from train/test. \nThis will take a while."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"985635ab-df43-e145-d3eb-b81a9d10f955"},"outputs":[],"source":"tr_q1_preprocessed = [preprocess(t, swords=stopwords) for t in training_data['question1']]\ntr_q2_preprocessed = [preprocess(t, swords=stopwords) for t in training_data['question2']]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25acd727-a70d-98a6-1734-0275916e2894"},"outputs":[],"source":"ts_q1_preprocessed = [preprocess(t, swords=stopwords) for t in testing_data['question1']]\nts_q2_preprocessed = [preprocess(t, swords=stopwords) for t in testing_data['question2']]"},{"cell_type":"markdown","metadata":{"_cell_guid":"c718bdb6-e4a5-4251-ef61-48f8237cb4c2"},"source":"### Load the word embedding model\nYou can get it at https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit (1.5 GB download)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4251ec8e-6811-11e1-591c-f00bb82a07ab"},"outputs":[],"source":"emb_mod = gensim.models.Word2Vec.load_word2vec_format(\"./assets/GoogleNews-vectors-negative300.bin\", \n                                                      binary=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f5f49567-2371-31c9-ce8b-b243f6106544"},"source":"###(Alternative) Learn the word embedding model\nInstead of using an external word-embedding model, we might just learn our own from the provided data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c5458ee-ac88-656c-5823-fd0cecc8cccb"},"outputs":[],"source":"all_texts = tr_q1_preprocessed+tr_q2_preprocessed+ts_q1_preprocessed+ts_q2_preprocessed"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0715840a-d9a4-e7dd-3a0c-ce147924af1f"},"outputs":[],"source":"emb_mod = gensim.models.Word2Vec(all_texts, min_count=7, size=128)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3f42f65-d5fe-9fcb-968e-25c9bc6681e4"},"outputs":[],"source":"# You might want to train the model more to get better results\nn_epochs = 5\nfor i in range(n_epochs)\n    emb_mod.train(all_texts)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a37196c-9892-37a9-d1a5-3d3c5330c794"},"source":"### Build a vocabulary of token identifiers and prepare word embedding matrix\nHere we only add tokens present in the embedding model to the vocabulary"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b1e02c3-c41c-f9d6-9ca5-fc3c069c6a3f"},"outputs":[],"source":"voc, rev_voc = build_vocab(all_texts, \n                           75000, emb_mod)\nembs_m = get_embeddings(emb_mod, rev_voc, emb_mod.vector_size)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ed236db3-a829-2133-b8b0-214801b0d24a"},"source":"### Represent train/test texts with token identifiers\nmax_len of 24 tokens seems to be sufficient"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c649246-5f9b-2605-5f79-f2c28935a1e4"},"outputs":[],"source":"v_tr_q1 = vectorize(tr_q1_preprocessed, voc, max_len=24)\nv_tr_q2 = vectorize(tr_q2_preprocessed, voc, max_len=24)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63cbcb60-dec5-de9c-6d37-c963453ffe02"},"outputs":[],"source":"v_ts_q1 = vectorize(ts_q1_preprocessed, voc, max_len=24)\nv_ts_q2 = vectorize(ts_q2_preprocessed, voc, max_len=24)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d578e2f1-9045-a238-19cc-2f3ee8f7eacf"},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"ed19546f-0255-ecdd-ab7e-38f233906c00"},"source":"### (Optional) Dump assets to disk\nWe might continue from there in case the kernel is reloaded"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"857d476e-9acc-a25d-5e22-a0280b44cd28"},"outputs":[],"source":"pickle.dump([v_tr_q1, v_tr_q2], open(\"vectorized_train\", \"wb\"))\npickle.dump([v_ts_q1, v_ts_q2], open(\"vectorized_test\", \"wb\"))\npickle.dump([voc, rev_voc], open(\"voc_rvoc\", \"wb\"))\npickle.dump(embs_m, open(\"embedding_matrix\", \"wb\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80d2e188-8058-9965-9dee-295284bb6fdc"},"outputs":[],"source":"v_tr_q1, v_tr_q2 = pickle.load(open(\"./assets/vectorized_train\", \"rb\"))\nv_ts_q1, v_ts_q2 = pickle.load(open(\"./assets/vectorized_test\", \"rb\"))\nvoc, rev_voc = pickle.load(open(\"./assets/voc_rvoc\", \"rb\"))\nembs_m = pickle.load(open(\"./assets/embedding_matrix\", \"rb\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2ad667c2-8649-0675-6c78-4d1c8196860c"},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"ca841d75-8f06-be2b-533c-140deefc0fee"},"source":"### Define a neural network"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4dae93a4-b577-28d0-41f9-07fafcd7b1b4"},"outputs":[],"source":"MAXLEN = 24\nDROPOUT = 0.5\nLSTM_UNITS = 600\nDENSE_UNITS = 600"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29331df3-9bf9-4370-b072-37127c0bfb95"},"outputs":[],"source":"def pairwise_dis(vests):\n    x, y = vests\n    return x-y\n\ndef pairwise_mul(vests):\n    x, y = vests\n    return x*y\n\ndef cosine_similarity(vests):\n    x, y = vests\n    x = K.l2_normalize(x, axis=-1)\n    y = K.l2_normalize(y, axis=-1)\n    return K.sum((x * y), axis=-1, keepdims=True)\n\ndef cosine_distance_output_shape(shapes):\n    shape1, shape2 = shapes\n    return shape1[0], 1\n\ndef contrastive_loss(y_true, y_pred):\n    '''Contrastive loss from Hadsell-et-al.'06\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    '''\n    margin = 1\n    return K.mean((1 - y_true) * K.square(y_pred) + y_true * K.square(K.maximum(margin - y_pred, 0)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9f9f3a0-6283-b1b0-0c51-c9c42cf0fb80"},"outputs":[],"source":"def build_rnn_mk1_encoder(embs_matrix):\n    \"\"\"\n    Basic Bidirectional LSTM encoder. \n    Word embedding layer is frozen to prevent overfitting.\n    \"\"\"\n    inp = Input(shape=(MAXLEN,))\n    emb = Embedding(embs_matrix.shape[0], embs_matrix.shape[1], input_length=MAXLEN, \n                    weights=[embs_matrix], trainable = False)(inp)\n    ls1 = Bidirectional(LSTM(LSTM_UNITS))(emb)\n    mod = Model(inputs=inp, outputs=ls1)\n    return mod\n\ndef build_sim_net(input_shape):\n    \"\"\"\n    MLP combining the representations of two question into one vector.\n    Takes into account distanse and angle between the input vectors.\n    For more information check out the blog post\n    https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning\n    \"\"\"\n    input_a = Input(shape=(input_shape[1],))\n    input_b = Input(shape=(input_shape[1],))\n    \n    mul_layer = Lambda(pairwise_mul, name='MultiplicationLayer')([input_a, input_b])\n    dis_layer = Lambda(pairwise_dis, name='SubstractionLayer')([input_a, input_b])\n\n    mer = concatenate([mul_layer, dis_layer])\n    bnr = BatchNormalization()(mer)\n    \n    dr1 = Dropout(DROPOUT)(bnr)\n    fc1 = Dense(DENSE_UNITS, activation='relu')(dr1)\n    \n    mod = Model(inputs=[input_a, input_b], outputs=fc1)\n    return mod\n\ndef build_model(embs_matrix):\n    \"\"\"\n    Combines the modules above into an end-to-end model\n    predicting similarity scores for pairs of questions.\n    \n    Keep in mind that you can plug in just about anything in place of the encoder.\n    As long as it predicts a fixed-length vector for each sentence, it should just work.\n    \"\"\"\n    \n    encoder = build_rnn_mk1_encoder(embs_matrix)\n    simnet = build_sim_net(encoder.layers[-1].output_shape)\n    \n    input_a = Input(shape=(MAXLEN,))\n    input_b = Input(shape=(MAXLEN,))\n    \n    enc_a = encoder(input_a)\n    enc_b = encoder(input_b)\n    \n    fc1 = simnet([enc_a, enc_b])\n    \n    fc2 = Dense(1, activation='sigmoid')(fc1)\n    \n    model = Model(inputs=[input_a, input_b], outputs=fc2)\n    feature_model = Model(inputs=[input_a, input_b], outputs=fc1)\n\n    model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n    \n    return model, feature_model, encoder"},{"cell_type":"markdown","metadata":{"_cell_guid":"9345068c-1555-b61a-8173-da70ab85e571"},"source":"### Build the neural network\nThe first one predicts similarity score, the second returns a feature vector."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6df9bbfb-6ab9-47c5-561a-1a7fda6a20bf"},"outputs":[],"source":"mmod, fmod, encmod = build_model(embs_m)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59bf085d-d4da-8f12-1396-33a9e65ff0a5"},"outputs":[],"source":"encmod.summary()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bfdbefc0-eca0-22bf-5e5e-1fdaa2e4a0f5"},"source":"### Do the train/val split"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d5aaa55-e9aa-c1f8-fedb-9cb49464064e"},"outputs":[],"source":"idx = list(range(len(v_tr_q1)))\nrandom.shuffle(idx)\ntrain_idx, val_idx = train_test_split(idx, train_size=0.9)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f4b34b0-5ee5-60df-9695-a0b6d909b4d4"},"outputs":[],"source":"train_X = [v_tr_q1[train_idx], v_tr_q2[train_idx]]\ntrain_Y = labels[train_idx]\n\nval_X = [v_tr_q1[val_idx], v_tr_q2[val_idx]]\nval_Y = labels[val_idx]"},{"cell_type":"markdown","metadata":{"_cell_guid":"07976505-1361-8085-32ea-7165f639418a"},"source":"## Train!\nTraining this takes ~1 hour on a GTX 1080 with recent CUDA/CUDNN"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49af117d-f74d-8c21-7001-a9adab02d4b7"},"outputs":[],"source":"checkpointer = ModelCheckpoint(filepath=\"quora_bilstm.hdf5\",\n                                       verbose=0, save_best_only=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32c88270-349a-3c1c-021a-e5b7f13c5119"},"outputs":[],"source":"hist = mmod.fit(train_X, train_Y, validation_data=(val_X, val_Y), \n                batch_size=256, epochs=20, \n                callbacks=[checkpointer])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88f706ba-8d4a-517c-ac9d-038420d282ad"},"outputs":[],"source":"mmod.load_weights(\"quora_bilstm.hdf5\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"070581e7-c9d6-de12-77fc-5368866c0c2a"},"source":"### Make a submission"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5908d50-a4c3-1e06-a33c-aa1b4e578f93"},"outputs":[],"source":"predictions = mmod.predict([v_ts_q1, v_ts_q2]).reshape(-1,)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ff6974e-2661-c8cc-8e68-7f08e9f4cfb7"},"outputs":[],"source":"sub = pd.DataFrame({'test_id': testing_data['test_id'], 'is_duplicate': predictions})\nsub.to_csv('sample_submission.csv', index=False)\nsub.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09bfd61a-e5fa-bafa-b5c2-766536f2de2c"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a28ac3c-3669-7518-6b52-fbd396379bbb"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}