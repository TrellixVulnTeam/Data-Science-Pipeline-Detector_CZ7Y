{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi,\n\nI recently started working on this competition and would love comments from everyone who reads and studies NLP. I have been working on some core NLP methods (excluding deep learning) and wanted to apply those and check the performance.\n\nIf you like my implementations, please upvote and/or comment.","metadata":{"_cell_guid":"cc1b687b-f72e-40a3-78c4-46a53be7ebae"}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nfrom collections import Counter\nfrom nltk import word_tokenize\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"48a1d24b-68e9-0de6-5403-3ea1e257d2b2","execution":{"iopub.status.busy":"2022-04-08T03:03:29.341701Z","iopub.execute_input":"2022-04-08T03:03:29.341947Z","iopub.status.idle":"2022-04-08T03:03:29.374099Z","shell.execute_reply.started":"2022-04-08T03:03:29.341918Z","shell.execute_reply":"2022-04-08T03:03:29.373225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, the tfidf basically measures the importance of a particular word in a document/paragraph/piece of text. So, in calculating tfidf, I have taken the questions as the documents and the qlist everywhere would be the list of all the questions (on both sides). The inverse document frequency is calculated over all of the questions being compared, i.e. everything given in the file.\n\nIn the next few cells I have implemented some methods to run calculate term frequency, inverse document frequency and some related functions.","metadata":{"_cell_guid":"82e5da20-c09c-50bf-eeb1-a20d975eaedf"}},{"cell_type":"code","source":"def tf(question, word):\n    if word not in question:\n        return 0\n    count = dict(Counter(question))\n    q_len = len(question)\n    return float(count[word]) / float(q_len)\n","metadata":{"_cell_guid":"2d194d35-2240-5692-bf74-92a6ed50e28e","execution":{"iopub.status.busy":"2022-04-08T03:03:29.377091Z","iopub.execute_input":"2022-04-08T03:03:29.377944Z","iopub.status.idle":"2022-04-08T03:03:29.383743Z","shell.execute_reply.started":"2022-04-08T03:03:29.377905Z","shell.execute_reply":"2022-04-08T03:03:29.383075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The two methods below are parts of implementation to the idf (inverse document frequency). The first function returns the count of sentences/documents/paragraphs where the word is present.\n\nThe second, idf method returns the value of the log.","metadata":{"_cell_guid":"eb20355a-00d7-4ebc-d17a-913c8c738890"}},{"cell_type":"code","source":"def n_containing(qlist, word):\n    return float(qlist[word])\n\ndef idf(qlist, word):\n    return math.log(float(len(qlist.keys())) / (1.0 + n_containing(qlist, word)))","metadata":{"_cell_guid":"e0c5d83c-3bc8-e6c2-8394-538c6636c016","execution":{"iopub.status.busy":"2022-04-08T03:03:29.386946Z","iopub.execute_input":"2022-04-08T03:03:29.387411Z","iopub.status.idle":"2022-04-08T03:03:29.39286Z","shell.execute_reply.started":"2022-04-08T03:03:29.387375Z","shell.execute_reply":"2022-04-08T03:03:29.392135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next function is the tfidf, which, as in the name, returns the tfidf value.","metadata":{"_cell_guid":"07072ddc-dff5-4e7b-7d31-c5bb3cd0050e"}},{"cell_type":"code","source":"def tfidf(question, qlist, word):\n    return tf(question, word) * idf(qlist, word)","metadata":{"_cell_guid":"97bf1c51-498e-01d4-799c-31f4b6e2649a","execution":{"iopub.status.busy":"2022-04-08T03:03:29.395121Z","iopub.execute_input":"2022-04-08T03:03:29.395389Z","iopub.status.idle":"2022-04-08T03:03:29.40094Z","shell.execute_reply.started":"2022-04-08T03:03:29.395353Z","shell.execute_reply":"2022-04-08T03:03:29.399915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tfidf function is used to return a number, of which I would be making a vector, of the two statements (I won't usually give a single word, the above is a utility function I used from an old implementation).\n\nFor the function below, v1 and v2 are two vectors (list of numbers in this case) of the same dimensions. Function returns the cosine distance between those which is the ratio of the dot product of the vectors over their RS.","metadata":{"_cell_guid":"2664b8fa-5fed-932d-8190-543b995dc06e"}},{"cell_type":"code","source":"def cosine(v1, v2):\n    v1 = np.array(v1)\n    v2 = np.array(v2)\n\n    return np.dot(v1, v2) / (np.sqrt(np.sum(v1**2)) * np.sqrt(np.sum(v2**2)))","metadata":{"_cell_guid":"2d62c12d-b9a0-e882-7f27-33838b949820","execution":{"iopub.status.busy":"2022-04-08T03:03:29.403179Z","iopub.execute_input":"2022-04-08T03:03:29.403694Z","iopub.status.idle":"2022-04-08T03:03:29.412195Z","shell.execute_reply.started":"2022-04-08T03:03:29.403667Z","shell.execute_reply":"2022-04-08T03:03:29.411221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, having implemented the necessary functions, let us start with running these over the dataset. We'll run it over the train.csv to check how good it performs. It is still running on my machine, and I trust this kernel very less (has been going down like after every cell I execute), so I might as well just post the output instead of running it on this again (with the code obviously).\n\nThe first is simple enough, gather the data. I am dropping the useless columns in the train data, i.e. I dropped the qids. If you are running the following cell, you need to be warned it takes some time.\n\nThere is also another quirk with this code snippet. I noticed in the questions that some of them have very less number of characters, like even 1 character questions (what were they trying to write '?'), parsing a question with less than 10 characters did not make much sense to me. The question (and its pair) containing less than 10 characters has been left out from the question list, but you can obviously use it if you would like.","metadata":{"_cell_guid":"38352ab5-16f0-9e0e-216f-9e173b47c88b"}},{"cell_type":"code","source":"train = pd.read_csv('../input/quora-question-pairs/train.csv.zip')\n\ntrain_qs = train[['id', 'question1', 'question2', 'is_duplicate']]\n\nqlist = []\ncount = 0\nfor row in train_qs.itertuples():\n    try:\n        if len(str(row[2])) > 10:\n            q1 = word_tokenize(row[2].lower())\n        if len(str(row[3])) > 10:\n            q2 = word_tokenize(row[3].lower())\n        qlist += q1 + q2\n        count+=1\n        if count%100000 == 0:\n            print('At'+str(count))\n#        qlist.append(q2)\n    except TypeError:\n        pass\n\n# print len(qlist)\nqlist = dict(Counter(qlist))\nimport json\nwith open('qlist.json', 'w') as f:\n    f.write(json.dumps(qlist, indent=2))\nprint('All Questions added to list')","metadata":{"_cell_guid":"19c2ffbd-8d0e-7f92-4165-fc279ba249cd","execution":{"iopub.status.busy":"2022-04-08T03:03:29.417888Z","iopub.execute_input":"2022-04-08T03:03:29.418073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, this is the final stage. Now the questions are going to be compared. I do have some results with me (and it is taking some more time to execute on my system, so it might as well get stopped by the kernel by the time it reaches halfway).\n\nSince I know this is gonna take a lot of time, I break the code after matching 100 pairs.","metadata":{"_cell_guid":"4b2acae1-9c6b-bc32-e5fb-92f88ef34df5"}},{"cell_type":"code","source":"with open('submission.csv', 'a') as f:\n    f.write('id,is_duplicate\\n')\nfor row in train_qs.itertuples():\n    if len(str(row[2])) > 10 and len(str(row[3])) > 10:\n        wordvec1 = word_tokenize(row[2].lower())\n        wordvec2 = word_tokenize(row[3].lower())\n        words = wordvec1 + wordvec2\n        words = list(set([word for word in words if word != '?']))\n\n        # print words\n\n        vec1 = []\n        vec2 = []\n        for word in words:\n            vec1.append(tfidf(wordvec1, qlist, word))\n            vec2.append(tfidf(wordvec2, qlist, word))\n\n        with open('submission.csv', 'a') as f:\n            f.write(str(row[1]) + \",\" + str(cosine(vec1, vec2)) + '\\n')\n    else:\n        with open('submission.csv', 'a') as f:\n            f.write(str(row[1]) + \",\" + '0' + '\\n')\n#    print str(row[1]) + \",\" + str(cosine(vec1, vec2))","metadata":{"_cell_guid":"cd418128-c1c0-b05b-29ec-ba69f8bfd48a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I agree this is not the best method to compare two questions, but this seems to set a benchmark. This is the most basic NLP implementation possible to compare two sentences using a tfidf representation of the two sentences and hence might not get a good response. This is also an unsupervised method which is basically ignoring the fact that there is a train.csv file available to \"train\" over.\n\nThank you, I appreciate anyone who reads this on spending some time on this kernel too. I would also like suggestions and methods for improvement. Also, since this is an unsupervised method I ran, could this method somehow be used in a supervised fashion, like use the cosine similarity as a feature on some other learning method.\n\nI am mostly looking for NLP methods to implement for this and would appreciate any help/motivation to better methods around (not using deep learning, for now). All and any help is appreciated.","metadata":{"_cell_guid":"a957a044-811a-ff88-4e0d-d451be56cd68"}}]}