{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Quora Question Pairs with Bert","metadata":{}},{"cell_type":"markdown","source":"# context","metadata":{}},{"cell_type":"markdown","source":"Where else but Quora can a physicist help a chef with a math problem and get cooking tips in return? Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.\n\nOver 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n\nCurrently, Quora uses a Random Forest model to identify duplicate questions. In this competition, Kagglers are challenged to tackle this natural language processing problem by applying advanced techniques to classify whether question pairs are duplicates or not. Doing so will make it easier to find high quality answers to questions resulting in an improved experience for Quora writers, seekers, and readers.","metadata":{}},{"cell_type":"markdown","source":"In short, judging whether the problem is repeated can improve the search efficiency, and can also help the platform save storage space","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-06T02:42:05.631545Z","iopub.execute_input":"2022-05-06T02:42:05.632329Z","iopub.status.idle":"2022-05-06T02:42:05.659666Z","shell.execute_reply.started":"2022-05-06T02:42:05.632225Z","shell.execute_reply":"2022-05-06T02:42:05.65891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\n","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv.zip\")\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The The task is to judge whether question 1 and question 2 are similar problems, and is_duplicate is used as the label","metadata":{}},{"cell_type":"markdown","source":"# Task analysis\n1. Overall this is a text matching task\n2. Need an indicator to measure whether question 1 and question 2 are similar\n3. Unstructured data(text) is stored in structured tables","metadata":{}},{"cell_type":"markdown","source":"# Code\nBecause my English level is poor, the comment part of the code is in Chinese","metadata":{}},{"cell_type":"code","source":"#导入必要工具包，这次我使用pytorch来实现bert模型对数据进行分析\nimport torch\nimport torch.nn as nn\n\n#有了它们才能画出可爱的小图图\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#进度条\nfrom tqdm import tqdm\n\n#用sklearn划分数据集\nfrom sklearn.model_selection import train_test_split\n\n#transformer模型的数据包\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\ntransformers.logging.set_verbosity_error()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:05.661169Z","iopub.execute_input":"2022-05-06T02:42:05.661752Z","iopub.status.idle":"2022-05-06T02:42:12.582621Z","shell.execute_reply.started":"2022-05-06T02:42:05.661709Z","shell.execute_reply":"2022-05-06T02:42:12.581788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#设置使用CPU还是GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:12.584276Z","iopub.execute_input":"2022-05-06T02:42:12.584517Z","iopub.status.idle":"2022-05-06T02:42:12.649353Z","shell.execute_reply.started":"2022-05-06T02:42:12.584483Z","shell.execute_reply":"2022-05-06T02:42:12.648512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#设置随机种子\nSEED = 1024\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:12.650648Z","iopub.execute_input":"2022-05-06T02:42:12.651597Z","iopub.status.idle":"2022-05-06T02:42:12.660885Z","shell.execute_reply.started":"2022-05-06T02:42:12.651545Z","shell.execute_reply":"2022-05-06T02:42:12.660182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"#读取训练数据，看看基本情况，有无缺失值\ntrain_df = pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv.zip\")\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:12.662902Z","iopub.execute_input":"2022-05-06T02:42:12.663193Z","iopub.status.idle":"2022-05-06T02:42:14.649528Z","shell.execute_reply.started":"2022-05-06T02:42:12.663158Z","shell.execute_reply":"2022-05-06T02:42:14.648782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 移除缺失值，因为数据是文本，所以先查看一下数据的长度\ntrain_df.dropna(inplace=True)\ntrain_sentences_lens = train_df['question1'].apply(lambda x: len(x.split(' '))).tolist()\ntrain_sentences_lens.extend(train_df['question2'].apply(lambda x: len(x.split(' '))).tolist())\nsns.distplot(train_sentences_lens)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:14.650819Z","iopub.execute_input":"2022-05-06T02:42:14.651216Z","iopub.status.idle":"2022-05-06T02:42:20.137042Z","shell.execute_reply.started":"2022-05-06T02:42:14.651178Z","shell.execute_reply":"2022-05-06T02:42:20.136384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#我们可以发现长度大于40的数据非常少，所以我们把模型读取文本的最大长度（单词数）设置为40\nMAX_LEN = 40","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:20.138402Z","iopub.execute_input":"2022-05-06T02:42:20.138854Z","iopub.status.idle":"2022-05-06T02:42:20.143614Z","shell.execute_reply.started":"2022-05-06T02:42:20.138817Z","shell.execute_reply":"2022-05-06T02:42:20.14198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#画个图，看看正负样本的分布情况，这个数据集里每条数据由两个问题以及判断它们是否相似的标签组成\ndef pie_chart(similar_questions_num, different_questions_num, set_type):\n    labels = 'Similiar', 'Different'\n    sizes = [similar_questions_num, different_questions_num]\n\n    fig1, ax1 = plt.subplots()\n    ax1.set_title(set_type)\n    ax1.pie(sizes, labels=labels, autopct='%1.2f%%', shadow=True, startangle=90)\n\n    plt.show()\n\nsimilar_samples_num = sum(train_df['is_duplicate'].values)\npie_chart(similar_samples_num, len(train_df['is_duplicate']) - similar_samples_num, 'train set')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:20.14478Z","iopub.execute_input":"2022-05-06T02:42:20.145426Z","iopub.status.idle":"2022-05-06T02:42:20.320054Z","shell.execute_reply.started":"2022-05-06T02:42:20.145386Z","shell.execute_reply":"2022-05-06T02:42:20.319089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#看一下总共有多少个不同的问题\nqids = pd.Series(list(train_df['qid1']) + list(train_df['qid2']))\n\nprint ('Unique Questions number: {}\\n'.format(len(np.unique(qids))))\n\nq_vals=qids.value_counts()[0:5]\nprint ('Top 5 most frequently asked questions: ')\n\nfor pair in q_vals.iteritems():\n    print(train_df.loc[train_df['qid2']==pair[0]]['question1'].head(1).values + \" count: \" + str(pair[1]))\n\nq_vals=q_vals.values","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:20.321835Z","iopub.execute_input":"2022-05-06T02:42:20.322263Z","iopub.status.idle":"2022-05-06T02:42:21.133269Z","shell.execute_reply.started":"2022-05-06T02:42:20.322225Z","shell.execute_reply":"2022-05-06T02:42:21.132484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#看看数据中有没有完全重复的两对问题\nduplicate_rows = train_df[train_df.duplicated(['qid1','qid2'])]\nprint (\"Number of duplicate questions : \", len(duplicate_rows))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:21.134563Z","iopub.execute_input":"2022-05-06T02:42:21.135067Z","iopub.status.idle":"2022-05-06T02:42:21.185582Z","shell.execute_reply.started":"2022-05-06T02:42:21.135027Z","shell.execute_reply":"2022-05-06T02:42:21.184084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#看来没有完全重复的数据，很好，接下来看看有多少问题是独一无二的，多少问题是重复的\nx = [\"Unique\" , \"Repeated\"]\ny =  [len(np.unique(qids)), np.sum(qids.value_counts() > 1)]\n\nplt.figure(figsize=(10, 8))\nplt.title (\"Unique and Repeated questions counts\")\nsns.barplot(x,y)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:21.188867Z","iopub.execute_input":"2022-05-06T02:42:21.189066Z","iopub.status.idle":"2022-05-06T02:42:21.453557Z","shell.execute_reply.started":"2022-05-06T02:42:21.189041Z","shell.execute_reply":"2022-05-06T02:42:21.452538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 看看问题1和问题2中独有词的分布，以及常用词的分布\ndef common_words(row):\n    q1_word_set = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    q2_word_set = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(q1_word_set & q2_word_set)\n\ntrain_df['common_words'] = train_df.apply(common_words, axis=1)\nplt.figure(figsize=(15, 10))\n\nplt.subplot(1,2,2)\nsns.distplot(train_df[train_df['is_duplicate'] == 1]['common_words'][0:] , label = \"1\", color = 'red')\nsns.distplot(train_df[train_df['is_duplicate'] == 0]['common_words'][0:] , label = \"0\" , color = 'blue' )\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'common_words', data = train_df[0:])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:21.455153Z","iopub.execute_input":"2022-05-06T02:42:21.455421Z","iopub.status.idle":"2022-05-06T02:42:36.413786Z","shell.execute_reply.started":"2022-05-06T02:42:21.455386Z","shell.execute_reply":"2022-05-06T02:42:36.413059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**我们可以看到，问题一和问题二中的独有词的分布或多或少是接近的，无论是在正样本还是负样本中**","metadata":{}},{"cell_type":"code","source":"# 我们也可以看看同时出现在问题一和问题二中的词的分布\ndef shared_words(row):\n    q1_word_set = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    q2_word_set = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(q1_word_set & q2_word_set) / (len(q1_word_set) + len(q2_word_set))    \n\ntrain_df['shared_words'] = train_df.apply(shared_words, axis=1)\nplt.figure(figsize=(15, 10))\n\nplt.subplot(1,2,2)\nsns.distplot(train_df[train_df['is_duplicate'] == 1]['shared_words'][0:] , label = \"1\", color = 'red')\nsns.distplot(train_df[train_df['is_duplicate'] == 0]['shared_words'][0:] , label = \"0\" , color = 'green' )\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'shared_words', data = train_df[0:])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:36.414904Z","iopub.execute_input":"2022-05-06T02:42:36.415167Z","iopub.status.idle":"2022-05-06T02:42:50.940155Z","shell.execute_reply.started":"2022-05-06T02:42:36.41513Z","shell.execute_reply":"2022-05-06T02:42:50.939454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**可以看出共有词的分布在正负样本中的分布还是有比较大的差异**","metadata":{}},{"cell_type":"markdown","source":"# Define a Bert model and read data into the model","metadata":{}},{"cell_type":"code","source":"#指定使用的预训练模型，下载\nBERT_VERSION = 'bert-base-uncased'\nPOOLED_OUTPUT_DIM = 768 \ntokenizer = BertTokenizer.from_pretrained(BERT_VERSION)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:50.941638Z","iopub.execute_input":"2022-05-06T02:42:50.942119Z","iopub.status.idle":"2022-05-06T02:42:55.086929Z","shell.execute_reply.started":"2022-05-06T02:42:50.94208Z","shell.execute_reply":"2022-05-06T02:42:55.086104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 将训练数据划分为训练集和验证集\ntrain_df, val_df = train_test_split(train_df, test_size=0.1)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:55.088138Z","iopub.execute_input":"2022-05-06T02:42:55.088478Z","iopub.status.idle":"2022-05-06T02:42:55.254471Z","shell.execute_reply.started":"2022-05-06T02:42:55.088434Z","shell.execute_reply":"2022-05-06T02:42:55.253731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#定义数据集，方便bert读取数据\nclass BertDataSet:\n    def __init__(self, first_questions, second_questions, targets, tokenizer):\n        self.first_questions = first_questions\n        self.second_questions = second_questions\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.length = len(first_questions)\n        \n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, item):\n        first_question = str(self.first_questions[item])\n        second_question = str(self.second_questions[item])\n\n        # 去除问题文本中多余的空格\n        first_question = \" \".join(first_question.split())\n        second_question = \" \".join(second_question.split())\n        \n        ### bert的数据输入格式如下 [CLS] question1 [SEP] questions2 [SEP] ... [PAD]\n        inputs = self.tokenizer.encode_plus(\n            first_question,\n            second_question,\n            add_special_tokens=True,\n            padding='max_length',\n            max_length=2 * MAX_LEN + 3, # max length of 2 questions and 3 special tokens\n            truncation=True   \n        )\n        \n        # 未找到数据时返回0\n        return {\n            \"ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n            \"targets\": torch.tensor(int(self.targets[item]), dtype=torch.long) if self.targets is not None else 0\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:55.255654Z","iopub.execute_input":"2022-05-06T02:42:55.255933Z","iopub.status.idle":"2022-05-06T02:42:55.26624Z","shell.execute_reply.started":"2022-05-06T02:42:55.255894Z","shell.execute_reply":"2022-05-06T02:42:55.265317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 创建数据集并且返回读取器\ndef get_data_loader(df, targets, batch_size, shuffle, tokenizer):\n    dataset = BertDataSet(\n        first_questions=df[\"question1\"].values,\n        second_questions=df[\"question2\"].values,\n        targets=targets,\n        tokenizer=tokenizer\n    )\n    \n    data_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size = batch_size,\n        shuffle=shuffle\n    )\n    \n    return data_loader","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:55.267641Z","iopub.execute_input":"2022-05-06T02:42:55.268478Z","iopub.status.idle":"2022-05-06T02:42:55.275017Z","shell.execute_reply.started":"2022-05-06T02:42:55.268434Z","shell.execute_reply":"2022-05-06T02:42:55.274082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BS = 128","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:55.276282Z","iopub.execute_input":"2022-05-06T02:42:55.276693Z","iopub.status.idle":"2022-05-06T02:42:55.289877Z","shell.execute_reply.started":"2022-05-06T02:42:55.276649Z","shell.execute_reply":"2022-05-06T02:42:55.289122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 使用数据读取器读取训练数据和验证数据\ntrain_data_loader = get_data_loader(\n    df=train_df,\n    targets=train_df[\"is_duplicate\"].values,\n    batch_size=BS,\n    shuffle=True,\n    tokenizer=tokenizer\n)\n\nval_data_loader = get_data_loader(\n    df=val_df,\n    targets=val_df[\"is_duplicate\"].values,\n    batch_size=4 * BS,\n    shuffle=True,\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:55.29082Z","iopub.execute_input":"2022-05-06T02:42:55.291526Z","iopub.status.idle":"2022-05-06T02:42:55.300287Z","shell.execute_reply.started":"2022-05-06T02:42:55.29149Z","shell.execute_reply":"2022-05-06T02:42:55.299433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#定义bert模型\nclass BertModel(nn.Module):\n    def __init__(self, bert_path):\n        super(BertModel, self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n        self.dropout = nn.Dropout(0.3)\n        self.out = nn.Linear(POOLED_OUTPUT_DIM, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, pooled = self.bert(ids, attention_mask=mask,token_type_ids=token_type_ids,return_dict=False)\n        \n        # add dropout to prevent overfitting.\n        pooled = self.dropout(pooled) \n        return self.out(pooled)\n\n#为了方便这里直接实例化模型\nmodel = BertModel(BERT_VERSION).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:42:55.302346Z","iopub.execute_input":"2022-05-06T02:42:55.30252Z","iopub.status.idle":"2022-05-06T02:43:12.314509Z","shell.execute_reply.started":"2022-05-06T02:42:55.302499Z","shell.execute_reply":"2022-05-06T02:43:12.31378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 因为这里是二分类问题，所以使用交叉熵损失，并将结果用sigmoid缩放到（0，1）之间\ndef loss_fn(outputs, targets):\n    outputs = torch.squeeze(outputs)\n    return nn.BCELoss()(nn.Sigmoid()(outputs), targets)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:43:12.315688Z","iopub.execute_input":"2022-05-06T02:43:12.315934Z","iopub.status.idle":"2022-05-06T02:43:12.321203Z","shell.execute_reply.started":"2022-05-06T02:43:12.3159Z","shell.execute_reply":"2022-05-06T02:43:12.320428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 在验证集计算损失和困惑度\ndef calculate_perplexity(data_loader, model, device):\n    model.eval()\n    \n    # 因为网络中层的梯度基本用不到，所以我们不必保存它们\n    with torch.no_grad():\n        total_loss = 0\n        for batch in data_loader:\n            ids = batch[\"ids\"].to(device, dtype=torch.long)\n            mask = batch[\"mask\"].to(device, dtype=torch.long)\n            token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n            targets = batch[\"targets\"].to(device, dtype=torch.float)\n\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            total_loss += loss_fn(outputs, targets).item()\n            \n    model.train()\n\n    return np.exp(total_loss / len(data_loader))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:43:12.322498Z","iopub.execute_input":"2022-05-06T02:43:12.3229Z","iopub.status.idle":"2022-05-06T02:43:12.332876Z","shell.execute_reply.started":"2022-05-06T02:43:12.322863Z","shell.execute_reply":"2022-05-06T02:43:12.332268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Training","metadata":{}},{"cell_type":"code","source":"#一切准备就绪，定义训练函数\ndef train_loop(epochs, train_data_loader, val_data_loader, model, optimizer, device, scheduler=None):\n    it = 1\n    total_loss = 0\n    curr_perplexity = None\n    perplexity = None\n    \n    model.train()\n    for epoch in range(epochs):\n        print('Epoch: ', epoch + 1)\n        for batch in train_data_loader:\n            ids = batch[\"ids\"].to(device, dtype=torch.long)\n            mask = batch[\"mask\"].to(device, dtype=torch.long)\n            token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n            targets = batch[\"targets\"].to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n            \n            # 正向传播，计算损失\n            outputs = model(ids, mask=mask, token_type_ids=token_type_ids)\n            \n            loss = loss_fn(outputs, targets)\n            total_loss += loss.item()\n            \n            # 反向传播，计算梯度\n            loss.backward()\n            \n            # 训练，用优化器更新模型中的参数\n            optimizer.step()\n            \n            # 每隔100个iter输出一次训练情况（损失、困惑度）\n            if it % 100 == 0:\n                \n                # 每隔500个iter计算一次困惑度\n                if it % 500 == 0:\n                    curr_perplexity = calculate_perplexity(val_data_loader, model, device)\n                    \n                    if scheduler is not None:\n                        scheduler.step()\n\n                    # 记录最优的模型参数\n                    if not perplexity or curr_perplexity < perplexity:\n                        torch.save(model.state_dict(), 'saved_model')\n                        perplexity = curr_perplexity\n\n                print('| Iter', it, '| Avg Train Loss', total_loss / 100, '| Dev Perplexity', curr_perplexity)\n                total_loss = 0\n\n            it += 1","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:43:12.334315Z","iopub.execute_input":"2022-05-06T02:43:12.33476Z","iopub.status.idle":"2022-05-06T02:43:12.346723Z","shell.execute_reply.started":"2022-05-06T02:43:12.334723Z","shell.execute_reply":"2022-05-06T02:43:12.345991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#运行训练的函数\ndef run(model, train_df, device, train_data_loader, val_data_loader):\n    EPOCHS = 1\n    \n    lr = 3e-5\n    num_training_steps = int(len(train_data_loader) * EPOCHS)\n    optimizer = AdamW(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n    \n    \n    train_loop(EPOCHS, train_data_loader, val_data_loader,  model, optimizer, device, scheduler)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:43:12.348188Z","iopub.execute_input":"2022-05-06T02:43:12.348847Z","iopub.status.idle":"2022-05-06T02:43:12.35815Z","shell.execute_reply.started":"2022-05-06T02:43:12.348763Z","shell.execute_reply":"2022-05-06T02:43:12.357242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"#模型训练\nrun(model, train_df, device, train_data_loader, val_data_loader)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T02:43:12.360476Z","iopub.execute_input":"2022-05-06T02:43:12.361789Z","iopub.status.idle":"2022-05-06T03:43:09.115632Z","shell.execute_reply.started":"2022-05-06T02:43:12.361755Z","shell.execute_reply":"2022-05-06T03:43:09.114881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test and get result","metadata":{}},{"cell_type":"code","source":"#读入测试数据\ntest_df = pd.read_csv(\"/kaggle/input/quora-question-pairs/test.csv\")\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T03:43:09.117335Z","iopub.execute_input":"2022-05-06T03:43:09.117567Z","iopub.status.idle":"2022-05-06T03:43:17.501155Z","shell.execute_reply.started":"2022-05-06T03:43:09.117537Z","shell.execute_reply":"2022-05-06T03:43:17.500425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#在测试集上进行预测，因为我的模型参数有点多，所以时间比较久，设置一个进度条来看看\ndef test(model, test_df, device):\n    predictions = torch.empty(0).to(device, dtype=torch.float)\n    \n    test_dataset = BertDataSet(\n        first_questions=test_df[\"question1\"].values,\n        second_questions=test_df[\"question2\"].values,\n        targets=None,\n        tokenizer=tokenizer\n    )\n    \n    test_data_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=512\n    )\n    \n    with torch.no_grad():\n        model.eval()\n        for batch in tqdm(test_data_loader):\n            ids = batch[\"ids\"]\n            mask = batch[\"mask\"]\n            token_type_ids = batch[\"token_type_ids\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            predictions = torch.cat((predictions, nn.Sigmoid()(outputs)))\n    \n    return predictions.cpu().numpy().squeeze()\n\npredictions = test(model, test_df, device)\nlen(predictions)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T03:43:17.502334Z","iopub.execute_input":"2022-05-06T03:43:17.503776Z","iopub.status.idle":"2022-05-06T05:11:23.280448Z","shell.execute_reply.started":"2022-05-06T03:43:17.503718Z","shell.execute_reply":"2022-05-06T05:11:23.279004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 将预测结果记录\ntest_df['is_duplicate'] = predictions","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:11:23.281648Z","iopub.execute_input":"2022-05-06T05:11:23.281966Z","iopub.status.idle":"2022-05-06T05:11:23.288803Z","shell.execute_reply.started":"2022-05-06T05:11:23.281929Z","shell.execute_reply":"2022-05-06T05:11:23.288006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 将结果保存到submission.csv\ntest_df[['test_id', 'is_duplicate']].to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:11:23.292681Z","iopub.execute_input":"2022-05-06T05:11:23.292971Z","iopub.status.idle":"2022-05-06T05:11:29.365174Z","shell.execute_reply.started":"2022-05-06T05:11:23.292932Z","shell.execute_reply":"2022-05-06T05:11:29.364311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Demonstration","metadata":{}},{"cell_type":"code","source":"# 这是我用于演示的函数，可以不要\ndef eval(model, tokenizer, first_question, second_question, device):\n    inputs = tokenizer.encode_plus(\n        first_question,\n        second_question,\n        add_special_tokens=True,\n    )\n\n    ids = torch.tensor([inputs[\"input_ids\"]], dtype=torch.long).to(device, dtype=torch.long)\n    mask = torch.tensor([inputs[\"attention_mask\"]], dtype=torch.long).to(device, dtype=torch.long)\n    token_type_ids = torch.tensor([inputs[\"token_type_ids\"]], dtype=torch.long).to(device, dtype=torch.long)\n\n    with torch.no_grad():\n        model.eval()\n        output = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n        prob = nn.Sigmoid()(output).item()\n\n        print(\"questions [{}] and [{}] are {} with score {}\".format(first_question, second_question, 'similar' if prob > 0.5 else 'not similar', prob))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:11:29.366385Z","iopub.execute_input":"2022-05-06T05:11:29.366672Z","iopub.status.idle":"2022-05-06T05:11:29.3754Z","shell.execute_reply.started":"2022-05-06T05:11:29.366626Z","shell.execute_reply":"2022-05-06T05:11:29.374472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 我们输入两个问题来看它们是否相似\nfirst_question = \"how to register on hackerrank with google account?\"\nsecond_question = \"Can I sign using google account on hackerrank?\"\n\neval(model, tokenizer, first_question, second_question, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}